{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kaiye/Desktop/COMPSCI646/LaMP-Retreival/src\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from src.model_tokenizer_generator import *\n",
    "from src.dataset_generator import *\n",
    "from src.models.model_utils import *\n",
    "from src.models.T5DataLoader import T5DataLoader\n",
    "from src.models.T5Model import T5Model\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "%cd src"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-29T18:02:56.218469Z",
     "start_time": "2023-11-29T18:02:51.279199Z"
    }
   },
   "id": "3864b447a2f210f0"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.0 (v3.9.0:9cf6752276, Oct  5 2020, 11:29:23) \n",
      "[Clang 6.0 (clang-600.0.57)]\n"
     ]
    }
   ],
   "source": [
    "print(sys.version)\n",
    "device = torch.device(\"cpu\")\n",
    "INPUT_MAX_LEN = 128 #input length\n",
    "OUTPUT_MAX_LEN = 128 # output length\n",
    "TRAIN_BATCH_SIZE = 8 # batch size of training\n",
    "VAL_BATCH_SIZE = 2 # batch size for validation\n",
    "EPOCHS = 5 # number of epoch\n",
    "T5_L_MODEL_PATH = \"./models/T5_model/flan-t5-large/config.json\"\n",
    "T5_L_TOKENIZER_PATH = \"./models/T5_model/flan-t5-large/\"\n",
    "check_point_path = \"./models/T5_model/fine-tune-check-points\"\n",
    "MISTRAVL_MODEL_PATH = \"./models/Mistravl/Mistral-7B/config.json\"\n",
    "MISTRAVL_TOKENIZER_PATH = \"./models/Mistravl/Mistral-7B\"\n",
    "NEURAL_CHAT_7B = \"Intel/neural-chat-7b-v3-1\"\n",
    "NEURAL_CHAT_7B_PATH = \"./model/Intel/neural-chat-7b-v3-1\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-29T18:02:57.465178Z",
     "start_time": "2023-11-29T18:02:57.448773Z"
    }
   },
   "id": "a4d94171939c1329"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "LaMP_1_val_X_prompted, LaMP_1_val_y_prompted = generate_np_X_y(\"dev\", \"LaMP_1\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-29T18:04:17.943358Z",
     "start_time": "2023-11-29T18:04:17.914651Z"
    }
   },
   "id": "bcd4813b9a2f155a"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Using Magnetic RAM to Build Low-Power and Soft Error-Resilient L1 Cache':\n",
      "title: \"Quasi-nonvolatile SSD: Trading flash memory nonvolatility to improve storage system performance for enterprise applications\" with keywords: [ssd, data, memory, system, refresh]\n",
      "title: \"Architecting high-performance energy-efficient soft error resilient cache under 3D integration technology\" with keywords: [cache, soft, dy, performance, access]\n",
      "title: \"Improving STT MRAM storage density through smaller-than-worst-case transistor sizing\" with keywords: [memory, transistor, design, storage, stt]\n",
      "title: \"Exploiting memory device wear-out dynamics to improve NAND flash memory system performance\" with keywords: [memory, improve, device, cycling, design]\n",
      "title: \"Impacts Of Though-Dram Vias In 3d Processor-Dram Integrated Systems\" with keywords: [dram, design, through-dram, tsvs, address]\n",
      "Given above information, for an author who has written the paper with the title \"Using Magnetic RAM to Build Low-Power and Soft Error-Resilient L1 Cache\", which reference is related? Just answer with [1] or [2] without explanation. [1]: \"Severless Search and Authentication Protocols for RFID\" [2]: \"Dynamic Backlight Scaling Optimization: A Cloud-Based Energy-Saving Service for Mobile Streaming Applications\"\n"
     ]
    }
   ],
   "source": [
    "print(LaMP_1_val_X_prompted[2])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-29T18:14:24.766216Z",
     "start_time": "2023-11-29T18:14:24.760013Z"
    }
   },
   "id": "3573f9c5823caffa"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='[1] \"Pronunciation Modeling for Improved Spelling Correction\"', role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": LaMP_1_val_X_prompted[0]}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-29T18:09:19.496765Z",
     "start_time": "2023-11-29T18:09:15.469705Z"
    }
   },
   "id": "eeaffdfc86d32793"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "fb38d833880bb413"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
