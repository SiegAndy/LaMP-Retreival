[
    {
        "id": "010",
        "input": "For an author who has written the paper with the title \"Visual-audio integration for user authentication system of partner robots\", which reference is related? Just answer with [1] or [2] without explanation. [1]: \"Pronunciation Modeling for Improved Spelling Correction\" [2]: \"Path following algorithm for highly redundant manipulators\"",
        "profile": [
            {
                "title": "Accurate Estimators for Improving Minwise Hashing and b-Bit Minwise Hashing",
                "abstract": "  Minwise hashing is the standard technique in the context of search and databases for efficiently estimating set (e.g., high-dimensional 0/1 vector) similarities. Recently, b-bit minwise hashing was proposed which significantly improves upon the original minwise hashing in practice by storing only the lowest b bits of each hashed value, as opposed to using 64 bits. b-bit hashing is particularly effective in applications which mainly concern sets of high similarities (e.g., the resemblance >0.5). However, there are other important applications in which not just pairs of high similarities matter. For example, many learning algorithms require all pairwise similarities and it is expected that only a small fraction of the pairs are similar. Furthermore, many applications care more about containment (e.g., how much one object is contained by another object) than the resemblance. In this paper, we show that the estimators for minwise hashing and b-bit minwise hashing used in the current practice can be systematically improved and the improvements are most significant for set pairs of low resemblance and high containment. ",
                "id": "0100"
            },
            {
                "title": "b-Bit Minwise Hashing for Large-Scale Linear SVM",
                "abstract": "  In this paper, we propose to (seamlessly) integrate b-bit minwise hashing with linear SVM to substantially improve the training (and testing) efficiency using much smaller memory, with essentially no loss of accuracy. Theoretically, we prove that the resemblance matrix, the minwise hashing matrix, and the b-bit minwise hashing matrix are all positive definite matrices (kernels). Interestingly, our proof for the positive definiteness of the b-bit minwise hashing kernel naturally suggests a simple strategy to integrate b-bit hashing with linear SVM. Our technique is particularly useful when the data can not fit in memory, which is an increasingly critical issue in large-scale machine learning. Our preliminary experimental results on a publicly available webspam dataset (350K samples and 16 million dimensions) verified the effectiveness of our algorithm. For example, the training time was reduced to merely a few seconds. In addition, our technique can be easily extended to many other linear and nonlinear machine learning applications such as logistic regression. ",
                "id": "0101"
            },
            {
                "title": "Learning Structured Low-Rank Representation via Matrix Factorization.",
                "abstract": "A vast body of recent works in the literature have shown that exploring structures beyond data low-rankness can boost the performance of subspace clustering methods such as Low-Rank Representation (LRR). It has also been well recognized that the matrix factorization framework might offer more flexibility on pursuing underlying structures of the data. In this paper, we propose to learn structured LRR by factorizing the nuclear norm regularized matrix, which leads to our proposed non-convex formulation NLRR. Interestingly, this formulation of NLRR provides a general framework for unifying a variety of popular algorithms including LRR, dictionary learning, robust principal component analysis, sparse subspace clustering, etc. Several variants of NLRR are also proposed, for example, to promote sparsity while preserving low-rankness. We design a practical algorithm for NLRR and its variants, and establish theoretical guarantee for the stability of the solution and the convergence of the algorithm. Perhaps surprisingly, the computational and memory cost of NLRR can be reduced by roughly one order of magnitude compared to the cost of LRR. Experiments on extensive simulations and real datasets confirm the robustness of efficiency of NLRR and the variants.",
                "id": "0102"
            },
            {
                "title": "Improved Asymmetric Locality Sensitive Hashing (ALSH) for Maximum Inner Product Search (MIPS)",
                "abstract": "Recently we showed that the problem of Maximum Inner Product Search (MIPS) is efficient and it admits provably sub-linear hashing algorithms. In [23], we used asymmetric transformations to convert the problem of approximate MIPS into the problem of approximate near neighbor search which can be efficiently solved using L2-LSH. In this paper, we revisit the problem of MIPS and argue that the quantizations used in L2-LSH is suboptimal for MIPS compared to signed random projections (SRP) which is another popular hashing scheme for cosine similarity (or correlations). Based on this observation, we provide different asymmetric transformations which convert the problem of approximate MIPS into the problem amenable to SRP instead of L2-LSH. An additional advantage of our scheme is that we also obtain LSH type space partitioning which is not possible with the existing scheme. Our theoretical analysis shows that the new scheme is significantly better than the original scheme for MIPS. Experimental evaluations strongly support the theoretical findings. In addition, we also provide the first empirical comparison that shows the superiority of hashing over tree based methods [21] for MIPS.",
                "id": "0103"
            },
            {
                "title": "On Practical Algorithms for Entropy Estimation and the Improved Sample Complexity of Compressed Counting",
                "abstract": "Estimating the p-th frequency moment of data stream is a very heavily studied\nproblem. The problem is actually trivial when p = 1, assuming the strict\nTurnstile model. The sample complexity of our proposed algorithm is essentially\nO(1) near p=1. This is a very large improvement over the previously believed\nO(1/eps^2) bound. The proposed algorithm makes the long-standing problem of\nentropy estimation an easy task, as verified by the experiments included in the\nappendix.",
                "id": "0104"
            },
            {
                "title": "Interval Type-2 Fuzzy Model Based On Inverse Controller Design For The Outlet Temperature Control System Of Ethylene Cracking Furnace",
                "abstract": "Multivariable coupling, nonlinear and large time delays exist in the coil outlet temperature (COT) control system of the ethylene cracking furnace, which make it hard to achieve accurate control over the COT of the furnace in actual production. To solve these problems, an inverse controller based on an interval type-2 fuzzy model control strategy is introduced. In this paper, the proposed control scheme is divided into two parts: one is the approach structure part of the interval type-2 fuzzy model (IT2-FM), which is utilized to approach the process output. The other is the interval type-2 fuzzy model inverse controller (IT2-FMIC) part, which is utilized to control the output process to achieve the target value. In addition, on the cyber-physical system platform, the actual industrial data are used to test and obtain the mathematical model of the COT control system of the ethylene cracking furnace. Finally, the proposed inverse controller based on the IT2-FM control scheme has been implemented on the COT control system of the ethylene cracking furnace, and the simulation results show that the proposed method is feasible.",
                "id": "0105"
            },
            {
                "title": "Compressed Nonnegative Sparse Coding",
                "abstract": "Sparse Coding (SC), which models the data vectors as sparse linear combinations over basis vectors, has been widely applied in machine learning, signal processing and neuroscience. In this paper, we propose a dual random projection method to provide an efficient solution to Nonnegative Sparse Coding (NSC) using small memory. Experiments on real world data demonstrate the effectiveness of the proposed method.",
                "id": "0106"
            },
            {
                "title": "The Visual-Audio Integrated Recognition Method For User Authentication System Of Partner Robots",
                "abstract": "Some of noncontact biometric ways have been used for user authentication system of partner robots, such as visual-based recognition methods and speech recognition. However, the methods of visual-based recognition are sensitive to the light noise and speech recognition systems are perturbed to the acoustic environment and sound noise. Inspiring from the human's capability of compensating visual information (looking) with audio information (hearing), a visual-audio integrating method is proposed to deal with the disturbance of light noise and to improve the recognition accuracy. Combining with the PCA-based and 2DPCA-based face recognition, a two-stage speaker recognition algorithm is used to extract useful personal identity information from speech signals. With the statistic properties of visual background noise, the visual-audio integrating method is performed to draw the final decision. The proposed method is evaluated on a public visual-audio dataset VidTIMIT and a partner robot authentication system. The results verified the visual-audio integrating method can obtain satisfied recognition results with strong robustness.",
                "id": "0107"
            },
            {
                "title": "Query spelling correction using multi-task learning",
                "abstract": "This paper explores the use of online multi-task learning for search query spelling correction, by effectively transferring information from different and biased training datasets for improving spelling correction across datasets. Experiments were conducted on three query spelling correction datasets, including the well-known TREC benchmark data. Our experimental results demonstrate that the proposed method considerably outperforms existing baseline systems in terms of accuracy. Importantly, the proposed method is about one-order of magnitude faster than baseline systems in terms of training speed. In contrast to existing methods which typically require more than (e.g.,) 50 training passes, our algorithm can very closely approach the empirical optimum in around five passes.",
                "id": "0108"
            },
            {
                "title": "A Very Efficient Scheme for Estimating Entropy of Data Streams Using Compressed Counting",
                "abstract": "Compressed Counting (CC) was recently proposed for approximating the \u03b1th frequency moments of data streams, for 0 < \u03b1 \u2264 2. Under the relaxed strict-Turnstile model, CC dramatically improves the standard algorithm based on symmetric stable random projections, especially as \u03b1 \u2192 1. A direct application of CC is to estimate the entropy, which is an important summary statistic in Web/network measure- ment and often serves a crucial \"feature\" for data mining. The Renyi entropy and the Tsallis entropy are functions of the \u03b1th frequency moments; and both approach the Shan- non entropy as \u03b1 \u2192 1. A recent theoretical work suggested using the \u03b1th frequency moment to approximate the Shan- non entropy with \u03b1 = 1+\u03b4 and very small |\u03b4| (e.g., < 10\u22124). In this study, we experiment using CC to estimate fre- quency moments, Renyi entropy, Tsallis entropy, and Shan- non entropy, on real Web crawl data. We demonstrate the variance-bias trade-off in estimating Shannon entropy and provide practical recommendations. In particular, our ex- periments enable us to draw some important conclusions: \u2022 As \u03b1 \u2192 1, CC dramatically improves symmetric stable random projections in estimating frequency moments, Renyi entropy, Tsallis entropy, and Shannon entropy. The improvements appear to approach \"infinity.\"",
                "id": "0109"
            },
            {
                "title": "FastInput: Improving Input Efficiency on Mobile Devices.",
                "abstract": "Mobile devices (e.g., smartphones) play a crucial role in our daily lives nowadays. People rely heavily on mobile devices for searching online, sending emails, chatting with friends, etc. As a result, input efficiency becomes increasingly important for real-time communication on mobile devices. Due to the small size of the screen on mobile devices, however, it is oftentimes frustrating for users to correct or update the input sequences on an even smaller input area on the screen. This often causes poor user experience. In this paper, we focus on improving the input efficiency on mobile devices to offer better user experience. In order to achieve efficient input, there are multiple challenges: 1) how to employ a single, unified representation of the keyboard layouts for different input languages; 2) how to build a framework to correct a mistouch immediately and predict the coming input texts (words or phrases) effectively; 3) how to deploy and evaluate the model on mobile devices with limited computational power. To address these challenges, we introduce \\em FastInput to improve the user input efficiency on mobile devices. Three key techniques are developed in FastInput -- layout modeling, instant mistouch correction and user input text prediction. We also design solutions for efficient deployment and evaluation of FastInput on mobile devices. The proposed FastInput achieves higher efficiency compared to the traditional input system over millions of user input sequences in different languages.\n\n",
                "id": "01010"
            },
            {
                "title": "Visual-audio integration for user authentication system of partner robots",
                "abstract": "As one of non-contact biometric way for user authentication system of partner robots, visual-based recognition methods still suffer from the disturbance of light noise in the applications. Inspiring from the human's capability of compensating visual information (looking) with audio information(hearing), a visual-audio integrating method is proposed to reduce the disturbance of light noise and to improve the recognition accuracy. Combining with the PCA-based face recognition, a two-stage speaker recognition algorithm is used to extract useful personal identity information from speech signals. With the measurement of visual background noise, the visual-audio integrating method is performed to draw the final decision. The proposed method is evaluated on a public visual-audio dataset VidTIMIT and a partner robot authentication system. The results verified the visual-audio integrating method can obtain satisfied recognition results with strong robustness.",
                "id": "01011"
            },
            {
                "title": "GPU-based minwise hashing: GPU-based minwise hashing",
                "abstract": "Minwise hashing is a standard technique for efficient set similarity estimation in the context of search. The recent work of b-bit minwise hashing provided a substantial improvement by storing only the lowest b bits of each hashed value. Both minwise hashing and b-bit minwise hashing require an expensive preprocessing step for applying k (e.g., k=500) permutations on the entire data in order to compute k minimal values as the hashed data. In this paper, we developed a parallelization scheme using GPUs, which reduced the processing time by a factor of 20-80. Reducing the preprocessing time is highly beneficial in practice, for example, for duplicate web page detection (where minwise hashing is a major step in the crawling pipeline) or for increasing the testing speed of online classifiers (when the test data are not preprocessed).",
                "id": "01012"
            },
            {
                "title": "Beyond Pairwise: Provably Fast Algorithms for Approximate $k$-Way Similarity Search",
                "abstract": "Use the \"Report an Issue\" link to request a name change.",
                "id": "01013"
            },
            {
                "title": "b-bit Marginal Regression",
                "abstract": "We consider the problem of sparse signal recovery from m linear measurements quantized to b bits. b-bit Marginal Regression is proposed as recovery algorithm. We study the question of choosing b in the setting of a given budget of bits B = m \u00b7 b and derive a single easy-to-compute expression characterizing the trade-off between m and b. The choice b = 1 turns out to be optimal for estimating the unit vector corresponding to the signal for any level of additive Gaussian noise before quantization as well as for adversarial noise. For b \u2265 2, we show that Lloyd-Max quantization constitutes an optimal quantization scheme and that the norm of the signal can be estimated consistently by maximum likelihood by extending [15].",
                "id": "01014"
            },
            {
                "title": "Knowledge Graph Embedding Based Question Answering.",
                "abstract": "Question answering over knowledge graph (QA-KG) aims to use facts in the knowledge graph (KG) to answer natural language questions. It helps end users more efficiently and more easily access the substantial and valuable knowledge in the KG, without knowing its data structures. QA-KG is a nontrivial problem since capturing the semantic meaning of natural language is difficult for a machine. Meanwhile, many knowledge graph embedding methods have been proposed. The key idea is to represent each predicate/entity as a low-dimensional vector, such that the relation information in the KG could be preserved. The learned vectors could benefit various applications such as KG completion and recommender systems. In this paper, we explore to use them to handle the QA-KG problem. However, this remains a challenging task since a predicate could be expressed in different ways in natural language questions. Also, the ambiguity of entity names and partial names makes the number of possible answers large. To bridge the gap, we propose an effective Knowledge Embedding based Question Answering (KEQA) framework. We focus on answering the most common types of questions, i.e., simple questions, in which each question could be answered by the machine straightforwardly if its single head entity and single predicate are correctly identified. To answer a simple question, instead of inferring its head entity and predicate directly, KEQA targets at jointly recovering the question's head entity, predicate, and tail entity representations in the KG embedding spaces. Based on a carefully-designed joint distance metric, the three learned vectors' closest fact in the KG is returned as the answer. Experiments on a widely-adopted benchmark demonstrate that the proposed KEQA outperforms the state-of-the-art QA-KG methods.\n\n",
                "id": "01015"
            },
            {
                "title": "One Scan 1-Bit Compressed Sensing",
                "abstract": "Based on a-stable random projections with small a, we develop a simple algorithm for compressed sensing (sparse signal recovery) by utilizing only the signs (i.e., 1-bit) of the measurements. Using only 1-bit information of the measurements results in substantial cost reduction in collection, storage, communication, and decoding for compressed sensing. The proposed algorithm is efficient in that the decoding procedure requires only one scan of the coordinates. Our analysis can precisely show that, for a K-sparse signal of length N, 12.3K logN/delta measurements (where delta is the confidence) would be sufficient for recovering the support and the signs of the signal. While the method is highly robust against typical measurement noises, we also provide the analysis of the scheme under random flipping of the signs of measurements. Compared to the well-known work on 1-bit marginal regression (which can also be viewed as a one-scan method), the proposed algorithm requires orders of magnitude fewer measurements. Compared to 1-bit Iterative Hard Thresholding (IHT) (which is not a one-scan algorithm), our method is still significantly more accurate. Furthermore, the proposed method is reasonably robust against random sign flipping while IHT is known to be very sensitive to this type of noise.",
                "id": "01016"
            },
            {
                "title": "One Permutation Hashing.",
                "abstract": "Use the \"Report an Issue\" link to request a name change.",
                "id": "01017"
            },
            {
                "title": "A Sketch Algorithm for Estimating Two-Way and Multi-Way Associations",
                "abstract": "We should not have to look at the entire corpus (e.g., the Web) to know if two (or more) words are strongly associated or not. One can often obtain estimates of associations from a small sample. We develop a sketch-based algorithm that constructs a contingency table for a sample. One can estimate the contingency table for the entire population using straightforward scaling. However, one can do better by taking advantage of the margins (also known as document frequencies). The proposed method cuts the errors roughly in half over Broder's sketches.",
                "id": "01018"
            },
            {
                "title": "Advancing Matrix Completion by Modeling Extra Structures beyond Low-Rankness.",
                "abstract": "  A well-known method for completing low-rank matrices based on convex optimization has been established by Cand{\\`e}s and Recht. Although theoretically complete, the method may not entirely solve the low-rank matrix completion problem. This is because the method captures only the low-rankness property which gives merely a rough constraint that the data points locate on some low-dimensional subspace, but generally ignores the extra structures which specify in more detail how the data points locate on the subspace. Whenever the geometric distribution of the data points is not uniform, the coherence parameters of data might be large and, accordingly, the method might fail even if the latent matrix we want to recover is fairly low-rank. To better handle non-uniform data, in this paper we propose a method termed Low-Rank Factor Decomposition (LRFD), which imposes an additional restriction that the data points must be represented as linear combinations of the bases in a dictionary constructed or learnt in advance. We show that LRFD can well handle non-uniform data, provided that the dictionary is configured properly: We mathematically prove that if the dictionary itself is low-rank then LRFD is immune to the coherence parameters which might be large on non-uniform data. This provides an elementary principle for learning the dictionary in LRFD and, naturally, leads to a practical algorithm for advancing matrix completion. Extensive experiments on randomly generated matrices and motion datasets show encouraging results. ",
                "id": "01019"
            },
            {
                "title": "Collaborative Multi-objective Ranking.",
                "abstract": "This paper proposes to jointly resolve row-wise and column-wise ranking problems when an explicit rating matrix is given. The row-wise ranking problem, also known as personalized ranking, aims to build user-specific models such that the correct order of items (in terms of user preference) is most accurately predicted and then items on the top of ranked list will be recommended to a specific user, while column-wise ranking aims to build item-specific models focusing on targeting users who are most interested in the specific item (for example, for distributing coupons to customers). In recommender systems, ranking-based collaborative filtering (known as collaborative ranking (CR)) algorithms are designed to solve the aforementioned ranking problems. The key part of CR algorithms is to learn effective user and item latent factors which are combined to decide user preference scores over items. In this paper, we demonstrate that by individually solving row-wise or column-wise ranking problems using typical CR algorithms is only able to learn one set of effective (user or item) latent factors. Therefore, we propose to jointly solve row-wise and column-wise ranking problems through a parameter sharing framework which optimizes three objectives together: to accurately predict rating scores, to satisfy the user-specific order constraints on all the rated items, and to satisfy the item-specific order constraints. Our extensive experimental results on popular datasets confirm significant performance gains of our proposed method over state-of-the-art CR approaches in both of row-wise and column-wise ranking tasks.\n\n",
                "id": "01020"
            },
            {
                "title": "Robust LogitBoost and Adaptive Base Class (ABC) LogitBoost",
                "abstract": "  Logitboost is an influential boosting algorithm for classification. In this paper, we develop robust logitboost to provide an explicit formulation of tree-split criterion for building weak learners (regression trees) for logitboost. This formulation leads to a numerically stable implementation of logitboost. We then propose abc-logitboost for multi-class classification, by combining robust logitboost with the prior work of abc-boost. Previously, abc-boost was implemented as abc-mart using the mart algorithm. Our extensive experiments on multi-class classification compare four algorithms: mart, abcmart, (robust) logitboost, and abc-logitboost, and demonstrate the superiority of abc-logitboost. Comparisons with other learning methods including SVM and deep learning are also available through prior publications. ",
                "id": "01021"
            },
            {
                "title": "Conditional Random Sampling: A Sketch-based Sampling Technique for Sparse Data",
                "abstract": "We1 develop Conditional Random Sampling (CRS) , a technique particularly suit- able for sparse data. In large-scale applications, the data are often highly sparse. CRS combines sketching and sampling in that it converts sketches of the data into conditional random samplesonline in the estimation stage, with the sample size determined retrospectively. This paper focuses on approximating pairwise l2 and l1 distances and comparing CRS with random projections. For boolean (0/1) data, CRS is provably better than random projections. We show using real-world data that CRS often outperforms random projections. This technique can be applied in learning, data mining, information retrieval, and databas e query optimizations.",
                "id": "01022"
            },
            {
                "title": "ABC-boost: adaptive base class boost for multi-class classification",
                "abstract": "We propose abc-boost (adaptive base class boost) for multi-class classification and present abc-mart, an implementation of abc-boost, based on the multinomial logit model. The key idea is that, at each boosting iteration, we adaptively and greedily choose a base class. Our experiments on public datasets demonstrate the improvement of abc-mart over the original mart algorithm.",
                "id": "01023"
            },
            {
                "title": "Theory and applications of b-bit minwise hashing",
                "abstract": "Efficient (approximate) computation of set similarity in very large datasets is a common task with many applications in information retrieval and data management. One common approach for this task is minwise hashing. This paper describes b-bit minwise hashing, which can provide an order of magnitude improvements in storage requirements and computational overhead over the original scheme in practice. We give both theoretical characterizations of the performance of the new algorithm as well as a practical evaluation on large real-life datasets and show that these match very closely. Moreover, we provide a detailed comparison with other important alternative techniques proposed for estimating set similarities. Our technique yields a very simple algorithm and can be realized with only minor modifications to the original minwise hashing scheme.",
                "id": "01024"
            },
            {
                "title": "Training Logistic Regression and SVM on 200GB Data Using b-Bit Minwise Hashing and Comparisons with Vowpal Wabbit (VW)",
                "abstract": "  We generated a dataset of 200 GB with 10^9 features, to test our recent b-bit minwise hashing algorithms for training very large-scale logistic regression and SVM. The results confirm our prior work that, compared with the VW hashing algorithm (which has the same variance as random projections), b-bit minwise hashing is substantially more accurate at the same storage. For example, with merely 30 hashed values per data point, b-bit minwise hashing can achieve similar accuracies as VW with 2^14 hashed values per data point.   We demonstrate that the preprocessing cost of b-bit minwise hashing is roughly on the same order of magnitude as the data loading time. Furthermore, by using a GPU, the preprocessing cost can be reduced to a small fraction of the data loading time.   Minwise hashing has been widely used in industry, at least in the context of search. One reason for its popularity is that one can efficiently simulate permutations by (e.g.,) universal hashing. In other words, there is no need to store the permutation matrix. In this paper, we empirically verify this practice, by demonstrating that even using the simplest 2-universal hashing does not degrade the learning performance. ",
                "id": "01025"
            },
            {
                "title": "Partial Hard Thresholding: Towards A Principled Analysis of Support Recovery.",
                "abstract": "In machine learning and compressed sensing, it is of central importance to understand when a tractable algorithm recovers the support of a sparse signal from its compressed measurements. In this paper, we present a principled analysis on the support recovery performance for a family of hard thresholding algorithms. To this end, we appeal to the partial hard thresholding (PHT) operator proposed recently by Jain et al. [IEEE Trans. Information Theory, 2017]. We show that under proper conditions, PHT recovers an arbitrary s-sparse signal within O(s kappa log kappa) iterations where kappa is an appropriate condition number. Specifying the PHT operator, we obtain the best known results for hard thresholding pursuit and orthogonal matching pursuit with replacement. Experiments on the simulated data complement our theoretical findings and also illustrate the effectiveness of PHT.",
                "id": "01026"
            },
            {
                "title": "Logician: A Unified End-to-End Neural Approach for Open-Domain Information Extraction.",
                "abstract": "In this paper, we consider the problem of open information extraction (OIE) for extracting entity and relation level intermediate structures from sentences in open-domain. We focus on four types of valuable intermediate structures (Relation, Attribute, Description, and Concept), and propose a unified knowledge expression form, SAOKE, to express them. We publicly release a data set which contains 48,248 sentences and the corresponding facts in the SAOKE format labeled by crowdsourcing. To our knowledge, this is the largest publicly available human labeled data set for open information extraction tasks. Using this labeled SAOKE data set, we train an end-to-end neural model using the sequence-to-sequence paradigm, called Logician, to transform sentences into facts. For each sentence, different to existing algorithms which generally focus on extracting each single fact without concerning other possible facts, Logician performs a global optimization over all possible involved facts, in which facts not only compete with each other to attract the attention of words, but also cooperate to share words. An experimental study on various types of open domain relation extraction tasks reveals the consistent superiority of Logician to other states-of-the-art algorithms. The experiments verify the reasonableness of SAOKE format, the valuableness of SAOKE data set, the effectiveness of the proposed Logician model, and the feasibility of the methodology to apply end-to-end learning paradigm on supervised data sets for the challenging tasks of open information extraction.\n\n",
                "id": "01027"
            },
            {
                "title": "Low-Rank Matrix Completion in the Presence of High Coherence.",
                "abstract": "Prevalent matrix completion methods capture only the low-rank property which gives merely a constraint that the data points lie on some low-dimensional subspace, but generally ignore the extra structures (beyond low-rank) that specify in more detail how the data points lie on the subspace. Whenever the data points are not uniformly distributed on the low-dimensional subspace, the row-coherence of the target matrix to recover could be considerably high and, accordingly, prevalent methods might fail even if the target matrix is fairly low-rank. To relieve this challenge, we suggest to consider a model termed low-rank factor decomposition (LRFD), which imposes an additional restriction that the data points must be represented as linear, compressive combinations of the bases in a given dictionary. We show that LRFD can effectively mitigate the challenges of high row-coherence, provided that its dictionary is configured properly. Namely, it is mathematically proven that if the dictionary is well-conditioned and low-rank, then LRFD can weaken the dependence on the row-coherence. In particular, if the dictionary itself is low-rank, then the dependence on the row-coherence can be entirely removed. Subsequently, we devise two practical algorithms to obtain proper dictionaries in unsupervised environments: one uses the existing matrix completion methods to construct the dictionary in LRFD, and the other tries to learn a proper dictionary from the data given. Experiments on randomly generated matrices and motion datasets show superior performance of our proposed algorithms.",
                "id": "01028"
            },
            {
                "title": "Computationally Efficient Estimators for Dimension Reductions Using Stable Random Projections",
                "abstract": "The method of stable random projections is an efficient tool for computing the l\u03b1 distances using low memory, where 0 1. We derive its theoretical error bound and establish the explicit (i.e., no hidden constants) sample complexity bound.",
                "id": "01029"
            },
            {
                "title": "An Interval Fuzzy Controller for Vehicle Active Suspension Systems",
                "abstract": "A novel interval type-2 fuzzy controller architecture is proposed to resolve nonlinear control problems of vehicle active suspension systems. It integrates the Takagi-Sugeno (T-S) fuzzy model, interval type-2 fuzzy reasoning, the Wu-Mendel uncertainty bound method, and selected optimization algorithms together to construct the switching routes between generated linear model control surfaces. The stability analysis of the proposed approach is presented. The proposed method is implemented into a numerical example and a case study on a nonlinear half-vehicle active suspension system. The simulation results demonstrate the effectiveness and efficiency of the proposed approach.",
                "id": "01030"
            },
            {
                "title": "Simple strategies for recovering inner products from coarsely quantized random projections.",
                "abstract": "Random projections have been increasingly adopted for a diverse set of tasks in machine learning involving dimensionality reduction. One specific line of research on this topic has investigated the use of quantization subsequent to projection with the aim of additional data compression. Motivated by applications in nearest neighbor search and linear learning, we revisit the problem of recovering inner products (respectively cosine similarities) in such setting. We show that even under coarse scalar quantization with 3 to 5 bits per projection, the loss in accuracy tends to range from \"negligible\" to \"moderate\". One implication is that in most scenarios of practical interest, there is no need for a sophisticated recovery approach like maximum likelihood estimation as considered in previous work on the subject. What we propose herein also yields considerable improvements in terms of accuracy over the Hamming distance-based approach in Li et al. (ICML 2014) which is comparable in terms of simplicity.",
                "id": "01031"
            },
            {
                "title": "Gradient Hard Thresholding Pursuit for Sparsity-Constrained Optimization.",
                "abstract": "  Hard Thresholding Pursuit (HTP) is an iterative greedy selection procedure for finding sparse solutions of underdetermined linear systems. This method has been shown to have strong theoretical guarantee and impressive numerical performance. In this paper, we generalize HTP from compressive sensing to a generic problem setup of sparsity-constrained convex optimization. The proposed algorithm iterates between a standard gradient descent step and a hard thresholding step with or without debiasing. We prove that our method enjoys the strong guarantees analogous to HTP in terms of rate of convergence and parameter estimation accuracy. Numerical evidences show that our method is superior to the state-of-the-art greedy selection methods in sparse logistic regression and sparse precision matrix estimation tasks. ",
                "id": "01032"
            },
            {
                "title": "Online Matrix Completion for Signed Link Prediction.",
                "abstract": "This work studies the binary matrix completion problem underlying a large body of real-world applications such as signed link prediction and information propagation. That is, each entry of the matrix indicates a binary preference such as \\\"like\\\" or \\\"dislike\\\", \\\"trust\\\" or \\\"distrust\\\". However, the performance of existing matrix completion methods may be hindered owing to three practical challenges: 1) the observed data are with binary label (i.e., not real value); 2) the data are typically sampled non-uniformly (i.e., positive links dominate the negative ones) and 3) a network may have a huge volume of data (i.e., memory and computational issue). In order to remedy these problems, we propose a novel framework which {i} maximizes the resemblance between predicted and observed matrices as well as penalizing the logistic loss to fit the binary data to produce binary estimates; {ii} constrains the matrix max-norm and maximizes the F-score to handle non-uniformness and {iii} presents online optimization technique, hence mitigating the memory cost. Extensive experiments performed on four large-scale datasets with up to hundreds of thousands of users demonstrate the superiority of our framework over the state-of-the-art matrix completion based methods and popular link prediction approaches.",
                "id": "01033"
            },
            {
                "title": "An extended fuzzy logic system for uncertainty modelling",
                "abstract": "An extended fuzzy logic system (EFLS) based on interval fuzzy membership functions is proposed for covering more uncertainty in practical applications. With the degree of uncertainty in fuzzy membership functions, interval fuzzy membership functions are self-generated to include uncertainties which occur from understanding linguistic knowledge and fuzzy rules in fuzzy methods. A novel adaptive strategy is designed to self-tune the interval fuzzy membership functions and to deduce the crisp outputs with feedback structure. An inverse kinematics modelling study based on a two-joint robotic arm has demonstrated that proposed EFLS outperforms conventional fuzzy methods.",
                "id": "01034"
            },
            {
                "title": "Recovery of Sparse Signals Using Multiple Orthogonal Least Squares.",
                "abstract": "Sparse recovery aims to reconstruct sparse signals from compressed linear measurements. In this paper, we propose a sparse recovery algorithm called multiple orthogonal least squares (MOLS), which extends the well-known orthogonal least squares (OLS) algorithm by allowing multiple $L$ indices to be selected per iteration. Owing to its ability to catch multiple support indices in each selection, MOLS often converges in fewer iterations and hence improves the computational efficiency over the conventional OLS algorithm. Theoretical analysis shows that MOLS ($L > 1$) performs exact recovery of $K$ -sparse signals ($K > 1$) in at most $K$ iterations, provided that the sensing matrix obeys the restricted isometry property with isometry constant $\\\\delta _{LK} < {\\\\sqrt{L}}/({\\\\sqrt{K} + 2 \\\\sqrt{L}}).$ When $L = 1,$ MOLS reduces to the conventional OLS algorithm and our analysis shows that exact recovery is guaranteed under $\\\\delta_{K +1} < 1 / (\\\\sqrt{K} + 2)$. This condition is nearly optimal with respect to $\\\\delta _{K+1}$ in the sense that, even with a small relaxation (e.g., $\\\\delta_{K + 1} = 1 / \\\\sqrt{K}$ ), exact recovery with OLS may not be guaranteed. The recovery performance of MOLS in the noisy scenario is also studied. It is shown that stable recovery of sparse signals can be achieved with the MOLS algorithm when the signal-to-noise ratio scales linearly with the sparsity level of input signals.",
                "id": "01035"
            },
            {
                "title": "An LMI approach to stability analysis of reactiondiffusion cohengrossberg neural networks concerning dirichlet boundary conditions and distributed delays",
                "abstract": "The global asymptotic stability problem for a class of reactiondiffusion CohenGrossberg neural networks with both time-varying delay and infinitely distributed delay is investigated under Dirichlet boundary conditions. Instead of using the $M$-matrix method and the algebraic inequality method, under some suitable assumptions and using a matrix decomposition method, we adopt the linear matrix inequality method to propose two sufficient stability conditions for the concerned neural networks with Dirichlet boundary conditions and different kinds of activation functions, respectively. The obtained results are easy to check and improve upon the existing stability results. Two examples are given to demonstrate the effectiveness of the obtained results. \u00a9 2010 IEEE.",
                "id": "01036"
            },
            {
                "title": "b-bit minwise hashing in practice",
                "abstract": "Minwise hashing is a standard technique in the context of search for approximating set similarities. The recent work [26, 32] demonstrated a potential use of b-bit minwise hashing [23, 24] for efficient search and learning on massive, high-dimensional, binary data (which are typical for many applications in Web search and text mining). In this paper, we focus on a number of critical issues which must be addressed before one can apply b-bit minwise hashing to the volumes of data often used industrial applications. Minwise hashing requires an expensive preprocessing step that computes k (e.g., 500) minimal values after applying the corresponding permutations for each data vector. We developed a parallelization scheme using GPUs and observed that the preprocessing time can be reduced by a factor of 20 ~ 80 and becomes substantially smaller than the data loading time. Reducing the preprocessing time is highly beneficial in practice, e.g., for duplicate Web page detection (where minwise hashing is a major step in the crawling pipeline) or for increasing the testing speed of online classifiers. Another critical issue is that for very large data sets it becomes im- possible to store a (fully) random permutation matrix, due to its space requirements. Our paper is the first study to demonstrate that b-bit minwise hashing implemented using simple hash functions, e.g., the 2-universal (2U) and 4-universal (4U) hash families, can produce very similar learning results as using fully random permutations. Experiments on datasets of up to 200GB are presented.",
                "id": "01037"
            },
            {
                "title": "b-Bit Minwise Hashing",
                "abstract": "This paper establishes the theoretical framework of b-bit minwise hashing.\nThe original minwise hashing method has become a standard technique for\nestimating set similarity (e.g., resemblance) with applications in information\nretrieval, data management, social networks and computational advertising.\n  By only storing the lowest $b$ bits of each (minwise) hashed value (e.g., b=1\nor 2), one can gain substantial advantages in terms of computational efficiency\nand storage space. We prove the basic theoretical results and provide an\nunbiased estimator of the resemblance for any b. We demonstrate that, even in\nthe least favorable scenario, using b=1 may reduce the storage space at least\nby a factor of 21.3 (or 10.7) compared to using b=64 (or b=32), if one is\ninterested in resemblance > 0.5.",
                "id": "01038"
            },
            {
                "title": "Improving random projections using marginal information",
                "abstract": "We present an improved version of random projections that takes advantage of marginal norms. Using a maximum likelihood estimator (MLE), margin-constrained random projections can improve estimation accuracy considerably. Theoretical properties of this estimator are analyzed in detail.",
                "id": "01039"
            },
            {
                "title": "Recovery of Coherent Data via Low-Rank Dictionary Pursuit.",
                "abstract": "The recently established RPCA [4] method provides a convenient way to restore low-rank matrices from grossly corrupted observations. While elegant in theory and powerful in reality, RPCA is not an ultimate solution to the low-rank matrix recovery problem. Indeed, its performance may not be perfect even when data are strictly low-rank. This is because RPCA ignores clustering structures of the data which are ubiquitous in applications. As the number of cluster grows, the coherence of data keeps increasing, and accordingly, the recovery performance of RPCA degrades. We show that the challenges raised by coherent data (i.e., data with high coherence) could be alleviated by Low-Rank Representation (LRR) [13], provided that the dictionary in LRR is configured appropriately. More precisely, we mathematically prove that if the dictionary itself is low-rank then LRR is immune to the coherence parameter which increases with the underlying cluster number. This provides an elementary principle for dealing with coherent data and naturally leads to a practical algorithm for obtaining proper dictionaries in unsupervised environments. Experiments on randomly generated matrices and real motion sequences verify our claims.",
                "id": "01040"
            },
            {
                "title": "Variable activation function extreme learning machine based on residual prediction compensation",
                "abstract": "For solving the problem that extreme learning machine (ELM) algorithm uses fixed activation function and cannot be residual compensation, a new learning algorithm called variable activation function extreme learning machine based on residual prediction compensation is proposed. In the learning process, the proposed method adjusts the steep degree, position and mapping scope simultaneously. To enhance the nonlinear mapping capability of ELM, particle swarm optimization algorithm is used to optimize variable parameters according to root-mean square error for the prediction accuracy of the mode. For further improving the predictive accuracy, the auto-regressive moving average model is used to model the residual errors between actual value and predicting value of variable activation function extreme learning machine (V-ELM). The prediction of residual errors is used to rectify the prediction value of V-ELM. Simulation results verified the effectiveness and feasibility of this method by using Pole, Auto-Mpg, Housing, Diabetes, Triazines and Stock benchmark datasets. Also, it was implemented to develop a soft sensor model for the gasoline dry point in delayed coking and some satisfied results were obtained.",
                "id": "01041"
            },
            {
                "title": "Adaptive Base Class Boost for Multi-class Classification",
                "abstract": "We1 develop the concept of ABC-Boost (Adaptive Base Class Boost) for multi-class classification and present ABC-MART, a concrete implementation of ABC-Boost. The original MART (Multiple Additive Regression Trees) algorithm has been very successful in large-scale applications. For binary classification, ABC-MART re- covers MART. For multi-class classification, ABC-MART cons iderably improves MART, as evaluated on several public data sets.",
                "id": "01042"
            },
            {
                "title": "Multi-Task Neural Learning Architecture for End-to-End Identification of Helpful Reviews.",
                "abstract": "Helpful reviews play a pivotal role in recommending desirable goods and accelerating purchase decisions of customers in e-commercial services. Given a large proportion of product reviews with unknown helpfulness/unhelpfulness, the research on automatic identification of helpful reviews has drawn much attention in recent years. However, state-of-the-art approaches still rely heavily on extracting heuristic text features from reviews with domain-specific knowledge. In this paper, we first introduce a multi-task neural learning (MTNL) architecture for identifying helpful reviews. The end-to-end neural architecture can learn to reconstruct effective features upon the raw input of words and even characters, and the multi-task learning paradigm helps to make more accurate predictions of helpful reviews based on a secondary task which fits the star ratings of reviews. We also build two datasets containing helpful/unhelpful reviews from different product categories in Amazon, and compare the performance of MTNL with several mainstream methods on both datasets. Experimental results confirm that MTNL outperforms the state-of-the-art approaches by a significant margin.\n\n",
                "id": "01043"
            },
            {
                "title": "Very sparse stable random projections for dimension reduction in lalpha (0 &lt;alpha<=2) norm",
                "abstract": "The method of stable random projections is a useful tool for efficiently computing the l(alpha) (0 < alpha <= 2) norms and distances in lmassive data in one pass. Consider a data matrix A is an element of R-nxD. If we multiply A with a projection matrix R is an element of R-DXk (k << D), whose entries are i.i.d. samples of an a-stable distribution, then the projected matrix B = A x R is an element of R-nXk contains enough information to approximately recover the l(alpha) properties in A. We propose very sparse stable random projections, by replacing the a-stable distribution with a (much simpler) mixture of a symmetric alpha-Pareto distribution (with probability beta, 0 < beta <= 1) and a point mass at the origin (with probability 1 - beta). This leads to a significant 1/beta-fold speedup for small beta when computing B = A x R and a 1/beta-fold cost reduction in storing R. By analyzing the convergence, we show that in \"reasonable\" datasets beta often can be very small (e.g., D-1/2) without hurting the estimation accuracy. Some numerical evaluations are conducted, on synthetic data, Web crawl data, and gene expression microarray data.",
                "id": "01044"
            },
            {
                "title": "A New Multi-task Learning Method for Personalized Activity Recognition",
                "abstract": "Personalized activity recognition usually faces the problem of data sparseness. We aim at improving accuracy of personalized activity recognition by incorporating the information from other persons. We propose a new online multi-task learning method for personalized activity recognition. The proposed online multi-task learning method automatically learns the ``transfer-factors\" (similarities) among different tasks (i.e., among different persons in our case). Experiments demonstrate that the proposed method significantly outperforms existing methods. The novelty of this paper is twofold: (1) A new multi-task learning framework, which can naturally learn similarities among tasks, (2) To our knowledge, this is the first study of large-scale personalized activity recognition.",
                "id": "01045"
            },
            {
                "title": "On Approximating Frequency Moments of Data Streams with Skewed Projections",
                "abstract": "We propose skewed stable random projectionsfor approximating the \u03b1th frequency moments of dynamic data streams (0 < \u03b1 \ufffd 2). We show the sample complexity (number of projections) k = G 1 \u01eb2 log ` 2 \u03b4 \u00b4 , where G ! \u01eb 2 log(1+\u01eb) = O (\u01eb) as \u03b1 ! 1, i.e., \u03b1 = 1 \u00b1 \ufffd with \ufffd ! 0. Previous results based on symmetric stable random projections(12, 16) required G = non-zero constant + O(\u01eb), even when \ufffd = 0. The case \ufffd ! 0 is practically important. For example, \ufffd might be the \"decay rate\" or \"interest rate,\" which is usuall y small; and hence one might view skewed stable random projectionsas a \"generalized counter\" for estimating the total value in the future, taking in account of the effect of decaying or i nterest accruement. We consider the popular Turnstile data stream model. The input data stream at = (i, It) arriving sequentially describes the underlying signal A, meaning At(i) = At 1(i) + It, i 2 (1, D). We allow the increment It to be either positive (i.e., insertion) or negative (i.e., del etion). By definition, the \u03b1th frequency moment F(\u03b1) = PD i=1 |At(i)| \u03b1. Our method only requires that, at the time t for the evaluation, A",
                "id": "01046"
            },
            {
                "title": "Sparse Recovery with Very Sparse Compressed Counting.",
                "abstract": "  Compressed sensing (sparse signal recovery) often encounters nonnegative data (e.g., images). Recently we developed the methodology of using (dense) Compressed Counting for recovering nonnegative K-sparse signals. In this paper, we adopt very sparse Compressed Counting for nonnegative signal recovery. Our design matrix is sampled from a maximally-skewed p-stable distribution (0<p<1), and we sparsify the design matrix so that on average (1-g)-fraction of the entries become zero. The idea is related to very sparse stable random projections (Li et al 2006 and Li 2007), the prior work for estimating summary statistics of the data.   In our theoretical analysis, we show that, when p->0, it suffices to use M= K/(1-exp(-gK) log N measurements, so that all coordinates can be recovered in one scan of the coordinates. If g = 1 (i.e., dense design), then M = K log N. If g= 1/K or 2/K (i.e., very sparse design), then M = 1.58K log N or M = 1.16K log N. This means the design matrix can be indeed very sparse at only a minor inflation of the sample complexity.   Interestingly, as p->1, the required number of measurements is essentially M = 2.7K log N, provided g= 1/K. It turns out that this result is a general worst-case bound. ",
                "id": "01047"
            },
            {
                "title": "Stability and Risk Bounds of Iterative Hard Thresholding",
                "abstract": "In this paper, we analyze the generalization performance of the Iterative Hard Thresholding (IHT) algorithm widely used for sparse recovery problems. The parameter estimation and sparsity recovery consistency of IHT has long been known in compressed sensing. From the perspective of statistical learning, another fundamental question is how well the IHT estimation would predict on unseen data. This paper makes progress towards answering this open question by introducing a novel sparse generalization theory for IHT under the notion of algorithmic stability. Our theory reveals that: 1) under natural conditions on the empirical risk function over \n<inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$n$ </tex-math></inline-formula>\n samples of dimension \n<inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$p$ </tex-math></inline-formula>\n, IHT with sparsity level \n<inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$k$ </tex-math></inline-formula>\n enjoys an \n<inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$\\tilde {\\mathcal {O}}(n^{-1/2}\\sqrt {k\\log (n)\\log (p)})$ </tex-math></inline-formula>\n rate of convergence in sparse excess risk; 2) a tighter \n<inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$\\tilde {\\mathcal {O}}(n^{-1/2}\\sqrt {\\log (n)})$ </tex-math></inline-formula>\n bound can be established by imposing an additional iteration stability condition on a hypothetical IHT procedure invoked to the population risk; and 3) a fast rate of order \n<inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$\\tilde {\\mathcal {O}}\\left ({n^{-1}k(\\log ^{3}(n)+\\log (p))}\\right)$ </tex-math></inline-formula>\n can be derived for strongly convex risk function under proper strong-signal conditions. The results have been substantialized to sparse linear regression and sparse logistic regression models to demonstrate the applicability of our theory. Preliminary numerical evidence is provided to support our theoretical predictions.",
                "id": "01048"
            },
            {
                "title": "Exact sparse recovery with L0 projections",
                "abstract": "Many applications (e.g., anomaly detection) concern sparse signals. This paper focuses on the problem of recovering a K-sparse signal x \u2208 R/1\u00d7N, i.e., K N and \u2211N/i=1 1{xi \u2260 0} = K. In the mainstream framework of compressed sensing (CS), \u00d7 is recovered from M linear measurements y = xS \u2208 R/1\u00d7M, where S \u2208 RN\u00d7M is often a Gaussian (or Gaussian-like) design matrix. In our proposed method, the design matrix S is generated from an \u03b1-stable distribution with \u03b1 \u2248 0. Our decoding algorithm mainly requires one linear scan of the coordinates, followed by a few iterations on a small number of coordinates which are \\\"undetermined\\\" in the previous iteration. Our practical algorithm consists of two estimators. In the first iteration, the (absolute) minimum estimator is able to filter out a majority of the zero coordinates. The gap estimator, which is applied in each iteration, can accurately recover the magnitudes of the nonzero coordinates. Comparisons with linear programming (LP) and orthogonal matching pursuit (OMP) demonstrate that our algorithm can be significantly faster in decoding speed and more accurate in recovery quality, for the task of exact spare recovery. Our procedure is robust against measurement noise. Even when there are no sufficient measurements, our algorithm can still reliably recover a significant portion of the nonzero coordinates.",
                "id": "01049"
            },
            {
                "title": "Improving clustering by learning a bi-stochastic data similarity matrix.",
                "abstract": "An idealized clustering algorithm seeks to learn a cluster-adjacency matrix such that, if two data points belong to the same cluster, the corresponding entry would be 1; otherwise, the entry would be 0. This integer (1/0) constraint makes it difficult to find the optimal solution. We propose a relaxation on the cluster-adjacency matrix, by deriving a bi-stochastic matrix from a data similarity (e.g., kernel) matrix according to the Bregman divergence. Our general method is named the Bregmanian Bi-Stochastication (BBS) algorithm. We focus on two popular choices of the Bregman divergence: the Euclidean distance and the Kullback\u2013Leibler (KL) divergence. Interestingly, the BBS algorithm using the KL divergence is equivalent to the Sinkhorn\u2013Knopp (SK) algorithm for deriving bi-stochastic matrices. We show that the BBS algorithm using the Euclidean distance is closely related to the relaxed k-means clustering and can often produce noticeably superior clustering results to the SK algorithm (and other algorithms such as Normalized Cut), through extensive experiments on public data sets.",
                "id": "01050"
            },
            {
                "title": "One sketch for all: Theory and Application of Conditional Random Sampling",
                "abstract": "Conditional Random Sampling (CRS) was originally proposed for efficiently computing pairwise (l2, l1) distances, in static, large-scale, and sparse data. This study modifies the original CRS and extends CRS to handle dynamic or stream- ing data, which much better reflect the real-world situation than assuming static data. Compared with many other sketching algorithms for dimension reductions such as stable random projections, CRS exhibits a significant advantage in that it is \"one-sketch-for-all.\" In particular, we demonstrate the effectiveness of CRS in efficiently computing the Hamming norm, the Hamming distance, the lp distance, and the \u00b42 distance. A generic estimator and an approximate variance formula are also provided, for approximating any type of distances. We recommend CRS as a promising tool for building highly scalable systems, in machine learning, data mining, recommender systems, and information retrieval.",
                "id": "01051"
            },
            {
                "title": "Densifying One Permutation Hashing via Rotation for Fast Near Neighbor Search.",
                "abstract": "The query complexity of \\em locality sensitive hashing (LSH) based similarity search is dominated by the number of hash evaluations, and this number grows with the data size \\citeProc:Indyk_STOC98. In industrial applications such as search where the data are often high-dimensional and binary (e.g., text n-grams), \\em minwise hashing is widely adopted, which requires applying a large number of permutations on the data. This is costly in computation and energy-consumption. In this paper, we propose a hashing technique which generates all the necessary hash evaluations needed for similarity search, using one single permutation. The heart of the proposed hash function is a \u201crotation\u201d scheme which densifies the sparse sketches of \\em one permutation hashing \\citeProc:Li_Owen_Zhang_NIPS12 in an unbiased fashion thereby maintaining the LSH property. This makes the obtained sketches suitable for hash table construction. This idea of rotation presented in this paper could be of independent interest for densifying other types of sparse sketches. Using our proposed hashing method, the query time of a (K,L)-parameterized LSH is reduced from the typical O(dKL) complexity to merely O(KL+dL), where d is the number of nonzeros of the data vector, K is the number of hashes in each hash table, and L is the number of hash tables. Our experimental evaluation on real data confirms that the proposed scheme significantly reduces the query processing time over minwise hashing without loss in retrieval accuracies.",
                "id": "01052"
            },
            {
                "title": "Blessing of Dimensionality: Recovering Mixture Data via Dictionary Pursuit.",
                "abstract": "This paper studies the problem of recovering the authentic samples that lie on a union of multiple subspaces from their corrupted observations. Due to the high-dimensional and massive nature of today\u2019s data-driven community, it is arguable that the target matrix (i.e., authentic sample matrix) to recover is often low-rank. In this case, the recently established Robust Principal Component Analysis (RPCA) method already provides us a convenient way to solve the problem of recovering mixture data. However, in general, RPCA is not good enough because the incoherent condition assumed by RPCA is not so consistent with the mixture structure of multiple subspaces. Namely, when the subspace number grows, the row-coherence of data keeps heightening and, accordingly, RPCA degrades. To overcome the challenges arising from mixture data, we suggest to consider LRR in this paper. We elucidate that LRR can well handle mixture data, as long as its dictionary is configured appropriately. More precisely, we mathematically prove that LRR can weaken the dependence on the row-coherence, provided that the dictionary is well-conditioned and has a rank of not too high. In particular, if the dictionary itself is sufficiently low-rank, then the dependence on the row-coherence can be completely removed. These provide some elementary principles for dictionary learning and naturally lead to a practical algorithm for recovering mixture data. Our experiments on randomly generated matrices and real motion sequences show promising results.",
                "id": "01053"
            },
            {
                "title": "Variational Flow Graphical Model",
                "abstract": "This paper introduces a novel approach embedding flow-based models in hierarchical structures. The proposed model learns the representation of high-dimensional data via a message-passing scheme by integrating flow-based functions through variational inference. Meanwhile, our model produces a representation of the data using a lower dimension, thus overcoming the drawbacks of many flow-based models, usually requiring a high dimensional latent space involving many trivial variables. With the proposed aggregation nodes, our model provides a new approach for distribution modeling and numerical inference on datasets. Multiple experiments on synthetic and real-world datasets show the benefits of our~proposed~method and potentially broad applications.",
                "id": "01054"
            },
            {
                "title": "McRank: Learning to Rank Using Multiple Classification and Gradient Boosting",
                "abstract": "We cast the ranking problem as (1) multiple classification (\" Mc\") (2) multiple or- dinal classification, which lead to computationally tracta ble learning algorithms for relevance ranking in Web search. We consider the DCG criterion (discounted cumulative gain), a standard quality measure in information retrieval. Our ap- proach is motivated by the fact that perfect classifications result in perfect DCG scores and the DCG errors are bounded by classification error s. We propose us- ing the Expected Relevanceto convert class probabilities into ranking scores. The class probabilities are learned using a gradient boosting t ree algorithm. Evalua- tions on large-scale datasets show that our approach can improve LambdaRank(5) and the regressions-based ranker (6), in terms of the (normalized) DCG scores. An efficient implementation of the boosting tree algorithm is a lso presented. 1 Introduction The general ranking problem has widespread applications including commercial search engines and recommender systems. We develop McRank, a computationally tractable learning algorithm for the general ranking problem; and we present our approach in the context of ranking in Web search. For a given user input query, a commercial search engine returns many pages of URLs, in an order determined by the underlying proprietary ranking algorithm. The quality of the returned results are largely evaluated on the URLs displayed in the very first page . The type of ranking problem in this study is sometimes referred to as dynamic ranking(or simply, just ranking), because the URLs are dynamically ranked (in real-time) according to the specific user input query. This is different from the query-independent static ranking based on, for example, \"page rank\" (3) or \"authorities and hubs\" (12), which may, at least conceptually, serve as an important \"feature\" for dynamic ranking or to guide the generation of a list of URLs fed to the dynamic ranker. There are two main categories of ranking algorithms. A popular scheme is based on learning pairwise preferences, including RankNet (4), LambdaRank (5), RankSVM (11), RankBoost (7), GBRank (14), and FRank (13). Both LambdaRankand RankNet used neural nets.1 RankNet used a cross-entropy type of loss function and LambdaRankused a gradient based on NDCG smoothed by the RankNet loss. Another scheme is based on regression (6). (6) considered the DCG measure (discounted cumulative gain) (10) and showed that the DCG errors are bounded by regression errors. In this study, we also consider the DCG measure. From the defin ition of DCG, it appears more direct to cast the ranking problem as multiple classification (\"Mc\" ) as opposed to regression. In order to convert classification results into ranking scores, we prop ose a simple and stable mechanism by using the Expected Relevance. Our evaluations on large-scale datasets demonstrate the superiority of the classification-based ranker ( McRank) over both the regression-based and pair-based schemes.",
                "id": "01055"
            },
            {
                "title": "Graph Kernels via Functional Embedding.",
                "abstract": "  We propose a representation of graph as a functional object derived from the power iteration of the underlying adjacency matrix. The proposed functional representation is a graph invariant, i.e., the functional remains unchanged under any reordering of the vertices. This property eliminates the difficulty of handling exponentially many isomorphic forms. Bhattacharyya kernel constructed between these functionals significantly outperforms the state-of-the-art graph kernels on 3 out of the 4 standard benchmark graph classification datasets, demonstrating the superiority of our approach. The proposed methodology is simple and runs in time linear in the number of edges, which makes our kernel more efficient and scalable compared to many widely adopted graph kernels with running time cubic in the number of vertices. ",
                "id": "01056"
            },
            {
                "title": "Compressed Counting Meets Compressed Sensing.",
                "abstract": "  Compressed sensing (sparse signal recovery) has been a popular and important research topic in recent years. By observing that natural signals are often nonnegative, we propose a new framework for nonnegative signal recovery using Compressed Counting (CC). CC is a technique built on maximally-skewed p-stable random projections originally developed for data stream computations. Our recovery procedure is computationally very efficient in that it requires only one linear scan of the coordinates. Our analysis demonstrates that, when 0<p<=0.5, it suffices to use M= O(C/eps^p log N) measurements so that all coordinates will be recovered within eps additive precision, in one scan of the coordinates. The constant C=1 when p->0 and C=pi/2 when p=0.5. In particular, when p->0 the required number of measurements is essentially M=K\\log N, where K is the number of nonzero coordinates of the signal. ",
                "id": "01057"
            },
            {
                "title": "A new space for comparing graphs.",
                "abstract": "Finding1 a new mathematical representation for graphs, which allows direct comparison between different graph structures, is an open-ended research direction. Having such a representation is the first prerequisite for a variety of machine learning algorithms like classification, clustering, etc., over graph datasets. In this paper, we propose a symmetric positive semidefinite matrix with the (i, j)-th entry equal to the covariance between normalized vectors Aie and Aje (e being vector of all ones) as a representation for a graph with adjacency matrix A. We show that the proposed matrix representation encodes the spectrum of the underlying adjacency matrix and it also contains information about the counts of small sub-structures present in the graph such as triangles and small paths. In addition, we show that this matrix is a \"graph invariant\". All these properties make the proposed matrix a suitable object for representing graphs.\n\nThe representation, being a covariance matrix in a fixed dimensional metric space, gives a mathematical embedding for graphs. This naturally leads to a measure of similarity on graph objects. We define similarity between two given graphs as a Bhattacharya similarity measure between their corresponding covariance matrix representations. As shown in our experimental study on the task of social network classification, such a similarity measure outperforms other widely used state-of-the-art methodologies. Our proposed method is also computationally efficient. The computation of both the matrix representation and the similarity value can be performed in operations linear in the number of edges. This makes our method scalable in practice.\n\nWe believe our theoretical and empirical results provide evidence for studying truncated power iterations, of the adjacency matrix, to characterize social networks.\n\n",
                "id": "01058"
            },
            {
                "title": "Object proposal with kernelized partial ranking.",
                "abstract": "\u2022In this paper, we propose a new partial ranking algorithm with support of non- linear kernel.\u2022We perform consistent weighted sampling on the features followed by learning our partial ranking model. Here learning a ranking model with non-linear kernel amounts to learning a linear hyperplane.\u2022Our algorithm greatly promotes baseline proposal generation methods in recall and average recall.\u2022Our method can be integrated with any proposal generation methods.",
                "id": "01059"
            },
            {
                "title": "Estimators and tail bounds for dimension reduction in l\u03b1 (0 < \u03b1 \u2264 2) using stable random projections.",
                "abstract": "The method of stable random projections is popular in data stream computations, data mining, information retrieval, and machine learning, for efficiently computing the l\u03b1 (0 < \u03b1 \u2264 2) distances using a small (memory) space, in one pass of the data.\n\nWe propose algorithms based on (1) the geometric mean estimator, for all 0 <\u03b1 \u2264 2, and (2) the harmonic mean estimator, only for small \u03b1 (e.g., \u03b1 < 0.344). Compared with the previous classical work [27], our main contributions include:\n\n\u2022 The general sample complexity bound for \u03b1 \u2260 1,2.\n\nFor \u03b1 = 1, [27] provided a nice argument based on the inverse of Cauchy density about the median, leading to a sample complexity bound, although they did not provide the constants and their proof restricted \u03b5 to be \"small enough.\"\n\nFor general \u03b1 \u2260 1, 2, however, the task becomes much more difficult. [27] provided the \"conceptual promise\" that the sample complexity bound similar to that for \u03b1 = 1 should exist for general \u03b1, if a \"non-uniform algorithm based on t-quantile\" could be implemented. Such a conceptual algorithm was only for supporting the arguments in [27], not a real implementation. We consider this is one of the main problems left open in [27]. In this study, we propose a practical algorithm based on the geometric mean estimator and derive the sample complexity bound for all 0 < \u03b1 \u2264 2.\n\n\u2022 The practical and optimal algorithm for \u03b1 = 0+\n\nThe l0 norm is an important case. Stable random projections can provide an approximation to the l0 norm using \u03b1 \u2192 0+. We provide an algorithm based on the harmonic mean estimator, which is simple and statistically optimal. Its tail bounds are sharper than the bounds derived based on the geometric mean. We also discover a (possibly surprising) fact: in boolean data, stable random projections using \u03b1 = 0+ with the harmonic mean estimator will be about twice as accurate as (l2) normal random projections. Because high-dimensional boolean data are common, we expect this fact will be practically quite useful.\n\n\u2022 The precise theoretical analysis and practical implications We provide the precise constants in the tail bounds for both the geometric mean and harmonic mean estimators. We also provide the variances (either exact or asymptotic) for the proposed estimators. These results can assist practitioners to choose sample sizes accurately.\n\n",
                "id": "01060"
            },
            {
                "title": "Nonlinear Estimators and Tail Bounds for Dimension Reduction in l 1 Using Cauchy Random Projections",
                "abstract": "For(1) dimension reduction in the l(1) norm, the method of Cauchy random projections multiplies the original data matrix A epsilon R-nxD with a random matrix R epsilon R-Dxk (k<<D) whose entries are i.i.d. samples of the standard Cauchy C (0, 1). Because of the impossibility result, one can not hope to recover the pairwise l(1) distances in A from B = AxR epsilon R-nxk, using linear estimators without incurring large errors. However, nonlinear estimators are still useful for certain applications in data stream computations, information retrieval, learning, and data mining. We study three types of nonlinear estimators: the sample median estimators, the geometric mean estimators, and the maximum likelihood estimators (MLE). We derive tail bounds for the geometric mean estimators and establish that k = O (logn/epsilon(2)) suffices with the constants explicitly given. Asymptotically (as k ->infinity), both the sample median and the geometric mean estimators are about 80% efficient compared to the MLE. We analyze the moments of the MLE and propose approximating its distribution of by an inverse Gaussian.",
                "id": "01061"
            },
            {
                "title": "Fast multi-task learning for query spelling correction",
                "abstract": "In this paper, we explore the use of a novel online multi-task learning framework for the task of search query spelling correction. In our procedure, correction candidates are initially generated by a ranker-based system and then re-ranked by our multi-task learning algorithm. With the proposed multi-task learning method, we are able to effectively transfer information from different and highly biased training datasets, for improving spelling correction on all datasets. Our experiments are conducted on three query spelling correction datasets including the well-known TREC benchmark dataset. The experimental results demonstrate that our proposed method considerably outperforms the existing baseline systems in terms of accuracy. Importantly, the proposed method is about one order of magnitude faster than baseline systems in terms of training speed. Compared to the commonly used online learning methods which typically require more than (e.g.,) 60 training passes, our proposed method is able to closely reach the empirical optimum in about 5 passes.",
                "id": "01062"
            },
            {
                "title": "Recovery of Sparse Signals via Generalized Orthogonal Matching Pursuit: A New Analysis.",
                "abstract": "As an extension of orthogonal matching pursuit (OMP) for improving the recovery performance of sparse signals, generalized OMP (gOMP) has recently been studied in the literature. In this paper, we present a new analysis of the gOMP algorithm using the restricted isometry property (RIP). We show that if a measurement matrix \u03a6 \u2208 \u211cm\u00d7n satisfies the RIP with isometry constant \u03b4max{9,S+1}K \u2264 1/8, then ...",
                "id": "01063"
            },
            {
                "title": "Asymmetric Minwise Hashing for Indexing Binary Inner Products and Set Containment",
                "abstract": "Minwise hashing (Minhash) is a widely popular indexing scheme in practice. Minhash is designed for estimating set resemblance and is known to be suboptimal in many applications where the desired measure is set overlap (i.e., inner product between binary vectors) or set containment. Minhash has inherent bias towards smaller sets, which adversely affects its performance in applications where such a penalization is not desirable. In this paper, we propose asymmetric minwise hashing ({\\\\em MH-ALSH}), to provide a solution to this well-known problem. The new scheme utilizes asymmetric transformations to cancel the bias of traditional minhash towards smaller sets, making the final ``collision probability'' monotonic in the inner product. Our theoretical comparisons show that, for the task of retrieving with binary inner products, asymmetric minhash is provably better than traditional minhash and other recently proposed hashing algorithms for general inner products. Thus, we obtain an algorithmic improvement over existing approaches in the literature. Experimental evaluations on four publicly available high-dimensional datasets validate our claims. The proposed scheme outperforms, often significantly, other hashing algorithms on the task of near neighbor retrieval with set containment. Our proposal is simple and easy to implement in practice.",
                "id": "01064"
            },
            {
                "title": "Learning a Bi-Stochastic Data Similarity Matrix",
                "abstract": "An idealized clustering algorithm seeks to learn a cluster-adjacency matrix such that, if two data points belong to the same cluster, the corresponding entry would be 1; otherwise the entry would be 0. This integer (1/0) constraint makes it difficult to find the optimal solution. We propose a relaxation on the cluster-adjacency matrix, by deriving a bi-stochastic matrix from a data similarity (e.g., kernel) matrix according to the Bregman divergence. Our general method is named the Bregmanian Bi-Stochastication (BBS) algorithm. We focus on two popular choices of the Bregman divergence: the Euclidian distance and the KL divergence. Interestingly, the BBS algorithm using the KL divergence is equivalent to the Sinkhorn-Knopp (SK) algorithm for deriving bi-stochastic matrices. We show that the BBS algorithm using the Euclidian distance is closely related to the relaxed K-means clustering and can often produce noticeably superior clustering results than the SK algorithm (and other algorithms such as Normalized Cut), through extensive experiments on public data sets.",
                "id": "01065"
            },
            {
                "title": "Very Sparse Stable Random Projections, Estimators and Tail Bounds for Stable Random Projections",
                "abstract": "The method of stable random projections(39, 41) is popular for data streaming computations, data mining, and machine learning. For example, in data streaming, stable random projections offer a unified, efficient, and elegant methodology for approximating the l\u03b1 norm of a single data stream, or the l\u03b1 distance between a pair of streams, for any 0 < \u03b1 \u2264 2. (18) and (20) applied stable random projections for approximating the Hamming norm and the max-dominance norm, respectively, using very small \u03b1. Another application is to approximate all pairwise l\u03b1 distances in a data matrix to speed up clustering, classifica tion, or kernel computations. Given that stable random projections have been successful in various applications, this paper will focus on three different aspects in improving the current practice of stable random projections. Firstly, we propose very sparse stable random projectionsto significantly reduce the processing and storage cost, by replacing the \u03b1-stable distribution with a mixture of a symmetric \u03b1-Pareto distribution (with probability \u03b2, 0 < \u03b2 \u2264 1) and a point mass at the origin (with a probability 1 \u2212 \u03b2). This leads to a significant 1 \u03b2 -fold speedup for small \u03b2. We analyze the rate of convergence as a function of \u03b2, \u03b1, and the data regularity conditions. For example, when \u03b1 = 1 and the data have bounded second moments, then if we choose \u03b2 = 1 D1/2 , the rate of convergence would be O D \u22121/2 \ufffd , which is fast even for moderate D.",
                "id": "01066"
            },
            {
                "title": "EGM: Enhanced Graph-based Model for Large-scale Video Advertisement Search",
                "abstract": "Video advertisements may grasp customers' attention instantly and are often adored by advertisers. Since the corpus is vast, achieving an efficient query-to-video search can be challenging. Because traditional approximate nearest neighborhood (ANN) search methods are based simple similarities (e.g., cosine or inner products) on embedding vectors. They are often not sufficient for bridging the modal gap between a text query and video advertisements and typically can only achieve sub-optimal performance in query-to-video search. Tree-based deep model (TDM) overcomes the limited matching capability of embedding-based methods but suffers from the data sparsity problem. Deep retrieval model adopts a graph-based model which overcomes the data sparsity problem in TDM by sharing the nodes. But the shared nodes entangle features of different items, making it difficult to distinguish similar items. In this work, we enhance the graph-based model through sub-path embedding to differentiate similar videos. The added sub-path embedding provides personalized characteristics, beneficial for modeling fine-grain details to discriminate similar items. After launching enhanced graph model (EGM), the click-through rate (CTR) relatively increases by 1.33%, and the conversion rate (CVR) relatively by 1.07%.",
                "id": "01067"
            },
            {
                "title": "Nonlinear Estimators and Tail Bounds for Dimension Reduction in l1 Using Cauchy Random Projections",
                "abstract": "For dimension reduction in the l1 norm, the method of Cauchy random projections multiplies the original data matrix A \u2208 \u211dn\u00d7D with a random matrix R \u2208 \u211dD\u00d7k (k\u226aD) whose entries are i.i.d. samples of the standard Cauchy C(0,1). Because of the impossibility result, one can not hope to recover the pairwise l1 distances in A from B=A\u00d7R\u2208 \u211dn\u00d7k, using linear estimators without incurring large errors. However, nonlinear estimators are still useful for certain applications in data stream computations, information retrieval, learning, and data mining. We study three types of nonlinear estimators: the sample median estimators, the geometric mean estimators, and the maximum likelihood estimators (MLE). We derive tail bounds for the geometric mean estimators and establish that k = O(log n / \u03b52) suffices with the constants explicitly given. Asymptotically (as k\u2192\u221e), both the sample median and the geometric mean estimators are about 80% efficient compared to the MLE. We analyze the moments of the MLE and propose approximating its distribution of by an inverse Gaussian.",
                "id": "01068"
            },
            {
                "title": "Nystrom Method for Approximating the GMM Kernel.",
                "abstract": " GMM (generalized min-max) kernel was recently proposed (Li, 2016) as a measure of data similarity and was demonstrated effective in machine learning tasks.  order to use the GMM kernel for large-scale datasets, the prior work resorted to the (generalized) consistent weighted sampling (GCWS) to convert the GMM kernel to linear kernel. We call this approach as ``GMM-GCWSu0027u0027. In the machine learning literature, there is a popular algorithm which we call ``RBF-RFFu0027u0027. That is, one can use the ``random Fourier featuresu0027u0027 (RFF) to convert the ``radial basis functionu0027u0027 (RBF) kernel to linear kernel. It was empirically shown in (Li, 2016) that RBF-RFF typically requires substantially more samples than GMM-GCWS in order to achieve comparable accuracies. The Nystrom method is a general tool for computing nonlinear kernels, which again converts nonlinear kernels into linear kernels. We apply the Nystrom method for approximating the GMM kernel, a strategy which we name as ``GMM-NYSu0027u0027.  this study, our extensive experiments on a set of fairly large datasets confirm that GMM-NYS is also a strong competitor of RBF-RFF.",
                "id": "01069"
            },
            {
                "title": "Hashing Algorithms for Large-Scale Learning",
                "abstract": "  In this paper, we first demonstrate that b-bit minwise hashing, whose estimators are positive definite kernels, can be naturally integrated with learning algorithms such as SVM and logistic regression. We adopt a simple scheme to transform the nonlinear (resemblance) kernel into linear (inner product) kernel; and hence large-scale problems can be solved extremely efficiently. Our method provides a simple effective solution to large-scale learning in massive and extremely high-dimensional datasets, especially when data do not fit in memory.   We then compare b-bit minwise hashing with the Vowpal Wabbit (VW) algorithm (which is related the Count-Min (CM) sketch). Interestingly, VW has the same variances as random projections. Our theoretical and empirical comparisons illustrate that usually $b$-bit minwise hashing is significantly more accurate (at the same storage) than VW (and random projections) in binary data. Furthermore, $b$-bit minwise hashing can be combined with VW to achieve further improvements in terms of training speed, especially when $b$ is large. ",
                "id": "01070"
            },
            {
                "title": "Improving compressed counting",
                "abstract": "Compressed Counting (CC) [22] was recently proposed for estimating the \u03b1th frequency moments of data streams, where 0 This paper presents a new algorithm for improving CC. The improvement is most substantial when \u03b1 \u2192 1--. For example, when \u03b1 = 0.99, the new algorithm reduces the estimation variance roughly by 100-fold. This new algorithm would make CC considerably more practical for estimating Shannon entropy. Furthermore, the new algorithm is statistically optimal when \u03b1 = 0.5.",
                "id": "01071"
            },
            {
                "title": "A Direct Adaptive Predictive Functional Control Based on T-S Fuzzy Model",
                "abstract": "An adaptive predictive functional control algorithm based on T-S fuzzy model for discrete-time nonlinear systems is proposed. In this algorithm the consequent parameters of T-S fuzzy model are identified by the weighting recursive least square method, Parameters of the fuzzy model are used to directly recursively compute the model prediction output, and needn't solve Diophantine equations. Analysis solution of the. predictive control input is derived. Simulation result shows that the proposed algorithm is an effective control strategy with excellent tracing ability and strong robustness.",
                "id": "01072"
            },
            {
                "title": "Linearized GMM Kernels and Normalized Random Fourier Features",
                "abstract": "The method of \\\"random Fourier features (RFF)\\\" has become a popular tool for approximating the \\\"radial basis function (RBF)\\\" kernel. The variance of RFF is actually large. Interestingly, the variance can be substantially reduced by a simple normalization step as we theoretically demonstrate. We name the improved scheme as the \\\"normalized RFF (NRFF)\\\", and we provide a technical proof of the asymptotic variance of NRFF, as validated by simulations. We also propose the \\\"generalized min-max (GMM)\\\" kernel as a measure of data similarity, where data vectors can have both positive and negative entries. GMM is positive definite as there is an associate hashing method named \\\"generalized consistent weighted sampling (GCWS)\\\" which linearizes this (nonlinear) kernel. We provide an extensive empirical evaluation of the RBF and GMM kernels on more than 50 datasets. For a majority of the datasets, the (tuning-free) GMM kernel outperforms the best-tuned RBF kernel. We then conduct extensive classification experiments for comparing the linearized RBF kernel using NRFF with the linearized GMM kernel using GCWS. We observe that, in order to reach a similar accuracy, GCWS typically requires substantially fewer samples than NRFF, even on datasets where the original RBF kernel outperforms the original GMM kernel. As the training, storage, and processing costs are directly proportional to the sample size, our experiments can help demonstrate that GCWS would be a more practical scheme for large-scale machine learning applications. The empirical success of GCWS (compared to NRFF) can also be explained theoretically, from at least two aspects. Firstly, the relative variance (normalized by the squared expectation) of GCWS is substantially smaller than that of NRFF, except for the very high similarity region (where the variances of both methods approach zero). Secondly, if we are allowed to make a general model assumption on the data, then we can show analytically that GCWS exhibits much smaller variance than NRFF for estimating the same object (e.g., the RBF kernel), except for the very high similarity region.",
                "id": "01073"
            },
            {
                "title": "Fast near neighbor search in high-dimensional binary data",
                "abstract": "Numerous applications in search, databases, machine learning, and computer vision, can benefit from efficient algorithms for near neighbor search. This paper proposes a simple framework for fast near neighbor search in high-dimensional binary data, which are common in practice (e.g., text). We develop a very simple and effective strategy for sub-linear time near neighbor search, by creating hash tables directly using the bits generated by b-bit minwise hashing. The advantages of our method are demonstrated through thorough comparisons with two strong baselines: spectral hashing and sign (1-bit) random projections.",
                "id": "01074"
            },
            {
                "title": "Very sparse random projections",
                "abstract": "There has been considerable interest in random projections, an approximate algorithm for estimating distances between pairs of points in a high-dimensional vector space. Let A in Rn x D be our n points in D dimensions. The method multiplies A by a random matrix R in RD x k, reducing the D dimensions down to just k for speeding up the computation. R typically consists of entries of standard normal N(0,1). It is well known that random projections preserve pairwise distances (in the expectation). Achlioptas proposed sparse random projections by replacing the N(0,1) entries in R with entries in -1,0,1 with probabilities 1/6, 2/3, 1/6, achieving a threefold speedup in processing time.We recommend using R of entries in -1,0,1 with probabilities 1/2\u221aD, 1-1\u221aD, 1/2\u221aD for achieving a significant \u221aD-fold speedup, with little loss in accuracy.",
                "id": "01075"
            },
            {
                "title": "Estimating Entropy of Data Streams Using Compressed Counting",
                "abstract": "The Shannon entropy is a widely used summary statistic, for example, network\ntraffic measurement, anomaly detection, neural computations, spike trains, etc.\nThis study focuses on estimating Shannon entropy of data streams. It is known\nthat Shannon entropy can be approximated by Reenyi entropy or Tsallis entropy,\nwhich are both functions of the p-th frequency moments and approach Shannon\nentropy as p->1.\n  Compressed Counting (CC) is a new method for approximating the p-th frequency\nmoments of data streams. Our contributions include:\n  1) We prove that Renyi entropy is (much) better than Tsallis entropy for\napproximating Shannon entropy.\n  2) We propose the optimal quantile estimator for CC, which considerably\nimproves the previous estimators.\n  3) Our experiments demonstrate that CC is indeed highly effective\napproximating the moments and entropies. We also demonstrate the crucial\nimportance of utilizing the variance-bias trade-off.",
                "id": "01076"
            },
            {
                "title": "Towards a universal sketch for origin-destination network measurements",
                "abstract": "Despite its importance in today's Internet, network measurement was not an integral part of the original Internet architecture, i.e., there was (and still is) little native support for many essential measurement tasks. Targeting the inadequacy of counting/accounting capabilities of existing routers, many data streaming and sketching techniques have been proposed to estimate the important statistics of traffic going through a network link. Most of these techniques are, however, developed to track one specific statistic and/or answer a specific type of query. Since there are a large number of such statistics and queries of interest, it is very difficult, if not impossible, for network vendors and operators to implement and deploy data streaming/sketching solutions for all of them, due to router resource (memory, CPU, bus bandwidth, etc.) constraints. In this paper, we propose a general-purpose solution that can not only answer a wide range of queries, but also be able to answer types of queries that were not known a priori. In particular, we introduce the use of the Conditional Random Sampling (CRS) sketch data structure for succinctly capturing network traffic data between a set of nodes in the network. This sketch is the first step towards a \"universal\" sketch data structure in the sense that it is not tied to measurement of a single quantity. We show that the CRS sketch can compute unbiased estimates for any linear summary statistic in the intersection of a pair of traffic streams, e.g., traffic and flow matrix information, flow counts, and entropy. We present detailed experiments, using data collected at a tier-1 ISP, that show that our sketch is capable of estimating this wide range of statistics with fairly high accuracy.",
                "id": "01077"
            },
            {
                "title": "ABC-LogitBoost for Multi-class Classification",
                "abstract": "We develop abc-logitboost, based on the prior work on abc-boost(10) and robust logitboost(11). Our ex- tensive experiments on a variety of datasets demonstrate the considerable improvement of abc-logitboost over logitboost and abc-mart.",
                "id": "01078"
            },
            {
                "title": "Coding for Random Projections and Approximate Near Neighbor Search.",
                "abstract": "  This technical note compares two coding (quantization) schemes for random projections in the context of sub-linear time approximate near neighbor search. The first scheme is based on uniform quantization while the second scheme utilizes a uniform quantization plus a uniformly random offset (which has been popular in practice). The prior work compared the two schemes in the context of similarity estimation and training linear classifiers, with the conclusion that the step of random offset is not necessary and may hurt the performance (depending on the similarity level). The task of near neighbor search is related to similarity estimation with importance distinctions and requires own study. In this paper, we demonstrate that in the context of near neighbor search, the step of random offset is not needed either and may hurt the performance (sometimes significantly so, depending on the similarity and other parameters). ",
                "id": "01079"
            },
            {
                "title": "0-Bit Consistent Weighted Sampling",
                "abstract": "We develop 0-bit consistent weighted sampling (CWS) for efficiently estimating min-max kernel, which is a generalization of the resemblance kernel originally designed for binary data. Because the estimator of 0-bit CWS constitutes a positive definite kernel, this method can be naturally applied to large-scale data mining problems. Basically, if we feed the sampled data from 0-bit CWS to a highly efficient linear classifier (e.g., linear SVM), we effectively (and approximately) train a nonlinear classifier based on the min-max kernel. The accuracy improves as we increase the sample size. In this paper, we first demonstrate, through an extensive classification study using kernel machines, that the min-max kernel often provides an effective measure of similarity for nonnegative data. This helps justify the use of min-max kernel. However, as the min-max kernel is nonlinear and might be difficult to be used for industrial applications with massive data, we propose to linearize the min-max kernel via 0-bit CWS, a simplification of the original CWS method. The previous remarkable work on {\\\\em consistent weighted sampling (CWS)} produces samples in the form of (i*, t*) where the i* records the location (and in fact also the weights) information analogous to the samples produced by classical minwise hashing on binary data. Because the t* is theoretically unbounded, it was not immediately clear how to effectively implement CWS for building large-scale linear classifiers. We provide a simple solution by discarding t* (which we refer to as the \\\"0-bit\\\" scheme). Via an extensive empirical study, we show that this 0-bit scheme does not lose essential information. We then apply 0-bit CWS for building linear classifiers to approximate min-max kernel classifiers, as extensively validated on a wide range of public datasets. We expect this work will generate interests among data mining practitioners who would like to efficiently utilize the nonlinear information of non-binary and nonnegative data.",
                "id": "01080"
            },
            {
                "title": "Using sketches to estimate associations",
                "abstract": "We should not have to look at the entire corpus (e.g., the Web) to know if two words are associated or not. A powerful sampling technique called Sketches was originally introduced to remove duplicate Web pages. We generalize sketches to estimate contingency tables and associations, using a maximum likelihood estimator to find the most likely contingency table given the sample, the margins (document frequencies) and the size of the collection. Not unsurprisingly, computational work and statistical accuracy (variance or errors) depend on sampling rate, as will be shown both theoretically and empirically. Sampling methods become more and more important with larger and larger collections. At Web scale, sampling rates as low as 10-4 may suffice.",
                "id": "01081"
            },
            {
                "title": "Fast ABC-Boost for Multi-Class Classification",
                "abstract": "Abc-boost is a new line of boosting algorithms for multi-class\nclassification, by utilizing the commonly used sum-to-zero constraint. To\nimplement abc-boost, a base class must be identified at each boosting step.\nPrior studies used a very expensive procedure based on exhaustive search for\ndetermining the base class at each boosting step. Good testing performances of\nabc-boost (implemented as abc-mart and abc-logitboost) on a variety of datasets\nwere reported.\n  For large datasets, however, the exhaustive search strategy adopted in prior\nabc-boost algorithms can be too prohibitive. To overcome this serious\nlimitation, this paper suggests a heuristic by introducing Gaps when computing\nthe base class during training. That is, we update the choice of the base class\nonly for every $G$ boosting steps (i.e., G=1 in prior studies). We test this\nidea on large datasets (Covertype and Poker) as well as datasets of moderate\nsizes. Our preliminary results are very encouraging. On the large datasets,\neven with G=100 (or larger), there is essentially no loss of test accuracy. On\nthe moderate datasets, no obvious loss of test accuracy is observed when G<=\n20~50. Therefore, aided by this heuristic, it is promising that abc-boost will\nbe a practical tool for accurate multi-class classification.",
                "id": "01082"
            },
            {
                "title": "Real-Time Implementation of Improved State-Space MPC for Air Supply in a Coke Furnace",
                "abstract": "In an industrial coke process, the dynamic relationship between the input and output devices is complicated. Since performance is closely related to the control used, improved control alternatives are necessary. This paper deals with the oxygen content control loop and proposes an improved state-space model predictive control (MPC) aimed at satisfying steady air supply into the process. The details of the proposed MPC are first described and tested on a typical example and then implemented on an industrial coke furnace. Simulation examples and real-time implementation results both show the effectiveness of the proposed scheme.",
                "id": "01083"
            },
            {
                "title": "Regularization-free estimation in trace regression with symmetric positive semidefinite matrices.",
                "abstract": "Trace regression models have received considerable attention in the context of matrix completion, quantum state tomography, and compressed sensing. Estimation of the underlying matrix from regularization-based approaches promoting low-rankedness, notably nuclear norm regularization, have enjoyed great popularity. In this paper, we argue that such regularization may no longer be necessary if the underlying matrix is symmetric positive semidefinite (spd) and the design satisfies certain conditions. In this situation, simple least squares estimation subject to an spd constraint may perform as well as regularization-based approaches with a proper choice of regularization parameter, which entails knowledge of the noise level and/or tuning. By contrast, constrained least squares estimation comes without any tuning parameter and may hence be preferred due to its simplicity.",
                "id": "01084"
            },
            {
                "title": "Adaptive Stochastic Alternating Direction Method of Multipliers.",
                "abstract": "The Alternating Direction Method of Multipliers (ADMM) has been studied for years. Traditional ADMM algorithms need to compute, at each iteration, an (empirical) expected loss function on all training examples, resulting in a computational complexity proportional to the number of training examples. To reduce the complexity, stochastic ADMM algorithms were proposed to replace the expected loss function with a random loss function associated with one uniformly drawn example plus a Bregman divergence term. The Bregman divergence, however, is derived from a simple 2nd-order proximal function, i.e., the half squared norm, which could be a suboptimal choice. In this paper, we present a new family of stochastic ADMM algorithms with optimal 2nd-order proximal functions, which produce a new family of adaptive stochastic ADMM methods. We theoretically prove that the regret bounds are as good as the bounds which could be achieved by the best proximal function that can be chosen in hindsight. Encouraging empirical results on a variety of real-world datasets confirm the effectiveness and efficiency of the proposed algorithms.",
                "id": "01085"
            },
            {
                "title": "Online Optimization for Max-Norm Regularization.",
                "abstract": "The max-norm regularizer has been extensively studied in the last decade as it promotes an effective low-rank estimation for the underlying data. However, such max-norm regularized problems are typically formulated and solved in a batch manner, which prevents it from processing big data due to possible memory bottleneck. In this paper, hence, we propose an online algorithm that is scalable to large problems. In particular, we consider the matrix decomposition problem as an example, although a simple variant of the algorithm and analysis can be adapted to other important problems such as matrix completion. The crucial technique in our implementation is to reformulate the max-norm to an equivalent matrix factorization form, where the factors consist of a (possibly overcomplete) basis component and a coefficients one. In this way, we may maintain the basis component in the memory and optimize over it and the coefficients for each sample alternatively. Since the size of the basis component is independent of the sample size, our algorithm is appealing when manipulating a large collection of samples. We prove that the sequence of the solutions (i.e., the basis component) produced by our algorithm converges to a stationary point of the expected loss function asymptotically. Numerical study demonstrates encouraging results for the robustness of our algorithm compared to the widely used nuclear norm solvers.",
                "id": "01086"
            },
            {
                "title": "On the Sample Complexity of Compressed Counting",
                "abstract": "Compressed Counting (CC), based on maximally skewed stable random\nprojections, was recently proposed for estimating the p-th frequency moments of\ndata streams. The case p->1 is extremely useful for estimating Shannon entropy\nof data streams. In this study, we provide a very simple algorithm based on the\nsample minimum estimator and prove a much improved sample complexity bound,\ncompared to prior results.",
                "id": "01087"
            },
            {
                "title": "User-level sentiment analysis incorporating social networks",
                "abstract": "We show that information about social relationships can be used to improve user-level sentiment analysis. The main motivation behind our approach is that users that are somehow \"connected\" may be more likely to hold similar opinions; therefore, relationship information can complement what we can extract about a user's viewpoints from their utterances. Employing Twitter as a source for our experimental data, and working within a semi-supervised framework, we propose models that are induced either from the Twitter follower/followee network or from the network in Twitter formed by users referring to each other using \"@\" mentions. Our transductive learning results reveal that incorporating social-network information can indeed lead to statistically significant sentiment classification improvements over the performance of an approach based on Support Vector Machines having access only to textual features.",
                "id": "01088"
            },
            {
                "title": "In Defense of MinHash Over SimHash.",
                "abstract": "MinHash and SimHash are the two widely adopted Locality Sensitive Hashing (LSH) algorithms for large-scale data processing applications. Deciding which LSH to use for a particular problem at hand is an important question, which has no clear answer in the existing literature. In this study, we provide a theoretical answer (validated by experiments) that MinHash virtually always outperforms SimHash when the data are binary, as common in practice such as search. The collision probability of MinHash is a function of resemblance similarity (R,), while the collision probability of SimHash is a function of cosine similarity (S). To provide a common basis for comparison, we evaluate retrieval results in terms of S for both MinHash and SimHash. This evaluation is valid as we can prove that MinHash is a valid LSH with respect to S, by using a general inequality S-2 < R <= S/2-S. Our worst case analysis can show that MinHash significantly outperforms SimHash in high similarity region. Interestingly, our intensive experiments reveal that MinHash is also substantially better than SimHash even in datasets where most of the data points are not too similar to each other. This is partly because, in practical data, often R >= s/z-s holds where z is only slightly larger than 2 (e.g., z < 2.1). Our restricted worst case analysis by assuming s/z-s <= R <= S/2-S shows that MinHash indeed significantly outperforms SimHash even in low similarity region. We believe the results in this paper will provide valuable guidelines for search in practice, especially when the data are sparse.",
                "id": "01089"
            },
            {
                "title": "One Permutation Hashing for Efficient Search and Learning",
                "abstract": "  Recently, the method of b-bit minwise hashing has been applied to large-scale linear learning and sublinear time near-neighbor search. The major drawback of minwise hashing is the expensive preprocessing cost, as the method requires applying (e.g.,) k=200 to 500 permutations on the data. The testing time can also be expensive if a new data point (e.g., a new document or image) has not been processed, which might be a significant issue in user-facing applications.   We develop a very simple solution based on one permutation hashing. Conceptually, given a massive binary data matrix, we permute the columns only once and divide the permuted columns evenly into k bins; and we simply store, for each data vector, the smallest nonzero location in each bin. The interesting probability analysis (which is validated by experiments) reveals that our one permutation scheme should perform very similarly to the original (k-permutation) minwise hashing. In fact, the one permutation scheme can be even slightly more accurate, due to the \"sample-without-replacement\" effect.   Our experiments with training linear SVM and logistic regression on the webspam dataset demonstrate that this one permutation hashing scheme can achieve the same (or even slightly better) accuracies compared to the original k-permutation scheme. To test the robustness of our method, we also experiment with the small news20 dataset which is very sparse and has merely on average 500 nonzeros in each data vector. Interestingly, our one permutation scheme noticeably outperforms the k-permutation scheme when k is not too small on the news20 dataset. In summary, our method can achieve at least the same accuracy as the original k-permutation scheme, at merely 1/k of the original preprocessing cost. ",
                "id": "01090"
            },
            {
                "title": "Entropy Estimations Using Correlated Symmetric Stable Random Projections.",
                "abstract": "Methods for efficiently estimating the Shannon entropy of data streams have important applications in learning, data mining, and network anomaly detections (e.g., the DDoS attacks). For nonnegative data streams, the method of Compressed Counting (CC) based on maximally-skewed stable random projections can provide accurate estimates of the Shannon entropy using small storage. However, CC is no longer applicable when entries of data streams can be below zero, which is a common scenario when comparing two streams. In this paper, we propose an algorithm for entropy estimation in general data streams which allow negative entries. In our method, the Shannon entropy is approximated by the finite difference of two correlated frequency moments estimated from correlated samples of symmetric stable random variables. Our experiments confirm that this method is able to substantially better approximate the Shannon entropy compared to the prior state-of-the-art.",
                "id": "01091"
            },
            {
                "title": "A Comparison Study of Nonlinear Kernels.",
                "abstract": "  In this paper, we compare 5 different nonlinear kernels: min-max, RBF, fRBF (folded RBF), acos, and acos-$\\chi^2$, on a wide range of publicly available datasets. The proposed fRBF kernel performs very similarly to the RBF kernel. Both RBF and fRBF kernels require an important tuning parameter ($\\gamma$). Interestingly, for a significant portion of the datasets, the min-max kernel outperforms the best-tuned RBF/fRBF kernels. The acos kernel and acos-$\\chi^2$ kernel also perform well in general and in some datasets achieve the best accuracies.   One crucial issue with the use of nonlinear kernels is the excessive computational and memory cost. These days, one increasingly popular strategy is to linearize the kernels through various randomization algorithms. In our study, the randomization method for the min-max kernel demonstrates excellent performance compared to the randomization methods for other types of nonlinear kernels, measured in terms of the number of nonzero terms in the transformed dataset.   Our study provides evidence for supporting the use of the min-max kernel and the corresponding randomized linearization method (i.e., the so-called \"0-bit CWS\"). Furthermore, the results motivate at least two directions for future research: (i) To develop new (and linearizable) nonlinear kernels for better accuracies; and (ii) To develop better linearization algorithms for improving the current linearization methods for the RBF kernel, the acos kernel, and the acos-$\\chi^2$ kernel. One attempt is to combine the min-max kernel with the acos kernel or the acos-$\\chi^2$ kernel. The advantages of these two new and tuning-free nonlinear kernels are demonstrated vias our extensive experiments. ",
                "id": "01092"
            },
            {
                "title": "An Empirical Evaluation of Four Algorithms for Multi-Class Classification: Mart, ABC-Mart, Robust LogitBoost, and ABC-LogitBoost",
                "abstract": "This empirical study is mainly devoted to comparing four tree-based boosting\nalgorithms: mart, abc-mart, robust logitboost, and abc-logitboost, for\nmulti-class classification on a variety of publicly available datasets. Some of\nthose datasets have been thoroughly tested in prior studies using a broad range\nof classification algorithms including SVM, neural nets, and deep learning.\n  In terms of the empirical classification errors, our experiment results\ndemonstrate:\n  1. Abc-mart considerably improves mart. 2. Abc-logitboost considerably\nimproves (robust) logitboost. 3. Robust) logitboost} considerably improves mart\non most datasets. 4. Abc-logitboost considerably improves abc-mart on most\ndatasets. 5. These four boosting algorithms (especially abc-logitboost)\noutperform SVM on many datasets. 6. Compared to the best deep learning methods,\nthese four boosting algorithms (especially abc-logitboost) are competitive.",
                "id": "01093"
            },
            {
                "title": "Asymmetric LSH (ALSH) for Sublinear Time Maximum Inner Product Search (MIPS).",
                "abstract": "We present the first provably sublinear time hashing algorithm for approximate Maximum Inner Product Search (MIPS). Searching with (un-normalized) inner product as the underlying similarity measure is a known difficult problem and finding hashing schemes for MIPS was considered hard. While the existing Locality Sensitive Hashing (LSH) framework is insufficient for solving MIPS, in this paper we extend the LSH framework to allow asymmetric hashing schemes. Our proposal is based on a key observation that the problem of finding maximum inner products, after independent asymmetric transformations, can be converted into the problem of approximate near neighbor search in classical settings. This key observation makes efficient sublinear hashing scheme for MIPS possible. Under the extended asymmetric LSH (ALSH) framework, this paper provides an example of explicit construction of provably fast hashing scheme for MIPS. Our proposed algorithm is simple and easy to implement. The proposed hashing scheme leads to significant computational savings over the two popular conventional LSH schemes: (i) Sign Random Projection (SRP) and (ii) hashing based on p-stable distributions for L-2 norm (L2LSH), in the collaborative filtering task of item recommendations on Netflix and Movielens (10M) datasets.",
                "id": "01094"
            },
            {
                "title": "Adaptive fuzzy logic controller for vehicle active suspensions with interval type-2 fuzzy membership functions",
                "abstract": "Elicited from the least means squares optimal algorithm (LMS), an adaptive fuzzy logic controller (AFC) based on interval type-2 fuzzy sets is proposed for vehicle non-linear active suspension systems. The interval membership functions (IMF2s) are utilized in the AFC design to deal with not only non-linearity and uncertainty caused from irregular road inputs and immeasurable disturbance, but also the potential uncertainty of expertpsilas knowledge and experience. The adaptive strategy is designed to self-tune the active force between the lower bounds and upper bounds of interval fuzzy outputs. A case study based on a quarter active suspension model has demonstrated that the proposed type-2 fuzzy controller significantly outperforms conventional fuzzy controllers of an active suspension and a passive suspension.",
                "id": "01095"
            }
        ]
    },
    {
        "id": "011",
        "input": "For an author who has written the paper with the title \"Perspectives on Cognitive Informatics and Cognitive Computing\", which reference is related? Just answer with [1] or [2] without explanation. [1]: \"Learning Discriminative And Shareable Features For Scene Classification\" [2]: \"Using design patterns to develop reusable object-oriented communication software\"",
        "profile": [
            {
                "title": "Formal Modeling And Specification Of Design Patterns Using Rtpa",
                "abstract": "Software patterns are recognized as an ideal documentation of expert knowledge in software design and development. However, its formal model and semantics have not been generalized and matured. The traditional UML specifications and related formalization efforts cannot capture the essence of generic patterns precisely, understandably, and essentially. A generic mathematical model of patterns is presented in this article using real-time process algebra (RTPA). The formal model of patterns are more readable and highly generic, which can be used as the meta model to denote any design patterns deductively, and can be translated into code in programming languages by supporting tools. This work reveals that a pattern is a highly complicated and dynamic structure for software design encapsulation, because of its complex and flexible internal associations between multiple abstract classes and instantiations. The generic model of patterns is not only applicable to existing patterns' description and comprehension, but also useful for future patterns' identification and formalization.",
                "id": "0110"
            },
            {
                "title": "The Formal Design Model of a File Management System FMS",
                "abstract": "Files are a typical abstract data type for data objects and software modeling, which provides a standard encapsulation and access interface for manipulating large-volume information and persistent data. File management systems are an indispensable component of operating systems and real-time systems for file manipulations. This paper develops a comprehensive design pattern of files and a File Management System FMS. A rigorous denotational mathematics, Real-Time Process Algebra RTPA, is adopted, which allows both architectural and behavioral models of files and FMS to be rigorously designed and implemented in a top-down approach. The conceptual model, architectural model, and the static/dynamic behavioral models of files and FMS are systematically presented. This work has been applied in the design and modeling of a real-time operating system RTOS+.",
                "id": "0111"
            },
            {
                "title": "Formal Description of Object-Oriented Software Measurement and Metrics in SEMS",
                "abstract": "The philosophy toward metrology is based on a fundamental assumption that one cannot control an object or a system if one cannot measure it. This paper presents a set of formally described measures for object-oriented software in the context of an innovated software engineering measurement system (SEMS). SEMS models over 304 meta- and derived-measures for software products and software engineering processes measurement. The measures for object-oriented architectures, sizes, and complexity in SEMS represent the nature and fundamental attributes of object-oriented software. This work also shows that a software measurement system can be described by an algebraic system, therefore new measures may be derived based on a core set of meta measures.",
                "id": "0112"
            },
            {
                "title": "The Formal Design Models of a Set of Abstract Data Types ADTs",
                "abstract": "Type theories are fundamental for underpinning data object modeling and system architectural design in computing and software engineering. Abstract Data Types ADTs are a set of highly generic and rigorously modeled data structures in type theory. ADTs also play a key role in Object-Oriented OO technologies for software system design and implementation. This paper presents a formal modeling methodology for ADTs using the Real-Time Process Algebra RTPA, which allows both architectural and behavioral models of ADTs and complex data objects. Formal architectures, static behaviors, and dynamic behaviors of a set of ADTs are comparatively studied. The architectural models of the ADTs are created using RTPA architectural modeling methodologies known as the Unified Data Models UDMs. The static behaviors of the ADTs are specified and refined by a set of Unified Process Models UPMs of RTPA. The dynamic behaviors of the ADTs are modeled by process dispatching technologies of RTPA. This work has been applied in a number of real-time and non-real-time system designs such as a Real-Time Operating System RTOS+, a Cognitive Learning Engine CLE, and the automatic code generator based on RTPA.",
                "id": "0113"
            },
            {
                "title": "Formal properties and rules of concept algebra",
                "abstract": "Concept algebra is a denotational mathematics for rigorously manipulating formal concepts and their algebraic operations in knowledge representation, semantic analyses, and machine learning. Properties of concept algebra are formally studied in order to elaborate the nature of formal concepts and their algebraic operations. This leads to a set of algebraic rules in the categories of relational, reproductive, and compositional operations on formal concepts. Relationship between algebraic operations of concept algebra is rigorously described. Proofs are provided for the algebraic rules of concept algebra elaborated by examples. This work enables a rigorous implementation of concept algebra in cognitive knowledge base manipulations and cognitive machine learning.",
                "id": "0114"
            },
            {
                "title": "On cognitive foundations of creativity and the cognitive process of creation",
                "abstract": "Creativity is a gifted ability of human as a higher-level cognitive process. This paper formally investigates into the cognitive process of creation and creativity as one of the most fantastic life functions. The cognitive foundations of creativity are explored, which explain the space of creativity, the approaches to creativity, the relationships of creation and problem solving, and the attributes of creative researchers. A set of mathematical models of creation and creativity is established on the basis of the tree structures and properties of human knowledge known as concept trees. The measurement of creativity is quantitatively analyzed, followed by the formal elaboration of the cognitive process of creation as a part of the Layered reference Model of the Brain (LRMB).",
                "id": "0115"
            },
            {
                "title": "On abstract intelligence and brain informatics: Mapping the cognitive functions onto the neural architectures",
                "abstract": "Summary form only given. A fundamental challenge for almost all scientific disciplines is to explain how natural intelligence is generated by physiological organs and what the logical model of the brain is beyond its neural architectures. According to cognitive informatics and abstract intelligence, the exploration of the brain is a complicated recursive problem where contemporary denotational mathematics is needed to efficiently deal with it. Cognitive psychology and medical science are used to explain that the brain works in a certain way based on empirical observations of corresponding activities in usually overlapped brain areas. However, the lack of precise models and rigorous causality in brain studies has dissatisfied the formal expectations of researchers in computational science and mathematics, because a computer, the logical counterpart of the brain, might not be explained in such a vigor and empirical approach without the support of a formal model and a rigorous means. In order to formally explain the architectures and functions of the brain, as well as their intricate relations and interactions, systematic models of the brain are sought for revealing the principles and mechanisms of the brain at the neural, physiological, cognitive, and logical (abstract) levels. Cognitive and brain informatics investigate into the brain via not only inductive syntheses through these four cognitive levels from the bottom up in order to form theories based on empirical observations, but also deductive analyses from the top down in order to explain various functional and behavioral instances according to the abstract intelligence theory. This keynote lecture presents systematic models of the brain from the facets of cognitive informatics, abstract intelligence, brain Informatics, neuroinformatics and cognitive psychology. A logical model of the brain is introduced that maps the cognitive functions of the brain onto its neural and physiological architectures. This work leads to a coherent abstract intelligence theory based on both denotational mathematical models and cognitive psychology observations, which rigorously explains the underpinning principles and mechanisms of the brain. On the basis of the abstract intelligence theories and the logical models of the brain, a comprehensive set of cognitive behaviors as identified in the Layered Reference Model of the Brain (LRMB) such as perception, inference and learning can be rigorously explained and simulated.The logical model of the brain and the abstract intelligence theory of the natural intelligence will enable the development of cognitive computers that perceive, think and learn. The functional and theoretical difference between cognitive computers and classic computers are that the latter are data processors based on Boolean algebra and its logical counterparts; while the former are knowledge processors based on contemporary denotational mathematics. A wide range of applications of the cognitive computers have been developing in ICIC and my laboratory such as , inter alia, cognitive robots, cognitive learning engines, cognitive Internet, cognitive agents, cognitive search engines, cognitive translators, cognitive control systems, and cognitive automobiles.",
                "id": "0116"
            },
            {
                "title": "Cognitive robotics and mathematical engineering",
                "abstract": "It is recognized that the core problems across contemporary disciplines such as cognitive science, intelligence science, robotics, knowledge science, brain science, and computational intelligence are a fundamental mathematical problem where none of them may be simply reduced onto any type of numbers. This keynote lecture presents an emerging field known as mathematical engineering (ME) underpinning cognitive robotics. ME is a contemporary form of abstract engineering that studies formal structural models and functions of complex, abstract, and mental objects and their systematic and rigorous manipulations. ME is embodied by denotational mathematics (DM) supplement to traditional analytic mathematics. DM is a category of novel mathematical structures as function of functions on hyperstructures beyond those of real numbers and bits, in order to formalize rigorous expressions and inferences. ME powered by DMs provides a novel approach to solve complex and intelligent computing problems centric in the development of cognitive robots towards autonomous perception, inference, and learning mimicking the cognitive mechanisms of the brain.",
                "id": "0117"
            },
            {
                "title": "Cognitive Learning Methodologies for Brain-Inspired Cognitive Robotics",
                "abstract": "Cognitive robots are brain-inspired robots that are capable of inference, perception, and learning mimicking the cognitive mechanisms of the brain. Cognitive learning theories and methodologies for knowledge and behavior acquisition are centric in cognitive robotics. This paper explores the cognitive foundations and denotational mathematical means of cognitive learning engines CLE and cognitive knowledge bases CKB for cognitive robots. The architectures and functions of CLE are formally presented. A content-addressed knowledge base access methodology for CKB is rigorously elaborated. The CLE and CKB methodologies are not only designed to explain the mechanisms of human knowledge acquisition and learning, but also applied in the development of cognitive robots, cognitive computers, and knowledge-based systems.",
                "id": "0118"
            },
            {
                "title": "On The Cognitive Processes Of Human Perception With Emotions, Motivations, And Attitudes",
                "abstract": "An interactive motivation-attitude theory is developed based on the Layered Reference Model of the Brain (LRMB) and the object-attribute-relation (OAR) model. This paper presents a rigorous model of human perceptual processes such as emotions, motivations, and attitudes. A set of mathematical models and formal cognitive processes of perception is developed. Interactions and relationships between motivation and attitude are formally described in real-time process algebra (RTPA). Applications of the mathematical models of motivations and attitudes in software engineering are demonstrated. This work is a part of the formalization of LRMB, which provides a comprehensive model for explaining the fundamental cognitive processes of the brain and their interactions. This work demonstrates that the complicated human emotional and perceptual phenomena can be rigorously modeled and formally treated based on cognitive informatics theories and denotational mathematics.",
                "id": "0119"
            },
            {
                "title": "Uncertain Data Clustering in Distributed Peer-to-Peer Networks.",
                "abstract": "Uncertain data clustering has been recognized as an essential task in the research of data mining. Many centralized clustering algorithms are extended by defining new distance or similarity measurements to tackle this issue. With the fast development of network applications, these centralized methods show their limitations in conducting data clustering in a large dynamic distributed peer-to-peer n...",
                "id": "01110"
            },
            {
                "title": "A Method for Built-in Tests in Component-based Software Maintenance",
                "abstract": "Viewing software structure as a composition of components is helpful to enable software maintenance carried out at a higher level than language statements, as well as to improve software maintenance efficiency and quality, and to increase reuse rate of existing software components in maintenance. In this paper, types of software maintenance are classified, and scopes of software components in component-based software maintenance are analyzed. A new kind of built-in test (BIT) components for maintainable software is developed. Based on this, a test/maintenance mode of maintainable software is proposed. Implementation approaches of the BIT components for software maintenance are developed. Applications of the BIT components in enhancing software maintainability, in reengineering legacy systems for maintainability, and in ensuring run-time consistency are described.",
                "id": "01111"
            },
            {
                "title": "The Cognitive Process of Comprehension",
                "abstract": "Comprehension is the action or capability of understanding. Comprehension is a higher cognitive process of the brain that searches relations between a given object or attribute and other objects, attributes, and relations in the long-term memory, and establishes a representational model for the object or attribute by connecting it to appropriate clusters of memory.It is recognized that although knowledge and information are powerful, before any information can be possessed and processed, it should be comprehended properly. This paper intends to model one of the cognitive life functions, comprehension, in cognitive informatics. The mechanism and process of comprehension are described based on thecognitive model of the brain developed by Wang. Then, Real-Time Process Algebra (RTPA) is used to formally describe the comprehension process.",
                "id": "01112"
            },
            {
                "title": "A European COTS Architecture with Built-in Tests",
                "abstract": "This paper presents a European approach to the development of a new industrial architecture of commercial off-the-shelf (COTS) software components, and a practical technology for design and implementation of test-reusable COTS. This work, known as the European COMPONENT+ project, is supported by the European 5th Framework programme and by a number of leading industrial partners in componentbased software engineering.Existing COTS architectural technologies were focused on code reuse. The following fundamental problems inherited in conventional COTS technologies have been identified: a) Low testability for end-users; b) Low maintainability for end-users; c) No support for run-time testing; and d) Separated software code and test cases.Being oriented to the problems and challenges as identified above, this paper presents new solutions and techniques for testable COTS architecture developed in the European Component+ project, such as: a) A new technology for implementing BIT-based COTS; b) A new approach to COTS test; c) An extension of OO technology from code reuse to test reuse in COTS development; and d) A new approach to enabling COTS test at run-time. BIT components can be embedded in any conventional COTS for enabling test reuse as well as code reuse. The BIT + COST technologies have been found a wide range of applications in component-based software engineering.",
                "id": "01113"
            },
            {
                "title": "Experience in Comparative Process Assessment with Multi-Process-Models.",
                "abstract": "For software process assessment and improvement, two of the top frequently-asked-questions in the software industry are that: (a) what is the interrelationships between current process models? and (b) how can a given capability level in one model be related to another objectively? This paper reports the findings of a comparative process assessment case study project for a software organisation using SPICE, CMM, BOOTSTRAP, ISO 9000 and SPRM methodologies. Interrelationship and transformability of the capability levels between the five process models are explored. In this approach, the capability level of a software organisation can be mutually transformed between current models. Therefore, the above significant problems in software process assessment can be solved. The stability of assessment results of current process models is tested by three specially designed cases. Empirical data on the time expended in process assessments by the five process models are derived and related to the unit of person-hours for estimating the costs in process assessment.",
                "id": "01114"
            },
            {
                "title": "A formal measurement of the cognitive complexity of texts in cognitive linguistics",
                "abstract": "The cognitive complexity of texts in natural languages is a fundamental measure of the properties of syntax and semantics in textual comprehension, processing, and search. This paper presents a formal metrics of text comprehension complexity in cognitive linguistics. Both objective and subjective aspects of text comprehension and their complexity are formally modeled. A formal language model is established that characterizes the discourse of natural languages. The mathematical models of cognitive complexity of texts and their comprehension are rigorously described. On the basis of the cognitive and mathematical models of cognitive linguistics, the measurement of cognitive complexity of texts is quantitatively established and tested by a set of case studies. A wide range of applications of the measurement of textual complexity are identified in cognitive linguistics and contemporary web technologies such as search engines, online document retrieval, natural language processing, cognitive linguistics, cognitive computing, cognitive machine learning, and computing with words.",
                "id": "01115"
            },
            {
                "title": "On Concept Algebra and Knowledge Representation.",
                "abstract": "A concept is a basic unit of cognition that carries certain meanings. In cognitive informatics, a concept is formally modeled as an abstract and dynamic mathematical structure that encapsulates attributes, objects, and relations. The most important feature of an abstract concept is its networking capability, or the adaptive capability to interrelate itself to other concepts. This paper presents a formal theory for abstract concepts and knowledge manipulation known as concept algebra. The mathematical models of concepts and knowledge are developed based on the object-attribute-relation (OAR) theory. The formal methodology for manipulating knowledge as a concept network is described. Case studies demonstrate that the concept network and concept algebra provide a generic and formal knowledge manipulation means that are capable to deal with complex knowledge structures and their algebraic operations.",
                "id": "01116"
            },
            {
                "title": "Guest editorial: On Modeling Object-Oriented Information Systems",
                "abstract": "Object-Oriented Information System (OOIS) is an information system that employs object-oriented technologies in system design and implementation. Recent research advances and industrial innovations in distributed system modeling and Internet applications have enabled OOIS design and implementation to be carried out on the basis of new technologies and platforms. This special section on Modeling Object-Oriented Information Systems presents readers with a set of best papers selected from the 7th International Conference on OOIS. Reviews of theories and applications of OOIS\u2019s are also provided for predicating trends in OOIS modeling.",
                "id": "01117"
            },
            {
                "title": "Formal Rules for Fuzzy Causal Analyses and Fuzzy Inferences",
                "abstract": "Causal inference is one of the central capabilities of the natural intelligence that plays a crucial role in thinking, perception, and problem solving. Fuzzy inferences are an extended form of formal inferences that provide a denotational mathematical means for rigorously dealing with degrees of matters, uncertainties, and vague semantics of linguistic variables, as well as for rational reasoning the semantics of fuzzy causalities. This paper presents a set of formal rules for causal analyses and fuzzy inferences such as those of deductive, inductive, abductive, and analogical inferences. Rules and methodologies for each of the fuzzy inferences are formally modeled and illustrated with real-world examples and cases of applications. The formalization of fuzzy inference methodologies enables machines to mimic complex human reasoning mechanisms in cognitive informatics, cognitive computing, soft computing, abstract intelligence, and computational intelligence.",
                "id": "01118"
            },
            {
                "title": "Cognitive Computational Models of Emotions and Affective Behaviors",
                "abstract": "Emotions are one of the important subconscious mechanisms that influence human behaviors, attentions, and decision making. The emotion process helps to determine how humans perceive their internal status and needs in order to form consciousness of an individual. Emotions have been studied from multidisciplinary perspectives and covered a wide range of empirical and psychological topics, such as understanding the emotional processes, creating cognitive and computational models of emotions, and applications in computational intelligence. This paper presents a comprehensive survey of cognitive and computational models of emotions resulted from multidisciplinary studies. It explores how cognitive models serve as the theoretical basis of computational models of emotions. The mechanisms underlying affective behaviors are examined as important elements in the design of these computational models. A comparative analysis of current approaches is elaborated based on recent advances towards a coherent cognitive computational model of emotions, which leads to the machine simulated emotions for cognitive robots and autonomous agent systems in cognitive informatics and cognitive computing.",
                "id": "01119"
            },
            {
                "title": "Big Data Analytics on the Characteristic Equilibrium of Collective Opinions in Social Networks",
                "abstract": "AbstractBig data are products of human collective intelligence that are exponentially increasing in all facets of quantity, complexity, semantics, distribution, and processing costs in computer science, cognitive informatics, web-based computing, cloud computing, and computational intelligence. This paper presents fundamental big data analysis and mining technologies in the domain of social networks as a typical paradigm of big data engineering. A key principle of computational sociology known as the characteristic opinion equilibrium is revealed in social networks and electoral systems. A set of numerical and fuzzy models for collective opinion analyses is formally presented. Fuzzy data mining methodologies are rigorously described for collective opinion elicitation and benchmarking in order to enhance the conventional counting and statistical methodologies for big data analytics.",
                "id": "01120"
            },
            {
                "title": "Measurement of the Cognitive Functional Complexity of Software",
                "abstract": "One of the central problems in software engineering is its inherited complexity. It is recognized that cognitive informatics plays an important role in understanding the fundamental characteristics of software. This paper models the cognitive weights of basic control structures of software, and develops a new concept of cognitive functional size for measuring software complexity. Comparative case studies between the cognitive functional sizes and physical sizes of 20 programs are conducted. It is found that for a given design, although the physical sizes of software may vary in a wide range, the cognitive functional sizes are much stable and independent from implementation technologies and programming languages. Therefore, the cognitive functional size provides a novel, stable, and practical software complexity measurement and is helpful to explain the fundamental nature of software complexity in the phase of design, implementation, or maintenance in software engineering.",
                "id": "01121"
            },
            {
                "title": "Neuroinformatics Models of Human Memory: Mapping the Cognitive Functions of Memory onto Neurophysiological Structures of the Brain",
                "abstract": "The human brain is a superbly marvelous and extremely complicated neurophysiological structure for generating natural intelligence that transforms cognitive information into colorful behaviors. The brain is the most complex and interesting objects in nature that requires rigorous scientific investigations by multidisciplinary methodologies and via transdisciplinary approaches where only low-level studies could not explain it. A fundamental problem and difficulty in contemporary brain science is the indistinguishable confusion of the cognitive mechanisms and neurophysiological structures of the kernel brain and its memories. This paper presents a set of formal neuroinformatics models of memory and a rigorous mapping between the cognitive functions of memory and their neurophysiological structures. The neurophysiological foundations of memory are rigorously described based on comprehensive cognitive models of memory. The cognitive architecture of human memory and its relationship to the intelligence power of the brain are logically analyzed. The cognitive roles of memory allocated in both cerebrum and cerebellum are revealed by mapping the functional models of memory onto corresponding neurophysiological structures of the brain. As a result, fundamental properties of memory and knowledge as well as their neurophysiological forms in the brain are systematically explained.",
                "id": "01122"
            },
            {
                "title": "A Computational Simulation of the Cognitive Process of Children Knowledge Acquisition and Memory Development",
                "abstract": "The cognitive mechanisms of knowledge representation, memory establishment, and learning are fundamental issues in understanding the brain. A basic approach to studying these mental processes is to observe and simulate how knowledge is memorized by little children. This paper presents a simulation tool for knowledge acquisition and memory development for young children of two to five years old. The cognitive mechanisms of memory, the mathematical model of concepts and knowledge, and the fundamental elements of internal knowledge representation are explored. The cognitive processes of children's memory and knowledge development are described based on concept algebra and the object-attribute-relation OAR model. The design of the simulation tool for children's knowledge acquisition and memory development is presented with the graphical representor of memory and the dynamic concept network of knowledge. Applications of the simulation tool are described by case studies on children's knowledge acquisition about family members, relatives, and transportation. This work is a part of the development of cognitive computers that mimic human knowledge processing and autonomous learning.",
                "id": "01123"
            },
            {
                "title": "Paradigms of Denotational Mathematics for Cognitive Informatics and Cognitive Computing",
                "abstract": "The abstract, rigorous, and expressive needs in cognitive informatics, intelligence science, software science, and knowledge science lead to new forms of mathematics collectively known as denotational mathematics. Denotational mathematics is a category of expressive mathematical structures that deals with high level mathematical entities beyond numbers and sets, such as abstract objects, complex relations, behavioral information, concepts, knowledge, processes, and systems. Denotational mathematics is usually in the form of abstract algebra that is a branch of mathematics in which a system of abstract notations is adopted to denote relations of abstract mathematical entities and their algebraic operations based on given axioms and laws. Four paradigms of denotational mathematics, known as concept algebra, system algebra, Real-Time Process Algebra (RTPA), and Visual Semantic Algebra (VSA), are introduced in this paper. Applications of denotational mathematics in cognitive informatics and computational intelligence are elaborated. Denotational mathematics is widely applicable to model and manipulate complex architectures and behaviors of both humans and intelligent systems, as well as long chains of inference processes.",
                "id": "01124"
            },
            {
                "title": "Intelligent Fault Recognition and Diagnosis for Rotating Machines using Neural Networks",
                "abstract": "Monitoring industrial machine health in real-time is not only in high demand, it is also complicated and difficult. Possible reasons for this include: a access to the machines on site is sometimes impracticable, and b the environment in which they operate is usually not human-friendly due to pollution, noise, hazardous wastes, etc. Despite theoretically sound findings on developing intelligent solutions for machine condition-based monitoring, few commercial tools exist in the market that can be readily used. This paper examines the development of an intelligent fault recognition and monitoring system Melvin I, which detects and diagnoses rotating machine conditions according to changes in fault frequency indicators. The signals and data are remotely collected from designated sections of machines via data acquisition cards. They are processed by a signal processor to extract characteristic vibration signals of ten key performance indicators KPIs. A 3-layer neural network is designed to recognize and classify faults based on a pre-determined set of KPIs. The system implemented in the laboratory and applied in the field can also incorporate new experiences into the knowledge base without overwriting previous training. Results show that Melvin I is a smart tool for both system vibration analysts and industrial machine operators.",
                "id": "01125"
            },
            {
                "title": "From information revolution to intelligence revolution: Big data science vs. intelligence science",
                "abstract": "The hierarchy of human knowledge is categorized at the levels of data, information, knowledge, and intelligence. For instance, given an AND-gate with 1,000-input pins, it may be described very much differently at various levels of perceptions in the knowledge hierarchy. At the data level on the bottom, it represents a 21,000 state space, known as `big data' in recent terms, which appears to be a big issue in engineering. However, at the information level, it just represents 1,000 bit information that is equivalent to the numbers of inputs. Further, at the knowledge level, it expresses only two rules that if all inputs are one, the output is one; and if any input is zero, the output is zero. Ultimately, at the intelligence level, it is simply an instance of the logical model of an AND-gate with arbitrary inputs. This problem reveals that human intelligence and wisdom are an extremely efficient and a fast convergent induction mechanism for knowledge and wisdom elicitation and abstraction where data are merely factual materials and arbitrary instances in the almost infinite state space of the real world. Although data and information processing have been relatively well studied, the nature, theories, and suitable mathematics underpinning knowledge and intelligence are yet to be systematically studied in cognitive informatics and cognitive computing. This will leads to a new era of human intelligence revolution following the industrial, computational, and information revolutions. This is also in accordance with the driving force of the hierarchical human needs from low-level material requirements to high-level ones such as knowledge, wisdom, and intelligence. The trend to the emerging intelligent revolution is to meet the ultimate human needs. The basic approach to intelligent revolution is to invent and embody cognitive computers, cognitive robots, and cognitive systems that extend human memory capacity, learning ability, wisdom, and creativity. Via intell- gence revolution, an interconnected cognitive intelligent Internet will enable ordinary people to access highly intelligent systems created based on the latest development of human knowledge and wisdom. Highly professional systems may help people to solve typical everyday problems. Towards these objectives, the latest advances in abstract intelligence and intelligence science investigated in cognitive informatics and cognitive computing are well positioned at the center of intelligence revolution. A wide range of applications of cognitive computers have been developing in ICIC [http://www.ucalgary.ca/icic/] such as, inter alia, cognitive computers, cognitive robots, cognitive learning engines, cognitive Internet, cognitive agents, cognitive search engines, cognitive translators, cognitive control systems, cognitive communications systems, and cognitive automobiles.",
                "id": "01126"
            },
            {
                "title": "The cognitive processes of perceptions on spatiality, time, and motion.",
                "abstract": "The human perceptual senses of spatiality, time, and motion are fundamental cognitive life functions according to the Layered Reference Model of the Brain (LRMB). This paper presents the cognitive process of human perceptual senses on spatiality, time, and motion. The sense of spatiality is investigated into the coordinate system, orientations, and cognitive maps. Then, the mathematical model and the cognitive process of human spatial sense are developed. The sense of time with the biological clocks, cognitive clocks, and their mathematical models are analyzed in order to explain the cognitive process of human time sense. On the basis of the formal models of senses of spatiality and time, the sense of motion is modeled as a complex sense incorporating both of spatiality and time. Then, the cognitive, mathematical, and process models of the sense of motion are rigorously established.",
                "id": "01127"
            },
            {
                "title": "The Formal Design Models of Tree Architectures and Behaviors",
                "abstract": "Trees are one of the most fundamental and widely used non-linear hierarchical structures of linked nodes. A binary tree B-Tree is a typical balanced tree where the fan-out of each node is at most two known as the left and right children. This paper develops a comprehensive design pattern of formal trees using the B-Tree architecture. A rigorous denotational mathematics, Real-Time Process Algebra RTPA, is adopted, which allows both architectural and behavioral models of B-Trees to be rigorously designed and implemented in a top-down approach. The architectural models of B-Trees are created using RTPA architectural modeling methodologies known as the Unified Data Models UDMs. The physical model of B-Trees is implemented using the left and right child nodes dynamically created in memory. The behavioral models of B-Trees are specified and refined by a set of Unified Process Models UPMs in three categories namely the management operations, traversal operations, and node I/O operations. This work has been applied in a number of real-time and nonreal-time system designs such as a real-time operating system RTOS+, a general system organization model, and the ADT library for an RTPA-based automatic code generator.",
                "id": "01128"
            },
            {
                "title": "Formal Rtpa Models For A Set Of Meta-Cognitive Processes Of The Brain",
                "abstract": "The cognitive processes modeled at the metacognitive level of the layered reference mode of the brain (LRMB) encompass those of object identification, abstraction, concept establishment, search, categorization, comparison, memorization, qualification, quantification, and selection. It is recognized that all higher layer cognitive processes of the brain rely on the metacognitive processes. Each of this set of fundamental cognitive processes is formally described by a mathematical model and a process model. Real-time process algebra (RTPA) is adopted as a denotational mathematical means for rigorous modeling and describing the metacognitive processes. All cognitive models and processes are explained on the basis of the object-attribute- relation (OAR) model for internal information and knowledge representation and manipulation.",
                "id": "01129"
            },
            {
                "title": "The Formal Design Model of a Real-Time Operating System RTOS+: Conceptual and Architectural Frameworks",
                "abstract": "A real-time operating system RTOS provides a platform for the design and implementation of a wide range of applications in real-time systems, embedded systems, and mission-critical systems. This paper presents a formal design model for a general RTOS known as RTOS+ that enables a specific target RTOS to be rigorously and efficiently derived in real-world applications. The methodology of a denotational mathematics, Real-Time Process Algebra RTPA, is described for formally modeling and refining architectures, static behaviors, and dynamic behaviors of RTOS+. The conceptual model of the RTOS+ system is introduced as the initial requirements for the system. The architectural model of RTOS+ is created using RTPA architectural modeling methodologies and refined by a set of Unified Data Models UDMs. The static behaviors of RTOS+ are specified and refined by a set of Unified Process Models UPMs. The dynamic behaviors of the RTOS+ system are specified and refined by the real-time process scheduler and system dispatcher. This work is presented in two papers; the conceptual and architectural models of RTOS+ is described in this paper, while the static and dynamic behavioral models of RTOS+ will be elaborated in a forthcoming paper.",
                "id": "01130"
            },
            {
                "title": "Formal Linguistics and the Deductive Grammar",
                "abstract": "Artificial Intelligence applications, particularly those involving natural language understanding, are actually less ambitious than they were decades ago. Statistical and machine learning techniques have had great success on tasks that can be treated ...",
                "id": "01131"
            },
            {
                "title": "Real-Time Process Algebra and Its Applications",
                "abstract": "It is recognized that human and system behaviors may be modeled by a 3-D process comprising actions, time, and space. Software behaviors, similarly, can be modeled in the three dimensions known as the mathematical operations, event/process timing, and memory manipulation. This paper introduces Real-Time Process Algebra (RTPA) as a coherent software engineering notation system. RTPA is used to address the 3-D problem in software system description and specification in terms of architecture, static and dynamic behaviors. Case studies on applications of RTPA in real-time system modeling and specification are provided in this paper with real-world examples.",
                "id": "01132"
            },
            {
                "title": "Design and Implementation of a Knowledge Base for Machine Knowledge Learning",
                "abstract": "Knowledge bases are a fundamental platform for knowledge acquisition, retaining, retrieval, reasoning and generation across machine learning, natural language processing and computational intelligence. The mathematical model of formal concepts is centric in knowledge bases for modeling the basic unit of human knowledge and thinking threads. This paper presents the design and implementation of a cognitive knowledge base. The structure of the knowledge base is created as a dynamic concept network mimicking human knowledge represented in the brain. The knowledge base enables a set of novel knowledge manipulations for machine learning such as knowledge acquisition, access, analysis, refinement, fusion and system maintenance. Experimental results demonstrate the performance and efficiency of the implementation of the generic knowledge base for cognitive robots and machine learning systems.",
                "id": "01133"
            },
            {
                "title": "Formal Description of the Cognitive Process of Decision Making",
                "abstract": "This keynote lecture explores the approaches to implement intelligent behaviors by biological organisms, silicon automata, and computing systems. Autonomous computing is introduced as the latest and advanced computing techniques built upon routine, algorithmic, ...",
                "id": "01134"
            },
            {
                "title": "A Formal Knowledge Representation System FKRS for the Intelligent Knowledge Base of a Cognitive Learning Engine",
                "abstract": "It is recognized that the generic form of machine learning is a knowledge acquisition and manipulation process mimicking the brain. Therefore, knowledge representation as a dynamic concept network is centric in the design and implementation of the intelligent knowledge base of a Cognitive Learning Engine CLE. This paper presents a Formal Knowledge Representation System FKRS for autonomous concept formation and manipulation based on concept algebra. The Object-Attribute-Relation OAR model for knowledge representation is adopted in the design of FKRS. The conceptual model, architectural model, and behavioral models of the FKRS system is formally designed and specified in Real-Time Process Algebra RTPA. The FKRS system is implemented in Java as a core component towards the development of the CLE and other knowledge-based systems in cognitive computing and computational intelligence.",
                "id": "01135"
            },
            {
                "title": "A semantic algebra for cognitive linguistics and cognitive computing",
                "abstract": "Semantics is the meaning of a language unit at the levels of word, phrase, sentence, paragraph, and essay. Cognitive linguistics focuses on cognitive semantics of sentences and its interaction with syntactic structures. A denotational mathematical framework of language semantics known as semantic algebra is developed in this paper. Semantic algebra reveals the nature of semantics by a general mathematical model. On the basis of the formal semantic structure, language semantics can be deductively manipulated by a set of algebraic operations at different levels of language units. According to semantic algebra, semantic interpretation and comprehension can be embodied as a process of formal semantic aggregation in cognitive linguistics from the bottom up. Applications of semantic algebra are illustrated in computational linguistics, computing with words, cognitive informatics, and cognitive computing.",
                "id": "01136"
            },
            {
                "title": "The Formal Design Model of an Automatic Teller Machine ATM",
                "abstract": "An Automated Teller Machine ATM is a safety-critical and real-time system that is highly complicated in design and implementation. This article presents the formal design, specification, and modeling of the ATM system using a denotational mathematics known as Real-Time Process Algebra RTPA. The conceptual model of the ATM system is introduced as the initial requirements for the system. The architectural model of the ATM system is created using RTPA architectural modeling methodologies and refined by a set of Unified Data Models UDMs, which share a generic mathematical model of tuples. The static behaviors of the ATM system are specified and refined by a set of Unified Process Models UPMs for the ATM transition processing and system supporting processes. The dynamic behaviors of the ATM system are specified and refined by process priority allocation, process deployment, and process dispatch models. Based on the formal design models of the ATM system, code can be automatically generated using the RTPA Code Generator RTPA-CG, or be seamlessly transformed into programs by programmers. The formal models of ATM may not only serve as a formal design paradigm of real-time software systems, but also a test bench for the expressive power and modeling capability of exiting formal methods in software engineering.",
                "id": "01137"
            },
            {
                "title": "Kinect Sensor Gesture And Activity Recognition New Applications For Consumer Cognitive Systems",
                "abstract": "Cognitive consumer electronics (CE), the fastest-growing sector worldwide that is driven by machine intelligence and cognitive systems, is triggered and enabled by audio- and video-capturing devices, smart sensors, health- and fitness-monitoring devices, security and education electronics, and intelligent systems. Smart consumer sensors and cognitive systems are synergized through the Internet of Things (IoT) for optimal information sharing, communication, real-time updates, data analytics, and enhanced support for decision making. Biometric-based devices, originally intended for large-scale applications in airports, border controls, disaster zones, or refugee migration zones, are enabling a wide range of applications in commercial and consumer sectors as standalone systems or with interconnected sensor networks. This article introduces the applications of Microsoft Kinect in cognitive systems for smart CE, and, using Kinect sensors, a human-behavior cognition technology is presented for gesture and activity recognition. As a novel front end of pervasive cognitive systems, the challenges and applications of a Kinect sensor-based system will be explored in CE, such as smart automobiles, health care, surveillance, and activity recognition.",
                "id": "01138"
            },
            {
                "title": "Inference Algebra IA: A Denotational Mathematics for Cognitive Computing and Machine Reasoning II",
                "abstract": "Inference as the basic mechanism of thought is abilities gifted to human beings, which is a cognitive process that creates rational causations between a pair of cause and effect based on empirical arguments, formal reasoning, and/or statistical norms. It's recognized that a coherent theory and mathematical means are needed for dealing with formal causal inferences. Presented is a novel denotational mathematical means for formal inferences known as Inference Algebra IA and structured as a set of algebraic operators on a set of formal causations. The taxonomy and framework of formal causal inferences of IA are explored in three categories: a Logical inferences; b Analytic inferences; and c Hybrid inferences. IA introduces the calculus of discrete causal differential and formal models of causations. IA enables artificial intelligence and computational intelligent systems to mimic human inference abilities by cognitive computing. A wide range of applications of IA are identified and demonstrated in cognitive informatics and computational intelligence towards novel theories and technologies for machine-enabled inferences and reasoning. This work is presented in two parts. The inference operators of IA as well as their extensions and applications will be presented in this paper; while the structure of formal inference, the framework of IA, and the mathematical models of formal causations has been published in the first part of the paper in IJCINI 54.",
                "id": "01139"
            },
            {
                "title": "Toward a Formal Knowledge System Theory and Its Cognitive Informatics Foundations",
                "abstract": "Knowledge science and engineering are an emerging field that studies the nature of human knowledge, and its manipulations such as acquisition, representation, creation, composition, memorization, retrieval, and depository. This paper presents the nature of human knowledge and its mathematical models, internal representations, and formal manipulations. The taxonomy of knowledge and the hierarchical abstraction model of knowledge are investigated. Based on a set of mathematical models of knowledge and the Object-Attribute-Relation (OAR) model for internal knowledge representation, rigorous knowledge manipulations are formally described by concept algebra. A coherent framework of formalized knowledge systems is modeled based on the analyses of roles of formal and empirical knowledge. Then, the theory of knowledge acquisition and the cognitive model of knowledge spaces are systematically developed.",
                "id": "01140"
            },
            {
                "title": "The cognitive processes of abstraction and formal inferences",
                "abstract": "Theoretical research is predominately an inductive process; while applied research is mainly a deductive process. Both inference processes are based on the cognitive process and means of abstraction. This paper describes the cognitive processes of abstraction and formal inferences such as deduction, induction, abduction, and analogy. The hierarchy of abstraction and the descriptivity of abstract means at different levels are analyzed. A set of mathematical models of formal inference methodologies are developed. Formal descriptions of the five cognitive processes of abstraction and inferences are presented using real-time process algebra (RTPA). Applications of abstraction and formal inferences in dealing with complicated problems in large-scale software system development in software engineering are discussed.",
                "id": "01141"
            },
            {
                "title": "Perspectives on Cognitive Computers and Knowledge Processors",
                "abstract": "Cognitive Informatics CI is a contemporary multidisciplinary field spanning across computer science, information science, cognitive science, brain science, intelligence science, knowledge science, cognitive linguistics, and cognitive philosophy. CI aims to investigate the internal information processing mechanisms and processes of the brain, the underlying abstract intelligence theories and denotational mathematics, and their engineering applications in cognitive computing and computational intelligence. This paper reports a set of eleven position statements presented in the plenary panel of IEEE ICCI*CC'13 on Cognitive Computers and Knowledge Processors contributed from invited panelists who are part of the world's renowned researchers and scholars in the field of cognitive informatics and cognitive computing.",
                "id": "01142"
            },
            {
                "title": "A Knowledge Representation Tool for Autonomous Machine Learning Based on Concept Algebra",
                "abstract": "Concept algebra is an abstract mathematical structure for the formal treatment of concepts and their algebraic relations, operations, and associative rules for composing complex concepts, which provides a denotational mathematic means for knowledge system representation and manipulation. This paper presents an implementation of concept algebra by a set of simulations in Java. A visualized knowledge representation tool for concept algebra is developed, which enables machines learn concepts and knowledge autonomously. A set of eight relational operations and nine compositional operations of concept algebra are implemented in the tool to rigorously manipulate knowledge by concept networks. The knowledge representation tool is capable of presenting concepts and knowledge systems in multiple ways in order to simulate and visualize the dynamic concept networks during machine learning based on concept algebra.",
                "id": "01143"
            },
            {
                "title": "Fuzzy inferences methodologies for cognitive informatics and computational intelligence.",
                "abstract": "A fuzzy inference is an extended form of formal inferences that enables symbolic and rigorous evaluation of the degree of a confidential level for a given causality on the basis of fuzzy expressions constructed with fuzzy sets and fuzzy logic operations. Fuzzy Inferences are powerful denotational mathematical means for rigorously dealing with degrees of matters, uncertainties, and vague semantics of linguistic variables, as well as for precisely reasoning the semantics of fuzzy causalities. This paper presents a denotational mathematical framework of a set of mathematical structures of fuzzy inferences encompassing deductive, inductive, abductive, and analogical inferences. Each of the fuzzy inference processes is formally modeled and illustrated with real-world examples and cases of applications. The formalization of fuzzy inferences and methodologies enables machines to mimic complex human reasoning mechanisms in cognitive informatics, soft computing, and computational intelligence.",
                "id": "01144"
            },
            {
                "title": "On Concept Algebra: A Denotational Mathematical Structure For Knowledge And Software Modeling",
                "abstract": "Concepts are the most fundamental unit of cognition that carries certain meanings in expression, thinking, reasoning, and system modeling. In denotational mathematics, a concept is formally modeled as an abstract and dynamic mathematical structure that encapsulates attributes, objects, and relations. The most important property of an abstract concept is its adaptive capability to autonomously interrelate itself to other concepts. This article presents a formal theory for abstract concepts and knowledge manipulation known as \"concept algebra.\" The mathematical models of concepts and knowledge are developed based on the object-attribute-relation (OAR) theory. The formal methodology for manipulating knowledge as a concept network is described. Case studies demonstrate that concept algebra provides a generic and formal knowledge manipulation means, which is capable to deal with complex knowledge and software structures as well as their algebraic operations.",
                "id": "01145"
            },
            {
                "title": "On The Big-R Notation For Describing Iterative And Recursive Behaviors",
                "abstract": "Iterative and recursive control structures are the most fundamental mechanisms of computing that make programming more effective and expressive. However, these constructs are perhaps the most diverse and confusable instructions in programming languages at both syntactic and semantic levels. This article introduces the big-R notation that provides a unifying mathematical treatment of iterations and recursions in computing. Mathematical models of iterations and recursions are developed using logical inductions. Based on the mathematical model of the big-R notation, fundamental properties of iterative and recursive behaviors of software are comparatively analyzed. The big-R notation has been adopted and implemented in Real-Time Process Algebra (RTPA) and its supporting tools. Case studies demonstrate that a convenient notation may dramatically reduce the difficulty and complexity in expressing a frequently used and highly recurring concept and notion in computing and software engineering.",
                "id": "01146"
            },
            {
                "title": "The Real-Time Process Algebra (RTPA)",
                "abstract": "The real-time process algebra (RTPA) is a set of new mathematical notations for formally describing system architectures, and static and dynamic behaviors. It is recognized that the specification of software behaviors is a three-dimensional problem known as: (i) mathematical operations, (ii) event/process timing, and (iii) memory manipulations. Conventional formal methods in software engineering were designed to describe the 1-D (type (i)) or 2-D (types (i) and (iii)) static behaviors of software systems via logic, set and type theories. However, they are inadequate to address the 3-D problems in real-time systems. A new notation system that is capable to describe and specify the 3-D real-time behaviors, the ireal-time process algebra (RTPA), is developed in this paper to meet the fundamental requirements in software engineering.RTPA is designed as a coherent software engineering notation system and a formal engineering method for addressing the 3-D problems in software system specification, refinement, and implementation, particularly for real-time and embedded systems. In this paper, the RTPA meta-processes, algebraic relations, system architectural notations, and a set of fundamental primary and abstract data types are described. On the basis of the RTPA notations, a system specification method and a refinement scheme of RTPA are developed. Then, a case study on a telephone switching system is provided, which demonstrates the expressive power of RTPA on formal specification of both software system architectures and behaviors. RTPA elicits and models 32 algebraic notations, which are the common core of existing formal methods and modern programming languages. The extremely small set of formal notations has been proven sufficient for modeling and specifying real-time systems, their architecture, and static/dynamic behaviors in real-world software engineering environment.",
                "id": "01147"
            },
            {
                "title": "On a Novel Cognitive Knowledge Base CKB for Cognitive Robots and Machine Learning",
                "abstract": "cognitive knowledge base CKB is a novel structure of intelligent knowledge base that represents and manipulates knowledge as a dynamic concept network mimicking human knowledge processing. The essence of CKB is the denotational mathematical model of formal concept that is dynamically associated to other concepts in a CKB beyond conventional rule-based or ontology-based knowledge bases. This paper presents a formal CKB and autonomous knowledge manipulation system based on recent advances in neuroinformatics, concept algebra, semantic algebra, and cognitive computing. An item knowledge in CKB is represented by a formal concept, while the entire knowledge base is embodied by a dynamic concept network. The CKB system is manipulated by algorithms of knowledge acquisition and retrieval on the basis of concept algebra. CKB serves as a kernel of cognitive learning engines for cognitive robots and machine learning systems. CKB plays a central role not only in explaining the mechanisms of human knowledge acquisition and learning, but also in the development of cognitive robots, cognitive learning engines, and knowledge-based systems.",
                "id": "01148"
            },
            {
                "title": "A Sociopsychological Perspective on Collective Intelligence in Metaheuristic Computing",
                "abstract": "In studies of genetic algorithms, evolutionary computing, and ant colony mechanisms, it is recognized that the higher-order forms of collective intelligence play an important role in metaheuristic computing and computational intelligence. Collective intelligence is an integration of collective behaviors of individuals in social groups or collective functions of components in computational intelligent systems. This paper presents the properties of collective intelligence and their applications in metaheuristic computing. A social psychological perspective on collected intelligence is elaborated toward the studies on the structure, organization, operation, and development of collective intelligence. The collective behaviors underpinning collective intelligence in groups and societies are analyzed via the fundamental phenomenon of the basic human needs. A key question on how collective intelligence is constrained by social environment and group settings is explained by a formal motivation/attitude-driven behavioral model. Then, a metaheuristic computational model for a generic cognitive process of human problem solving is developed. This work helps to explain the cognitive and collective intelligent foundations of metaheuristic computing and its engineering applications.",
                "id": "01149"
            },
            {
                "title": "On built-in test reuse in object-oriented framework design",
                "abstract": "Object-oriented frameworks have extended reusability of software from code modules to architectural and domain information. This paper further extends software reusability from code and architecture to built-in tests (BITs) in object-orient ed framework development. Methods for embedding BITs at object and object-oriented framework levels are addressed. Behaviours of objects and object-oriented frameworks with BITs in the normal and test modes are analyzed. Systematic reuse methods of BITs in object-oriented framework development are provided. The most interesting development in the paper is that the BITs in object-oriented frameworks can be inherited and reused as that of code. Therefore testability and maintainability of the test-built-in object-oriented frameworks can be improved by the BIT approach. The BIT method can be used in analysis, design and coding of object-oriented frameworks.",
                "id": "01150"
            },
            {
                "title": "Editors' introduction: Comparative software engineering: Review and perspectives",
                "abstract": "Engineering is a set of disciplines seeking solutions for complicated problems and systems that could not be done by individuals. The aim of engineering is to repetitively produce complicated artefacts in an efficient way. This paper describes a set of generic engineering principles and an engineering maturity model. With the engineering principles and model, the nature and status of software engineering are analysed. Interesting findings on what software engineering can learn from generic engineering principles are presented. This paper intends to show the nature, status and problems of software engineering, as well as its future trends, based on the comparative studies between the generic engineering principles and software engineering practices.",
                "id": "01151"
            },
            {
                "title": "Formalization of UML Models by RTPA",
                "abstract": "Real-time process algebra (RTPA) is a set of mathematical notations for rigorous system specification. The RTPA notation has a structure comprising of operands, primitive types, abstract data types, control logic, and relationships. It is capable to effectively capture a system design in terms of its architecture, static behaviors, and dynamic behaviors. However, the preferred approach to codify system definition is through visualization in the form of UML. Although UML has expressive graphical constructs that is easily understood, it is generally viewed as being informal. This paper proposes an automatic transformation between UML and RTPA. A transformation template is in the form of an UML profile to be used in the design of a system, which is an extension mechanism that allows specialization of the UML for a particular domain. The approach is based upon understanding RTPA and UML constructs and proceeds to identify, characterize, and rank semantic relationships in order to construct an optimal translation. The semantic relationship refers to the distance between RTPA and UML constructs and uses a linguistic distance measure that is reliable and sufficient for determining correspondence. Ultimately, the transformation template becomes a schema to transform UML system models into RTPA notation",
                "id": "01152"
            },
            {
                "title": "The Cognitive Informatics Theory And Mathematical Models Of Visual Information Processing In The Brain",
                "abstract": "It is recognized that the internal mechanisms for visual information processing are based on semantic inferences where visual information is represented and processed as visual semantic objects rather than direct images or episode pictures in the long-term memory. This article presents a cognitive informatics theory of visual information and knowledge processing in the brain. A set of cognitive principles of visual perception is reviewed particularly the classic gestalt principles, the cognitive informatics principles, and the hypercolumn theory. A visual frame theory is developed to explain the visual information processing mechanisms of human vision, where the size of a unit visual frame is tested and calibrated based on vision experiments. The framework of human visual information processing is established in order to elaborate mechanisms of visual information processing and the compatibility of internal representations between visual and abstract information and knowledge in the brain. [Article copies are available for purchase from InfoSci-on-Demand.com]",
                "id": "01153"
            },
            {
                "title": "A Unified Mathematical Model of Programs.",
                "abstract": "Despite the rich depository of empirical knowledge on programming and software engineering, the theoretical model of programs is still unknown. This paper presents an embedded relational model (ERM) for describing the nature of programs. ERM provides a unified mathematical treatment of programs, which reveals that a program is a large and finite set of embedded binary relations between a given current statement and all previous ones that formed the semantic context or environment of computing. According to the ERM model, a program is a composed listing and a logical combination of multiple statements according to certain composing rules. A set of 17 meta statements and a set of 17 compositional relations in computing are elicited in real-time process algebra (RTPA). Based on the ERM model, a set of mathematical laws of programming is formally established.",
                "id": "01154"
            },
            {
                "title": "Empirical Studies on the Functional Complexity of Software in Large-Scale Software Systems",
                "abstract": "Functional complexity is one of the most fundamental properties of software because almost all other software attributes and properties such as functional size, development effort, costs, quality, and project duration are highly dependent on it. The functional complexity of software is a macro-scope problem concerning the semantic properties of software and human cognitive complexity towards a given software system; while the computational complexity is a micro-scope problem concerning algorithmic analyses towards machine throughput and time/space efficiency. This paper presents an empirical study on the functional complexity of software known as cognitive complexity based on large-scale samples using a Software Cognitive Complexity Analysis Tool SCCAT. Empirical data are obtained with SCCAT on 7,531 programs and five formally specified software systems. The theoretical foundation of software functional complexity is introduced and the metric of software cognitive complexity is formally modeled. The functional complexities of a large-scale software system and the air traffic control systems ATCS are rigorously analyzed. A novel approach to represent software functional complexities and their distributions in software systems is developed. The nature of functional complexity of software in software engineering is rigorously explained. The relationship between the symbolic and functional complexities of software is quantitatively analyzed.",
                "id": "01155"
            },
            {
                "title": "The Cognitive Mechanisms and Formal Models of Consciousness",
                "abstract": "Consciousness is the sense of self and the sign of life in natural intelligence. One of the profound myths in cognitive informatics, psychology, brain science, and computational intelligence is how consciousness is generated by physiological organs and neural networks in the bran. This paper presents a formal model and a cognitive process of consciousness in order to explain how abstract consciousness is generated and what its cognitive mechanisms are. The hierarchical levels of consciousness are explored from the facets of neurology, physiology, and computational intelligence. A rigorous mathematical model of consciousness is created that elaborates the nature of consciousness. The cognitive process of consciousness is formally described using denotational mathematics. It is recognized that consciousness is a set of real-time mental information about bodily and emotional status of an individual stored in the cerebellums known as the Conscious Status Memory CSM and is processed/interpreted by the thalamus. The abstract intelligence model of consciousness can be applied in cognitive informatics, cognitive computing, and computational intelligence toward the mimicry and simulation of human perception and awareness of the internal states, external environment, and their interactions in reflexive, perceptive, cognitive, and instructive intelligence.",
                "id": "01156"
            },
            {
                "title": "Preface: Cognitive Informatics, Cognitive Computing, and Their Denotational Mathematical Foundations (I)",
                "abstract": "Cognitive Informatics is a cutting-edge and multidisciplinary research area that tackles the fundamental problems shared by modern informatics, computing, software engineering, AI, cybernetics, cognitive science, neuropsychology, medical science, systems science, philosophy, linguistics, economics, management science, and life sciences. This editorial introduces the emerging field of cognitive informatics and its applications in cognitive computing, abstract intelligence, computational mathematics, and computational intelligence. The themes and structure of this special issue on cognitive informatics are described, and then, focuses of the selected papers in this special issue are highlighted.",
                "id": "01157"
            },
            {
                "title": "Formal Models and Cognitive Mechanisms of the Human Sensory System.",
                "abstract": "The human sensory system is a perfect natural real-time distributed system. It transforms physical and chemical stimuli of the external environment into electronic neural signals by specialized sensory receptors. This paper presents a comprehensive framework of the human sensory system as well as its cognitive and theoretical foundations. A set of primary and perceptual sensory and neural receptors is formally modeled and analyzed. Sensory neural interfaces and interactions to the central and peripheral nervous systems of the brain and associated memories are systematically described. This work is a part of a strategic project towards the development of cognitive computers and cognitive robots.",
                "id": "01158"
            },
            {
                "title": "Simulation and Visualization of Concept Algebra in MATLAB",
                "abstract": "Concept algebra (CA) is a denotational mathematics for formal knowledge manipulation and natural language processing. In order to explicitly demonstrate the mathematical models of formal concepts and their algebraic operations in CA, a simulation and visualization software is developed in the MATLAB environment known as the Visual Simulator of Concept Algebra (VSCA). This paper presents the design and implementation of VSCA and the theories underpinning its development. Visual simulations for the sets of reproductive and compositional operations of CA are demonstrated by real-world examples throughout the elaborations of CA and VSCA.",
                "id": "01159"
            },
            {
                "title": "A Web Knowledge Discovery Engine Based on Concept Algebra",
                "abstract": "On-line knowledge discovery is an important area of knowledge engineering. This paper develops a visualized concept network explorer and a semantic analyzer to locate, capture, and refine queries based on concept algebra. A graphical interface is built using concept and semantic models to refine users' query structures. This tool kit can generate a structured XML query package that accurately express users' information needs for on-line searching and knowledge acquisition.",
                "id": "01160"
            },
            {
                "title": "The Cognitive Process Of Decision Making",
                "abstract": "Decision making is one of the basic cognitive processes of human behaviors by which a preferred option or a course of actions is chosen from among a set of alternatives based on certain criteria. Decision theories are widely applied in many disciplines encompassing cognitive informatics, computer science, management science, economics, sociology, psychology, political science, and statistics. A number of decision strategies have been proposed from different angles and application domains such as the maximum expected utility and Bayesian method. However, there is still a lack of a fundamental and mathematical decision model and a rigorous cognitive process for decision making. This article presents a fundamental cognitive decision making process and its mathematical model, which is described as a sequence of Cartesian-product based selections. A rigorous description of the decision process in real-time process algebra (RTPA) is provided. Real-world decisions are perceived as a repetitive application of the fundamental cognitive process. The result shows that all categories of decision strategies fit in the formally described decision process. The cognitive process of decision making may be applied in a wide range of decision-based systems such as cognitive informatics, software agent systems, expert systems, and decision support systems.",
                "id": "01161"
            },
            {
                "title": "Psychological experiments on the cognitive complexities of fundamental control structures of software systems",
                "abstract": "The measurement of cognitive complexity and functional size of software systems are an age-long problem in software engineering. Although, the symbolic complexity of software may be measured in lines of code, the functional complexity of software is too abstract to be measured or even estimated. Because numerous attributes of software systems are highly dependent on the understanding and measurability of software functional complicity, it has to be formally treated and empirically studied based on cognitive informatics and theoretical software engineering methodologies. This talk reveals that the cognitive functional size (CFS) of software is a product of its architectural and operational complexities based on the studies in cognitive informatics and abstract system theories. The fundamental basic control structures (BCSs) are elicited from software architectural and behavioral specifications and descriptions. The cognitive weights of those BCSs are derived and calibrated via a series of psychological experiments. Based on this work, CFS of a software system may be rigorously measured and analyzed by the unit of function-object (FO).",
                "id": "01162"
            },
            {
                "title": "Cognitive Informatics and Cognitive Computing in Year 10 and Beyond",
                "abstract": "Cognitive Informatics CI is a transdisciplinary enquiry of computer science, information sciences, cognitive science, and intelligence science that investigates into the internal information processing mechanisms and processes of the brain and natural intelligence, as well as their engineering applications in cognitive computing. The latest advances in CI leads to the establishment of cognitive computing theories and methodologies, as well as the development of Cognitive Computers CogC that perceive, infer, and learn. This paper reports a set of nine position statements presented in the plenary panel of IEEE ICCI*CC'11 on Cognitive Informatics in Year 10 and Beyond contributed from invited panelists who are part of the world's renowned researchers and scholars in the field of cognitive informatics and cognitive computing.",
                "id": "01163"
            },
            {
                "title": "Process-Based Software Engineering: Building the Infrastructures",
                "abstract": "A recent trend in software engineering is the shift from a focus on laboratory-oriented software engineering to a more industry-oriented view of software engineering processes. This complements preceding ideas about software engineering in terms of organization and process-orientation. From the domain coverage point of view, many of the existing software engineering approaches have mainly concentrated on the technical aspects of software development. Important areas of software engineering, such as the technical and organizational infrastructures, have been left untouched. As software systems increase in scales, issues of complexity and professional practices become involved. Software development as an academic or laboratory activity, has to engage with software development as a key industrialized process.This expanded domain of software engineering exposes the limitations of existing methodologies that often address only individual subdomains. There is, therefore, a demand for an overarching approach that provide a basis for theoretical and practical infrastructures capable of accommodating the whole range of modern software engineering practices and requirements. One approach is provided by Process-Based Software Engineering (PBSE); part of the more general trend towards a focus on process.This paper provides a review of process techniques for software engineering and a high-level perspective on PBSE. Typical approaches and techniques for the establishment, assessment, improvement and benchmarking of software engineering process systems are introduced in this paper, and many are developed further in other contributions to this volume.",
                "id": "01164"
            },
            {
                "title": "On Autonomous Computing and Cognitive Processes",
                "abstract": "This keynote lecture explores the approaches to implement intelligent behaviors by biological organisms, silicon automata, and computing systems. Autonomous computing is introduced as the latest and advanced computing techniques built upon routine, algorithmic, and adaptive systems. The theory and philosophy behind autonomous computing is cognitive informatics. In other words, autonomous computing systems are applications of cognitive informatics. A Layered Reference Model of the Brain (LRMB) and its cognitive mechanisms and processes are described in this talk, which form the foundation for designing and implementing autonomous computing systems. Real-Time Process Algebra (RTPA) is introduced to formally and rigorously describe autonomous computing systems and cognitive behaviors. It is believed that applications of cognitive informatics and autonomous computing will result in the development of new generation computing architectures and novel information processing systems.",
                "id": "01165"
            },
            {
                "title": "A Formal Syntax of Natural Languages and the Deductive Grammar",
                "abstract": "This paper presents a formal syntax framework of natural languages for computational linguistics. The abstract syntax of natural languages, particularly English, and their formal manipulations are described. On the basis of the abstract syntax, a universal language processing model and the deductive grammar of English are developed toward the formalization of Chomsky's universal grammar in linguistics. Comparative analyses of natural and programming languages, as well as the linguistic perception on software engineering, are discussed. A wide range of applications of the deductive grammar of English have been explored in language acquisition, comprehension, generation, and processing in cognitive informatics, computational intelligence, and cognitive computing.",
                "id": "01166"
            },
            {
                "title": "Formal Description of the Cognitive Process of Memorization",
                "abstract": "Memorization is a key cognitive process of the brain because almost all human intelligence is functioning based on it. This paper presents a neuroinformatics theory of memory and a cognitive process of memorization. Cognitive informatics foundations and functional models of memory and memorization are explored toward a rigorous explanation of memorization. The cognitive process of memorization is studied that reveals how and when memory is created in long-term memory. On the basis of the formal memory and memorization models, the cognitive process of memorization is rigorously described using Real-Time Process Algebra (RTPA). This work is one of the fundamental enquiries on the mechanisms of the brain and natural intelligence according to the Layered Reference Model of the Brain (LRMB) developed in cognitive informatics.",
                "id": "01167"
            },
            {
                "title": "An Image Retrieval Method Using Modified LBG Algorithm",
                "abstract": "In this paper, the investigation on using the proposed modified LBG algorithm for the image retrieval system is presented. The proposed algorithm transforms an image into multi-layer bitstream, the base layer and enhancement layer bitstream. Then the multi-layer bitstream is coded with modified LBG algorithm using Partial Search Partial Distortion for coding the wavelet coefficients to speed up the codebook generation. The experimental results show that the proposed algorithm outperforms JPEG algorithm, in compression efficiency. On the other hand, the base layer and enhancement layer bitstream, it provides superior benefits to the retrieval system compared to other compression methods. Moreover, the enhancement layer bitstream also allows fast transmission that enhances the performance of retrieval systems.",
                "id": "01168"
            },
            {
                "title": "On Cognitive Models of Causal Inferences and Causation Networks",
                "abstract": "Human thought, perception, reasoning, and problem solving are highly dependent on causal inferences. This paper presents a set of cognitive models for causation analyses and causal inferences. The taxonomy and mathematical models of causations are created. The framework and properties of causal inferences are elaborated. Methodologies for uncertain causal inferences are discussed. The theoretical foundation of humor and jokes as false causality is revealed. The formalization of causal inference methodologies enables machines to mimic complex human reasoning mechanisms in cognitive informatics, cognitive computing, and computational intelligence.",
                "id": "01169"
            },
            {
                "title": "Specification of the RTPA grammar and its recognition",
                "abstract": "A new type of descriptive mathematics, real-time process algebra (RTPA), is developed for describing complicated behaviors of human beings and software systems. This paper describes the key syntax of RTPA defined by a set of 280 LL(k) grammar rules and their recognition techniques. The LL(k) grammar of RTPA is formally described by using the EBNF notations. The design of the RTPA parser and type checker are presented on the basis of the RTPA grammar. To deal with the special non-LL(k) grammar rules of RTPA, the ANTLR syntactic predicates are used to create guarded and extended rules. The tasks of type checking in RTPA recognition can be classified into three categories: (a) identifier type compliancy, (b) expression type compliancy, and (c) process constraint consistency. The implementation of the RTPA parser and type checker provides an important tool for RTPA recognition and for generating executable code for formal specifications of system architectures and behaviors in RTPA.",
                "id": "01170"
            },
            {
                "title": "Perspectives on cognitive computing and applications: Summary of plenary Panel I of IEEE ICCI'10.",
                "abstract": "Cognitive Computing (CC) is an emerging paradigm of intelligent computing theories and technologies based on cognitive informatics that implements computational intelligence by autonomous inferences and perceptions mimicking the mechanisms of the brain. The development of Cognitive Computers (cC) is centric in cognitive computing methodologies. A cC is an intelligent computer for knowledge processing as that of a conventional von Neumann computer for data processing. This paper summarizes the presentations of a set of 9 position papers presented in the ICCI'10 Panel on Cognitive Computing and Applications contributed from invited panelists who are part of the world's renowned researchers and scholars in the field of cognitive informatics and cognitive computing. \u00a9 2010 IEEE.",
                "id": "01171"
            },
            {
                "title": "Sentence Comprehension and Semantic Syntheses by Cognitive Machine Learning",
                "abstract": "Recent development in machine learning and computational linguistics has enabled cognitive machines to understand the semantics of human expressions. A system for sentence syntactic analysis and semantic synthesis is developed based on denotational mathematics. Machine sentence learning and comprehension are reduced to the building of a composed concept that maps the semantics of the subject onto the counterpart of object(s) represented by formal concepts and phrases. A set of semantic operations such as concept composition, modification, generalization, specification, extension and reduction is formally specified based on concept algebra and semantic algebra for machine learning. An Algorithm for Unsupervised Sentence Learning (AUSL) is designed and implemented, which expresses a learnt sentence as a knowledge graph related to the semantic hierarchy of the machine's knowledge base. Experimental results demonstrate the autonomous learning algorithm and case studies on machine learning towards applications in cognitive robots and knowledge learning systems.",
                "id": "01172"
            },
            {
                "title": "The Formal Design Model of Doubly-Linked-Circular Lists DLC-Lists",
                "abstract": "Abstract Data Types ADTs are a set of highly generic and rigorously modeled data structures in type theory. Lists as a finite sequence of elements are one of the most fundamental and widely used ADTs in system modeling, which provide a standard encapsulation and access interface for manipulating large-volume information and persistent data. This paper develops a comprehensive design pattern of formal lists using a doubly-linked-circular DLC list architecture. A rigorous denotational mathematics, Real-Time Process Algebra RTPA, is adopted, which allows both architectural and behavioral models of lists to be rigorously designed and implemented in a top-down approach. The architectural models of DLC-Lists are created using RTPA architectural modeling methodologies known as the Unified Data Models UDMs. The behavioral models of DLC-Lists are specified and refined by a set of Unified Process Models UPMs in three categories namely the management operations, traversal operations, and node I/O operations. This work has been applied in a number of real-time and nonreal-time system designs such as a real-time operating system RTOS+, a file management system FMS, and the ADT library for an RTPA-based automatic code generation tool.",
                "id": "01173"
            },
            {
                "title": "Cognitive informatics and contemporary mathematics for knowledge manipulation",
                "abstract": "Although there are various ways to express entities, notions, relations, actions, and behaviors in natural languages, it is found in Cognitive Informatics (CI) that human and system behaviors may be classified into three basic categories known as to be, to have, and to do. All mathematical means and forms, in general, are an abstract and formal description of these three categories of system behaviors and their common rules. Taking this view, mathematical logic may be perceived as the abstract means for describing \u2018to be,' set theory for describing \u2018to have,' and algebras, particularly the process algebra, for describing \u2018to do.' This paper presents the latest development in a new transdisciplinary field known as CI. Three types of new mathematical structures, Concept Algebra (CA), System Algebra (SA), and Real-Time Process Algebra (RTPA), are created to enable rigorous treatment of knowledge representation and manipulation in terms of to be / to have / to do in a formal and coherent framework. A wide range of applications of the three knowledge algebras in the framework of CI has been identified in knowledge and software engineering",
                "id": "01174"
            },
            {
                "title": "On coping with real-time software dynamic inconsistency by built-in tests",
                "abstract": "In real&dash;time systems, dynamic inconsistencies of software are hardly detected, diagnosed and handled. A built&dash;in test (BIT) method is developed to cope with software dynamic inconsistency. BIT is defined as a new kind of software testing which is explicitly described in object&dash;oriented source code as member functions. BITs can be activated at any designed moment at run&dash;time to detect, diagnose and handle software dynamic inconsistencies. This paper develops a new approach to cope with software dynamic inconsistencies at run&dash;time by BITs. In this paper, the concept of BITs is introduced. The standard structures which incorporate BITs into conventional object&dash;oriented software are analysed. Reuse methodologies for BITs in OO software are developed at object and system levels. A case study is provided for showing how to create BIT and how to inherit and reuse BITs in OO programming. Methods for incorporating BITs into OO software at object, class and system levels are provided. An approach to dynamic inconsistency control by BITs is developed.",
                "id": "01175"
            },
            {
                "title": "On the Incremental Union of Relations: A Key Property of General Systems Explained",
                "abstract": "AbstractRelations are one of the most important conceptual models and mathematical entities in logic, discrete mathematics, computer science, software science, system science, and formal semantics. However, some fundamental and indispensable operations on formal relations were overlooked in traditional studies. This paper presents an extended relation theory with a set of novel algebraic operators on relations beyond classic operations. The algebraic operators on formal relations known as the incremental union and decremental disunion are formally elaborated. The property of relational gains is mathematically modeled, which explains the dynamic mechanism of relations generated by associations of static sets of objects in physical or abstract systems.",
                "id": "01176"
            },
            {
                "title": "The Theoretical Framework and Cognitive Process of Learning",
                "abstract": "Learning is a fundamental cognitive process of human intelligence. According to cognitive informatics, learning as a collective term can be classified into the categories of transitive, objective, and complex learning. This paper presents a theoretical framework of learning and explains its cognitive processes. The neural informatics foundations of learning, particularly the Hierarchical Neural Cluster (HNC) model and the Object-Attribute-Relation (OAR) model, are explored. The taxonomy and theory of learning are described based on Concept Algebra. The mathematical models of learning are systematically established for the categories of the transitive, objective, and complex learning. On the basis of the fundamental theories of leaming, the cognitive processes of learning are formally described using Real-Time Process Algebra (RTPA). The theoretical framework established in this work can be applied to both human and machine learning systems.",
                "id": "01177"
            },
            {
                "title": "A Novel Machine Learning Algorithm for Cognitive Concept Elicitation by Cognitive Robots",
                "abstract": "AbstractCognitive knowledge learning CKL is a fundamental methodology for cognitive robots and machine learning. Traditional technologies for machine learning deal with object identification, cluster classification, pattern recognition, functional regression and behavior acquisition. A new category of CKL is presented in this paper embodied by the Algorithm of Cognitive Concept Elicitation ACCE. Formal concepts are autonomously generated based on collective intension attributes and extension objects elicited from informal descriptions in dictionaries. A system of formal concept generation by cognitive robots is implemented based on the ACCE algorithm. Experiments on machine learning for knowledge acquisition reveal that a cognitive robot is able to learn synergized concepts in human knowledge in order to build its own knowledge base. The machine-generated knowledge base demonstrates that the ACCE algorithm can outperform human knowledge expressions in terms of relevance, accuracy, quantification and cohesiveness.",
                "id": "01178"
            },
            {
                "title": "On Abstract Systems and System Algebra.",
                "abstract": "Systems are the most complicated entities and phenomena in the physical, information, and social worlds across all science and engineering disciplines. This paper presents a mathematical theory of system algebra and its applications in cognitive informatics, system engineering, and software engineering. A rigorous treatment of abstract systems is described, and the algebraic relations and operations of abstract systems are analyzed. Important properties of systems such as system mutation, work done by systems, the maximum output of systems, system equilibriums, system synchronization, and system dissimilation, are formally modeled. An age-long myth in system theory that states 'the whole is larger than the sum of its parts' is formally explained. On the basis of the abstract system theory, a wide range of real world phenomena and problems can be explained and solved.",
                "id": "01179"
            },
            {
                "title": "Formal Description of the Cognitive Process of Problem Solving",
                "abstract": "One of the fundamental human cognitive processes is problem solving. Most of the decisions we make relate to some kind of problems we try to solve no matter how trivial and critical the problem may be. The problem solving process entails performing in a new situation with information acquired and knowledge learned from past situations. As a higher level cognitive process, problem solving involves the correlation process effort to connect newly encounter problem object(s) with the objectattribute- relation (OAR) model representation of knowledge in the brain. The goal of problem solving is to search along various solution paths within the problem solver\u00fds knowledge base in the memory. When a problem object is identified, problem solving can be perceived as a search process in the memory space for finding a relationship between a set of problem-solving goals and a set of alternative paths. This paper presents a mathematical and cognitive model that describes problem solving as a cognitive process. The cognitive structures of the brain and the mechanisms of internal knowledge representation behind the cognitive process of problem solving are explained. The cognitive process will then be formally and rigorously described using Real-Time Process Algebra (RTPA) base on the aforementioned models. Extended discussions are presented on applications of the cognitive process model of problem solving in software engineering and psychology.",
                "id": "01180"
            },
            {
                "title": "Cognitive Computing And Machinable Thought",
                "abstract": "Cognitive Computing (CC) is an emerging paradigm of intelligent computing methodologies and systems that implements computational intelligence by autonomous inferences and perceptions mimicking the mechanisms of the brain [ 1, 3, 4, 5, 6, 12, 13, 15, 16, 18, 20, 22, 23]. CC is emerged and developed based on the transdisciplinary research in cognitive informatics and abstract intelligence. Cognitive Informatics (CI) is a transdisciplinary enquiry of computer science, information science, cognitive science, and intelligence science that investigates into the internal information processing mechanisms and processes of the brain and natural intelligence, as well as their engineering applications [1, 3, 6, 12, 13, 20, 22]. The theoretical framework of cognitive informatics [6] covers the Information-Matter-Energy (IME) model [5], the Layered Reference Model of the Brain (LRMB) [ 19], the Object-Attribute-Relation (OAR) model of information representation in the brain [7], the cognitive informatics model of the brain [ 17), Natural Intelligence (NI) [6], and neuroinformatics [6]. Recent studies on LRMB in cognitive informatics reveal an entire set of cognitive functions of the brain and their cognitive process models, which explain the functional mechanisms and cognitive processes of the natural intelligence with 43 cognitive processes at seven layers known as the sensation, memory, perception, action, meta-cognitive, meta-inference, and higher cognitive layers from the bottom up [19].Abstract Intelligence (alpha I) is the universal mathematical form of intelligence that transfers information into knowledge and behaviors [12]. The studies on alpha I form a human enquiry of both natural and artificial intelligence at the reductive levels of neural, cognitive, functional, and logical forms. The paradigms of alpha I are such as natural, artificial, machinable, and computational intelligence. The studies in C1 and alpha I lay a theoretical foundation toward revealing the basic mechanisms of different forms of intelligence. As a result, cognitive computers may be developed, which are characterized as knowledge processors beyond those of data processors in conventional computing.Denotational Mathematics (DM) is a category of expressive mathematical structures that deals with high-level mathematical entities beyond numbers and sets, such as abstract objects, complex relations, perceptual information, abstract concepts, knowledge, intelligent behaviors, behavioral processes, and systems [8]. It is recognized that the maturity of a scientific discipline is characterized by the maturity of its mathematical (meta-methodological) means. The paradigms of DM are such as concept algebra [9], system algebra [10], real-time process algebra [2, 11], granular algebra [2 1], visual semantic algebra [14], fuzzy quantification/qualification, fuzzy inferences, and fuzzy causality analyses. DM provides a coherent set of contemporary mathematical means and explicit expressive power for Cl, alpha I, CC, AI, and computational intelligence.The latest advances in Cl, alpha I, CC, and DM lead to a systematic solution for future generation intelligent computers known as cognitive computers that think and feel [4, 13], which will enable the simulation of machinable thought such as computational inferences, reasoning, and causality analyses. A wide range of applications of Cl, alpha I, CC, and DM are expected toward the implementation of highly intelligent machinable thought such as formal inference, symbolic reasoning, problem solving, decision making, cognitive knowledge representation, semantic searching, and autonomous learning.",
                "id": "01181"
            },
            {
                "title": "Toward Theoretical Foundations Of Autonomic Computing",
                "abstract": "Autonomic computing (AC) is an intelligent computing approach that autonomously carries out robotic and interactive applications based on goal- and inference-driven mechanisms. This article attempts to explore the theoretical foundations and technical paradigms of AC. It reviews the historical development that leads to the transition from imperative computing to AC. It surveys transdisciplinary theoretical foundations for AC such as those of behaviorism, cognitive informatics, denotational mathematics, and intelligent science. On the basis of this work, a coherent framework toward AC may be established for both interdisciplinary theories and application paradigms, which will result in the development of new generation computing architectures and novel information processing systems.",
                "id": "01182"
            },
            {
                "title": "On Long Lifespan systems.",
                "abstract": "Complex systems are not only characterized by their magnitudes, but also their lifespans. This paper presents a theory of Long-Lifespan systems (LLS's) and its applications in explaining the properties of complex systems from both the function and time dimensions. The mathematical models of LLS's such as those of the abstract systems, system magnitudes, and system lifespan, are established. Then, properties of LLS's, particularly those of system conservation, system equilibrium, and system self-organization, are formally analyzed. The fundamental mechanisms and applications of LLS's in economics are elaborated, followed by the description of the global warning and the greenhouse effect in order to explain the rational causality of global climate system as a typical LLS system.",
                "id": "01183"
            },
            {
                "title": "Towards a Methodology for RTPA-MATLAB Code Generation Based on Machine Learning Rules",
                "abstract": "Autonomous program code generation by machine learning is not only an ultimate goal but also a theoretical challenge to software science and engineering. A methodology and case study for code generation based on Real-Time Process Algebra (RTPA) by machine learning are presented in this paper. It describes a machine learning approach for code generation in MATLAB based on acquired RTPA rules and formal specifications. The design and implementation of the RTPA-MATLAB code generator is introduced, which is implemented by an RTPA parser and an MATLAB code builder. The experimental case studies have demonstrated the novelty of the theories and methodologies for code generation based on machine-learnt programming rules.",
                "id": "01184"
            },
            {
                "title": "A Knowledge Representation Tool Based on Concept Algebra",
                "abstract": "A formal theory for abstract concepts and knowledge manipulation is created by Wang recently known as concept algebra. Concept algebra is an abstract mathematical structure for the formal treatment of concepts and their algebraic relations, operations, and associative rules for composing complex concepts, which provides a denotational mathematic means for knowledge system representation and manipulation. This paper presents a method that shows how the theory of concept algebra is implemented and simulated using typical computers. A visualized knowledge representation tool for concept algebra is developed, which enables machines learn concepts and knowledge autonomously. Nine forms of concept association operations are implemented in the tool to rigorously manipulate knowledge by concept networks. The knowledge representation tool is capable to present concepts and knowledge systems in multiple ways, in order to visualize concept networks by computer simulations.",
                "id": "01185"
            },
            {
                "title": "On Laws Of Work Organization In Human Cooperation",
                "abstract": "From the perspective of cognitive informatics (CI), this paper proposes internal relations between distance and orientation knowledge of extended objects, and presents a formal representation of spatial knowledge. The connection relation is taken as primitive. Notions of near extension regions and the nearer predicate are developed. Distance relations between extended objects are understood as degrees of the near extension from one object to the other. Orientation relations are understood as distance comparison from one object to the sides of the other object. Therefore, distance and orientation relations can be internally related through the connection relation. The notion of the fiat projection is presented to model the mental formation of the deictic orientation reference framework. This article introduces a new axiom to govern the connection relation in the literature and presents examples to show diagrammatically the internal relations between distance and orientation relations of extended objects.",
                "id": "01186"
            },
            {
                "title": "On cognitive mechanism of the eyes: the sensor vs. the browser of the brain",
                "abstract": "In neuropsychology it is commonly recognized that 70% of all the sensory receptors in the brain are inputted from the eyes. However, an important internal cognitive function of the eyes as the perceptual browser of the memory and the mind has not yet been recognized. This paper contrasts the cognitive mechanisms of the eyes as both the sensor of the brain externally and the browser of the mind internally. The key assertion is that the eyes function as a bi-directional organ: a visual sensor of the brain, more important, a perceptual browser of the mind. The sensory of the brain can be categorized into external and internal senses. The former encompass vision, auditory, smell, tactility, and tastes. The latter refers to perceptivity that forms the sixth sense of the brain. The perceptual sense encompasses consciousness, memory searching, motivation, willingness, goal setting, emotion, sense of spatiality, and sense of motion. The reveal of the internal perceptual mechanisms of the eyes is not only theoretically significant to identify the physiological organ of the thinking engine of the brain, but also practically useful to explain a wide range of cognitive mechanisms of he brain and mind.",
                "id": "01187"
            },
            {
                "title": "Software Engineering Process Benchmarking",
                "abstract": "This paper proposes a Software Engineering Process (SEP) benchmarking methodology and a benchmark-gap analysis technique to assist industrial practitioners in quantitative software engineering. This work adopts the comprehensive SEP Assessment model (SEPRM), as a foundation to SEP benchmarking. In this approach, a number of conventional benchmarking challenges may be overcome. Case studies are presented to demonstrate the usage of the benchmarking technologies and supporting tools.",
                "id": "01188"
            },
            {
                "title": "Toward a generic mathematical model of abstract game theories",
                "abstract": "Games are a complex mathematical structure for modeling dynamicdecision processes under competition where opponent players compete for themaximum gain or toward a success state in the same environment according tothe same rules of the game. Games are conventionally dealt with payoff tablesbased on random strategies, which are found inadequate to describe the dynamicbehaviors of games and to rigorously predict the outcomes of games.This paper presents an abstract game theory, which enables a formal treatmentof games by a set of mathematical models for both the layouts and behaviors ofgames. A generic mathematical model of abstract games is introduced, based onwhich the properties of games in terms of decision strategies and serial matchesare described. A wide range of generic zero-sum and nonzero-sum games areformally modeled and analyzed using the generic mathematical models of abstractgames.",
                "id": "01189"
            },
            {
                "title": "From cognitive psychology to cognitive informatics",
                "abstract": "Cognitive informatics is the transdisciplinary study of cognitive and information sciences that investigates into the internal information processing mechanism and processes of the natural intelligence - human brains and minds. This paper presents an extensive literature review on related research in psychology, or cognitive science, informatics, computer science that led to the emergence of cognitive informatics.",
                "id": "01190"
            },
            {
                "title": "A novel fuzzy multimodal information fusion technology for human biometric traits identification",
                "abstract": "In recent years, biometric based security systems achieved more attention due to continuous terrorism threats around the world. However, a security system comprised of a single form of biometric information cannot fulfill users' expectations and may suffer from noisy sensor data, intra and inter class variations and continuous spoof attacks. To overcome some of these problems, multimodal biometric systems with multiple physiological, behavioral, and soft biometric information are becoming more popular due to increased recognition accuracy. In order to take full advantage of the multimodal approaches, one of the main issues is to implement the fusion mechanism for different biometric information. In this research, we utilize the physiological attributes (face, ear and iris) along with soft biometric information (gender, ethnicity and eye color). A fuzzy fusion mechanism for robust and reliable multimodal biometric based security systems is developed. The proposed fuzzy fusion scheme adopts rank, match score and soft biometrics information as the input and produces final identification decision via a fuzzy rule-based inference system. The experimental results show that the fuzzy fusion method can provide us faster, higher and more reliable recognition performance than conventional unimodal methods. The system can be effectively used at any security critical applications.",
                "id": "01191"
            },
            {
                "title": "On Constraints And Count-Measures For Software Engineering",
                "abstract": "Constraints of software engineering are inherent by its intangibility, complexity, and diversity. A comprehensive set of 14 basic constraints of software engineering are identified in this paper, which can be classified into three categories known as the cognitive, organizational, and resource constraints. The relationships between the three categories of basic constraints of software engineering can be described by the Software Engineering Constraint Model (SECM). A set of 31 fundamental principles for software engineering as the key measures for coping with the basic constraints is identified. A mapping between the software engineering constraints and measures is developed, which can be used as a guideline for allocating certain software engineering methodologies for coping with a given problem in a software engineering projects.",
                "id": "01192"
            },
            {
                "title": "Cognitive Informatics in Year 10 and Beyond: summary of the plenary panel.",
                "abstract": "The contemporary wonder of sciences and engineering has recently refocused on the starting point of them: How the brain processes internal and external information autonomously and cognitively rather than imperatively as those of conventional computers do? This leads to the advances in the field of Cognitive Informatics (CI) as a transdisciplinary enquiry of computer science, information sciences, cognitive science, and intelligence science. CI investigates into the internal information processing mechanisms and processes of the brain and natural intelligence, as well as their engineering applications in cognitive computing. This paper reports a set of nine position statements presented in the plenary panel of IEEE ICCI*CC'11 on Cognitive Informatics in Year 10 and Beyond contributed from invited panelists who are part of the world's renowned researchers and scholars in the field of cognitive informatics and cognitive computing.",
                "id": "01193"
            },
            {
                "title": "On contemporary denotational mathematics for computational intelligence",
                "abstract": "Denotational mathematics is a category of expressive mathematicalstructures that deals with high-level mathematical entities beyond numbers andsets, such as abstract objects, complex relations, behavioral information, concepts,knowledge, processes, intelligence, and systems. New forms of mathematicsare sought, collectively known as denotational mathematics, in order todeal with complex mathematical entities emerged in cognitive informatics,computational intelligence, software engineering, and knowledge engineering.The domain and architecture of denotational mathematics are presented in thispaper. Three paradigms of denotational mathematics, known as concept algebra,system algebra, and Real-Time Process Algebra (RTPA), are introduced.Applications of denotational mathematics in cognitive informatics and computationalintelligence are elaborated. A set of case studies is presented on themodeling of iterative and recursive systems architectures and behaviors byRTPA, the modeling of autonomic machine learning by concept algebra, andthe modeling of granular computing by system algebra.",
                "id": "01194"
            },
            {
                "title": "The Cognitive Process of Comprehension: A Formal description",
                "abstract": "Comprehension is an ability to understand the meaning of a concept or an action. Comprehension is an important intelligent power of abstract thought and reasoning of humans or intelligent systems. It is highly curious to explore the internal process of comprehension in the brain and to explain its basic mechanisms in cognitive informatics and computational intelligence. This paper presents a formal model of the cognitive process of comprehension. The mechanism and process of comprehension are systematically explained with its conceptual, mathematical, and process models based on the Layered Reference Model of the Brain LRMB and the Object-Attribute-Relation OAR model for internal knowledge representation. Contemporary denotational mathematics such as concept algebra and Real-Time Process Algebra RTPA are adopted in order to formally describe the comprehension process and its interaction with other cognitive processes of the brain.",
                "id": "01195"
            },
            {
                "title": "On Built-in Test Classes for Object-Oriented and Component-Based Information Systems",
                "abstract": "This paper presents a new architecture of built-in test (BIT) class in object-oriented (OO) software design and implementation. Conventional technologies for OO software test are mainly static, non-reusable, and unavailable at run-time. The BIT class technology enables the development of self-testable, test-reusable, and run-time testable code on the same platform of conventional 00 programming.",
                "id": "01196"
            },
            {
                "title": "A Denotational Semantics Of Real-Time Process Algebra (Rtpa)",
                "abstract": "Real-time process algebra (RIM) is a form of denotational mathematics far dealing with fundamental system behaviors such as timing, internfp', concurrency, and event/time/interrupt-driven system dispatching. Because some key RIPilproce.sses cannot be described adequately in conventional denotational semantic paradigms, a new frameworkfOrmodeling time and processes is sought in order to represent RTP4 in denotational semantics. Within this framework, time is modeled by the elapse ofprocess execution. The process environment encompasses states ofall variables represented as mathematical maps, which project variables to their corresponding values. Duration is introduced as a pair of time intervals and the environment to represent the changes ofthe.process environment during a time interval. Temporal ordered durations and operations on them are used to denote process executions. On the basis of these means, a comprehensive set of denotational semantics for RTPA are systematically developed and formally expressed.",
                "id": "01197"
            },
            {
                "title": "Semantic Manipulations and Formal Ontology for Machine Learning based on Concept Algebra",
                "abstract": "Towards the formalization of ontological methodologies for dynamic machine learning and semantic analyses, a new form of denotational mathematics known as concept algebra is introduced. Concept Algebra CA is a denotational mathematical structure for formal knowledge representation and manipulation in machine learning and cognitive computing. CA provides a rigorous knowledge modeling and processing tool, which extends the informal, static, and application-specific ontological technologies to a formal, dynamic, and general mathematical means. An operational semantics for the calculus of CA is formally elaborated using a set of computational processes in real-time process algebra RTPA. A case study is presented on how machines, cognitive robots, and software agents may mimic the key ability of human beings to autonomously manipulate knowledge in generic learning using CA. This work demonstrates the expressive power and a wide range of applications of CA for both humans and machines in cognitive computing, semantic computing, machine learning, and computational intelligence.",
                "id": "01198"
            },
            {
                "title": "Contemporary cybernetics and its facets of cognitive informatics and computational intelligence.",
                "abstract": "This paper explores the architecture, theoretical foundations, and paradigms of contemporary cybernetics from perspectives of cognitive informatics (CI) and computational intelligence. The modern domain and the hierarchical behavioral model of cybernetics are elaborated at the imperative, autonomic, and cognitive layers. The CI facet of cybernetics is presented, which explains how the brain may be mimicked in cybernetics via CI and neural informatics. The computational intelligence facet is described with a generic intelligence model of cybernetics. The compatibility between natural and cybernetic intelligence is analyzed. A coherent framework of contemporary cybernetics is presented toward the development of transdisciplinary theories and applications in cybernetics, CI, and computational intelligence.",
                "id": "01199"
            },
            {
                "title": "Design and Implementation of an Automatic RTPA Code Generator",
                "abstract": "Real-time process algebra (RTPA) is a mathematics-based notation system for the specification and refinement of realtime and safety-critical systems. This paper presents the work on designing and developing a set of tools that facilitate automatic generation of C++ code from RTPA specifications of system architectures and behaviors. A two-phrase strategy has been employed in the design of the toolkit. In the first phrase, an RTPA specification is lexically and syntactically analyzed and type-checked, which results in a set of abstract syntax trees. In the second phrase, C++ code is generated using the RTPA-to-C++ mapping strategies and patterns when walking down the abstract syntax trees. The RTPA-to-C++ mapping strategies specify both the traditional sequential part of RTPA mapping onto the standard C++ library and the real-time features of RTPA, such as interrupt, concurrency, duration, and event/time-driven dispatch, mapping onto the RTPA run-time library, which provides real-time support and is implemented using real-time kernel techniques. The toolkit implemented includes an RTPA lexer, an RTPA parser, an RTPA type-checker, and an RTPA code-generator. The experimental results show that RTPA specifications can be rigorously checked and corresponding C++ code can be automatically generated from RTPA specifications using the toolkit. The automatically generated code is executable and effective under the support of the standard C++ library and the specially developed RTPA run-time library",
                "id": "011100"
            },
            {
                "title": "The Oar Model Of Neural Informatics For Internal Knowledge Representation In The Brain",
                "abstract": "The cognitive models of information representation are fundamental research areas in cognitive informatics, which attempts to reveal the mechanisms and potential of the brain in learning and knowledge representation. Because memory is the foundation of all forms of natural intelligence, a generic model of memory, particularly the long-term memory, may explain the fundamental mechanism of internal information representation and the forms of learning results. This article presents the Object-Attribute-Relation (OAR) model to formally represent the structures of internal information and knowledge acquired and learned in the brain. The neural informatics model of human memory is introduced with particular focus on the long-term memory. Then, the OAR model that explains the mechanisms of internal knowledge and information representation in the brain is formally described, and the physical and physiological meanings of this model are explained. Based on the OAR model, knowledge structures and learning mechanisms are rigorously explained. Further, the magnitude of human memory capacity is rigorously estimated on the basis of OAR, by which the memory capacity is derived to be in the order of 10(8,432) bits.",
                "id": "011101"
            },
            {
                "title": "Formal Descriptions Of Cognitive Processes Of Perceptions On Spatiality, Time, And Motion",
                "abstract": "Recent researches in both cognitive informatics and computational intelligence are interested in the human perceptual senses of spatiality, time, and motion, which are fundamental cognitive life functions according to the Layered Reference Model of the Brain (LRMB). This paper presents the cognitive process of human perceptual senses on spatiality, time, and motion. The sense of spatiality is investigated into the coordinate system, orientations, and cognitive maps, followed by the development of the mathematical model and the cognitive process of human spatial senses. The sense of time with the biological clocks, cognitive clocks, and their mathematical models are analyzed in order to explain the cognitive process of human time sense. On the basis of the formal models of senses of spatiality and time, the sense of motion is modeled as a complex sense incorporating both of spatiality and time. Then, the cognitive, mathematical, and process models of the sense of motion are rigorously established. This work provides a theoretical framework for the rigorous implementation of the intelligent behaviors of cognitive computers, autonomous agent systems, and robots in cognitive informatics and computational intelligence.",
                "id": "011102"
            },
            {
                "title": "On Abstract Intelligence And Its Denotational Mathematics Foundations",
                "abstract": "Recent researches reveal that various paradigms of intelligence, such as natural, artificial, machinable, and computational intelligence, can be unified at the logical and functional levels known as abstract intelligence. This paper introduces abstract intelligence as a form of driving force that transfers information into knowledge and behaviors. An architectural framework of abstract intelligence and the Generic Abstract Intelligence Mode (GAIM) are formally developed that provide a unified theory for explaining the mechanisms of advanced intelligence. In order to deal with the highly complex and abstract objects in abstract intelligence, denotational mathematics is introduced as a category of expressive mathematical structures for modeling and manipulating high-level mathematical entities beyond numbers and sets, such as abstract objects, complex relations, behavioral information, abstract concepts, knowledge, processes, and systems. Applications of denotational mathematics in abstract intelligence, cognitive informatics, and computational intelligence are elaborated.",
                "id": "011103"
            },
            {
                "title": "Formal Descriptions of a Set of Meta Cognitive Processes of the Brain",
                "abstract": "Artificial Intelligence applications, particularly those involving natural language understanding, are actually less ambitious than they were decades ago. Statistical and machine learning techniques have had great success on tasks that can be treated ...",
                "id": "011104"
            },
            {
                "title": "Big Data Analyses for Collective Opinion Elicitation in Social Networks",
                "abstract": "Big data are extremely large-scaled data in terms of quantity, complexity, semantics, distribution, and processing costs in computer science, cognitive informatics, web-based computing, cloud computing, and computational intelligence. Censuses and elections are a typical paradigm of big data engineering in modern digital democracy and social networks. This paper analyzes the mechanisms of voting systems and collective opinions using big data analysis technologies. A set of numerical and fuzzy models for collective opinion analyses is presented for applications in social networks, online voting, and general elections. A fundamental insight on the collective opinion equilibrium is revealed among electoral distributions and in voting systems. Fuzzy analysis methods for collective opinions are rigorously developed and applied in poll data mining, collective opinion determination, and quantitative electoral data processing.",
                "id": "011105"
            },
            {
                "title": "Towards the Synergy of Cognitive Informatics, Neural Informatics, Brain Informatics, and Cognitive Computing",
                "abstract": "The contemporary wonder of sciences and engineering recently refocused on the starting point: how the brain processes internal and external information autonomously rather than imperatively as those of conventional computers? This paper explores the interplay and synergy of cognitive informatics, neural informatics, abstract intelligence, denotational mathematics, brain informatics, and computational intelligence. A key notion recognized in recent studies in cognitive informatics is that the root and profound objective in natural, abstract, and artificial intelligence, and in cognitive informatics and cognitive computing, is to seek suitable mathematical means for their special needs. A layered reference model of the brain and a set of cognitive processes of the mind are systematically developed towards the exploration of the theoretical framework of cognitive informatics. A wide range of applications of cognitive informatics and denotational mathematics are recognized in the development of highly intelligent systems such as cognitive computers, cognitive knowledge search engines, autonomous learning machines, and cognitive robots.",
                "id": "011106"
            },
            {
                "title": "Cognitive models of the brain",
                "abstract": "The human brain is the most complicated organ in the universe and a new frontier yet to be explored in an interdisciplinary approach. Investigation of the brain is a unique problem that requires recursive mental power to explore the brain using the brain. This paper attempts to develop functional and cognitive models of the brain by using cognitive informatics and formal methodologies. This paper adopts a memory-based approach to explore the brain, and to demonstrate that memory is the foundation for any natural intelligence. Structures of memories are explored and cognitive models of the natural intelligence are proposed. Cognitive mechanisms of the brain, including hypotheses and theories on the thinking engine of the brain, long-term memory establishment, and roles of sleep in long-term memory development, are investigated. The models and theories are applied to explain a number of fundamental physiological and psychological phenomena.",
                "id": "011107"
            },
            {
                "title": "Perspectives On Ebrain And Cognitive Computing",
                "abstract": "Cognitive Informatics (CI) is a discipline spanning across computer science, information science, cognitive science, brain science, intelligence science, knowledge science, and cognitive linguistics. CI aims to investigate the internal information processing mechanisms and processes of the brain, the underlying abstract intelligence theories and denotational mathematics, and their engineering applications in cognitive computing and computational intelligence. This paper reports a set of nine position statements presented in the plenary panel of IEEE ICCI*CC'12 on eBrain and Cognitive Computers contributed from invited panelists who are part of the world's renowned researchers and scholars in the field of cognitive informatics and cognitive computing.",
                "id": "011108"
            },
            {
                "title": "Design of an Integrated Hyper Specification Documentation Tool",
                "abstract": "This paper presents an integrated hyper specification documentation (IHSD) methodology and tool for coherent software engineering documentation. The IHSD tool is designed for automatically creating hyperlinks between system conceptual models in UML; formal models in real-time process algebra (RTPA); and code in a programming language. The three types of design documents for a system in UML, RTPA, and C++ program are stored in a standard HTML file format. When a built-in hyperlink in a system model is clicked, the corresponding HTML page in the integrated file is show up. The IHSD method provides a powerful and convenient integration of traditionally separated system design documents by hyperlinks in a coherent environment. Under the support of the IHSD tool, readers can traverse from any point of interested objects to any other ones among the conceptual and formal models of systems as well as corresponding programs. Therefore, the readability and maintainability of large-scale software systems are dramatically improved",
                "id": "011109"
            },
            {
                "title": "The Formal Design Model of a Real-Time Operating System RTOS+: Static and Dynamic Behaviors",
                "abstract": "A real-time operating system RTOS provides a platform for the design and implementation of a wide range of applications in real-time systems, embedded systems, and mission-critical systems. This paper presents a formal design model for a general RTOS known as RTOS+ that enables a specific target RTOS to be rigorously and efficiently derived in real-world applications. The methodology of a denotational mathematics, Real-Time Process Algebra RTPA, is described for formally modeling and refining architectures, static behaviors, and dynamic behaviors of RTOS+. The conceptual model of the RTOS+ system is introduced as the initial requirements for the system. The architectural model of RTOS+ is created using RTPA architectural modeling methodologies and refined by a set of Unified Data Models UDMs. The static behaviors of RTOS+ are specified and refined by a set of Unified Process Models UPMs. The dynamic behaviors of the RTOS+ system are specified and refined by the real-time process scheduler and system dispatcher. This work is presented in two papers in serial due to its excessive length. The static and dynamic behavioral models of RTOS+ is described in this paper; while the conceptual and architectural models of RTOS+ has been published in IJSSCI 22.",
                "id": "011110"
            },
            {
                "title": "Guest editorial: special issue on cybernetics and cognitive informatics",
                "abstract": "The three greatest theories in science and engineering developed in the 1940s are cybernetics, information theory, and systems theory. Cybernetics is the science of communication and control in humans, machines, organizations, and societies across the reductive hierarchy of neural, cognitive, functional, and logical levels. A contemporary form of cybernetics, known as cognitive informatics (CI), is a transdisciplinary inquiry of cognitive and information sciences that investigates into the internal information processing mechanisms and processes of the brain and natural intelligence and their engineering applications via an interdisciplinary approach. This special issue on cybernetics and CI focuses on the theme of \"convergence of CI and cybernetics,\" which investigates the shared foundations of cybernetics and CI and their impacts on cybernetic and cognitive systems. This editorial demonstrates that the investigation into CI and cybernetics may encouragingly result in fundamental discoveries toward the development of next-generation intelligent systems and cognitive computing technologies.",
                "id": "011111"
            },
            {
                "title": "Perspectives on Cognitive Informatics and Cognitive Computing",
                "abstract": "Cognitive informatics is a transdisciplinary enquiry of computer science, information sciences, cognitive science, and intelligence science that investigates the internal information processing mechanisms and processes of the brain and natural intelligence, as well as their engineering applications in cognitive computing. Cognitive computing is an emerging paradigm of intelligent computing methodologies and systems based on cognitive informatics that implements computational intelligence by autonomous inferences and perceptions mimicking the mechanisms of the brain. This article presents a set of collective perspectives on cognitive informatics and cognitive computing, as well as their applications in abstract intelligence, computational intelligence, computational linguistics, knowledge representation, symbiotic computing, granular computing, semantic computing, machine learning, and social computing.",
                "id": "011112"
            },
            {
                "title": "A Cognitive Informatics Theory For Visual Information Processing",
                "abstract": "This paper presents a cognitive informatics theory of visual information and knowledge processing in the brain and natural intelligence. A set of cognitive principles of visual perception is reviewed, such as the gestalt principles, the cognitive informatics principles, and the hypercolumn theory. A visual frame theory is developed to explain the visual information processing mechanisms of human vision, where the size of a unit visual frame is tested and calibrated based on vision experiments. Then, the framework of human visual information processing is established. Based on it, the mechanisms of visual information processing and the compatibility of internal representations between visual and abstract information and knowledge are elaborated.",
                "id": "011113"
            },
            {
                "title": "Perspectives on denotational mathematics: new means of thought",
                "abstract": "The denotational and expressive needs in cognitive informatics,computational intelligence, software engineering, and knowledge engineeringlead to the development of new forms of mathematics collectively known asdenotational mathematics. Denotational mathematics is a category ofmathematical structures that formalize rigorous expressions and long-chain inferences of system compositions and behaviors with abstract concepts,complex relations, and dynamic processes. Typical paradigms of denotationalmathematics are such as concept algebra, system algebra, Real-Time ProcessAlgebra (RTPA), Visual Semantic Algebra (VSA), fuzzy logic, and rough sets.A wide range of applications of denotational mathematics have been identifiedin many modern science and engineering disciplines that deal with complex andintricate mathematical entities and structures beyond numbers, Booleanvariables, and traditional sets.",
                "id": "011114"
            },
            {
                "title": "The Formal Design Models of a Universal Array UA and its Implementation",
                "abstract": "Arrays are one of the most fundamental and widely applied data structures, which are useful for modeling both logical designs and physical implementations of multi-dimensional data objects sharing the same type of homogeneous elements. However, there is a lack of a formal model of the universal array based on it any array instance can be derived. This paper studies the fundamental properties of Universal Array UA and presents a comprehensive design pattern. A denotational mathematics, Real-Time Process Algebra RTPA, allows both architectural and behavioral models of UA to be rigorously designed and refined in a top-down approach. The conceptual model of UA is rigorously described by tuple-and matrix-based mathematical models. The architectural models of UA are created using RTPA architectural modeling methodologies known as the Unified Data Models UDMs. The physical model of UA is implemented using linear list that is indexed by an offset pointer of elements. The behavioral models of UA are specified and refined by a set of Unified Process Models UPMs. As a case study, the formal UA models are implemented in Java. This work has been applied in a number of real-time and nonreal-time systems such as compilers, a file management system, the real-time operating system RTOS+, and the ADT library for an RTPA-based automatic code generation tool.",
                "id": "011115"
            },
            {
                "title": "Cognitive computational models of emotions",
                "abstract": "Emotions are one of the important unconscious mechanisms that influence human behaviors, attentions, and decision making. The emotion process helps to determine how humans perceive their internal status and needs in order to form consciousness of an individual. Emotions have been studied from multidisciplinary perspectives and covered a wide range of empirical and psychological topics, such as understanding the emotional processes, creating cognitive and computational models of emotions, and applications in computational intelligence. This paper presents a comprehensive survey of cognitive and computational models of emotions resulted from multidisciplinary studies. We concentrate on exploring how cognitive models have served as the theoretical basis of the computational models of emotions. A comparative analysis of current approaches is elaborated with discussions of how recent advances in this area may improve the development of a coherent Cognitive Computational Model of Emotions (C2MEs), which leads to the machine simulated emotions for cognitive robots and autonomous agent systems in cognitive informatics and cognitive computing.",
                "id": "011116"
            },
            {
                "title": "Granular Algebra For Modeling Granular Systems And Granular Computing",
                "abstract": "Granular computing provides a new perspective on computing architectures and behaviors. This paper presents a recent development in denotational mathematics known as granular algebra, which enables a rigorous treatment of computing granules as a generic abstract mathematical structure and granular behaviors as a set of algebraic operations. An abstract granule is modeled as a mathematical entity that elicits a set of basic properties of computing granules. Then, a set of algebraic operations on abstract granules is defined Such as the relational, reproductive, and compositional operations. A real-world case Study is presented that demonstrates how concrete granules and their algebraic operations are derived based on granular algebra. This work shows that granular algebra is not only a powerful conceptual modeling methodology for granular systems, but also a functional specification methodology for granular computing.",
                "id": "011117"
            },
            {
                "title": "A New Mathematical Notation for Describing Notion and Thought in Software Design",
                "abstract": "This paper discusses the properties of a controllable, flexible, hybrid parallel computingarchitecture that potentially merges pattern recognition and arithmetic. Humans perform integer arithmetic in a fundamentally different way than logic-based computers. ...",
                "id": "011118"
            },
            {
                "title": "Perspectives on Cognitive Computing and Applications",
                "abstract": "Cognitive Computing CC is an emerging paradigm of intelligent computing theories and technologies based on cognitive informatics, which implements computational intelligence by autonomous inferences and perceptions mimicking the mechanisms of the brain. The development of Cognitive Computers cC is centric in cognitive computing methodologies. A cC is an intelligent computer for knowledge processing as that of a conventional von Neumann computer for data processing. This paper summarizes the presentations of a set of 6 position papers presented in the ICCI'10 Plenary Panel on Cognitive Computing and Applications contributed from invited panelists who are part of the world's renowned researchers and scholars in the field of cognitive informatics and cognitive computing.",
                "id": "011119"
            },
            {
                "title": "On the Latest Development in Cognitive Informatics: Conference Summary of the First IEEE International Conference on Cognitive Informatics (ICCI '02)",
                "abstract": "Cognitive informatics is a cutting-edge and profound interdisciplinary research area that tackles the common root problems and foundations of modern informatics, computation, software engineering, AI, and life sciences. Cognitive informatics is a new frontier that studies internal information processing mechanisms and processes of the brain, and their engineering applications in computing, software, and IT industries. The functional architecture of the brain and the natural intelligence of the mind are the last domain yet to be explored in cognitive informatics.This article reports the latest development at the First IEEE International Conference on Cognitive Informatics (ICCI '02). This report intends to draw attention of researchers, practitioners and graduate students on the investigation of cognitive mechanisms and processes of human information processing, and to stimulate the collaborative international effort on cognitive informatics research and engineering applications.",
                "id": "011120"
            },
            {
                "title": "Formal Description Of Time Management In Real-Time Operating Systems",
                "abstract": "This paper describes the formal specification of the lime management subsystem of a real-time operating system. Real-time process algebra (RTPA) is adopted to formally specify the system. The architecture, static behaviors, and dynamic behaviors of a CPU time manager are systematically specified that form an abstract model of the system. The formal specifications are implemented in C, on which the system performance can be tested and verified.",
                "id": "011121"
            },
            {
                "title": "On formal models for cognitive linguistics",
                "abstract": "Cognitive linguistics is an emerging discipline that studies the cognitive properties of natural languages and the cognitive models of languages in computational linguistics, cognitive computing, and computational intelligence. This paper presents the theoretical framework of cognitive linguistics in order to systematically formalize the syntaxes and grammars of natural languages. An abstract language model of cognitive linguistics is created at the top level. Based on it, the cognitive structures of languages at the levels of lexis, phrase, clauses, sentence, paragraph, and essay are formally modeled from the bottom up. Using contemporary denotational mathematics, the deductive grammar of English is formally modeled and rigorously analyzed. The basic research provides support for a wide range of applications in computational linguistics, cognitive informatics, online text processing, web search engines, machine language comprehension, autonomous machine learning, smart cell phones, semantic computing, and computing with words.",
                "id": "011122"
            },
            {
                "title": "A Cognitive Machine Learning System for Phrases Composition and Semantic Comprehension",
                "abstract": "Although lexical and syntactic theories for phrase analyses have been well studied in linguistic theories and computational linguistics, semantic synthesis theories for cognitive computing are still a challenge in machine learning and brain-inspired systems. This paper studies theories and mathematical models of machine knowledge learning and semantic comprehension. An Algorithm of Unsupervised Phrase Learning (AUPL) is developed that enables cognitive machines to autonomously learn phrase semantics in the sixth category of machine knowledge learning. A set of experimental results is reported to demonstrate the methodology and algorithm. This work plays a fundamental role for sentence learning where the semantics of natural languages is reduced onto those of phrases and terminal words represented by formal concepts in cognitive systems.",
                "id": "011123"
            },
            {
                "title": "The Cognitive Processes Of Analysis And Synthesis In Formal Inferences",
                "abstract": "Analyses and syntheses are a pair of fundamental cognitive processes at the meta-inference layer of the brain. This paper presents the cognitive processes of analysis and synthesis as a part of the brain's inference mechanisms. The cognitive foundations of analysis and synthesis are explored, and their mathematical models are created in concept algebra and system algebra by concept and/or system decomposition, composition, specification, and aggregation. Based on the cognitive and mathematical models, the cognitive processes of analysis and synthesis are formally described in Real-Time Process Algebra (RTPA). The formal modeling and rigorous explanation of this pair of cognitive processes for formal inference reveal a part of the mechanisms of the brain as modeled in the Layered Reference Model of the Brain (LRMB).",
                "id": "011124"
            },
            {
                "title": "Formal Description of the Mechanisms and Cognitive Process of Memorization",
                "abstract": "Memorization is a key cognitive process of the brain because almost all human intelligence is functioning based on it. This paper presents the theory of memory and the cognitive process of memorization. Neural informatics foundations and functional models of memory and memorization are explored toward the development of the mathematical models of memory. The mechanisms of memorization as a cognitive process is investigated that explains how and when memory is created in long-term memory. On the basis of the formalized memory and memorization models, the cognitive process of memorization is rigorously described using Real-Time Process Algebra (RTPA). This work forms one of the core enquiries for formally explaining the mechanisms of the brain and the natural intelligence according to the Layered Reference Model of the Brain (LRMB) developed in cognitive informatics.",
                "id": "011125"
            },
            {
                "title": "On Cognitive Informatics",
                "abstract": "Information is the third essence in modeling the natural world. This paper attempts to explore an emerging discipline know as cognitive informatics. Cognitive Informatics is a profound interdisciplinary research area that tackles the common root problems of modern informatics, computation, software engineering, artificial intelligence (AI), neural psychology, and cognitive science. Cognitive informatics studies the internal information processing mechanisms and natural intelligence of the brain. This paper describes historical development of informatics from the classical information theory, contemporary informatics, to cognitive informatics. The domain of cognitive informatics and its interdisciplinary nature are explored. Foundations of cognitive informatics, particularly the brain vs. the mind, the acquired life functions vs. the inherited ones, and generic relationships between information, matter and energy are investigated. The potential engineering applications of cognitive informatics and perspectives on future research are discussed. It is expected that the investigation into cognitive informatics will result in fundamental findings towards the development of next generation IT and software technologies, such as neural computers, bioinformatics, quantum information processing, new software development approaches, and new architectures of information systems.",
                "id": "011126"
            },
            {
                "title": "Implementing the Real-Time Processes of RTPA using Real-Time Java",
                "abstract": "This paper discusses the implementation of the real-time processes of real-time process algebra (RTPA) using real-time Java. The difficulty in implementing real-time software from formal specifications comes from the fact that some realtime features expressed using a formal specification language cannot be easily transformed into a programming language. The paper shows how a combination of a real-time support library for RTPA (RTPASupportLib) and the recently developed extensions to real-time Java can overcome some of these difficulties. The strategies and techniques employed in implementing RTPA real-time features such as concurrency, interleaving, and interrupt/event/time-driven dispatching of real-time processes are presented. A number of case studies on the implementation of RTPA real-time features show that the approach is a feasible one",
                "id": "011127"
            },
            {
                "title": "On the cognitive process of human problem solving",
                "abstract": "One of the fundamental human cognitive processes is problem solving. As a higher-layer cognitive process, problem solving interacts with many other cognitive processes such as abstraction, searching, learning, decision making, inference, analysis, and synthesis on the basis of internal knowledge representation by the object-attribute-relation (OAR) model. Problem solving is a cognitive process of the brain that searches a solution for a given problem or finds a path to reach a given goal. When a problem object is identified, problem solving can be perceived as a search process in the memory space for finding a relationship between a set of solution goals and a set of alternative paths. This paper presents both a cognitive model and a mathematical model of the problem solving process. The cognitive structures of the brain and the mechanisms of internal knowledge representation behind the cognitive process of problem solving are explained. The cognitive process is formally described using real-time process algebra (RTPA) and concept algebra. This work is a part of the cognitive computing project that designed to reveal and simulate the fundamental mechanisms and processes of the brain according to Wang's layered reference model of the brain (LRMB), which is expected to lead to the development of future generation methodologies for cognitive computing and novel cognitive computers that are capable of think, learn, and perceive.",
                "id": "011128"
            },
            {
                "title": "Perspectives on cognitive informatics and cognitive computing: Summary of the panel of ieee iccf09",
                "abstract": "Cognitive Informatics (CI) is a transdisciplinary enquiry of computer science, information sciences, cognitive science, and intelligence science that investigates into the internal information processing mechanisms and processes of the brain and natural intelligence, as well as their engineering applications in cognitive computing. This paper summarizes the presentations of a set of 14 position papers in the ICCI'09 Panel on Cognitive Informatics and Cognitive Computing contributed from invited panelists who are part of the world's renowned researchers and schola~s. in the field of cognitive informatics and cognitive computing. \u00a9 2009 IEEE.",
                "id": "011129"
            },
            {
                "title": "Seamless Implementation of a Telephone Switching System Based on Formal Specifications in RTPA",
                "abstract": "The Telephone Switching Systems TSS is a typical real-time system that is highly complicated in design and implementation. In order to deal with the extreme complexity in real-world settings, a suitable and efficient mathematical means is required beyond any programming language. To this purpose, an efficient and precise denotational mathematics known as the Real-Time Process Algebra RTPA and the RTPA methodology for system modeling are introduced. Empirical experimental results are reported in this paper on the implementation of TSS based on formal models of the system in RTPA. Three phases of experiments are designed on TSS conceptual modeling, system interface design, and programming implementation and testing. All groups in the experiments with 7 to 8 members have been able to efficiently understood, design, and implement the TSS system in a simplified version in four weeks, which has been estimated as a 10+ person-year project in the industry. The efficiency and expressiveness of RTPA are empirically demonstrated base on the case studies in the experiments.",
                "id": "011130"
            },
            {
                "title": "The Theoretical Framework Of Cognitive Informatics",
                "abstract": "Cognitive Informatics (CI) is a transdisciplinary enquiry of the internal information processing mechanisms and processes of the brain and natural intelligence shared by almost all science and engineering disciplines. This article presents an intensive review of the new field of CI. The structure of the theoretical framework of CI is described encompassing the Layered Reference Model of the Brain (LRMB), the OAR model of information representation, Natural Intelligence (NI) vs. Artificial Intelligence (AI), Autonomic Computing (AC) vs. imperative computing, CI laws of software, the mechanism of human perception processes, the cognitive processes of formal inferences, and the formal knowledge system. Three types of new structures of mathematics, Concept Algebra (CA), Real-Time Process Algebra (RTPA), and System Algebra (SA), are created to enable rigorous treatment of cognitive processes of the brain as well as knowledge representation and manipulation in a formal and coherent framework. A wide range of applications of CI in cognitive psychology, computing, knowledge engineering, and software engineering has been identified and discussed.",
                "id": "011131"
            },
            {
                "title": "Towards a fuzzy logical algebra (FLA) for formal inferences in cognitive computing and cognitive robotics",
                "abstract": "The fuzzy and algebraic treatment of the hyperstructured logical variables and complex logical expressions are centric in inference methodologies of many contemporary disciplines such as cognitive informatics, cognitive computing, computational linguistics, cognitive semantics, causal inferences, computing with words, cognitive robotics, and computational intelligence. This paper presents a denotational mathematics of fuzzy logical algebra and a formal theory for cognitive inference systems. Fuzzy logical propositions are formally modeled based on fuzzy logical expressions and variables. An algebraic structure of extended fuzzy logic is created by a set of algebraic operators on formal fuzzy propositions. Fuzzy logical algebra provides a denotational mathematical means for algebraic manipulations of fuzzy logical expressions. A set of insightful and interesting properties of fuzzy logical algebra is revealed.",
                "id": "011132"
            },
            {
                "title": "A cognitive model of motor planning for virtual creatures",
                "abstract": "The planning is one of the most important cognitive functions of the human beings. It is essential for the solution of mental and physical problems. This paper shows a conceptual and computational model of motor planning. It is based on recent findings of the human brain and how the human can develop several motor plans in order to solve their daily problems. The objective of our bio-inspired model is to develop appropriate mechanisms to endow virtual creatures with them, in order to emulate both internal and external processes of the human brain.",
                "id": "011133"
            },
            {
                "title": "Toward Formal Models of the Theoretical Framework of Fundamental Economics",
                "abstract": "Many fundamental theories and doctrines of micro- and macro-economics are not formally studied in economics. This paper presents a rigorous treatment and explanation of a set of fundamental empirical theories of economics. It is recognized that the adaptive equilibrium of market systems is rooted in the negative feedbackmechanisms of demands and supplies. A mathematical model of economic equilibrium developed in this paper provides a formal proof of Adam Smith's hypothesis of the 'invisible hand'. An economic equilibrium theorem is derived to deal with more complicated multivariable equilibrium problems that could not be handled in conventional economic theories. Then, a set of mathematical models is developed for dynamic cost analyses and the estimation of economical outcomes of engineering projects.",
                "id": "011134"
            },
            {
                "title": "Transforming RTPA Mathematical Models of System Behaviors Into C++",
                "abstract": "Real-time process algebra (RTPA) is an expressive mathematical means for describing cognitive behaviors and processes of human beings and software systems. This paper presents the strategies and patterns for transforming mathematical models of system behaviors in RTPA into C++. An RTPA code generator towards the simulation of system behaviors and processes is implemented, which transforms system architectures and behaviors into C++ based on the RTPA-to-C++ mappings strategies and patterns, as well as the RTPA runtime library. The generated code in C++ is executable and effective for simulating system behaviors specified in RTPA",
                "id": "011135"
            },
            {
                "title": "The Formal Design Models of Digraph Architectures and Behaviors",
                "abstract": "Graphs are one of the most fundamental and widely used non-linear hierarchical structures of linked nodes. Problems in sciences and engineering can be formulated and solved by the graph model. This paper develops a comprehensive design pattern of formal digraphs using the Doubly-Linked List DLL architecture. The most complicated form of graphs known as the weighted digraph is selected as a general graph model, based on it simple graphs such as nondirected and/or nonweighted ones can be easily derived and tailored. A rigorous denotational mathematics, Real-Time Process Algebra RTPA, is adopted, which allows both architectural and behavioral models of digraphs to be rigorously designed and implemented in a top-down approach. The architectural models of digraphs are created using RTPA architectural modeling methodologies known as the Unified Data Models UDMs. The physical model of digraphs is implemented using nodes of DLL dynamically created in the memory. The behavioral models of digraphs are specified and refined by a set of 18 Unified Process Models UPMs in three categories namely the management operations, traversal operations, and node manipulation operations. This work has been applied in a number of real-time and nonreal-time system designs and specifications such as a Real-Time Operating System RTOS+, graph-based and tree-based applications, and the ADT library for an RTPA-based automatic code generation tool.",
                "id": "011136"
            },
            {
                "title": "A worldwide survey of base process activities towards software engineering process excellence",
                "abstract": "A survey been designed to seek the practical foundation of base process activities (BPAs) in the software industry and to support research in modelling the software engineering processes. A superset of BPAs [1] compatible with the current software process models, such as SPICE (ISO 15504), CMM, ISO 9000 and BOOTSTRAP [2-3], were identified for construction of the questionnaires.This paper reports the survey findings on BPAs in software engineering processes. A summary of the current software engineering process techniques and practices modelled by 83 BPAs in 10 processes and three categories is given. Each BPA is benchmarked on attributes of mean importance and ratios of significance, practice and effectiveness.Based on the benchmarks, and by comparing with the current practice of the reader's organization, recommendations can be given on which specific areas need to have processes established first, and which areas should be highest priority for process improvement.",
                "id": "011137"
            },
            {
                "title": "On the Cognitive Informatics Foundations of Software Engineering",
                "abstract": "This keynote lecture explores the approaches to implement intelligent behaviors by biological organisms, silicon automata, and computing systems. Autonomous computing is introduced as the latest and advanced computing techniques built upon routine, algorithmic, ...",
                "id": "011138"
            },
            {
                "title": "On Abstract Intelligence And Brain Informatics: Mapping Cognitive Functions Of The Brain Onto Its Neural Structures",
                "abstract": "A key notion in abstract intelligence and cognitive informatics is that the brain and natural intelligence may only be explained by a hierarchical and reductive theory that maps the brain through the embodied neurological, physiological, cognitive, and logical levels from bottom-up induction and top-down deduction. This paper presents an abstract intelligence framework for modeling the structures and functions of the brain across these four levels. A set of abstract intelligent model, cognitive functional model, and neurophysiological model of the brain is systematically developed. On the basis of the abstract intelligent models of the brain at different levels, the conventionally highly overlapped, redundant, and even contradicted empirical observations in brain studies and cognitive psychology may be rigorously clarified and neatly explained. The improved understanding about the brain has led to the development of a wide range of novel technologies and systems such as cognitive computers, cognitive robots, and other applied cognitive systems.",
                "id": "011139"
            },
            {
                "title": "Cognitive Informatics: Towards Future Generation Computers that Think and Feel.",
                "abstract": "This keynote lecture presents a set of the latest advances in Cognitive Informatics (Cl) that leads to the design and implementation of future generation computers known as the cognitive computers that are capable of thinking and feeling. The theory and philosophy behind the next generation computers and computing technologies are Cl. The theoretical framework of Cl may be classified as an entire set of cognitive functions and processes of the brain and an enriched set of descriptive mathematics. the cognitive computers are created for cognitive and perceptible concept/knowledge processing based on contemporary mathematics such as Concept Algebra, Real-Time Process Algebra, and System Algebra. Because the cognitive computers implement the fundamental cognitive processes of the natural intelligence such as the learning, thinking, formal inference, and perception processes, they are novel information processing systems that think and feel. The cognitive computers are centered by the parallel inference engine and perception engine that implement autonomic learning/reasoning and perception mechanisms based on descriptive mathematics.",
                "id": "011140"
            },
            {
                "title": "Formal Description of the UML Architecture and Extensibility",
                "abstract": "Since its emergence in 1995, the Unified Modeling Language (UML) has become part of the mainstream of object-oriented software development in a wide range of applications. This paper presents a formal description of UML technologies for visualized specification and modeling of software systems, and analyzes the usability of UML views and diagrams. Requirements and extension of UML capability to real-time software system specification and description are explored, and a real time process-algebra-based approach for extending UML's descriptivity for real-time system modeling is provided. In addition, findings and improvement approaches on UML applications in real-world projects are discussed. R\u00c9SUM\u00c9: Depuis son \u00e9mergence en 1995, UML est devenu pr\u00e9pond\u00e9rant dans les d\u00e9veloppements de logiciels orient\u00e9-objets et pour une grande vari\u00e9t\u00e9 d'applications. Ce papier pr\u00e9sente une approche formelle des techniques UML pour la pr\u00e9sentation visuelle des sp\u00e9cifications et la mod\u00e9lisation de syst\u00e8mes logiciels. Il analyse \u00e9galement l'utilisation des vues et des diagrammes UML. Les besoins et les extensions d'UML vers les syst\u00e8mes temps- r\u00e9el sont explor\u00e9s et une alg\u00e8bre de processus temps-r\u00e9el est propos\u00e9e. Finalement des r\u00e9f\u00e9rences et des am\u00e9liorations d'UML pour des applications ou des projets r\u00e9els sont discut\u00e9es.",
                "id": "011141"
            },
            {
                "title": "A Benchmark-Based Adaptable Software Process Model",
                "abstract": "Benchmark-based process assessment and improvement is a cutting-edge technology [1] in software engineering for adaptive and relative process improvement. It is found that benchmark-based process improvement is more adaptable, feasible, and economical in process-based software engineering. By adopting the benchmark-based process improvement, the target capability levels of a software organization may be set related to the benchmarks of the software industry or a specific sector, rather than a virtual highest capability level according to a goal-oriented process model.The objective of this project is to develop a benchmark-based and organization and project size and features adaptable software process model (BBASPM). This report describes the architecture of the BBASPM model and its process and capability dimensions. BBASPM is developed by referring to the SEPRM model and benchmarks [1] in process selection and target process capability determination. Adaptation guidelines of BBASPM to different sized organisations and projects are provided, especially for small organizations and small projects. BBASPM is developed as a superset of CMM, ISO 15504 and ISO 9001. Using the mapping mechanism provided in SEPRM, the BBASPM capability levels can be transformed into other process models and standards, such as CMM and ISO 15504. We also describe our experience in using BBASPM on a process improvement project.",
                "id": "011142"
            },
            {
                "title": "On Cognitive Informatics Foundations of Knowledge and Formal Knowledge Systems",
                "abstract": "Knowledge is acquired information in forms of data, behavior, experience, and skills retained in memory through learning. This paper presents the nature of human knowledge, its mathematical model, and its manipulation by concept algebra. The taxonomy of knowledge and its hierarchical abstraction model of knowledge are investigated. Mathematical models of knowledge and its manipulation by concept algebra are developed. The theory of knowledge acquisition and the cognitive model of knowledge spaces are formally described covering the effort and complexity models of knowledge creation and acquisition. The framework of formalized knowledge systems is developed based on the analyses of formal and empirical knowledge.",
                "id": "011143"
            },
            {
                "title": "On the Cognitive Foundations of Autonomous Systems and General AI",
                "abstract": "It is recognized that autonomous systems are advanced intelligent systems towards general AI that may think and infer. This work investigates into basic research on the cognitive foundations of autonomous systems. Key challenges to real-time intelligence generation are analyzed in classic stored-programmed-controlled computing and data-regressed ion AI, which lead to a general intelligence generation methodology of knowledge-inferred AI. A theoretical framework of autonomous AI systems is elaborated towards autonomous systems for enabling deep machine thinking, knowledge learning, cognitive computing, and mission-critical intelligent systems.",
                "id": "011144"
            },
            {
                "title": "An Operational Semantics Of Real-Time Process Algebra (Rtpa)",
                "abstract": "The need for new forms of mathematics to express software engineering concepts and entities has been widely recognized. Real-time process algebra (RTPA) is a denotational mathematical stricture and a system modeling methodoloAy for describing the architectures and behaviors of real-time and nonreal-time software systems. This article presents an operational semantics of RTPA, which explains how syntactic constructs in RTPA can be reduced to values on an abstract-reduction machine. The operational semantics of RIPA provides a comprehensive paradigm offormal semantics that establishes an entire set of operational semantic rules of software. RTPA has been successfully applied in real-world system modeling and code generation for software systems, human cognitive processes, and intelligent systems.",
                "id": "011145"
            },
            {
                "title": "The Cognitive Processes Of Formal Inferences",
                "abstract": "Theoretical research is predominately an inductive process; while applied research is mainly a deductive process. Both inference processes are based on the cognitive process and means of abstraction. This article describes the cognitive processes of formal inferences such as deduction, induction, abduction, and analogy. Conventional propositional arguments adopt static causal inference. This article introduces more rigorous and dynamic inference methodologies, which are modeled and described as a set of cognitive processes encompassing a series of basic inference steps. A set of mathematical models of formal inference methodologies is developed. Formal descriptions of the four forms of cognitive processes of inferences are presented using Real-Time Process Algebra (RTPA). The cognitive processes and mental mechanisms of inferences are systematically explored and rigorously modeled. Applications of abstraction and formal inferences in both the revilement of the fundamental mechanisms of the brain and the investigation of next generation cognitive computers are explored.",
                "id": "011146"
            },
            {
                "title": "Deductive Semantics Of Rtpa",
                "abstract": "Deductive semantics is a novel software semantic theory that deduces the semantics of a program in a given programming language from a unique abstract semantic function to the concrete semantics embodied by the changes of status of a finite set of variables constituting the semantic environment of the program. There is a lack of a generic semantic function and its unified mathematical model in conventional semantics, which may be used to explain a comprehensive set of programming statements and computing behaviors. This article presents a complete paradigm of formal semantics that explains how deductive semantics is applied to specify the semantics of real-time process algebra (RTPA) and how RTPA challenges conventional formal semantic theories. Deductive semantics can be applied to define abstract and concrete semantics of programming languages, formal notation systems, and large-scale software systems, to facilitate software comprehension and recognition, to support tool development, to enable semantics-based software testing and verification, and to explore the semantic complexity of software systems. Deductive semantics may greatly simplify the description and analysis of the semantics of complicated software systems specified in formal notations and implemented in programming languages.",
                "id": "011147"
            },
            {
                "title": "On The Mathematical Theories And Cognitive Foundations Of Information",
                "abstract": "A recent discovery in computer and software sciences is that information in general is a deterministic abstract quantity rather than a probability-based property of the nature. Information is a general form of abstract objects represented by symbolical, mathematical, communication, computing, and cognitive systems. Therefore, information science is one of the contemporary scientific disciplines collectively known as abstract sciences such as system, information, cybernetics, cognition, knowledge, and intelligence sciences. This paper presents the cognitive foundations, mathematical models, and formal properties of information towards an extended theory of information science. From this point of view, information is classified into the categories of classic, computational, and cognitive information in the contexts of communication, computation, and cognition, respectively. Based on the three generations of information theories, a coherent framework of contemporary information is introduced, which reveals the nature of information and the fundamental principles of information science and engineering.",
                "id": "011148"
            },
            {
                "title": "Formal Specification and Representation of Design Patterns Using RTPA",
                "abstract": "Software patterns are recognized as an ideal documentation of expert knowledge in software design and development. However, its formal model and rigorous semantics have never been generalized and matured. The traditional UML specification and some formal specification attempts cannot capture the essence of generic patterns precisely, understandably, and essentially. A generic model of patterns is presented in this paper using real-time process algebra (RTPA). The formal model of patterns are more readable and highly generic, which can be used as the meta model to denote any design patterns deductively, and can be translated into code in programming languages by supporting tools. This work reveals that a pattern is a highly complicated and dynamic structure of software design encapsulation, because of its complex and flexible internal associations between multiple abstract classes and instantiations. The generic model of patterns is not only applicable to existing patterns' description and comprehension, but also useful for future patterns' identification and formalization",
                "id": "011149"
            },
            {
                "title": "Cognitive Complexity of Software and its Measurement.",
                "abstract": "The estimation and measurement of functional complexity of software are an age-long problem in software engineering. The cognitive complexity of software presented in this paper is a new measurement for cross-platform analysis of complexities, sizes, and comprehension effort of software specifications and implementations in the phases of design, implementation, and maintenance in software engineering. This paper reveals that the cognitive complexity of software is a product of its architectural and operational complexities on the basis of deductive semantics and the abstract system theory. Ten fundamental basic control structures (BCS's) are elicited from software architectural and behavioral specifications and descriptions. The cognitive weights of those BCS's are derived and calibrated via a series of,psychological experiments. Based on this work, the cognitive complexity of software systems can be rigorously and accurately measured and analyzed. Comparative case studies demonstrate that the cognitive complexity is highly distinguishable in software functional complexity and size measurement in software engineering.",
                "id": "011150"
            },
            {
                "title": "Unveiling the Cognitive Mechanisms of Eyes: The Visual Sensor Vs. the Perceptive Browser of the Brain",
                "abstract": "Eyes as the unique organ possess intensively direct connections to the brain and dynamically perceptual accessibility to the mind. This paper analyzes the cognitive mechanisms of eyes not only as the sensory of vision, but also the browser of internal memory in thinking and perception. The browse function of eyes is created by abstract conditioning of the eye's tracking pathway for accessing internal memories, which enables eye movements to function as the driver of the perceptive thinking engine of the brain. The dual mechanisms of the eyes as both the external sensor of the brain and the internal browser of the mind are explained based on evidences and cognitive experiences in cognitive informatics, neuropsychology, cognitive science, and brain science. The finding on the experiment's internal browsing mechanism of eyes reveals a crucial role of eyes interacting with the brain for accessing internal memory and the cognitive knowledge base in thinking, perception, attention, consciousness, learning, memorization, and inference.",
                "id": "011151"
            },
            {
                "title": "Perspectives on the Field of Cognitive Informatics and its Future Development",
                "abstract": "The contemporary wonder of sciences and engineering has recently refocused on the beginning point of: how the brain processes internal and external information autonomously and cognitively rather than imperatively like conventional computers. Cognitive Informatics CI is a transdisciplinary enquiry of computer science, information sciences, cognitive science, and intelligence science that investigates the internal information processing mechanisms and processes of the brain and natural intelligence, as well as their engineering applications in cognitive computing. This paper reports a set of eight position statements presented in the plenary panel of IEEE ICCI'10 on Cognitive Informatics and Its Future Development contributed from invited panelists who are part of the world's renowned researchers and scholars in the field of cognitive informatics and cognitive computing.",
                "id": "011152"
            },
            {
                "title": "Fuzzy Causal Patterns of Humor and Jokes for Cognitive and Affective Computing",
                "abstract": "Humor is an advanced emotional and cognitive ability of mankind that involves complex semantic inference and deep passionate appreciation. This paper presents the cognitive foundations of amusement and a general theory of humor based on the recent advances in cognitive informatics, cognitive linguistics, cognitive computing, and fuzzy causal analyses. A theory of fuzzy false causation FFC is introduced that reveals humor and jokes as false causations in fuzzy causal inferences. Base on the FFC theory, a general pattern of humor GPH is formalized for analyzing the settings and appreciations of a set of sample jokes. A formal measurement of the degree of amusement in jokes and humor is quantitatively described towards the rational explanation of jokes based on cognitive affective assessment. The formal models of humor and jokes enable machines for humor comprehension and appreciation in artificial intelligence, cognitive computing, computational intelligence, and cognitive robots.",
                "id": "011153"
            },
            {
                "title": "On the Emergence of Autonomous Systems towards Deep Thinking Machines and General AI Driven by Abstract Intelligence Theories and Intelligent Mathematics",
                "abstract": "Thinking and inference are key essences of human intelligence as Ren\u00e9 Descartes stated that \"I think therefore I am (1637).\" The ultimate goal of General AI (GAI) [1], [2], [3], [4] is to enable machine thinking and inference [5], [6], [7], [8], [9] beyond data-based learning [10], [11] towards run-time intelligence generation [9], [12] driven by Autonomous Systems (AS) [13], [14] and cognitive robots [15]. Basic research on machine thinking is powered by Abstract Intelligence theories [16], [17], [18], [19], [20], [21] and Intelligent Mathematics (IM) [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32] towards enabling symbiotic and collective intelligence [13], [33] and cognitive systems [34], [35], [36], [37], [38], [39].This keynote lecture presents theoretical foundations of and latest advances in machine thinking and inference driven by AS, GAI, and cognitive robots. Latest breakthroughs in IMs and run-time behavior generation by cognitive systems and AS are elaborated. Emerging paradigms of machine thinking autonomies are introduced to demonstrate the next generation of general AI and symbiotic intelligent systems. The advances in AS are expected to lead to a wide range of novel applications in deep machine thinking, machine knowledge learning, cognitive robots, inference computers, brain-inspired systems, and mission-critical intelligent systems.",
                "id": "011154"
            },
            {
                "title": "On inference algebra: A formal means for machine reasoning and cognitive computing.",
                "abstract": "Inference as a fundamental mechanism of thought is one of the gifted abilities of human beings. Inference can be described as a cognitive process that creates rational causations between a pair of cause and effect based on empirical arguments, formal reasoning, and/or statistical regulations [2, 15, 24, 26]. Conventional logic inferences may be classified as logical arguments, deductive, inductive, abductive, and analogical inferences [4, 9, 10, 11, 15].",
                "id": "011155"
            },
            {
                "title": "Abstract intelligence and cognitive robots",
                "abstract": "Abstract intelligence is the human enquiry of both natural and artificial intelligence at the neural, cognitive, functional,\n and logical levels reductively from the bottom up. According to the abstract intelligence theory, a cognitive robot is an autonomous robot that is capable of thought, perception, and learning based on the three-level computational intelligence\n known as the imperative, autonomic, and cognitive intelligence. This paper presents the theoretical foundations of cognitive\n robots based on the latest advances in abstract intelligence, cognitive informatics, and denotational mathematics. A formal\n model of intelligence known as the Generic Abstract Intelligence Mode (GAIM) is developed, which provides a foundation to explain the mechanisms of advanced natural intelligence such as thinking,\n learning, and inference. A set of denotational mathematics is introduced for rigorously modeling and manipulating the behaviors\n of cognitive robots. A case study on applications of a denotational mathematics, visual semantic algebra (VSA), is presented\n in architectural and behavioral modeling of cognitive robots based on the theory of abstract intelligence.",
                "id": "011156"
            },
            {
                "title": "Design Of A Cognitive Complexities Measurement And Analysis Tool",
                "abstract": "The complexities of software systems are more prominent now than ever, due to progression of simple software applications to large integrated systems, either centralized or distributed. The challenges to software engineer somehow to understand and manage such large software systems and deal with software cognitive complexities. Although there are research areas such as human machine interface or human factor engineering, there is still little research on how to quantitatively measure and analyze human cognitive complexities in comprehending and understanding complex software systems. This paper studies human interaction with software programs in the cognitive perspective on software complexity. Based on the findings, a pilot software cognitive complexity measurement and analyses tool (CCMAT) is presented.",
                "id": "011157"
            },
            {
                "title": "On Visual Semantic Algebra (VSA) and the cognitive process of pattern recognition.",
                "abstract": "A new form of denotational mathematics known as Visual Semantic Algebra (VSA) is presented for abstract visual object and architecture manipulation. The cognitive theories for pattern recognition, such as cognitive principles of visual perception and basic mechanisms of object and pattern recognition, are explored. A number of case studies on VSA in pattern recognition are presented to demonstrate VAS' expressive power for algebraic manipulation of visual objects. The cognitive process of pattern recognition is rigorously modeled using VSA and Real-Time Process Algebra (RTPA), which reveals the fundamental mechanisms of natural pattern recognition by the brain. The theories and case studies demonstrate that VSA can be applied not only in machine visual and spatial reasoning, but also in computational intelligence system designs as a powerful man-machine language for representing and manipulating visual geometrical systems. On the basis of VSA, computational intelligence systems such as robots and cognitive computers may process and inference visual and image object rigorously and efficiently.",
                "id": "011158"
            },
            {
                "title": "Cognitive informatics and denotational mathematical means for brain informatics",
                "abstract": "Cognitive informatics studies the natural intelligence and the brain from a theoretical and a computational approach, which rigorously explains the mechanisms of the brain by a fundamental theory known as abstract intelligence, and formally models the brain by contemporary denotational mathematics. This paper, as an extended summary of the invited keynote presented in AMT-BI 2010, describes the interplay of cognitive informatics, abstract intelligence, denotational mathematics, brain informatics, and computational intelligence. Some of the theoretical foundations for brain informatics developed in cognitive informatics are elaborated. A key notion recognized in recent studies in cognitive informatics is that the root and profound objective in natural, abstract, and artificial intelligence in general, and in cognitive informatics and brain informatics in particular, is to seek suitable mathematical means for their special needs that were missing in the last six decades. A layered reference model of the brain and a set of cognitive processes of the mind are systematically developed towards the exploration of the theoretical framework of brain informatics. The current methodologies for brain studies are reviewed and their strengths and weaknesses are analyzed. A wide range of applications of cognitive informatics and denotational mathematics are recognized in brain informatics toward the implementation of highly intelligent systems such as world-wide wisdom (WWW+), cognitive knowledge search engines, autonomous learning machines, and cognitive robots.",
                "id": "011159"
            },
            {
                "title": "A Survey and Formal Analyses on Sequence Learning Methodologies and Deep Neural Networks",
                "abstract": "Sequence learning is one of the hard challenges to current machine learning technologies and deep neural network technologies. This paper presents a literature survey and analysis on a variety of neural networks towards sequence learning. The conceptual models, methodologies, mathematical models and usages of classic neural networks and their learning capabilities are contrasted. Advantages and disadvantages of neural networks for sequence learning are formally analyzed. The state-of-the-art, theoretical problems and technical constraints of existing methodologies are reviewed. The needs for understanding temporal sequences by unsupervised or intensive-training-free learning theories and technologies are elaborated.",
                "id": "011160"
            },
            {
                "title": "A large-scale empirical study on the cognitive complexity of software",
                "abstract": "There are many measures for software complexities in software engineering. An emerging developer-oriented measure is recently developed known as the cognitive complexity from the field of cognitive informatics and cognitive computing. This paper describes an empirical approach using a Software Cognitive Complexity Analysis Tool (SCCAT) to analyze a comprehensive set of real-world software system programs. Practical and quantifiable results of the cognitive complexity measure are then presented. Additional characteristics of cognitive complexity measure are described that provide insights into human cognition in software engineering.",
                "id": "011161"
            },
            {
                "title": "On Cognitive Foundations and Mathematical Theories of Knowledge Science.",
                "abstract": "Knowledge is one of the fundamental cognitive objects in the brain among those of data, information, and intelligence. Knowledge can be classified into two main categories, i.e., conceptual knowledge for knowing to-be and behavioral knowledge for knowing to-do, particularly the former. This paper presents a basic study on a mathematical theory of knowledge towards knowledge science. The taxonomy and cognitive foundations of knowledge are explored, which reveal that the basic cognitive structure of conceptual knowledge is a formal concept and that of behavioral knowledge is a formal process. Mathematical models of knowledge are created in order to enable formal representation and rigorous manipulation of knowledge. A set of formal principles and properties of knowledge is elicited and elaborated towards the development of knowledge science and cognitive knowledge systems. It is discovered that the basic unit of knowledge is a binary relation, shortly bir, as a counterpart of bit a binary digit for information and data.",
                "id": "011162"
            },
            {
                "title": "Cognitive Informatics Foundations of Nature and Machine Intelligence",
                "abstract": "Artificial Intelligence applications, particularly those involving natural language understanding, are actually less ambitious than they were decades ago. Statistical and machine learning techniques have had great success on tasks that can be treated ...",
                "id": "011163"
            },
            {
                "title": "On the cognitive processes of human perception",
                "abstract": "This paper presents a rigorous treatment of human perceptual processes such as emotions, motivations, and attitudes. A set of mathematical models and formally described cognitive processes is developed. The interactions and relationships between motivation and attitude are formally described in real-time process algebra (RTPA). Applications of the mathematical models of motivations and attitudes in software engineering are demonstrated. This work is the detailed description of a part of the layered reference model of the brain (LRMB) that provides a comprehensive model for explaining the fundamental cognitive processes of the brain and their interactions.",
                "id": "011164"
            },
            {
                "title": "Autolearner: An Autonomic Machine Learning System Based on Concept Algebra",
                "abstract": "Artificial Intelligence applications, particularly those involving natural language understanding, are actually less ambitious than they were decades ago. Statistical and machine learning techniques have had great success on tasks that can be treated ...",
                "id": "011165"
            },
            {
                "title": "On the Informatics Laws of Software",
                "abstract": "This paper discusses the properties of a controllable, flexible, hybrid parallel computingarchitecture that potentially merges pattern recognition and arithmetic. Humans perform integer arithmetic in a fundamentally different way than logic-based computers. ...",
                "id": "011166"
            },
            {
                "title": "A hierarchical abstraction model for software engineering",
                "abstract": "Abstraction is a basic foundation and a powerful means in science and engineering, such as philosophy, mathematics, cognitive informatics, computing, and software engineering, because any complex inference process is based on it. A Hierarchical Abstraction Model (HAM) is presented in this paper, which elaborates the five levels of abstraction known as the analogue objects, diagrams, natural languages, professional notation systems, and mathematics. On the basis of the HAM model, main roles of abstraction in software engineering are explained, and a number of fundamental principles for software engineering practice are derived. The cognitive constraints, limitations of conventional descriptive means and modeling methodologies, as well as the need for more powerful denotational mathematics for software engineering are identified.",
                "id": "011167"
            },
            {
                "title": "The Cognitive Processes Of Consciousness And Attention",
                "abstract": "This paper presents the cognitive foundations and processes of consciousness and attention. It explores one of the profound myths in cognitive informatics, brain science, and intelligence science, and explains how abstract consciousness is generated by physical and physiological organs. Consciousness as the sense of self and the sign of life is systematically studied. The hierarchical levels of consciousness and its generation are analyzed. Then, the mathematical models and cognitive process of consciousness are developed that elaborate the nature of consciousness with a computational intelligence treatment. As an important part of consciousness, attention is formally modeled in mathematics and the cognitive process of attention is elaborated.",
                "id": "011168"
            },
            {
                "title": "The Latest Development on Cognitive Informatics",
                "abstract": "Cognitive informatics (CI) is a cutting-edge and profound interdisciplinary research area that tackles the common root problems\n and foundations of modern informatics, computation, software engineering, AI, and life sciences. CI is a new frontier that\n studies internal information processing mechanisms and processes of the brain, and their applications in computing, software,\n and ICT industries. Conventional information theory (Shannon, 1948) treated information as a measure of the probability of\n messages received from a channel. It was focused on information transmission rather than information itself. Modern information\n theory perceives information as any aspect of the natural world that can be abstracted and digitally represented. With this\n orientation, information is regarded as entities of messages, rather than a probabilistic measurement of them as in the classical\n information theory. However, it is recognized that current information theories are still in a category of external informatics.\n Complementing to these, there is a whole range of extremely interesting new research areas known as CI, a term which was coined\n by the author in 2001. I think, therefore I am (Rene Descartes, 1596\u20131650).\u2019 CI draws the attention of research on the internal\n information processing mechanisms of the brain, which are perceived as the foundation of next generation IT and software technologies,\n such as neural computers, bio-computers, novel methodologies of software engineering, quantum information processing, and\n powerful ICT systems. This talk focuses on the natural information processing mechanisms and cognitive processes. The latest\n development on CI at the First IEEE International Conference on Cognitive Informatics (ICCI\u201902) will be reported. At heoretical\n framework of CI as an emerging and interdisciplinary research area will be presented. The objectives of this talk are to draw\n attention of researchers, practitioners and graduate students on the investigation of cognitive mechanisms and processes of\n human information processing, and to stimulate the collaborative international effort on CI research and engineering applications.\n ",
                "id": "011169"
            },
            {
                "title": "On mathematical laws of software",
                "abstract": "Recent studies on the laws and mathematical constraints of softwarehave resulted in fundamental discoveries in computing and software engineeringtoward exploring the nature of software. It was recognized that software isnot constrained by any physical laws discovered in the natural world. However,software obeys the laws of mathematics, cognitive informatics, system science,and formal linguistics. This paper investigates into the mathematical laws ofsoftware and computing behaviors. A generic mathematical model of programsis created that reveals the nature of software as abstract processes and itsuniqueness beyond other mathematical entities such as sets, relations, functions,and abstract concepts. A comprehensive set of mathematical laws for softwareand its behaviors is established based on the generic mathematical model ofprograms and the fundamental computing behaviors elicited in Real-Time ProcessAlgebra (RTPA). A set of 95 algebraic laws of software behaviors is systematicallyderived, which encompasses the laws of meta-processes, processrelations, and system compositions. The comprehensive set of mathematicallaws of software lays a theoretical foundation for analyzing and modeling softwarebehaviors and software system architectures, as well as for guiding rigorouspractice in programming. They are also widely applicable for the rigorousmodeling and manipulation of human cognitive processes and computational intelligentbehaviors.",
                "id": "011170"
            },
            {
                "title": "Design and Implementation of an Autonomic Code Generator Based on RTPA",
                "abstract": "Real-Time Process Algebra RTPA is a denotational mathematics for the algebraic modeling and manipulations of software system architectures and behaviors by the Unified Data Models UDMs and Unified Process Models UPMs. On the basis of the RTPA specification and refinement methodologies, automatic software code generation is enabled toward improving software development productivity. This paper examines designing and developing the RTPA-based software code generator RTPA-CG that transfers system models in RTPA architectures and behaviors into C++ or Java. A two-phrase strategy has been employed in the design of the code generator. The first phrase analyzes the lexical, syntactical, and type specifications of a software system modeled in RTPA, which results in a set of abstract syntax trees ASTs. The second phrase translates the ASTs into C++ or Java based on predesigned mapping strategies and code generation rules. The toolkit of RTPA code generator encompasses an RTPA lexer, parser, type-checker, and a code builder. Experimental results show that system models in RTPA can be rigorously processed and corresponding C++/Java code can be automatically generated using the toolkit. The code generated is executable and effective under the support of an RTPA run-time library.",
                "id": "011171"
            },
            {
                "title": "Big Data Analytics: A Cognitive Perspectives.",
                "abstract": "Big data are pervasively generated by human cognitive processes, formal inferences, and system quantifications. This paper presents the cognitive foundations of big data systems towards big data science. The key perceptual model of big data systems is the recursively typed hyperstructure RTHS. The RTHS model reveals the inherited complexities and unprecedented difficulty in big data engineering. This finding leads to a set of mathematical and computational models for efficiently processing big data systems. The cognitive relationship between data, information, knowledge, and intelligence is formally described.",
                "id": "011172"
            }
        ]
    },
    {
        "id": "012",
        "input": "For an author who has written the paper with the title \"Using Magnetic RAM to Build Low-Power and Soft Error-Resilient L1 Cache\", which reference is related? Just answer with [1] or [2] without explanation. [1]: \"Severless Search and Authentication Protocols for RFID\" [2]: \"Dynamic Backlight Scaling Optimization: A Cloud-Based Energy-Saving Service for Mobile Streaming Applications\"",
        "profile": [
            {
                "title": "Improving multi-level NAND flash memory storage reliability using concatenated TCM-BCH coding",
                "abstract": "By storing multiple bits in each memory cell, multi-level per cell (MLC) NAND flash memories have been increasingly dominant in the flash memory market due to their obvious storage density advantage. However, MLC NAND flash memories are much more subject to storage reliability degradation as the technology continues to scale down. This paper presents an error correcting solution by concatenating trellis coded modulation (TCM) with an outer BCH code, which can greatly improve the performance compared with the current design practice that uses BCH codes only. The key is that TCM can well match to the multi-level storage characteristic in order to reduce the memory bit error rate and hence relieve the burden of outer BCH code, at no cost of extra redundant memory cells. The superior error correcting performance of such concatenated TCM-BCH coding systems for MLC NAND flash memories has been well demonstrated through computer simulations, and their silicon implementation efficiency has been evaluated through ASIC design at 65nm node.",
                "id": "0120"
            },
            {
                "title": "Exploiting workload dynamics to improve SSD read latency via differentiated error correction codes",
                "abstract": "This article presents a cross-layer codesign approach to reduce SSD read response latency. The key is to cohesively exploit the NAND flash memory device write speed vs. raw storage reliability trade-off at the physical layer and runtime data access workload dynamics at the system level. Leveraging runtime data access workload variation, we can opportunistically slow down NAND flash memory write speed and hence improve NAND flash memory raw storage reliability. This naturally enables an opportunistic use of weaker error correction schemes that can directly reduce SSD read access latency. We develop a disk-level scheduling scheme to effectively smooth the write workload in order to maximize the occurrence of runtime opportunistic NAND flash memory write slowdown. Using 2 bits/cell NAND flash memory with BCH-based error correction correction as a test vehicle, we carry out extensive simulations over various workloads and demonstrate that this developed cross-layer co-design solution can reduce the average SSD read latency by up to 59.4&percnt; without sacrificing the write throughput performance.",
                "id": "0121"
            },
            {
                "title": "Error Rate-Based Wear-Leveling for nand Flash Memory at Highly Scaled Technology Nodes",
                "abstract": "This brief presents a nand Flash memory wear-leveling algorithm that explicitly uses memory raw bit error rate (BER) as the optimization target. Although nand Flash memory wear-leveling has been well studied, all the existing algorithms aim to equalize the number of programming/erase cycles among all the memory blocks. Unfortunately, such a conventional design practice becomes increasingly suboptimal as inter-block variation becomes increasingly significant with the technology scaling. This brief presents a dynamic BER-based greedy wear-leveling algorithm that uses BER statistics as the measurement of memory block wear-out pace, and guides dynamic memory block data swapping to fully maximize the wear-leveling efficiency. Simulations have been carried out to quantitatively demonstrate its advantages over existing wear-leveling algorithms.",
                "id": "0122"
            },
            {
                "title": "Using time-aware memory sensing to address resistance drift issue in multi-level phase change memory",
                "abstract": "Because of its great scalability potential and support of multi-level per cell storage, phase change memory has become a topic of great current interest. However, recent studies show that structural relaxation effect makes the resistance of phase change material drift over the time, which can severely degrade multi-level phase change memory storage reliability. This paper studies the potential of using a time-aware memory sensing strategy to address this challenge. The basic idea is to keep track of memory content lifetime and, when memory is being read, accordingly adjust the memory sensing configuration to minimize the negative impact of time-dependent resistance drift on memory storage reliability. Because multi-level phase change memory may demand the use of powerful error correction code (ECC) whose decoding can request either hard-decision or soft-decision log-likelihood (LLR) memory sensing, we discuss both hard-decision and soft-decision time-aware memory sensing in details. Using BCH code and LDPC code as ECC for 4-level/cell and 8-level/cell phase change memory, we carry out simulations and the results show that, compared with time-independent static memory sensing, time-aware memory sensing can increase allowable memory content lifetime by several orders of magnitude.",
                "id": "0123"
            },
            {
                "title": "Efficient implementation of decoupling capacitors in 3D processor-dram integrated computing systems",
                "abstract": "Three-dimensional (3D) integration of a single high performance microprocessor die and multiple DRAM dies has been considered as a viable option to tackle the looming memory wall problem. Meanwhile, on-chip decoupling capacitors are becoming increasingly important to ensure power delivery integrity, particularly for high-performance integrated circuits. Targeting at 3D processor-DRAM integrated computing systems, this paper proposes to use 3D stacked DRAM dies to provide decoupling capacitors for the processor die. This can well leverage the superior capacitor fabrication ability of DRAM to eliminate the area penalty of decoupling capacitor insertion on the processor die. For its practical implementation, a simple uniform decoupling capacitor network design strategy is presented, and circuit SPICE simulations and computer system simulations are carried out to quantitatively demonstrate the effectiveness and illustrate various design trade-offs.",
                "id": "0124"
            },
            {
                "title": "Systematic Design of Original and Modified Mastrovito Multipliers for General Irreducible Polynomials",
                "abstract": "This paper considers the design of bit-parallel dedicated finite field multipliers using standard basis. An explicit algorithm is proposed for efficient construction of Mastrovito product matrix, based on which we present a systematic design of Mastrovito multiplier applicable to $GF(2^m)$ generated by an arbitrary irreducible polynomial. This design effectively exploits the spatial correlation of elements in Mastrovito product matrix to reduce the complexity. Using a similar methodology, we propose a systematic design of modified Mastrovito multiplier, which is suitable for $GF(2^m)$ generated by high-Hamming weight irreducible polynomials. For both original and modified Mastrovito multipliers, the developed multiplier architectures are highly modular, which is desirable for VLSI hardware implementation. Applying the proposed algorithm and design approach, we study the Mastrovito multipliers for several special irreducible polynomials, such as trinomial and equally-spaced-polynomial, and the obtained complexity results match the best known results. Moreover, we have discovered several new special irreducible polynomials which also lead to low-complexity Mastrovito multipliers.",
                "id": "0125"
            },
            {
                "title": "Computation Error Analysis in Digital Signal Processing Systems With Overscaled Supply Voltage",
                "abstract": "It has been recently demonstrated that digital signal processing systems may possibly leverage unconventional voltage overscaling (VOS) to reduce energy consumption while maintaining satisfactory signal processing performance. Due to the computation-intensive nature of most signal processing algorithms, the energy saving potential largely depends on the behavior of computer arithmetic units in response to overscaled supply voltage. This paper shows that different hardware implementations of the same computer arithmetic function may respond to VOS very differently and result in different energy saving potentials. Therefore, the selection of appropriate computer arithmetic architecture is an important issue in voltage-overscaled signal processing system design. This paper presents an analytical method to estimate the statistics of computer arithmetic computation errors due to supply voltage overscaling. Compared with computation-intensive circuit simulations, this analytical approach can be several orders of magnitude faster and can achieve a reasonable accuracy. This approach can be used to choose the appropriate computer arithmetic architecture in voltage-overscaled signal processing systems. Finally, we carry out case studies on a coordinate rotation digital computer processor and a finite-impulse-response filter to further demonstrate the importance of choosing proper computer arithmetic implementations.",
                "id": "0126"
            },
            {
                "title": "Using Magnetic RAM to Build Low-Power and Soft Error-Resilient L1 Cache",
                "abstract": "Due to its great scalability, fast read access, low leakage power, and nonvolatility, magnetic random access memory (MRAM) appears to be a promising memory technology for on-chip cache memory in microprocessors. However, the write-to-MRAM process is relatively slow and results in high dynamic power consumption. Such inherent disadvantages of MRAM make researchers easily conclude that MRAM can only be used for low-level caches (e.g., L2 or L3 cache), where cache memories are less frequently accessed and slow write to MRAM can be more easily compensated using simple architectural techniques. By developing a hybrid cache architecture, this paper attempts to show that, with appropriate architecture design, MRAM can also be used in L1 cache to improve both the energy efficiency and soft error immunity. The basic idea is to supplement the MRAM L1 cache with several small SRAM buffers, which can substantially mitigate the performance degradation and dynamic energy overhead induced by MRAM write operations. Moreover, the proposed hybrid cache architecture is also an efficient solution to protect cache memory from radiation-induced soft errors, as MRAM is inherently invulnerable to emissive particles. Simulation results show that, with only less than 2% performance degradation, the proposed design approach can reduce the power consumption by up to 76.1% on average compared with the traditional SRAM L1 cache. In addition, the architectural vulnerability factor of L1 data cache is reduced from 28.3% to as low as 0.5%.",
                "id": "0127"
            },
            {
                "title": "Design of voltage overscaled low-power trellis decoders in presence of process variations",
                "abstract": "In hardware implementations of many signal processing functions, timing errors on different circuit signals may have largely different importance with respect to the overall signal processing performance. This motivates us to apply the concept of unequal error tolerance to enable the use of voltage overscaling at minimal signal processing performance degradation. Realization of unequal error tolerance involves two main issues, including how to quantify the importance of each circuit signal and how to incorporate the importance quantification into signal processing circuit design. We developed techniques to tackle these two issues and applied them to two types of trellis decoders including Viterbi decoder for convolutional code decoding and Max-Log-Maximum A Posteriori (MAP) decoder for Turbo code decoding. Simulation results demonstrated promising energy saving potentials of the proposed design solution on both trellis decoding computation and memory storage at small decoding performance degradation.",
                "id": "0128"
            },
            {
                "title": "Improving Multi-Level NAND Flash Memory Storage Reliability Using Concatenated BCH-TCM Coding",
                "abstract": "By storing more than one bit in each memory cell, multi-level per cell (MLC) NAND flash memories are dominating global flash memory market due to their appealing storage density advantage. However, continuous technology scaling makes MLC NAND flash memories increasingly subject to worse raw storage reliability. This paper presents a memory fault tolerance design solution geared to MLC NAND flash memories. The basic idea is to concatenate trellis coded modulation (TCM) with an outer BCH code, which can greatly improve the error correction performance compared with the current design practice that uses BCH codes only. The key is that TCM can well leverage the multi-level storage characteristic to reduce the memory bit error rate and hence relieve the burden of outer BCH code, at no cost of extra redundant memory cells. The superior performance of such concatenated BCH-TCM coding systems for MLC NAND flash memories has been well demonstrated through computer simulations. A modified TCM demodulation approach is further proposed to improve the tolerance to static memory cell defects. We also address the associated practical implementation issues in case of using either single-page or multi-page programming strategy, and demonstrate the silicon implementation efficiency through application-specific integrated circuit design at 65 nm node.",
                "id": "0129"
            },
            {
                "title": "DRAM-based FPGA enabled by three-dimensional (3d) memory stacking (abstract only)",
                "abstract": "Motivated by the emerging three-dimensional (3D integration technologies, this paper studies the potential of applying 3D memory stacking to enable FPGA devices use on-chip DRAM cells to store configuration data. In current design practice, FPGAs do not use on-chip DRAM cells for configuration data storage mainly because on-chip DRAM self-refreshing involves destructive DRAM read operations. This problem can be solved if we use a 3D stacked memory as primary FPGA configuration data storage and externally refresh on-chip DRAM cells. Since the 3D stacked memory can easily store multiple sets of configuration data, it can meanwhile enable high-speed FPGA dynamic reconfiguration. In this paper, we study such DRAM-based FPGA design enabled by 3D memory stacking and investigate potential design issues, and employ the VPR tool set to demonstrate that DRAM-based FPGAs can noticeably reduce FPGA die area and hence improve speed and energy consumption performance, compared their SRAM-based counterparts.",
                "id": "01210"
            },
            {
                "title": "On the Use of Soft-Decision Error-Correction Codes in nand Flash Memory",
                "abstract": "As technology continues to scale down, NAND Flash memory has been increasingly relying on error-correction codes (ECCs) to ensure the overall data storage integrity. Although advanced ECCs such as low-density parity-check (LDPC) codes can provide significantly stronger error-correction capability over BCH codes being used in current practice, their decoding requires soft-decision log-likelihood ratio (LLR) information. This results in two critical issues. First, accurate calculation of LLR demands fine-grained memory-cell sensing, which nevertheless tends to incur implementation overhead and access latency penalty. Hence, it is critical to minimize the fine-grained memory sensing precision. Second, accurate calculation of LLR also demands the availability of a memory-cell threshold-voltage distribution model. As the major source for memory-cell threshold-voltage distribution distortion, cell-to-cell interference must be carefully incorporated into the model. However, these two critical issues have not been ever addressed in the open literature. This paper attempts to address these open issues. We derive mathematical formulations to approximately model the threshold-voltage distribution of memory cells in the presence of cell-to-cell interference, based on which the calculation of LLRs is mathematically formulated. This paper also proposes a nonuniform memory sensing strategy to reduce the memory sensing precision and, thus, sensing latency while still maintaining good error-correction performance. In addition, we investigate these design issues under the scenario when we can also sense interfering cells and hence explicitly estimate cell-to-cell interference strength. We carry out extensive computer simulations to demonstrate the effectiveness and involved tradeoffs, assuming the use of LDPC codes in 2-bits/cell NAND Flash memory.",
                "id": "01211"
            },
            {
                "title": "Design techniques to improve the device write margin for MRAM-based cache memory",
                "abstract": "As one promising non-volatile memory technology, magnetoresistive RAM (MRAM) based on magnetic tunneling junctions (MTJs) has recently attracted much attention. However, latest device research has discovered that, in order to maintain sufficient MTJ write margin to prevent device breakdown, MTJs will be subject to unconventionally high random write error rates (e.g., 10-3 and above) as memory cell size is being scaled down. This new discovery seriously threatens the scalability of MRAM, and the material/device research community is actively searching for solutions to largely reduce MTJ write error rates and meanwhile maintain sufficient device write margin. In this paper, we attempt to address this challenge from the architecture level when using MRAM to implement cache memory. In particular, we show that two simple cache architecture design techniques can be used to effectively tolerate high MTJ write error rates at small performance and implementation cost, which makes it much easier to maintain sufficient MTJ write margin and hence push the MRAM scalability envelope. Using the full system simulator PTLsim and a variety of benchmarks, we show that the proposed design techniques can readily accommodate MTJ write error rate up to 0.75% at the penalty of less than 4% processor performance degradation, less than 10% silicon area overhead, and 6% energy consumption overhead.",
                "id": "01212"
            },
            {
                "title": "Over-clocked SSD: Safely running beyond flash memory chip I/O clock specs",
                "abstract": "This paper presents a design strategy that enables aggressive use of flash memory chip I/O link over-clocking in solid-state drives (SSDs) without sacrificing storage reliability. The gradual wear-out and process variation of NAND flash memory makes the worst-case oriented error correction code (ECC) in SSDs largely under-utilized most of the time. This work proposes to opportunistically leverage under-utilized error correction strength to allow error-prone flash memory I/O link over-clocking. Its rationale and key design issues are presented and studied in this paper, and its potential effectiveness has been verified through hardware experiments and system simulations. Using sub-22nm NAND flash memory chips with I/O specs of 166MBps, we carried out extensive experiments and show that the proposed design strategy can enable SSDs safely operate with error-prone I/O link running at 275MBps. Trace-driven SSD simulations over a variety of workload traces show the system read response time can be reduced by over 20%.",
                "id": "01213"
            },
            {
                "title": "Self-timed dynamically pipelined adaptive signal processing system: a case study of DLMS equalizer for read channel.",
                "abstract": "Many pipelined adaptive signal processing systems are subject to a tradeoff between throughput and signal processing performance incurred by the pipelined adaptation feedback loops. In the conventional synchronous design regime, such throughput/performance tradeoff is typically fixed since the pipeline depth is usually determined in the design phase and remains unchanged in the run time. Neverthel...",
                "id": "01214"
            },
            {
                "title": "Low power soft-output signal detector design for wireless MIMO communication systems",
                "abstract": "Energy-efficient realization of soft-output signal detection is of great importance in emerging high-speed multiple-input multiple-output (MIMO) wireless communication systems. This paper presents three algorithm-level complexity-reduction techniques for soft-output detector design to achieve significant energy savings. To demonstrate their effectiveness, we designed a soft-output detector for 4-4 MIMO with 64-QAM using 65nm CMOS technology. While achieving near-optimum detection performance, the detector can support over 100Mbps throughput with only 0.24mm2 silicon area and 11mw power, leading to a \u00d710 improvement over the state of the art.",
                "id": "01215"
            },
            {
                "title": "Combined magnetic- and circuit-level enhancements for the nondestructive self-reference scheme of STT-RAM",
                "abstract": "A nondestructive self-reference read scheme (NSRS) was recently proposed to overcome the bit-to-bit variation in Spin-Transfer Torque Random Access Memory (STT-RAM). In this work, we introduced three magnetic- and circuit-level techniques, including 1) R-I curve skewing, 2) yield-driven sensing current selection, and 3) ratio matching to improve the sense margin and robustness of NSRS. The measurements of our 16Kb STT-RAM test chip show that compared to the original NSRS design, our proposed technologies successfully increased the sense margin by 2.5X with minimized impacts on the memory reliability and hardware cost.",
                "id": "01216"
            },
            {
                "title": "Low-Power State-Parallel Relaxed Adaptive Viterbi Decoder",
                "abstract": "Although it possesses reduced computational complexity and great power saving potential, conventional adaptive Viterbi algorithm implementations contain a global best survivor path metric search operation that prevents it from being directly implemented in a high-throughput state-parallel decoder. This limitation also incurs power and silicon area overhead. This paper presents a modified adaptive ...",
                "id": "01217"
            },
            {
                "title": "Architecture design exploration of three-dimensional (3D) integrated DRAM",
                "abstract": "Motivated by increasingly promising three-dimensional (3D) integration technologies, this paper reports an architecture design of 3D integrated dynamic RAM (DRAM). To accommodate the potentially significant pitch mismatch between DRAM word-line/bit-line and through silicon vias (TSVs) for 3D integration, this paper presents two modestly different coarse-grained inter-sub-array 3D DRAM architecture partitioning strategies. Furthermore, to mitigate the potential yield loss induced by 3D integration, we propose an interdie inter-sub-array redundancy repair approach to improve the memory repair success rate. For the purpose of evaluation, we modified CACTI 5 to support the proposed coarse-grained 3D partitioning strategies. Estimation results show that, for the realization of a 1Gb DRAM with 8 banks and 256-bit data I/O, such 3D DRAM design strategies can effectively reduce the silicon area, access latency, and energy consumption compared with 3D packaging with wire bonding and conventional 2D design. We further developed a memory redundancy repair simulator to demonstrate the effectiveness of proposed inter-die inter-subarray redundancy repair approach.",
                "id": "01218"
            },
            {
                "title": "A nondestructive self-reference scheme for Spin-Transfer Torque Random Access Memory (STT-RAM)",
                "abstract": "We proposed a novel self-reference sensing scheme for Spin-Transfer Torque Random Access Memory (STT-RAM) to overcome the large bit-to-bit variation of Magnetic Tunneling Junction (MTJ) resistance. Different from all the existing schemes, our solution is nondestructive: The stored value in the STT-RAM cell does NOT need to be overwritten by a reference value. And hence, long write-back operation (of the original stored value) is eliminated. The robustness analyses of the existing scheme and our proposed nondestructive scheme are also presented. The measurement results from a 16kb testing chip successfully confirmed the effectiveness of our technique.",
                "id": "01219"
            },
            {
                "title": "Architecting high-performance energy-efficient soft error resilient cache under 3D integration technology",
                "abstract": "Radiation-induced soft error has become an emerging reliability threat to high performance microprocessor design. As the size of on chip cache memory steadily increased for the past decades, resilient techniques against soft errors in cache are becoming increasingly important for processor reliability. However, conventional soft error resilient techniques have significantly increased the access latency and energy consumption in cache memory, thereby resulting in undesirable performance and energy efficiency degradation. The emerging 3D integration technology provides an attractive advantage, as the 3D microarchitecture exhibits heterogeneous soft error resilient characteristics due to the shielding effect of die stacking. Moreover, the 3D shielding effect can offer several inner dies that are inherently invulnerable to soft error, as they are implicitly protected by the outer dies. To exploit the invulnerability benefit, we propose a soft error resilient 3D cache architecture, in which data blocks on the soft error invulnerable dies have no protection against soft error, therefore, access to the data block on the soft error invulnerable die incurs a considerably reduced access latency and energy. Furthermore, we propose to maximize the access on the soft error invulnerable dies by dynamically moving data blocks among different dies, thereby achieving further performance and energy efficiency improvement. Simulation results show that the proposed 3D cache architecture can reduce the power consumption by up to 65% for the L1 instruction cache, 60% for the L1 data cache and 20% for the L2 cache, respectively. In general, the overall IPC performance can be improved by 5% on average.",
                "id": "01220"
            },
            {
                "title": "Design of low-power variation tolerant signal processing systems with adaptive finite word-length configuration",
                "abstract": "This paper concerns the design of low power digital signal processing integrated circuits in the presence of significant process variations. The basic idea is to leave smaller-than-worst-case timing margin for improving energy efficiency during the design phase and selectively reduce the finite word-length of circuit datapaths in post-silicon to eliminate all the timing faults during the run time. This simple idea can be intuitively justified by the fact that process variations may render only a few post-silicon datapaths to timing faults, while reducing the finite word-length of a few datapaths in signal processing systems may not necessarily make the overall algorithm-level performance unacceptable in run time. We present a design flow to implement this method and propose a dual finite word-length configuration strategy to simplify its real-life realization. Using linear low-pass filter and Turbo code decoder design at 45 nm node as case studies, we quantitatively demonstrate that this adaptive finite word-length configuration design strategy may effectively relax the timing margin and accordingly reduce the power consumption by over 18% over conventional worst-case design approach.",
                "id": "01221"
            },
            {
                "title": "Improving STT MRAM storage density through smaller-than-worst-case transistor sizing",
                "abstract": "This paper presents a technique to improve the storage density of spin-torque transfer (STT) magnetoresistive random access memory (MRAM) in the presence of significant magnetic tunneling junction (MTJ) write current threshold variability. In conventional design practice, the nMOS transistor within each memory cell is sized to be large enough to carry a current larger than the worst-case MTJ write current threshold, leading to an increasing storage density penalty as the technology scales down. To mitigate such variability-induced storage density penalty, this paper presents a smaller-than-worst-case transistor sizing approach with the underlying theme of jointly considering memory cell transistor sizing and defect tolerance. Its effectiveness is demonstrated using 256 Mb STT MRAM design at 45 nm node as a test vehicle. Results show that, under a normalized write current threshold deviation of 20%, the overall memory die size can be reduced by more than 20% compared with the conventional worst-case transistor sizing design practice.",
                "id": "01222"
            },
            {
                "title": "Exploiting memory device wear-out dynamics to improve NAND flash memory system performance",
                "abstract": "This paper advocates a device-aware design strategy to improve various NAND flash memory system performance metrics. It is well known that NAND flash memory program/erase (PE) cycling gradually degrades memory device raw storage reliability, and sufficiently strong error correction codes (ECC) must be used to ensure the PE cycling endurance. Hence, memory manufacturers must fabricate enough number of redundant memory cells geared to the worst-case device reliability at the end of memory lifetime. Given the memory device wear-out dynamics, the existing worst-case oriented ECC redundancy is largely under-utilized over the entire memory lifetime, which can be adaptively traded for improving certain NAND flash memory system performance metrics. This paper explores such device-aware adaptive system design space from two perspectives, including (1) how to improve memory program speed, and (2) how to improve memory defect tolerance and hence enable aggressive fabrication technology scaling. To enable quantitative evaluation, we for the first time develop a NAND flash memory device model to capture the effects of PE cycling from the system level. We carry out simulations using the DiskSim-based SSD simulator and a variety of traces, and the results demonstrate up to 32% SSD average response time reduction. We further demonstrate that the potential on achieving very good defect tolerance, and finally show that these two design approaches can be readily combined together to noticeably improve SSD average response time even in the presence of high memory defect rates.",
                "id": "01223"
            },
            {
                "title": "Impacts Of Though-Dram Vias In 3d Processor-Dram Integrated Systems",
                "abstract": "As a promising option to address the memory wall problem, 3D processor-DRAM integration has recently received many attentions. Since DRAM tiers must be stacked between the processor tier and package substrate, we must fabricate a large number of through-DRAM through-silicon vias (TSVs) to connect the processor tier and package for power and I/O signal delivery. Although such through-DRAM TSVs will inevitably Interfere with DRAM design and induce non-negligible power consumption overhead, little research has been done to study how to allocate these TSVs on the DRAM tiers and analyze their impacts. To address this open issue, this paper first presents a through-DRAM TSV allocation strategy that fits well to the regular DRAM architecture. To demonstrate this design strategy and evaluate trade-offs involved, we develop a CACTI-based modeling tool to carry out extensive simulations over a wide range of design parameters.",
                "id": "01224"
            },
            {
                "title": "Overclocking nand Flash Memory I/O Link in LDPC-Based SSDs",
                "abstract": "Due to the increasingly significant process variation and gradual Flash memory cell wear-out, the worst-case-oriented error correction code (ECC) in solid-state drives (SSDs) is mostly underutilized throughout the entire lifetime. Error-prone overclocking of Flash memory chip I/O links can trade such ECC underutilization for opportunistically improving SSD speed performance, and its effectiveness strongly depends on how well the ECC decoding can handle the overclocking-induced I/O link errors. As SSDs are quickly adopting low-density parity-check (LDPC) code, this brief concerns LPDC-based overclocked SSDs. Experiments with 20-nm nand Flash memory chips reveal unique bit error characteristics of the overclocked I/O link, based upon which this brief develops solutions that can leverage the error characteristics to improve LDPC decoding performance. Results show that the developed techniques can reduce LDPC code decoding power consumption by 60% and reduce the decoding failure rate by over two orders of magnitude.",
                "id": "01225"
            },
            {
                "title": "DiffECC: Improving SSD Read Performance Using Differentiated Error Correction Coding Schemes",
                "abstract": "This paper presents a cross-layer co-design approach to reduce SSD read response latency. The key is to cohesively exploit the NAND flash memory device write speed vs. raw storage reliability trade-off at the physical layer and run-time data access workload variation at the system level. Leveraging run-time data access workload variation, we can opportunistically slow down NAND flash memory write speed and hence improve NAND flash memory raw storage reliability. This naturally enables an opportunistic use of weaker error correction schemes that can directly reduce SSD read access latency. We develop a disk-level scheduling scheme to effectively smooth the write workload in order to maximize the occurrence of run-time opportunistic NAND flash memory write slow down. Using 2 bits/cell NAND flash memory with BCH-based error correction correction as a test vehicle, we carry out extensive simulations over various workloads and demonstrate that this developed cross-layer co-design solution can reduce the average SSD read latency by up to 96%.",
                "id": "01226"
            },
            {
                "title": "3D DRAM Design and Application to 3D Multicore Systems",
                "abstract": "Editor's note:From a system architecture perspective, 3D technology can satisfy the high memory bandwidth demands that future multicore/manycore architectures require. This article presents a 3D DRAM architecture design and the potential for using 3D DRAM stacking for both L2 cache and main memory in 3D multicore architecture.\u2014Yuan Xie, Pennsylvania State University",
                "id": "01227"
            },
            {
                "title": "LDPC Decoding with Limited-Precision Soft Information in Flash Memories",
                "abstract": "  This paper investigates the application of low-density parity-check (LDPC) codes to Flash memories. Multiple cell reads with distinct word-line voltages provide limited-precision soft information for the LDPC decoder. The values of the word-line voltages (also called reference voltages) are optimized by maximizing the mutual information (MI) between the input and output of the multiple-read channel. Constraining the maximum mutual-information (MMI) quantization to enforce a constant-ratio constraint provides a significant simplification with no noticeable loss in performance.   Our simulation results suggest that for a well-designed LDPC code, the quantization that maximizes the mutual information will also minimize the frame error rate. However, care must be taken to design the code to perform well in the quantized channel. An LDPC code designed for a full-precision Gaussian channel may perform poorly in the quantized setting. Our LDPC code designs provide an example where quantization increases the importance of absorbing sets thus changing how the LDPC code should be optimized.   Simulation results show that small increases in precision enable the LDPC code to significantly outperform a BCH code with comparable rate and block length (but without the benefit of the soft information) over a range of frame error rates. ",
                "id": "01228"
            },
            {
                "title": "Proximate control stream assisted video transcoding for heterogeneous content delivery network",
                "abstract": "Video transcoding can be used to facilitate video streaming in content delivery network. A concept of control stream assisted transcoding has been recently proposed aiming to reduce transcoding computational complexity at the cost of data storage/transmission overhead, and the control stream is obtained by directly removing the residual information from the target video bitstream of transcoding. However, it is subject to relatively significant storage/transmission overhead, and more importantly does not well match to the increasingly heterogeneous networking environment with varying computation/storage/transmission resources at different nodes. This work presents a proximate control stream assisted transcoding design strategy that can reduce the control stream size and enable a large storage/transmission vs. computational complexity trade-off design space. Experiments demonstrate its effectiveness and noticeable advantages over other alternatives including simulcast and SVC (scalable video coding).",
                "id": "01229"
            },
            {
                "title": "Using Quasi-EZ-NAND Flash Memory to Build Large-Capacity Solid-State Drives in Computing Systems",
                "abstract": "Future flash-based solid-state drives (SSDs) must employ increasingly powerful error correction code (ECC) and digital signal processing (DSP) techniques to compensate the negative impact of technology scaling on NAND flash memory device reliability. Currently, all the ECC and DSP functions are implemented in a central SSD controller. However, the use of more powerful ECC and DSP makes such design practice subject to significant speed performance degradation and complicated controller implementation. An EZ-NAND (Error Zero NAND) flash memory design strategy is emerging in the industry, which moves all the ECC and DSP functions to each memory chip. Although EZ-NAND flash can simplify controller design and achieve high system speed performance, its high silicon cost may not be affordable for large-capacity SSDs in computing systems. We propose a quasi-EZ-NAND design strategy that hierarchically distributes ECC and DSP functions on both NAND flash memory chips and the central SSD controller. Compared with EZ-NAND design concept, it can maintain almost the same speed performance while reducing silicon cost overhead. Assuming the use of low-density parity-check (LDPC) code and postcompensation DSP technique, trace-based simulations show that SSDs using quasi-EZ-NAND flash can realize almost the same speed as SSDs using EZ-NAND flash, and both can reduce the average SSD response time by over 90 percent compared with conventional design practice. Silicon design at 65 nm node shows that quasi-EZ-NAND can reduce the silicon cost overhead by up to 44 percent compared with EZ-NAND.",
                "id": "01230"
            },
            {
                "title": "Spin-transfer torque magnetoresistive content addressable memory (CAM) cell structure design with enhanced search noise margin",
                "abstract": "This paper presents a new memory cell structure for content addressable memory (CAM) based on magnetic tunneling junction (MTJ). Each CAM cell uses a pair of differential MTJs as basic storage element and incorporates transistors to greatly improve the cell search noise margin at low sensing current. Using the same design principle, we further develop an area-efficient cell structure for ternary CAM (TCAM), which occupies about 25% less area compared with directly using two CAM cells to form one TCAM cell. The effectiveness of the proposed CAM and TCAM cell structures has been demonstrated by circuit simulation at 0.18 mum CMOS technology.",
                "id": "01231"
            },
            {
                "title": "Exploring QoE for Power Efficiency: A Field Study on Mobile Videos with LCD Displays",
                "abstract": "Display power consumption has become a major concern for both mobile users and design engineers, especially considering the prevalence of today's video-rich mobile services. The power consumption of liquid crystal display (LCD), a dominant mobile display technology, can be reduced by dynamic backlight scaling (DBS). However, such dynamic changes of screen brightness may degrade users' quality of experience (QoE) in viewing videos. How would QoE be impacted by different DBS strategies has not yet been understood clearly and thus obscures the way to achieve systematic power saving. In this paper, we take a first step to explore the QoE of DBS on smartphones and aim at maximally enhancing the display power performance without negatively impacting users' QoE. In particular, we conduct three motivational studies to uncover the inherent relationship between QoE and backlight scaling frequency, magnitude, and temporal consistency, respectively. Motivated by the findings of these studies, we design a suite of techniques to implement a comprehensive DBS strategy. We demonstrate an example application of the proposed DBS designs in a mobile video streaming system. Measurements and user evaluations show that more than 40% system power reduction, or equivalently, 20% more power savings than the non-QoE approaches, can be achieved without QoE impairment.",
                "id": "01232"
            },
            {
                "title": "Using Lifetime-Aware Progressive Programming to Improve SLC NAND Flash Memory Write Endurance",
                "abstract": "This paper advocates a lifetime-aware progressive programming concept to improve single-level per cell NAND flash memory write endurance. NAND flash memory program/erase (P/E) cycling gradually degrades memory cell storage noise margin, and sufficiently strong fault tolerance must be used to ensure the memory P/E cycling endurance. As a result, the relatively large cell storage noise margin in early memory lifetime is essentially wasted in conventional design practice. This paper proposes to always fully utilize the available cell storage noise margin by adaptively adjusting the number of storage levels per cell, and progressively use these levels to realize multiple 1-bit programming operations between two consecutive erase operations. This simple progressive programming design concept is realized by two different implementation strategies, which are discussed and compared in detail. On the basis of an approximate NAND flash memory device model, we carried out simulations to quantitatively evaluate this design concept. The results show that it can improve the write endurance by 35.9% and in the meanwhile improve the average programming speed by 12% without sacrificing read speed.",
                "id": "01233"
            },
            {
                "title": "Leveraging Progressive Programmability of SLC Flash Pages to Realize Zero-overhead Delta Compression for Metadata Storage",
                "abstract": "This paper presents a method to implement delta compression for metadata storage in flash memory. With the abundant temporal redundancy in metadata, it is very intuitive to expect flash-based metadata storage can significantly benefit from delta compression. However, straightforward realization of delta compression demands the storage of the original data and the deltas among different versions in different flash memory physical pages, which leads to significant overhead in terms of read/write latency and data management complexity. Through experiments with 20nm NAND flash memory chips, we observed that, when operating in SLC mode, flash memory page can be programmed in a progressive manner, i.e., different portion of the same SLC flash memory page can be programmed at different time. This motivates us to propose a simple design approach that can realize delta compression for metadata storage without latency and data management complexity overheads. The key idea is to allocate SLC-mode flash memory pages for metadata, and store the original data and all the subsequent deltas in the same physical page through progressive programming. Experimental results show that this approach can significantly reduce the metadata write traffic without any latency overhead.",
                "id": "01234"
            },
            {
                "title": "Realizing Unequal Error Correction for nand Flash Memory at Minimal Read Latency Overhead",
                "abstract": "In nand Flash memory, all pages have the same storage capacity and hence accommodate the same amount of redundancy in support of error correction. In current practice, user data in all the pages are protected by the same error correction code. However, different types of pages in multibit per cell memory have largely different bit error rates, for which appropriate unequal error correction can achieve a better utilization of memory redundancy and hence improve program/erase (P/E) cycling endurance. Nevertheless, a straightforward realization of unequal error correction suffers from severe memory read latency penalty. This brief presents a design strategy to implement unequal error correction through concatenated coding, which can well match the unequal error rates among different types of pages at minimal memory read latency penalty. Based on measurement results from commercial sub-22-nm 2 bits/cell nand Flash memory chips, we carried out simulations from both the coding and storage system perspectives, and the results show that this design strategy can improve the P/E cycling endurance by 20% and only incur less than 7% increase of storage system read response time at the end of Flash memory lifetime with the P/E cycling of around 1800.",
                "id": "01235"
            },
            {
                "title": "Improving 3D DRAM Fault Tolerance Through Weak Cell Aware Error Correction.",
                "abstract": "Although the emerging 3D DRAM products can significantly improve the computing system performance, the relatively high cost is one of the most critical issues that prevent their wide real-life adoption. Intuitively, a strong memory fault tolerance can be leveraged to reduce the fabrication cost of DRAM dies, and the total cost will reduce if the fabrication cost saving can off-set the cost overhead of memory fault tolerance. Nevertheless, such a simple concept can be a practically viable option only for 3D DRAM because: (1) The stacked logic die can solely implement memory fault tolerance inside 3D DRAM chips, obviating any changes on the host CPUs and CPU-DRAM interfaces. (2) With the total ownership of both the logic die and DRAM dies inside 3D DRAM chips, DRAM manufacturers can fully exploit the potential to truly minimize the 3D DRAM bit cost. Following this intuition, we developed a 3D DRAM fault tolerance design strategy. It can achieve a very strong tolerance to weak DRAM cells at very small redundancy and latency overhead. The key is to cohesively leverage the detectability of weak cells and runtime configurability of error correction code (ECC) decoding. In addition, this design strategy can gracefully embrace the inaccuracy of weak cell detection (e.g., weak cell miss-detection and false-detection). We carried out thorough mathematical analysis, and the results show that, under the redundancy overhead of 1:8 (same as today\u2019s ECC DIMM), this design strategy can tolerate the weak cell rate of as high as $10^{-4}$ and 6 $\\\\times 10^{-5}$ if 100 and 90 percent of all the weak cells are known in prior. Using Micron\u2019s hybrid memory cube (HMC) 3D DRAM chips as the test vehicle, we evaluated the implementation cost and the results show that it only consumes less than 0.4 mm2 (45 nm node) on the logic die. Using CPU and DRAM simulators, we further carried out simulations over a variety of computing benchmarks and the results show that this design solution only incurs less than 2 percent performance degradation on average.",
                "id": "01236"
            },
            {
                "title": "Volume and Surface Area Distributions of Cracks in Concrete",
                "abstract": "Volumetric images of small mortar samples under load are acquired by X-ray microtomography. The images are binarized at many different threshold values, and over a million connected components are extracted at each threshold with a new, space and time efficient program. The rapid increase in the volume and surface area of the foreground components (cracks and air holes) is explained in terms of a simple model of digitization. Analysis of the data indicates that the foreground consists of thin, convoluted manifolds with a complex network topology, and that the crack surface area, whose increase with strain must correspond to the external work, is higher than expected.",
                "id": "01237"
            },
            {
                "title": "On the selection of arithmetic unit structure in voltage overscaled soft digital signal processing",
                "abstract": "A soft digital signal processing (DSP) design paradigm has been recently proposed to reduce the energy consumption of DSP systems through voltage overscaling. This paper shows that the selection of arithmetic unit structure can be an important and non-trivial issue in soft DSP system design. We present an optimal formulation and propose sub-optimal low-complexity approximations for selecting the appropriate arithmetic unit structure in voltage overscaled signal processing systems. We further present a case study on choosing the appropriate MAC (multiply-accumulate) structure in voltage overscaled FIR (finite impulse response) filter.",
                "id": "01238"
            },
            {
                "title": "Reducing DRAM Image Data Access Energy Consumption in Video Processing",
                "abstract": "This paper presents domain-specific techniques to reduce DRAM energy consumption for image data access in video processing. In mobile devices, video processing is one of the most energy-hungry tasks, and DRAM image data access energy consumption becomes increasingly dominant in overall video processing system energy consumption. Hence, it is highly desirable to develop domain-specific techniques that can exploit unique image data access characteristics to improve DRAM energy efficiency. Nevertheless, prior efforts on reducing DRAM energy consumption in video processing pale in comparison with that on reducing video processing logic energy consumption. In this work, we first apply three simple yet effective data manipulation techniques that exploit image data spatial/temporal correlation to reduce DRAM image data access energy consumption, then propose a heterogeneous DRAM architecture that can better adapt to unbalanced image access in most video processing to further improve DRAM energy efficiency. DRAM modeling and power estimation have been carried out to evaluate these domain-specific design techniques, and the results show that they can reduce DRAM energy consumption by up to 92%.",
                "id": "01239"
            },
            {
                "title": "A class of efficient-encoding generalized low-density parity-check codes",
                "abstract": "In this paper, we investigate an efficient encoding approach for generalized low-density (GLD) parity check codes, a generalization of Gallager's (1962, 1963) low-density parity check (LDPC) codes. We propose a systematic approach to construct an approximate upper triangular GLD parity check matrix which defines a class of efficient-encoding GLD codes. It is shown that such GLD codes have equally good performance. By effectively exploiting structure sharing in the encoding process, we also present a hardware/software codesign for practical encoder implementation of these efficient-encoding GLD codes",
                "id": "01240"
            },
            {
                "title": "3-D Data Storage, Power Delivery, and RF/Optical Transceiver\u2014Case Studies of 3-D Integration From System Design Perspectives",
                "abstract": "Three-dimensional (3-D) integration of systems by vertically stacking and interconnecting multiple materials, technologies, and functional components offers a wide range of benefits, including speed, bandwidth and density increase, power reduction, small form factor, packaging reduction, yield and reliability increase, flexible heterogeneous integration with multifunctionality, and overall cost re...",
                "id": "01241"
            },
            {
                "title": "Optimizing the Use of STT-RAM in SSDs Through Data-Dependent Error Tolerance",
                "abstract": "This brief presents a design strategy for spin-transfer torque (STT)-RAM to reduce the error-tolerance redundancy overhead and increase effective storage capacity without sacrificing its reliability. The key is to cohesively exploit the run-time data characteristics (e.g., access unit length and access frequency) and the fundamental read disturbance versus sensing error tradeoff in STT-RAM. It presents three specific data-dependent error-tolerance design techniques, and demonstrates their effectiveness in the context of using STT-RAM to replace DRAM in solid-state drives. Based on detailed modeling/simulations down to 22-nm node, we showed that these design solutions can increase the effective STT-RAM storage capacity by 26%, compared with conventional design practice.",
                "id": "01242"
            },
            {
                "title": "A 130 nm 1.2 V/3.3 V 16 Kb Spin-Transfer Torque Random Access Memory With Nondestructive Self-Reference Sensing Scheme",
                "abstract": "Among all the emerging memories, Spin-Transfer Torque Random Access Memory (STT-RAM) has demonstrated many promising features such as fast access speed, nonvolatility, excellent scalability, and compatibility to CMOS process. However, the large process variations of both magnetic tunneling junction (MTJ) and MOS transistors in the scaled technologies severely limit the yield of STT-RAM chips. In this work, we proposed a new sensing scheme, named as nondestructive self-reference sensing, or NSRS, for STT-RAM. By leveraging the different dependencies of the high and low resistance states of MTJs on the cell current amplitude, the proposed NSRS technique can work well at the scenario when bit-to-bit variation of MTJ resistances is large. Furthermore, we proposed three combined magnetic- and circuit-level techniques, including R-I curve skewing, yield-driven cell current selection, and ratio matching, to further improve the sense margin and robustness of NSRS sensing scheme. The measurement results of a 16 Kb STT-RAM test chip show that our proposed nondestructive self-reference sensing technique can reliably readout all the measured memory bits, of which 10% read failure rate was observed by using the conventional sensing technique. The three enhancement technologies ensure a 20 mV minimum sense margin and the whole sensing process can complete within 15 ns.",
                "id": "01243"
            },
            {
                "title": "Parallelism/regularity-driven MIMO detection algorithm design",
                "abstract": "Efficient VLSI implementation of multiple-input multiple-output (MIMO) detectors plays an important role in the real-life implementation of MIMO communication systems. However, most high-performance MIMO detection algorithms developed so far largely lack the operational parallelism and regularity that are essential for high-throughput and low-power VLSI implementations. Following the theme of parallelism/regularity-driven algorithm design, we propose hard/soft-output MIMO detection algorithms that have high operational parallelism and a regular/static data flow structure with fixed detection delay. Besides those properties desirable for VLSI implementations, such algorithms can achieve superior detection performance, as demonstrated in the simulations.",
                "id": "01244"
            },
            {
                "title": "Thermal-Assisted Spin Transfer Torque Memory (STT-RAM) Cell Design Exploration",
                "abstract": "Thermal-assisted spin-transfer torque random access memory (STT-RAM) has been considered as a promising candidate of next-generation nonvolatile memory technology. We conducted finite element simulation on thermal dynamics in the programming process of thermal-assisted STT-RAM. Special attentions have been paid to the scalability and design space of the thermal-assist programming scheme by varying the memory element dimension and resistance-area product. We also provide systematic analysis and comparison between the thermal-assisted STT-RAM and standard STT-RAM. Discussions on the writeability and scalability of thermal-assisted STT-RAM are also conducted.",
                "id": "01245"
            },
            {
                "title": "Using Lossless Data Compression in Data Storage Systems: Not for Saving Space",
                "abstract": "Lossless data compression for data storage has become less popular as mass data storage systems are becoming increasingly cheap. This leaves many files stored on mass data storage media uncompressed although they are losslessly compressible. This paper proposes to exploit the lossless compressibility of those files to improve the underlying storage system performance metrics such as energy efficiency and access speed, other than saving storage space as in conventional practice. The key idea is to apply runtime lossless data compression to enable an opportunistic use of a stronger error correction code (ECC) with more coding redundancy in data storage systems, and trade such opportunistic extra error correction capability to improve other system performance metrics in the runtime. Since data storage is typically realized in the unit of equal-sized sectors (e.g., 512 B or 4 KB user data per sector), we only apply this strategy to each individual sector independently in order to be completely transparent to the firmware, operating systems, and users. Using low-density parity check (LDPC) code as ECC in storage systems, this paper quantitatively studies the effectiveness of this design strategy in both hard disk drives and NAND flash memories. For hard disk drives, we use this design strategy to reduce average hard disk drive read channel signal processing energy consumption, and results show that up to 38 percent read channel energy saving can be achieved. For NAND flash memories, we use this design strategy to improve average NAND flash memory write speed, and results show that up to 36 percent write speed improvement can be achieved for 2 bits/cell NAND flash memories.",
                "id": "01246"
            },
            {
                "title": "Reducing Solid-State Storage Device Write Stress through Opportunistic In-place Delta Compression.",
                "abstract": "Inside modern SSDs, a small portion of MLC/TLC NAND flash memory blocks operate in SLC-mode to serve as write buffer/cache and/or store hot data. These SLC-mode blocks absorb a large percentage of write operations. To balance memory wear-out, such MLC/TLC-to-SLC configuration rotates among all the memory blocks inside SSDs. This paper presents a simple yet effective design approach to reduce write stress on SLC-mode flash blocks and hence improve the overall SSD lifetime. The key is to implement well-known delta compression without being subject to the read latency and data management complexity penalties inherent to conventional practice. The underlying theme is to leverage the partial programmability of SLC-mode flash memory pages to ensure that the original data and all the subsequent deltas always reside in the same memory physical page. To avoid the storage capacity overhead, we further propose to combine intra-sector lossless data compression with intra-page delta compression, leading to opportunistic in-place delta compression. This paper presents specific techniques to address important issues for its practical implementation, including data error correction, and intra-page data placement and management. We carried out comprehensive experiments, simulations, and ASIC (application-specific integrated circuit) design. The results show that the proposed design solution can largely reduce the write stress on SLC-mode flash memory pages without significant latency overhead and meanwhile incurs relatively small silicon implementation cost.",
                "id": "01247"
            },
            {
                "title": "How much can data compressibility help to improve NAND flash memory lifetime?",
                "abstract": "Although data compression can benefit flash memory lifetime, little work has been done to rigorously study the full potential of exploiting data compressibility to improve memory lifetime. This work attempts to fill this missing link. Motivated by the fact that memory cell damage strongly depends on the data content being stored, we first propose an implicit data compression approach (i.e., compress each data sector but do not increase the number of sectors per flash memory page) as a complement to conventional explicit data compression that aims to increase the number of sectors per flash memory page. Due to the runtime variation of data compressibility, each flash memory page almost always contains some unused storage space left by compressed data sectors. We develop a set of design strategies for exploiting such unused storage space to reduce the overall memory physical damage. We derive a set of mathematical formulations that can quantitatively estimate flash memory physical damage reduction gained by the proposed design strategies for both explicit and implicit data compression. Using 20nm MLC NAND flash memory chips, we carry out extensive experiments to quantify the content dependency of memory cell damage, based upon which we empirically evaluate and compare the effectiveness of the proposed design strategies under a wide spectrum of data compressibility characteristics.",
                "id": "01248"
            },
            {
                "title": "Exploiting Intracell Bit-Error Characteristics to Improve Min-Sum LDPC Decoding for MLC NAND Flash-Based Storage in Mobile Device.",
                "abstract": "A multilevel per cell (MLC) technique significantly improves the storage density, but also poses serious data integrity challenge for NAND flash memory. This consequently makes the low-density parity-check (LDPC) code and the soft-decision memory sensing become indispensable in the next-generation flash-based solid-state storage devices. However, the use of LDPC codes inevitably increases memory r...",
                "id": "01249"
            },
            {
                "title": "Enhanced Precision Through Multiple Reads for LDPC Decoding in Flash Memories",
                "abstract": "Multiple reads of the same Flash memory cell with distinct word-line voltages provide enhanced precision for LDPC decoding. In this paper, the word-line voltages are optimized by maximizing the mutual information (MI) of the quantized channel. The enhanced precision from a few additional reads allows frame error rate (FER) performance to approach that of full-precision soft information and enables an LDPC code to significantly outperform a BCH code. A constant-ratio constraint provides a significant simplification in the optimization with no noticeable loss in performance. For a well-designed LDPC code, the quantization that maximizes the mutual information also minimizes the FER in our simulations. However, for an example LDPC code with a high error floor caused by small absorbing sets, the MMI quantization does not provide the lowest frame error rate. The best quantization in this case introduces more erasures than would be optimal for the channel MI in order to mitigate the absorbing sets of the poorly designed code. The paper also identifies a trade-off in LDPC code design when decoding is performed with multiple precision levels; the best code at one level of precision will typically not be the best code at a different level of precision.",
                "id": "01250"
            },
            {
                "title": "Enabling NAND Flash Memory Use Soft-Decision Error Correction Codes at Minimal Read Latency Overhead",
                "abstract": "With the aggressive technology scaling and use of multi-bit per cell storage, NAND flash memory is subject to continuous degradation of raw storage reliability and demands more and more powerful error correction codes (ECC). This inevitable trend makes conventional BCH code increasingly inadequate, and iterative coding solutions such as LDPC codes become very natural alternative options. However, these powerful coding solutions demand soft-decision memory sensing, which results in longer on-chip memory sensing latency and memory-to-controller data transfer latency. Leveraging well-established lossless data compression theories, this paper presents several simple design techniques that can reduce such latency penalty caused by soft-decision ECCs. Their effectiveness have been well demonstrated through extensive simulations, and the results suggest that the latency can be reduced by up to 85.3%.",
                "id": "01251"
            },
            {
                "title": "Design of last-level on-chip cache using spin-torque transfer RAM (STT RAM)",
                "abstract": "Because of its high storage density with superior scalability, low integration cost and reasonably high access speed, spin-torque transfer random access memory (STT RAM) appears to have a promising potential to replace SRAM as last-level on-chip cache (e.g., L2 or L3 cache) for microprocessors. Due to unique operational characteristics of its storage device magnetic tunneling junction (MTJ), STT RAM is inherently subject to a write latency versus read latency tradeoff that is determined by the memory cell size. This paper first quantitatively studies how different memory cell sizing may impact the overall computing system performance, and shows that different computing workloads may have conflicting expectations on memory cell sizing. Leveraging MTJ device switching characteristics, we further propose an STT RAM architecture design method that can make STT RAM cache with relatively small memory cell size perform well over a wide spectrum of computing benchmarks. This has been well demonstrated using CACTI-based memory modeling and computing system performance simulations using SimpleScalar. Moreover, we show that this design method can also reduce STT RAM cache energy consumption by up to 30% over a variety of benchmarks.",
                "id": "01252"
            },
            {
                "title": "Facilitating Magnetic Recording Technology Scaling for Data Center Hard Disk Drives through Filesystem-Level Transparent Local Erasure Coding.",
                "abstract": "This paper presents a simple yet effective design solution to facilitate technology scaling for hard disk drives (HDDs) being deployed in data centers. Emerging magnetic recording technologies improve storage areal density mainly through reducing the track pitch, which however makes HDDs subject to higher read retry rates. More frequent HDD read retries could cause intolerable tail latency for large-scale systems such as data centers. To reduce the occurrence of costly read retry, one intuitive solution is to apply erasure coding locally on each HDD or JBOD (just a bunch of disks). To be practically viable, local erasure coding must have very low coding redundancy, which demands very long codeword length (e.g., one codeword spans hundreds of 4kB sectors) and hence large file size. This makes local erasure coding mainly suitable for data center applications. This paper contends that local erasure coding should be implemented transparently within filesystems, and accordingly presents a basic design framework and elaborates on important design issues. Meanwhile, this paper derives the mathematical formulations for estimating its effect on reducing HDD read tail latency. Using Reed-Solomon (RS) based erasure codes as test vehicles, we carried out detailed analysis and experiments to evaluate its implementation feasibility and effectiveness. We integrated the developed design solution into ext4 to further demonstrate its feasibility and quantitatively measure its impact on average speed performance of various big data benchmarks.",
                "id": "01253"
            },
            {
                "title": "Reducing latency overhead caused by using LDPC codes in NAND flash memory.",
                "abstract": "Semiconductor technology scaling makes NAND flash memory subject to continuous raw storage reliability degradation, leading to the demand for more and more powerful error correction codes. This inevitable trend makes conventional BCH code increasingly inadequate, and iterative coding solutions such as low-density parity-check (LDPC) codes become very natural alternative options. However, fine-grained soft-decision memory sensing must be used in order to fully leverage the strong error correction capability of LDPC codes, which results in significant data access latency overhead. This article presents a simple design technique that can reduce such latency overhead. The key is to cohesively exploit the NAND flash memory wear-out dynamics and impact of LDPC code structure on decoding performance. Based upon detailed memory device modeling and ASIC design, we carried out simulations to demonstrate the potential effectiveness of this design method and evaluate the involved trade-offs. \u00a9 2012 Zhao et al.; licensee Springer.",
                "id": "01254"
            },
            {
                "title": "Leveraging Access Locality for the Efficient Use of Multibit Error-Correcting Codes in L2 Cache",
                "abstract": "It is almost evident that SRAM-based cache memories will be subject to a significant degree of parametric random defects if one wants to leverage the technology scaling to its full extent. Although strong multibit error-correcting codes (ECC) appear to be a natural choice to handle a large number of random defects, investigation of their applications in cache remains largely missing arguably because it is commonly believed that multibit ECC may incur prohibitive performance degradation and silicon/energy cost. By developing a cost-effective L2 cache architecture using multibit ECC, this paper attempts to show that, with appropriate cache architecture design, this common belief may not necessarily hold true for L2 cache. The basic idea is to supplement a conventional L2 cache core with several special-purpose small caches/buffers, which can greatly reduce the silicon cost and minimize the probability of explicitly executing multibit ECC decoding on the cache read critical path, and meanwhile, maintain soft error tolerance. Experiments show that, at the random defect density of 0.5 percent, this design approach can maintain almost the same instruction per cycle (IPC) performance over a wide spectrum of benchmarks compared with ideal defect-free L2 cache, while only incurring less than 3 percent of silicon area overhead and 36 percent power consumption overhead.",
                "id": "01255"
            },
            {
                "title": "Joint Source-Channel Coding and Channelization for Embedded Video Processing With Flash Memory Storage",
                "abstract": "This paper presents a joint source coding, channel coding, and flash memory channelization design framework to obtain optimized tradeoffs among energy consumption, bit rate, and end-to-end distortion (i.e., optimal E-R-D tradeoff space) for embedded and mobile devices with limited power source and abundant flash memory storage capacity. The optimal E-R-D tradeoff space enables embedded and mobile devices to cohesively optimize the source coding and data storage system operations subject to run-time power source, storage capacity, and/or distortion constraints. By treating flash memory as a communication channel, this work differs from classical joint source-channel coding from two perspectives: i) Classical joint source-channel coding aims to obtain an optimized R-D (bit rate and distortion) tradeoff space, while we aim to obtain an optimized E-R-D tradeoff space; ii) Flash memory can be configured (or channelized) over an energy consumption versus raw bit error rate tradeoff spectrum, and channelization is an integral part of the joint design. With the focus on video coding, this paper presents theoretical investigations and specific approaches for both scenarios where channel can and cannot contribute to end-to-end distortion. Based on detailed power estimation and representative video sequences, we quantitatively demonstrate the application of the proposed design approaches for obtaining optimized E-R-D tradeoff space.",
                "id": "01256"
            },
            {
                "title": "Using Data Postcompensation and Predistortion to Tolerate Cell-to-Cell Interference in MLC nand Flash Memory",
                "abstract": "With the appealing storage-density advantage, multilevel-per-cell (MLC) NAND Flash memory that stores more than 1 bit in each memory cell now largely dominates the global Flash memory market. However, due to the inherent smaller noise margin, the MLC NAND Flash memory is more subject to various device/circuit variability and noise, particularly as the industry is pushing the limit of technology scaling and a more aggressive use of MLC storage. Cell-to-cell interference has been well recognized as a major noise source responsible for raw-memory-storage reliability degradation. Leveraging the fact that cell-to-cell interference is a deterministic data-dependent process and can be mathematically described with a simple formula, we present two simple yet effective data-processing techniques that can well tolerate significant cell-to-cell interference at the system level. These two techniques essentially originate from two signal-processing techniques being widely used in digital communication systems to compensate communication-channel intersymbol interference. The effectiveness of these two techniques have been well demonstrated through computer simulations and analysis under an information theoretical framework, and the involved design tradeoffs are discussed in detail.",
                "id": "01257"
            },
            {
                "title": "Relaxed K-best MIMO signal detector design and VLSI implementation",
                "abstract": "Signal detector is a key element in a multiple-input multiple-output (MIMO) wireless communication receiver. It has been well demonstrated that nonlinear tree search MIMO detectors can achieve near-optimum detection performance, nevertheless their efficient high-speed VLSI implementations are not trivial. For example, the hardware design of hard- or soft- output detectors for a 4 \u00d7 4 MIMO system with 64 quadrature amplitude modulation (QAM) still remains missing in the open literature. As an attempt to tackle this challenge, this paper presents an implementation-oriented breadth-first tree search MIMO detector design solution. The key is to appropriately modify the conventional breadth-first tree search detection algorithm in order to largely improve the suitability for efficient hardware implementation, while maintaining good detection performance. To demonstrate the effectiveness of the proposed design solution, using 0.13-\u00b5m CMOS standard cell and memory libraries, we designed a soft-output signal detector for 4 \u00d7 4 MIMO with 64-QAM. With the silicon area of about 31 mm2, the detector can achieve above 100 Mb/s and realize the performance very close to that of the sphere decoding algorithm.",
                "id": "01258"
            },
            {
                "title": "Scheduling Algorithms for Handling Updates in Shingled Magnetic Recording",
                "abstract": "Shingled recording has recently emerged as one promising candidate to sustain the historical growth of magnetic recording storage areal density. However, since the convenient update-in-place feature is no longer available in shingled recording, many sectors must be read and written back in order to update one sector. This leads to a significant update-induced latency overhead and makes conventional hard disk drive scheduling algorithms perform poorly. This paper concerns with the development of appropriate scheduling algorithms for shingled recording based hard disk drives. We first present a simple partial-update scheduling algorithm that can naturally embrace the update latency issue and achieves significant gains over conventional scheduling algorithms. We enhance this algorithm by incorporating a shortest update first policy, which can further reduce the update response time on an average by 70%. Finally, motivated by abundant workload spatial and temporal locality, we develop a spatio-temporal band coalescing scheme that can achieve an additional reduction of update response time of up to 96.8%.",
                "id": "01259"
            },
            {
                "title": "Soft Clock Skew Scheduling for Variation-Tolerant Signal Processing Circuits: A Case Study of Viterbi Decoders",
                "abstract": "This paper concerns the variation tolerance in signal processing integrated circuits. Motivated by the fact that variation-induced timing faults at different locations in signal processing circuits have different effects on the sig- nal processing performance, we developed an importance- aware clock skew scheduling technique, called soft clock skew scheduling, that can realize system-level tolerance to variation-induced timing faults. With state-parallel Viterbi decoders as test vehicles, we demonstrated its effectiveness on increasing the achievable clock frequency in presence of significant variation-induced timing faults, while maintain- ing good decoding performance.",
                "id": "01260"
            },
            {
                "title": "Mutual-Information Optimized Quantization for LDPC Decoding of Accurately Modeled Flash Data",
                "abstract": "  High-capacity NAND flash memories use multi-level cells (MLCs) to store multiple bits per cell and achieve high storage densities. Higher densities cause increased raw bit error rates (BERs), which demand powerful error correcting codes. Low-density parity-check (LDPC) codes are a well-known class of capacity-approaching codes in AWGN channels. However, LDPC codes traditionally use soft information while the flash read channel provides only hard information. Low resolution soft information may be obtained by performing multiple reads per cell with distinct word-line voltages.   We select the values of these word-line voltages to maximize the mutual information between the input and output of the equivalent multiple-read channel under any specified noise model. Our results show that maximum mutual-information (MMI) quantization provides better soft information for LDPC decoding given the quantization level than the constant-pdf-ratio quantization approach. We also show that adjusting the LDPC code degree distribution for the quantized setting provides a significant performance improvement. ",
                "id": "01261"
            },
            {
                "title": "LDPC-in-SSD: making advanced error correction codes work effectively in solid state drives",
                "abstract": "Conventional error correction codes (ECCs), such as the commonly used BCH code, have become increasingly inadequate for solid state drives (SSDs) as the capacity of NAND flash memory continues to increase and its reliability continues to degrade. It is highly desirable to deploy a much more powerful ECC, such as low-density parity-check (LDPC) code, to significantly improve the reliability of SSDs. Although LDPC code has had its success in commercial hard disk drives, to fully exploit its error correction capability in SSDs demands unconventional fine-grained flash memory sensing, leading to an increased memory read latency. To address this important but largely unexplored issue, this paper presents three techniques to mitigate the LDPC-induced response time delay so that SSDs can benefit its strong error correction capability to the full extent. We quantitatively evaluate these techniques by carrying out trace-based SSD simulations with runtime characterization of NAND flash memory reliability and LDPC code decoding. Our study based on intensive experiments shows that these techniques used in an integrated way in SSDs can reduce the worst-case system read response time delay from over 100% down to below 20%. With our proposed techniques, a strong ECC alternative can be used in NAND flash memory to retain its reliability to respond the continuous cost reduction, and its relatively small increase of response time delay is acceptable to mainstream application users, considering a huge gain in SSD capacity, its reliability, and the price reduction.",
                "id": "01262"
            },
            {
                "title": "Improving VLIW Processor Performance Using Three-Dimensional (3D) DRAM Stacking",
                "abstract": "This work studies the potential of using emerging 3D integration to improve embedded VLIW computing system. We focus on the 3D integration of one VLIW processor die with multiple high-capacity DRAM dies. Our proposed memory architecture employs 3D stacking technology to bond one die containing several processing clusters to multiple DRAM dies for a primary memory. The 3D technology also enables wide low-latency buses between clusters and memory and enable the latency of 3D DRAM L2 cache comparable to 2D SRAM L2 cache. These enable it to replace the 2D SRAM L2 cache with 3D DRAM L2 cache. The die area for 2D SRAM L2 cache can be re-allocated to additional clusters that can improve the performance of the system. From the simulation results, we find 3D stacking DRAM main memory can improve the system performance by 10%~80% than 2D off-chip DRAM main memory depending on different benchmarks. Also, for a similar logic die area, a four clusters system with 3D DRAM L2 cache and 3D DRAM main memory outperforms a two clusters system with 2D SRAM L2 cache and 3D DRAM main memory by about 10%.",
                "id": "01263"
            },
            {
                "title": "Data manipulation techniques to reduce phase change memory write energy",
                "abstract": "Due to its great scalability potential, phase change memory has become a topic of great current interest. However, high write energy consumption appears to be one of the biggest challenges to be tackled before phase change memory can be adopted as a mainstream memory technology. This paper presents architecture level technique to reduce phase change memory write energy consumption through data manipulations. Motivated by the fact that phase change memory read incurs much less energy than write and write of different value to a phase change memory cell incurs largely different energy, we present two memory write data manipulation techniques that can effectively reduce the overall memory write energy consumption. Their effectiveness has been demonstrated based on mathematical analysis and computer system simulation using phase change memory as the main memory in the computer memory hierarchy. Significant energy savings with up to more than 60% have been shown over a wide range of computer system benchmarks.",
                "id": "01264"
            },
            {
                "title": "Parallel Logic Simulation of Million-Gate VLSI Circuits",
                "abstract": "The complexity of today\u9a74s VLSI chip designs makes veri fication a necessary step before fabrication. As a result, gate-level logic simulation has became an integral component of the VLSI circuit design process which verifies the design and analyzes its behavior. Since the designs constantly grow in size and complexity, there is a need for ever more ef?cient simulations to keep the gate-level logic veri- fication time acceptably small. The focus of this paper is an efficient simulation of large chip designs. We present the design and implementation of a new parallel simulator, called DSIM, and demonstrate DSIM\u9a74s efficiency and speed by simulating a million gate circuit using different numbers of processors.",
                "id": "01265"
            },
            {
                "title": "Design of Spin-Torque Transfer Magnetoresistive RAM and CAM/TCAM with High Sensing and Search Speed",
                "abstract": "With a great scalability potential, nonvolatile magnetoresistive memory with spin-torque transfer (STT) programming has become a topic of great current interest. This paper addresses cell structure design for STT magnetoresistive RAM, content addressable memory (CAM) and ternary CAM (TCAM). We propose a new RAM cell structure design that can realize high speed and reliable sensing operations in the presence of relatively poor magnetoresistive ratio, while maintaining low sensing current through magnetic tunneling junctions (MTJs). We further apply the same basic design principle to develop new cell structures for nonvolatile CAM, and TCAM. The effectiveness of the proposed RAM, CAM and TCAM cell structures has been demonstrated by circuit simulation at 0.18 ??m CMOS technology.",
                "id": "01266"
            },
            {
                "title": "True-Damage-Aware Enumerative Coding for Improving nand Flash Memory Endurance",
                "abstract": "This brief presents a technique that can fully exploit the data dependency of flash memory cell damage to improve the program/erase (P/E) cycling endurance of nand flash memory. The key is to opportunistically leverage data lossless compressibility and utilize the compression gain to realize memory-damage-aware data manipulation to reduce the cycling-induced physical damage. Based upon experiments using commercial sub-22-nm MLC nand flash memory chips, we show that the proposed design technique can improve the P/E cycling endurance by 50%. We further carried out application-specific integrated circuit design to demonstrate the practical feasibility for implementing the proposed design technique.",
                "id": "01267"
            },
            {
                "title": "Joint code-encoder-decoder design for LDPC coding system VLSI implementation",
                "abstract": "This paper presents a design approach for low-density parity-check (LDPC) coding system hardware implementation by jointly conceiving irregular LDPC code construction and VLSI implementations of encoder and decoder. The key idea is to construct good irregular LDPC codes subject to two constraints that ensure the effective LDPC encoder and decoder hardware implementations. We propose a heuristic algorithm to construct such implementation-aware irregular LDPC codes that can achieve very good error correction performance. The encoder and decoder hardware architectures are correspondingly presented.",
                "id": "01268"
            },
            {
                "title": "High-Rate Quasi-Cyclic Ldpc Codes For Magnetic Recording Channel With Low Error Floor",
                "abstract": "By implementing an FPGA-based simulator, we investigate the performance of high-rate quasi-cyclic (QC) LDPC codes for the magnetic recording channel at very low sector error rates. Results show that error-floor-free performance can be realized by randomly constructed high-rate regular QC-LDPC codes with column weight 4 for sector error rates as low as 10(-9). We also conjecture several rules for designing randomly constructed high-rate regular QC-LDPC codes with low error floor. We also present a decoder architecture that is well suited to achieving high decoding throughput for these high-rate QC-LDPC codes with low error floor.",
                "id": "01269"
            },
            {
                "title": "Block-LDPC: a practical LDPC coding system design approach.",
                "abstract": "This paper presents a joint low-density parity-check (LDPC) code-encoder-decoder design approach, called Block-LDPC, for practical LDPC coding system implementations. The key idea is to construct LDPC codes subject to certain hardware-oriented constraints that ensure the effective encoder and decoder hardware implementations. We develop a set of hardware-oriented constraints, subject to which a se...",
                "id": "01270"
            },
            {
                "title": "A Time-Aware Fault Tolerance Scheme to Improve Reliability of Multilevel Phase-Change Memory in the Presence of Significant Resistance Drift",
                "abstract": "Because of its promising scalability potential and support of multilevel per cell storage, phase-change memory has become a topic of great current interest. However, recent studies show that structural relaxation effect makes the resistance of phase-change material drift over the time, which can severely degrade multilevel per cell phase-change memory storage reliability. This makes powerful memory fault tolerance solutions indispensable, where error correction code (ECC) will play an essential role. This work aims to develop fault tolerance solutions that can effectively compensate memory cell resistance drift. First, based upon information-theoretical study, we show that conventional use of ECC, which is unaware of memory content lifetime, can only achieve the performance with a big gap from the information-theoretical bounds. This motivates us to study the potential of time-aware memory fault tolerance, where the basic idea is to keep track the memory content lifetime and use this lifetime information to accordingly adjust how memory cell resistance is quantized and interpreted for ECC decoding. Under this time-aware fault tolerance framework, we study the use of two types of ECCs, including classical codes such as BCH that only demand hard-decision input and advanced codes such as low-density parity-check (LDPC) codes that demand soft-decision probability input. Using hypothetical four-level per cell and eight-level per cell phase-change memory with BCH and LDPC codes as test vehicles, we carry out extensive analysis and simulations, which demonstrate very significant performance advantages of such time-aware memory fault tolerance strategy in the presence of significant memory cell resistance drift.",
                "id": "01271"
            },
            {
                "title": "A High Throughput Limited Search Trellis Decoder For Convolutional Code Decoding",
                "abstract": "Due to the lack of operational parallelism and structured data storage/retrieval, limited search trellis decoding algorithms have been traditionally ruled out for applications demanding high throughput convolutional code decoding. Among various limited search algorithms, the T-algorithm performs breadth-first limited search and has good potential for parallel decoding. In this paper, we propose two techniques at the algorithm and VLSI architecture levels for the T-algorithm to improve the decoding parallelism and tackle the data storage/retrieval problem, which enables the high throughput path-parallel T-algorithm decoder VLSI implementation. This work provides a vehicle for exploiting the merits of the T-algorithm. i.e., low computational complexity that is adaptive to the channel distortion, in high throughput applications.",
                "id": "01272"
            },
            {
                "title": "Realization of L2 Cache Defect Tolerance Using Multi-bit ECC",
                "abstract": "This paper presents a design solution that enables the use of powerful multi-bit error-correcting code (ECC) to realize L2 cache defect tolerance at minimal latency and silicon area cost. This work is motivated by the observation that the continuous CMOS Technology scaling may result in an increasing level defect density and make conventional cache memory defect tolerance strategies inadequate. The basic idea is to complement conventional L2 cache core with two separate fully associative caches, one stores multi-bit ECC check bits for realizing area-efficient selective multi-bit ECC protection and another one stores the most recently decoded multi-bit ECC code word to minimize the impact of explicit multi-bit ECC decoding on L2 cache access latency. Its effectiveness has been demonstrated using SimpleScalar and Cacti tools. At the defect density of 0.5%, this design approach can maintain almost the same instruction per cycle (IPC) performance over a wide spectrum of benchmarks compared with ideal L2 cache without defects, while only incurring less than 2.5% of silicon area overhead.",
                "id": "01273"
            },
            {
                "title": "Estimating information-theoretical NAND flash memory storage capacity and its implication to memory system design space exploration",
                "abstract": "Today and future NAND flash memory will heavily rely on system-level fault-tolerance techniques such as error correction code (ECC) to ensure the overall system storage integrity. Since ECC demands the storage of coding redundancy and hence degrades effective cell storage efficiency, it is highly desirable to use more powerful coding solutions that can maintain the system storage reliability at less coding redundancy. This has motivated a growing interest in the industry to search for alternatives to BCH code being used in today. Regardless to specific ECCs, it is of great practical importance to know the theoretical limit on the achievable cell storage efficiency, which motivates this work. We first develop an approximate NAND flash memory channel model that explicitly incorporates program/erase (P/E) cycling effects and cell-to-cell interference, based on which we then develop strategies for estimating the information-theoretical bounds on cell storage efficiency. We show that it can readily reveal the tradeoffs among cell storage efficiency, P/E cycling endurance, and retention limit, which can provide important insights for system designers. Finally, motivated by the dynamics of P/E cycling effect revealed by the information-theoretical study, we propose two memory system design techniques that can improve the average NAND flash memory programming speed and increase the total amount of user data that can be stored in NAND flash cell over its entire lifetime.",
                "id": "01274"
            },
            {
                "title": "Design Techniques to Facilitate Processor Power Delivery in 3-D Processor-DRAM Integrated Systems",
                "abstract": "As a promising option to address the memory wall problem, 3-D processor-DRAM integration has recently received many attentions. Since DRAM dies should be stacked between the processor die and package substrate, we have to fabricate a large number of through-DRAM through-silicon vias (TSVs) to connect the processor die and package for power and input/output (I/O) signal delivery. Although such through-DRAM TSVs will inevitably interfere with DRAM design and induce non-negligible power consumption overhead, little prior research has been done to study how to allocate these through-DRAM TSVs on the DRAM dies and analyze their impacts. To address this open issue, this paper first presents a through-DRAM TSV allocation strategy that well fits to the regular DRAM architecture. Meanwhile, due to the longer path between power/ground pads and processor die, power delivery integrity issue may become more serious in such 3-D processor-DRAM integrated systems. Decoupling capacitor insertion is the most popular method to deal with power delivery integrity issue in high-performance integrated circuits. This paper further proposes to use 3-D stacked DRAM dies to provide decoupling capacitors for the processor die. This can well leverage the superior capacitor fabrication ability of DRAM to reduce the area penalty of decoupling capacitor insertion on the processor die. For its practical implementation, a simple uniform decoupling capacitor network design strategy is presented. To demonstrate through-DRAM TSV allocation and decoupling capacitor insertion strategy and evaluate involved tradeoffs, circuit SPICE simulations and computer system simulations are carried out to quantitatively demonstrate the effectiveness and investigate various design tradeoffs.",
                "id": "01275"
            },
            {
                "title": "Modeling and evaluation for electrical characteristics of through-strata-vias (TSVS) in three-dimensional integration",
                "abstract": "This paper discusses through-strata-vias (TSVs) technology and presents modeling results of their electrical performance using Agilent's ADS and Momentum simulator. Since TSV is an essential component in three-dimensional (3D) integration/packaging, it is important to explore and investigate its electrical characteristics. A simple face-to-back TSV is studied in frequency domain and time domain. The impact of physical configurations and materials on TSV electrical characteristics is evaluated. An equivalent circuit model is proposed, and the values of passive elements (resistance, inductance and capacitance) within the model are extracted from full-wave scattering parameters.",
                "id": "01276"
            },
            {
                "title": "Joint (3,k)-regular LDPC code and decoder/encoder design",
                "abstract": "In the past few years, Gallager's Low-Density Parity-Check (LDPC) codes have received a lot of atten- tion and tremendous efforts have been devoted to analyze and improve their error-correcting performance. However, little consideration has been given to the practical LDPC codec hardware implementations. The straightforward fully parallel LDPC decoder architecture usually incurs too high complexity for many prac- tical purposes, thus effective partly parallel decoder design approaches are highly desirable. Due to the randomness of LDPC code, it is extremely difficult, if not impossible, to develop a direct transformation from fully parallel architecture to partly parallel ones for a given LDPC code. Meanwhile, also because of its randomness, the direct LDPC encoding scheme has quadratic complexity in the block length, which makes the efficient encoder design not trivial. We believe that jointly conceiving code construction and decoder/encoder design is a promising direction to develop high performance LDPC coding systems. In this paper, we propose a joint ( )-regular LDPC code and decoder/encoder design approach to construct a class of ( )-regular LDPC codes that not only have very good performance but also exactly fit to a high- speed partly parallel decoder and low-complexity encoder. Moreover, we propose a modified joint design approach in order to further reduce the decoder hardware complexity for those high-rate ( )-regular LDPC codes applied to silicon area critical applications.",
                "id": "01277"
            },
            {
                "title": "Reducing data transfer latency of NAND flash memory with soft-decision sensing",
                "abstract": "With the aggressive technology scaling and use of multi-bit per cell storage, NAND flash memory is subject to continuous degradation of raw storage reliability and demands more and more powerful error correction codes (ECC). This inevitable trend makes conventional BCH code increasingly inadequate, and iterative coding solutions such as LDPC codes become very natural alternative options. However, these powerful coding solutions demand soft-decision memory sensing, which results in longer on-chip memory sensing latency and memory-to-controller data transfer latency. This paper presents two simple design techniques that can reduce the memory-to-controller data transfer latency. The key is to appropriately apply entropy coding to compress the memory sensing results. Simulation results show that the proposed design solutions can reduce the data transfer latency by up to 64% for soft-decision memory sensing.",
                "id": "01278"
            },
            {
                "title": "Relaxed tree search MIMO signal detection algorithm design and VLSI implementation",
                "abstract": "This paper presents an implementation-oriented breadth-first tree search MIMO detector design solution. Techniques at algorithm and VLSI architecture levels are developed to improve the implementation efficiency. Using Synopsys synthesis tool with 0.13 mum CMOS technology, we designed soft-output detectors for 4 times 4 MIMO channel with 64-QAM modulation. With the silicon areas less than 15 mm 2, the detectors can achieve up to about 80 Mbps and realize the performance very close to detectors using the sphere decoding algorithm",
                "id": "01279"
            },
            {
                "title": "Using Planar Embedded DRAM in Memory Intensive Signal Processing Circuits: Case Studies on LDPC Decoding and Motion Estimation.",
                "abstract": "This paper studies the feasibility and potential of using planar embedded DRAM (eDRAM), which is completely compatible with CMOS logic process, to improve circuit implementation efficiency of memory-hungry signal processing algorithms. In spite of its apparent cell area efficiency advantage over SRAM, planar eDRAM is not being widely used in practice, mainly due to its very short retention time (e.g., few \u03bcs and even a few hundreds ns). In this work, we contend that short retention time may not necessarily be a fundamental issue for implementing signal processing algorithms because they typically handle streaming data, which exhibits regular and predictable data access patterns, and has a large algorithm/architecture design space. This study elaborates on the rationale and application of using a planar eDRAM in memory-hungry signal processing circuit implementations, and discusses the possible algorithm and architecture design strategies to better embrace the use of planar eDRAM. For the purpose of demonstration, we use low-density parity-check (LDPC) code decoding and motion estimation in video encoding as test vehicles. Beyond a straightforward SRAM replacement, we propose an interleaved read/write page-mode DRAM operation to reduce planar eDRAM energy consumption by leveraging LDPC code decoding data access pattern, and we investigate the potential of using planar eDRAM to enable a higher degree of image data reuse in motion estimation by proposing a folded scan structure to further improve its effectiveness. We carried out detailed planar eDRAM SPICE simulations at 45 nm node to obtain its characteristics, based on which we quantitatively evaluate the effectiveness of using planar eDRAM in these two case studies. \u00a9 Springer Science+Business Media New York 2013.",
                "id": "01280"
            },
            {
                "title": "Quasi-nonvolatile SSD: Trading flash memory nonvolatility to improve storage system performance for enterprise applications",
                "abstract": "This paper advocates a quasi-nonvolatile solid-state drive (SSD) design strategy for enterprise applications. The basic idea is to trade data retention time of NAND flash memory for other system performance metrics including program/erase (P/E) cycling endurance and memory programming speed, and meanwhile use explicit internal data refresh to accommodate very short data retention time (e.g., few weeks or even days). We also propose SSD scheduling schemes to minimize the impact of internal data refresh on normal I/O requests. Based upon detailed memory cell device modeling and SSD system modeling, we carried out simulations that clearly show the potential of using this simple quasi-nonvolatile SSD design strategy to improve system cycling endurance and speed performance. We also performed detailed energy consumption estimation, which shows the energy consumption overhead induced by data refresh is negligible.",
                "id": "01281"
            }
        ]
    }
]