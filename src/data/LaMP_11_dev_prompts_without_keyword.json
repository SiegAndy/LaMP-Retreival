{
    "010": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Visual-audio integration for user authentication system of partner robots':",
        "title: \"Adaptive fuzzy logic controller for vehicle active suspensions with interval type-2 fuzzy membership functions\" with abstract: \"Elicited from the least means squares optimal algorithm (LMS), an adaptive fuzzy logic controller (AFC) based on interval type-2 fuzzy sets is proposed for vehicle non-linear active suspension systems. The interval membership functions (IMF2s) are utilized in the AFC design to deal with not only non-linearity and uncertainty caused from irregular road inputs and immeasurable disturbance, but also the potential uncertainty of expertpsilas knowledge and experience. The adaptive strategy is designed to self-tune the active force between the lower bounds and upper bounds of interval fuzzy outputs. A case study based on a quarter active suspension model has demonstrated that the proposed type-2 fuzzy controller significantly outperforms conventional fuzzy controllers of an active suspension and a passive suspension.\"",
        "title: \"Asymmetric LSH (ALSH) for Sublinear Time Maximum Inner Product Search (MIPS).\" with abstract: \"We present the first provably sublinear time hashing algorithm for approximate Maximum Inner Product Search (MIPS). Searching with (un-normalized) inner product as the underlying similarity measure is a known difficult problem and finding hashing schemes for MIPS was considered hard. While the existing Locality Sensitive Hashing (LSH) framework is insufficient for solving MIPS, in this paper we extend the LSH framework to allow asymmetric hashing schemes. Our proposal is based on a key observation that the problem of finding maximum inner products, after independent asymmetric transformations, can be converted into the problem of approximate near neighbor search in classical settings. This key observation makes efficient sublinear hashing scheme for MIPS possible. Under the extended asymmetric LSH (ALSH) framework, this paper provides an example of explicit construction of provably fast hashing scheme for MIPS. Our proposed algorithm is simple and easy to implement. The proposed hashing scheme leads to significant computational savings over the two popular conventional LSH schemes: (i) Sign Random Projection (SRP) and (ii) hashing based on p-stable distributions for L-2 norm (L2LSH), in the collaborative filtering task of item recommendations on Netflix and Movielens (10M) datasets.\"",
        "title: \"Training Logistic Regression and SVM on 200GB Data Using b-Bit Minwise Hashing and Comparisons with Vowpal Wabbit (VW)\" with abstract: \"  We generated a dataset of 200 GB with 10^9 features, to test our recent b-bit minwise hashing algorithms for training very large-scale logistic regression and SVM. The results confirm our prior work that, compared with the VW hashing algorithm (which has the same variance as random projections), b-bit minwise hashing is substantially more accurate at the same storage. For example, with merely 30 hashed values per data point, b-bit minwise hashing can achieve similar accuracies as VW with 2^14 hashed values per data point.   We demonstrate that the preprocessing cost of b-bit minwise hashing is roughly on the same order of magnitude as the data loading time. Furthermore, by using a GPU, the preprocessing cost can be reduced to a small fraction of the data loading time.   Minwise hashing has been widely used in industry, at least in the context of search. One reason for its popularity is that one can efficiently simulate permutations by (e.g.,) universal hashing. In other words, there is no need to store the permutation matrix. In this paper, we empirically verify this practice, by demonstrating that even using the simplest 2-universal hashing does not degrade the learning performance. \"",
        "title: \"Partial Hard Thresholding: Towards A Principled Analysis of Support Recovery.\" with abstract: \"In machine learning and compressed sensing, it is of central importance to understand when a tractable algorithm recovers the support of a sparse signal from its compressed measurements. In this paper, we present a principled analysis on the support recovery performance for a family of hard thresholding algorithms. To this end, we appeal to the partial hard thresholding (PHT) operator proposed recently by Jain et al. [IEEE Trans. Information Theory, 2017]. We show that under proper conditions, PHT recovers an arbitrary s-sparse signal within O(s kappa log kappa) iterations where kappa is an appropriate condition number. Specifying the PHT operator, we obtain the best known results for hard thresholding pursuit and orthogonal matching pursuit with replacement. Experiments on the simulated data complement our theoretical findings and also illustrate the effectiveness of PHT.\"",
        "title: \"Logician: A Unified End-to-End Neural Approach for Open-Domain Information Extraction.\" with abstract: \"In this paper, we consider the problem of open information extraction (OIE) for extracting entity and relation level intermediate structures from sentences in open-domain. We focus on four types of valuable intermediate structures (Relation, Attribute, Description, and Concept), and propose a unified knowledge expression form, SAOKE, to express them. We publicly release a data set which contains 48,248 sentences and the corresponding facts in the SAOKE format labeled by crowdsourcing. To our knowledge, this is the largest publicly available human labeled data set for open information extraction tasks. Using this labeled SAOKE data set, we train an end-to-end neural model using the sequence-to-sequence paradigm, called Logician, to transform sentences into facts. For each sentence, different to existing algorithms which generally focus on extracting each single fact without concerning other possible facts, Logician performs a global optimization over all possible involved facts, in which facts not only compete with each other to attract the attention of words, but also cooperate to share words. An experimental study on various types of open domain relation extraction tasks reveals the consistent superiority of Logician to other states-of-the-art algorithms. The experiments verify the reasonableness of SAOKE format, the valuableness of SAOKE data set, the effectiveness of the proposed Logician model, and the feasibility of the methodology to apply end-to-end learning paradigm on supervised data sets for the challenging tasks of open information extraction.\n\n\"",
        "1 is \"Pronunciation Modeling for Improved Spelling Correction\", 2 is \"Path following algorithm for highly redundant manipulators\".",
        "\nGiven above information, for an author who has written the paper with the title \"Visual-audio integration for user authentication system of partner robots\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "011": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Perspectives on Cognitive Informatics and Cognitive Computing':",
        "title: \"Big Data Analytics: A Cognitive Perspectives.\" with abstract: \"Big data are pervasively generated by human cognitive processes, formal inferences, and system quantifications. This paper presents the cognitive foundations of big data systems towards big data science. The key perceptual model of big data systems is the recursively typed hyperstructure RTHS. The RTHS model reveals the inherited complexities and unprecedented difficulty in big data engineering. This finding leads to a set of mathematical and computational models for efficiently processing big data systems. The cognitive relationship between data, information, knowledge, and intelligence is formally described.\"",
        "title: \"The Cognitive Informatics Theory And Mathematical Models Of Visual Information Processing In The Brain\" with abstract: \"It is recognized that the internal mechanisms for visual information processing are based on semantic inferences where visual information is represented and processed as visual semantic objects rather than direct images or episode pictures in the long-term memory. This article presents a cognitive informatics theory of visual information and knowledge processing in the brain. A set of cognitive principles of visual perception is reviewed particularly the classic gestalt principles, the cognitive informatics principles, and the hypercolumn theory. A visual frame theory is developed to explain the visual information processing mechanisms of human vision, where the size of a unit visual frame is tested and calibrated based on vision experiments. The framework of human visual information processing is established in order to elaborate mechanisms of visual information processing and the compatibility of internal representations between visual and abstract information and knowledge in the brain. [Article copies are available for purchase from InfoSci-on-Demand.com]\"",
        "title: \"The Cognitive Process Of Decision Making\" with abstract: \"Decision making is one of the basic cognitive processes of human behaviors by which a preferred option or a course of actions is chosen from among a set of alternatives based on certain criteria. Decision theories are widely applied in many disciplines encompassing cognitive informatics, computer science, management science, economics, sociology, psychology, political science, and statistics. A number of decision strategies have been proposed from different angles and application domains such as the maximum expected utility and Bayesian method. However, there is still a lack of a fundamental and mathematical decision model and a rigorous cognitive process for decision making. This article presents a fundamental cognitive decision making process and its mathematical model, which is described as a sequence of Cartesian-product based selections. A rigorous description of the decision process in real-time process algebra (RTPA) is provided. Real-world decisions are perceived as a repetitive application of the fundamental cognitive process. The result shows that all categories of decision strategies fit in the formally described decision process. The cognitive process of decision making may be applied in a wide range of decision-based systems such as cognitive informatics, software agent systems, expert systems, and decision support systems.\"",
        "title: \"A Web Knowledge Discovery Engine Based on Concept Algebra\" with abstract: \"On-line knowledge discovery is an important area of knowledge engineering. This paper develops a visualized concept network explorer and a semantic analyzer to locate, capture, and refine queries based on concept algebra. A graphical interface is built using concept and semantic models to refine users' query structures. This tool kit can generate a structured XML query package that accurately express users' information needs for on-line searching and knowledge acquisition.\"",
        "title: \"Simulation and Visualization of Concept Algebra in MATLAB\" with abstract: \"Concept algebra (CA) is a denotational mathematics for formal knowledge manipulation and natural language processing. In order to explicitly demonstrate the mathematical models of formal concepts and their algebraic operations in CA, a simulation and visualization software is developed in the MATLAB environment known as the Visual Simulator of Concept Algebra (VSCA). This paper presents the design and implementation of VSCA and the theories underpinning its development. Visual simulations for the sets of reproductive and compositional operations of CA are demonstrated by real-world examples throughout the elaborations of CA and VSCA.\"",
        "1 is \"Learning Discriminative And Shareable Features For Scene Classification\", 2 is \"Using design patterns to develop reusable object-oriented communication software\".",
        "\nGiven above information, for an author who has written the paper with the title \"Perspectives on Cognitive Informatics and Cognitive Computing\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "012": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Using Magnetic RAM to Build Low-Power and Soft Error-Resilient L1 Cache':",
        "title: \"Quasi-nonvolatile SSD: Trading flash memory nonvolatility to improve storage system performance for enterprise applications\" with abstract: \"This paper advocates a quasi-nonvolatile solid-state drive (SSD) design strategy for enterprise applications. The basic idea is to trade data retention time of NAND flash memory for other system performance metrics including program/erase (P/E) cycling endurance and memory programming speed, and meanwhile use explicit internal data refresh to accommodate very short data retention time (e.g., few weeks or even days). We also propose SSD scheduling schemes to minimize the impact of internal data refresh on normal I/O requests. Based upon detailed memory cell device modeling and SSD system modeling, we carried out simulations that clearly show the potential of using this simple quasi-nonvolatile SSD design strategy to improve system cycling endurance and speed performance. We also performed detailed energy consumption estimation, which shows the energy consumption overhead induced by data refresh is negligible.\"",
        "title: \"Architecting high-performance energy-efficient soft error resilient cache under 3D integration technology\" with abstract: \"Radiation-induced soft error has become an emerging reliability threat to high performance microprocessor design. As the size of on chip cache memory steadily increased for the past decades, resilient techniques against soft errors in cache are becoming increasingly important for processor reliability. However, conventional soft error resilient techniques have significantly increased the access latency and energy consumption in cache memory, thereby resulting in undesirable performance and energy efficiency degradation. The emerging 3D integration technology provides an attractive advantage, as the 3D microarchitecture exhibits heterogeneous soft error resilient characteristics due to the shielding effect of die stacking. Moreover, the 3D shielding effect can offer several inner dies that are inherently invulnerable to soft error, as they are implicitly protected by the outer dies. To exploit the invulnerability benefit, we propose a soft error resilient 3D cache architecture, in which data blocks on the soft error invulnerable dies have no protection against soft error, therefore, access to the data block on the soft error invulnerable die incurs a considerably reduced access latency and energy. Furthermore, we propose to maximize the access on the soft error invulnerable dies by dynamically moving data blocks among different dies, thereby achieving further performance and energy efficiency improvement. Simulation results show that the proposed 3D cache architecture can reduce the power consumption by up to 65% for the L1 instruction cache, 60% for the L1 data cache and 20% for the L2 cache, respectively. In general, the overall IPC performance can be improved by 5% on average.\"",
        "title: \"Improving STT MRAM storage density through smaller-than-worst-case transistor sizing\" with abstract: \"This paper presents a technique to improve the storage density of spin-torque transfer (STT) magnetoresistive random access memory (MRAM) in the presence of significant magnetic tunneling junction (MTJ) write current threshold variability. In conventional design practice, the nMOS transistor within each memory cell is sized to be large enough to carry a current larger than the worst-case MTJ write current threshold, leading to an increasing storage density penalty as the technology scales down. To mitigate such variability-induced storage density penalty, this paper presents a smaller-than-worst-case transistor sizing approach with the underlying theme of jointly considering memory cell transistor sizing and defect tolerance. Its effectiveness is demonstrated using 256 Mb STT MRAM design at 45 nm node as a test vehicle. Results show that, under a normalized write current threshold deviation of 20%, the overall memory die size can be reduced by more than 20% compared with the conventional worst-case transistor sizing design practice.\"",
        "title: \"Exploiting memory device wear-out dynamics to improve NAND flash memory system performance\" with abstract: \"This paper advocates a device-aware design strategy to improve various NAND flash memory system performance metrics. It is well known that NAND flash memory program/erase (PE) cycling gradually degrades memory device raw storage reliability, and sufficiently strong error correction codes (ECC) must be used to ensure the PE cycling endurance. Hence, memory manufacturers must fabricate enough number of redundant memory cells geared to the worst-case device reliability at the end of memory lifetime. Given the memory device wear-out dynamics, the existing worst-case oriented ECC redundancy is largely under-utilized over the entire memory lifetime, which can be adaptively traded for improving certain NAND flash memory system performance metrics. This paper explores such device-aware adaptive system design space from two perspectives, including (1) how to improve memory program speed, and (2) how to improve memory defect tolerance and hence enable aggressive fabrication technology scaling. To enable quantitative evaluation, we for the first time develop a NAND flash memory device model to capture the effects of PE cycling from the system level. We carry out simulations using the DiskSim-based SSD simulator and a variety of traces, and the results demonstrate up to 32% SSD average response time reduction. We further demonstrate that the potential on achieving very good defect tolerance, and finally show that these two design approaches can be readily combined together to noticeably improve SSD average response time even in the presence of high memory defect rates.\"",
        "title: \"Impacts Of Though-Dram Vias In 3d Processor-Dram Integrated Systems\" with abstract: \"As a promising option to address the memory wall problem, 3D processor-DRAM integration has recently received many attentions. Since DRAM tiers must be stacked between the processor tier and package substrate, we must fabricate a large number of through-DRAM through-silicon vias (TSVs) to connect the processor tier and package for power and I/O signal delivery. Although such through-DRAM TSVs will inevitably Interfere with DRAM design and induce non-negligible power consumption overhead, little research has been done to study how to allocate these TSVs on the DRAM tiers and analyze their impacts. To address this open issue, this paper first presents a through-DRAM TSV allocation strategy that fits well to the regular DRAM architecture. To demonstrate this design strategy and evaluate trade-offs involved, we develop a CACTI-based modeling tool to carry out extensive simulations over a wide range of design parameters.\"",
        "1 is \"Severless Search and Authentication Protocols for RFID\", 2 is \"Dynamic Backlight Scaling Optimization: A Cloud-Based Energy-Saving Service for Mobile Streaming Applications\".",
        "\nGiven above information, for an author who has written the paper with the title \"Using Magnetic RAM to Build Low-Power and Soft Error-Resilient L1 Cache\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "013": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Voronoi diagrams for direction-sensitive distances':",
        "title: \"Parallel algorithms for maximum matching in interval graphs\" with abstract: \"Given a set of n intervals representing an interval graph, the problem of finding a maximum matching between pairs of disjoint (nonintersecting) intervals has been considered in the sequential model. We present parallel algorithms for computing maximum cardinality matchings among pairs of disjoint intervals in interval graphs an the EREW PRAM and hypercube models. For the general case of the problem, our algorithms compute a maximum matching in O(log/sup 3/ n) time using O(n/log/sup 2/ n) processors on the EREW PRAM and using O(n) processors on the hypercubes. For the case of proper interval graphs, our algorithm runs in O(log n) time using O(n) processors if the input intervals are not given already sorted and using O(n/log n) processors otherwise, on the EREW PRAM. On n-processor hypercubes, our algorithm for this case takes O(log n loglog n) time for unsorted input and O(log n) time for sorted input. Our parallel results also lead to optimal sequential algorithms for computing maximum matchings among disjoint intervals. We also present an improved parallel algorithm for maximum matching between overlapping intervals in proper interval graphs.\"",
        "title: \"On approximating the maximum simple sharing problem\" with abstract: \"In the maximum simple sharing problem (MSS), we want to compute a set of node-disjoint simple paths in an undirected bipartite graph covering as many nodes as possible of one layer of the graph, with the constraint that all paths have both endpoints in the other layer. This is a variation of the maximum sharing problem (MS) that finds important applications in the design of molecular quantum-dot cellular automata (QCA) circuits and physical synthesis in VLSI. It also generalizes the maximum weight node-disjoint path cover problem. We show that MSS is NP-complete, present a polynomial-time $5\\over 3$-approximation algorithm, and show that it cannot be approximated with a factor better than $740\\over 739$ unless P = NP.\"",
        "title: \"Scheduling for power reduction in a real-time system\" with abstract: \"This paper describes how, through a combination of scheduling and buffer insertion, real-time systems may be optimized for power consumption while maintaining deadlines. Beginning with simple examples (components that have no internal pipelines and in which the only design freedoms are buffer insertion and scheduling), we? illustrate the effect of adjusting the time at which data are processed on power consumption. Algorithms for optimizing the energy saving are proposed for several real-time system implementations including non-pipelined and pipelined. We also discuss extension to this preliminary work including selection of alternate processing units in order to reduce power consumption while maintaining deadlines.\"",
        "title: \"Planar Spanners and Approximate Shortest Path Queries among Obstacles in the Plane\" with abstract: \"We consider the problem of finding an obstacle-avoiding path between two points s and t in the plane, amidst a set of disjoint polyg- onal obstacles with a total of n vertices. The length of this path should be within a small constant factor c of the length of the shortest possible obstacle-avoiding s-t path measured in the Lv-metric. Such an approxi- mate shortest path is called a c-short path, or a short path with stretch )actor c. The goal is to preprocess the obstacle-scattered plane by creat- ing an efficient data structure that enables fast reporting of a c-short path (or its length). In this paper, we give a family of algorithms for the above problem that achieve an interesting trade-off between the stretch factor, the query time and the preprocessing bounds. Our main results are al- gorithms that achieve logarithmic length query time, after subquadratic time and space preprocessing.\"",
        "title: \"Biomedical Image Segmentation Using Fully Convolutional Networks on TrueNorth\" with abstract: \"With the rapid growth of medical and biomedical image data, energy-efficient solutions for analyzing such image data that can be processed fast and accurately on platforms with low power budget are highly desirable. This paper uses segmenting glial cells in brain microscopy images as a case study to demonstrate how to achieve biomedical image segmentation with significant energy saving and minimal comprise in accuracy. Specifically, we design, train, implement, and evaluate Fully Convolutional Networks (FCNs) for biomedical image segmentation on IBM's neurosynaptic DNN processor - TrueNorth (TN). Comparisons in terms of accuracy and energy dissipation of TN with that of a low power NVIDIA TX2 mobile GPU platform have been conducted. Experimental results show that TN can offer at least two orders of magnitude improvement in energy efficiency when compared to TX2 GPU for the same workload.\"",
        "1 is \"Fast Algorithms for Finding Maximum-Density Segments of a Sequence with Applications to Bioinformatics\", 2 is \"Power minimization in IC design: principles and applications\".",
        "\nGiven above information, for an author who has written the paper with the title \"Voronoi diagrams for direction-sensitive distances\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "014": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'QuickSense: Fast and energy-efficient channel sensing for dynamic spectrum access networks':",
        "title: \"Fast and Accurate Performance Analysis of LTE Radio Access Networks.\" with abstract: \"An increasing amount of analytics is performed on data that is procured in a real-time fashion to make real-time decisions. Such tasks include simple reporting on streams to sophisticated model building. However, the practicality of such analyses are impeded in several domains because they are faced with a fundamental trade-off between data collection latency and analysis accuracy. In this paper, we study this trade-off in the context of a specific domain, Cellular Radio Access Networks (RAN). Our choice of this domain is influenced by its commonalities with several other domains that produce real-time data, our access to a large live dataset, and their real-time nature and dimensionality which makes it a natural fit for a popular analysis technique, machine learning (ML). We find that the latency accuracy trade-off can be resolved using two broad, general techniques: intelligent data grouping and task formulations that leverage domain characteristics. Based on this, we present CellScope, a system that addresses this challenge by applying a domain specific formulation and application of Multi-task Learning (MTL) to RAN performance analysis. It achieves this goal using three techniques: feature engineering to transform raw data into effective features, a PCA inspired similarity metric to group data from geographically nearby base stations sharing performance commonalities, and a hybrid online-offline model for efficient model updates. Our evaluation of CellScope shows that its accuracy improvements over direct application of ML range from 2.5x to 4.4x while reducing the model update overhead by up to 4.8x. We have also used CellScope to analyze a live LTE consisting of over 2 million subscribers for a period of over 10 months, where it uncovered several problems and insights, some of them previously unknown.\"",
        "title: \"Stable Egress Route Selection for Interdomain Traffic Engineering: Model and Analysis\" with abstract: \"We present a general model of interdomain route selection to study interdomain traffic engineering. In this model, the routing of multiple destinations can be coordinated. Thus the model can capture general traffic engineering behaviors such as load balancing and link capacity constraints. We first identify potential routing instability and inefficiency of interdomain traffic engineering. We then derive a sufficient condition to guarantee convergence. We also show that the constraints on local policies imposed by business considerations in the Internet can guarantee stability without global coordination. Using realistic Internet topology, we evaluate the extent to which routing instability of interdomain traffic engineering can happen when the constraints are violated.\"",
        "title: \"Large-scale IP traceback in high-speed internet: practical techniques and information-theoretic foundation\" with abstract: \"Tracing attack packets to their sources, known as IP traceback, is an important step to counter distributed denial-of-service (DDoS) attacks. In this paper, we propose a novel packet logging based (i.e., hash-based) traceback scheme that requires an order of magnitude smaller processing and storage cost than the hash-based scheme proposed by Snoeren et al. [1], thereby being able to scalable to much higher link speed (e.g., OC-768). The base-line idea of our approach is to sample and log a small percentage (e.g., 3.3%) of packets. The challenge of this low sampling rate is that much more sophisticated techniques need to be used for traceback. Our solution is to construct the attack tree using the correlation between the attack packets sampled by neighboring routers. The scheme using naive independent random sampling does not perform well due to the low correlation between the packets sampled by neighboring routers. We invent a sampling scheme that improves this correlation and the overall efficiency significantly. Another major contribution of this work is that we introduce a novel information-theoretic framework for our traceback scheme to answer important questions on system parameter tuning and the fundamental tradeoff between the resource used for traceback and the traceback accuracy. Simulation results based on real-world network topologies (e.g., Skitter) match very well with results from the information-theoretic analysis. The simulation results also demonstrate that our traceback scheme can achieve high accuracy, and scale very well to a large number of attackers (e.g., 5000+).\"",
        "title: \"Time-evolving graph processing at scale.\" with abstract: \"Time-evolving graph-structured big data arises naturally in many application domains such as social networks and communication networks. However, existing graph processing systems lack support for efficient computations on dynamic graphs. In this paper, we represent most computations on time evolving graphs into (1) a stream of consistent and resilient graph snapshots, and (2) a small set of operators that manipulate such streams of snapshots. We then introduce GraphTau, a time-evolving graph processing framework built on top of Apache Spark, a widely used distributed dataflow system. GraphTau quickly builds fault-tolerant graph snapshots as each small batch of new data arrives. GraphTau achieves high performance and fault tolerant graph stream processing via a number of optimizations. GraphTau also unifies data streaming and graph streaming processing. Our preliminary evaluations on two representative datasets show promising results. Besides performance benefit, GraphTau API relieves programmers from handling graph snapshot generation, windowing operators and sophisticated differential computation mechanisms.\"",
        "title: \"Coordination mechanisms for selfish scheduling\" with abstract: \"In machine scheduling, a set of n jobs must be scheduled on a set of m machines. Each job i incurs a processing time of pij on machine j and the goal is to schedule jobs so as to minimize some global objective function, such as the maximum makespan of the schedule considered in this paper. Often in practice, each job is controlled by an independent selfish agent who chooses to schedule his job on machine which minimizes the (expected) completion time of his job. This scenario can be formalized as a game in which the players are job owners; the strategies are machines; and the disutility to each player in a strategy profile is the completion time of his job in the corresponding schedule (a player\u2019s objective is to minimize his disutility). The equilibria of these games may result in larger-than-optimal overall makespan. The ratio of the worst-case equilibrium makespan to the optimal makespan is called the price of anarchy of the game. In this paper, we design and analyze scheduling policies, or coordination mechanisms, for machines which aim to minimize the price of anarchy (restricted to pure Nash equilibria) of the corresponding game. We study coordination mechanisms for four classes of multiprocessor machine scheduling problems and derive upper and lower bounds for the price of anarchy of these mechanisms. For several of the proposed mechanisms, we also are able to prove that the system converges to a pure Nash equilibrium in a linear number of rounds. Finally, we note that our results are applicable to several practical problems arising in networking.\"",
        "1 is \"A first look at cellular network performance during crowded events\", 2 is \"On the complexity of scheduling in wireless networks\".",
        "\nGiven above information, for an author who has written the paper with the title \"QuickSense: Fast and energy-efficient channel sensing for dynamic spectrum access networks\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "015": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Argos: practical many-antenna base stations':",
        "title: \"Uniformly optimal 3-D fan filters for optical moving target detection\" with abstract: \"Broad-band (e.g. white) noise is sometimes the limiting factor in the detection of moving targets in a sequence of images. If the target velocity is known (and if the target intensity distribution is known) then the 3-D matched filter is optimal with respect to signal to noise ratio (SNR), however it suffers from velocity mismatch if the velocity is unknown. A bank of 3-D matched filters, each tuned to a different velocity, has been proposed for dealing with this problem. Alternatively one can use a new type of 3-D fan filter which passes undistorted all targets over a specified velocity set while providing maximum noise attenuation, thereby avoiding velocity mismatch. This paper obtains analytical expressions for the SNR improvement of these filters which facilitate the design of banks of these filters as well as performance trade-off studies.\"",
        "title: \"Performance of Conjugate and Zero-Forcing Beamforming in Large-Scale Antenna Systems\" with abstract: \"Large-Scale Antenna Systems (LSAS) is a form of multi-user MIMO technology in which unprecedented numbers of antennas serve a significantly smaller number of autonomous terminals. We compare the two most prominent linear pre-coders, conjugate beamforming and zero-forcing, with respect to net spectral-efficiency and radiated energy-efficiency in a simplified single-cell scenario where propagation is governed by independent Rayleigh fading, and where channel-state information (CSI) acquisition and data transmission are both performed during a short coherence interval. An effective-noise analysis of the pre-coded forward channel yields explicit lower bounds on net capacity which account for CSI acquisition overhead and errors as well as the sub-optimality of the pre-coders. In turn the bounds generate trade-off curves between radiated energy-efficiency and net spectral-efficiency. For high spectral-efficiency and low energy-efficiency zero-forcing outperforms conjugate beamforming, while at low spectral-efficiency and high energy-efficiency the opposite holds. Surprisingly, in an optimized system, the total LSAS-critical computational burden of conjugate beamforming may be greater than that of zero-forcing. Conjugate beamforming may still be preferable to zero-forcing because of its greater robustness, and because conjugate beamforming lends itself to a de-centralized architecture and de-centralized signal processing.\"",
        "title: \"Cell-Free Massive MIMO: Uniformly great service for everyone\" with abstract: \"We consider the downlink of Cell-Free Massive MIMO systems, where a very large number of distributed access points (APs) simultaneously serve a much smaller number of users. Each AP uses local channel estimates obtained from received uplink pilots and applies conjugate beamforming to transmit data to the users. We derive a closed-form expression for the achievable rate. This expression enables us to design an optimal max-min power control scheme that gives equal quality of service to all users. We further compare the performance of the Cell-Free Massive MIMO system to that of a conventional small-cell network and show that the throughput of the Cell-Free system is much more concentrated around its median compared to that of the smallcell system. The Cell-Free Massive MIMO system can provide an almost 20-fold increase in 95%-likely per-user throughput, compared with the small-cell system. Furthermore, Cell-Free systems are more robust to shadow fading correlation than smallcell systems.\"",
        "title: \"Cell-Free Massive Mimo Systems\" with abstract: \"Cell-Free Massive MIMO systems comprise a large number of distributed, low cost, and low power access point antennas, connected to a network controller. The number of antennas is significantly larger than the number of users. The system is not partitioned into cells and each user is served by all access point antennas simultaneously.In this paper, we define cell-free systems and analyze algorithms for power optimization and linear pre-coding. Compared with the conventional small-cell scheme, Cell-Free Massive MIMO can yield more than ten-fold improvement in terms of 5%-outage rate.\"",
        "title: \"Performance of cell-free massive MIMO systems with MMSE and LSFD receivers\" with abstract: \"Cell-Free Massive MIMO comprises a large number of distributed single-antenna access points (APs) serving a much smaller number of users. There is no partitioning into cells and each user is served by all APs. In this paper, the uplink performance of cell-free systems with minimum mean squared error (MMSE) and large scale fading decoding (LSFD) receivers is investigated. The main idea of LSFD receiver is to maximize achievable throughput using only large scale fading coefficients between APs and users. Capacity lower bounds for MMSE and LSFD receivers are derived. An asymptotic approximation for signal-to-interference-plus-noise ratio (SINR) of MMSE receiver is derived as a function of large scale fading coefficients only. The obtained approximation is accurate even for a small number of antennas. MMSE and LSFD receivers demonstrate five-fold and two-fold gains respectively over matched filter (MF) receiver in terms of 5%-outage rate.\"",
        "1 is \"Joint Beamforming and Broadcasting in Massive MIMO.\", 2 is \"Power modeling of graphical user interfaces on OLED displays\".",
        "\nGiven above information, for an author who has written the paper with the title \"Argos: practical many-antenna base stations\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "016": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Power estimation for cycle-accurate functional descriptions of hardware':",
        "title: \"Interconnect-aware high-level synthesis for low power\" with abstract: \"Interconnects (wires, buffers, clock distribution networks, multiplexers and buses) consume a significant fraction of total circuit power. In this work, we demonstrate the importance of optimizing on-chip interconnects for power during high-level synthesis. We present a methodology to integrate interconnect power optimization into high-level synthesis. Our binding algorithm not only reduces power consumption in functional units and registers in the resultant register-transfer level (RTL) architecture, but also optimizes interconnects for power. We take physical design information into account for this purpose. To estimate interconnect power consumption accurately for deep sub-micron (DSM) technologies, wire coupling capacitance is taken into. consideration. We observed that there is significant spurious (i.e., unnecessary) switching activity in the interconnects and propose techniques to reduce it. Compared to interconnect-unaware power-optimized circuits, our experimental results show that interconnect power can be reduced by 53.1% on an average, while reducing overall power by an average of 26.8% with 0.5% area overhead. Compared to area-optimized circuits, the interconnect power reduction is 72.9% and overall power reduction is 56.0% with 44.4% area overhead.\"",
        "title: \"Demo: Rio: a system solution for sharing I/O between mobile systems\" with abstract: \"A user nowadays owns a variety of mobile systems, including smartphones, tablets, smart glasses, and smart watches, each equipped with a plethora of I/O devices, such as cameras, speakers, microphones, sensors, and cellular modems. There are many interesting use cases in which an application running on one mobile system accesses I/O on another system, for three fundamental reasons. (i) Mobile systems can be in different physical locations or orientations. For example, one can control a smartphone's high-resolution camera from a tablet camera application to more easily capture a self-portrait. (ii) Mobile systems can serve different users. For example, one can a play music for another user if one's smartphone can access the other device's speaker. (iii) Certain mobile systems have unique I/O devices due to their distinct form factor and targeted use cases. For example, a user can make a phone call from her tablet using the modem and SIM card in her smartphone. Solutions exist for sharing I/O devices, e.g., for camera [1], speaker [2], and modem (for messaging) [3]. However, these solutions have three limitations. (i) They do not support unmodified applications. (ii) They do not expose all the functionality of an I/O device for sharing. (iii) They are I/O class-specific, requiring significant engineering effort to support new I/O devices. We demonstrate Rio (Remote I/O), an I/O sharing solution for mobile systems that overcomes all three aforementioned limitations. Rio adopts a split-stack I/O sharing model, in which the I/O stack is split between the two mobile systems at a certain boundary. All communications that cross this boundary are intercepted on the mobile system hosting the application and forwarded to the mobile system with the I/O device, where they are served by the rest of the I/O stack. Rio uses device files as its boundary of choice. Device files are used in Unix-like OSes, such as Android and iOS, to abstract many classes of I/O devices, providing an I/O class-agnostic boundary. The device file boundary supports I/O sharing for unmodified applications, as it is transparent to the application layer. It also exposes the full functionality of each I/O device to other mobile systems by allowing processes in one system to directly communicate with the device drivers in another. Rio is not the first system to exploit the device file boundary; our previous work, Paradice [5], uses device files as the boundary for I/O virtualization inside a single system. However, Rio faces a different set of challenges regarding how to properly exploit this boundary, as explained in the full paper [6]. In this demo, we use a prototype implementation of Rio for Android systems. Our implementation supports four important I/O classes: camera, audio devices such as speaker and microphone, sensors such as accelerometer, and cellular modem (for phone calls and SMS). It consists of about 7100 lines of code, of which less than 500 are specific to I/O classes. Rio also supports I/O sharing between heterogeneous mobile systems, including tablets and smartphones. See [4] for a video of the demo.\"",
        "title: \"Power signal processing: a new perspective for power analysis and optimization\" with abstract: \"To address the productivity bottlenecks in power analysis and optimization of modern systems, we propose to treat power as a signal and leverage the rich set of signal processing techniques. We first investigate the power signal properties of digital systems and analyze their limitations. We then study signal processing techniques to detect temporal and structural correlations of power signals. Finally, we employ these techniques to accelerate the simulation of an architecture-level power simulator. Our experiments with the SPEC2000 benchmark suite show that it is possible to accelerate power simulation by 100X without introducing significant errors at various resolution levels.\"",
        "title: \"Video: Rio: a system solution for sharing i/o between mobile systems\" with abstract: \"Modern mobile systems are equipped with a diverse collection of I/O devices, including cameras, microphones, various sensors, and cellular modem. There exist many novel use cases for allowing an application on one mobile system to utilize I/O devices from another. This video demonstrates Rio, an I/O sharing solution that supports unmodified applications and realizes many of these novel use cases. Rio's design is common to many classes of I/O devices, significantly reducing the engineering effort to support new I/O devices. Moreover, it supports all the functionalities of an I/O device for sharing. Rio also supports I/O sharing between mobile systems of different form factors, including smartphones and tablets.\"",
        "title: \"Data broadcasting using mobile FM radio: design, realization and application\" with abstract: \"In this work, we offer a novel system, MicroStation (\u03bcStation) that allows ubiquitous data broadcasting applications using the FM radio on mobile devices such as smartphones. \u03bcStation includes two key modules to enable data broadcasting based on existing mobile FM radio hardware. Channel Selector assigns different FM channels to neighboring \u03bcStation broadcasters to avoid collision and guides \u03bcStation listeners to find their broadcasting of interest. Data Codec realizes bit-level communication between mobile devices through existing FM radio hardware. We describe an implementation of \u03bcStation on the Nokia N900 smartphone, and provide low-level APIs and services to support application development. We also demonstrate two representative applications: Facebook-FM and Sync-Flash. These applications demonstrate the capability of \u03bcStation to readily enable a new class of ubiquitous data broadcasting applications on mobile devices.\"",
        "1 is \"Medium Access Control Protocols using Directional Antennas in Ad Hoc Networks\", 2 is \"Dynamic state and objective learning for sequential circuit automatic test generation using recomposition equivalence\".",
        "\nGiven above information, for an author who has written the paper with the title \"Power estimation for cycle-accurate functional descriptions of hardware\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "017": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Tag recommendations in social bookmarking systems':",
        "title: \"Geo_ML @ MediaEval Placing Task 2015.\" with abstract: \"We participated in the MediaEval Benchmarking whose goal is to concentrate on the multimodal geo-location prediction on the Yahoo! Flickr Creative Commons 100M dataset - the placing task. It challenges participants to develop models and/or techniques to estimate the geographic locations of the Flickr resources based on textual metadata, e.g. titles, descriptions and tags. We aim to nd a procedure that is conceptual to understand, simple to implement and exible to integrate dierent techniques. In this paper, we present a three-step approach to tackle the locale-based sub-task.\"",
        "title: \"Hybrid Matrix Factorization Update for Progress Modeling in Intelligent Tutoring Systems.\" with abstract: \"Intelligent Tutoring Systems often profit of intelligent components, which allow to personalize the proposed contents' characteristics and sequence. Adaptive sequencing, in particular, requires either a detrimental data collection for users or extensive domain information provided by experts of the educational area. In this paper we propose an efficient domain independent method to model student progress that can be later used to sequence tasks in large commercial systems. The developed method is based on the integration of domain independent Matrix Factorization Performance Prediction with Kalman Filters state modeling abilities. Our solution not only reduces the prediction error, but also possesses a more computationally efficient model update. Finally, we give hints about a potential interpretability of student's state computed by Matrix Factorization, that, because of its implicit modeling, did not allow human experts, to monitor user's knowledge acquisition.\"",
        "title: \"Hyperparameter Search Space Pruning \u2013 A New Component for Sequential Model-Based Hyperparameter Optimization\" with abstract: \"The optimization of hyperparameters is often done manually or exhaustively but recent work has shown that automatic methods can optimize hyperparameters faster and even achieve better final performance. Sequential model-based optimization SMBO is the current state of the art framework for automatic hyperparameter optimization. Currently, it consists of three components: a surrogate model, an acquisition function and an initialization technique. We propose to add a fourth component, a way of pruning the hyperparameter search space which is a common way of accelerating the search in many domains but yet has not been applied to hyperparameter optimization. We propose to discard regions of the search space that are unlikely to contain better hyperparameter configurations by transferring knowledge from past experiments on other data sets as well as taking into account the evaluations already done on the current data set. Pruning as a new component for SMBO is an orthogonal contribution but nevertheless we compare it to surrogate models that learn across data sets and extensively investigate the impact of pruning with and without initialization for various state of the art surrogate models. The experiments are conducted on two newly created meta-data sets which we make publicly available. One of these meta-data sets is created on 59 data sets using 19 different classifiers resulting in a total of about 1.3 million experiments. This is by more than four times larger than all the results collaboratively collected by OpenML.\"",
        "title: \"Combining Multi-Distributed Mixture Models and Bayesian Networks for Semi-Supervised Learning\" with abstract: \"In many real world scenarios, mixture models have successfully been used for analyzing features in data ([11, 13, 21]). Usually, multivariate Gaussian distributions for continuous data ([2, 8, 4]) or Bayesian networks for nominal data ([15, 16]) are applied. In this paper, we combine both approaches in a family of Bayesian models for continuous data that are able to handle univariate as well as multivariate nodes, different types of distributions, e.g. Gaussian as well as Poisson distributed nodes, and dependencies between nodes. The models we introduce can be used for unsupervised, semi-supervised as well as for fully supervised learning tasks. We evaluate our models empirically on generated synthetic data and on public datasets thereby showing that they outperform classifiers such as SVMs and logistic regression on mixture data.\"",
        "title: \"Ultra-Fast Shapelets for Time Series Classification.\" with abstract: \"  Time series shapelets are discriminative subsequences and their similarity to a time series can be used for time series classification. Since the discovery of time series shapelets is costly in terms of time, the applicability on long or multivariate time series is difficult. In this work we propose Ultra-Fast Shapelets that uses a number of random shapelets. It is shown that Ultra-Fast Shapelets yield the same prediction quality as current state-of-the-art shapelet-based time series classifiers that carefully select the shapelets by being by up to three orders of magnitudes. Since this method allows a ultra-fast shapelet discovery, using shapelets for long multivariate time series classification becomes feasible.   A method for using shapelets for multivariate time series is proposed and Ultra-Fast Shapelets is proven to be successful in comparison to state-of-the-art multivariate time series classifiers on 15 multivariate time series datasets from various domains. Finally, time series derivatives that have proven to be useful for other time series classifiers are investigated for the shapelet-based classifiers. It is shown that they have a positive impact and that they are easy to integrate with a simple preprocessing step, without the need of adapting the shapelet discovery algorithm. \"",
        "1 is \"An Instance-based Approach for Identifying Candidate Ontology Relations within a Multi-Agent System\", 2 is \"Spreading Activation Models for Trust Propagation\".",
        "\nGiven above information, for an author who has written the paper with the title \"Tag recommendations in social bookmarking systems\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "018": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Online Robust Low-Rank Tensor Modeling for Streaming Data Analysis.':",
        "title: \"Deep Recurrent Regression for Facial Landmark Detection.\" with abstract: \"We propose a novel end-to-end deep architecture for face landmark detection, based on a deep convolutional and deconvolutional network followed by carefully designed recurrent network structures. The pipeline of this architecture consists of three parts. Through the first part, we encode an input face image to resolution-preserved deconvolutional feature maps via a deep network with stacked convol...\"",
        "title: \"Exact Low Tubal Rank Tensor Recovery from Gaussian Measurements.\" with abstract: \"The recent proposed Tensor Nuclear Norm (TNN) [Lu et al., 2016; 2018a] is an interesting convex penalty induced by the tensor SVD [Kilmer and Martin, 2011]. It plays a similar role as the matrix nuclear norm which is the convex surrogate of the matrix rank. Considering that the TNN based Tensor Robust PCA [Lu et al., 2018a] is an elegant extension of Robust PCA with a similar tight recovery bound, it is natural to solve other low rank tensor recovery problems extended from the matrix cases. However, the extensions and proofs are generally tedious. The general atomic norm provides a unified view of low-complexity structures induced norms, e.g., the $ell_1$-norm and nuclear norm. The sharp estimates of the required number of generic measurements for exact recovery based on the atomic norm are known in the literature. In this work, with a careful choice of the atomic set, we prove that TNN is a special atomic norm. Then by computing the Gaussian width of certain cone which is necessary for the sharp estimate, we achieve a simple bound for guaranteed low tubal rank tensor recovery from Gaussian measurements. Specifically, we show that by solving a TNN minimization problem, the underlying tensor of size $n_1times n_2times n_3$ with tubal rank $r$ can be exactly recovered when the given number of Gaussian measurements is $O(r(n_1+n_2-r)n_3)$. It is order optimal when comparing with the degrees of freedom $r(n_1+n_2-r)n_3$. Beyond the Gaussian mapping, we also give the recovery guarantee of tensor completion based on the uniform random mapping by TNN minimization. Numerical experiments verify our theoretical results.\"",
        "title: \"Image Classification by Selective Regularized Subspace Learning\" with abstract: \"Feature learning is an intensively studied research topic in image classification. Although existing methods like sparse coding, locality-constrained linear coding, fisher vector encoding, etc., have shown their effectiveness in image representation, most of them overlook a phenomenon called thesmall sample size problem, where the number of training samples is relatively smaller than the dimensionality of the features, which may limit the predictive power of the classifier. Subspace learning is a strategy to mitigate this problem by reducing the dimensionality of the features. However, most conventional subspace learning methods attempt to learn a global subspace to discriminate all the classes, which proves to be difficult and ineffective in multi-class classification task. To this end, we propose to learn a local subspace for each sample instead of learning a global subspace for all samples. Our key observation is that, in multi-class image classification, the label of each testing sample is only confused by a few classes which have very similar visual appearance to it. Thus, in this work, we propose a coarse-to-fine strategy, which first picks out such classes, and then conducts a local subspace learning to discriminate them. As the subspace learning method is regularized and conducted within some selected classes, we term it selective regularized subspace learning (SRSL), and we term our classification pipeline selective regularized subspace learning based multi-class image classification (SRSL_MIC). Experimental results on four representative datasets (Caltech-101, Indoor-67, ORL Faces and AR Faces) demonstrate the effectiveness of the proposed method.\"",
        "title: \"Diversified Visual Attention Networks for Fine-Grained Object Classification.\" with abstract: \"Fine-grained object classification attracts increasing attention in multimedia applications. However, it is a quite challenging problem due to the subtle interclass difference and large intraclass variation. Recently, visual attention models have been applied to automatically localize the discriminative regions of an image for better capturing critical difference, which have demonstrated promising performance. Unfortunately, without consideration of the diversity in attention process, most of existing attention models perform poorly in classifying fine-grained objects. In this paper, we propose a diversified visual attention network (DVAN) to address the problem of fine-grained object classification, which substantially relieves the dependency on strongly supervised information for learning to localize discriminative regions com-pared with attention-less models. More importantly, DVAN explicitly pursues the diversity of attention and is able to gather discriminative information to the maximal extent. Multiple attention canvases are generated to extract convolutional features for attention. An LSTM recurrent unit is employed to learn the attentiveness and discrimination of attention canvases. The proposed DVAN has the ability to attend the object from coarse to fine granularity, and a dynamic internal representation for classification is built up by incrementally combining the information from different locations and scales of the image. Extensive experiments conducted on CUB-2011, Stanford Dogs, and Stanford Cars datasets have demonstrated that the pro-posed DVAN achieves competitive performance compared to the state-of-the-art approaches, without using any prior knowledge, user interaction, or external resource in training and testing.\"",
        "title: \"Robust multiperson detection and tracking for mobile service and social robots.\" with abstract: \"This paper proposes an efficient system which integrates multiple vision models for robust multiperson detection and tracking for mobile service and social robots in public environments. The core technique is a novel maximum likelihood (ML)-based algorithm which combines the multimodel detections in mean-shift tracking. First, a likelihood probability which integrates detections and similarity to local appearance is defined. Then, an expectation-maximization (EM)-like mean-shift algorithm is derived under the ML framework. In each iteration, the E-step estimates the associations to the detections, and the M-step locates the new position according to the ML criterion. To be robust to the complex crowded scenarios for multiperson tracking, an improved sequential strategy to perform the mean-shift tracking is proposed. Under this strategy, human objects are tracked sequentially according to their priority order. To balance the efficiency and robustness for real-time performance, at each stage, the first two objects from the list of the priority order are tested, and the one with the higher score is selected. The proposed method has been successfully implemented on real-world service and social robots. The vision system integrates stereo-based and histograms-of-oriented-gradients-based human detections, occlusion reasoning, and sequential mean-shift tracking. Various examples to show the advantages and robustness of the proposed system for multiperson tracking from mobile robots are presented. Quantitative evaluations on the performance of multiperson tracking are also performed. Experimental results indicate that significant improvements have been achieved by using the proposed method.\"",
        "1 is \"Non-homogeneous Content-driven Video-retargeting\", 2 is \"Exploring Context with Deep Structured models for Semantic Segmentation.\".",
        "\nGiven above information, for an author who has written the paper with the title \"Online Robust Low-Rank Tensor Modeling for Streaming Data Analysis.\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "019": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'A framework for ETH-tight algorithms and lower bounds in geometric intersection graphs.':",
        "title: \"Covering Many or Few Points with Unit Disks\" with abstract: \"Let P be a set of n weighted points. We study approximation algorithms for the following two continuous facility-location problems. In the first problem we want to place m unit disks, for a given constant m\u22651, such that the total weight of the points from P inside the union of the disks is maximized. We present a deterministic algorithm that can compute, for any \u03b50, a (1\u2212\u03b5)-approximation to the optimal solution in O(n logn + \u03b5$^{{\\rm -4}{\\it m}}$log$^{\\rm 2{\\it m}}$ (1/\u03b5)) time. In the second problem we want to place a single disk with center in a given constant-complexity region X such that the total weight of the points from P inside the disk is minimized. Here we present an algorithm that can compute, for any \u03b50, with high probability a (1+\u03b5)-approximation to the optimal solution in O(n (log3n + \u03b5\u22124 log2n )) expected time.\"",
        "title: \"Treemaps with bounded aspect ratio\" with abstract: \"Treemaps are a popular technique to visualize hierarchical data. The input is a weighted tree T where the weight of each node is the sum of the weights of its children. A treemap for T is a hierarchical partition of a rectangle into simply connected regions, usually rectangles. Each region represents a node of T and its area is proportional to the weight of the corresponding node. An important quality criterion for treemaps is the aspect ratio of its regions. One cannot bound the aspect ratio if the regions are restricted to be rectangles. In contrast, polygonal partitions, that use convex polygons, can have bounded aspect ratio. We are the first to obtain convex partitions with optimal aspect ratio O(depth(T)). However, depth(T) still depends on the input tree. Hence we introduce a new type of treemaps, namely orthoconvex treemaps, where regions representing leaves are rectangles, L-, and S-shapes, and regions representing internal nodes are orthoconvex polygons. We prove that any input tree, irrespective of the weights of the nodes and the depth of the tree, admits an orthoconvex treemap of constant aspect ratio. We also obtain several specialized results for single-level treemaps, that is, treemaps where the input tree has depth 1.\"",
        "title: \"On The Design Of Genetic Algorithms For Geographical Applications\" with abstract: \"In many geographical optimization problems, the linkage (which determines the structure of building blocks) is determined by the spatial relationships between the components of a solution. Therefore the linkage can be identified easily, unlike in most other problems. Based ox this observation, we develop a hybrid GA that uses a geometrically local optimiser-one that computes good solutions to subproblems that are local in the geometric sense. One of the main advantages of our method is that it leads to GA's that are easily adapted to slight changes in the problem definition, without the need to tune many parameters in the fitness function. We apply our method to the map labeling problem, (placing as many names as possible on a map, without overlap), where it leads to good results.\"",
        "title: \"Improved Bounds for the Union of Locally Fat Objects in the Plane.\" with abstract: \"We show that, for any gamma > 0, the combinatorial complexity of the union of n locally gamma-fat objects of constant complexity in the plane is n/gamma(4)2(O(log* n)). For the special case of gamma-fat triangles, the bound improves to O(n log* n + n/gamma log(2) 1/gamma).\"",
        "title: \"Delineating imprecise regions via shortest-path graphs\" with abstract: \"An imprecise region, also called a vernacular region, is a region without a precise or administrative boundary. We present a new method to delineate imprecise regions from a set of points that are likely to lie inside the region. We use shortest-path graphs based on the squared Euclidean distance which capture the shape of region boundaries well. Shortest-path graphs naturally adapt to point sets of varying density, and they are always connected. As opposed to neighborhood graphs, they use a non-local criterion to determine which points to connect. Furthermore, shortest-path graphs can easily be extended to take geographic context into account by modeling context as \"soft\" obstacles. We present efficient algorithms to compute shortest-path graphs with or without geographic context. We experimentally evaluate the quality of the imprecise regions computed with our method. To fairly compare our results to those obtained by the common KDE approach, we also show how to integrate context into KDE by again using soft obstacles.\"",
        "1 is \"Decomposition of domains based on the micro-structure of finite constraint-satisfaction problems\", 2 is \"Improved algorithms for fully dynamic geometric spanners and geometric routing\".",
        "\nGiven above information, for an author who has written the paper with the title \"A framework for ETH-tight algorithms and lower bounds in geometric intersection graphs.\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0110": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'A Framework For Actively Selecting Viewpoints In Object Recognition':",
        "title: \"Learning with few examples for binary and multiclass classification using regularization of randomized trees\" with abstract: \"The human visual system is often able to learn to recognize difficult object categories from only a single view, whereas automatic object recognition with few training examples is still a challenging task. This is mainly due to the human ability to transfer knowledge from related classes. Therefore, an extension to Randomized Decision Trees is introduced for learning with very few examples by exploiting interclass relationships. The approach consists of a maximum a posteriori estimation of classifier parameters using a prior distribution learned from similar object categories. Experiments on binary and multiclass classification tasks show significant performance gains\"",
        "title: \"Online Next-Best-View Planning for Accuracy Optimization Using an Extended E-Criterion\" with abstract: \"Next-best-view (NBV) planning is an important aspect for three-dimensional (3D) reconstruction within controlled environments, such as a camera mounted on a robotic arm. NBV methods aim at a purposive 3D reconstruction sustaining predefined goals and limitations. Up to now, literature mainly presents NBV methods for range sensors, model-based approaches or algorithms that address the reconstruction of a finite set of primitives. For this work, we use an intensity camera without active illumination. We present a novel combined online approach comprising feature tracking, 3D reconstruction, and NBV planning that addresses arbitrary unknown objects. In particular we focus on accuracy optimization based on the reconstruction uncertainty. To this end we introduce an extension of the statistical E-criterion to model directional uncertainty, and we present a closed-form, optimal solution to this NBV planning problem. Our experimental evaluation demonstrates the effectivity of our approach using an absolute error measure.\"",
        "title: \"A comparison of nearest neighbor search algorithms for generic object recognition\" with abstract: \"The nearest neighbor (NN) classifier is well suited for generic object recognition. However, it requires storing the complete training data, and classification time is linear in the amount of data. There are several approaches to improve runtime and/or memory requirements of nearest neighbor methods: Thinning methods select and store only part of the training data for the classifier. Efficient query structures reduce query times. In this paper, we present an experimental comparison and analysis of such methods using the ETH-80 database. We evaluate the following algorithms. Thinning: condensed nearest neighbor, reduced nearest neighbor, Baram's algorithm, the Baram-RNN hybrid algorithm, Gabriel and GSASH thinning. Query structures: kd-tree and approximate nearest neighbor. For the first four thinning algorithms, we also present an extension to k-NN which allows tuning the trade-off between data reduction and classifier degradation. The experiments show that most of the above methods are well suited for generic object recognition.\"",
        "title: \"Semantic segmentation with millions of features: integrating multiple cues in a combined random forest approach\" with abstract: \"In this paper, we present a new combined approach for feature extraction, classification, and context modeling in an iterative framework based on random decision trees and a huge amount of features. A major focus of this paper is to integrate different kinds of feature types like color, geometric context, and auto context features in a joint, flexible and fast manner. Furthermore, we perform an in-depth analysis of multiple feature extraction methods and different feature types. Extensive experiments are performed on challenging facade recognition datasets, where we show that our approach significantly outperforms previous approaches with a performance gain of more than 15% on the most difficult dataset.\"",
        "title: \"Tracking and reconstruction in a combined optimization approach.\" with abstract: \"We present a novel approach to the structure-from-motion problem which combines the search for correspondences and geometric reconstruction, rather than treating these as separate steps. Through the combination of the two steps, we achieve an implicit feedback of 3D information to aid the correspondence search, and at the same time we avoid an explicit model for tracking errors. The reconstruction results are therefore optimal in case of, for example, Gaussian noise on image intensities. We also present an efficient online framework for structure-from-motion with our combined approach, thoroughly evaluate the method in experiments and compare the results to state-of-the-art methods.\"",
        "1 is \"Active Search for Real-Time Vision\", 2 is \"The Random Subspace Method for Constructing Decision Forests\".",
        "\nGiven above information, for an author who has written the paper with the title \"A Framework For Actively Selecting Viewpoints In Object Recognition\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0111": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Model-Driven web form validation with UML and OCL':",
        "title: \"Animation Can Show Only the Presence of Errors, Never Their Absence\" with abstract: \"Abstract: A formal specification animator executes and interprets traces on a specification. Similar to software testing, animation can only show the presence of errors, never their absence. However, animation is a powerful means of finding errors, and it is important that we adequately exercise a specification when we animate it. This paper outlines a systematic approach to the animation of formal specifications. We demonstrate the method on a small example, and then discuss its application to a non-trivial, system-level specification. Our aim is to provide a method for planned, documented and maintainable animation of specifications, so that we can achieve a high level of coverage, evaluate the adequacy of the animation, and repeat the process at a later time.\"",
        "title: \"Specification-Based Class Testing: A Case Study\" with abstract: \"This paper contains a case study demonstrating a complete process for specification-based class testing. The process starts with an abstract specification written in Object-Z and concludes by exercising an implementation with test cases and evaluating the results. The test cases are derived using the Test Template Framework for each individual operation. They are analysed to generate a finite state machine that can execute test sequences within the ClassBench framework. An oracle is also derived from the Object-Z specification. The case study demonstrates how a formal specification contributes to the development of practical tests that can be executed by a testing tool. It also shows how a test oracle can be derived from a specification and used by the same testing tool to evaluate test results.\"",
        "title: \"Refinement of higher-order logic programs\" with abstract: \"A refinement calculus provides a method for transforming specifications to executable code, maintaining the correctness of the code with respect to its specification. In this paper we extend the refinement calculus for logic programs to include higher-order programming capabilities in specifications and programs, such as procedures as terms and lambda abstraction. We use a higher-order type and term system to describe programs, and provide a semantics for the higher-order language and refinement. The calculus is illustrated by refinement examples.\"",
        "title: \"Grammar-based test generation with YouGen\" with abstract: \"Grammars are traditionally used to recognize or parse sentences in a language, but they can also be used to generate sentences. In grammar-based test generation (GBTG), context-free grammars are used to generate sentences that are interpreted as test cases. A generator reads a grammar G and generates L(G), the language accepted by the grammar. Often L(G) is so large that it is not practical to execute all of the generated cases. Therefore, GBTG tools support \u2018tags\u2019: extra-grammatical annotations which restrict the generation. Since its introduction in the early 1970s, GBTG has become well established: proven on industrial projects and widely published in academic venues. Despite the demonstrated effectiveness, the tool support is uneven; some tools target specific domains, e.g. compiler testing, while others are proprietary. The tools can be difficult to use and the precise meaning of the tags are sometimes unclear. As a result, while many testing practitioners and researchers are aware of GBTG, few have detailed knowledge or experience. We present YouGen, a new GBTG tool supporting many of the tags provided by previous tools. In addition, YouGen incorporates covering-array tags, which support a generalized form of pairwise testing. These tags add considerable power to GBTG tools and have been available only in limited form in previous GBTG tools. We provide semantics for the YouGen tags using parse trees and a new construct, generation trees. We illustrate YouGen with both simple examples and a number of industrial case studies. Copyright \u00a9 2010 John Wiley & Sons, Ltd.\"",
        "title: \"A state-of-practice questionnaire on verification and validation for concurrent programs\" with abstract: \"Research in verification and validation (V&V) for concurrent programs can be guided by practitioner information. A survey was therefore run to gain state-of-practice information in this context. The survey presented in this paper collected state-of-practice information on V&V technology in concurrency from 35 respondents. The results of the survey can help refine existing V&V technology by providing a better understanding of the context of V&V technology usage. Responses to questions regarding the motivation for selecting V&V technologies can help refine a systematic approach to V&V technology selection.\"",
        "1 is \"Impartiality, Justice and Fairness: The Ethics of Concurrent Termination\", 2 is \"Software product lines: a case study\".",
        "\nGiven above information, for an author who has written the paper with the title \"Model-Driven web form validation with UML and OCL\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0112": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Probabilistic Roadmaps for Path Planning in High-Dimensional Configuration Spaces':",
        "title: \"Natural Motion Generation for Humanoid Robots\" with abstract: \"This paper presents a method of generating natural-looking motion primitives for humanoid robots. An optimization-based approach is used to generate these primitives, but the objective function is tailored to each one and complexity is reduced by identifying relevant degrees of freedom. Several examples are shown in simulation: for an arm movement to reach an object, it is better to minimize the acceleration of key parts of the robot over its entire trajectory; for a single step on flat ground, it is better to minimize the torque and instantaneous angular momentum at every posture. The primitives are precomputed off-line, but might be used by on-line planner either to provide a fixed set of maneuvers or to bias a probabilistic, sample-based search for motions.\"",
        "title: \"Landmark-Based Robot Navigation\" with abstract: \" Achieving goals despite uncertainty in control and sensing may require robots toperform complicated motion planning and execution monitoring. This paper describesa reduced version of the general planning problem in the presence of uncertainty anda complete polynomial algorithm solving it. The planner computes a guaranteed plan(for given uncertainty bounds) by backchaining omnidirectional backprojections of thegoal until one fully contains the set of possible initial positions of the robot. ... \"",
        "title: \"Motion Strategies For Maintaining Visibility Of A Moving Target\" with abstract: \"We introduce the problem of computing robot motion strategies that maintain visibility of a moving target in a cluttered workspace. Both motion constraints(as considered in standard motion planning) and visibility constraints (as considered in visual tracking) must be satisfied. Additional criteria, such as the total distance traveled, can be optimized. The general problem is divided into two categories, on the basis of whether the target is predictable. For the predictable cease, an algorithm that computes optimal, numerical solutions is presented. For the more challenging case of a partially-predictable target, two on-line algorithms are presented that each attempt to maintain future visibility with limited prediction. One strategy maximizes the probability that the target will remain in view in a subsequent time step, and the other maximizes the minimum time in which the target could escape the visibility region. We additionally discuss issues resulting from our implementation and experiments on a mobile robot system.\"",
        "title: \"Modeling Structural Heterogeneity in Proteins from X-Ray Data\" with abstract: \"In a crystallographic experiment, a protein is precipitated to obtain a crystalline sample (crystal) containing many copies of the molecule. An electron density map (EDM) is calculated from diffraction images obtained from focusing X-rays through the sample at different angles. This involves iterative phase determination and density calculation. The protein conformation is modeled by placing the atoms in 3-D space to best match the electron density. In practice, the copies of a protein in a crystal are not exactly in the same conformation. Consequently the obtained EDM, which corresponds to the cumulative distribution of atomic positions over all conformations, is blurred. Existing modeling methods compute an \"average\" protein conformation by maximizing its fit with the EDM and explain structural heterogeneity in the crystal with a harmonic distribution of the position of each atom. However, proteins undergo coordinated conformational variations leading to substantial correlated changes in atomic positions. These variations are biologically important. This paper presents a sample-select approach to model structural heterogeneity by computing an ensemble of conformations (along with occupancies) that, collectively, provide a near-optimal explanation of the EDM. The focus is on deformable protein fragments, mainly loops and side-chains. Tests were successfully conducted on simulated and experimental EDMs.\"",
        "title: \"Planning motions with intentions\" with abstract: \"We apply manipulation planning to computer animation. A new path planner is presented that automatically computes the collision-free trajectories for several cooperating arms to manipulate a movable object between two configurations. This implemented planner is capable of dealing with complicated tasks where regrasping is involved. In addition, we present a new inverse kinematics algorithm for the human arms. This algorithm is utilized by the planner for the generation of realistic human arm motions as they manipulate objects. We view our system as a tool for facilitating the production of animation.\"",
        "1 is \"A probabilistic roadmap approach for systems with closed kinematic chains\", 2 is \"Geometric And Computational Aspects Of Gravity Casting\".",
        "\nGiven above information, for an author who has written the paper with the title \"Probabilistic Roadmaps for Path Planning in High-Dimensional Configuration Spaces\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0113": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'The effect of memory capacity on Time Warp performance':",
        "title: \"Parallel simulation techniques for large-scale networks\" with abstract: \"Simulation has always been an indispensable tool in the design and analysis of telecommunication networks. Due to performance limitations of the majority of simulators, usually network simulations have been done for rather small network models and for short timescales. In contrast, many difficult design problems facing today's network engineers concern the behavior of very large hierarchical multihop networks carrying millions of multiprotocol flows over long timescales. Examples include scalability and stability of routing protocols, packet losses in core routers, of long-lasting transient behavior due to observed self-similarity of traffic patterns. Simulation of such systems would greatly benefit from application of parallel computing technologies, especially now that multiprocessor workstations and servers have become commonly available. However, parallel simulation has not yet been widely embraced by the telecommunications community due to a number of difficulties. Based on our accumulated experience in parallel network simulation projects, we believe that parallel simulation technology has matured to the point that it is ready to be used in industrial practice of network simulation. This article highlights work in parallel simulations of networks and their promise\"",
        "title: \"Energy consumption of HLA data distribution management approaches\" with abstract: \"Energy and power aware data distribution methods are essential for using these approaches in energy constrained devices and environments. Data Distribution Management (DDM) is a set of services defined in the High Level Architecture (HLA) that aims to efficiently propagate distributed simulation state information. This paper describes an empirical study of the energy consumption of computation and communication components of region based and grid based DDM approaches in mobile devices. Experimental data illustrate that region based approaches tend to consume more energy than grid based approaches for computations, but less for communications. These results also show that the choice of grid cell size and grid cell constraints on publication regions can play an important role in the energy efficiency of grid based approaches.\n\n\"",
        "title: \"Next Generation Real-Time RTI Software\" with abstract: \"Abstract: This paper describes the recent changes to the RTI-Kit and changes being studied to develop new paradigms for real-time distributed simulation execution. We discuss design options for hard real-time (HRT) extensions to the High-Level Architecture and current efforts to adapt the high-performance RTI-Kit modules to a HRT RTI. We give some preliminary results supporting the efficacy of a hard real-time RTI implementation and discuss the impact of its availability.\n\n\"",
        "title: \"Scalable simulation of electromagnetic hybrid codes\" with abstract: \"New discrete-event formulations of physics simulation models are emerging that can outperform models based on traditional time-stepped techniques. Detailed simulation of the Earth's magnetosphere, for example, requires execution of sub-models that are at widely differing timescales. In contrast to time-stepped simulation which requires tightly coupled updates to entire system state at regular time intervals, the new discrete event simulation (DES) approaches help evolve the states of sub-models on relatively indepen-dent timescales. However, parallel execution of DES-based models raises challenges with respect to their scalability and performance. One of the key challenges is to improve the computation granularity to offset synchronization and communication overheads within and across processors. Our previous work was limited in scalability and runtime performance due to the parallelization challenges. Here we report on optimizations we performed on DES-based plasma simulation models to improve parallel performance. The mapping of model to simulation processes is optimized via aggregation techniques, and the parallel runtime engine is optimized for communication and memory efficiency. The net result is the capability to simulate hybrid particle-in-cell (PIC) models with over 2 billion ion particles using 512 processors on supercomputing platforms.\"",
        "title: \"Real-time data driven arterial simulation for performance measures estimation\" with abstract: \"Transportation professionals are increasingly exploring multi-pronged solutions to alleviate traffic congestion. Real-time information systems for travelers and facility managers are one approach that has been the focus of many recent efforts. Real-time performance information can facilitate more efficient roadway usage and operations. Toward this end, a dynamic data driven simulation based system for estimating and predicting performance measures along arterial streets in real-time is described that uses microscopic traffic simulations, driven by point sensor data. Current practices of real-time estimation of roadway performance measures are reviewed. The proposed real-time data driven arterial simulation methodology to estimate performance measures along arterials is presented as well as preliminary field results that provide evidence to validate this approach.\"",
        "1 is \"Using Web services to integrate heterogeneous simulations in a grid environment\", 2 is \"Gathering correlated data in sensor networks\".",
        "\nGiven above information, for an author who has written the paper with the title \"The effect of memory capacity on Time Warp performance\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0114": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'A Frame-Level Rate Control Scheme Based on Texture and Nontexture Rate Models for High Efficiency Video Coding':",
        "title: \"Motion-decision based spatiotemporal saliency for video sequences\" with abstract: \"An adaptive spatiotemporal saliency algorithm for video attention detection using motion vector decision is proposed, motivated by the importance of motion information in video sequences for human visual system. This novel system can detect the saliency regions quickly by using only part of the classic saliency features in each iteration. Motion vectors calculated by block matching and optical flow are used to determine the decision condition. When significant motion contrast occurs (decision condition is satisfied), the saliency area is detected by motion and intensity features. Otherwise, when motion contrast is low, color and orientation features are added to form a more detailed saliency map. Experimental results show that the proposed algorithm can detect salient objects and actions in video sequences robustly and efficiently.\"",
        "title: \"An effective example-based learning method for denoising of medical images corrupted by heavy Gaussian noise and poisson noise\" with abstract: \"Denoising is an essential application to improve image quality, especially in medical imaging. This paper introduces an example and patch-based learning method for reducing Gaussian noise and Poisson noise which often appear in medical imaging modalities using ionizing radiation. In the proposed method, denoising is performed by learning the regression model based on a set of the nearest neighbors of a given noisy patch, with the help of a given set of standard images. The method is evaluated and compared to several state-of-the-art denoising methods. The obtained results confirm its efficiency, especially for heavy noise.\"",
        "title: \"Robust tracking and mapping with a handheld RGB-D camera\" with abstract: \"In this paper, we propose a robust method for camera tracking and surface mapping using a handheld RGB-D camera which is effective in challenging situations such as fast camera motion or geometrically featureless scenes. The main contributions are threefold. First, we introduce a robust orientation estimation based on quaternion method for initial sparse estimation. By using visual feature points detection and matching, no prior or small movement assumption is required to estimate a rigid transformation between frames. Second, a weighted ICP (Iterative Closest Point) method for better rate of convergence in optimization and accuracy in resulting trajectory is proposed. While the conventional ICP fails when there is no 3D features in the scene, our approach achieves robustness by emphasizing the influence of points that contain more geometric information of the scene. Finally, we show quantitative results on an RGB-D benchmark dataset. The experiments on an RGB-D trajectory benchmark dataset demonstrate that our method is able to track camera pose accurately.\"",
        "title: \"Accelerating GMM-based patch priors for image restoration: Three ingredients for a 100\u00d7 speed-up.\" with abstract: \"Image restoration methods aim to recover the underlying clean image from corrupted observations. The expected patch log-likelihood (EPLL) algorithm is a powerful image restoration method that uses a Gaussian mixture model (GMM) prior on the patches of natural images. Although it is very effective for restoring images, its high runtime complexity makes the EPLL ill-suited for most practical applica...\"",
        "title: \"Vector sparse representation of color image using quaternion matrix analysis.\" with abstract: \"Traditional sparse image models treat color image pixel as a scalar, which represents color channels separately or concatenate color channels as a monochrome image. In this paper, we propose a vector sparse representation model for color images using quaternion matrix analysis. As a new tool for color image representation, its potential applications in several image-processing tasks are presented, including color image reconstruction, denoising, inpainting, and super-resolution. The proposed model represents the color image as a quaternion matrix, where a quaternion-based dictionary learning algorithm is presented using the K-quaternion singular value decomposition (QSVD) (generalized K-means clustering for QSVD) method. It conducts the sparse basis selection in quaternion space, which uniformly transforms the channel images to an orthogonal color space. In this new color space, it is significant that the inherent color structures can be completely preserved during vector reconstruction. Moreover, the proposed sparse model is more efficient comparing with the current sparse models for image restoration tasks due to lower redundancy between the atoms of different color channels. The experimental results demonstrate that the proposed sparse image model avoids the hue bias issue successfully and shows its potential as a general and powerful tool in color image analysis and processing domain.\"",
        "1 is \"Monte-Carlo sure: a black-box optimization of regularization parameters for general denoising algorithms.\", 2 is \"On Distributed Systems and Social Engineering\".",
        "\nGiven above information, for an author who has written the paper with the title \"A Frame-Level Rate Control Scheme Based on Texture and Nontexture Rate Models for High Efficiency Video Coding\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0115": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Efficient and adaptive proportional share I/O scheduling':",
        "title: \"Scheduling multiple flows on parallel disks\" with abstract: \"We examine the problem of scheduling concurrent independent flows on multiple-disk I/O storage systems. Two models are considered: in the shared buffer model the memory buffer is shared among all the flows, while in the partitioned buffer model each flow has a private buffer. For the parallel disk model with d 1 disks it is shown that the problem of minimizing the schedule length of n 2 concurrent flows is NP-complete for both buffer models. A randomized scheduling algorithm for the partitioned buffer model is analyzed and probabilistic bounds on the schedule length are presented. Finally a heuristic based on static buffer allocation for the shared buffer model is discussed.\"",
        "title: \"Statistical workload shaping for storage systems\" with abstract: \"Data center computing is gaining popularity due to the cost efficiencies of consolidation, centralized management, and high reliability. The unpredictable bursty nature of typical workloads where the instantaneous arrival rates can significantly exceed the average long-term rate, requires the server to significantly over provision resources in order to meet response-time service level agreements (SLAs), resulting in low resource utilization and higher costs. In this paper we consider a statistical on-off model for bursty workloads to explore the relationship between burst size, frequency, capacity, and response time distribution. We analyze the performance of a workload shaping method to reduce capacity requirements by providing a graduated two-level SLA. Statistics of the request arrival to the overflow queue are characterized in terms of the underlying Markov chain passage times, and used to estimate its capacity.\"",
        "title: \"Lexicographic QoS scheduling for parallel I/O\" with abstract: \"High-end shared storage systems serving multiple independent workloads must assure that concurrently executing clients will receive a fair or agreed-upon share of system I/O resources. In a parallel I/O system an application makes requests for specific disks at different steps of its computation depending on the data layout and its computational state. Different applications contend for disk access making the problem of maintaining fair allocation challenging.We propose a model for differentiated disk bandwidth allocation based on lexicographic minimization, and provide new efficient scheduling algorithms to allocate the I/O bandwidth fairly among contending applications. A major contribution of our model is its ability to handle multiple parallel disks and contention for disks among the concurrent applications. Analysis and simulation-based evaluation shows that our algorithms provide performance isolation, weighted allocation of resources, and are work conserving. The solutions are also applicable to other shared resource environments dealing with non-uniform heterogeneous servers.\"",
        "title: \"Optimal Read-Once Parallel Disk Scheduling\" with abstract: \"An optimal prefetching and I/O scheduling algorithm L-OPT, for parallel I/O systems, using a read-once model of block references is presented. The algorithm uses knowledge of the next $L$ references, $L$-block lookahead, to create a minimal-length I/O schedule. For a system with $D$ disks and a buffer of capacity $m$ blocks, we show that the competitive ratio of L-OPT is $\\Theta(\\sqrt{mD/L})$ when $L \\geq m$, which matches the lower bound of any prefetching algorithm with $L$-block lookahead. Tight bounds for the remaining ranges of lookahead are also presented. In addition we show that L-OPT is the optimal offline algorithm: when the lookahead consists of the entire reference string, it performs the absolute minimum possible number of I/Os. Finally, we show that L-OPT is comparable with the best online algorithm with the same amount of lookahead; the ratio of the length of its schedule to the length of the optimal schedule is always within a constant factor.\"",
        "title: \"Brief Announcement: Hardware Transactional Storage Class Memory.\" with abstract: \"Emerging persistent memory technologies (generically referred to as Storage Class Memory or SCM) hold tremendous promise for accelerating popular data-management applications like in-memory databases. However, programmers now need to deal with ensuring the atomicity of transactions on SCM-resident data and maintaining consistency between the persistent and in-memory execution orders of concurrent transactions. The problem is specially challenging when high-performance isolation mechanisms like Hardware Transaction Memory (HTM) are used for concurrency control. In this work we show how SCM-based HTM transactions can be ordered correctly using existing CPU instructions, without requiring any changes to existing processor cache hardware or HTM protocols. We describe a method that employs HTM for concurrency control and enforces atomic persistence and consistency with a novel software protocol and back-end external memory controller. In contrast, previous approaches require significant hardware changes to existing processor microarchitectures.\"",
        "1 is \"Scalable Distributed Communication Architectures to Support Advanced Metering Infrastructure in Smart Grid\", 2 is \"Optimal parallel merging and sorting without memory conflicts\".",
        "\nGiven above information, for an author who has written the paper with the title \"Efficient and adaptive proportional share I/O scheduling\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0116": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Emergent Semantics and Cooperation in Multi-knowledge Communities: the ESTEEM Approach':",
        "title: \"An algorithm for implementing BFT registers in distributed systems with bounded churn\" with abstract: \"Distributed storage service is one of the main abstractions provided to the developers of distributed applications due to its capability to hide the complexity generated by the messages exchanged between processes. Many protocols have been proposed to build byzantine-faulttolerant storage services on top of a message-passing system, but they do not consider the possibility to have servers joining and leaving the computation (churn phenomenon). This phenomenon, if not properly mastered, can either block protocols or violate the safety of the storage. In this paper, we address the problem of building of a safe register storage resilient to byzantine failures in a distributed system affected from churn. A protocol implementing a safe register in an eventually synchronous system is proposed and some feasibility constraints on the arrival and departure of the processes are given. The protocol is proved to be correct under the assumption that the constraint on the churn is satisfied.\"",
        "title: \"An Index-Based Checkpointing Algorithm for Autonomous Distributed Systems\" with abstract: \"This paper presents an index-based checkpointing algorithm for distributed systems with the aim of reducing the total number of checkpoints while ensuring that each checkpoint belongs to at least one consistent global checkpoint (or recovery line). The algorithm is based on an equivalence relation defined between pairs of successive checkpoints of a process which allows us, in some cases, to advance the recovery line of the computation without forcing checkpoints in other processes. The algorithm is well-suited for autonomous and heterogeneous environments, where each process does not know any private information about other processes and private information of the same type of distinct processes is not related (e.g., clock granularity, local checkpointing strategy, etc.). We also present a simulation study which compares the checkpointing-recovery overhead of this algorithm to the ones of previous solutions.\"",
        "title: \"Direct Dependency-Based Determination Of Consistent Global Checkpoints\" with abstract: \"Building consistent global checkpoints that contain a given set of local checkpoints has been usually handled by using transitive dependency tracking. This imply the usage of a vector of integers piggybacked on each message of the computation (the vector size being given by the number of processes). In this paper we address the problem to get consistent global checkpoints including a given subset of local checkpoints tracking just direct dependencies. In that case application messages are required to piggyback one integer. An algorithm is proposed that takes a set of local checkpoints as an input and returns the minimum consistent global checkpoint, if any, including that set. Otherwise it returns the first consistent global checkpoint that follows this subset. Among the applications of the algorithm there are rollback-recovery and global predicate detection.\"",
        "title: \"From Crash Fault-Tolerance to Arbitrary-Fault Tolerance: Towards a Modular Approach\" with abstract: \"The design of protocols able to cope with processes exhibiting an arbitrary faulty behavior is a real practical challenge due to malicious attacks or unexpected software errors. Nowadays, there are many protocols able to cope with process crashes, but, unfortunately, a process crash represents only a particular faulty behavior. Then, a good engineering argument would be to take a protocol resilient to process crashes and to transform it into one resilient to arbitrary failures. This paper presents a generic methodology to perform the previous transformation in the case where processes run the same text and regularly exchange messages (i.e., the case of round-based protocols). This modular approach encapsulates the detection of arbitrary failures in specific modules. Such a methodology can be the starting point for designing tools that allow automatic transformation. We show an application of this methodology to the case of consensus.\"",
        "title: \"On the Deterministic Tracking of Moving Objects with a Binary Sensor Network\" with abstract: \"This paper studies the problem of associating deterministically a track revealed by a binary sensor network with the trajectory of a unique moving anonymous object, namely the Multiple Object Tracking and Identification(MOTI) problem. In our model, the network is represented by a sparse connected graph where each vertex represents a binary sensor and there is an edge between two sensors if an object can pass from a sensed region to another without activating any other remaining sensor. The difficulty of MOTI lies in the fact that trajectories of two or more objects can be so close (track merging) that the corresponding tracks on the sensor network can no longer be distinguished, thus confusing the deterministic association between an object trajectory and a track.The paper presents several results. We first show that MOTI cannot be solved on a general graph of ideal binary sensors even by an omniscient external observer if all the objects can freely move on the graph. Then, we describe some restrictions that can be imposed a priorieither on the graph, on the object movements or both, to make MOTI problem always solvable. We also discuss the consequences of our results and present some related open problems.\"",
        "1 is \"Sample Evaluation of Ontology-Matching Systems\", 2 is \"Efficient Byzantine-resilient reliable multicast on a hybrid failure model\".",
        "\nGiven above information, for an author who has written the paper with the title \"Emergent Semantics and Cooperation in Multi-knowledge Communities: the ESTEEM Approach\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0117": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Multimedia indexing and retrieval: ever great challenges':",
        "title: \"Evolutionary artificial neural networks by multi-dimensional particle swarm optimization.\" with abstract: \"In this paper, we propose a novel technique for the automatic design of Artificial Neural Networks (ANNs) by evolving to the optimal network configuration(s) within an architecture space. It is entirely based on a multi-dimensional Particle Swarm Optimization (MD PSO) technique, which re-forms the native structure of swarm particles in such a way that they can make inter-dimensional passes with a dedicated dimensional PSO process. Therefore, in a multidimensional search space where the optimum dimension is unknown, swarm particles can seek both positional and dimensional optima. This eventually removes the necessity of setting a fixed dimension a priori, which is a common drawback for the family of swarm optimizers. With the proper encoding of the network configurations and parameters into particles, MD PSO can then seek the positional optimum in the error space and the dimensional optimum in the architecture space. The optimum dimension converged at the end of a MD PSO process corresponds to a unique ANN configuration where the network parameters (connections, weights and biases) can then be resolved from the positional optimum reached on that dimension. In addition to this, the proposed technique generates a ranked list of network configurations, from the best to the worst. This is indeed a crucial piece of information, indicating what potential configurations can be alternatives to the best one, and which configurations should not be used at all for a particular problem. In this study, the architecture space is defined over feed-forward, fully-connected ANNs so as to use the conventional techniques such as back-propagation and some other evolutionary methods in this field. The proposed technique is applied over the most challenging synthetic problems to test its optimality on evolving networks and over the benchmark problems to test its generalization capability as well as to make comparative evaluations with the several competing techniques. The experimental results show that the MD PSO evolves to optimum or near-optimum networks in general and has a superior generalization capability. Furthermore, the MD PSO naturally favors a low-dimension solution when it exhibits a competitive performance with a high dimension counterpart and such a native tendency eventually yields the evolution process to the compact network configurations in the architecture space rather than the complex ones, as long as the optimality prevails.\"",
        "title: \"Personalized long-term ECG classification: A systematic approach\" with abstract: \"This paper presents a personalized long-term electrocardiogram (ECG) classification framework, which addresses the problem within a long-term ECG signal, known as Holter register, recorded from an individual patient. Due to the massive amount of ECG beats in a Holter register, visual inspection is quite difficult and cumbersome, if not impossible. Therefore, the proposed system helps professionals to quickly and accurately diagnose any latent heart disease by examining only the representative beats (the so-called master key-beats) each of which is automatically extracted from a time frame of homogeneous (similar) beats. We tested the system on a benchmark database where beats of each Holter register have been manually labeled by cardiologists. The selection of the right master key-beats is the key factor for achieving a highly accurate classification and thus we used exhaustiveK-means clustering in order to find out (near-) optimal number of key-beats as well as the master key-beats. The classification process produced results that were consistent with the manual labels with over 99% average accuracy, which basically shows the efficiency and the robustness of the proposed system over massive data (feature) collections in high dimensions.\"",
        "title: \"A Dynamic Content-Based Indexing Method For Multimedia Databases: Hierarchical Cellular Tree\" with abstract: \"This paper presents a novel indexing technique, Hierarchical Cellular Tree, which is designed to bring an effective solution especially for indexing on large-scale multimedia databases. A pre-emptive cell search mechanism is introduced in order to prevent the corruption of large multimedia item collections due to the limited discrimination obtained from the visual and aural descriptors. In addition to this, the similar items are focused within appropriate cellular structures, which will be the subject to mitosis operations when the dissimilarity emerges as a result of irrelevant item insertions. Mitosis operations ensure to keep the cells in a focused and compact form and yet the cells can grow into any dimension as long as the compactness prevails. The proposed indexing scheme is then optimized for a novel query method, the Progressive Query, in order to maximize the retrieval efficiency for the user point of view. Experimental results show that the speed of the retrievals is significantly improved.\"",
        "title: \"The visual goodness evaluation of color-based retrieval processes\" with abstract: \"A content-based image retrieval system helps users to find suitable images or videos from large databases for their purposes. In this paper we present the results of analyses of visual outputs from our image retrieval system, based on color matching. These analyses are based on human evaluation of color content similarity. Computer-based simulation results have been analyzed visually with the opinion scores method, i.e. how good the results are according to the human visual system. Tests have shown that there is no superior retrieval method that is best in every case. The goodness results of each retrieval method will be presented in this paper. The usability and usefulness of each system will be presented as well as a possible utilization domain.\"",
        "title: \"Semi-Supervised Learning for Ill-Posed Polarimetric SAR Classification\" with abstract: \"In recent years, the interest in semi-supervised learning has increased, combining supervised and unsupervised learning approaches. This is especially valid for classification applications in remote sensing, while the data acquisition rate in current systems has become fairly large considering high-and very-high resolution data; yet on the other hand, the process of obtaining the ground truth data may be cumbersome for such large repositories. In this paper, we investigate the application of semi-supervised learning approaches and particularly focus on the small sample size problem. To that extend, we consider two basic unsupervised approaches by enlarging the initial labeled training set as well as an ensemble-based self-training method. We propose different strategies within self-training on how to select more reliable candidates from the pool of unlabeled samples to speed-up the learning process and to improve the classification performance of the underlying classifier ensemble. We evaluate the effectiveness of the proposed semi-supervised learning approach over polarimetric SAR data. Results show that the proposed self-training approach using an ensemble-based classifier that is initially trained over a small training set can achieve a similar performance level of a fully supervised learning approach where the training is performed over significantly larger labeled data. Considering the difficulties of the manual data labeling in such massive volumes of SAR repositories, this is indeed a promising accomplishment for semi-supervised SAR classification.\"",
        "1 is \"Lossless plenoptic image compression using adaptive block differential prediction\", 2 is \"MSLD: A robust descriptor for line matching\".",
        "\nGiven above information, for an author who has written the paper with the title \"Multimedia indexing and retrieval: ever great challenges\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0118": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Novel Inter-file Coded Placement and D2D Delivery for a Cache-aided Fog-RAN Architecture.':",
        "title: \"A Survey of Physical Layer Security Techniques for 5G Wireless Networks and Challenges Ahead.\" with abstract: \"Physical layer security which safeguards data confidentiality based on the information-theoretic approaches has received significant research interest recently. The key idea behind physical layer security is to utilize the intrinsic randomness of the transmission channel to guarantee the security in physical layer. The evolution toward 5G wireless communications poses new challenges for physical l...\"",
        "title: \"Order-Optimal Rate of Caching and Coded Multicasting with Random Demands.\" with abstract: \"We consider the canonical shared link caching network formed by a source node, hosting a library of $m$ information messages (files), connected via a noiseless multicast link to $n$ user nodes, each equipped with a cache of size $M$ files. Users request files independently at random according to an a-priori known demand distribution q. A coding scheme for this network consists of two phases: cache placement and delivery. The cache placement is a mapping of the library files onto the user caches that can be optimized as a function of the demand statistics, but is agnostic of the actual demand realization. After the user demands are revealed, during the delivery phase the source sends a codeword (function of the library files, cache placement, and demands) to the users, such that each user retrieves its requested file with arbitrarily high probability. The goal is to minimize the average transmission length of the delivery phase, referred to as rate (expressed in channel symbols per file). In the case of deterministic demands, the optimal min-max rate has been characterized within a constant multiplicative factor, independent of the network parameters. The case of random demands was previously addressed by applying the order-optimal min-max scheme separately within groups of files requested with similar probability. However, no complete characterization of order-optimality was previously provided for random demands under the average rate performance criterion. In this paper, we consider the random demand setting and, for the special yet relevant case of a Zipf demand distribution, we provide a comprehensive characterization of the order-optimal rate for all regimes of the system parameters, as well as an explicit placement and delivery scheme achieving order-optimal rates. We present also numerical results that confirm the superiority of our scheme with respect to previously proposed schemes for the same setting.\"",
        "title: \"Allocations for heterogenous distributed storage\" with abstract: \"We study the problem of storing a data object in a set of data nodes that fail independently with given probabilities. Our problem is a natural generalization of a homogenous storage allocation problem where all the nodes had the same reliability and is naturally motivated for peer-to-peer and cloud storage systems with different types of nodes. Assuming optimal erasure coding (MDS), the goal is to find a storage allocation (i.e, how much to store in each node) to maximize the probability of successful recovery. This problem turns out to be a challenging combinatorial optimization problem. In this work we introduce an approximation framework based on large deviation inequalities and convex optimization. We propose two approximation algorithms and study the asymptotic performance of the resulting allocations.\"",
        "title: \"Interference alignment, carrier pairing, and lattice decoding\" with abstract: \"We discuss how to take advantage of the discreteness of signal constellations at the detector side, in various interference alignment schemes for which linear zero-forcing of the interference has been traditionally considered. We show that lattice decoding, with possible \u201cgeneralized decision feedback equalization\u201d preprocessing can achieve significant gains in symbol error rate at the detector output, at finite SNR. Furthermore, we discuss the effect of channel coefficient dynamics on the Cadambe and Jafar interference alignment precoding scheme, and point out the need of careful carrier pairing in order to limit these dynamics.\"",
        "title: \"Optimal tradeoff of secure PUF-based authentication\" with abstract: \"We consider a problem of secure PUF-based authentication under a privacy constraint on the PUF responses. Given a randomly chosen challenge, an adversary who has access to the (publicly) stored helper data and side information correlated with the challenge -response pairs may try to mimic the corresponding PUF response in order to get authenticated. We characterize the optimal tradeoff between rate of the compressed helper data, information leakage rate of the PUF response at the adversary, and the adversary's maximum false acceptance probability exponent.. The result captures a tradeoff between authentication performance and security aspects of PUF-based authentication such as authentication security and unpredictability of PUF when used in authentication applications.\"",
        "1 is \"Gaussian multiaccess channels with ISI: capacity region and multiuser water-filling\", 2 is \"Coverage and Rate Analysis for Millimeter-Wave Cellular Networks\".",
        "\nGiven above information, for an author who has written the paper with the title \"Novel Inter-file Coded Placement and D2D Delivery for a Cache-aided Fog-RAN Architecture.\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0119": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Multi-cell MIMO cooperative networks: a new look at interference':",
        "title: \"Memory-Based Opportunistic Multi-User Beamforming\" with abstract: \"A scheme exploiting memory in opportunistic multiuser beamforming is proposed. The scheme builds on recent advances realized in [1] in the area of multi-user downlink precoding and scheduling based on partial transmitter channel state information (CSIT). Although the precoding and scheduling done in [1] is optimal within the set of unitary precoders, it is only so asymptotically for large number of users. Secondly, this scheme is unable to exploit potential time correlation of the channel. In this paper, we show 1- that exploiting memory in the transmitter allows to fill the gap to optimality for fixed (even low) number of users for time correlated channels, 2- how such schemes ([1] and ours) can be extended to take fairness into account in the proportional fair sense.\"",
        "title: \"A Semi-Blind Approach To Structured Channel Equalization\" with abstract: \"This paper describes a direct equalization approach for channels with some underlying structure. A semi-blind approach is taken here where a snail amount of training symbols is available. A family of MMSE equalizers is obtained that includes some prior information about the channel structure. The channel structure assumed in this paper is that the channel vector lies approximately in the subspace of a matric associated with the samples of the transmit pulse shape. Blind identifiability issues of the structured equalizer are also addressed. Numerical results using experimental indoor channel data indicate that these structured equalizers can achieve bit error rates that are significantly lower than traditional non-blind MMSE equalizers.\"",
        "title: \"Interference alignment in the partially connected K-user MIMO interference channel\" with abstract: \"We consider interference alignment in the partially connected K-user MIMO interference channel (IC). Conversely to the fully-connected case, we show that interference alignment can be achievable for an arbitrary number of users K in the network, while the per-user signaling dimension remains fixed, provided that the number of interference links per user is bounded. For this class of channels, which we denote by L-interfering K-user MIMO IC, we provide a criterion applicable to symmetric systems for the system of IA equations to be proper, according to the framework introduced earlier by Yetis et al. Properness is a necessary condition for IA to be feasible. Interestingly, this criterion is independent from the number of users K. Furthermore, we propose an iterative algorithm to solve the alignment problem for this class of channels.\"",
        "title: \"Some Systems Aspects Regarding Compressive Relaying with Wireless Infrastructure Links\" with abstract: \"In this paper, we consider single-cell cellular networks assisted with fixed relay station (RS), used by mobile stations (MS) to access the base station (BTS) via a relaying strategy. The RSs are positioned around the BTS, in such a way that wireless channels on the relay link (from RSs to the BTS) are line-of-sight, we analyze the achievable sum-of-rates for up-link communications. We compare two relaying strategies at the RSs, namely amplify-and-forward (AF) and compress-and-forward (CF). It is assumed that mobile signals and relay signals are emitted on orthogonal bands (FDD), with the possibility of having a larger bandwidth (BW) on the relay-to-base links. We predict the system gains bought by relays, in comparison with two other reference systems. One reference is an ideal relay-based system where the relays enjoy noiseless communications to the BTSs, i.e. a so-called distributed antenna system (DAS). The second reference is offered by a conventional cellular systems without relays, but same number of overall infrastructure antennas. In this paper, it is demonstrated the surprising result that with a relay bandwidth just twice that of the mobile's bandwidth, the system capacity approaches that of an ideal distributed antenna system, (while probably being much superior in practice in terms of ease of deployment and cost). The capacity gains of the relay-assisted network over a conventional network are also analyzed.\"",
        "title: \"Rate loss analysis of transmitter cooperation with distributed CSIT\" with abstract: \"We consider in this work the problem of determining the number of feedback bits which should be used to quantize the channel state information (CSI) in a broadcast channel (BC) with K transmit antennas (or equivalently K single-antenna transmitters (TXs)) and K single-antenna receivers (RXs). We focus on an extension of the conventional centralized CSI at the TX (CSIT) model, where instead of having a single channel estimate, or quantized version, perfectly shared by all the TX antennas, each TX receives its own estimate of the global multiuser channel. This CSIT configuration, denoted as distributed CSIT, is particularly suited to model the joint transmission from TXs which are not colocated. With centralized CSIT, a very important design guideline for the feedback link was provided by Jindal [Trans. Inf. Theory 2006] by providing a sufficient feedback rate to ensure that the rate loss stays below a maximum value. In the distributed CSIT setting, additional errors occur and the design guidelines for the centralized case are no longer valid. Consequently, we obtain a new relation between the rate loss and the number of feedback bits. Interestingly, the feedback rate derived in the distributed CSIT setting is roughly K log2(K) bits larger than its counterpart in the centralized case. This highlights the critical impact of the CSIT distributedness over the performance.\"",
        "1 is \"Scaling Laws and Techniques in Decentralized Processing of Interfered Gaussian Channels\", 2 is \"Transmit power adaptation for multiuser OFDM systems\".",
        "\nGiven above information, for an author who has written the paper with the title \"Multi-cell MIMO cooperative networks: a new look at interference\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0120": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Deciding stability and mortality of piecewise affine dynamical systems':",
        "title: \"Safe Recursion Over an Arbitrary Structure: PAR, PH and DPH\" with abstract: \"Considering the Blum, Shub, and Smale computational model for real numbers, extended by Poizat to general structures, classical complexity can be considered as the restriction to finite structures of a more general notion of computability and complexity working over arbitrary structures.\"",
        "title: \"Reachability Problems For One-Dimensional Piecewise Affine Maps\" with abstract: \"Piecewise affine maps (PAMs) are frequently used as a reference model to discuss the frontier between known and open questions about the decidability for reachability questions. In particular, the reachability problem for one-dimensional PAM is still an open problem, even if restricted to only two intervals. As the main contribution of this paper we introduce new techniques for solving reachability problems based on p-adic norms and weights as well as showing decidability for two classes of maps. Then we show the connections between topological properties for PAM's orbits, reachability problems and representation of numbers in a rational base system. Finally we construct an example where the distribution properties of well studied sequences can be significantly disrupted by taking fractional parts after regular shifts. The study of such sequences could help with understanding similar sequences generated in PAMs or in well known Mahler's 3/2 problem.\"",
        "title: \"Computability over an arbitrary structure: sequential and parallel polynomial time\" with abstract: \"We provide several machine-independent characterizations of deterministic complexity classes in the model of computation proposed by L. Blum, M. Shub and S. Smale. We provide a characterization of partial recursive functions over any arbitrary structure. We show that polynomial time computable functions over any arbitrary structure can be characterized in term of safe recursive functions. We show that polynomial parallel time decision problems over any arbitrary structure can be characterized in terms of safe recursive functions with substitutions.\"",
        "title: \"Real recursive functions and real extensions of recursive functions\" with abstract: \"Recently, functions over the reals that extend elementarily computable functions over the integers have been proved to correspond to the smallest class of real functions containing some basic functions and closed by composition and linear integration. We extend this result to all computable functions: functions over the reals that extend total recursive functions over the integers are proved to correspond to the smallest class of real functions containing some basic functions and closed by composition, linear integration and a very natural unique minimization schema.\"",
        "title: \"A Framework for Algebraic Characterizations in Recursive Analysis.\" with abstract: \"Algebraic characterizations of the computational aspects of functions defined over the real numbers provide very effective tool to understand what computability and complexity over the reals, and generally over continuous spaces, mean. This is relevant for both communities of computer scientists and mathematical analysts, particularly the latter who do not understand (and/or like) the language of machines and string encodings. Recursive analysis can be considered the most standard framework of computation over continuous spaces, it is however defined in a very machine specific way which does not leave much to intuitiveness. Recently several characterizations, in the form of function algebras, of recursively computable functions and some sub-recursive classes were introduced. These characterizations shed light on the hidden behavior of recursive analysis as they convert complex computational operations on sequences of real objects to simple intuitive mathematical operations such as integration or taking limits. The authors previously presented a framework for obtaining algebraic characterizations at the complexity level over compact domains. The current paper presents a comprehensive extension to that framework. Though we focus our attention in this paper on functions defined over the whole real line, the framework, and accordingly the obtained results, can be easily extended to functions defined over arbitrary domains.\"",
        "1 is \"Rapid Object Indexing and Recognition Using Enhanced Geometric Hashing\", 2 is \"Factoring multivariate polynomials via partial differential equations\".",
        "\nGiven above information, for an author who has written the paper with the title \"Deciding stability and mortality of piecewise affine dynamical systems\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0121": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Tree exploration with little memory':",
        "title: \"Epidemic Algorithms and Processes: From Theory to Applications (Dagstuhl Seminar 13042).\" with abstract: \"This report documents the program and the outcomes of Dagstuhl Seminar 13042 \"Epidemic Algorithms and Processes: From Theory to Applications\", which took place from January 20 to 25, 2013 at Schloss Dagstuhl - Leibniz Center for \r\nInformatics. Several research topics were covered by the seminar participants, including scientists working in Theoretical Computer Science, as well as researchers from the more practical area of Computer Systems. Most of the participants presented their recent results on the topic of the seminar, as well as some challenging new directions and open problems. The presentations contained a description of the main research area for a wide audience. During the seminar, ample time was reserved for informal discussions between participants working on different topics. In our executive summary, we describe the main field of the seminar, as well as our goals in general. Then, we present the abstracts of the presentations given during the seminar.\"",
        "title: \"Redundancy in Distributed Proofs.\" with abstract: \"Distributed proofs are mechanisms enabling the nodes of a network to collectivity and efficiently check the correctness of Boolean predicates on the structure of the network, or on data-structures distributed over the nodes (e.g., spanning trees or routing tables). We consider mechanisms consisting of two components: a emph{prover} assigning a emph{certificate} to each node, and a distributed algorithm called emph{verifier} that is in charge of verifying the distributed proof formed by the collection of all certificates.  this paper, we show that many network predicates have distributed proofs offering a high level of redundancy, explicitly or implicitly. We use this remarkable property of distributed proofs for establishing perfect tradeoffs between the emph{size of the certificate} stored at every node, and the emph{number of rounds} of the verification protocol. If we allow every node to communicate to distance at most $t$, one might expect that the certificate sizes can be reduced by a multiplicative factor of at least~$t$. In trees, cycles and grids, we show that such tradeoffs can be established for emph{all} network predicates, i.e., it is always possible to linearly decrease the certificate size. In arbitrary graphs, we show that any part of the certificates common to all nodes can be evenly redistributed among these nodes, achieving even a better tradeoff: this common part of the certificate can be reduced by the size of a smallest ball of radius $t$ in the network.  addition to these general results, we establish several upper and lower bounds on the certificate sizes used for distributed proofs for spanning trees, minimum-weight spanning trees, diameter, additive and multiplicative spanners, and more, improving and generalizing previous results from the literature.\"",
        "title: \"On the searchability of small-world networks with arbitrary underlying structure\" with abstract: \"Revisiting the \"small-world\" experiments of the '60s, Kleinberg observed that individuals are very effective at constructing short chains of acquaintances between any two people, and he proposed a mathematical model of this phenomenon. In this model, individuals are the nodes of a base graph, the square grid, capturing the underlying structure of the social network; and this base graph is augmented with additional edges from each node to a few long-range contacts of this node, chosen according to some natural distance-based distribution. In this augmented graph, a greedy search algorithm takes only a polylogarithmic number of steps in the graph size. Following this work, several papers investigated the correlations between underlying structure and long-range connections that yield efficient decentralized search, generalizing Kleinberg's results to broad classes of underlying structures, such as metrics of bounded doubling dimension, and minor-excluding graphs. We focus on the case of arbitrary base graphs. We show that for a simple long-range contact distribution consistent with empirical observations on social networks, a slight variation of greedy search, where the next hop is to a distant node only if it yields sufficient progress towards the target, requires no(1) steps, where $n$ is the number of nodes. Precisely, the expected number of steps for any source-target pair is at most 2(log n)1/2+o(1). This bound almost matches the best known lower bound of \u03a9(2\u221alog n) steps, which applies to a general class of search algorithms. In the context of social networks, our result could be interpreted as: individuals may well be able to construct short chains between people regardless of the underlying structure of the social network.\"",
        "title: \"Eclecticism shrinks even small worlds\" with abstract: \"We consider small world graphs as defined by Kleinberg (2000), i.e., graphs obtained from a d-dimensional mesh by adding links chosen at random according to the d-harmonic distribution. This model aims at giving formal support to the \"six degrees of separation\" between individuals experienced by Milgram (1967),and verified recently by Dodds, Muhamad, and Watts (2003). In particular, Kleinberg shows that greedy routing performs in O(log2n) expected number of steps in d-dimensional augmented meshes, with O(log2n) bits of topological awareness per node, for any constant d \u2265 1. We show that giving O(log2n) bits of topological awareness per node decreases the expected number of steps of greedy routing to O(log1+1/dn) in d-dimensional augmented meshes. We also show that, independently of the amount of topological awareness given to the nodes, greedy routing performs in \u03a9(log1+1/dn) expected number of steps. In particular, augmenting the topological awareness above this optimum of O(log2n) bits would drastically decrease the performances of greedy routing. Moreover, our model demonstrates that the efficiency of greedy routing is sensible to the \"world's dimension\", in the sense that high dimensional worlds enjoy faster greedy routing than low dimensional ones. This could not be observed in Kleinberg's model. In addition to bringing new light to Milgram's experiment, our protocol presents several desirable properties. In particular, it is totally oblivious i.e., there is no header modification along the path from the source to the target, and the routing decision depends only on the target, and on information stored locally at each node. Finally, our protocol can obviously be used for the design of DHTs, in the same spirit as Symphony (2003).\"",
        "title: \"Survey of Distributed Decision.\" with abstract: \"We survey the recent distributed computing literature on checking whether a given distributed system configuration satisfies a given boolean predicate, i.e., whether the configuration is legal or illegal w.r.t. that predicate. We consider classical distributed computing environments, including mostly synchronous fault-free network computing (LOCAL and CONGEST models), but also asynchronous crash-prone shared-memory computing (WAIT-FREE model), and mobile computing (FSYNC model).\"",
        "1 is \"Models and algorithms for coscheduling compute-intensive taks on a network of workstations\", 2 is \"Trading Space for Time in Undirected $s-t$ Connectivity\".",
        "\nGiven above information, for an author who has written the paper with the title \"Tree exploration with little memory\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0122": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Leakage Current: Moore's Law Meets Static Power':",
        "title: \"High Performance and Energy Efficient Serial Prefetch Architecture\" with abstract: \"Energy efficient architecture research has flourished recently, in an attempt to address packaging and cooling concerns of current microprocessor designs, as well as battery life for mobile computers. Moreover, architects have become increasingly concerned with the complexity of their designs in the face of scalability, verification, and manufacturing concerns.In this paper, we propose and evaluate a high performance, energy and complexity efficient front-end prefetch architecture. This design, called Serial Prefetching, combines a high fetch bandwidth branch prediction and efficient instruction prefetching architecture with a low-energy instruction cache. Serial Prefetching explores the benefit of decoupling the tag component of the cache from the data component. Cache blocks are first verified by the tag component of the cache, and then the accesses are put into a queue to be consumed by the data component of the instruction cache. Energy is saved by only accessing the correct way of the data component specified by the tag lookup in a previous cycle. The tag component does not stall on a I-cache miss, only the data component. The accesses that miss in the tag component are speculatively brought in from lower levels of the memory hierarchy. This in effect performs a prefetch, while the access migrates through the queue to be consumed by the data component.\"",
        "title: \"What input-language is the best choice for high level synthesis (HLS)?\" with abstract: \"As of 2010, over 30 of the world's top semiconductor / systems companies have adopted HLS. In 2009, SOCs tape-outs containing IPs developed using HLS exceeded 50 for the first time. Now that the practicality and value of HLS is established, engineers are turning to the question of \"what input-language works best?\" The answer is critical because it drives key decisions regarding the tool/methodology infrastructure companies will create around this new flow. ANSI-C/C++ advocates cite ease-of-learning, simulation speed. SystemC advocates make similar claims, and point to SystemC's hardware-oriented features. Proponents of BSV (Bluespec SystemVerilog) claim that language enhances architectural transparency and control. To maximize the benefits of HLS, companies must consider many factors and tradeoffs.\"",
        "title: \"Architecting a reliable CMP switch architecture\" with abstract: \"As silicon technologies move into the nanometer regime, transistor reliability is expected to wane as devices become subject to extreme process variation, particle-induced transient errors, and transistor wear-out. Unless these challenges are addressed, computer vendors can expect low yields and short mean-times-to-failure. In this article, we examine the challenges of designing complex computing systems in the presence of transient and permanent faults. We select one small aspect of a typical chip multiprocessor (CMP) system to study in detail, a single CMP router switch. Our goal is to design a BulletProof CMP switch architecture capable of tolerating significant levels of various types of defects. We first assess the vulnerability of the CMP switch to transient faults. To better understand the impact of these faults, we evaluate our CMP switch designs using circuit-level timing on detailed physical layouts. Our infrastructure represents a new level of fidelity in architectural-level fault analysis, as we can accurately track faults as they occur, noting whether they manifest or not, because of masking in the circuits, logic, or architecture. Our experimental results are quite illuminating. We find that transient faults, because of their fleeting nature, are of little concern for our CMP switch, even within large switch fabrics with fast clocks. Next, we develop a unified model of permanent faults, based on the time-tested bathtub curve. Using this convenient abstraction, we analyze the reliability versus area tradeoff across a wide spectrum of CMP switch designs, ranging from unprotected designs to fully protected designs with on-line repair and recovery capabilities. Protection is considered at multiple levels from the entire system down through arbitrary partitions of the design. We find that designs are attainable that can tolerate a larger number of defects with less overhead than na\u00efve triple-modular redundancy, using domain-specific techniques, such as end-to-end error detection, resource sparing, automatic circuit decomposition, and iterative diagnosis and reconfiguration.\"",
        "title: \"MEVBench: A mobile computer vision benchmarking suite\" with abstract: \"The growth in mobile vision applications, coupled with the performance limitations of mobile platforms, has led to a growing need to understand computer vision applications. Computationally intensive mobile vision applications, such as augmented reality or object recognition, place significant performance and power demands on existing embedded platforms, often leading to degraded application quality. With a better understanding of this growing application space, it will be possible to more effectively optimize future embedded platforms. In this work, we introduce and evaluate a custom benchmark suite for mobile embedded vision applications named MEVBench. MEVBench provides a wide range of mobile vision applications such as face detection, feature classification, object tracking and feature extraction. To better understand mobile vision processing characteristics at the architectural level, we analyze single and multithread implementations of many algorithms to evaluate performance, scalability, and memory characteristics. We provide insights into the major areas where architecture can improve the performance of these applications in embedded systems.\"",
        "title: \"Scalable hybrid verification of complex microprocessors\" with abstract: \"We introduce a new verification methodology for modern micro-processors that uses a simple checker processor to validate the exe-cution of a companion high-performance processor. The checker can be viewed as an at-speed emulator that is formally verified to be compliant to an ISA specification. This verification approach en-ables the practical deployment of formal methods without impact-ing overall performance.\"",
        "1 is \"Using data compression in an MPSoC architecture for improving performance\", 2 is \"Trace scheduling: a technique for global microcode compaction\".",
        "\nGiven above information, for an author who has written the paper with the title \"Leakage Current: Moore's Law Meets Static Power\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0123": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'From Fingerprint to Footprint: Revealing Physical World Privacy Leakage by Cyberspace Cookie Logs.':",
        "title: \"Fundamental trade-offs in aggregate packet scheduling\" with abstract: \"We investigate the fundamental trade-offs in aggregate packet scheduling for the support of guaranteed delay service. Besides the simple FIFO packet scheduling algorithm, we consider two new classes of aggregate packet scheduling algorithms: the static earliest time first (SETF) and dynamic earliest time first (DETF). Through these two classes of aggregate packet scheduling, we show that, with additional time stamp information encoded in the packet header for scheduling purpose, we can significantly increase the maximum allowable network utilization level, while at the same time reducing the worst-case edge-to-edge delay bound. Furthermore, we demonstrate how the number of the bits used to encode the time stamp information affects the trade-off between the maximum allowable network utilization level and the worst-case edge-to-edge delay bound. In addition, the more complex DETF algorithms have far better performance than the simpler SETF algorithms. These results illustrate the fundamental trade-offs in aggregate packet scheduling algorithms and shed light on their provisioning power in support of guaranteed delay service.\"",
        "title: \"Coverage-Aware Proxy Placement for Dynamic Content Management over the Internet\" with abstract: \"In an effort to differentiate service quality, service providers have resorted to employing Content Distribution Networks (CDNs) over the Internet CDNs deploy geographically distributed proxy servers which manage content on behalf of the service provider's servers for better performance and enhanced availability. In this paper we explore the proxy placement problem for content distribution over the Internet. Its goal is to strategically place a number of proxies in the network to optimize certain criteria which improve performance of proxies. We motivate the various necessary factors and constraints that need to be taken into account for a good placement of proxies over the Internet which reflect real world scenario more accurately and which we claim hitherto has not been completely addressed. We introduce a novel concept of host coverage characterizing every Autonomous Systems (AS) and use this stable, coarse grained measure as a long-term estimate of the load being serviced by the proxy system. We then pose an optimal formulation of the proxy placement problem taking into consideration all the relevant factors. We propose a couple of proxy placement algorithms that solve the above problem and analyze their behavior. Finally we present the performance of those algorithms against the optimal solution and other schemes proposed in literature. We also study the stability of the proposed algorithms through a variety of experiments.\"",
        "title: \"MTBF: an efficient multicast group aggregation scheme for the global area multicast\" with abstract: \"IP Multicast is an important enabling service for the current and future Internet. With the explosive, growth of the Internet, a challenging issue facing IP multicast is scalability, in particular, the problem of multicast forwarding state and control explosion. In this paper, we propose a new methodology to address the multicast scalability problem for backbone domains Multicast Tunneling with Branch Filtering (MTBF). This multicast group aggregation scheme is designed on top of the inter-domain protocol architecture such as MASC/BGMP, and is independent of any underlying intra-domain multicast protocols. It aggregates multicast groups by constructing Border Router (BR)-based multicast routing trees and forwards data by using an encapsulation technique called Multicast Tunneling (MT). To minimize excess traffic due to aggregate multicast address based data forwarding, an efficient Dynamic Filtering Point Selection (DFPS) algorithm is used. The feasibility and performance of our scheme is demonstrated through analysis and simulations\"",
        "title: \"Protocol independent multicast group aggregation scheme for the global area multicast\" with abstract: \"IP multicast is an important enabling service for the current and future Internet. With the explosive growth of the Internet, a challenging issue facing IP multicast is scalability, in particular, the problem of multicast forwarding state and control explosion. In this paper, we propose a new methodology to address the multicast scalability problem for backbone domains-multicast tunneling with branch filtering (MTBF). This multicast group aggregation scheme is designed on top of the inter-domain protocol architecture such as MASC/BGMF, and is independent of any underlying intra-domain multicast protocols. It aggregates multicast groups by constructing bolder router (BR)-based multicast routing trees and forwards data by using an encapsulation technique called multicast tunneling (MT). The feasibility and performance of our scheme is demonstrated through analysis and simulations\"",
        "title: \"Profiling users in a 3g network using hourglass co-clustering\" with abstract: \"With widespread popularity of smart phones, more and more users are accessing the Internet on the go. Understanding mobile user browsing behavior is of great significance for several reasons. For example, it can help cellular (data) service providers (CSPs) to improve service performance, thus increasing user satisfaction. It can also provide valuable insights about how to enhance mobile user experience by providing dynamic content personalization and recommendation, or location-aware services. In this paper, we try to understand mobile user browsing behavior by investigating whether there exists distinct \"behavior patterns\" among mobile users. Our study is based on real mobile network data collected from a large 3G CSP in North America. We formulate this user behavior profiling problem as a \"co-clustering\" problem, i.e., we group both users (who share similar browsing behavior), and browsing profiles (of like-minded users) simultaneously. We propose and develop a scalable co-clustering methodology, Phantom, using a novel hourglass model. The proposed hourglass model first reduces the dimensions of the input data and performs divisive hierarchical co-clustering on the lower dimensional data; it then carries out an expansion step that restores the original dimensions. Applying Phantom to the mobile network data, we find that there exists a number of prevalent and distinct behavior patterns that persist over time, suggesting that user browsing behavior in 3G cellular networks can be captured using a small number of co-clusters. For instance, behavior of most users can be classified as either homogeneous (users with very limited set of browsing interests) or heterogeneous (users with very diverse browsing interests), and such behavior profiles do not change significantly at either short (30-min) or long (6 hour) time scales.\"",
        "1 is \"Conflict classification and analysis of distributed firewall policies\", 2 is \"A policy-aware switching layer for data centers\".",
        "\nGiven above information, for an author who has written the paper with the title \"From Fingerprint to Footprint: Revealing Physical World Privacy Leakage by Cyberspace Cookie Logs.\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0124": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Characteristics of YouTube network traffic at a campus network - Measurements, models, and implications':",
        "title: \"A visual progranmming approach to manufacturing modeling\" with abstract: \"The RESearch Queueing Package Modeling Environment (RESQME) is an interactive, graphics-oriented workstation environment for iteratively constructing, running and analyzing models of resource contention systems. It is built on top of the RESearch Queueing Package (RESQ) which provides the functionality to evaluate extended queueing networks. In this paper we describe how to create a \u201cpull\u201d system, a manufacturing line with finite buffers, using the graphical interface and hierarchical modeling capability. This serves as an example of the kind of manufacturing submodels that can be created in this system and then used for higher-level modeling.\"",
        "title: \"On managing quality of experience of multiple video streams in wireless networks.\" with abstract: \"Managing the Quality-of-Experience (QoE) of video streaming for wireless clients is becoming increasingly important due to the rapid growth of video traffic on wireless networks. The inherent variability of the wireless channel as well as the Variable Bit Rate (VBR) of the compressed video streams make QoE management a challenging problem. Prior work has studied this problem in the context of transmitting a single video stream. In this paper, we investigate multiplexing schemes to transmit multiple video streams from a base station to mobile clients that use number of playout stalls as a performance metric.In this context, we present an epoch-by-epoch framework to fairly allocate wireless transmission slots to streaming videos. In each epoch our scheme essentially reduces the vulnerability to stalling by allocating slots to videos in a way that maximizes the minimum 'playout lead' across all videos. Next, we show that the problem of allocating slots fairly is NP-complete even for a constant number of videos. We then present a fast lead-aware greedy algorithm for the problem. Our choice of greedy algorithm is motivated by the fact that this algorithm is optimal when the channel quality of a user remains unchanged within an epoch (but different users may experience different channel quality). Moreover, our experimental results based on public MPEG-4 video traces and wireless channel traces that we collected from a WiMAX test-bed show that the lead-aware greedy approach performs a fair distribution of stalls across the clients when compared to other algorithms, while still maintaining similar or lower average number of stalls per client.\"",
        "title: \"A study of proactive hybrid FEC/ARQ and scalable feedback techniques for reliable, real-time multicast\" with abstract: \"In this paper, we examine the performance benefits of using parity encoding for reliable delivery of data to multiple receivers, where the receivers have heterogeneous loss and delay characteristics. First, we examine the effect of sending parity loss repairs proactively to receivers before it is known whether or not the repairs are needed to repair losses. We show that in many cases, proactive repair can reduce feedback implosion and the expected delay of reliable delivery without increasing bandwidth usage. Next, we examine how proactive repairs can be used to meet end-to-end deadlines to within a tunable probability. Last, we examine several receiver feedback mechanisms for parity repairs in a heterogeneous networking environment, and find that the various mechanisms offer different levels of robustness. Our examinations take into account temporally correlated (bursty) loss, as well as loss of feedback messages from receivers.\"",
        "title: \"The effectiveness of affinity-based scheduling in multiprocessor network protocol processing (extended version)\" with abstract: \"Techtdques for avoiding the high memory overheads found on many modern shared-memory mtdtiprocessors are of increasing importance in the development of high-performance mtdtiprocessor protocol implementations. One such technique is processor-cache aflirdty schedtding, which can significantly lower packet latency and substantially increase protocol processing throughput (30). In tlds paper, we evaluate severtd aspects of the effectiveness of affinity-based scheduling in mtdtiproeessor network protocol processing, under packet-level and connection- level parallelization approaches Speeifieally, we evaluate the performance of the scheduling technique 1) when a large number of streams are concurrently supported, 2) when processing in- cludes copying of uncached packet data, 3) as applied to send-side protocol processing, and 4) in the presence of stream burstiness and source locality, two well-known properties of network t-c. We find that ~ty-based schedtding performs well under these conditions, emphasizing its rubustneatsand general effectiveness in multiprocessor network processing. In addition, we explore a technique which improves the cachhsg behavior and available packet-level concurrency under connection-level parallelism, and find performance improves dramatically.\"",
        "title: \"Online smoothing of variable-bit-rate streaming video\" with abstract: \"Bandwidth smoothing techniques for stored video perform end to end workahead transmission of frames into the client playback buffer, in advance of their display times. Such techniques are very effective in reducing the burstiness of the bandwidth requirements for transmitting compressed, stored video. This paper addresses online bandwidth smoothing for a growing number of streaming video applications such as newscasts, sportscasts, and distance learning, where many clients may be willing to tolerate a playback delay of a few seconds in exchange for a smaller bandwidth requirement. The smoothing can be performed at either the source of the videocast or at special smoothing server(s) (e.g., proxies or gateways) within the network. In contrast to previous work on stored video, the online smoothing server has limited knowledge of frame sizes and access to only a segment of the video at a time. This is either because the feed is live or because it is streaming past the server. We formulate an online smoothing model which incorporates playback delay, client and server buffer sizes, server processing capacity, and frame size prediction techniques. Our model can accommodate an arbitrary arrival process. Using techniques for smoothing stored video at the source as a starting point, we develop an online, window-based smoothing algorithm for delay tolerant applications. Extensive experiments with MPEG-1 and M-JPEG video traces demonstrate that online smoothing significantly reduces the peak rate, coefficient of variation, and effective bandwidth of variable-bit-rate video streams. These reductions can be achieved with modest playback delays of a few seconds to a few tens of seconds and moderate client buffer sizes, and closely approximate the performance of optimal offline smoothing of stored video. In addition, we show that frame size prediction can offer further reduction in resource requirements, though prediction becomes relatively less important for longer playback delays. However, the ability to predict future frame sizes affects the appropriate division of buffer space between the server and client sites. Our experiments show that the optimal buffer allocation shifts to placing more memory at the server as the server has progressively less information about future frame sizes\"",
        "1 is \"An end-to-end adaptation protocol for layered video multicast using optimal rate allocation\", 2 is \"Random early detection gateways for congestion avoidance\".",
        "\nGiven above information, for an author who has written the paper with the title \"Characteristics of YouTube network traffic at a campus network - Measurements, models, and implications\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0125": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Improving Random Walk Estimation Accuracy with Uniform Restarts':",
        "title: \"Analysis and design of controllers for AQM routers supporting TCP flows\" with abstract: \"In active queue management (AQM), core routers signal transmission control protocol (TCP) sources with the ob- jective of managing queue utilization and delay. It is essentially a feedback control problem. Based on a recently developed dynamic model of TCPs congestion-avoidance mode, this paper does three things. First, it relates key network parameters such as the number of TCP sessions, link capacity and round-trip time to the under- lying feedback control problem. Second, it analyzes the present de facto AQM standard: random early detection (RED) and de- termines that REDs queue-averaging is not beneficial. Finally, it recommends alternative AQM schemes which amount to classical proportional and proportional-integral control. We illustrate our results using ns simulations and demonstrate the practical impact of proportional-integral control on managing queue utilization and delay.\"",
        "title: \"Optimal scheduling policies for a class of queues with customer deadlines to the beginning of service\" with abstract: \"Many problems can be modeled as single-server queues with impatient customers. An example is that of the transmission of voice packets over a packet-switched network. If the voice packets do not reach their destination within a certain time interval of their transmission, they are useless to the receiver and considered lost. It is therefore desirable to schedule the customers such that the fraction of customers served within their respective deadlines is maximized. For this measure of performance, it is shown that the shortest time to extinction (STE) policy is optimal for a class of continuous and discrete time nonpreemptive M/G/1 queues that do not allow unforced idle times. When unforced idle times are allowed, the best policies belong to the class of shortest time to extinction with inserted idle time (STEI) policies. An STEI policy requires that the customer closest to his or her deadline be scheduled whenever it schedules a customer. It also has the choice of inserting idle times while the queue is nonempty. It is also shown that the STE policy is optimal for the discrete time G/D/1 queue where all customers receive one unit of service. The paper concludes with a comparison of the expected customer loss using an STE policy with that of the first-come, first-served (FCFS) scheduling policy for one specific queue.\"",
        "title: \"Joint Data Compression and Caching: Approaching Optimality with Guarantees.\" with abstract: \"We consider the problem of optimally compressing and caching data across a communication network. Given the data generated at edge nodes and a routing path, our goal is to determine the optimal data compression ratios and caching decisions across the network in order to minimize average latency, which can be shown to be equivalent to maximizing the compression and caching gain  under an energy consumption constraint. We show that this problem is NP-hard in general and the hardness is caused by the caching decision subproblem, while the compression sub-problem is polynomial-time solvable. We then propose an approximation algorithm that achieves a $(1-1/e)$-approximation solution to the optimum in strongly polynomial time. We show that our proposed algorithm achieve the near-optimal performance in synthetic-based evaluations. In this paper, we consider a tree-structured network as an illustrative example, but our results easily extend to general network topology at the expense of more complicated notations.\n\n\"",
        "title: \"Minfer: A Method Of Inferring Motif Statistics From Sampled Edges\" with abstract: \"Characterizing motif (i.e., locally connected subgraph patterns) statistics is important for understanding complex networks such as online social networks and communication networks. Previous work made the strong assumption that the graph topology of interest is known in advance. In practice, sometimes researchers have to deal with the situation where the graph topology is unknown because it is expensive to collect and store all topological and meta information. Hence, typically what is available to researchers is only a snapshot of the graph, i.e., a subgraph of the graph. Crawling methods such as breadth first sampling can be used to generate the snapshot. However, these methods fail to sample a streaming graph represented as a high speed stream of edges. Therefore, graph mining applications such as network traffic monitoring use random edge sampling (i.e., sample each edge with a fixed probability) to collect edges and generate a sampled graph, which we called a \" RE Sampled graph \". Clearly, a RESampled graph's motif statistics may be quite different from those of the underlying original graph. To resolve this, we propose a framework and implement a system called Minfer, which takes the given RESampled graph and accurately infers the underlying graph's motif statistics. We also apply Fisher information to bound the errors of our estimates. Experiments using large scale datasets show the accuracy and efficiency of our method.\"",
        "title: \"Covert communications on Poisson packet channels.\" with abstract: \"Consider a channel where authorized transmitter Jack sends packets to authorized receiver Steve according to a Poisson process with rate lambda packets per second for a time period T. Suppose that covert transmitter Alice wishes to communicate information to covert receiver Bob on the same channel without being detected by a watchful adversary Willie. We consider two scenarios. In the first scenario, we assume that warden Willie cannot look at packet contents but rather can only observe packet timings, and Alice must send information by inserting her own packets into the channel. We show that the number of packets that Alice can covertly transmit to Bob is on the order of the square root of the number of packets that Jack transmits to Steve; conversely, if Alice transmits more than that, she will be detected by Willie with high probability. In the second scenario, we assume that Willie can look at packet contents but that Alice can communicate across an M / M / 1 queue to Bob by altering the timings of the packets going from Jack to Steve. First, Alice builds a codebook, with each codeword consisting of a sequence of packet timings to be employed for conveying the information associated with that codeword. However, to successfully employ this codebook, Alice must always have a packet to send at the appropriate time. Hence, leveraging our result from the first scenario, we propose a construction where Alice covertly slows down the packet stream so as to buffer packets to use during a succeeding codeword transmission phase. Using this approach, Alice can covertly and reliably transmit O (lambda T) covert bits to Bob in time period T over an M / M / 1 queue with service rate mu > e.lambda.\"",
        "1 is \"Service differentiation and guarantees for TCP-based elastic traffic\", 2 is \"Trade-offs in optimizing the cache deployments of CDNs\".",
        "\nGiven above information, for an author who has written the paper with the title \"Improving Random Walk Estimation Accuracy with Uniform Restarts\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0126": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'An Economically-Principled Generative Model of AS Graph Connectivity':",
        "title: \"Self-correcting sampling-based dynamic multi-unit auctions\" with abstract: \"We exploit methods of sample-based stochastic optimization for the purpose of strategyproof dynamic, multi-unit auctions. There are no analytic characterizations of optimal policies for this domain and thus a heuristic approach, such as that proposed here, seems necessary in practice. Following the suggestion of Parkes and Duong [17], we perform sensitivity analysis on the allocation decisions of an online algorithm for stochastic optimization, and correct the decisions to enable a strategyproof auction. In applying this approach to the allocation of non-expiring goods, the technical problem that we must address is related to achieving strategyproofness for reports of departure. This cannot be achieved through self-correction without canceling many allocation decisions, and must instead be achieved by first modifying the underlying algorithm. We introduce the NowWait method for this purpose, prove its successful interfacing with sensitivity analysis and demonstrate good empirical performance. Our method is quite general, requiring a technical property of uncertainty independence, and that values are not too positively correlated with agent patience. We also show how to incorporate \"virtual valuations\" in order to increase the seller's revenue.\"",
        "title: \"Expressive banner ad auctions and model-based online optimization for clearing\" with abstract: \"We present the design of a banner advertising auction which is considerably more expressive than current designs. We describe a general model of expressive ad contract/bidding and an allocation model that can be executed in real time through the assignment of fractions of relevant ad channels to specific advertiser contracts. The uncertainty in channel supply and demand is addresscd by the formulation of a stochastic combinatorial optimization problem for channel allocation that is rerun periodically. We solve this in two different ways: fast deterministic optimization with respect to expectations; and a novel online sample-based stochastic optimization method-- that can be applied to continuous decision spaces--which exploits the deterministic optimization as a black box. Experiments demonstrate the importance of expressive bidding and the value of stochastic optimization.\"",
        "title: \"Hybrid transitive trust mechanisms\" with abstract: \"Establishing trust amongst agents is of central importance to the development of well-functioning multi-agent systems. For example, the anonymity of transactions on the Internet can lead to inefficiencies; e.g., a seller on eBay failing to ship a good as promised, or a user free-riding on a file-sharing network. Trust (or reputation) mechanisms can help by aggregating and sharing trust information between agents. Unfortunately these mechanisms can often be manipulated by strategic agents. Existing mechanisms are either very robust to manipulation (i.e., manipulations are not beneficial for strategic agents), or they are very informative (i.e., good at aggregating trust data), but never both. This paper explores this trade-off between these competing desiderata. First, we introduce a metric to evaluate the informativeness of existing trust mechanisms. We then show analytically that trust mechanisms can be combined to generate new hybrid mechanisms with intermediate robustness properties. We establish through simulation that hybrid mechanisms can achieve higher overall efficiency in environments with risky transactions and mixtures of agent types (some cooperative, some malicious, and some strategic) than any previously known mechanism.\"",
        "title: \"Why markets could (but don't currently) solve resource allocation problems in systems\" with abstract: \"Using market mechanisms for resource allocation in distributed systems is not a new idea, nor is it one that has caught on in practice or with a large body of computer science research. Yet, projects that use markets for distributed resource allocation recur every few years [1, 2, 3], and a new generation of research is exploring market-based resource allocation mechanisms [4, 5, 6, 7, 8] for distributed environments such as Planetlab, Netbed, and computational grids. This paper has three goals. The first goal is to explore why markets can be appropriate to use for allocation, when simpler allocation mechanisms exist. The second goal is to demonstrate why a new look at markets for allocation could be timely, and not a re-hash of previous research. The third goal is to point out some of the thorny problems inherent in market deployment and to suggest action items both for market designers and for the greater research community. We are optimistic about the power of market design, but we also believe that key challenges exist for a markets/systems integration that must be overcome for market-based computer resource allocation systems to succeed.\"",
        "title: \"Online mechanism design for electric vehicle charging\" with abstract: \"Plug-in hybrid electric vehicles are expected to place a considerable strain on local electricity distribution networks, requiring charging to be coordinated in order to accommodate capacity constraints. We design a novel online auction protocol for this problem, wherein vehicle owners use agents to bid for power and also state time windows in which a vehicle is available for charging. This is a multi-dimensional mechanism design domain, with owners having non-increasing marginal valuations for each subsequent unit of electricity. In our design, we couple a greedy allocation algorithm with the occasional \"burning\" of allocated power, leaving it unallocated, in order to adjust an allocation and achieve monotonicity and thus truthfulness. We consider two variations: burning at each time step or on-departure. Both mechanisms are evaluated in depth, using data from a real-world trial of electric vehicles in the UK to simulate system dynamics and valuations. The mechanisms provide higher allocative efficiency than a fixed price system, are almost competitive with a standard scheduling heuristic which assumes non-strategic agents, and can sustain a substantially larger number of vehicles at the same per-owner fuel cost saving than a simple random scheme.\"",
        "1 is \"Solving transition independent decentralized Markov decision processes\", 2 is \"In search of database consistency\".",
        "\nGiven above information, for an author who has written the paper with the title \"An Economically-Principled Generative Model of AS Graph Connectivity\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0127": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Converging to approximated max-min flow fairness in logarithmic time':",
        "title: \"Multipath routing slice experiments in federated testbeds\" with abstract: \"The Internet today consist of many heterogeneous infrastructures, owned and maintained by separate and potentially competing administrative authorities. On top of this a wide variety of applications has different requirements with regard to quality, reliability and security from the underlying networks. The number of stakeholders who participate in provisioning of network and services is growing. More demanding applications (like eGovernment, eHealth, critical and emergency infrastructures) are on the rise. Therefore we assume that these two basic characteristics, a) multiple authorities and b) applications with very diverse demands, are likely to stay or even increase in the Internet of the future. In such an environment federation and virtualization of resources are key features that should be supported in a future Internet. The ability to form slices across domains that meet application specific requirements enables many of the desired features in future networks. In this paper, we present a Multipath Routing Slice experiment that we performed over multiple federated testbeds. We combined capabilities from different experimental facilities, since one single testbed did not offer all the required capabilities. This paper summarizes the conducted experiment, our experience with the usability of federated testbeds and our experience with the use of advanced measurement technologies within experimental facilities. We believe that this experiment provides a good example use case for the future Internet itself because we assume that the Internet will consist of multiple different infrastructures that have to be combined in application specific overlays or routing slices, very much like the experimental facilities we used in this experiment. We also assume that the growing demands will push towards a much better measurement instrumentation of the future Internet. The tools used in our experiment can provide a starting point for this.\"",
        "title: \"Topological trends of internet content providers\" with abstract: \"The Internet is constantly changing, and its hierarchy was recently shown to become flatter. Recent studies of inter-domain traffic showed that large content providers drive this change by bypassing tier-1 networks and reaching closer to their users, enabling them to save transit costs and reduce reliance of transit networks as new services are being deployed, and traffic shaping is becoming increasingly popular. In this paper we take a first look at the evolving connectivity of large content provider networks, from a topological point of view of the autonomous systems (AS) graph. We perform a 5-year longitudinal study of the topological trends of large content providers, by analyzing several large content providers and comparing these trends to those observed for large tier-1 networks. We study trends in the connectivity of the networks, neighbor diversity and geographical spread, their hierarchy, the adoption of IXPs as a convenient method for peering, and their centrality. Our observations indicate that content providers gradually increase and diversify their connectivity, enabling them to improve their centrality in the Internet, while tier-1 networks lose dominance over time.\"",
        "title: \"A practical revocation scheme for broadcast encryption using smartcards\" with abstract: \"We present an anti-pirate revocation scheme for broadcast encryption systems (e.g., pay TV), in which the data is encrypted to ensure payment by users. In the systems we consider, decryption of keys is done on smartcards and key management is done in-band. Our starting point is a scheme of Naor and Pinkas. Their basic scheme uses secret sharing to remove up to t parties, is information-theoretic secure against coalitions of size t, and is capable of creating a new group key. However, with current smartcard technology, this scheme is only feasible for small system parameters, allowing up to about 100 pirates to be revoked before all the smartcards need to be replaced. We first present a novel implementation method of their basic scheme that distributes the work among the smartcard, set-top terminal, and center. Based on this, we construct several improved schemes for many revocation rounds that scale to realistic system sizes. We allow up to about 10,000 pirates to be revoked using current smartcard technology before recarding is needed. The transmission lengths of our constructions are on par with those of the best tree-based schemes. However, our constructions have much lower smartcard CPU complexity: only O(1) smartcard operations per revocation round (a single 10-byte field multiplication and addition), as opposed to the complexity of the best tree-based schemes, which is polylogarithmic in the number of users. We evaluate the system behavior via an exhaustive simulation study coupled with a queueing theory analysis. Our simulations show that with mild assumptions on the piracy discovery rate, our constructions can perform effective pirate revocation for realistic broadcast encryption scenarios.\"",
        "title: \"Topology aggregation for directed graphs\" with abstract: \"This paper addresses the problem of aggregating the topology of a sub-network in a compact way with minimum distortion. The problem arises from networks that have a hierarchical structure, where each sub-network must advertise the cost of routing between each pair of its border nodes. The straight-forward solution of advertising the exact cost for each pair has a quadratic cost which is not practical. We look at the realistic scenario of networks where all links are bidirectional, but their cost (or distance) in the opposite directions might differ significantly. The paper presents a solution with distortion that is bounded by the logarithm of the number of border nodes and the square-root of the asymmetry in the cost of a link. This is the first time that a theoretical bound is given to an undirected graph. We show how to apply our solution to PNNI, and suggest some other heuristics that are tested to perform better than the provenly bounded solution\"",
        "title: \"Approximation and heuristic algorithms for minimum-delay application-layer multicast trees\" with abstract: \"In this paper we investigate the problem of finding minimum-delay application-layer multicast trees, such as the trees constructed in overlay networks. It is accepted that shortest path trees are not a good solution for the problem since such trees can have nodes with very large degree, termed high-load nodes. The load on these nodes makes them a bottleneck in the distribution tree, due to computation load and access link bandwidth constraints. Many previous solutions limited the maximum degree of the nodes by introducing arbitrary constraints. In this work, we show how to directly map the node load to the delay penalty at the application host, and create a new model that captures the trade offs between the desire to select shortest path trees and the need to constrain the load on the hosts. In this model the problem is shown to be NP-hard. We therefore present an approximation algorithm and an alternative heuristic algorithm. Our heuristic algorithm is shown by simulations to be scalable for large group sizes, and produces results that are very close to optimal.\"",
        "1 is \"A One-Pass Algorithm for Accurately Estimating Quantiles for Disk-Resident Data\", 2 is \"Greedy dynamic routing on arrays\".",
        "\nGiven above information, for an author who has written the paper with the title \"Converging to approximated max-min flow fairness in logarithmic time\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0128": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Computing the success factors in consistent acquisition and recognition of objects in color digital images by explicit preconditioning':",
        "title: \"Extraction of mid-level semantics from gesture videos using a Bayesian network\" with abstract: \"A method for extraction of mid-level semantics from sign language videos is proposed, by employing high level domain knowledge. The semantics concern labelling of the depicted head and hands as well as the occlusion events, which are essential for interpretation and for higher level semantic indexing. A Bayesian network is employed to bridge in a probabilistic fashion the gap between the high level knowledge about the valid spatiotemporal configurations of the human body and the extractor. The approach is applied here in sign language videos, but it can be generalised to any case, where semantically rich information can be derived from gesture.\"",
        "title: \"GOrevenge: A Novel Generic Reverse Engineering Method for the Identification of Critical Molecular Players, Through the Use of Ontologies\" with abstract: \"The ever-increasing use of ontologies in modern biological analysis and interpretation facilitates the understanding of the cellular procedures, their hierarchical organization, and their potential interactions at a system's level. Currently, the gene ontology serves as a paradigm, where through the annotation of whole genomes of certain organisms, genes subsets selected, either from high-throughput experiments or with an established pivotal role regarding the probed disease, can act as a starting point for the exploration of their underlying functional interconnections. This may also aid the elucidation of hidden regulatory mechanisms among genes. Reverse engineering the functional relevance of genes to specific cellular pathways and vice versa, through the exploitation of the inner structure of the ontological vocabularies, may help impart insight regarding the identification and prioritization of the critical role of specific genes. The proposed graph-theoretical method is showcased in a pancreatic cancer and a T-cell acute lymphoblastic leukemia gene set, incorporating edge and Resnik semantic similarity metrics, and systematically evaluated regarding its performance.\"",
        "title: \"Inference Of A Robust Diagnostic Signature In The Case Of Melanoma: Gene Selection By Information Gain And Gene Ontology Tree Exploration\" with abstract: \"Integrated datasets originating from multi-modal data can be used towards the identification of causal biological actions that through a systems level process trigger the development of a disease. We use, here, an integrated dataset related to cutaneous melanoma that comes from two separate sets (microarray and imaging) and the application of data imputation methods. Our goal is to select a subset of genes that comprise candidate biomarkers and compare these to imaging features, that characterize disease at a macroscopic level. Using information gain ratio measurements and exploration of Gene Ontology (GO) tree, we identified a set of 33 genes both highly correlated to the disease status and with a central role in regulatory mechanisms. Selected genes were used to train various classifiers that could generalize well when discriminating malignant from benign melanoma samples. Results showed that classifiers performed better when selected genes were used as input, rather than imaging features selected by information gain measurements. Thus, genes in the backstage of low-level biological processes showed to carry higher information content than the macroscopic imaging features.\"",
        "title: \"On-Line Fall Detection Via Mobile Accelerometer Data\" with abstract: \"Mobile devices have entered our daily life in several forms, such as tablets, smartphones, smartwatches and wearable devices, in general. The majority of those devices have built-in several motion sensors, such as accelerometers, gyroscopes, orientation and rotation sensors. The activity recognition or emergency event detection in cases of falls or abnormal activity conduce a challenging task, especially for elder people living independently in their homes. In this work, we present a methodology capable of performing real time fall detect, using data from a mobile accelerometer sensor. To this end, data taken from the 3-axis accelerometer is transformed using the Incremental Principal Components Analysis methodology. Next, we utilize the cumulative sum algorithm, which is capable of detecting changes using devices having limited CPU power and memory resources. Our experimental results are promising and indicate that using the proposed methodology, real time fall detection is feasible.\"",
        "title: \"Electronic Roads in Historical Documents: A Student Oriented Approach\" with abstract: \"The new educational approaches, as far as teaching of history in secondary education is concerned, are characterized by a shift away from sterile memorization and towards a critical approach of historical facts and phenomena; the aim is to contribute to both the development of students' historical concept and conscience and the promotion of critical thought. These teaching goals are pursued by promoting initiative of the students through self study assignments in the form of projects; students can be greatly supported in such tasks by computer-based applications, which can offer access to vast amounts of historical texts and data to be used next to the main scholar textbook and be analyzed by students. Still, existing applications seem to be quite inadequate for this purpose, as they require that the student be already informed on a matter, before the initiation of a quest for data. In this paper, we describe an intelligent information system that is designed to facilitate browsing of educational material and historical sources, thus allowing students to efficiently retrieve information on topics that are not yet known to them and expand in this way their historical knowledge. This can help in fulfilling the aforementioned teaching goals. Our system relies on the notion of the Electronic Road.\"",
        "1 is \"Automated semantic web service discovery with OWLS-MX\", 2 is \"Model-based articulated hand motion tracking for gesture recognition\".",
        "\nGiven above information, for an author who has written the paper with the title \"Computing the success factors in consistent acquisition and recognition of objects in color digital images by explicit preconditioning\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0129": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'A General Framework for Hardware Trojan Detection in Digital Circuits by Statistical Learning Algorithms.':",
        "title: \"ESE: Efficient Speech Recognition Engine with Sparse LSTM on FPGA.\" with abstract: \"Long Short-Term Memory (LSTM) is widely used in speech recognition. In order to achieve higher prediction accuracy, machine learning scientists have built increasingly larger models. Such large model is both computation intensive and memory intensive. Deploying such bulky model results in high power consumption and leads to a high total cost of ownership (TCO) of a data center. To speedup the prediction and make it energy efficient, we first propose a load-balance-aware pruning method that can compress the LSTM model size by 20x (10x from pruning and 2x from quantization) with negligible loss of the prediction accuracy. The pruned model is friendly for parallel processing. Next, we propose a scheduler that encodes and partitions the compressed model to multiple PEs for parallelism and schedule the complicated LSTM data flow. Finally, we design the hardware architecture, named Efficient Speech Recognition Engine (ESE) that works directly on the sparse LSTM model. Implemented on Xilinx KU060 FPGA running at 200MHz, ESE has a performance of 282 GOPS working directly on the sparse LSTM network, corresponding to 2.52 TOPS on the dense one, and processes a full LSTM for speech recognition with a power dissipation of 41 Watts. Evaluated on the LSTM for speech recognition benchmark, ESE is 43x and 3x faster than Core i7 5930k CPU and Pascal Titan X GPU implementations. It achieves 40x and 11.5x higher energy efficiency compared with the CPU and GPU respectively.\"",
        "title: \"Improving energy efficiency of write-asymmetric memories by log style write\" with abstract: \"The significant scaling challenges of conventional memories, i.e., SRAM and DRAM, motivated the research on emerging memory technologies. Many promising memory technology candidates, however, suffer from a common issue in their write operations: the switching processes at different write operations (i.e., 0 \u2192 1 and 1 \u2192 0) are asymmetric. Using a pessimistic design corner to cover the worst case of a write operation incurs large power and performance cost in the existing emerging memory technology designs. In this work, we propose a universal log style write methodology to mitigate this asymmetry issue by operating two switching processes in separate stages. The dedicated design optimizations are allowed on either switching process. The simulation results on the spin-transfer-torque random access memory based last-level cache show that our technique can improve the system performance by 4% while receiving 35% power reduction on average1.\"",
        "title: \"Training itself: Mixed-signal training acceleration for memristor-based neural network\" with abstract: \"The artificial neural network (ANN) is among the most widely used methods in data processing applications. The memristor-based neural network further demonstrates a power efficient hardware realization of ANN. Training phase is the critical operation of memristor-based neural network. However, the traditional training method for memristor-based neural network is time consuming and energy inefficient. Users have to first work out the parameters of memristors through digital computing systems and then tune the memristor to the corresponding state. In this work, we introduce a mixed-signal training acceleration framework, which realizes the self-training of memristor-based neural network. We first modify the original stochastic gradient descent algorithm by approximating calculations and designing an alternative computing method. We then propose a mixed-signal acceleration architecture for the modified training algorithm by equipping the original memristor-based neural network architecture with the copy crossbar technique, weight update units, sign calculation units and other assistant units. The experiment on the MNIST database demonstrates that the proposed mixed-signal acceleration is 3 orders of magnitude faster and 4 orders of magnitude more energy efficient than the CPU implementation counterpart at the cost of a slight decrease of the recognition accuracy (<; 5%).\"",
        "title: \"FPGA-based acceleration of neural network for ranking in web search engine with a streaming architecture\" with abstract: \"Web search engine companies are intensively running learning to rank algorithms to improve the search relevance. Neural network (NN)-based approaches, such as LambdaRank, can significantly increase the ranking quality. While, their training is very slow on a single computer and inherent coarse-grained parallelism could be hardly utilized by computer clusters. Thus an efficient implementation is necessary to timely generate acceptable NN models on frequently updated training datasets. This paper presents our work in accelerator. A SIMD streaming architecture is proposed to i) efficiently map the query-level NN computation and data structure to FPGA, ii) fully exploit the inherent fine-grained parallelism, and iii) provide scalability to large scale datasets. The accelerator shows up to 17.9X speedup over the software implementation on datasets from a commercial search engine.\"",
        "title: \"An evaluation method for video semantic models\" with abstract: \"The development of video technology and video-related applications demands strong support in semantic data models. To meet such a requirement, many video semantic data models have been proposed. The semantic model plays a key role in providing query capability and other features for a video database. However, to our knowledge, the criteria for a good semantic model remain open at present. As a result, people lack the rules for evaluating an existing model and the guidelines for the design of a new data model when necessary. To address this issue, this paper proposes twenty one properties as the criteria for video semantic models, and gives the evaluation result of eleven existing rich semantic models according to these properties. It shows that these models mostly concentrate on basic expressive power and query capability, and fulfill users' primary requirements. But in some advanced features such as expressive power, acquisition and analysis of semantic information, and query capability etc., there are rooms for further enhancement. The paper concludes by indicating some research directions for video semantic models.\"",
        "1 is \"A nondestructive self-reference scheme for Spin-Transfer Torque Random Access Memory (STT-RAM)\", 2 is \"VideoGraph: a graphical object-based model for representing and querying video data\".",
        "\nGiven above information, for an author who has written the paper with the title \"A General Framework for Hardware Trojan Detection in Digital Circuits by Statistical Learning Algorithms.\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0130": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Generalized network design polyhedra':",
        "title: \"The general routing polyhedron: A unifying framework\" with abstract: \"It is shown how to transform the General Routing Problem (GRP) into a variant of the Graphical Travelling Salesman Problem (GTSP). This transformation yields a projective characterisation of the GRP polyhedron. Using this characterisation, it is shown how to convert facets of the GTSP polyhedron into valid inequalities for the GRP polyhedron. The resulting classes of inequalities subsume several previously published classes.\"",
        "title: \"Small bipartite subgraph polytopes\" with abstract: \"We compute a complete linear description of the bipartite subgraph polytope, for up to seven nodes, and a conjectured complete description for eight nodes. We then show how these descriptions were used to compute the integrality ratio of various relaxations of the max-cut problem, again for up to eight nodes.\"",
        "title: \"New techniques for cost sharing in combinatorial optimization games\" with abstract: \"Combinatorial optimization games form an important subclass of cooperative games. In recent years, increased attention has been given to the issue of finding good cost shares for such games. In this paper, we define a very general class of games, called integer minimization games, which includes the combinatorial optimization games in the literature as special cases. We then present new techniques, based on row and column generation, for computing good cost shares for these games. To illustrate the power of these techniques, we apply them to traveling salesman and vehicle routing games. Our results generalize and unify several results in the literature. The main underlying idea is that suitable valid inequalities for the associated combinatorial optimization problems can be used to derive improved cost shares.\"",
        "title: \"Exploiting sparsity in pricing routines for the capacitated arc routing problem\" with abstract: \"The capacitated arc routing problem (CARP) is a well-known and fundamental vehicle routing problem. A promising exact solution approach to the CARP is to model it as a set covering problem and solve it via branch-cut-and-price. The bottleneck in this approach is the pricing (column generation) routine. In this paper, we note that most CARP instances arising in practical applications are defined on sparse graphs. We show how to exploit this sparsity to yield faster pricing routines. Extensive computational results are given.\"",
        "title: \"Gap inequalities for non-convex mixed-integer quadratic programs.\" with abstract: \"Laurent and Poljak introduced a very general class of valid linear inequalities, called gap inequalities, for the max-cut problem. We show that an analogous class of inequalities can be defined for general non-convex mixed-integer quadratic programs. These inequalities dominate some inequalities arising from a natural semidefinite relaxation.\"",
        "1 is \"On the boolean-quadric packinguncapacitated facility-location polytope\", 2 is \"{0, 1/2}-Chv\u00e1tal-Gomory cuts\".",
        "\nGiven above information, for an author who has written the paper with the title \"Generalized network design polyhedra\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0131": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Exponential Structures for Efficient Cache-Oblivious Algorithms':",
        "title: \"Priority Queues: Small, Monotone and Trans-dichotomous\" with abstract: \"We consider two data-structuring problems which involve performing priority queue (pq) operations on a set of integers in the range 0..2w\u22121 on a unit-cost RAM with word size \u03c9 bits.\"",
        "title: \"Fast Deterministic Selection on Mesh-Connected Processor Arrays\" with abstract: \"We present a deterministic algorithm for selecting the element of rank k among N=n2 elements, 1kN, on an n\u00d7n mesh-connected processor array in (1.44+ parallel computation steps, for any constant >0, using constant sized queues. This is a considerable improvement over the best previous deterministic algorithm, which was based upon sorting and required 3n steps. Our algorithm can be generalized to solve the problem of selection on higher dimensional meshes, achieving time bounds better than the known results in each case.\"",
        "title: \"Optimal trade-offs for succinct string indexes\" with abstract: \"Let s be a string whose symbols are solely available through access(i), a read-only operation that probes s and returns the symbol at position i in s. Many compressed data structures for strings, trees, and graphs, require two kinds of queries on s: select(c, j), returning the position in s containing the jth occurrence of c, and rank(c, p), counting how many occurrences of c are found in the first p positions of s. We give matching upper and lower bounds for this problem. The main contribution is to introduce a general technique for proving lower bounds on succinct data structures, that is based on the access patterns of the supported operations, abstracting from the particular operations at hand.\"",
        "title: \"Engineering succinct DOM\" with abstract: \"We describe the engineering of Succinct DOM (SDOM), a DOM implementation, written in C++, which is suitable for in-memory representation of large static XML documents. SDOM avoids the use of pointers, and is based upon succinct data structures, which use an information-theoretically minimum amount of space to represent an object. SDOM gives a space-efficient in-memory representation, with stable and predictable memory usage. The space used by SDOM is an order of magnitude less than that used by a standard C++ DOM representation such as Xerces, but SDOM is extremely fast: navigation is in some cases faster than for a pointer-based representation such as Xerces (even for moderate-sized documents which can comfortably be loaded into main memory by Xerces). A variant, SDOM-CT, applies bzip-based compression to textual and attribute data, and its space usage is comparable with \"queryable\" XML compressors. Some of these compressors support navigation and/or querying (e.g. subpath queries) of the compressed file. SDOM-CT does not support querying directly, but remains extremely fast: it is several orders of magnitude faster for navigation than queryable XML compressors that support navigation (and only a few times slower than say Xerces).\"",
        "title: \"Random access to grammar-compressed strings\" with abstract: \"Let S be a string of length N compressed into a context-free grammar S of size n. We present two representations of S achieving O(log N) random access time, and either O(n \u00b7 \u03b1k(n)) construction time and space on the pointer machine model, or O(n) construction time and space on the RAM. Here, \u03b1k(n) is the inverse of the kth row of Ackermann's function. Our representations also efficiently support decompression of any substring in S: we can decompress any substring of length m in the same complexity as a single random access query and additional O(m) time. Combining these results with fast algorithms for uncompressed approximate string matching leads to several efficient algorithms for approximate string matching on grammar-compressed strings without decompression. For instance, we can find all approximate occurrences of a pattern P with at most k errors in time O(n(min{|P|k, k4 +|P|} +log N) + occ), where occ is the number of occurrences of P in S. Finally, we are able to generalize our results to navigation and other operations on grammar-compressed trees. All of the above bounds significantly improve the currently best known results. To achieve these bounds, we introduce several new techniques and data structures of independent interest, including a predecessor data structure, two \"biased\" weighted ancestor data structures, and a compact representation of heavy-paths in grammars.\"",
        "1 is \"On RAM Priority Queues\", 2 is \"Triangulating a polygon in parallel\".",
        "\nGiven above information, for an author who has written the paper with the title \"Exponential Structures for Efficient Cache-Oblivious Algorithms\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0132": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Statistical Eigenmode Transmission Over Jointly Correlated MIMO Channels':",
        "title: \"Energy Efficient Link Adaptation for Downlink Transmission of LTE/LTE-A Systems\" with abstract: \"As a large percentage of the energy required by a cellular network is consumed at base station (BS) sites, how to reduce the energy consumption at BS sites has recently received much attention. In this paper, we propose an energy efficient link adaptation scheme to improve the BS's energy efficiency (EE) for long term evalution (LTE) systems. Based on the traditional spectral- efficiency centered link adaptation scheme, the proposed scheme treats transmit power as a new feedback parameter, aiming to maximize the BS's EE under a certain block error rate constraint. In addition, in order to reduce the transmit power adjustment frequency, a semi-static power control scheme is presented. Simulation results indicate that our proposed schemes can improve the BS's EE significantly.\"",
        "title: \"Limited feedback unitary precoding using improved Euclidean distance metrics for spatial multiplexing systems\" with abstract: \"This paper presents a new codeword selection criterion based on the Euclidean distance in the limited feedback unitary precoding systems. Because of the non-uniqueness of singular value decomposition (SVD), various precoding matrices may result in the same performance. We take the rotation and permutation invariance property into the definition of distortion function, then bring forward a new codeword selection criterion and distance metric derived from the improved Euclidean distance. A codebook design method based on the generalized Lloyd algorithm (GLA) is proposed thereafter. For the traditional subspace distance on the Grassmann manifold, the transmit data streams must be less than the transmit antennas, otherwise all the precoding matrices result in the same distortion results, however, our new distance metric can overcome this limitation. Simulation results show that our proposed distance metric can work properly with the unitary precoding codebook, apart from this, it can also acquire excellent performance under other spatial multiplexing environments.\"",
        "title: \"A Minimum Error Probability NOMA Design\" with abstract: \"Non-orthogonal multiple access (NOMA) enables massive connectivity and achieves high spectral efficiency. The vast majority of the NOMA literature has adopted the ideal information rate as performance metric assuming perfect successive interference cancellation (SIC) without any error propagation, which, however, may lead to NOMA designs adverse to SIC. In this paper, we take into account imperfec...\"",
        "title: \"Analysis and comparison of different feedback schemes for coordinated cellular networks\" with abstract: \"This paper compares several feedback schemes of a limited feedback system in a coordinated cellular networks. In a coordinated system one user can be served by different base stations (BSs) simultaneously. Conventional codebook is intended for single BS, its aim is to maximize the amplitude of the receive signal. When multiple BSs are equipped, phase discrimination among different BSs at the receive side may lead to performance degradation. Brute-force search through the entire BSs is a heavy burden especially when the BS number is large. In order to mitigate the negative effects, we present one solution dedicated to the coordinated system in this paper. When the BS number is small, we put forward a codebook design scheme based on the Lloyd codebook. When the BS number is large, we propose phase criterion instead of amplitude criterion in choosing the beamforming vector. Both schemes work well under their operating region. With our solution we can effectively reduce performance degradation in a coordinated cellular networks while still maintain a low complexity burden. Simulation results verify the effectiveness of our proposed schemes.\"",
        "title: \"Statistical 3-D Beamforming for Large-Scale MIMO Downlink Systems Over Rician Fading Channels.\" with abstract: \"In this paper, we investigate a downlink transmission algorithm for single-cell multiuser systems with two-dimensional (2-D) large-scale antenna array at the base station (BS) over Rician fading channels. We first derive some properties of the channel&#39;s line-of-sight (LOS) component in the large-scale antenna array scenario. Next, based on these properties and under the assumption of only statisti...\"",
        "1 is \"On scheduling and power allocation over multiuser MIMO-OFDMA: Fundamental design and performance evaluation in WiMAX systems\", 2 is \"Efficient resource allocation algorithm for cognitive OFDM systems\".",
        "\nGiven above information, for an author who has written the paper with the title \"Statistical Eigenmode Transmission Over Jointly Correlated MIMO Channels\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0133": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Nonadiabatic Ab initio surface-hopping dynamics calculation in a grid environment - first experiences':",
        "title: \"Native Double Precision LINPACK Implementation on a Hybrid Reconfigurable CPU\" with abstract: \"Applications requiring double precision (DP) arithmetic executed on embedded CPUs without native DP support suffer from prohibitively low performance and power efficiency. Hybrid reconfigurable CPUs, allowing for reconfiguration of the instruction set at runtime, appear as a viable computing platform for applications requiring instructions not supported by existing fixed architectures. Our experiments on a Stretch S6 as prototypical platform show that limited reconfigurable resources on such architectures are sufficient for providing native support of DP arithmetic. Our design using a DP fused multiply-accumulate (FMA) extension instruction achieves a peak performance of 200~MFlop/s and a sustained performance of 22.7~MFlop/s at a clock frequency of 100~MHz. It outperforms LINPACK using software-emulated DP floating-point arithmetic on the S6 by a factor of 5.7 while achieving slightly higher numerical accuracy. In single precision, multiple floating-point operators can be implemented in parallel on the S6.\"",
        "title: \"Effcient Solution of Evolution Models for Virus Populations\" with abstract: \"The computation of the quasispecies in Eigen's quasispecies model requires the solution of a very large scale eigenvalue problem. Since the problem dimension is of an exponential growing nature the well known methods for dealing with such a problem run out of resources already far away from practically relevant cases. We propose the use of an implicit matrix vector product using the special problem structure as building block for eigenvalue solvers to partially overcome the exponential growth, which let us reach unexpected large problem sizes. As we will show our implicit matrix vector product is a prime example for an algorithm perfectly matching the requirements of GPU computing since it has low space and high parallel computation requirements. Therefore we will also present an GPU implementation delivering a speedup factor of about 100 compared to a standard implementation.\"",
        "title: \"Energy consumption vs. latency in a new boundary identification method for WSNs with a mobile sink\" with abstract: \"In a recent paper we introduced the boundary identification scheme MoSBoD for wireless sensor networks. This method exploits a mobile sink equipped with a directional antenna to accomplish the desired task. In this paper we provide a careful analysis of the MoSBoD scheme in terms of energy efficiency and completion time (latency). Based on the results, we extend our previous work along two fronts. On the one hand, in order to improve the quality of the boundary identified using MoSBoD, we use a more restrictive definition of the boundary, which leads to reduced completion time for the boundary identification as well as to reduced energy consumption from the network. On the other hand, we propose a modification of the MoSBoD method which further reduces the completion time based on a selective increase of the duty cycles of sensor nodes. This strategy also increases the energy consumption. It is therefore required to carefully balance the expected reduction in the completion time with the increase in energy consumption. Thus, we also provide a deterministic model based analysis for energy consumption and completion time of the new boundary identification scheme, comparing it with the state-of-the-art.\"",
        "title: \"Mining agile DNS traffic using graph analysis for cybercrime detection.\" with abstract: \"We consider the analysis of network traffic data for identifying highly agile DNS patterns which are widely considered indicative for cybercrime. In contrast to related approaches, our methodology is capable of explicitly distinguishing between the individual, inherent agility of benign Internet services and criminal sites. Although some benign services use a large number of addresses, they are confined to a subset of IP addresses, due to operational requirements and contractual agreements with certain Content Distribution Networks. We discuss DNSMap, a system which analyzes observed DNS traffic, and continuously learns which FQDNs are hosted on which IP addresses. Any significant changes over time are mapped to bipartite graphs, which are then further pruned for cybercrime activity. Graph analysis enables the detection of transitive relations between FQDNs and IPs, and reveals clusters of malicious FQDNs and IP addresses hosting them. We developed a prototype system which is designed for realtime analysis, requires no costly classifier retraining, and no excessive whitelisting. We evaluate our system using large data sets from an ISP with several 100,000 customers, and demonstrate that even moderately agile criminal sites can be detected reliably and almost immediately.\"",
        "title: \"Efficient scheduling of sporadic tasks for real-time wireless sensor networks\" with abstract: \"Industrial automation requires hard real-time delivery of data that can be of periodic or sporadic in nature. It is challenging to ensure hard real-time delivery of periodic and sporadic data to a multi-hop away destination in a bandwidth constrained environment, such as wireless sensor networks. In this regard, research has been done for joint scheduling of periodic and sporadic data delivery using the IEEE 802.15.4 standard. However, delivery to a destination that is multiple hops away has not been handled. Moreover, the IEEE 802.15.4 standard compliance is not fully taken care of. This study proposes a novel communication protocol that is IEEE 802.15.4 standard compliant, for real-time delivery of sporadic and periodic data to a destination that is multiple hops away. Implicit decisions at various nodes result in the minimal increase in the control traffic and reduced energy dissipation. The proposed protocol is tested using the OPNET simulator and the correctness is proven through a multitude of simulation results.\"",
        "1 is \"Harmony: an execution model and runtime for heterogeneous many core systems\", 2 is \"Boundary estimation in sensor networks: theory and methods\".",
        "\nGiven above information, for an author who has written the paper with the title \"Nonadiabatic Ab initio surface-hopping dynamics calculation in a grid environment - first experiences\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0134": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Semi-supervised discriminative classification with application to tumorous tissues segmentation of MR brain images':",
        "title: \"Towards Model-Agnostic Post-Hoc Adjustment for Balancing Ranking Fairness and Algorithm Utility\" with abstract: \"ABSTRACTBipartite ranking, which aims to learn a scoring function that ranks positive individuals higher than negative ones from labeled data, is widely adopted in various applications where sample prioritization is needed. Recently, there have been rising concerns on whether the learned scoring function can cause systematic disparity across different protected groups defined by sensitive attributes. While there could be trade-off between fairness and performance, in this paper we propose a model agnostic post-processing framework for balancing them in the bipartite ranking scenario. Specifically, we maximize a weighted sum of the utility and fairness by directly adjusting the relative ordering of samples across groups. By formulating this problem as the identification of an optimal warping path across different protected groups, we propose a non-parametric method to search for such an optimal path through a dynamic programming process. Our method is compatible with various classification models and applicable to a variety of ranking fairness metrics. Comprehensive experiments on a suite of benchmark data sets and two real-world patient electronic health record repositories show that our method can achieve a great balance between the algorithm utility and ranking fairness. Furthermore, we experimentally verify the robustness of our method when faced with the fewer training samples and the difference between training and testing ranking score distributions.\"",
        "title: \"Believe It Today or Tomorrow? Detecting Untrustworthy Information from Dynamic Multi-Source Data.\" with abstract: \"A vast ocean of data is collected every day, and numerous applications call for the extraction of actionable insights from data. One important task is to detect untrustworthy information because such information usually indicates critical, unusual, or suspicious activities. In this paper, we study the important problem of detecting untrustworthy information from a novel perspective of correlating and comparing multiple sources that describe the same set of items. Different from existing work, we recognize the importance of time dimension in modeling the commonalities among multiple sources. We represent dynamic multi-source data as tensors and develop a joint non-negative tensor factorization approach to capture the common patterns across sources. We then conduct a comparison between source input and common patterns to identify inconsistencies as an indicator of untrustworthiness. An incremental factorization approach is developed to improve the computational efficiency on dynamically arriving data. We also propose a method to handle data sparseness. Experiments are conducted on hotel rating, network traffic flow, and weather forecast data that are collected from multiple sources. Results demonstrate the advantages of the proposed approach in detecting inconsistent and untrustworthy information.\"",
        "title: \"Predicting Seizures from Electroencephalography Recordings: A Knowledge Transfer Strategy\" with abstract: \"Epilepsy, a brain disorder afflicts nearly 1% of the world's population, is characterized by the occurrence of spontaneous seizures. For most epilepsy patients, the drugs are either not effective or produce severe side-effects. Seizure forecasting systems have the potential to help patients with epilepsy lead more normal lives. Recently multi-center clinical studies showed evidence of premonitory symptoms in 6.2% of 500 patients with epilepsy, and some interviews of epilepsy patients also found that a certain amount of patients felt \"auras\". All these are promising signs suggesting that seizure might be predictable. In this paper, we will study the application of deep learning techniques for seizure prediction with EEG signals. Deep learning methods have been shown to be very effective on exploring the latent structures from continuous signals and they have achieved state-of-the-art performance on speech analysis. One potential requirement for deep learning algorithms to work is a huge training set, which could be difficult for a specific medical problem. Therefore we specifically investigated a transfer learning strategy: we performed the major seizure prediction task on the data from American Epilepsy Society Seizure Prediction Challenge1, and we adopted another 6 publicly available EEG datasets2, which are not directly related to seizure prediction, as auxiliary information to pre-train the deep neural network for getting a good initial point. Our results show that with those auxiliary information, the prediction performance can be boosted. This observation is validated with different predictive models, which opens another gate for effective integration and utilization of medical data resources.\"",
        "title: \"Smoothness Maximization via Gradient Descents.\" with abstract: \"The recent years have witnessed a surge of interest in graph based semi-supervised learning. However, despite its extensive research, there has been little work on graph construction. In this study, employing the idea of gradient descent, we propose a novel method called Iterative Smoothness Maximization (ISM), to learn an optimal graph automatically for a semi-supervised learning task. The main procedure of ISM is to minimize the upper bound of semi-supervised classification error through an iterative gradient descent approach. We also prove the convergence of ISM theoretically, and finally experimental results on two real-world data sets are provided to demonstrate the effectiveness of ISM. \u00a9 2007 IEEE.\"",
        "title: \"Towards heterogeneous temporal clinical event pattern discovery: a convolutional approach\" with abstract: \"Large collections of electronic clinical records today provide us with a vast source of information on medical practice. However, the utilization of those data for exploratory analysis to support clinical decisions is still limited. Extracting useful patterns from such data is particularly challenging because it is longitudinal, sparse and heterogeneous. In this paper, we propose a Nonnegative Matrix Factorization (NMF) based framework using a convolutional approach for open-ended temporal pattern discovery over large collections of clinical records. We call the method One-Sided Convolutional NMF (OSC-NMF). Our framework can mine common as well as individual shift-invariant temporal patterns from heterogeneous events over different patient groups, and handle sparsity as well as scalability problems well. Furthermore, we use an event matrix based representation that can encode quantitatively all key temporal concepts including order, concurrency and synchronicity. We derive efficient multiplicative update rules for OSC-NMF, and also prove theoretically its convergence. Finally, the experimental results on both synthetic and real world electronic patient data are presented to demonstrate the effectiveness of the proposed method.\"",
        "1 is \"Vision and the Atmosphere\", 2 is \"Localized Supervised Metric Learning on Temporal Physiological Data\".",
        "\nGiven above information, for an author who has written the paper with the title \"Semi-supervised discriminative classification with application to tumorous tissues segmentation of MR brain images\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0135": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Analysis of classification margin for classification accuracy with applications':",
        "title: \"Pseudo relevance feedback based on iterative probabilistic one-class SVMs in web image retrieval\" with abstract: \"To improve the precision of top-ranked images returned by a web image search engine, we propose in this paper a novel pseudo relevance feedback method named iterative probabilistic one-class SVMs to re-rank the retrieved images. By assuming that most top-ranked images are relevant to the query, we iteratively train one-class SVMs, and convert the outputs to probabilities so as to combine the decision from different image representation. The effectiveness of our method is validated by systematic experiments even if the assumption is not well satisfied.\"",
        "title: \"Visual object recognition using probabilistic kernel subspace similarity\" with abstract: \"Probabilistic subspace similarity-based face matching is an efficient face recognition algorithm proposed by Moghaddam et al. It makes one basic assumption: the intra-class face image set spans a linear space. However, there are yet no rational geometric interpretations of the similarity under that assumption. This paper investigates two subjects. First, we present one interpretation of the intra-class linear subspace assumption from the perspective of manifold analysis, and thus discover the geometric nature of the similarity. Second, we also note that the linear subspace assumption does not hold in some cases, and generalize it to nonlinear cases by introducing kernel tricks. The proposed model is named probabilistic kernel subspace similarity (PKSS). Experiments on synthetic data and real visual object recognition tasks show that PKSS can achieve promising performance, and outperform many other current popular object recognition algorithms.\"",
        "title: \"Optimizing Top-k Multiclass SVM via Semismooth Newton Algorithm.\" with abstract: \"Top-k performance has recently received increasing attention in large data categories. Advances, like a top-k multiclass support vector machine (SVM), have consistently improved the top-k accuracy. However, the key ingredient in the state-of-the-art optimization scheme based upon stochastic dual coordinate ascent relies on the sorting method, which yields O(d log d) complexity. In this paper, we l...\"",
        "title: \"Regularized clustering for documents\" with abstract: \"In recent years, document clustering has been receiving more and more attentions as an important and fundamental technique for unsupervised document organization, automatictopic extraction, and fast information retrieval or filtering. In this paper, we propose a novel method for clustering documents using regularization. Unlike traditional globally regularized clustering methods, our method first construct a local regularized linear label predictor for each document vector, and then combine all those local regularizers with a global smoothness regularizer. So we call our algorithm Clustering with Local and Global Regularization (CLGR). We will show that the cluster memberships of the documents can be achieved by eigenvalue decomposition of a sparse symmetric matrix, which can be efficiently solved by iterative methods. Finally our experimental evaluations on several datasets are presented to show the superiorities of CLGR over traditional document clustering methods.\"",
        "title: \"Semi-Supervised Classification with Universum\" with abstract: \"The Universum data, deflned as a collection of \"non- examples\" that do not belong to any class of inter- est, have been shown to encode some prior knowledge by representing meaningful concepts in the same do- main as the problem at hand. In this paper, we ad- dress a novel semi-supervised classiflcation problem, called semi-supervised Universum, that can simultane- ously utilize the labeled data, unlabeled data and the Universum data to improve the classiflcation perfor- mance. We propose a graph based method to make use of the Universum data to help depict the prior infor- mation for possible classiflers. Like conventional graph based semi-supervised methods, the graph regulariza- tion is also utilized to favor the consistency between the labels. Furthermore, since the proposed method is a graph based one, it can be easily extended to the multi- class case. The empirical experiments on the USPS and MNIST datasets are presented to show that the pro- posed method can obtain superior performances over conventional supervised and semi-supervised methods.\"",
        "1 is \"High-performing feature selection for text classification\", 2 is \"YouSlow: a performance analysis tool for adaptive bitrate video streaming\".",
        "\nGiven above information, for an author who has written the paper with the title \"Analysis of classification margin for classification accuracy with applications\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0136": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Architecting high-performance energy-efficient soft error resilient cache under 3D integration technology':",
        "title: \"An Immune Multi-agent System for Network Intrusion Detection\" with abstract: \"Inspired by the immune theory and multi-agent systems, an immune multi-agent system for network intrusion detection is established. The concept of immune agent is introduced. And its logical structure and running mechanism are established. This model implements the multi-layer and distributed mechanism for network intrusion detection. The experimental results show that the new model not only reduces the False-Negative rate and False-Positive rate effectively but also has the feature to adapt to continuous changing network environments.\"",
        "title: \"Mining logs files for data-driven system management\" with abstract: \"With advancement in science and technology, computing systems are becoming increasingly more complex with an increasing variety of heterogeneous software and hardware components. They are thus becoming increasingly more difficult to monitor, manage and maintain. Traditional approaches to system management have been largely based on domain experts through a knowledge acquisition process that translates domain knowledge into operating rules and policies. This has been well known and experienced as a cumber-some, labor intensive, and error prone process. In addition, this process is difficult to keep up with the rapidly changing environments. There is thus a pressing need for automatic and efficient approaches to monitor and manage complex computing systems.A popular approach to system management is based on analyzing system log files. However, some new aspects of the log files have been less emphasized in existing methods from data mining and machine learning community. The various formats and relatively short text messages of log files, and temporal characteristics in data representation pose new challenges. In this paper, we will describe our research efforts on mining system log files for automatic management. In particular, we apply text mining techniques to categorize messages in log files into common situations, improve categorization accuracy by considering the temporal characteristics of log messages, and utilize visualization tools to evaluate and validate the interesting temporal patterns for system management.\"",
        "title: \"Personalized Recommendation via Parameter-Free Contextual Bandits\" with abstract: \"Personalized recommendation services have gained increasing popularity and attention in recent years as most useful information can be accessed online in real-time. Most online recommender systems try to address the information needs of users by virtue of both user and content information. Despite extensive recent advances, the problem of personalized recommendation remains challenging for at least two reasons. First, the user and item repositories undergo frequent changes, which makes traditional recommendation algorithms ineffective. Second, the so-called cold-start problem is difficult to address, as the information for learning a recommendation model is limited for new items or new users. Both challenges are formed by the dilemma of exploration and exploitation. In this paper, we formulate personalized recommendation as a contextual bandit problem to solve the exploration/exploitation dilemma. Specifically in our work, we propose a parameter-free bandit strategy, which employs a principled resampling approach called online bootstrap, to derive the distribution of estimated models in an online manner. Under the paradigm of probability matching, the proposed algorithm randomly samples a model from the derived distribution for every recommendation. Extensive empirical experiments on two real-world collections of web data (including online advertising and news recommendation) demonstrate the effectiveness of the proposed algorithm in terms of the click-through rate. The experimental results also show that this proposed algorithm is robust in the cold-start situation, in which there is no sufficient data or knowledge to tune the hyper-parameters.\"",
        "title: \"Workload Characterization of Bioinformatics Applications\" with abstract: \"The exponential growth in the amount of genomic information has spurred growing interest in large scale analysis of genetic data. Bioinformatics applications represent the increasingly important workloads. However, very few results on the behavior of these applications running on the state-of-the-art microprocessor and systems have been published. This paper proposes a suite of widely used bioinformatics applications and studies the execution characteristics of these benchmarks on a representative architecture - the Intel Pentium 4. To understand the impacts and implications of bioinformatics workloads on the microprocessor designs, we contrast the characteristics of bioinformatics workloads and the widely used SPEC 2000 integer benchmarks. The proposed bioinformatics benchmark suite as well as the input datasets can be downloaded from the following website: http://www.ideal.ece.ufl.edu/BioInfoMark.\"",
        "title: \"A comparative study of feature selection and multiclass classification methods for tissue classification based on gene expression.\" with abstract: \"This paper studies the problem of building multiclass classifiers for tissue classification based on gene expression. The recent development of microarray technologies has enabled biologists to quantify gene expression of tens of thousands of genes in a single experiment. Biologists have begun collecting gene expression for a large number of samples. One of the urgent issues in the use of microarray data is to develop methods for characterizing samples based on their gene expression. The most basic step in the research direction is binary sample classification, which has been studied extensively over the past few years. This paper investigates the next step-multiclass classification of samples based on gene expression. The characteristics of expression data (e.g. large number of genes with small sample size) makes the classification problem more challenging. The process of building multiclass classifiers is divided into two components: (i) selection of the features (i.e. genes) to be used for training and testing and (ii) selection of the classification method. This paper compares various feature selection methods as well as various state-of-the-art classification methods on various multiclass gene expression datasets. Our study indicates that multiclass classification problem is much more difficult than the binary one for the gene expression datasets. The difficulty lies in the fact that the data are of high dimensionality and that the sample size is small. The classification accuracy appears to degrade very rapidly as the number of classes increases. In particular, the accuracy was very low regardless of the choices of the methods for large-class datasets (e.g. NCI60 and GCM). While increasing the number of samples is a plausible solution to the problem of accuracy degradation, it is important to develop algorithms that are able to analyze effectively multiple-class expression data for these special datasets.\"",
        "1 is \"The Skyline operator\", 2 is \"Wave-pipelining: a tutorial and research survey\".",
        "\nGiven above information, for an author who has written the paper with the title \"Architecting high-performance energy-efficient soft error resilient cache under 3D integration technology\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0137": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Hybrid Filter Banks With Fractional Delays: Minimax Design and Application to Multichannel Sampling':",
        "title: \"Joint Optimization of Source Power Allocation and Cooperative Beamforming for SC-FDMA Multi-User Multi-Relay Networks\" with abstract: \"This paper is concerned with design problems of joint source power allocation and relay beamforming in multi-user multi-relay networks that use single-carrier frequency division multiple access (SC-FDMA) and amplify-and-forward relaying. Examined are the joint programs of (i) maximizing the minimum signal-to-interference-plus-noise ratio (SINR) under various transmitted power constraints, and (ii) minimizing the total transmitted power subject to prescribed SINR thresholds of users. Although these optimization problems are highly nonconvex and have large dimensions, by exploiting their partial convexities and making elegant nonlinear variable changes, they are recast as d.c. (difference of two convex) programs. Efficient d.c. iterative procedures are then developed to find the solutions. Simplified joint programs under the two cases of equal source power and equal relay beamforming weights, respectively, are also considered. Branch-and-bound algorithms of deterministic global optimization are then proposed for solving the simplified joint programs. Simulation results confirm the excellent performance and computational efficiency of all the proposed solutions.\"",
        "title: \"Orthogonal affine precoding and decoding for channel stimation and source detection in MIMO frequency-selective fading channels\" with abstract: \"A new affine precoding and decoding method for multiple-input multiple-output (MIMO) frequency-selective fading channels is proposed. The optimal design of the affine precoder consists of a linear precoder and a training sequence, which is superimposed on the linearly precoded data in an orthogonal manner. The channel is estimated independently of the data, while the training sequence is completely nulled out in the source data detection. Furthermore, the optimal power allocation between the data and training signals is also analytically derived. Simulation results show that the proposed method is significantly better than other affine precoding methods with regard to source detection performance and computational complexity.\"",
        "title: \"Relay Beamforming Designs in Multi-User Wireless Relay Networks Based on Throughput Maximin Optimization.\" with abstract: \"Beamforming design for multi-user wireless relay networks under the criterion of maximin information throughput is an important but also very hard optimization problem due to its nonconvex nature. The existing approach to reformulate the design as a matrix rank-one constrained optimization problem is highly inefficient. This paper exploits the d.c. (difference of two convex functions) structure of the objective function and the convex structure of the constraints in such a global optimization problem to develop efficient iterative algorithms of very low complexity to find the solutions. Both cases of concurrent and orthogonal transmissions from sources to relays are considered. Numerical results indicate that the proposed algorithms provide solutions that are very close to the upper bound on the solution of the non-orthogonal source transmissions case and are almost equal to the optimal solution of the orthogonal source transmissions case. This demonstrates the ability of the developed algorithms to locate approximations close to the global optimal solutions in a few iterations. Moreover, the proposed methods are superior to other methods in both performance and computation complexity.\"",
        "title: \"Joint Optimization of Source Precoding and Relay Beamforming in Wireless MIMO Relay Networks.\" with abstract: \"This paper considers joint linear processing at multi-antenna sources and one multiple-input multiple-output (MIMO) relay station for both one-way and two-way relay-assisted wireless communications. The one-way relaying is applicable in the scenario of downlink transmission by a multi-antenna base station to multiple single-antenna users with the help of one MIMO relay. In such a scenario, the objective of join linear processing is to maximize the information throughput to users. The design problem is equivalently formulated as the maximization of the worst signal-to-interference-plus-noise ratio (SINR) among all users subject to various transmission power constraints. Such a program of nonconvex objective minimization under nonconvex constraints is transformed to a canonical d.c. (difference of convex functions/sets) program of d.c. function optimization under convex constraints through nonconvex duality with zero duality gap. An efficient iterative algorithm is then applied to solve this canonical d.c program. For the scenario of using one MIMO relay to assist two sources exchanging their information in two-way relying manner, the joint linear processing aims at either minimizing the maximum mean square error (MSE) or maximizing the total information throughput of the two sources. By applying tractable optimization for the linear minimum MSE estimator and d.c. programming, an iterative algorithm is developed to solve these two optimization problems. Extensive simulation results demonstrate that the proposed methods substantially outperform previously-known joint optimization methods.\"",
        "title: \"A Novel and Efficient Mapping of 32-QAM Constellation for BICM-ID Systems\" with abstract: \"Bit-interleaved coded modulation with iterative decoding (BICM-ID) is a bandwidth-efficient technique for both additive white Gaussian noise and fading channels. The asymptotic performance of BICM-ID is strongly determined by how the coded bits are mapped to the symbols of the signal constellation. In this paper an explicit mapping method is presented for 32-QAM using two criteria: (i) maximization of the minimum Euclidean distance between the symbols with Hamming distance one, and (ii) minimizing the number of symbols which have jointly the minimum Hamming distance and the minimum Euclidean distance from each other. Our method is much simpler than the previously-known methods. Compared to previously-known best mapping, the mapping obtained by our method performs significantly better in a BICM-ID system implemented with hard-decision feedback, while its asymptotic performance is almost the same in a BICM-ID system using soft-decision feedback.\"",
        "1 is \"Local Layering for Joint Motion Estimation and Occlusion Detection\", 2 is \"Differential unitary space-time modulation\".",
        "\nGiven above information, for an author who has written the paper with the title \"Hybrid Filter Banks With Fractional Delays: Minimax Design and Application to Multichannel Sampling\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0138": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'LMI characterization for the convex hull of trigonometric curves and applications':",
        "title: \"Joint Optimization of Source Power Allocation and Cooperative Beamforming for SC-FDMA Multi-User Multi-Relay Networks\" with abstract: \"This paper is concerned with design problems of joint source power allocation and relay beamforming in multi-user multi-relay networks that use single-carrier frequency division multiple access (SC-FDMA) and amplify-and-forward relaying. Examined are the joint programs of (i) maximizing the minimum signal-to-interference-plus-noise ratio (SINR) under various transmitted power constraints, and (ii) minimizing the total transmitted power subject to prescribed SINR thresholds of users. Although these optimization problems are highly nonconvex and have large dimensions, by exploiting their partial convexities and making elegant nonlinear variable changes, they are recast as d.c. (difference of two convex) programs. Efficient d.c. iterative procedures are then developed to find the solutions. Simplified joint programs under the two cases of equal source power and equal relay beamforming weights, respectively, are also considered. Branch-and-bound algorithms of deterministic global optimization are then proposed for solving the simplified joint programs. Simulation results confirm the excellent performance and computational efficiency of all the proposed solutions.\"",
        "title: \"Precoder Design for Signal Superposition in MIMO-NOMA Multicell Networks.\" with abstract: \"The throughput of users with poor channel conditions, such as those at a cell edge, is a bottleneck in wireless systems. A major part of the power budget must be allocated to serve these users in guaranteeing their quality-of-service (QoS) requirements, hampering QoS for other users, and thus compromising the system reliability. In non-orthogonal multiple access (NOMA), the message intended for a user with a poor channel condition is decoded by itself and by another user with a better channel condition. The message intended for the latter is then successively decoded by itself after canceling the interference of the former. The overall information throughput is thus improved by this particular successive decoding and interference cancellation. This paper aims to design linear precoders/beamformers for signal superposition at the base stations of NOMA multiple-input multiple-output multi-cellular systems to maximize the overall sum throughput subject to the users' QoS requirements, which are imposed independently on the users' channel conditions. This design problem is formulated as the maximization of a highly nonlinear and nonsmooth function subject to nonconvex constraints, which is very computationally challenging. Path-following algorithms for its solution, which invoke only a simple convex problem of moderate dimension at each iteration, are developed. Generating a sequence of improved points, these algorithms converge at least to a local optimum. Extensive numerical simulations are then provided to demonstrate their merit.\"",
        "title: \"Robust control via concave minimization local and global algorithms\" with abstract: \"This paper is concerned with the robust control problem of linear fractional representation (LFT) uncertain systems depending on a time-varying parameter uncertainty. Our main result exploits a linear matrix inequality (LMI) characterization involving scalings and Lyapunov variables subject to an additional essentially nonconvex algebraic constraint. The nonconvexity enters the problem in the form of a rank deficiency condition or matrix inverse relation on the scalings only. It is shown that such problems, but also more generally rank inequalities and bilinear constraints, can be formulated as the minimization of a concave functional subject to LMI constraints. First of all, a local Frank and Wolfe (1956) feasible direction algorithm is introduced in this context to tackle this hard optimization problem. Exploiting the attractive concavity structure of the problem, several efficient global concave programming methods are then introduced and combined with the local feasible direction method to secure and certify global optimality of the solutions. Computational experiments indicate the viability of our algorithms, and in the worst case, they require the solution of a few LMI programs\"",
        "title: \"Optimum multi-user detection by nonsmooth optimization\" with abstract: \"The optimum multiuser detection (OMD) is a discrete (binary) optimization. The previously developed approaches often relax it by a semi-definite program (SDP) and then employ randomization for searching the optimal solution around the solution of this relaxed SDP. In this paper, we show the limited capacity of this SDP program, which at the end cannot give a better solution than the simple linear minimum mean square error detector (LMMSE). Our departure point is to express the problem as quadratic minimization over quadratic equality constraint (QMQE) or concave quadratic minimization over a box of continuous optimization (CQOB). The QMQE allows us to develop a nonsmooth optimization algorithm to locate the global optimal solution of OMD, while CQOB facilities effective confirmation of the solutions found by QMQE. Our intensive simulation clearly shows that the algorithm outperforms all previously developed algorithms while the computational burden is essentially reduced.\"",
        "title: \"MPC-Based UAV Navigation for Simultaneous Solar-Energy Harvesting and Two-Way Communications\" with abstract: \"The paper is the first work that considers a constrained feedback control strategy to navigate an unmanned aerial vehicle (UAV) from a given starting point to a given terminal point while harvesting solar energy and providing a wireless communication service for ground users. Wireless communication channels are stochastic and cannot be known off-line, making the problem of off-line UAV path planni...\"",
        "1 is \"L2 optimal filter reduction: a closed-loop approach\", 2 is \"Real-time spatiotemporal stereo matching using the dual-cross-bilateral grid\".",
        "\nGiven above information, for an author who has written the paper with the title \"LMI characterization for the convex hull of trigonometric curves and applications\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0139": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'A two-ball mouse affords three degrees of freedom':",
        "title: \"Fitts' throughput and the speed-accuracy tradeoff\" with abstract: \"We describe an experiment to test the hypothesis that Fitts' throughput is independent of the speed-accuracy tradeoff. Eighteen participants used a mouse in performing a total of 5,400 target selection trials. Comparing nominal, speed-emphasis, and accuracy-emphasis conditions, significant main effects were found on movement time (ms) and error rate (%), but not on throughput (bits/s). In the latter case, failure to reject the null hypothesis of \"no significant difference\" (i.e., .05\"",
        "title: \"An empirical comparison of \"wiimote\" gun attachments for pointing tasks\" with abstract: \"We evaluated and compared four input methods using the Nintendo Wii Remote for pointing tasks. The methods used (i) the \"A\" button on top of the device, (ii) the \"B\" button on the bottom of the device, (iii) the Intec Wii Combat Shooter attachment and (iv) the Nintendo Wii Zapper attachment. Fitts' throughput for all four input methods was calculated for both button-up and button-down events. Results indicate that the throughput of the Wii Remote using the A button is 2.85 bps for button-down events. Performance with the Intec Wii Combat Shooter attachment was significantly worse than with the other input methods, likely due to the trigger mechanism. Throughput for button-down target selection using the B button was highest at 2.93 bps.\"",
        "title: \"slab: smart labeling of family photos through an interactive interface\" with abstract: \"A novel technique for semi-automatic photo annotation is proposed and evaluated. The technique, sLab, uses face processing algorithms and a simplified user interface for labeling family photos. A user study compared our system with two others. One was Adobe Photoshop Element. The other was an in-house implementation of a face clustering interface recently proposed in the research community. Nine participants performed an annotation task with each system on faces extracted from a set of 150 images from their own family photo albums. As the faces were all well known to participants, accuracy was near perfect with all three systems. On annotation time, sLab was 25% faster than Photoshop Element and 16% faster than the face clustering interface.\"",
        "title: \"Graffiti vs. unistrokes: an empirical comparison\" with abstract: \"Unistrokes and Graffiti are stylus-based text entry techniques. While Unistrokes is recognized in academia, Graffiti is commercially prevalent in PDAs. Though numerous studies have investigated the usability of Graffiti, none exists to compare its long-term performance with that of Unistrokes. This paper presents a longitudinal study comparing entry speed, correction rate, stroke duration, and preparation (i.e., inter-stroke) time of these two techniques. Over twenty fifteen-phrase sessions, performance increased from 4.0 wpm to 11.4 wpm for Graffiti and from 4.1 wpm to 15.8 wpm for Unistrokes. Correction rates were high for both techniques. However, rates for Graffiti remained relatively consistent at 26%, while those for Unistrokes decreased from 43% to 16%.\"",
        "title: \"Testing pointing device performance and user assessment with the ISO 9241, Part 9 standard\" with abstract: \"The IS0 9241, Part 9 Draft International Standard for testingcomputer pointing devices proposes an evaluation of performance andcomfort. In this paper we evaluate the scientific validity andpracticality of these dimensions for two pointing devices forlaptop computers, a finger-controlled isometric joystick and atouchpad. Using a between-subjects design, evaluation ofperformance using the measure of throughput was done forone-direction and multi-directional pointing and selecting. Resultsshow a significant difference in throughput for themulti-directional task, with the joystick 27% higher; results forthe one-direction task were non-significant. After the experiment,participants rated the device for comfort, including operation,fatigue, and usability. The questionnaire showed no overalldifference in the responses, and a significant statisticaldifference in only the question concerning force required tooperate the device - the joystick requiring slightly more force.The paper concludes with a discussion of problems in implementingthe IS0 standard and recommendations for improvement.\"",
        "1 is \"Model-based and empirical evaluation of multimodal interactive error correction\", 2 is \"Probabilistic Fusion of Stereo with Color and Contrast for Bi-Layer Segmentation\".",
        "\nGiven above information, for an author who has written the paper with the title \"A two-ball mouse affords three degrees of freedom\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0140": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'User Elicitation on Single-hand Microgestures.':",
        "title: \"Perceptual grouping: selection assistance for digital sketching\" with abstract: \"Modifying a digital sketch may require multiple selections before a particular editing tool can be applied. Especially on large interactive surfaces, such interactions can be fatiguing. Accordingly, we propose a method, called Suggero, to facilitate the selection process of digital ink. Suggero identifies groups of perceptually related drawing objects. These \"perceptual groups\" are used to suggest possible extensions in response to a person's initial selection. Two studies were conducted. First, a background study investigated participant's expectations of such a selection assistance tool. Then, an empirical study compared the effectiveness of Suggero with an existing manual technique. The results revealed that Suggero required fewer pen interactions and less pen movement, suggesting that Suggero minimizes fatigue during digital sketching.\"",
        "title: \"Novel user interfaces for diagram versioning and differencing\" with abstract: \"Easily available software for diagram creation does not support the comparison of different versions and the merging of such versions. We present new methods and techniques for easy versioning of general two-dimensional diagrams. Multiple novel versioning methods for diagram versioning are compared to each other as well as to previous work in a user study. Participants in a user study preferred the Translucency View and Master Diagram Scenario to the other investigated methods and scenarios.\"",
        "title: \"Modeling learning effects in mobile texting\" with abstract: \"No work on mobile text messaging so far has taken into account the effect of learning on the change in visual exploration behavior as users progress from non-expert to expert level. We discuss within the domain of multi-tap texting on mobile phone and address the process of searching versus selecting a letter on the keypad interface. We develop a simulation model that forecasts the probability of letter location recall by non-expert users and thereby models learning, as the user acquires expertise in recalling, with practice, session after session. We then plugin this probability within a model of visual strategy that combines the effect of different ways visual exploration: non-expert users search for a letter while expert users select a letter. The observed non-expert non-motor time preceding a key press (for a letter) correlates extremely well with the simulation results.\"",
        "title: \"Laser Pointers As Collaborative Pointing Devices\" with abstract: \"Single Display Groupware (SDG) is a research area that focuses oil providing collaborative computing environments. Traditionally, most hardware platforms for SDG support only one person interacting at any given time, which limits collaboration. In this paper, we present laser pointers as input devices that can provide concurrent input streams ideally required to the SDG environment.First, we discuss several issues related to utilization of laser pointers and present the new concept of computer controlled laser pointers. Then we briefly present a performance evaluation of laser pointers as input devices and a baseline comparison with the mouse according to the ISO 9241-9 standard.Finally, we describe a new system that uses multiple computer controlled laser pointers as interaction devices for one or more displays. Several alternatives for distinguishing between different laser pointers arc presented, and ail implementation of one of them is demonstrated with SDG applications.\"",
        "title: \"A cognitive simulation model for novice text entry on cell phone keypads\" with abstract: \"Motivation -- To create a cognitive simulation model that predicts text entry performance and learning on cell phone keypads by novice users. Research approach -- A programmable cognitive architecture, ACT-R, is used to execute the simulation model. Part of the simulation result is compared with the result of a previous user study. Findings/Design -- The proposed model is an a priori model (not tuned to any real user data) that predicts the amount of time spent in finding a key on the keypad and pressing it repeatedly. The predicted amount of time in finding a key differs by 6% and the time between two repeated key-presses of the same key by 27% compared to the results of a previous user study. The model also captures the learning of keypad layout by novice users. Memorization of keypad layout is simulated using task repetition. Research limitations/Implications -- This research has several limitations described towards the end of this paper. An important one among them is that the work does not model the impact of visual distracters in the field of view (frontal surface of the handset) on user performance. Originality/Value -- This is the first cognitive simulation model of novice user's text entry performance and learning on cell phone keypads. Take away message -- This work introduces an a priori congnitive model of text entry by novice users. This forms a basis for systematic exploration of keypad designs for cell phones in shorter time and lower cost.\"",
        "1 is \"Feeling bumps and holes without a haptic interface: the perception of pseudo-haptic textures\", 2 is \"VisPorter: facilitating information sharing for collaborative sensemaking on multiple displays\".",
        "\nGiven above information, for an author who has written the paper with the title \"User Elicitation on Single-hand Microgestures.\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0141": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'GenDeR: A Generic Diversified Ranking Algorithm.':",
        "title: \"Classification of digital photos taken by photographers or home users\" with abstract: \"In this paper, we address a specific image classification task, i.e. to group images according to whether they were taken by photographers or home users. Firstly, a set of low-level features explicitly related to such high-level semantic concept are investigated together with a set of general-purpose low-level features. Next, two different schemes are proposed to find out those most discriminative features and feed them to suitable classifiers: one resorts to boosting to perform feature selection and classifier training simultaneously; the other makes use of the information of the label by Principle Component Analysis for feature re-extraction and feature de-correlation; followed by Maximum Marginal Diversity for feature selection and Bayesian classifier or Support Vector Machine for classification. In addition, we show an application in No-Reference holistic quality assessment as a natural extension of such image classification. Experimental results demonstrate the effectiveness of our methods.\"",
        "title: \"Function-on-Function Regression with Mode-Sparsity Regularization.\" with abstract: \"Functional data is ubiquitous in many domains, such as healthcare, social media, manufacturing process, sensor networks, and so on. The goal of function-on-function regression is to build a mapping from functional predictors to functional response. In this article, we propose a novel function-on-function regression model based on mode-sparsity regularization. The main idea is to represent the regression coefficient function between predictor and response as the double expansion of basis functions, and then use a mode-sparsity regularization to automatically filter out irrelevant basis functions for both predictors and responses. The proposed approach is further extended to the tensor version to accommodate multiple functional predictors. While allowing the dimensionality of the regression weight matrix or tensor to be relatively large, the mode-sparsity regularized model facilitates the multi-way shrinking of basis functions for each mode. The proposed mode-sparsity regularization covers a wide spectrum of sparse models for function-on-function regression. The resulting optimization problem is challenging due to the non-smooth property of the mode-sparsity regularization. We develop an efficient algorithm to solve the problem, which works in an iterative update fashion, and converges to the global optimum. Furthermore, we analyze the generalization performance of the proposed method and derive an upper bound for the consistency between the recovered function and the underlying true function. The effectiveness of the proposed approach is verified on benchmark functional datasets in various domains.\n\n\"",
        "title: \"On the Connectivity of Multi-layered Networks: Models, Measures and Optimal Control\" with abstract: \"Networks appear naturally in many high-impact real-world applications. In an increasingly connected and coupled world, the networks arising from many application domains are often collected from different channels, forming the so-called multi-layered networks, such as cyber-physical systems, organization-level collaboration platforms, critical infrastructure networks and many more. Compared with single-layered networks, multi-layered networks are more vulnerable as even a small disturbance on one supporting layer/network might cause a ripple effect to all the dependent layers, leading to a catastrophic/cascading failure of the entire system. The state-of-the-art has been largely focusing on modeling and manipulating the cascading effect of two-layered interdependent network systems for some specific type of network connectivity measure. This paper generalizes the challenge to multiple dimensions. First, we propose a new data model for multi-layered networks MULAN, which admits an arbitrary number of layers with a much more flexible dependency structure among different layers, beyond the current pair-wise dependency. Second, we unify a wide range of classic network connectivity measures SUBLINE. Third, we show that for any connectivity measure in the SUBLINE family, it enjoys the diminishing returns property which in turn lends itself to a family of provable near-optimal control algorithms with linear complexity. Finally, we conduct extensive empirical evaluations on real network data, to validate the effectiveness of the proposed algorithms.\"",
        "title: \"GenDeR: A Generic Diversified Ranking Algorithm.\" with abstract: \"Diversified ranking is a fundamental task in machine learning. It is broadly applicable in many real world problems, e.g., information retrieval, team assembling, product search, etc. In this paper, we consider a generic setting where we aim to diversify the top-k ranking list based on an arbitrary relevance function and an arbitrary similarity function among all the examples. We formulate it as an optimization problem and show that in general it is NP-hard. Then, we show that for a large volume of the parameter space, the proposed objective function enjoys the diminishing returns property, which enables us to design a scalable, greedy algorithm to find the near-optimal solution. Experimental results on real data sets demonstrate the effectiveness of the proposed algorithm.\"",
        "title: \"Feature co-shrinking for co-clustering.\" with abstract: \"\u2022We propose a novel non-negative matrix tri-factorization model based on cosparsity regularization to enable the co-feature-selection for co-clustering. It aims to learn the inter-correlation among the multi-way features while co-shrinking the irrelevant ones by encouraging the co-sparsity of the model parameters.\u2022We propose an efficient algorithm to solve the non-smooth optimization problem. It works in an iteratively update fashion, and is guaranteed to converge.\u2022Experimental results on various data sets show the effectiveness of the proposed approach.\"",
        "1 is \"Approximating the Expansion Profile and Almost Optimal Local Graph Clustering\", 2 is \"Urban sensing systems: opportunistic or participatory?\".",
        "\nGiven above information, for an author who has written the paper with the title \"GenDeR: A Generic Diversified Ranking Algorithm.\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0142": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Fast and Flexible Top-k Similarity Search on Large Networks':",
        "title: \"Pseudo relevance feedback based on iterative probabilistic one-class SVMs in web image retrieval\" with abstract: \"To improve the precision of top-ranked images returned by a web image search engine, we propose in this paper a novel pseudo relevance feedback method named iterative probabilistic one-class SVMs to re-rank the retrieved images. By assuming that most top-ranked images are relevant to the query, we iteratively train one-class SVMs, and convert the outputs to probabilities so as to combine the decision from different image representation. The effectiveness of our method is validated by systematic experiments even if the assumption is not well satisfied.\"",
        "title: \"Structured Low-Rank Matrix Factorization with Missing and Grossly Corrupted Observations.\" with abstract: \"  Recovering low-rank and sparse matrices from incomplete or corrupted observations is an important problem in machine learning, statistics, bioinformatics, computer vision, as well as signal and image processing. In theory, this problem can be solved by the natural convex joint/mixed relaxations (i.e., l_{1}-norm and trace norm) under certain conditions. However, all current provable algorithms suffer from superlinear per-iteration cost, which severely limits their applicability to large-scale problems. In this paper, we propose a scalable, provable structured low-rank matrix factorization method to recover low-rank and sparse matrices from missing and grossly corrupted data, i.e., robust matrix completion (RMC) problems, or incomplete and grossly corrupted measurements, i.e., compressive principal component pursuit (CPCP) problems. Specifically, we first present two small-scale matrix trace norm regularized bilinear structured factorization models for RMC and CPCP problems, in which repetitively calculating SVD of a large-scale matrix is replaced by updating two much smaller factor matrices. Then, we apply the alternating direction method of multipliers (ADMM) to efficiently solve the RMC problems. Finally, we provide the convergence analysis of our algorithm, and extend it to address general CPCP problems. Experimental results verified both the efficiency and effectiveness of our method compared with the state-of-the-art methods. \"",
        "title: \"Joint voting prediction for questions and answers in CQA\" with abstract: \"Community Question Answering (CQA) sites have become valuable repositories that host a massive volume of human knowledge. How can we detect a high-value answer which clears the doubts of many users? Can we tell the user if the question s/he is posting would attract a good answer? In this paper, we aim to answer these questions from the perspective of the voting outcome by the site users. Our key observation is that the voting score of an answer is strongly positively correlated with that of its question, and such correlation could be in turn used to boost the prediction performance. Armed with this observation, we propose a family of algorithms to jointly predict the voting scores of questions and answers soon after they are posted in the CQA sites. Experimental evaluations demonstrate the effectiveness of our approaches.\"",
        "title: \"Manifold-ranking-based keyword propagation for image retrieval\" with abstract: \"A novel keyword propagation method is proposed for image retrieval based on a recently developed manifold-ranking algorithm. In contrast to existing methods which train a binary classifier for each keyword, our keyword model is constructed in a straight forward manner by exploring the relationship among all images in the feature space in the learning stage. In relevance feedback, the feedback information can be naturally incorporated to refine the retrieval result by additional propagation processes. In order to speed up the convergence of the query concept, we adopt two active learning schemes to select images during relevance feedback. Furthermore, by means of keyword model update, the system can be self-improved constantly. The updating procedure can be performed online during relevance feedback without extra offline training. Systematic experiments on a general-purpose image database consisting of 5000 Corel images validate the effectiveness of the proposed method.\"",
        "title: \"A Local Algorithm for Structure-Preserving Graph Cut\" with abstract: \"Nowadays, large-scale graph data is being generated in a variety of real-world applications, from social networks to co-authorship networks, from protein-protein interaction networks to road traffic networks. Many existing works on graph mining focus on the vertices and edges, with the first-order Markov chain as the underlying model. They fail to explore the high-order network structures, which are of key importance in many high impact domains. For example, in bank customer personally identifiable information (PII) networks, the star structures often correspond to a set of synthetic identities; in financial transaction networks, the loop structures may indicate the existence of money laundering. In this paper, we focus on mining user-specified high-order network structures and aim to find a structure-rich subgraph which does not break many such structures by separating the subgraph from the rest. A key challenge associated with finding a structure-rich subgraph is the prohibitive computational cost. To address this problem, inspired by the family of local graph clustering algorithms for efficiently identifying a low-conductance cut without exploring the entire graph, we propose to generalize the key idea to model high-order network structures. In particular, we start with a generic definition of high-order conductance, and define the high-order diffusion core, which is based on a high-order random walk induced by user-specified high-order network structure. Then we propose a novel High-Order Structure-Preserving LOcal Cut (HOSPLOC) algorithm, which runs in polylogarithmic time with respect to the number of edges in the graph. It starts with a seed vertex and iteratively explores its neighborhood until a subgraph with a small high-order conductance is found. Furthermore, we analyze its performance in terms of both effectiveness and efficiency. The experimental results on both synthetic graphs and real graphs demonstrate the effectiveness and efficiency of our proposed HOSPLOC algorithm.\"",
        "1 is \"DEX: Deep EXpectation of apparent age from a single image\", 2 is \"Modeling dynamic behavior in large evolving graphs\".",
        "\nGiven above information, for an author who has written the paper with the title \"Fast and Flexible Top-k Similarity Search on Large Networks\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0143": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'About the lossless reduction of the minimal generator family of a context':",
        "title: \"Constructing Iceberg Lattices from Frequent Closures Using Generators\" with abstract: \"Frequent closures (FCIs) and generators (FGs) as well as the precedence relation on FCIs are key components in the definition of a variety of association rule bases. Although their joint computation has been studied in concept analysis, no scalable algorithm exists for the task at present. We propose here to reverse a method from the latter field using a fundamental property of hypergraph theory. The goal is to extract the precedence relation from a more common mining output, i.e. closures and generators. The resulting order computation algorithm proves to be highly efficient, benefiting from peculiarities of generator families in typical mining datasets. Due to its genericity, the new algorithm fits an arbitrary FCI/FG-miner.\"",
        "title: \"Generating frequent itemsets incrementally: two novel approaches based on Galois lattice theory\" with abstract: \"Galois (concept) lattice theory has been successfully applied in data mining for the resolution of the association rule problem. In particular, structural results about lattices have been used in the design of efficient procedures for mining the frequent patterns (itemsets) in transaction databases. Since such databases are often dynamic, we propose a detailed study of the incremental aspects in lattice construction to support effective procedures for incremental mining of frequent closed itemsets (FCIs). Based on a set of descriptive results about lattice substructures involved in incremental updates, the paper presents a novel algorithm for lattice construction that explores only limited parts of a lattice for updating. Two new methods for incremental FCI mining are studied: the first inherits its extensive search strategy from a classical lattice method, whereas the second applies the new lattice construction strategy to the itemset mining context. Unlike batch techniques based on FCIs, both methods avoid rebuilding the FCI family from scratch whenever new transactions are added to the database and/or when the minimal support is changed.\"",
        "title: \"T-GOWler: Discovering Generalized Process Models Within Texts.\" with abstract: \"Contemporary workflow management systems are driven by explicit process models specifying the interdependencies between tasks. Creating these models is a challenging and timeconsuming task. Existing approaches to mining concrete workflows into models tackle design aspects related to the diverging abstraction levels of the tasks. Concrete workflow logs represent tasks and cases of concrete events-partially or totally ordered-grounding hidden multilevel (abstract) semantics and contexts. Relevant generalized events could be rediscovered within these processes. We propose, in this article, an ontology-based workflow mining systemto generate patterns fromsequences of events that are themselves extracted from texts. Our system T-GOWler (Generalized Ontology-basedWorkfLow minER within Texts) is based on two ontology-basedmodules: a workflow extractor and a patternminer. To this end, it uses two different ontologies: a domain one (to support workflow extraction from texts) and a processual one (to mine generalized patterns from extracted workflows).\"",
        "title: \"On the Assessment of Concept Relevance in FCA-Based Ontology Restructuring\" with abstract: \"Along their lif-cycle, Ontologies typically undergo a number of structural operations that might deplete their structural quality. Ontology restructuring is a process of improving that quality by reconsidering the way the knowledge is spread across the class and property hierarchies. This often leads to the discovery of new abstractions whose relevance to the ontology must be assessed. We address the relevance assessment within a context where it is crucial: Our restructuring method performs a enhanced concept analysis on the initial ontology that outputs all abstractions of a predefined language. Thus, the key step is the selection of the abstractions to include into the restructured ontology. We propose a set of relevance metrics and a rule-based algorithm for combining them into a single filtering criterion. Their effectiveness is evaluated using a collection of ontologies that have been analyzed beforehand to provide ground truth.\"",
        "title: \"Formal Concept Analysis for Knowledge Discovery and Data Mining: The New Challenges\" with abstract: \"Data mining (DM) is the extraction of regularities from raw data, which are further transformed within the wider process of knowledge discovery in databases (KDD) into non-trivial facts intended to support decision making. Formal concept analysis (FCA) offers an appropriate framework for KDD, whereby our focus here is on its potential for DM support. A variety of mining methods powered by FCA have been published and the figures grow steadily, especially in the association rule mining (ARM) field. However, an analysis of current ARM practices suggests the impact of FCA has not reached its limits, i.e., appropriate FCA-based techniques could successfully apply in a larger set of situations. As a first step in the projected FCA expansion, we discuss the existing ARM methods, provide a set of guidelines for the design of novel ones, and list some open algorithmic issues on the FCA side. As an illustration, we propose two on-line methods computing the minimal generators of a closure system.\"",
        "1 is \"Frequent Itemset Mining from Databases Including One Evidential Attribute\", 2 is \"Developing a robust part-of-speech tagger for biomedical text\".",
        "\nGiven above information, for an author who has written the paper with the title \"About the lossless reduction of the minimal generator family of a context\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0144": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'On The Discovery of Semantically Enhanced Sequential Patterns':",
        "title: \"Distributed and Parallel Computation of the Canonical Direct Basis.\" with abstract: \"Mining association rules, including implications, is an important topic in Knowledge Discovery research area and in Formal Concept Analysis (FCA). In this paper, we present a novel algorithm that computes in a parallel way the canonical direct unit basis of a formal context in FCA. To that end, the algorithm first performs a horizontal split of the initial context into subcontexts and then exploits the notion of minimal dual transversal to merge the canonical direct unit bases generated from subcontexts.\"",
        "title: \"Group disappearance in social networks with communities\" with abstract: \"The purpose of this paper is to handle the disappearance of a group of nodes in a social network. The quality of the information flow is used as a key performance indicator to conduct network changes after group disappearance. Nodes as well as node sets are first classified into categories (critical and non-critical nodes, and scattered, contiguous and hybrid groups) and then analyzed according to two distinct perspectives: the network as a whole or its identified communities. Finally, algorithms are devised to manage group disappearance according to different cases. New links are added in a parsimonious way and a possible substitute for a leaving group is found based on the adage \u201cbirds of a feather flock together\u201d and the homophily principle. This means that new links (e.g., relationships) and a potential substitute are found only between individuals that share common characteristics such as beliefs, values, and education, i.e., individuals that are more likely neighbors of the leaving node or group. To validate our approach, an empirical study is conducted using various kinds of data sets and a set of criteria. The results show the benefits of our solution in terms of response time, number of added links and metrics of the overall network topology.\"",
        "title: \"Conceptual modeling for data and knowledge management\" with abstract: \"In order to exploit knowledge embedded in databases and to migrate from data to knowledge management environments, conceptual modeling languages must offer more expressiveness than traditional modeling languages. This paper proposes the conceptual graph formalism as such a modeling language. It shows through an example and a comparison with Telos, a semantically rich knowledge modeling language, that it is suited for that purpose. The conceptual graph formalism offers simplicity of use through its graphical components and small set of constructs and operators. It allows easy migration from database to knowledge base environments. Thus, this paper advocates its use. (C) 2000 Elsevier Science B.V. All rights reserved.\"",
        "title: \"Formal Concept Analysis for Knowledge Discovery and Data Mining: The New Challenges\" with abstract: \"Data mining (DM) is the extraction of regularities from raw data, which are further transformed within the wider process of knowledge discovery in databases (KDD) into non-trivial facts intended to support decision making. Formal concept analysis (FCA) offers an appropriate framework for KDD, whereby our focus here is on its potential for DM support. A variety of mining methods powered by FCA have been published and the figures grow steadily, especially in the association rule mining (ARM) field. However, an analysis of current ARM practices suggests the impact of FCA has not reached its limits, i.e., appropriate FCA-based techniques could successfully apply in a larger set of situations. As a first step in the projected FCA expansion, we discuss the existing ARM methods, provide a set of guidelines for the design of novel ones, and list some open algorithmic issues on the FCA side. As an illustration, we propose two on-line methods computing the minimal generators of a closure system.\"",
        "title: \"Computing Implications with Negation from a Formal Context\" with abstract: \"The objective of this article is to define an approach towards generating implications with (or without) negation when only a formal context K = (G, M, I) is provided. To that end, we define a two-step procedure which first (i) computes implications whose premise is a key in the context K | $\\tilde{\\rm K}$ representing the apposition of the context K and its complementary $\\tilde{\\rm K}$ with attributes in $\\tilde{\\rm M}$ (negative attributes), and then (ii) uses an inference axiom we have defined to produce the whole set of implications.\"",
        "1 is \"Improving pattern quality in web usage mining by using semantic information\", 2 is \"Semantic Query Optimization for Object Databases\".",
        "\nGiven above information, for an author who has written the paper with the title \"On The Discovery of Semantically Enhanced Sequential Patterns\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0145": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Finding large degree-anonymous subgraphs is hard.':",
        "title: \"Satisfactory graph partition, variants, and generalizations\" with abstract: \"The Satisfactory Partition problem asks for deciding if a given graph has a partition of its vertex set into two nonempty parts such that each vertex has at least as many neighbors in its part as in the other part. This problem was introduced by Gerber and Kobler [M. Gerber, D. Kobler, Algorithmic approach to the satisfactory graph partitioning problem, European Journal of Operational Research 125 (2000) 283\u2013291] and studied further by other authors. In this paper we first review some applications and related problems. Then, we survey structural, complexity, and approximation results obtained for Satisfactory Partition and for some of its variants and generalizations. A list of open questions concludes this survey.\"",
        "title: \"On the Complexity Landscape of the Domination Chain.\" with abstract: \"In this paper, we survey and supplement the complexity landscape of the domination chain parameters as a whole, including classifications according to approximability and parameterised complexity. Moreover, we provide clear pointers to yet open questions. As this posed the majority of hitherto unsettled problems, we focus on Upper Irredundance and Lower Irredundance that correspond to finding the largest irredundant set and resp. the smallest maximal irredundant set. The problems are proved NP-hard even for planar cubic graphs. While Lower Irredundance is proved not $$c\\\\log n$$-approximable in polynomial time unless $$\\\\mathrm {NP}\\\\subseteq \\\\mathrm {DTIME}n^{\\\\log \\\\log n}$$, no such result is known for Upper Irredundance. Their complementary versions are constant-factor approximable in polynomial time. All these four versions are APX-hard even on cubic graphs.\"",
        "title: \"An efficient implementation for the 0-1 multi-objective Knapsack problem\" with abstract: \"In this paper, we present an approach, based on dynamic programming, for solving 0-1 multi-objective knapsack problems. The main idea of the approach relies on the use of several complementary dominance relations to discard partial solutions that cannot lead to new nondominated criterion vectors. This way, we obtain an efficient method that outperforms the existing methods both in terms of CPU time and size of solved instances. Extensive numerical experiments on various types of instances are reported. A comparison with other exact methods is also performed. In addition, for the first time to our knowledge, we present experiments in the three-objective case.\"",
        "title: \"Parameterized complexity of firefighting.\" with abstract: \"The Firefighter problem is to place firefighters on the vertices of a graph to prevent a fire with known starting point from lighting up the entire graph. In each time step, a firefighter may be placed on an unburned vertex, permanently protecting it, and the fire spreads to all neighboring unprotected vertices of burning vertices. The goal is to let as few vertices burn as possible. In this paper, we consider a generalization of this problem, where at each time step b\u2a7e1 firefighters can be deployed. Our results answer several open questions raised by Cai et al. [8]. We show that this problem is W[1]-hard when parameterized by the number of saved vertices, protected vertices, and burned vertices. We also investigate several combined parameterizations for which the problem is fixed-parameter tractable. Some of our algorithms improve on previously known algorithms. We also establish lower bounds to polynomial kernelization.\"",
        "title: \"Min-max and min-max regret versions of combinatorial optimization problems: A survey\" with abstract: \"Min\u2013max and min\u2013max regret criteria are commonly used to define robust solutions. After motivating the use of these criteria, we present general results. Then, we survey complexity results for the min\u2013max and min\u2013max regret versions of some combinatorial optimization problems: shortest path, spanning tree, assignment, min cut, min s\u2013t cut, knapsack. Since most of these problems are NP-hard, we also investigate the approximability of these problems. Furthermore, we present algorithms to solve these problems to optimality.\"",
        "1 is \"Approximability of maximum splitting of k-sets and some other Apx-complete problems\", 2 is \"Kernelization: New Upper and Lower Bound Techniques\".",
        "\nGiven above information, for an author who has written the paper with the title \"Finding large degree-anonymous subgraphs is hard.\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0146": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Bounds on the game transversal number in hypergraphs.':",
        "title: \"Graphs with no induced C4 and 2K2\" with abstract: \"We characterize the structure of graphs containing neither the 4-cycle nor its complement as an induced subgraph. This self-complementary class G of graphs includes split graphs, which are graphs whose vertex set is the union of a clique and an independent set. In the members of G , the number of cliques (as well as the number of maximal independent sets) cannot exceed the number of vertices. Moreover, these graphs are almost extremal to the theorem of Nordhaus and Gaddum (1956).\"",
        "title: \"New models of graph-bin packing.\" with abstract: \"In Bujt\u00e1s et al. (2011) 4 the authors introduced a very general problem called Graph-Bin Packing (GBP). It requires a mapping \u00b5 : V ( G ) \u017a V ( H ) from the vertex set of an input graph G into a fixed host graph H, which, among other conditions, satisfies that for each pair u , v of adjacent vertices the distance of \u00b5 ( u ) and \u00b5 ( v ) in H is between two prescribed bounds. In this paper we propose two online versions of the Graph-Bin Packing problem. In both cases the vertices can arrive in an arbitrary order where each new vertex is adjacent to some of the previous ones. One version is a Maker-Breaker game whose rules are defined by the packing conditions. A subclass of Maker-win input graphs is what we call 'well-packable'; it means that a packing of G is obtained whenever the mapping \u00b5 ( u ) is generated by selecting an arbitrary feasible vertex of the host graph for the next vertex of G in each step. The other model is connected-online packing where we are looking for an online algorithm which can always find a feasible packing. In both models we present some sufficient and some necessary conditions for packability. In the connected-online version we also give bounds on the size of used part of the host graph.\"",
        "title: \"Characterization of (m,1)-transitive and (3,2)-transitive semi-complete directed graphs\" with abstract: \"A directed graph is called ( m , k )-transitive if for every directed path x 0 x 1 \u2026 x m there is a directed path y 0 y 1 \u2026 y k such that x 0 = y 0 , x m = y k , and { y i |0\u2a7d i \u2a7d k } \u2282{ x i |0\u2a7d i \u2a7d m }. We describe the structure of those ( m , 1)-transitive and (3,2)-transitive directed graphs in which each pair of vertices is adjacent by an arc in at least one direction, and present an algorithm with running time O( n 2 ) that tests ( m, k )-transitivity in such graphs on n vertices for every m and k =1, and for m =3 and k =2.\"",
        "title: \"Complexity of most vital nodes for independent set in graphs related to tree structures\" with abstract: \"Given an undirected graph with weights on its vertices, the k most vital nodes independent set problem consists of determining a set of k vertices whose removal results in the greatest decrease in the maximum weight of independent sets. We also consider the complementary problem, minimum node blocker independent set that consists of removing a subset of vertices of minimum size such that the maximum weight of independent sets in the remaining graph is at most a specified value. We show that these problems are NP-hard on bipartite graphs but polynomial-time solvable on unweighted bipartite graphs. Furthermore, these problems are polynomial also on graphs of bounded treewidth and cographs. A result on the non-existence of a ptas is presented, too.\"",
        "title: \"On the Existence and Determination of Satisfactory Partitions in a Graph\" with abstract: \"The SATISFACTORY PARTITION problem consists in deciding if a given graph has a partition of its vertex set into two nonempty sets V-1, V-2 such that for each vertex upsilon, if upsilon is an element of V-i then dv(i) (upsilon) greater than or equal to S(upsilon), where s(upsilon) less than or equal to d(less than or equal to) is a given integer-valued function. This problem was introduced by Gerber and Kobler [EJOR 125 (2000), 283-291] for s = [d/2]. In this paper we study the complexity of this problem for different values of s.\"",
        "1 is \"On diameter 2-critical graphs\", 2 is \"The size of minimum 3-trees\".",
        "\nGiven above information, for an author who has written the paper with the title \"Bounds on the game transversal number in hypergraphs.\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0147": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Bi-objective matchings with the triangle inequality.':",
        "title: \"Efficient computation of the search region in multi-objective optimization.\" with abstract: \"\u2022We define a neighborhood relation on search zones.\u2022We develop an algorithm to update the search region that uses this relation.\u2022We show by numerical experiments that it enables a much more efficient exploration of the search region than earlier approaches.\"",
        "title: \"Discrete representation of the non-dominated set for multi-objective optimization problems using kernels.\" with abstract: \"\u2022We give algorithms which produce discrete representations of the non-dominated set.\u2022These representations satisfy conditions of covering, spacing, and minimum size.\u2022We introduce the concept of kernel for this purpose and study its properties.\u2022In the bi-objective case we give exact and approximate algorithms to build kernels.\u2022For more than two objectives we give some positive and negative results.\"",
        "title: \"On the number of non-dominated points of a multicriteria optimization problem\" with abstract: \"This work proposes an upper bound on the maximal number of non-dominated points of a multicriteria optimization problem. Assuming that the number of values taken on each criterion is known, the criterion space corresponds to a comparability graph or a product of chains. Thus, the upper bound can be interpreted as the stability number of a comparability graph or, equivalently, as the width of a product of chains. Standard approaches or formulas for computing these numbers are impractical. We develop a practical formula which only depends on the number of criteria. We also investigate the tightness of this upper bound and the reduction of this bound when feasible, possibly efficient, solutions are known.\"",
        "title: \"Efficient determination of the k most vital edges for the minimum spanning tree problem\" with abstract: \"We study in this paper the problem of finding in a graph a subset of k edges whose deletion causes the largest increase in the weight of a minimum spanning tree. We propose for this problem an explicit enumeration algorithm whose complexity, when compared to the current best algorithm, is better for general k but very slightly worse for fixed k. More interestingly, unlike in the previous algorithms, we can easily adapt our algorithm so as to transform it into an implicit enumeration algorithm based on a branch and bound scheme. We also propose a mixed integer programming formulation for this problem. Computational results show a clear superiority of the implicit enumeration algorithm both over the explicit enumeration algorithm and the mixed integer program.\"",
        "title: \"A unified framework for multiple criteria auction mechanisms\" with abstract: \"Multi-attribute auctions allow negotiations over multiple attributes besides price. Multiple criteria English reverse auction mechanisms differ regarding the aggregation model used to represent the buyer's preferences and the feedback information provided to bidders during the auction. In this paper, we present a unified framework, called MERA in order to integrate existing mechanisms and guide the design of new ones. Framework MERA both provides an automated buyer agent that manages auctions and a formal model that describes an auction mechanism. This model includes a class of preference relations used to express the buyer's preference relation, a class of request relations used to formulate the feedback, and a class of constraints used to express the initial requirements on the purchased item. We study efficiency for multiple criteria English reverse auction processes, and we show that any process derived from framework MERA is efficient.\"",
        "1 is \"Database merging strategy based on logistic regression\", 2 is \"Approximation results for the weighted P4 partition problem\".",
        "\nGiven above information, for an author who has written the paper with the title \"Bi-objective matchings with the triangle inequality.\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0148": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Uniform intersecting families with covering number four':",
        "title: \"Extremal k-edge-hamiltonian hypergraphs\" with abstract: \"Abstract An r -uniform hypergraph is k -edge-hamiltonian iff it still contains a hamiltonian chain after deleting any k edges of the hypergraph. What is the minimum number of edges in such a hypergraph? We give lower and upper bounds for this question for several values of r and k.\"",
        "title: \"The Erdos-Ko-Rado Theorem for Integer Sequences\" with abstract: \"For positive integers n,q,t we determine the maximum number of integer sequences (a1,...,an) which satisfy 1 ai q for 1 i n, and any two sequences agree in at least t positions. The result gives an armative answer to a conjecture of Frankl and Furedi.\"",
        "title: \"Two-colorings with many monochromatic cliques in both colors\" with abstract: \"Color the edges of the n-vertex complete graph in red and blue, and suppose that red k-cliques are fewer than blue k-cliques. We show that the number of red k-cliques is always less than c\"kn^k, where c\"k@?(0,1) is the unique root of the equation z^k=(1-z)^k+kz(1-z)^k^-^1. On the other hand, we construct a coloring in which there are at least c\"kn^k-O(n^k^-^1) red k-cliques and at least the same number of blue k-cliques.\"",
        "title: \"Linear independence, a unifying approach to shadow theorems.\" with abstract: \"The intersection shadow theorem of Katona is an important tool in extremal set theory. The original proof is purely combinatorial. The aim of the present paper is to show how it is using linear independence latently.\"",
        "title: \"A size-sensitive inequality for cross-intersecting families.\" with abstract: \"Two families A and B of k-subsets of an n-set are called cross-intersecting if AB0 for all AA,BB. Strengthening the classical ErdsKoRado theorem, Pyber proved that |A||B|n1k12 holds for n2k. In the present paper we sharpen this inequality. We prove that assuming |B|n1k1+niki+1 for some 3ik+1 the stronger inequality |A||B|(n1k1+niki+1)(n1k1nik1) holds. These inequalities are best possible. We also present a new short proof of Pybers inequality and a short computation-free proof of an inequality due to Frankl and Tokushige (1992).\"",
        "1 is \"The pagenumber of genus g graphs is O(g)\", 2 is \"Perfect matching in 3 uniform hypergraphs with large vertex degree\".",
        "\nGiven above information, for an author who has written the paper with the title \"Uniform intersecting families with covering number four\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0149": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'TWOPRIME: A Fast Stream Ciphering Algorithm':",
        "title: \"Authentication Schemes from Highly Nonlinear Functions\" with abstract: \"We construct two families of authentication schemes using highly nonlinear functions on finite fields of characteristic 2. This leads to improvements on an earlier construction by Ding and Niederreiter if one chooses, for instance, an almost bent function as the highly nonlinear function\"",
        "title: \"Maximal arcs and extended cyclic codes\" with abstract: \"It is proved that for every \\(d\\ge 2\\) such that \\(d-1\\) divides \\(q-1\\), where q is a power of 2, there exists a Denniston maximal arc A of degree d in \\({\\mathrm {PG}}(2,q)\\), being invariant under a cyclic linear group that fixes one point of A and acts regularly on the set of the remaining points of A. Two alternative proofs are given, one geometric proof based on Abatangelo\u2013Larato\u2019s characterization of Denniston arcs, and a second coding-theoretical proof based on cyclotomy and the link between maximal arcs and two-weight codes.\"",
        "title: \"Cyclotomic Linear Codes Of Order 3\" with abstract: \"In this correspondence, two classes of cyclotomic linear codes over GF (q) of order 3 are constructed and their weight distributions are determined. The two classes are two-weight codes and contain optimal codes. They are not equivalent to irreducible cyclic codes in general when q > 2.\"",
        "title: \"The Bose and Minimum Distance of a Class of BCH Codes\" with abstract: \"Cyclic codes are an interesting class of linear codes due to their efficient encoding and decoding algorithms. Bose-Ray-Chaudhuri-Hocquenghem (BCH) codes form a subclass of cyclic codes and are very important in both theory and practice as they have good error-correcting capability and are widely used in communication systems, storage devices, and consumer electronics. However, the dimension and minimum distance of BCH codes are not known in general. The objective of this paper is to determine the Bose and minimum distances of a class of narrow-sense primitive BCH codes.\"",
        "title: \"Cyclotomic optical orthogonal codes of composite lengths\" with abstract: \"Optical orthogonal codes (OOCs) have applications in optical code-division multiple-access communications systems and other wideband code-division multiple environments. They can also be used to construct protocol sequences for multiuser collision channel without feedback, and constant-weight codes for error detection and correction. We have given a cyclotomic construction of several classes of (2m-1,w,2) OOCs recently. The purpose of this paper is to present five classes of (q-1,w,2) OOCs, and thus five classes of binary constant-weight cyclic codes, where q is a power of an odd prime.\"",
        "1 is \"State complexity of some operations on binary regular languages\", 2 is \"New cyclic difference sets with Singer parameters\".",
        "\nGiven above information, for an author who has written the paper with the title \"TWOPRIME: A Fast Stream Ciphering Algorithm\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0150": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Power-Distortion Optimization For Wireless Image/Video Softcast By Transform Coefficients Energy Modeling With Adaptive Chunk Division':",
        "title: \"Fast Image Super-Resolution via Local Adaptive Gradient Field Sharpening Transform.\" with abstract: \"This paper proposes a single-image super-resolution scheme by introducing a gradient field sharpening transform that converts the blurry gradient field of upsampled low-resolution (LR) image to a much sharper gradient field of original high-resolution (HR) image. Different from the existing methods that need to figure out the whole gradient profile structure and locate the edge points, we derive a...\"",
        "title: \"Fully Connected Network-Based Intra Prediction for Image Coding.\" with abstract: \"This paper proposes a deep learning method for intra prediction. Different from traditional methods utilizing some fixed rules, we propose using a fully connected network to learn an end-to-end mapping from neighboring reconstructed pixels to the current block. In the proposed method, the network is fed by multiple reference lines. Compared with traditional single line-based methods, more contextu...\"",
        "title: \"Compressive gradient based scalable image SoftCast\" with abstract: \"In wireless visual communication systems, it is crucial to effectively utilize channel power and bandwidth in the pursue of optimal performance, and it is worthwhile to adapt the transmission scheme to human vision system (HVS) so as to achieve perceptually appealing results. Inspired by observations that visual quality of an image is closely related to the gradient data, this paper proposes to convey visual information by random projection measurements of image gradients in an analog framework. Since HVS is more sensitive to luminance variations of image contents, which are contained in the gradient data, the proposed scheme achieves better perceptual quality than conventional analog uncoded schemes like SoftCast. Besides, the gradient transform removes the low and medium frequency components of the image hence substantially reduces the power of the signal transmitted in the analog channel, thus evidently improves the power-distortion performance of the system. Furthermore, by applying random projection to the gradients, the number of transmitted data can be adjusted according to bandwidth conditions. Another contribution of this paper is to develop an effective optimization scheme for the compressive gradient based reconstruction problem. Experimental results validate the effectiveness of the proposed transmission and reconstruction scheme under different channel signal-to-noise ratio and bandwidth conditions.\"",
        "title: \"WaveCast: Wavelet based wireless video broadcast using lossy transmission\" with abstract: \"Wireless video broadcasting is a popular application of mobile network. However, the traditional approaches have limited supports to the accommodation of users with diverse channel conditions. The newly emerged Softcast approach provides smooth multicast performance but is not very efficient in inter frame compression. In this work, we propose a new video multicast approach: WaveCast. Different from softcast, WaveCast utilizes motion compensated temporal filter (MCTF) to exploit inter frame redundancy, and utilizes conventional framework to transmit motion information such that the MVs can be reconstructed losslessly. Meanwhile, WaveCast transmits the transform coefficients in lossy mode and performs gracefully in multicast. In experiments, Wave-Cast outperforms softcast 2dB in video PSNR at low channel SNR, and outperforms H.264 based framework up to 8dB in broadcast.\"",
        "title: \"Hybridcast: A wireless image/video SoftCast scheme using layered representation and hybrid digital-analog modulation\" with abstract: \"The recently proposed SoftCast scheme employs analog-like transmission for wireless visual communication, providing graceful reconstruction quality degradation for drastically changing channel conditions. However, the transmission in SoftCast is not always efficient in terms of power usage. In this paper, we propose a wireless image/video SoftCast scheme which employs layered representation with hybrid digital-analog modulation. In this scheme, a coarse approximation of the image is coded in a base layer in digital ways, while the residual image details are delivered in an enhancement layer in analog-like way. The outputs from the two layers are superimposed for transmission, using a hybrid digital-analog modulation scheme. Since a major part of the signal is handled by the base layer, the power efficiency of the SoftCast layer is significantly improved. Experimental results show that the proposed scheme outperforms the original SoftCast remarkably, while still preserving the smooth quality degradation characteristic of the SoftCast scheme.\"",
        "1 is \"Image Denoising by Sparse 3-D Transform-Domain Collaborative Filtering\", 2 is \"Low-Complexity Encoder Framework For Window-Level Rate Control Optimization\".",
        "\nGiven above information, for an author who has written the paper with the title \"Power-Distortion Optimization For Wireless Image/Video Softcast By Transform Coefficients Energy Modeling With Adaptive Chunk Division\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0151": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Adaptive Lzma-Based Coding For Screen Content':",
        "title: \"Weighted Rate-Distortion Optimization for Screen Content Coding.\" with abstract: \"Unlike camera-captured video, screen content (SC) often contains a lot of repeating patterns, which makes some blocks used as references much more important than others. However, conventional rate-distortion optimization (RDO) schemes in video coding do not consider the dependence among image blocks, which often leads to a locally optimal parameter selection, especially for SC. In this paper, we p...\"",
        "title: \"Rate-distortion optimization with adaptive weighted distortion in high Efficiency Video Coding\" with abstract: \"This paper presents an adaptive weighted distortion optimization algorithm used in the Rate-Distortion Optimization (RDO) process of the High Efficiency Video Coding (HEVC). RDO is an important tool to improve the coding efficiency. Usually the distortion weights of different color components are equal or predetermined. In this paper, an adaptive weighted distortion optimization algorithm is introduced to improve the coding efficiency. The distortion weight is estimated according to the previous coded pictures belonging to the same temporal level, such that encoding complexity is almost unchanged. With the proposed adaptive weighted distortion optimization method, on average about 3.3% and up to 10.6% bit-saving are obtained based on the latest HEVC reference software, HM-8.0 and the corresponding common test conditions. The proposed algorithm can also be applied to other coding schemes such as H.264/MPEG-4 AVC.\"",
        "title: \"A Cross-Resolution Leaky Prediction Scheme for In-Band Wavelet Video Coding With Spatial Scalability\" with abstract: \"In most existing in-band wavelet video coding schemes, over-complete wavelet transform is used for the motion-compensated temporal filtering (MCTF) of each spatial subband. It can overcome the shift-variance of critical sampling wavelet transform and improve the coding efficiency of the in-band scheme. However, a dilemma exists in the current implementations of in-band MCTF (IBMCTF), which is whether or not to exploit the spatial highpass subbands in motion compensation of the spatial lowpass subband. The absence of the spatial highpass subbands will result in significant quality loss in the reconstructed full-resolution video, whereas the presence of the spatial highpass subbands may bring serious mismatch error in the decoded low-resolution video since the corresponding highpass subbands may be unavailable at the decoder. In this paper, we first analyze the mismatch error propagation in decoding the low-resolution video. Based on our analysis, we then propose a frame-based cross-resolution leaky prediction scheme for IBMCTF. It can make a good tradeoff between alleviating the low-resolution mismatch and improving the full-resolution coding efficiency. Experimental results show that the proposed scheme can dramatically reduce the mismatch error by 0.3-2.5 dB for low resolution, while the performance loss is marginal for high resolution.\"",
        "title: \"Directional filtering transform\" with abstract: \"This paper proposes the directional filtering transform (dFT, in order to distinguish from the common usage on DFT) to better exploit intra-frame correlation in H.264 intra-frame coding. It consists of a directional filtering and an optional DCT transform. In the proposed directional filtering, there are two different approaches. One is the uni-directional filtering (UDF) that is similar to H.264 directional intra prediction. In this approach, only samples from neighboring blocks can be used in prediction. Another is bidirectional filtering (BDF) that exploits the correlations among samples from not only neighboring blocks but also the current block. The prediction structure in this approach is hierarchical multi-layer. In this paper, we present mathematical analyses on UDF and BDF and show the advantage to combine them together. The proposed dFT is integrated into H.264 intra-frame coding too. The preliminary experimental results in H.264 demonstrate its superiority.\"",
        "title: \"Intra-Predictive Transforms for Block-Based Image Coding\" with abstract: \"This paper presents the theory and the design of intra-predictive transforms, which unify the inter-block prediction and block-based transforms in block-based image coding. Motivated by interpreting inter-block prediction as a transform with a larger size, we derive the concept of intra-predictive transforms. Conventional predictions and transforms can be viewed as special cases of intra-predictive transforms. Intra-predictive transforms are able to exploit both inter and intra-block correlations. We derive the tight upper bound of the coding gain of intra-predictive transforms for stationary Gaussian sources. It turns out that the coding gain can be greater than that of conventional transforms. The optimal intra-predictive transform that achieves the upper bound is also derived. We also design a practical intra-predictive transform using frequency-domain prediction that can achieve better performance in image coding while exhibiting low computational complexity. Experimental results confirm the effectiveness of the proposed intra-predictive transforms in block-based image coding systems and show the improvements over the current design.\"",
        "1 is \"Immune K-SVD algorithm for dictionary learning in speech denoising.\", 2 is \"A New Compressive Video Sensing Framework for Mobile Broadcast\".",
        "\nGiven above information, for an author who has written the paper with the title \"Adaptive Lzma-Based Coding For Screen Content\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0152": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'WaveCast: Wavelet based wireless video broadcast using lossy transmission':",
        "title: \"Wyner\u2013Ziv Switching Scheme for Multiple Bit-Rate Video Streaming\" with abstract: \"This paper proposes a Wyner-Ziv (WZ) switching scheme for multiple bit-rate (MBR) video streaming over networks. Identical video content is encoded into a set of normal streams, which are generated by conventional hybrid video coding with multiple bit rates, so that streaming can dynamically switch among these normal streams according to available bandwidth. At encoder side, the WZ codec generates a switching stream by compressing the reconstructed frames of a certain normal stream that will be switched to, no matter which normal stream it switches from. At decoder side, for switching to the same frame, the same WZ switching stream is used to reconstruct the switching-to frame by taking the switching-from frame as the side information. The number of required WZ bits depends on the inherent mutual correlation between two frames switching to and from. Since the WZ switching streams are generated independently of the normal switching-from streams, given normal streams that can switch from any one to another, the proposed scheme reduces the number of switching streams from to . Furthermore, switching streams do not deteriorate the coding efficiency of normal streams when no switching occurs. However a big problem here, similar to requesting bits in distributed video coding, is how many WZ bits should be transmitted when a switching happens because the streaming scenario does not tolerate too much extra delay caused by the requests back and forth. Therefore, a Laplacian model, which is proved in the simplified case, is proposed to characterize the correlation between switching-to and switching-from frames. It can be used to accurately estimate the number of WZ bits at the server side.\"",
        "title: \"Seamless rate adaptation for wireless networking\" with abstract: \"This paper aims at designing a Seamless Rate Adaptation for wireless networking which achieves smooth rate adjustment in a broad dynamic range of channel conditions. Conventional rate adaptation can only achieve a stair-case rate adjustment. Even when combining with hybrid ARQ, it suffers from an irreconcilable conflict between throughput and dynamic range. We tackle this problem from a new perspective by relying on modulation, instead of channel coding, for rate adaptation. We propose rate compatible modulation (RCM), in which modulation signals are incrementally generated from information bits through weighted mapping. Rate adaptation is achieved through varying the number of modulated signals. As more signals are transmitted, information bits gradually accumulate energy. The weights in bit-to-symbol mapping are delicately designed to ensure fine-grained energy accumulation so that smoothness and efficiency can both be achieved. We design and implement a rate adaptation system, called SRA and evaluate its performance through a software radio testbed. Results show that, under highly dynamic channel conditions, SRA achieves over 80% throughput gain over 802.11a adaptive modulation and coding, and achieves 28.8% and 43.8% gain over HARQ systems implemented with Turbo code and Raptor code. We believe that the concept of rate compatible modulation opens up a fresh research avenue toward the wireless rate adaptation problem.\"",
        "title: \"Design and Analysis of Compressive Data Persistence in Large-Scale Wireless Sensor Networks\" with abstract: \"This paper addresses the data persistence problem in wireless sensor networks (WSNs) where static sinks are not present and the sensed data have to be temporarily but resiliently stored in the network. Based on the observation that sensor readings are correlated, we propose compressive data persistence (CDP) scheme that makes use of the compressive sensing (CS) theory. Each sensor node independently computes and stores a random projection of the sensed data, such that a mobile sink can recover the data with high probability after visiting a small and random portion of the network. As a prerequisite of distributed CS encoding, sensor readings from all nodes are disseminated within the network through random walk. Therefore, the CS measurement matrix depends heavily on how the random walk is performed. In this paper, we present an in-depth analysis on the interplay between random walk parameters and sensing data characteristics, and derive the conditions in successful CS data recovery. In addition, we discover that there is a tradeoff between the number of random walk instances and steps in order to achieve the required data persistence performance. Experiments using real sensor data verify that the proposed CDP scheme achieves much lower decoding ratio than the state-of-theart Fountain code based schemes or the decentralized erasure codes based schemes, and demonstrate that there exist energyoptimized random walk parameters for CDP.\"",
        "title: \"Off-Line Motion Description For Fast Video Stream Generation In Mpeg-4 Avc/H.264\" with abstract: \"The rate-distortion optimal mode decision as well as motion estimation adopted in H.264 brings a big challenge to real-time encoding and transcoding duo to the high computation complexity. In this paper, we propose a hierarchical motion description mode I to present the motion data of each macroblock (MB) from coarsely to finely. A preprocessing approach is developed to estimate the motion data for each MB at each quality level with regard to its reference quality, its adjacent MBs and the target bit-rate. The resulting motion data can be coded and stored as metadata in a media file or a stream. Moreover, we propose a method to readily extract the specific motion data from the model for each MB at given bit-rates. Experimental results have shown the effectiveness of our proposed motion description model in terms of coding efficiency as well as fast bit-rate adaptation in comparison with that of H.264.\"",
        "title: \"Compressive cooperation for gaussian half-duplex relay channel.\" with abstract: \"Motivated by the compressive sensing (CS) theory and its close relationship with low-density parity-check code, we propose compressive transmission which utilizes CS as the channel code and directly transmits multi-level CS random projections through amplitude modulation. This article focuses on the compressive cooperation strategies in a relay channel. Four decode-and-forward (DF) strategies, namely receiver diversity, code diversity, successive decoding and concatenated decoding, are analyzed and their achievable rates in a three-terminal half-duplex Gaussian relay channel are quantified. The comparison among the four schemes is made through both numerical calculation and simulation experiments. In addition, we compare compressive cooperation with a separate source channel coding scheme for transmitting sparse sources. Results show that compressive cooperation has great potential in both transmission efficiency and its adaptation capability to channel variations.\"",
        "1 is \"Content adaptive prediction unit size decision algorithm for HEVC intra coding\", 2 is \"Characterization of signals from multiscale edges\".",
        "\nGiven above information, for an author who has written the paper with the title \"WaveCast: Wavelet based wireless video broadcast using lossy transmission\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0153": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'On network bandwidth sharing for transporting rate-adaptive packet video using feedback.':",
        "title: \"Variable Bit-Rate Video Transmission In The Broad-Band Isdn Environment\" with abstract: \"While the ATM-based broadband ISDN network gives the possibility to support source coding at the variable bit rate (VBR), it also benefits from the VBR coding. The advantages of VBR coding include consistent picture quality, bandwidth savings, and delay reduction. However, one of the major drawbacks of ATM networks is the cell loss and consequent aggravated picture degradation in case of VBR coding. Many compensative measures have been proposed recently to make the cell loss subjectively imperceptible. These schemes include the simple ARQ scheme, error concealment by command refreshment, appropriate queuing discipline and priority switching design, layered source coding schemes, and other signal processing techniques. This paper intends to summarize these efforts and, in particular, elaborate on different layered source coding schemes. Four types of signal priority classification schemes are identified: bit-plane separation (BPS), frequency-domain separation (FDS) combined bit-plane-frequency separation (CBFS), and feature plane separation (FPS). Different layered coding techniques are discussed and compared. Some open questions and recommendations are also given for further research and study.\"",
        "title: \"Network-Adaptive Rate Control With Tcp-Friendly Protocol For Multiple Video Objects\" with abstract: \"Delivering visual content over the Internet with acceptable quality is an important and challenging task. In order to maintain good quality of service (QoS) in the current best-effort Internet, bandwidth adaptation to the varying network conditions is desired for streaming video over the internet. In this paper, we propose a new rate control scheme for streaming MPEG4 video containing multiple video objects in conjunction with a TCP-friendly transport protocol that dynamically estimates available network bandwidth on the fly. Furthermore, network packet loss rate is taken into account in rate adaptation so that the end-to-end distortion is minimized. Simulation results demonstrate the effectiveness of our proposed scheme.\"",
        "title: \"Scalable video transport over wireless IP networks\" with abstract: \"There has been great interest in transporting real-time video over wireless IP networks from both industry and academia. Real-time video applications have quality-of-service (QoS) requirements. However, the fluctuations of wireless channel conditions pose many challenges to providing QoS for video transmission over wireless IP networks. It has been shown that scalable video coding and adaptive services are viable solutions under a time-varying wireless environment. We propose an adaptive framework to support quality video communication over wireless IP networks. The adaptive framework includes: (1) scalable video representations, (2) network-aware video applications, and (3) adaptive services. Under this framework, as wireless channel conditions change, the mobile terminal and network elements can scale the video streams and transport the scaled video streams to receivers with acceptable perceptual quality. The key advantages of the adaptive framework are: (1) perceptual quality is degraded gracefully under severe channel conditions; (2) network resources are efficiently utilized; and (3) the resources are shared in a fair manner\"",
        "title: \"Channel-Adaptive Unequal Error Protection For Scalable Video Transmission Over Wireless Channel\" with abstract: \"Scalable video delivery over wireless link is a very challenging task due to the time-varying characteristics of wireless channels. This paper proposes a channel-adaptive error control scheme for efficiently video delivery, which consists of dynamically channel estimation and channel-adaptive Unequal Error Protection (UEP). In our proposed channel-adaptive UEP scheme, a bit allocation algorithm is presented to periodically allocate the available bits among different video layers based on varying channel conditions so as to minimize the end-to-end distortion. Simulation results show that our proposed scheme is efficient under various channel conditions.\"",
        "title: \"On the compression of image based rendering scene\" with abstract: \"In image based rendering (IBR), a 3D scene is recorded through a set of photos, and a novel view is rendered by assembling data from the photo set. Compression is essential to reduce the huge data amount of IBR. We examine three categories of IBR compression algorithms: the block coder, the reference coder and the high dimensional transform (wavelet) coder. It is observed that the block coder consumes the least computation resource, however, its compression ratio is low. The reference coder achieves a good compression ratio with reasonable computation complexity. The high dimensional wavelet coder achieves the best compression ratio, however, it is also the most complex.\"",
        "1 is \"Congestion avoidance in computer networks with a connectionless network layer\", 2 is \"Statistics of video signals for viewphone-type pictures\".",
        "\nGiven above information, for an author who has written the paper with the title \"On network bandwidth sharing for transporting rate-adaptive packet video using feedback.\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0154": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of '1.2 V BiCMOS Sinh-Domain Filters':",
        "title: \"Simple Multi-Function Fractional-Order Filter Designs\" with abstract: \"A multi-function fractional-order filter topology, constructed from 2 MOS transistors only, is introduced in this paper. The required time-constants are realized through the utilization of the small-signal transconductance parameter of the MOS transistor, offering electronic adjustment of the frequency characteristics of the derived filters. The main attractive offered benefit, in comparison with the literature, is the significant reduction of MOS transistor count and, consequently, the reduced power dissipation. The behavior of the filters is evaluated using the Cadence software and MOS transistor models provided by the 0.35\u03bcm Austria Mikro Systeme CMOS process.\"",
        "title: \"Transient and Steady-State Response of a Fractional-Order Dynamic PV Model Under Different Loads.\" with abstract: \"In this paper, a fractional-order dynamic model of the photovoltaic (PV) solar module is introduced. Dynamic modeling of PV solar modules is useful when used in switching circuits and grid-connected situations. The dynamic elements of the proposed model are a fractional-order inductor and capacitor of two independent orders which allow for two extra degrees of freedom over the conventional dynamic model. The step response and transfer function of the load current are investigated for different orders under resistive and supercapacitor loading conditions. Closed-form expressions for the time response of the load current at equal orders of capacitor and inductor are derived. Stability analysis of the load current transfer function is carried out for different orders and loading conditions. The regions for pure real and pure imaginary input admittance scenarios are calculated numerically for both resistive and supercapacitor load cases. It is found that the order of the inductor has a dominant effect on the responses. As a proof of concept, the model is fitted to experimental data to show its flexibility in regenerating the actual response. The fitted fractional-order model response is compared to optimized integer-order ones from literature showing noticeable improvement.\"",
        "title: \"Design of square-root domain filters by substituting the passive elements of the prototype filter by their equivalents\" with abstract: \"A novel technique for designing square-root domain (SRD) filters is introduced in this paper. The concept of the proposed method is based on the substitution of the passive elements of the corresponding prototype filter by their SRD equivalents. The signal processing performed by the proposed SRD equivalents achieves that the voltage at each terminal of the SRD equivalent is the compressed version of the voltage at the corresponding terminal of the passive element, and that the current that flows through the SRD equivalent is the same as that flows through the passive element. The main attractive characteristic of the proposed method is that a quick procedure for designing SRD filters is offered. The validity of the proposed technique was verified by studying the behaviour of a 5th-order SRD low-pass filter. In order to demonstrate the benefits offered by the proposed technique, a SRD leapfrog filter was also designed and its performance is compared with that of the active filter that topologically simulates the same prototype filter. Copyright \u00a9 2007 John Wiley & Sons, Ltd.\"",
        "title: \"Log-domain wave filters\" with abstract: \"A systematic method for designing log-domain wave filters is presented. Wave filters simulate topologically and functionally passive doubly terminated LC ladder prototype filters of low sensitivity. The design in the log-domain is based on a transposition of the signal flow graph (SFG) that corresponds to the wave equivalent of elementary two-port blocks in the linear domain, to the corresponding log-domain SFG. This is achieved by using an appropriate set of complementary operators, in order to preserve the linear operation of the whole circuit. Simulation results of a fifth-order low-pass and a fourth-order bandpass log-domain wave filter are given, using HSPICE. The proposed circuits are suitable for low-voltage operation and in high-frequency applications.\"",
        "title: \"Electronically controlled multiphase sinusoidal oscillators using current amplifiers\" with abstract: \"A novel current-mode multiphase oscillator topology is introduced in this letter. This is realized by employing current amplifiers and only grounded capacitors. Attractive characteristics offered by the new topology are the electronic adjustment of the oscillation frequency, the absence of passive resistors, and the requirement of only grounded capacitors. Comparison with the corresponding already published current follower based structure shows that the proposed topology has better performance in terms of the number of required active elements, the employment of passive resistors, and the ability for electronic adjustment of the oscillation frequency. Copyright \u00a9 2008 John Wiley & Sons, Ltd.\"",
        "1 is \"Graphics recognition - from re-engineering to retrieval\", 2 is \"Diffusion process modeling by using fractional-order models.\".",
        "\nGiven above information, for an author who has written the paper with the title \"1.2 V BiCMOS Sinh-Domain Filters\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0155": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'CMC: a pragmatic approach to model checking real code':",
        "title: \"Model checking large network protocol implementations\" with abstract: \"Network protocols must work. The effects of protocol specification or implementation errors range from reduced performance, to security breaches, to bringing down entire networks. However, network protocols are difficult to test due to the exponential size of the state space they define. Ideally, a protocol implementation must be validated against all possible events (packet arrivals, packet losses, timeouts, etc.) in all possible protocol states. Conventional means of testing can explore only a minute fraction of these possible combinations. This paper focuses on how to effectively find errors in large network protocol implementations using model checking, a formal verification technique. Model checking involves a systematic exploration of the possible states of a system, and is well-suited to finding intricate errors lurking deep in exponential state spaces. Its primary limitation has been the effort needed to use it on software. The primary contribution of this paper are novel techniques that allow us to model check complex, real-world, well-tested protocol implementations with reasonable effort. We have implemented these techniques in CMC, a C model checker [30] and applied the result to the Linux TCP/IP implementation, finding four errors in the protocol implementation.\"",
        "title: \"Model checking system software with CMC\" with abstract: \"Complex systems have errors that involve mishandled corner cases in intricate sequences of events. Conventional testing techniques usually miss these errors. In recent years, formal verification techniques such as [5] have gained popularity in checking a property in all possible behaviors of a system. However, such techniques involve generating an abstract model of the system. Such an abstraction process is unreliable, difficult and miss a lot of implementation errors.CMC is a framework for model checking a broad class of software written in the C programming language. CMC runs the software implementation directly without deriving an abstract model of the code. We used CMC to model check an existing implementation of AODV (Ad Hoc On Demand Distance Vector) routing protocol and found a total of 29 bugs in two implementations [7],[6] of the protocol. One of them is a bug in the actual specification of the AODV protocol [3]. We also used CMC on the IP Fragmentation module in the Linux TCP/IPv4 stack and verified its correctness for up to 4 fragments per packet.\"",
        "title: \"RWset: attacking path explosion in constraint-based test generation\" with abstract: \"Recent work has used variations of symbolic execution to automatically generate high-coverage test inputs [3, 4, 7, 8, 14]. Such tools have demonstrated their ability to find very subtle errors. However, one challenge they all face is how to effectively handle the exponential number of paths in checked code. This paper presents a new technique for reducing the number of traversed code paths by discarding those that must have side-effects identical to some previously explored path. Our results on a mix of open source applications and device drivers show that this (sound) optimization reduces the numbers of paths traversed by several orders of magnitude, often achieving program coverage far out of reach for a standard constraint-based execution system.\"",
        "title: \"Static Analysis versus Software Model Checking for Bug Finding\" with abstract: \"Abstract: This paper describes experiences with software model checking after several years of using static analysis to find errors. We initially thought that the trade-off between the two was clear: static analysis was easy but would mainly find shallow bugs, while model checking would require more work but would be strictly better - it would find more errors, the errors would be deeper, and the approach would be more powerful. These expectations were often wrong.\"",
        "title: \"Static analysis versus model checking for bug finding\" with abstract: \"This talk tries to distill several years of experience using both model checking and static analysis to find errors in large software systems. We initially thought that the tradeoffs between the two was clear: static analysis was easy but would mainly find shallow bugs, while model checking would require more work but would be strictly better -- it would find more errors, the errors would be deeper and the approach would be more powerful. These expectations were often wrong. This talk will describe some of the sharper tradeoffs between the two, as well as a detailed discussion of one domain -- finding errors in file systems code -- where model checking seems to work very well.\"",
        "1 is \"Auto-parallelizing stateful distributed streaming applications\", 2 is \"Factor graphs and the sum-product algorithm\".",
        "\nGiven above information, for an author who has written the paper with the title \"CMC: a pragmatic approach to model checking real code\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0156": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'A rule engine for relevance assessment in a contextualized information delivery system':",
        "title: \"On the relationship between workflow models and document types\" with abstract: \"The best practice in information system development is to model the business processes that have to be supported and the database of the information system separately. This is inefficient because they are closely related. Therefore we present a framework in which it is possible to derive one from the other. To this end we introduce a special class of Petri nets, called Jackson nets, to model the business processes, and a document type, called Jackson types, to model the database. We show that there is a one-to-one correspondence between Jackson nets and Jackson types. We illustrate the use of the framework by an example.\"",
        "title: \"Towards a Calculus for Collection-Oriented Scientific Workflows with Side Effects\" with abstract: \"In this paper we propose a calculus that can be used to describe the semantics of collection-oriented scientific workflow systems such as the Taverna workbench. Typically such systems focus on the specification and execution of workflows with a relatively simple control flow and a more complex data flow that involves large nested collections of data. An essential operation in such workflows is the instantiation of a certain nested workflow for each element of a collection. We argue that if such workflows call external services, their semantics must be described not only in terms of input-output behavior but also take side effects into account. Based on this assumption a trace semantics is defined that corresponds to the observational equivalence of two workflow specifications. We show that under such a semantics a relatively small calculus with a structural semantics can be defined and used to describe such workflows. This is demonstrated by giving a translation of Taverna workflows in terms of this calculus.\"",
        "title: \"PG-Keys: Keys for Property Graphs\" with abstract: \"ABSTRACTWe report on a community effort between industry and academia to shape the future of property graph constraints. The standardization for a property graph query language is currently underway through the ISO Graph Query Language (GQL) project. Our position is that this project should pay close attention to schemas and constraints, and should focus next on key constraints. The main purposes of keys are enforcing data integrity and allowing the referencing and identifying of objects. Motivated by use cases from our industry partners, we argue that key constraints should be able to have different modes, which are combinations of basic restriction that require the key to be exclusive, mandatory, and singleton. Moreover, keys should be applicable to nodes, edges, and properties since these all can represent valid real-life entities. Our result is PG-Keys, a flexible and powerful framework for defining key constraints, which fulfills the above goals. PG-Keys is a design by the Linked Data Benchmark Council's Property Graph Schema Working Group, consisting of members from industry, academia, and ISO GQL standards group, intending to bring the best of all worlds to property graph practitioners. PG-Keys aims to guide the evolution of the standardization efforts towards making systems more useful, powerful, and expressive.\"",
        "title: \"Report from the first workshop on scalable workflow enactment engines and technology (SWEET'12)\" with abstract: \"This report summarizes the presentations and discussions of SWEET 2012, the First InternationalWorkshop on ScalableWorkflow Enactment Engines and Technologies. SWEET was held in conjunction with the 2012 SIGMOD conference in Scottsdale, Arizona, USA on May 20th, 2012. The goal of the workshop was to bring together researchers and practitioners to explore the state of the art in workflow-based programming for data-intensive applications, and the potential of cloud-based computing in this area. The program featured two very well attended invited talks by Pawel Garbacki from Google and Jimmy Lin from the University of Maryland, on leave at Twitter at the time, as well as a tutorial on Oozie, Yahoo's workflow engine based on Hadoop, by Mohammad Islam from Yahoo/Cloudera.\"",
        "title: \"On generating *-sound nets with substitution\" with abstract: \"We present a method for hierarchically generating sound workflow nets by substitution of nets with multiple inputs and outputs. We show that this method is correct and generalizes the class of nets generated by other hierarchical approaches. The method involves a new notion of soundness which is preserved by the generalized type of substitution that is presented in this paper. We show that this notion is better suited than @?-soundness for use with the presented type of generalized substitution, since @?-soundness is not preserved by it. It is moreover shown that it is in some sense the optimal notion of soundness for the purpose of generating sound nets by the presented type of substitution.\"",
        "1 is \"What is Twitter, a social network or a news media?\", 2 is \"A probabilistic computational model of cross-situational word learning.\".",
        "\nGiven above information, for an author who has written the paper with the title \"A rule engine for relevance assessment in a contextualized information delivery system\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0157": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Music2Share - Copyright-Compliant Music Sharing in P2P Systems':",
        "title: \"When Game Becomes Life: The Creators and Spectators of Online Game Replays and Live Streaming.\" with abstract: \"Online gaming franchises such as World of Tanks, Defense of the Ancients, and StarCraft have attracted hundreds of millions of users who, apart from playing the game, also socialize with each other through gaming and viewing gamecasts. As a form of User Generated Content (UGC), gamecasts play an important role in user entertainment and gamer education. They deserve the attention of both industrial partners and the academic communities, corresponding to the large amount of revenue involved and the interesting research problems associated with UGC sites and social networks. Although previous work has put much effort into analyzing general UGC sites such as YouTube, relatively little is known about the gamecast sharing sites. In this work, we provide the first comprehensive study of gamecast sharing sites, including commercial streaming-based sites such as Amazon\u2019s Twitch.tv and community-maintained replay-based sites such as WoTreplays. We collect and share a novel dataset on WoTreplays that includes more than 380,000 game replays, shared by more than 60,000 creators with more than 1.9 million gamers. Together with an earlier published dataset on Twitch.tv, we investigate basic characteristics of gamecast sharing sites, and we analyze the activities of their creators and spectators. Among our results, we find that (i) WoTreplays and Twitch.tv are both fast-consumed repositories, with millions of gamecasts being uploaded, viewed, and soon forgotten; (ii) both the gamecasts and the creators exhibit highly skewed popularity, with a significant heavy tail phenomenon; and (iii) the upload and download preferences of creators and spectators are different: while the creators emphasize their individual skills, the spectators appreciate team-wise tactics. Our findings provide important knowledge for infrastructure and service improvement, for example, in the design of proper resource allocation mechanisms that consider future gamecasting and in the tuning of incentive policies that further help player retention.\"",
        "title: \"Reducing the history in decentralized interaction-based reputation systems\" with abstract: \"In decentralized interaction-based reputation systems, nodes store information about the past interactions of other nodes. Based on this information, they compute reputations in order to take decisions about future interactions. Computing the reputations with the complete history of interactions is inefficient due to its resource requirements. Furthermore, the complete history of interactions accumulates old information, which may impede the nodes from capturing the dynamic behavior of the system when computing reputations. In this paper, we propose a scheme for reducing the amount of history maintained in decentralized interaction-based reputation systems based on elements such as the age of nodes, and we explore its effect on the computed reputations showing its effectiveness in both synthetic and real-world graphs.\"",
        "title: \"Correlating Topology and Path Characteristics of Overlay Networks and the Internet\" with abstract: \"Real-world IP applications such as peer-to-peer file-sharing are now able to benefit from network and location awareness. It is therefore crucial to understand the relation between underlay and overlay networks and to characterize the behavior of real users with regard to the Internet. For this purpose, we have designed and implemented MULTI-PROBE, a framework for large-scale P2P file-sharing measurements. Using this framework, we have performed measurements of BitTorrent, which is currently the P2P file sharing network with the largest amount of Internet traffic. We analyze and correlate these measurements to provide new insights into the topology, the connectivity, and the path characteristics of the Internet parts underlying P2P networks, as well as to present unique information on the BitTorrent throughput and connectivity\"",
        "title: \"An Empirical Performance Evaluation of GPU-Enabled Graph-Processing Systems\" with abstract: \"Graph processing is increasingly used in knowledge economies and in science, in advanced marketing, social networking, bioinformatics, etc. A number of graph-processing systems, including the GPU-enabled Medusa and Totem, have been developed recently. Understanding their performance is key to system selection, tuning, and improvement. Previous performance evaluation studies have been conducted for CPU-based graph-processing systems, such as Graph and GraphX. Unlike them, the performance of GPU-enabled systems is still not thoroughly evaluated and compared. To address this gap, we propose an empirical method for evaluating GPU-enabled graph-processing systems, which includes new performance metrics and a selection of new datasets and algorithms. By selecting 9 diverse graphs and 3 typical graph-processing algorithms, we conduct a comparative performance study of 3 GPU-enabled systems, Medusa, Totem, and MapGraph. We present the first comprehensive evaluation of GPU-enabled systems with results giving insight into raw processing power, performance breakdown into core components, scalability, and the impact on performance of system-specific optimization techniques and of the GPU generation. We present and discuss many findings that would benefit users and developers interested in GPU acceleration for graph processing.\"",
        "title: \"The distributed ASCI Supercomputer project\" with abstract: \"The Distributed ASCI Supercomputer (DAS) is a homogeneous wide-area distributed system consisting of four cluster computers at different locations. DAS has been used for research on communication software, parallel languages and programming systems, schedulers, parallel applications, and distributed applications. The paper gives a preview of the most interesting research results obtained so far in the DAS project.\"",
        "1 is \"A Model for Usage Policy-Based Resource Allocation in Grids\", 2 is \"FIRE: FInding Rogue nEtworks\".",
        "\nGiven above information, for an author who has written the paper with the title \"Music2Share - Copyright-Compliant Music Sharing in P2P Systems\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0158": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'HPS-HDS: High Performance Scheduling for Heterogeneous Distributed Systems.':",
        "title: \"When Game Becomes Life: The Creators and Spectators of Online Game Replays and Live Streaming.\" with abstract: \"Online gaming franchises such as World of Tanks, Defense of the Ancients, and StarCraft have attracted hundreds of millions of users who, apart from playing the game, also socialize with each other through gaming and viewing gamecasts. As a form of User Generated Content (UGC), gamecasts play an important role in user entertainment and gamer education. They deserve the attention of both industrial partners and the academic communities, corresponding to the large amount of revenue involved and the interesting research problems associated with UGC sites and social networks. Although previous work has put much effort into analyzing general UGC sites such as YouTube, relatively little is known about the gamecast sharing sites. In this work, we provide the first comprehensive study of gamecast sharing sites, including commercial streaming-based sites such as Amazon\u2019s Twitch.tv and community-maintained replay-based sites such as WoTreplays. We collect and share a novel dataset on WoTreplays that includes more than 380,000 game replays, shared by more than 60,000 creators with more than 1.9 million gamers. Together with an earlier published dataset on Twitch.tv, we investigate basic characteristics of gamecast sharing sites, and we analyze the activities of their creators and spectators. Among our results, we find that (i) WoTreplays and Twitch.tv are both fast-consumed repositories, with millions of gamecasts being uploaded, viewed, and soon forgotten; (ii) both the gamecasts and the creators exhibit highly skewed popularity, with a significant heavy tail phenomenon; and (iii) the upload and download preferences of creators and spectators are different: while the creators emphasize their individual skills, the spectators appreciate team-wise tactics. Our findings provide important knowledge for infrastructure and service improvement, for example, in the design of proper resource allocation mechanisms that consider future gamecasting and in the tuning of incentive policies that further help player retention.\"",
        "title: \"The Failure Trace Archive: Enabling Comparative Analysis of Failures in Diverse Distributed Systems\" with abstract: \"With the increasing functionality and complexity of distributed systems, resource failures are inevitable. While numerous models and algorithms for dealing with failures exist, the lack of public trace data sets and tools has prevented meaningful comparisons. To facilitate the design, validation, and comparison of fault-tolerant models and algorithms, we have created the Failure Trace Archive (FTA) as an online public repository of availability traces taken from diverse parallel and distributed systems. Our main contributions in this study are the following. First, we describe the design of the archive, in particular the rationale of the standard FTA format, and the design of a toolbox that facilitates automated analysis of trace data sets. Second, applying the toolbox, we present a uniform comparative analysis with statistics and models of failures in nine distributed systems. Third, we show how different interpretations of these data sets can result in different conclusions. This emphasizes the critical need for the public availability of trace data and methods for their analysis.\"",
        "title: \"Analyzing Implicit Social Networks In Multiplayer Online Games\" with abstract: \"Understanding the social structures that people implicitly form when playing networked games helps developers create innovative gaming services to benefit both players and operators. But how can we extract and analyze this implicit social structure? The authors' proposed formalism suggests various ways to map interactions to social structure. Applying this formalism to real-world data collected from three game genres reveals the implications of the mappings on in-game and gaming-related services, ranging from network and socially aware player matchmaking to an investigation of social network robustness against player departure.\"",
        "title: \"How are Real Grids Used? The Analysis of Four Grid Traces and Its Implications\" with abstract: \"The Grid computing vision promises to provide the needed platform for a new and more demanding range of applications. For this promise to become true, a number of hurdles, including the design and deployment of adequate resource management and information services, need to be overcome. In this context, understanding the characteristics of real Grid workloads is a crucial step for improving the quality of existing Grid services, and in guiding the design of new solutions. Towards this goal, in this work we present the characteristics of traces of four real Grid environments, namely LCG, Grid3, and TeraGrid, which are among the largest production Grids currently deployed, and the DAS, which is a research Grid. We focus our analysis on virtual organizations, on users, and on individual jobs characteristics. We further attempt to quantify the evolution and the performance of the Grid systems from which our traces originate. Finally, given the scarcity of the information available for analysis purposes, we discuss the requirements of a new format for Grid traces, and we propose the establishment of a virtual center for workload-based Grid benchmarking data: The Grid Workloads Archive.\"",
        "title: \"A model for space-correlated failures in large-scale distributed systems\" with abstract: \"Distributed systems such as grids, peer-to-peer systems, and even Internet DNS servers have grown significantly in size and complexity in the last decade. This rapid growth has allowed distributed systems to serve a large and increasing number of users, but has also made resource and system failures inevitable. Moreover, perhaps as a result of system complexity, in distributed systems a single failure can trigger within a short time span several more failures, forming a group of time-correlated failures. To eliminate or alleviate the significant effects of failures on performance and functionality, the techniques for dealing with failures require good failure models. However, not many such models are available, and the available models are valid for few or even a single distributed system. In contrast, in this work we propose a model that considers groups of time-correlated failures and is valid for many types of distributed systems. Our model includes three components, the group size, the group inter-arrival time, and the resource downtime caused by the group. To validate this model, we use failure traces corresponding to fifteen distributed systems. We find that space-correlated failures are dominant in terms of resource downtime in seven of the fifteen studied systems. For each of these seven systems, we provide a set of model parameters that can be used in research studies or for tuning distributed systems. Last, as a result of our work six of the studied traces have been made available through the Failure Trace Archive (http://fta.inria.fr).\"",
        "1 is \"Who plays, how much, and why? Debunking the stereotypical gamer profile\", 2 is \"Reality mining: sensing complex social systems\".",
        "\nGiven above information, for an author who has written the paper with the title \"HPS-HDS: High Performance Scheduling for Heterogeneous Distributed Systems.\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0159": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Community Detection In Multi-Relational Social Networks':",
        "title: \"hPSD: A Hybrid PU-Learning-Based Spammer Detection Model for Product Reviews.\" with abstract: \"Spammers, who manipulate online reviews to promote or suppress products, are flooding in online commerce. To combat this trend, there has been a great deal of research focused on detecting review spammers, most of which design diversified features and thus develop various classifiers. The widespread growth of crowdsourcing platforms has created large-scale deceptive review writers who behave more like normal users, that the way they can more easily evade detection by the classifiers that are purely based on fixed characteristics. In this paper, we propose a hybrid semisupervised learning model titled hybrid PU-learning-based spammer detection (hPSD) for spammer detection to leverage both the users\u2019 characteristics and the user\u2013product relations. Specifically, the hPSD model can iteratively detect multitype spammers by injecting different positive samples, and allows the construction of classifiers in a semisupervised hybrid learning framework. Comprehensive experiments on movie dataset with shilling injection confirm the superior performance of hPSD over existing baseline methods. The hPSD is then utilized to detect the hidden spammers from real-life Amazon data. A set of spammers and their underlying employers (e.g., book publishers) are successfully discovered and validated. These demonstrate that hPSD meets the real-world application scenarios and can thus effectively detect the potentially deceptive review writers.\"",
        "title: \"Hybrid Collaborative Filtering algorithm for bidirectional Web service recommendation.\" with abstract: \"Web service recommendation has become a hot yet fundamental research topic in service computing. The most popular technique is the Collaborative Filtering (CF) based on a user-item matrix. However, it cannot well capture the relationship between Web services and providers. To address this issue, we first design a cube model to explicitly describe the relationship among providers, consumers and Web services. And then, we present a Standard Deviation based Hybrid Collaborative Filtering (SD-HCF) for Web Service Recommendation (WSRec) and an Inverse consumer Frequency based User Collaborative Filtering (IF-UCF) for Potential Consumers Recommendation (PCRec). Finally, the decision-making process of bidirectional recommendation is provided for both providers and consumers. Sets of experiments are conducted on real-world data provided by Planet-Lab. In the experiment phase, we show how the parameters of SD-HCF impact on the prediction quality as well as demonstrate that the SD-HCF is much better than extant methods on recommendation quality, including the CF based on user, the CF based on item and general HCF. Experimental comparison between IF-UCF and UCF indicates the effectiveness of adding inverse consumer frequency to UCF. \u00a9 2012 Springer-Verlag London.\"",
        "title: \"CAMAS: A cluster-aware multiagent system for attributed graph clustering.\" with abstract: \"Abstract   Attributed graphs describe nodes via attribute vectors and also relationships between different nodes via edges. To partition nodes into clusters with tighter correlations, an effective way is applying clustering techniques on attributed graphs based on various criteria such as node connectivity and/or attribute similarity. Even though clusters typically form around nodes with tight edges and similar attributes, existing methods have only focused on one of these two data modalities. In this paper, we comprehend each node as an autonomous agent and develop an accurate and scalable multiagent system for extracting overlapping clusters in attributed graphs. First, a kernel function with a tunable bandwidth factor  \u03b4  is introduced to measure the influence of each agent, and those agents with highest local influence can be viewed as the \u201cleader\u201d agents. Then, a novel local expansion strategy is proposed, which can be applied by each leader agent to absorb the most relevant followers in the graph. Finally, we design the cluster-aware multiagent system (CAMAS), in which agents communicate with each other freely under an efficient communication mechanism. Using the proposed multiagent system, we are able to uncover the optimal overlapping cluster configuration, i.e. nodes within one cluster are not only connected closely with each other but also with similar attributes. Our method is highly efficient, and the computational time is shown that nearly linearly dependent on the number of edges when  \u03b4  \u2208 [0.5, 1). Finally, applications of the proposed method on a variety of synthetic benchmark graphs and real-life attributed graphs are demonstrated to verify the systematic performance.\"",
        "title: \"A generalized game theoretic framework for mining communities in complex networks.\" with abstract: \"\u2022We introduce a game theoretic framework to model community formation in networks.\u2022We deduce many commonly used objective functions as potential functions in the model.\u2022The proposed framework has been proved to exist fixed (equilibrium) point.\u2022A synchronous learning mechanism has be proposed to reach the local equilibrium.\"",
        "title: \"Dynamic advance reservation for grid system using resource pools\" with abstract: \"Dynamic behavior of resources is a non-negligible feature in grid system, and most research efforts on advance reservation cannot effectively deal with the negative effect resulted from the dynamic feature. In this paper, a new grid system architecture using resource pool is proposed firstly. Theoretical analysis demonstrates that resource pool can well adapt to dynamic behavior of resources. Secondly, Quality of Service (QoS) distance computation method for hybrid variable types is presented. Then, k-set Availability Prediction Admission Control (kAPAC) algorithm is described in detail. Experimental results show that kAPAC can significantly increase success ratio of reservation, resource utilization and stability of grid system.\"",
        "1 is \"A novel and better fitness evaluation for rough set based minimum attribute reduction problem\", 2 is \"Exploring the Capabilities of Mobile Agents in Distributed Data Mining\".",
        "\nGiven above information, for an author who has written the paper with the title \"Community Detection In Multi-Relational Social Networks\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0160": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Mechanisms and Evaluation of Cross-Layer Fault-Tolerance for Supercomputing':",
        "title: \"Analyzing Behavior Specialized Acceleration.\" with abstract: \"Hardware specialization has become a promising paradigm for overcoming the inefficiencies of general purpose microprocessors. Of significant interest are Behavioral Specialized Accelerators (BSAs), which are designed to efficiently execute code with only certain properties, but remain largely configurable or programmable. The most important strength of BSAs -- their ability to target a wide variety of codes -- also makes their interactions and analysis complex, raising the following questions: can multiple BSAs be composed synergistically, what are their interactions with the general purpose core, and what combinations favor which workloads? From a methodological standpoint, BSAs are also challenging, as they each require ISA development, compiler and assembler extensions, and either simulator or RTL models.   To study the potential of BSAs, we propose a novel modeling technique called the Transformable Dependence Graph (TDG) - a higher level alternative to the time-consuming traditional compiler+simulator approach, while still enabling detailed microarchitectural models for both general cores and accelerators. We then propose a multi-BSA organization, called ExoCore, which we model and study using the TDG. A design space exploration reveals that an ExoCore organization can push designs beyond the established energy-performance frontiers for general purpose cores. For example, a 2-wide OOO processor with three BSAs matches the performance of a conventional 6-wide OOO core, has 40% lower area, and is 2.6x more energy efficient.\"",
        "title: \"Universal Mechanisms for Data-Parallel Architectures\" with abstract: \"Data-parallel programs are both growing in importanceand increasing in diversity, resulting in specialized processorstargeted at specific classes of these programs. This paperpresents a classification scheme for data-parallelprogram attributes, and proposes micro-architecturalmechanisms to support applications with diverse behaviorusing a single reconfigurable architecture. We focuson the following four broad kinds of data-parallel programs- DSP/multimedia, scientific, networking, andreal-time graphics workloads. While all of these programsexhibit high computational intensity, coarse-grainregular control behavior, and some regular memory accessbehavior, they show wide variance in the computationrequirements, fine grain control behavior, and the frequencyof other types of memory accesses. Based onthis study of application attributes, this paper proposesa set of general micro-architectural mechanismsthat enable a baseline architecture to be dynamically tailoredto the demands of a particular application. Thesemechanisms provide efficient execution across a spectrumof data-parallel applications and can be applied todiverse architectures ranging from vector cores to conventionalsuperscalar cores. Our results using a baselineTRIPS processor show that the configurability of the architectureto the application demands provides harmonicmean performance improvement of 5%-55% over scalableyet less flexible architectures, and performs competitivelyagainst specialized architectures.\"",
        "title: \"Implementing Signatures for Transactional Memory\" with abstract: \"Transactional Memory (TM) systems must track the read and write sets--items read and written during a transaction--to detect conflicts among concurrent trans- actions. Several TMs use signatures, which summarize unbounded read/write sets in bounded hardware at a per- formance cost of false positives (conflicts detected when none exists). This paper examines different organizations to achieve hardware-efficient and accurate TM signatures. First, we find that implementing each signature with a single k-hash- function Bloom filter (True Bloom signature) is inefficient, as it requires multi-ported SRAMs. Instead, we advocate using k single-hash-function Bloom filters in parallel (Par- allel Bloom signature), using area-efficient single-ported SRAMs. Our formal analysis shows that both organiza- tions perform equally well in theory and our simulation- based evaluation shows this to hold approximately in prac- tice. We also show that by choosing high-quality hash func- tions we can achieve signature designs noticeably more ac- curate than the previously proposed implementations. Fi- nally, we adapt Pagh and Rodler's cuckoo hashing to im- plement Cuckoo-Bloom signatures. While this representa- tion does not support set intersection, it mitigates false pos- itives for the common case of small read/write sets and per- forms like a Bloom filter for large sets.\"",
        "title: \"A general constraint-centric scheduling framework for spatial architectures\" with abstract: \"Specialized execution using spatial architectures provides energy efficient computation, but requires effective algorithms for spatially scheduling the computation. Generally, this has been solved with architecture-specific heuristics, an approach which suffers from poor compiler/architect productivity, lack of insight on optimality, and inhibits migration of techniques between architectures. Our goal is to develop a scheduling framework usable for all spatial architectures. To this end, we expresses spatial scheduling as a constraint satisfaction problem using Integer Linear Programming (ILP). We observe that architecture primitives and scheduler responsibilities can be related through five abstractions: placement of computation, routing of data, managing event timing, managing resource utilization, and forming the optimization objectives. We encode these responsibilities as 20 general ILP constraints, which are used to create schedulers for the disparate TRIPS, DySER, and PLUG architectures. Our results show that a general declarative approach using ILP is implementable, practical, and typically matches or outperforms specialized schedulers.\"",
        "title: \"Performance evaluation of a DySER FPGA prototype system spanning the compiler, microarchitecture, and hardware implementation\" with abstract: \"Specialization and accelerators are being proposed as an effective way to address the slowdown of Dennard scaling. DySER is one such accelerator, which dynamically synthesizes large compound functional units to match program regions, using a co-designed compiler and microarchitecture. We have completed a full prototype implementation of DySER integrated into the OpenSPARC processor (called SPARC-DySER), a co-designed compiler in LLVM, and a detailed performance evaluation on an FPGA system, which runs an Ubuntu Linux distribution and full applications. Through the prototype, this paper evaluates the fundamental principles of DySER acceleration. Our two key findings are: i) the DySER execution model and microarchitecture provides energy efficient speedups and the integration of DySER does not introduce overheads ??? overall, DySER???s performance improvement to OpenSPARC is 6X, consuming only 200mW ; ii) on the compiler side, the DySER compiler is effective at extracting computationally intensive regular and irregular code. For non-computationally intense irregular code, two control flow shapes curtail the compiler???s effectiveness, and we identify potential adaptive mechanisms. Finally, our experience of bringing up an end-to-end prototype of an ISA-exposed accelerator has made clear that two particular artifacts are greatly needed to perform this type of design more quickly and effectively: 1) Open-source implementations of high-performance baseline processors, and 2) Declarative tools for quickly specifying combinations of known compiler transforms.\"",
        "1 is \"Accountable internet protocol (aip)\", 2 is \"An Algebra for Cross-Experiment Performance Analysis\".",
        "\nGiven above information, for an author who has written the paper with the title \"Mechanisms and Evaluation of Cross-Layer Fault-Tolerance for Supercomputing\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0161": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'On the Outage Probability of Device-to-Device-Communication-Enabled Multichannel Cellular Networks: An RSS-Threshold-Based Perspective':",
        "title: \"Learning bundle manifold by double neighborhood graphs\" with abstract: \"In this paper, instead of the ordinary manifold assumption, we introduced the bundle manifold assumption that imagines data points lie on a bundle manifold Under this assumption, we proposed an unsupervised algorithm, named as Bundle Manifold Embedding (BME), to embed the bundle manifold into low dimensional space In BME, we construct two neighborhood graphs that one is used to model the global metric structure in local neighborhood and the other is used to provide the information of subtle structure, and then apply the spectral graph method to obtain the low-dimensional embedding Incorporating some prior information, it is possible to find the subtle structures on bundle manifold in an unsupervised manner Experiments conducted on benchmark datasets demonstrated the feasibility of the proposed BME algorithm, and the difference compared with ISOMAP, LLE and Laplacian Eigenmaps.\"",
        "title: \"2D projective transformation based active shape model for facial feature location.\" with abstract: \"Active shape model statistically represents a shape by a set of well-defined landmark points and can model object variations using principal component analysis. However, the face modeling becomes degenerate when the test images are taken from different viewpoints other than frontal. In this paper, we present a 2D projective transformation based active shape model to address this problem. First, a 2D projective transformation based alignment algorithm is proposed to model pose variations. Then, projective local profiles are calculated, which make the intensity profiles be scale independent. We evaluate our approach on two different datasets containing both frontal facial images and images with a large range of variations in pose and expression. Experimental results demonstrate the efficiency and effectiveness of the proposed approach. \u00a9 2011 IEEE.\"",
        "title: \"Emulating biological strategies for uncontrolled face recognition\" with abstract: \"Face recognition technology is of great significance for applications involving national security and crime prevention. Despite enormous progress in this field, machine-based system is still far from the goal of matching the versatility and reliability of human face recognition. In this paper, we show that a simple system designed by emulating biological strategies of human visual system can largely surpass the state-of-the-art performance on uncontrolled face recognition. In particular, the proposed system integrates dual retinal texture and color features for face representation, an incremental robust discriminant model for high level face coding, and a hierarchical cue-fusion method for similarity qualification. We demonstrate the strength of the system on the large-scale face verification task following the evaluation protocol of the Face Recognition Grand Challenge (FRGC) version 2 Experiment 4. The results are surprisingly well: Its modules significantly outperform their state-of-the-art counterparts, such as Gabor image representation, local binary patterns, and enhanced Fisher linear discriminant model. Furthermore, applying the integrated system to the FRGC version 2 Experiment 4, the verification rate at the false acceptance rate of 0.1 percent reaches to 93.12 percent.\"",
        "title: \"Statistical Color Model Based Adult Video Filter\" with abstract: \"This paper, guided by Statistical Color Models, proposes a real-time Adult Video detector to filter the adult content in the video. A generic color model is constructed by statistical analysis of the sample images containing adult pixels. We fully utilize the video continuity characteristics, i.e. preceding and following N frames considered in the classification. Our method, through experimental, displays a satisfactory performance for detecting adult content. The reminder of the paper addresses the application of real-time adult video filter that blocks adult content from kids.\"",
        "title: \"Support Vector Machines Optimization Based Wavelet Transform Liner Equalizer and Decision Directed Combined Blind Equalization Algorithm.\" with abstract: \"Aiming at the defects of time varying and multipath fading in underwater acoustic channel, support vector machines optimization based wavelet transform liner equalizer and decision directed combined blind equalization algorithm is proposed under the standard of statistical properties. The proposed algorithm improves the convergence speed through normalized orthogonal wavelet transform, and makes the weight vectors converge to global optimal points by using support vector machines. In order to correct signals' phase rotation and reduce steady-state error, the proposed algorithm integrate the advantages of decision directed algorithm and soft decision. Experiment simulations in underwater acoustic channel indicate that the proposed algorithm has ability to improve the convergence rate and to reduce the mean square error. especially to compensate phase rotation.\"",
        "1 is \"Falling asleep with Angry Birds, Facebook and Kindle: a large scale study on mobile application usage\", 2 is \"Stastical Estimation of the Intrinsic Dimensionality of a Noisy Signal Collection\".",
        "\nGiven above information, for an author who has written the paper with the title \"On the Outage Probability of Device-to-Device-Communication-Enabled Multichannel Cellular Networks: An RSS-Threshold-Based Perspective\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0162": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Two web-based approaches for noun sense disambiguation':",
        "title: \"Gated Multimodal Units for Information Fusion.\" with abstract: \"This paper presents a novel model for multimodal learning based on gated neural networks. The Gated Multimodal Unit (GMU) model is intended to be used as an internal unit in a neural network architecture whose purpose is to find an intermediate representation based on a combination of data from different modalities. The GMU learns to decide how modalities influence the activation of the unit using multiplicative gates. It was evaluated on a multilabel scenario for genre classification of movies using the plot and the poster. The GMU improved the macro f-score performance of single-modality approaches and outperformed other fusion strategies, including mixture of experts models. Along with this work, the MM-IMDb dataset is released which, to the best of our knowledge, is the largest publicly available multimodal dataset for genre prediction on movies.\"",
        "title: \"Comparaci\u00f3n de algoritmos de aprendizaje para identificaci\u00f3n del usuario a trav\u00e9s de la voz\" with abstract: \"En este trabajo presentamos una comparaci\u00f3n entre cuatro algoritmos de aprendizaje autom\u00e1tico para identificaci\u00f3n del hablante. El estudio hace hincapi\u00e9 en la simplificaci\u00f3n de la caracterizaci\u00f3n de la se\u00f1al de voz al no usar reconocimiento fon\u00e9tico. Los resultados hasta ahora alcanzados nos brindan elementos para preferir el algoritmo de M\u00e1quinas de Vectores de Soporte (SVM).\"",
        "title: \"Question answering for spanish supported by lexical context annotation\" with abstract: \"This paper describes the prototype developed by the Language Technologies Laboratory at INAOE for Spanish monolingual QA evaluation task at CLEF 2004. Our approach is centered on the use of context at a lexical level in order to identify possible answers to factoid questions. This method is supported by an alternative one based on pattern recognition in order to identify candidate answers to definition questions. We describe the methods applied at different stages of the system and our prototype architecture for question answering. The paper shows and discusses the results we achieved with this approach.\"",
        "title: \"Applying dependency trees and term density for answer selection reinforcement\" with abstract: \"This paper describes the experiments performed for the QA@CLEF- 2006 within the joint participation of the eLing Division at VEng and the Language Technologies Laboratory at INAOE. The aim of these experiments was to observe and quantify the improvements in the final step of the Question Answering prototype when some syntactic features were included into the decision process. In order to reach this goal, a shallow approach to answer ranking based on the term density measure has been integrated into the weighting schema. This approach has shown an interesting improvement against the same prototype without this module. The paper discusses the results achieved, the conclusions and further directions within this research.\"",
        "title: \"INAOE at CLEF 2006: Experiments in Spanish Question Answering.\" with abstract: \"This paper describes the system developed by the Language Technologies Lab at INAOE for the Spanish Question Answering task at CLEF 2006. The presented system is centered in a full data- driven architecture that uses machine learning and text mining techniques to identify the most probable answers to factoid and definition questions respectively. Its major quality is that it mainly relies on the use of lexical information and avoids applying any complex language processing re- source such as named entity classifiers, parsers or ontologies. Experimental results show that the proposed architecture can be a practical solution for monolingual question answering reaching an answer precision as high as 51%.\"",
        "1 is \"Uncovering the Semantics of Wikipedia Pagelinks.\", 2 is \"Optimization models of sound systems using genetic algorithms\".",
        "\nGiven above information, for an author who has written the paper with the title \"Two web-based approaches for noun sense disambiguation\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0163": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'R2D2 at GeoCLEF 2006: a combined approach':",
        "title: \"The UPV at GeoCLEF 2008: The GeoWorSE System.\" with abstract: \"This year our system was complemented with a map-based lter. During the indexing phase, all places are disambiguated and assigned their coordinates on the map. These coordinates are stored in a separate index. The search process is carried out in two phases: in the rst one, we search the collection with the same method applied in 2007, which exploits the expansion of index terms by means of WordNet synonyms and holonyms. The next phase consists in a re-ranking of the results of the previous phase depending on the distance of document toponyms from the toponyms in the query, or depending on the fact that the document contains toponyms that are included in an area dened by the query. The area is calculated from the toponyms in the query and their meronyms. This is the rst attempt to use GeoWordNet, a resource that includes the geographical coordinates of the places listed in WordNet, for the Geographical Information Retrieval task. The results show that map-based ltering allows to improve the results obtained by the base system, based only on the textual information.\"",
        "title: \"Borda-based voting schemes for semantic role labeling\" with abstract: \"In this article, we have studied the possibility of applying Borda and Fuzzy Borda voting schemes to combine semantic role labeling systems. To better select the correct semantic role, among those provided by different experts, we have introduced two measures: the first one calculates the overlap between labeled sentences, whereas the second one adds different scoring levels depending on the verbs that have been parsed.\"",
        "title: \"On the relative hardness of clustering corpora\" with abstract: \"Clustering is often considered the most important unsupervised learning problem and several clustering algorithms have been proposed over the years. Many of these algorithms have been tested on classical clustering corpora such as Reuters and 20 Newsgroups in order to determine their quality. However, up to now the relative hardness of those corpora has not been determined. The relative clustering hardness of a given corpus may be of high interest, since it would help to determine whether the usual corpora used to benchmark the clustering algorithms are hard enough. Moreover, if it is possible to find a set of features involved in the hardness of the clustering task itself, specific clustering techniques may be used instead of general ones in order to improve the quality of the obtained clusters. In this paper, we are presenting a study of the specific feature of the vocabulary overlapping among documents of a given corpus. Our preliminary experiments were carried out on three different corpora: the train and test version of the R8 subset of the Reuters collection and a reduced version of the 20 Newsgroups (Mini20Newsgroups). We figured out that a possible relation between the vocabulary overlapping and the F-Measure may be introduced.\"",
        "title: \"Detecting Deceptive Opinions: Intra And Cross-Domain Classification Using An Efficient Representation\" with abstract: \"Online opinions play an important role for customers and companies because of the increasing use they do to make purchase and business decisions. A consequence of that is the growing tendency to post fake reviews in order to change purchase decisions and opinions about products and services. Therefore, it is really important to filter out deceptive comments from the retrieved opinions. In this paper we propose the character n-grams in tokens, an efficient and effective variant of the traditional character n-grams model, which we use to obtain a low dimensionality representation of opinions. A Support Vector Machines classifier was used to evaluate our proposal on available corpora with reviews of hotels, doctors and restaurants. In order to study the performance of our model, we make experiments with intra and cross-domain cases. The aim of the latter experiment is to evaluate our approach in a realistic cross-domain scenario where deceptive opinions are available in a domain but not in another one. After comparing our method with state-of-the-art ones we may conclude that using character n-grams in tokens allows to obtain competitive results with a low dimensionality representation.\"",
        "title: \"POS tagging in Amazighe using support vector machines and conditional random fields\" with abstract: \"The aim of this paper is to present the first Amazighe POS tagger. Very few linguistic resources have been developed so far for Amazighe and we believe that the development of a POS tagger tool is the first step needed for automatic text processing. The used data have been manually collected and annotated. We have used state-of-art supervised machine learning approaches to build our POS-tagging models. The obtained accuracy achieved 92.58% and we have used the 10-fold technique to further validate our results.\"",
        "1 is \"Subjectivity Word Sense Disambiguation\", 2 is \"A survey of modern authorship attribution methods\".",
        "\nGiven above information, for an author who has written the paper with the title \"R2D2 at GeoCLEF 2006: a combined approach\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0164": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'A mapping between classifiers and training conditions for WSD':",
        "title: \"A text mining approach for definition question answering\" with abstract: \"This paper describes a method for definition question answering based on the use of surface text patterns. The method is specially suited to answer questions about person\u2019s positions and acronym\u2019s descriptions. It considers two main steps. First, it applies a sequence-mining algorithm to discover a set of definition-related text patterns from the Web. Then, using these patterns, it extracts a collection of concept-description pairs from a target document database, and applies the sequence-mining algorithm to determine the most adequate answer to a given question. Experimental results on the Spanish CLEF 2005 data set indicate that this method can be a practical solution for answering this kind of definition questions, reaching a precision as high as 84%.\"",
        "title: \"The Use of Lexical Context in Question Answering for Spanish\" with abstract: \"This paper describes the prototype developed by the Language Technologies Laboratory at INAOE for Spanish monolingual QA evaluation task at CLEF 2004. Our approach is centered in the use of context at a lexical level in order to identify possible answers to factoid questions. Such method is supported by an alternative one based on pattern recognition in order to identify candidate answers to definition questions. The methods applied at different stages of the system and prototype architecture for question answering are described. The paper shows and discusses the results achieved with this approach.\"",
        "title: \"Fusing Affective Dimensions and Audio-Visual Features from Segmented Video for Depression Recognition: INAOE-BUAP's Participation at AVEC'14 Challenge\" with abstract: \"Depression is a disease that affects a considerable portion of the world population. Severe cases of depression interfere with the common live of patients, for those patients a strict monitoring is necessary in order to control the progress of the disease and to prevent undesired side effects. A way to keep track of patients with depression is by means of online monitoring via human-computer-interaction. The AVEC'14 challenge aims at developing technology towards the online monitoring of depression patients. This paper describes an approach to depression recognition from audiovisual information in the context of the AVEC'14 challenge. The proposed method relies on an effective voice segmentation procedure, followed by segment-level feature extraction and aggregation. Finally, a meta-model is trained to fuse mono-modal information. The main novel features of our proposal are that (1) we use affective dimensions for building depression recognition models; (2) we extract visual information from voice and silence segments separately; (3) we consolidate features and use a meta-model for fusion. The proposed methodology is evaluated, experimental results reveal the method is competitive.\"",
        "title: \"A Web-Based Self-training Approach for Authorship Attribution\" with abstract: \"As any other text categorization task, authorship attribution requires a large number of training examples. These examples, which are easily obtained for most of the tasks, are particularly difficult to obtain for this case. Based on this fact, in this paper we investigate the possibility of using Web-based text mining methods for the identification of the author of a given poem. In particular, we propose a semi-supervised method that is specially suited to work with justfew training examples in order to tackle the problem of the lack of data with the same writing style. The method considers the automatic extraction of the unlabeled examples from the Web and its iterative integration into the training data set. To the knowledge of the authors, a semi-supervised method which makes use of the Web as support lexical resource has not been previously employed in this task. The results obtained on poem categorization show that this method may improve the classification accuracy and it is appropriate to handle the attribution of short documents.\"",
        "title: \"DIMEx100: A New Phonetic and Speech Corpus for Mexican Spanish\" with abstract: \"In this paper the phonetic and speech corpus DIMEx100 for Mexican Spanish is presented. We discuss both the linguistic motivation and the computational tools employed for the design, collection and transcription of the corpus. The phonetic transcription methodology is based on recent empirical studies proposing a new basic set of allophones and phonological rules for the dialect of the central part of Mexico. These phonological rules have been implemented in a visualization tool that provides the expected phonetic representation of a text, and also a default temporal alignment between the spoken corpus and its phonetic representation. The tools are also used to compute the properties of the corpus and compare these figures with previous work.\"",
        "1 is \"Computing semantic relatedness using Wikipedia-based explicit semantic analysis\", 2 is \"Proposal of acoustic measures for automatic detection of vocal fry\".",
        "\nGiven above information, for an author who has written the paper with the title \"A mapping between classifiers and training conditions for WSD\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0165": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Rate bounds for MIMO relay channels':",
        "title: \"Work Capacity of Regulated Freelance Platforms: Fundamental Limits and Decentralized Schemes.\" with abstract: \"Crowdsourcing of jobs to online freelance platforms is rapidly gaining popularity. Most crowdsourcing platforms are uncontrolled and offer freedom to customers and freelancers to choose each other. This works well for unskilled jobs (e.g., image classification) with no specific quality requirement since freelancers are functionally identical. For skilled jobs (e.g., software development) with spec...\"",
        "title: \"Network Utility Maximization: A Rate-Distortion Perspective\" with abstract: \"Network utility maximization (NUM) represents a vast and growing body of\nliterature in optimizing network operation such as throughput and fairness,\ngiven a set of constraints. This framework has resulted in a better\nunderstanding of optimal operation of and interaction among layers of the\nprotocol stack, including congestion control, routing, access and physical\nlayer transmission. However, traditional NUM optimization does not incorporate\nlossy compression (rate-distortion) into its formulation - data is assumed\npre-compressed and packetized prior to analysis. Since rate-distortion has a\nsubstantial impact on end-user experience (for example, in video/multimedia\ndelivery), this paper generalizes the traditional NUM framework to include\ncompression control. It develops a distributed compression control for binary\nsources, and solves the coupled NUM problem in special cases to illustrate\nimportant aspects of compression control. Finally, this paper discusses a\nstochastic framework that includes compression control, and provide insights on\nadaptive control of networks.\"",
        "title: \"Adaptive turbo-coded modulation for flat-fading channels\" with abstract: \"We consider a turbo-coded system employed on a flat-fading channel where the transmitter and receiver adapt the encoder, decoder, modulation scheme, and transmit power to the state of the channel. Assuming instantaneous and error-free channel gain and phase knowledge at the transmitter and the receiver, we determine the optimal adaptation strategy that maximizes the throughput of this system, whil...\"",
        "title: \"On the Capacity of a Class of MIMO Cognitive Radios\" with abstract: \"Cognitive radios have been studied recently as a means to utilize spectrum in a more efficient manner. This paper focuses on the fundamental limits of operation of a MIMO cognitive radio network with a single licensed user and a single cognitive user. The channel setting is equivalent to an interference channel with degraded message sets (with the cognitive user having access to the licensed user's message). An achievable region and an outer bound is derived for such a network setting. It is shown that the achievable region is optimal for a portion of the capacity region that includes sum capacity.\"",
        "title: \"Distributed Rate Allocation for Wireless Networks\" with abstract: \"This paper develops a distributed algorithm for rate allocation in wireless networks that achieves the same throughput region as optimal centralized algorithms. This cross-layer algorithm jointly performs medium access control and physical-layer rate adaptation. The paper establishes that this algorithm is throughput-optimal for general rate regions. In contrast to on-off scheduling, rate allocation enables optimal utilization of physical-layer schemes by scheduling multiple rate levels. The algorithm is based on local queue-length information, and thus the algorithm is of significant practical value. An important application of this algorithm is in multiple-band multiple-radio throughput-optimal distributed scheduling for white-space networks.\"",
        "1 is \"A vector-perturbation technique for near-capacity multiantenna multiuser communication-part I: channel inversion and regularization\", 2 is \"An 11b 3.6GS/s time-interleaved SAR ADC in 65nm CMOS\".",
        "\nGiven above information, for an author who has written the paper with the title \"Rate bounds for MIMO relay channels\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0166": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Using The Future To ''Sort Out'' The Present: Rankprop And Multitask Learning For Medical Risk Evaluation':",
        "title: \"Learning \"forgiving\" hash functions: algorithms and large scale tests\" with abstract: \"The problem of efficiently finding similar items in a large corpus of high-dimensional data points arises in many real-world tasks, such as music, image, and video retrieval. Beyond the scaling difficulties that arise with lookups in large data sets, the complexity in these domains is exacerbated by an imprecise definition of similarity. In this paper, we describe a method to learn a similarity function from only weakly labeled positive examples. Once learned, this similarity function is used as the basis of a hash function to severely constrain the number of points considered for each lookup. Tested on a large real-world audio dataset, only a tiny fraction of the points (\u223c0.27%) are ever considered for each lookup. To increase efficiency, no comparisons in the original high-dimensional space of points are required. The performance far surpasses, in terms of both efficiency and accuracy, a state-of-the-art Locality-Sensitive-Hashing based technique for the same problem and data set.\"",
        "title: \"Audio Fingerprinting: Combining Computer Vision & Data Stream Processing\" with abstract: \"In this paper, we present waveprint, a novel system for audio identification. Waveprint uses a combination of computer-vision techniques and large-scale-data-stream processing algorithms to create compact fingerprints of audio data that can be efficiently matched. The resulting system has excellent identification capabilities for small snippets of audio that have been degraded in a variety of manners, including competing noise, poor recording quality, and cell-phone playback. We measure the tradeoffs between performance, memory usage, and computation through extensive experimentation. The system is more efficient in terms of memory usage and computation, while being more accurate, when compared with previous state of the art systems.\"",
        "title: \"Using a priori knowledge to create probabilistic models for optimization\" with abstract: \"Recent studies have examined the effectiveness of using probabilistic models to guide the sample generation process for searching high dimensional spaces. Although the simplest models, which do not account for parameter interdependencies, often perform well on many problems, they may perform poorly when used on problems that have a high degree of interdependence between parameters. More complex dependency networks that can account for the interactions between parameters are required. However, building these networks may necessitate enormous amounts of sampling. In this paper, we demonstrate how a priori knowledge of parameter dependencies, even incomplete knowledge, can be incorporated to efficiently obtain accurate models that account for parameter interdependencies. This is achieved by effectively putting priors on the network structures that are created. These more accurate models yield improved results when used to guide the sample generation process for search and also when used to initialize the starting points of other search algorithms.\"",
        "title: \"The Virtues of Peer Pressure: A Simple Method for Discovering High-Value Mistakes\" with abstract: \"Much of the recent success of neural networks can be attributed to the deeper architectures that have become prevalent. However, the deeper architectures often yield unintelligible solutions, require enormous amounts of labeled data, and still remain brittle and easily broken. In this paper, we present a method to efficiently and intuitively discover input instances that are misclassified by well-trained neural networks. As in previous studies, we can identify instances that are so similar to previously seen examples such that the transformation is visually imperceptible. Additionally, unlike in previous studies, we can also generate mistakes that are significantly different from any training sample, while, importantly, still remaining in the space of samples that the network should be able to classify correctly. This is achieved by training a basket of N \\\"peer networks\\\" rather than a single network. These are similarly trained networks that serve to provide consistency pressure on each other. When an example is found for which a single network, S, disagrees with all of the other $$N-1$$ networks, which are consistent in their prediction, that example is a potential mistake for S. We present a simple method to find such examples and demonstrate it on two visual tasks. The examples discovered yield realistic images that clearly illuminate the weaknesses of the trained models, as well as provide a source of numerous, diverse, labeled-training samples.\"",
        "title: \"Evolution-Based Methods for Selecting Point Data for Object Localization: Applications to Computer-Assisted Surgery\" with abstract: \"Object localization has applications in many areas of engineeringand science. The goal is to spatially locate an arbitrarily shaped object.In many applications, it is desirable to minimize the number of measurementscollected while ensuring sufficient localization accuracy. In surgery, forexample, collecting a large number of localization measurements may eitherextend the time required to perform a surgical procedure or increase theradiation dosage to which a patient is exposed.Localization accuracy is a function of the spatial distribution ofdiscrete measurements over an object when measurement noise is present. Inprevious work (J. of Image Guided Surgery, Simon et al., 1995), metrics werepresented to evaluate the information available from a set of discreteobject measurements. In this study, new approaches to the discrete pointdata selection problem are described. These include hillclimbing, geneticalgorithms (GAs), and Population-Based Incremental Learning (PBIL).Extensions of the standard GA and PBIL methods that employ multipleparallel populations are explored. The results of extensive empiricaltesting are provided. The results suggest that a combination of PBIL andhillclimbing result in the best overall performance. A computer-assistedsurgical system that incorporates some of the methods presented in thispaper is currently being evaluated in cadaver trials.\"",
        "1 is \"Neural Based Steganography\", 2 is \"The Efficient Learning Of Multiple Task Sequences\".",
        "\nGiven above information, for an author who has written the paper with the title \"Using The Future To ''Sort Out'' The Present: Rankprop And Multitask Learning For Medical Risk Evaluation\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0167": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Well Structured Transition Systems With History':",
        "title: \"Automatic Verification of Parameterized Cache Coherence Protocols\" with abstract: \" . We propose a new method for the verication of parameterized cache coherence protocols. Cache coherence protocols are used tomaintain data consistency in multiprocessor systems equipped with localfast caches. In our approach we use arithmetic constraints to model possiblyinnite sets of global states of a multiprocessor system with manyidentical caches. In preliminary experiments using symbolic model checkersfor innite-state systems based on real arithmetics (HyTech [HHW97]and... \"",
        "title: \"Parameterized Tree Systems\" with abstract: \"Several recent works have considered parameterized verification, i.e. automatic verification of systems consisting of an arbitrary number of finite-state processes organized in a linear array. The aim of this paper is to extend these works by giving a simple and efficient method to prove safety properties for systems\n with tree-like architectures. A process in the system is a finite-state automaton and a transition is performed jointly by a process and\n its parent and children processes. The method derives an over-approximation of the induced transition system, which allows\n the use of finite trees as symbolic representations of infinite sets of configurations. Compared to traditional methods for\n parameterized verification of systems with tree topologies, our method does not require the manipulation of tree transducers,\n hence its simplicity and efficiency. We have implemented a prototype which works well on several nontrivial tree-based protocols.\n \"",
        "title: \"A Biologically Inspired Model with Fusion and Clonation of Membranes\" with abstract: \"P-systems represent an important class of biologically inspired computational models. In this paper, we study computational properties of a variation of P-systems with rules that model in an abstract way fusion and clonation of membranes. We focus our attention on extended P-systems with an interleaving semantics and symbol objects and we investigate decision problems like reachability of a configuration, boundedness (finiteness of the state space), and coverability (verification of safety properties). In particular we use the theory of well-structured transition systems to prove that both the coverability and the boundedness problems are decidable for PB systems with fusion and clonation. Our results represent a preliminary step towards the development of automated verification procedures for concurrent systems with biologically inspired operations like fusion and clonation.\"",
        "title: \"Parameterized verification of time-sensitive models of ad hoc network protocols.\" with abstract: \"We study decidability and undecidability results for parameterized verification of a formal model of timed Ad Hoc network protocols. The communication topology is defined by an undirected graph and the behaviour of each node is defined by a timed automaton communicating with its neighbours via broadcast messages. We consider parameterized verification problems formulated in terms of reachability. In particular we are interested in searching for an initial configuration from which an individual node can reach an error state. We study the problem for dense and discrete time and compare the results with those obtained for (fully connected) networks of timed automata.\"",
        "title: \"A Linear Logic Calculus Objects\" with abstract: \" This paper presents a linear logic programming language, called O \\Gammaffi , that gives a complete account of an object-oriented calculus with inheritance and override. This language is best understood as a logical counterpart the object and record extensions of functional programming that have recently been proposed in the literature. From these proposals, O \\Gammaffi inherits the representation of objects as composite data structures, with attribute and method fields, as well as their... \"",
        "1 is \"Program analysis via graph reachability\", 2 is \"Detecting conflicts in commitments\".",
        "\nGiven above information, for an author who has written the paper with the title \"Well Structured Transition Systems With History\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0168": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Predictive Absolute Moment Block Truncation Coding For Image Compression':",
        "title: \"A backward segmented-block matching algorithm for video compression\" with abstract: \"In this paper, we present a segmentation-based block matching algorithm for video compression. First, the decoded frame is segmented into background and foreground using a two-step predictive procedure. This segmentation is then used in estimating the motion in the next frame. Being a backward procedure, the segmentation information is not transmitted to the receiver. Experimental results show that blocks at the leading edges of moving objects get better compensated with improved subjective quality of the motion compensated frame.\"",
        "title: \"Video object error coding method based on compressive sensing\" with abstract: \"The recently emerged theory of compressive sensing (CS) has a remarkable result that signals having sparse representations in some known basis can be represented (with high probability) by taking a few random projection measurements of the signals. In this paper, we study some CS sparse reconstruction methods and propose a video object error coding method based on CS theory. The proposed system first assumes the moving objects have been segmented from background image and object-based motion compensated from the previous reconstruction frame, and then the resulting object error is encoded by using CS random matrix projection. Finally the coded measurements can be quantized to store or transmit. Experimental results demonstrate the object error blocks can be effectively recovered by using CS sparse reconstruction algorithms. This proposed method would be widely used in the object-based video compression fields.\"",
        "title: \"Signal Recovery from Random Measurements via Extended Orthogonal Matching Pursuit\" with abstract: \"Orthogonal Matching Pursuit (OMP) and Basis Pursuit (BP) are two well-known recovery algorithms in compressed sensing. To recover a -dimensional -sparse signal with high probability, OMP needs number of measurements, whereas BP needs only number of measurements. In contrary, OMP is a practically more appealing algorithm due to its superior execution speed. In this piece of work, we have proposed a scheme that brings the required number of measurements for OMP closer to BP. We have termed this scheme as , which runs OMP for -iterations instead of -iterations, by choosing a value of . It is shown that guarantees a high probability signal recovery with number of measurements. Another limitation of OMP unlike BP is that it requires the knowledge of . In order to overcome this limitation, we have extended the idea of to illustrate another recovery scheme called , which runs OMP until th- signal residue vanishes. It is shown that can achieve a close to -norm recovery without any knowledge of like BP.\"",
        "title: \"Kernel-Based Spatial-Color Modeling for Fast Moving Object Tracking.\" with abstract: \"Visual tracking has been a challenging problem in computer vision over the decades. The applications of Visual Tracking are far-reaching, ranging from surveillance and monitoring to smart rooms. Mean- shift (MS) tracker, which gained more attention recently, is known for tracking objects in a cluttered environment and its low compu- tational complexity. The major problem encountered in histogram- based MS is its inability to track rapidly moving objects. In order to track fast moving objects, we propose a new robust mean-shift tracker that uses both spatial similarity measure and color histogram- based similarity measure. The inability of MS tracker to handle large displacements is circumvented by the spatial similarity-based track- ing module, which lacks robustness to object's appearance change. The performance of the proposed tracker is better than the individual trackers for tracking fast-moving objects with better accuracy.\"",
        "title: \"On the closeness of the space spanned by the lattice structures for a class of linear phase perfect reconstruction filter banks\" with abstract: \"The incompleteness of the existing lattice structures has been well established for M-channel FIR linear phase perfect reconstruction filter banks (LPPRFBs) with filter length L2M in the literature, and even the nonexistence of complete order-one lattice has been reported recently. Thus, a question arises naturally as to how large the space spanned by the existing lattice structure is, and about its closeness over some polynomial transformations. The study for such issue can reveal what sense of optimality the lattice based design for LPPRFBs possesses. Inspired from this perspective, this paper firstly studies the closeness of the space spanned by the existing lattice structures under the polynomial transformations for arbitrary equal-length LPPRFBs. We have shown that this space is closed under the popular polynomial transforms widely used in FB design, which establishes the suboptimality of the lattice based design methods for LPPRFBs. Furthermore, the explicit relationship between the lattice parameters before and after transformations has been shown for describing the closeness of the space spanned by those lattice structures.\"",
        "1 is \"A novel and efficient design of multidimensional PR two-channel filter banks with hourglass-shaped passband support\", 2 is \"Parametric prediction of heap memory requirements\".",
        "\nGiven above information, for an author who has written the paper with the title \"Predictive Absolute Moment Block Truncation Coding For Image Compression\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0169": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'A study on two-sided linear prediction approach for land mine detection':",
        "title: \"TDOA Positioning Irrespective of Source Range.\" with abstract: \"TDOA localization requires the knowledge if the source is in the near-field or far-field, for the purpose to decide using the curved wavefront model that enables point positioning or the linear wavefront model that provides only the DOA. Such prior knowledge is often not available in practice. The far-field model can cause a considerable amount of DOA bias if the source is not sufficiently distant from the sensor array. This paper proposes a unified model to locate a source irrespective of whether it is in the near field, the far field or in between. The proposed model represents the source location by the direction and the inverse-range. It yields the unique coordinate if the source is near or the DOA if it is distant. We developed the Maximumm Likelihood Estimator for the proposed model through the Gauss\u2013Newton iteration and semidefinite relaxation. We analyze the proposed model using the Hybrid Bhattacharyya\u2013Barankin bound and show that the proposed model does not have the thresholding effect as the source range increases, validating that there is no need to resort to the far-field model even if the source range is large. We also perform bias analysis and elaborate a benefit of the proposed approach in reducing the DOA bias as compared to the far-field model.\"",
        "title: \"Adaptive time-delay estimation in nonstationary signal and/or noise power environments\" with abstract: \"A model for an adaptive time-delay estimator is proposed to improve its performance in estimating the difference in arrival time of a bandlimited random signal received by two spatially separated sensors in an environment where the signal and noise power are time varying. The system comprises two adaptive units: a filter to compensate time shift between the two receiver channels and a gain control to provide Wiener filtering. Both the filter coefficients and the variable gain are adjusted simultaneously by using modifications from the stochastic mean-square-error gradient in the traditional adaptive least-mean-square time-delay estimation (LMSTDE) method. The convergence characteristics of the proposed system are analyzed in detail and compared with those obtained by the traditional technique. Theoretical results show that, unlike the LMSTDE configuration, this arrangement can decouple the adaptation of time shift from the changing signal and/or noise power, which in turn gives rise to better convergence behavior of the delay estimate. Simulation results are included to illustrate the effectiveness of the new model and corroborate the theoretical developments\"",
        "title: \"An iterative algorithm for two-scale wavelet decomposition\" with abstract: \"This correspondence proposes an iterative method to decompose an arbitrary mother wavelet into a bandpass filter and a lowpass filter, where the filter pair will reproduce the mother wavelet through the two-scale equations. This problem is not straightforward because the two-scale relationship between the filter pair and the mother wavelet is nonlinear. The method finds the filter pair by minimizing an objective function with respect to one filter in an iteration and the other in the next iteration until convergence. The algorithm requires least-squares minimization only and is computationally efficient. The performance of the proposed algorithm is supported by simulations\"",
        "title: \"Efficient closed-form estimators for multistatic sonar localization\" with abstract: \"This paper addresses the multistatic sonar localization problem using time measurements only or with bearings in the presence of Gaussian transmitter positions, receiver positions, and propagation speed variations. We develop efficient algebraic solutions to this nonlinear estimation problem through parameter transformation and multistage processing. Both situations of known and unknown statistical distribution of the propagation speed are considered. Analysis supports their performance in reaching the hybrid Cramer-Rao lower bound (CRLB) over the small error region.\"",
        "title: \"Accurate Localization of a Rigid Body Using Multiple Sensors and Landmarks\" with abstract: \"This paper develops estimators for locating a rigid body using the time measurements, and the Doppler as well if it is moving, between the sensors in the rigid body and a few landmarks outside. The challenge of rigid body localization is that in addition to the position, we are also interested in obtaining the rotation parameters of the rigid body that must belong to the special orthogonal group. ...\"",
        "1 is \"A Unified Convergence Analysis of Block Successive Minimization Methods for Nonsmooth Optimization.\", 2 is \"A study of cross-validation and bootstrap for accuracy estimation and model selection\".",
        "\nGiven above information, for an author who has written the paper with the title \"A study on two-sided linear prediction approach for land mine detection\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0170": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Multiresolution analysis for meshes with appearance attributes':",
        "title: \"An Active Vision System for Obtaining High Resolution Depth Information\" with abstract: \"A low-cost active vision head with ten degrees of freedom is presented that has been build from off-the-shelf parts. To obtain high resolution depth information of fixated objects in the scene a general purpose calibration procedure is proposed which estimates intrinsic and extrinsic camera parameters including the vergence axes of both cameras. To produce enhanced dense depth maps a hierarchical block matching procedure is presented that employs color information. To simplify the development of controlling strategies for the head a modular hierarchy is applied that distributes various tasks among different levels employing basic capabilities of the components of the head.\"",
        "title: \"Linking Feature Lines on 3D Triangle Meshes with Artificial Potential Fields\" with abstract: \"We propose artificial potential fields as a support theory for a feature linking algorithm. This algorithm operates on 3D triangle meshes derived from multiple range scans of an object, and the features of interest are curvature extrema on the object's surface. A problem that arises with detecting these features is that results from standard algorithms are often incomplete in that feature lines are broken and discontinuous. Our novel linking algorithm closes these broken feature lines to form a more complete feature description. The main contribution of this algorithm is the use of arti- ficial potential fields to govern the linking process. In this paper, we discuss the feature detection process itself and then define the linking procedure in the context of potential fields. We present results for both synthetic and scanned models.\"",
        "title: \"Studies on the Effectiveness of Multispectral Images for Face Recognition: Comparative Studies and New Approaches\" with abstract: \"In this paper, we investigate face recognition in unconstrained illumination conditions. A twofold contribution is proposed: First, three state of the art algorithms, namely Multiblock Local Binary Pattern (MBLBP), Histogram of Gabor Phase Patterns (HGPP) and Local Gabor Binary Pattern Histogram Sequence (LGBPHS) are challenged against the IRIS-M3 multispectral face data base to evaluate their robustness against high illumination variation. Second, we propose to enhance the Performance of the three mentioned algorithms, which has been drastically decreased because of the non-monotonic illumination variation that distinguishes the IRIS-M3 face database. Instead of the usual braod band images, we use narrow band sub spectral images selected from the visible spectrum. Selection of best spectral bands is formulated as a pursuit optimization problem wherein the vector of weights determining the importance of each visible spectral band is supposed to be sparse, and hence can be determined by minimizing its L1-norm. The results highlight further the still challenging problem of face recognition in conditions with high illumination variation, as well as the effectiveness of our sub spectral images based approach to increase the accuracy of the studied algorithms by at least 14% upon the proposed database.\"",
        "title: \"A kernelized sparsity-based approach for best spectral bands selection for face recognition\" with abstract: \"We study face recognition in unconstrained illumination conditions. A twofold contribution is proposed: First, the robustness of four state-of-the-art algorithms, namely Multi-block Local Binary Pattern (MBLBP), Histogram of Gabor Phase Patterns (HGPP), Local Gabor Binary Pattern Histogram Sequence (LGBPHS) and Patterns of Oriented Edge Magnitudes (POEM-WPCA) against high illumination variation is studied. Second, we propose to enhance the performance of the four mentioned algorithms, which has been drastically decreased upon the day lighted face images provided by IRIS-M3 face database. For this purpose, we use visible narrow band subspectral images selected from the mentioned database. We formulate best spectral bands selection as a pursuit optimization problem wherein the vector of weights determining the importance of each visible spectral band is supposed to be sparse, and hence can be determined by minimizing its L1-norm. Several fusing approaches are then applied on selected best spectral bands using multi-scale and multi-orientation Gabor wavelets. The results highlight further the still challenging problem of face recognition in conditions with high illumination variation, as well as the effectiveness of our subspectral images based approach with its two components; bands selection and bands fusion, to increase the accuracy of the studied algorithms by at least 14 % upon the proposed database.\"",
        "title: \"Multiresolution analysis for meshes with appearance attributes\" with abstract: \"We present a new multiresolution analysis framework for ir- regular meshes with attributes based on the lifting scheme. We introduce a surface prediction operator to compute the detail coefcients for the geometry and the attributes of the model. Attribute analysis gives appearance information to complete the geometrical analysis of the model. A set of experimental results are given to show the efcienc y of our framework. We present two applications to adaptive visual- ization and denoising.\"",
        "1 is \"SURE-Based Non-Local Means.\", 2 is \"Uncertainty Modeling and Model Selection for Geometric Inference\".",
        "\nGiven above information, for an author who has written the paper with the title \"Multiresolution analysis for meshes with appearance attributes\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0171": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Optical flow-based real-time object tracking using non-prior training active feature model':",
        "title: \"Multifocus Image Fusion by Establishing Focal Connectivity\" with abstract: \"Multifocus fusion is the process of unifying focal information from a set of input images acquired with limited depth of field. In this effort, we present a general purpose multifocus fusion algorithm, which can be applied to varied applications ranging from microscopic to long range scenes. The main contribution in this paper is the segmentation of the input images into partitions based on focal connectivity. Focal connectivity is established by isolating regions in an input image that fall on the same focal plane. Our method uses focal connectivity and does not rely on physical properties like edges directly for segmentation. Our method establishes sharpness maps to the input images, which are used to isolate and attribute image partitions to input images. The partitions are mosaiced seamlessly to form the fused image. Illustrative examples of multifocus fusion using our method are shown. Comparisons against existing methods are made and the results are discussed.\"",
        "title: \"Camera Handoff with Adaptive Resource Management for Multi-camera Multi-target Surveillance\" with abstract: \"Camera handoff is a crucial step to generate a continuously tracked and consistently labeled trajectory of the object of interest in multi-camera surveillance systems. Most existing camera handoff algorithms concentrate on data association, namely consistent labeling, where images of the same object are matched across different cameras. However, most real-time object tracking systems see a decrease in the system's frame rate as the number of tracked objects increases. To address this issue, we propose to incorporate an adaptive resource management mechanism into camera handoff. In so doing, cameras\u00c2\u00bf resources can be dynamically allocated to multiple objects according to their priorities and hence the required minimum frame rate can be maintained. Experimental results illustrate that the proposed camera handoff algorithm is capable of maintaining a constant frame rate and of achieving a substantially improved handoff success rate by approximately 20% in comparison with the algorithm presented by Khan and Shah.\"",
        "title: \"Triangle mesh-based edge detection and its application to surface segmentation and adaptive surface smoothing\" with abstract: \"Triangle meshes are widely used in representing surfaces in com- puter vision and computer graphics. Although 2D image processing- based edge detection techniques have been popular in many appli- cation areas, they are not well developed for surfaces represented by triangle meshes. This paper proposes a robust edge detection algorithm for triangle meshes and its applications to surface seg- mentation and adaptive surface smoothing. The proposed edge de- tection technique is based on eigen analysis of the surface normal vector field in a geodesic window. To compute the edge strength of a certain vertex, the neighboring vertices in a specified geodesic distance are involved. Edge information are used further to seg- ment the surfaces with watershed algorithm and to achieve edge- preserved, adaptive surface smoothing. The proposed algorithm is novel in robustly detecting edges on triangle meshes against noise. The 3D watershed algorithm is an extension from previous work. Experimental results on surfaces reconstructed from multi-view real range images are presented.\"",
        "title: \"Integration of multiple range and intensity image pairs using a volumetric method to create textured three-dimensional models\" with abstract: \"We present a volumetric approach to three-dimensional (3D) object modeling that differs from previous techniques in that both object texture and geometry are considered in the reconstruction process. The motivation for the research is the simulation of a thermal tire inspection station. integrating 3D geometry information with two-dimensional thermal images permits the thermal information to be displayed as a texture map on the tire structure, enhancing analysis capabilities. Additionally, constructing the fire geometry during the inspection process allows the tire to be examined for structural defects that might be missed if the thermal data were textured onto a predefined model. Experimental results demonstrate the efficacy of the proposed approach and quantitative experiments indicate that the volumetric integration technique compares favorably to a state-of-the-art, mesh-based integration approach in terms of geometrical accuracy. Future research goals are also noted. (C) 2001 SPIE and IS&T.\"",
        "title: \"Impact Of Intensity Edge Map On Segmentation Of Noisy Range Images\" with abstract: \"In this paper, we investigate the impact of intensity edge maps (IEMs) on the segmentation of noisy range images. Two edge-based segmentation algorithms are considered. The first is a watershed-based segmentation technique and the other is the scan-line grouping technique. Each of these algorithms is implemented in two different forms. In the first form, an IEM is fused with the range edge map prior to segmentation. In the second form, the range edge map alone is used. The performance of each algorithm, with and without the use of the IEM information, is evaluated and reported in terms of correct segmentation rate. For our experiments, two sets of real range images are used. The first set comprises inherently noisy images. The other set is composed of images with varying levels of artificial, additive Gaussian noise. The experimental results indicate that the use of IEMs can significantly improve edge-based segmentation of noisy range images. Considering these results, it seems that segmentation tasks involving range images captured by noisy scanners would benefit from the use of IEM information. Additionally, the experiments indicate that higher quality edge information dan be obtained by fusing range and intensity edge information.\"",
        "1 is \"Fast Image Restoration for Reducing Block Artifacts Based on Adaptive Constrained Optimization\", 2 is \"Detection and Tracking of Moving Objects from Overlapping EO and IR Sensors\".",
        "\nGiven above information, for an author who has written the paper with the title \"Optical flow-based real-time object tracking using non-prior training active feature model\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0172": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Synchronization In Oscillator Networks: Switching Topologies And Non-Homogeneous Delays':",
        "title: \"Using economic Model Predictive Control to design sustainable policies for mitigating climate change\" with abstract: \"Reducing greenhouse gas emissions is now an important and pressing matter. Systems control theory, and in particular feedback control, can contribute to the design of policies that achieve sustainable levels of emissions of CO2 (and other greenhouse gases) while minimizing the impact on the economy, and at the same time explicitly addressing the high levels of uncertainty associated with predictions of future emissions. In this paper, preliminary results are described for an approach where economic Model Predictive Control (MPC) is applied to a Regional dynamic Integrated model of Climate and the Economy (RICE model) as a test bed to design savings rates and global carbon tax for greenhouse gas emissions. Using feedback control, the policies are updated on the basis of the observed emissions, rather than on the predicted level of emissions. The basic structure and principle of the RICE model is firstly introduced and some key equations are described. The idea of introducing feedback control is then explained and economic MPC is applied to design policies for CO2 emissions. Simulation results are presented to demonstrate the effectiveness of the proposed method for two different scenarios. Feedback control design provides a degree of robustness against disturbances and model uncertainties, which is illustrated through a simulation study with two particular types of uncertainties. The results obtained in this paper illustrate the strength of the proposed design approach and form the basis for future research on using systems control theory to design optimal sustainable policies.\"",
        "title: \"Analysis of aircraft pitch axis stability augmentation system using sum of squares optimization\" with abstract: \"The region of attraction of a trim point for the pitch axis of a nonlinear modeled aircraft modulated via linear dynamic inversion based controller was determined numerically. The model incorporates uncertainty in the position of center of gravity along the X-body axis. The stability regions are computed using SOSTOOLS, which converts the required sum of squares conditions to an appropriate semi definite program that is then solved using SeDuMi.\"",
        "title: \"Stochastic processes and feedback-linearisation for online identification and Bayesian adaptive control of fully-actuated mechanical systems.\" with abstract: \"  This work proposes a new method for simultaneous probabilistic identification and control of an observable, fully-actuated mechanical system. Identification is achieved by conditioning stochastic process priors on observations of configurations and noisy estimates of configuration derivatives. In contrast to previous work that has used stochastic processes for identification, we leverage the structural knowledge afforded by Lagrangian mechanics and learn the drift and control input matrix functions of the control-affine system separately. We utilise feedback-linearisation to reduce, in expectation, the uncertain nonlinear control problem to one that is easy to regulate in a desired manner. Thereby, our method combines the flexibility of nonparametric Bayesian learning with epistemological guarantees on the expected closed-loop trajectory. We illustrate our method in the context of torque-actuated pendula where the dynamics are learned with a combination of normal and log-normal processes. \"",
        "title: \"Structured Sum Of Squares For Networked Systems Analysis\" with abstract: \"In this paper we introduce a structured Sum of Squares technique that enables Sum of Squares programming to be applied to networked systems analysis. By taking the structure of the network into account, we limit the site and number of decision variables in the LMI representation of the Sum of Squares, which improves the scalability of the technique for networked systems beyond taking advantage of symmetry and sparsity. We apply the technique to test non-negativity of fourth order structured polynomials in many variables and show that for these problems the technique has improved scalability over existing Sum of Squares techniques.\"",
        "title: \"Improving the Performance of Network Congestion Control Algorithms\" with abstract: \"This technical note describes a redesign framework for fluid-flow models of network congestion control algorithms. Motivated by the augmented Lagrangian method, we introduce extra dynamics to algorithms resulting from traditional primal-dual methods to improve their performance while guaranteeing stability. We use our method to redesign the primal-dual, primal and dual algorithms for network flow control. In particular, we investigate the influence of the gains resulting from the extra dynamics on system stability and robustness to time delays. We provide a method to improve the transient performance and delay robustness of the overall system by tuning these gains.\"",
        "1 is \"On the Evaluation Complexity of Cubic Regularization Methods for Potentially Rank-Deficient Nonlinear Least-Squares Problems and Its Relevance to Constrained Nonlinear Optimization.\", 2 is \"Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming\".",
        "\nGiven above information, for an author who has written the paper with the title \"Synchronization In Oscillator Networks: Switching Topologies And Non-Homogeneous Delays\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0173": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Embedding Digitized Fibre Fields In Finite Element Models Of Muscles':",
        "title: \"Intensity-based registration of prostate brachytherapy implants and ultrasound\" with abstract: \"Purpose: In prostate brachytherapy, determining the 3D location of the seeds relative to surrounding structures is necessary for calculating dosimetry. Ultrasound imaging provides the ability to visualize soft tissues, and implanted seeds can be reconstructed from C-arm fluoroscopy. Registration between these two complementary modalities would allow us to make immediate provisions for dosimetric deviation from the optimal implant plan. Methods: We propose intensity- based registration between ultrasound and a reconstructed model of seeds from fluoroscopy. The ultrasound images are pre-processed with recursive thresholding and phase congruency. Then a 3D ultrasound volume is reconstructed and registered to the implant model using mutual information. Results: A standard training phantom was implanted with 49 seeds. Average registration error between corresponding seeds relative to the ground truth is 0.09 mm. The effect of false positives in ultrasound was investigated by masking seeds from the fluoroscopy reconstructed model. The registration error remained below 0.5 mm at a rate of 30% false positives. Conclusion: Our method promises to be clinically adequate, where requirements for registration is 1.5 mm.\"",
        "title: \"Real-time ultrasound image classification for spine anesthesia using local directional Hadamard features\" with abstract: \"Injection therapy is a commonly used solution for back pain management. This procedure typically involves percutaneous insertion of a needle between or around the vertebrae, to deliver anesthetics near nerve bundles. Most frequently, spinal injections are performed either blindly using palpation or under the guidance of fluoroscopy or computed tomography. Recently, due to the drawbacks of the ionizing radiation of such imaging modalities, there has been a growing interest in using ultrasound imaging as an alternative. However, the complex spinal anatomy with different wave-like structures, affected by speckle noise, makes the accurate identification of the appropriate injection plane difficult. The aim of this study was to propose an automated system that can identify the optimal plane for epidural steroid injections and facet joint injections.\"",
        "title: \"Using Hidden Markov Models to capture temporal aspects of ultrasound data in prostate cancer\" with abstract: \"Recent studies highlight temporal ultrasound data as highly promising in differentiating between malignant and benign tissues in prostate cancer patients. Since Hidden Markov Models can be used for capturing order and patterns in time varying signals, we employ them to model temporal aspects of ultrasound data that are typically not incorporated in existing models. By comparing order-preserving and orderaltering models, we demonstrate that the order encoded in the series is necessary to model the variability in ultrasound data of prostate tissues. In future studies, we will investigate the influence of order on the differentiation between malignant and benign tissues.\"",
        "title: \"Information processing in computer-assisted interventions: 5th international conference, 2014\" with abstract: \"Information processing is an increasingly important tool for surgical interventions. In computer-assisted surgery and interventional radiology, computer systems present valuable information during procedures which is used to help clinical decision-making at the point of treatment.With the paradigm shift towards minimally invasive surgery, where the surgeon or interventionalist has an inherently restricted view of the operating field, this information has a significant influence on how procedures are performed. In fact, surgical instrumentation and technology is increasing in complexity in\"",
        "title: \"A New Approach for Creating Customizable Cytoarchitectonic Probabilistic Maps without a Template\" with abstract: \"We present a novel technique for creating template-free probabilistic maps of the cytoarchitectonic areas using a groupwise registration. We use the technique to transform 10 human post-mortem structural MR data sets, together with their corresponding cytoarchitectonic information, to a common space. We have targeted the cytoarchitectonically defined subregions of the primary auditory cortex. Thanks to the template-free groupwise registration, the created maps are not macroanatomically biased towards a specific geometry/topology. The advantage of the groupwise versus pairwise registration in avoiding such anatomical bias is better revealed in studies with small number of subjects and a high degree of variability among the individuals such as the post-mortem data. A leave-one-out cross-validation method was used to compare the sensitivity, specificity and positive predictive value of the proposed and published maps. We observe a significant improvement in localization of cytoarchitectonically defined subregions in primary auditory cortex using the proposed maps. The proposed maps can be tailored to any subject space by registering the subject image to the average of the groupwise-registered post-mortem images.\"",
        "1 is \"Synthesizing sounds from rigid-body simulations\", 2 is \"Nonrigid liver registration for image-guided surgery using partial surface data: a novel iterative approach\".",
        "\nGiven above information, for an author who has written the paper with the title \"Embedding Digitized Fibre Fields In Finite Element Models Of Muscles\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0174": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Probabilistic Explanation Based Learning':",
        "title: \"A model for mining relevant and non-redundant information\" with abstract: \"We propose a relatively simple yet powerful model for choosing relevant and non-redundant pieces of information. The model addresses data mining or information retrieval settings where relevance is measured with respect to a set of key or query objects, either specified by the user or obtained by a data mining step. The problem addressed is not only to identify other relevant objects, but also ensure that they are not related to possible negative query objects, and that they are not redundant with respect to each other. The model proposed here only assumes a similarity or distance function for the objects. It has simple parameterization to allow for different behaviors with respect to query objects. We analyze the model and give two efficient, approximate methods. We illustrate and evaluate the proposed model on different applications: linguistics and social networks. The results indicate that the model and methods are useful in finding a relevant and non-redundant set of results. While this area has been a popular topic of research, our contribution is to provide a simple, generic model that covers several related approaches while providing a systematic model for taking account of positive and negative query objects as well as non-redundancy of the output.\"",
        "title: \"Contrasting Subgroup Discovery\" with abstract: \"Subgroup discovery methods find interesting subsets of objects of a given class. Motivated by an application in bioinformatics, we first define a generalized subgroup discovery problem. In this setting, a subgroup is interesting if its members are characteristic for their class, even if the classes are not identical. Then we further refine this setting for the case where subsets of objects, for example, subsets of objects that represent different time points or different phenotypes, are contrasted. We show that this allows finding subgroups of objects that could not be found with classical subgroup discovery. To find such subgroups, we propose an approach that consists of two subgroup discovery steps and an intermediate, contrast set definition step. This approach is applicable in various application areas. An example is biology, where interesting subgroups of genes are searched by using gene expression data. We address the problem of finding enriched gene sets that are specific for virus-infected samples for a specific time point or a specific phenotype. We report on experimental results on a time series dataset for virus-infected Solanum tuberosum (potato) plants. The results on S. tuberosum's response to virus-infection revealed new research hypotheses for plant biologists.\"",
        "title: \"Unobtrusive Online Monitoring Of Sleep At Home\" with abstract: \"We describe an online sleep monitoring service, based on unobtrusive ballistocardiography (BCG) measurement in an ordinary bed. The novelty of the system is that the sleep tracking web application is based on measurements from a fully unobtrusive sensor. The BCG signal is measured with a piezoelectric film sensor under the mattress topper, and sent to the web server for analysis. Heart rate and respiratory variation, activity, sleep stages, and stress reactions are inferred based on the signal. The sleep information is presented to the user along with measurements of the sleeping environment (temperature, noise, luminosity) and user-logged tags (e.g. stress, alcohol, exercise). The approach is designed for long-term use at home, allowing users to follow the development of their sleep over months and years. The service has also a medical use, as sleep disorder patients can be measured for long periods before and after interventions.\"",
        "title: \"Mining Relaxed Graph Properties in Internet\" with abstract: \"Many real world datasets are represented in the form of graphs. The classical graph properties found in the data, like cliques or independent sets, can reveal new interesting information in the data. However, such properties can be either too rare or too trivial in the given context. By relaxing the criteria of the classical properties, we can find more and totally new patterns in the data. In this paper, we define relaxed graph properties and study their use in analyzing and processing graph-based data. Especially, we consider the problem of finding self-referring groups in WWW, and give a general algorithm for mining all such patterns from a collection of WWW pages. We suggest that such self-referring groups can reveal web communities or other clusterings in WWW and also help in compression of graph-formed data.\"",
        "title: \"SegMine workflows for semantic microarray data analysis in Orange4WS.\" with abstract: \"In experimental data analysis, bioinformatics researchers increasingly rely on tools that enable the composition and reuse of scientific workflows. The utility of current bioinformatics workflow environments can be significantly increased by offering advanced data mining services as workflow components. Such services can support, for instance, knowledge discovery from diverse distributed data and knowledge sources (such as GO, KEGG, PubMed, and experimental databases). Specifically, cutting-edge data analysis approaches, such as semantic data mining, link discovery, and visualization, have not yet been made available to researchers investigating complex biological datasets.We present a new methodology, SegMine, for semantic analysis of microarray data by exploiting general biological knowledge, and a new workflow environment, Orange4WS, with integrated support for web services in which the SegMine methodology is implemented. The SegMine methodology consists of two main steps. First, the semantic subgroup discovery algorithm is used to construct elaborate rules that identify enriched gene sets. Then, a link discovery service is used for the creation and visualization of new biological hypotheses. The utility of SegMine, implemented as a set of workflows in Orange4WS, is demonstrated in two microarray data analysis applications. In the analysis of senescence in human stem cells, the use of SegMine resulted in three novel research hypotheses that could improve understanding of the underlying mechanisms of senescence and identification of candidate marker genes.Compared to the available data analysis systems, SegMine offers improved hypothesis generation and data interpretation for bioinformatics in an easy-to-use integrated workflow environment.\"",
        "1 is \"Link mining: a new data mining challenge\", 2 is \"Mining Association Rules in Multiple Relations\".",
        "\nGiven above information, for an author who has written the paper with the title \"Probabilistic Explanation Based Learning\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0175": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'A Component-Based Integrated Toolkit':",
        "title: \"Kangaroo: A Tenant-Centric Software-Defined Cloud Infrastructure\" with abstract: \"Applications on cloud infrastructures acquire virtual machines (VMs) from providers when necessary. The current interface for acquiring VMs from most providers, however, is too limiting for the tenants, in terms of granularity in which VMs can be acquired (e.g., small, medium, large, etc.), while giving very limited control over their placement. The former leads to VM underutilization, and the latter has performance implications, both translating into higher costs for the tenants. In this work, we leverage nested virtualization and a networking overlay to tackle these problems. We present Kangaroo, an Open Stack-based virtual infrastructure provider, and IPOPsm, a virtual networking switch for communication between nested VMs over different infrastructure VMs. In addition, we design and implement Skippy, the realization of our proposed virtual infrastructure API for programming Kangaroo. Our benchmarks show that through careful mapping of nested VMs to infrastructure VMs, Kangaroo achieves up to an order of magnitude better performance, with only half the cost on Amazon EC2. Further, Kangaroo's unified Open Stack API allows us to migrate an entire application between Amazon EC2 and our local Open Nebula deployment within a few minutes, without any downtime or modification to the application code.\"",
        "title: \"The Case For Smartphones As An Urgent Computing Client Platform\" with abstract: \"The computing world is now populated with smartphones which combine the features of a phone with a general purpose computer and come loaded with sensors including digital cameras, global positioning system (GPS) receivers, accelerometers and many more. In this paper we argue that these devices are an ideal platform for collecting data for use in urgent computing simulations. We describe how these devices will have far reaching impacts on how people connect with and organize their communities and discuss how this coincides with the rise of community driven response to disasters and the need for decentralized command and control that is discussed in disaster management literature. We also that smartphones, by providing technology which can network people without the use of centralized infrastructure, and which are carried, used, and maintained as part of daily life, are a promising platform for building distributed disaster management applications, which could be part of the inputs provided to an urgent computing simulation. In this paper we describe not only the potential of the platform, but also analyze the challenges it faces in order to realize that potential, and discuss how our middleware is designed to meet these challenges and bring about the future of disaster management applications for smartphones using urgent computing.\"",
        "title: \"Coordination Requirements for Open Distributed Systems\" with abstract: \" ion).Inter--agent actions have to be cleanly separated from intra--agent actions in order todistinguish between the concerns of coordination on one hand and of computationson the other (Separation of Concerns).Last but not least, a coordination model should be based on a rigorous formal semanticsin order to allow reasoning about specifications.3. Commercially available systems.The RM--ODP model conceptually provides the basis for commercially available systems.In this model,... \"",
        "title: \"Network performance-aware collective communication for clustered wide-area systems\" with abstract: \"Metacomputing infrastructures couple multiple clusters (or MPPs) via wide-area networks. A major problem in programming parallel applications for such platforms is their hierarchical network structure: latency and bandwidth of WANs often are orders of magnitude worse than those of local networks. Our goal is to optimize MPI's collective operations for such platforms. We use two techniques: selecting suitable communication graph shapes, and splitting messages into multiple segments that are sent in parallel over different WAN links. To optimize graph shape and segment size at runtime, we introduce a performance model called Parameterized Log P ( P \u2212 Log P ) , a hierarchical extension of the Log P model that covers messages of arbitrary length. An experimental performance evaluation shows that the newly implemented collective operations have significantly improved performance for large messages, and that there is a close match between the theoretical model and the measured completion times. Keywords Grid computing MPI Collective communication for clustered wide-area systems 1 Introduction Research on global computational infrastructures has raised considerable interest in running parallel applications on wide-area distributed systems, often called metacomputers or computational grids [8,13] . Communication insensitive applications like the ones based on the master-worker paradigm can easily be deployed in grid environments [18] . However, writing applications with frequently communicating processes is much more difficult when targeting at grids rather than traditional parallel machines, due to the presence of different (local and wide-area) networks. As the wide-area links are orders of magnitude slower than the interconnects within clusters (or MPPs), metacomputers have a hierarchical structure. In earlier work [29,30] , we discussed how collective communication libraries can be used to simplify wide-area parallel programming. We implemented an MPI-compatible library, called MagPIe, which optimizes MPI's collective operations for wide-area systems. MagPIe exploits the hierarchical structure, resulting in much less wide-area communication than other MPI libraries [30] . MagPIe's existing implementation efficiently deals with the high WAN latency but has suboptimal performance with long messages which are more sensitive to WAN bandwidth. Collective communication with long messages needs a more detailed model, including latency and bandwidth of the local and wide-area networks, the number of clusters, the number of processors in each cluster, and the length of the messages. Therefore, we introduce a new performance model for wide-area collective communication, called the parameterized Log P model ( P \u2212 Log P ), which extends the Log P model [9] . We use this model to optimize collective communication using message segmentation and tree shape determination. We optimized four important collective operations: broadcast, scatter, gather, and allgather. We further describe heuristics with which the optimizations can be performed dynamically (at runtime), based on measured model parameters of the computing platform. We currently make some simplifying assumptions about the networks, as we use regular topologies and constant latencies and bandwidths. Assuming latency and bandwidth to be constant is certainly realistic for the duration of a single collective operation. In general, MagPIe's network performance data can be updated regularly during an application run. Our fast, heuristic optimizations are designed to cope with such dynamically changing information. Assuming a regular wide-area topology allows us to focus on the impact of network performance on the design of optimized collective operations. We have evaluated our model and the optimizations on an experimental wide-area testbed, both with simulated WAN links and with a real WAN, using clusters located at four universities in The Netherlands. Our wide-area simulator allows us to study the impact of wide-area performance quantitatively and in detail. Our experiments on the real system confirmed the simulator results qualitatively. Our experimental results are encouraging to further pursue our approach. For example, while using the simulator-based system for broadcasting messages to 7 remote clusters with a 1 Mb/sB/s link each, we achieved an aggregate bandwidth of 6.65 Mb/s, compared to 0.96 Mb/s without message segmentation. Furthermore, we compared theoretically estimated and actually measured completion times and found close matches; for example, with broadcast, the difference is between 1% and 4%. Comparing our optimization heuristics with offline, exhaustive searching revealed that the heuristics missed the global optima only in a few exceptional cases. In the broadcast example, differences were always below 1%. Our ultimate goal, however, is to develop an MPI library that does not have the mentioned limitations and that adapts at runtime to changing network conditions. We intend to use our performance model in combination with dynamic information about topology and network performance, the latter as provided by the Network Weather Service [38] . To allow such runtime decisions, our optimization algorithms themselves also are efficient and are executed on-line, as part of the MagPIe library. This paper is organized as follows. In Section 2 , we introduce our parameterized Log P model. In Section 3 , we apply it to optimize collective operations in hierarchical systems. We experimentally verify our approach using a real wide-area system in Section 4 . Related work will be discussed in Section 5 . Section 6 presents our conclusions. Appendix A summarizes the symbols used throughout the paper. 2 Modeling message-passing performance To motivate our performance model, we first analyze the communication behavior of the MPI implementation on our experimentation platform, called the DAS system. DAS consists of four cluster computers, each containing Pentium Pros that are locally connected by Myrinet [6] . The clusters are located at four Dutch universities and are connected by the Dutch academic Internet backbone, SURFnet. The system is described in detail on http://www.cs.vu.nl/das/ and in [30] . Fig. 1 illustrates the general system structure we assume throughout this paper, consisting of multiple clusters with fully connected local networks and a fully connected WAN. Each cluster has a gateway that is connected both to the LAN and to the WAN. MagPIe re-implements the collective operations of a given MPI implementation on top of MPI's point-to-point communication. We run MagPIe on top of MPICH [20] , a widely used public MPI implementation, which we have ported to the wide-area DAS system. Our MPICH port uses the Panda communication sublayer [2] and implements a Panda-specific MPICH device. Panda gives access to IP and Myrinet. On Myrinet, Panda uses the LFC [5] control program. One of the DAS clusters has 128 CPUs, and has been set up to allow easy experimentation with different WAN latencies and bandwidths, by adding delay loops to the networking subsystem [30] . This wide-area simulation is part of Panda and thus is transparent to software layers on top of it, like MPI. We use this simulation system for the performance experiments reported throughout Sections 2 and 3 . We use a varying number of clusters, and simulate realistic values for wide-area latency (10 ms), and for wide-area bandwidth (1 Mb/s). In comparison, the latency over Myrinet is about 20 \u03bc s and the MPI-level throughput is about 50 Mb/s, so there is almost two orders of magnitude difference between the local and the simulated wide-area network. We emphasize that our simulation results have been measured on a real parallel machine, using the same software on the compute nodes as in the real wide-area system; only the wide-area links are simulated over some of the Myrinet links. To achieve this, the software on the gateway uses a different option of Panda. In Section 4 , we verify our simulator-based results with experiments performed on the real wide-area DAS system. Throughout this paper, we assume all message data to be in contiguous data buffers. The MPI standard also allows so-called derived data types, which may have non-contiguous memory layout, possibly causing higher processing overhead for sender and receiver. However, this additional overhead depends on the data types in use, preventing a generally applicable performance model. The influence of derived data types on communication performance is a topic of its own [21] and not covered here. The granularity of message segmentation is affected by the layering of MagPIe on top of MPI's point-to-point communication. According to MPI's philosophy, MagPIe treats messages as vectors of elements of a base type. MagPIe constructs message segments from multiple vector elements. Hence, the granularity of message segments is the size of a single vector element. This is sufficient for common-case applications using data types of small size, like vectors of floating point numbers. However, derived data types might exceptionally reach sizes larger than a suitable message segment. In such a case, MagPIe has to use suboptimal segment sizes. This problem could only be resolved by implementing MagPIe's algorithms as an integral part of an MPI library. In that case, however, MagPIe could no longer be used across multiple heterogeneous systems using their respective native MPI implementations. Our MagPIe library uses point-to-point messages provided by the underlying MPI implementation. Thus, to obtain a realistic performance model for collective operations on a clustered wide-area system, we first study the performance of point-to-point communication by looking at the Log P parameters for a local network and a wide-area network. Fig. 2 shows send overhead and gap , measured for the MPI _ Isend routine for various message sizes on both networks, Myrinet and WAN. In analogy to the Log P model, the send overhead o is the completion time of MPI _ Isend for a given message size, while the gap g is the minimum time interval between consecutive calls to MPI _ Isend . We measured those parameters with the method described in Section 2.2 . The MPI standard prescribes that the non-blocking MPI _ Isend only initiates the send operation. The actual implementation is free to perform as much of the sending task as is convenient, as long as it guarantees that it will never wait for the receiving process. The application can later check whether the transfer has actually been completed. Our MPICH port to Panda sends until it reaches a point at which it would have to block, and then it returns. For short messages, this usually means that the entire message has been sent when MPI _ Isend returns. In fact, Fig. 2 shows that over Myrinet, MPI _ Isend behaves exactly like this up to a message size of 128 KB. At this size, MPICH switches to rendezvous mode; as MPI _ Isend would now have to wait for a reply message from the receiver, it returns immediately in this case. When a slow wide-area network is used instead of the fast Myrinet, the behavior is similar, except that between 64 and 128 KB, MPI _ Isend returns as soon as it would block waiting for flow-control information from the receiver. The corresponding gap values are as expected. They start rather \u201cflat\u201d for short messages and get into a linear increase for sufficiently large messages. On the WAN, there is another non-linearity at 128 KB when the send mode changes. We can draw several conclusions from these measurements, which are useful for developing a realistic performance model. On Myrinet, the difference between overhead o and gap g is small (up to 128 KB), indicating that the end-to-end bandwidth is limited by the host computers and not by the network. On the wide-area link, on the other hand, the difference between o and g is two orders of magnitude for small messages. With this huge difference, Log P's assumption that a sender cannot transmit a message faster than g time units after a preceding message is much too pessimistic and yields misleading results for collective communication. This assumption is only true if the next message follows the same network links. With collective operations, a sender typically sends messages to different destinations in a row, so the next send can already start after o time units, but not earlier than g time units as constrained by the local area network. Another important observation from Fig. 2 is that o and g depend not only on message size and network bandwidth, but also on the behavior of the underlying MPI implementation. Collective operations thus have to be optimized carefully. Neither the assumption that MPI _ Isend will always return \u201cimmediately\u201d nor linear approximations for o and g in general give realistic performance models. 2.1 Parameterized Log P ( P \u2212 Log P ) Based on these observations, we now present the parameterized Log P model ( P \u2212 Log P ) . For a single-layer network, it defines five parameters. P is the number of processors. L is the end-to-end latency from process to process, combining all contributing factors such as copying data to and from network interfaces and the transfer over the physical network. os ( m ), or ( m ), and g ( m ) are send overhead, receive overhead, and gap. They are defined as functions of the message size m . os ( m ) and or ( m ) are the times the CPUs on both sides are busy sending and receiving a message of size m . For sufficiently long messages, receiving may already start while the sender is still busy, so os and or may overlap. The gap g ( m ) is the minimum time interval between consecutive message transmissions or receptions along the same link or connection. It is the reciprocal value of the end-to-end bandwidth from process to process for messages of a given size m . Like L , g ( m ) covers all contributing factors. From g ( m ) covering os ( m ) and or ( m ), follows g ( m )\u2a7e os ( m ) and g ( m )\u2a7e or ( m ). A network N is characterized as N =( L , os , or , g , P ). To illustrate how the parameters are used, we introduce s ( m ) and r ( m ), the times for sending and receiving a message of size m when both sender and receiver simultaneously start their operations. s ( m )= g ( m ) is the time at which the sender is ready to send the next message. Whenever the network itself is the transmission bottleneck, os ( m )< g ( m ), and the sender may continue computing after os ( m ) time. But because g ( m ) models the time a message \u201coccupies\u201d the network, the next message cannot be sent before g ( m ) time. r ( m )= L + g ( m ) is the time at which the receiver has received the message. The latency L can be seen as the time it takes for the first bit of a message to travel from sender to receiver. The message gap adds the time after the first bit has been received until the last bit of the message has been received. Fig. 3 illustrates this model. When a sender transmits several messages in a row, the latency will contribute only once to the receiver completion time but the gap values of all messages sum up. This can be expressed as r ( m 1 ,\u2026, m n )= L + g ( m 1 )+\u22ef+ g ( m n ). For completeness, we show that parameterized Log P subsumes the original model Log P [9] and its version for long messages, Log GP [1] . In Table 1 , Log GP's parameters are expressed in terms of parameterized Log P. We use 1 byte as the size for short messages; any other reasonable \u201cshort\u201d size may as well be used instead. Note that neither Log P nor Log GP distinguishes between os and or . For short messages, they use r = o + L + o to relate the L parameter to receiver completion time which gives L a slightly different meaning compared to parameterized Log P. We use this equation to derive Log P's L from our own parameters. For clustered wide-area systems, we use two parameter sets, identified by a subscript l for the LAN and w for the WAN. For example, L l denotes the latency within a cluster and L w is the latency when the sender and receiver are in different clusters. For a local network we get: s l (m)=g l (m), r l (m)=L l +g l (m). A wide-area transmission takes three steps: the sender forwards the message to its local gateway, which in turn sends the message to the gateway of the receiver's cluster, which finally forwards the message to the receiving node. The value of r w always depends on the wide-area bandwidth and can be expressed in analogy to r l . The value of s w may either be determined by wide-area overhead os w ( m ) or local-area gap g l ( m ), whichever is higher: the sender cannot send the next message before g l ( m ) time, but it might have to wait even longer, for example while waiting for flow-control information from its wide-area peer. Unlike the local-area case, the sender is decoupled from the wide-area gap. This gives us the following equations for the wide-area case: s w (m)= max g l (m),os w (m) , r w (m)=L w +g w (m). 2.2 Efficient parameter measurement for P \u2212 Log P An important issue is how to measure the P \u2212 Log P parameters described above on an actual wide-area system. Previous Log P micro benchmarks [10,23] measure the gap values by saturating the link for each message size. However, this approach is too intrusive to be feasible for wide-area links which are typically shared with many other users. Our method has to use saturation only for obtaining g (0), which can be done fast and thus is hardly intrusive. In [28] we showed that our measurement procedure, compared to the saturation-based method, yields the same results while reducing the link utilization by 90\u201395%. Below, we summarize this method. As we use g (0) for deriving other values, we measure it first using two processes, measure and mirror (see Fig. 4 ). We measure the time RTT n (0) for a roundtrip consisting of n messages sent in a row by measure, and a single, empty reply message sent back by mirror. We take the time measured (after subtracting RTT 1 (0)/2 for the reply) as n \u00b7 g (0). The procedure first uses n =1 and then n =10. From this value on n is doubled until the gap per message changes only by \u03f5 =1%. At this point, saturation is assumed to be reached and we compute the gap from the measurement with the so-far largest n . We start with a small value for n (to speed up the measurement) and double it until the roundtrip time is dominated by bandwidth rather than latency, namely until RTT 1 (0)< \u03f5 \u00b7 RTT n (0) holds in addition to the saturation test. By waiting for a reply we enforce that the messages are really sent to mirror instead of just being buffered locally. All other parameters can be determined by the procedure shown in Fig. 4 . It starts with a synchronization message by which the mirror process indicates being ready. For each size m , two message roundtrips are necessary from measure to mirror and back. (We use RTT ( m )= RTT 1 ( m ).) In the first roundtrip, measure sends an m -bytes message and in turn receives a zero-bytes message. We measure the time for just sending and for the complete roundtrip. The send time directly yields os ( m ). g ( m ) and L can be determined by solving the equations for RTT (0) and RTT ( m ), according to the timing breakdown in Fig. 3 : RTT(0)=2(L+g(0)), RTT(m)=L+g(m)+L+g(0), g(m)=RTT(m)\u2212RTT(0)+g(0), L=(RTT(0)\u22122g(0))/2. In the second roundtrip, the measure process sends a 0-bytes message, waits for \u0394 > RTT ( m ) time, and then receives an m -bytes message. Measuring the receive operation now yields or ( m ), because after \u0394 > RTT ( m ) time, the message from mirror is available at measure for immediate receiving. For each message size, the roundtrip tests are initially run a small number of times. As long as the variance of measurements is too high, we successively increase the number of roundtrips until a sufficiently small (90%) confidence interval is obtained, or until an upper bound on the total number of iterations is reached (60 for small messages, 15 for large messages). In the latter case, we trade accuracy for non-intrusiveness. Initially, measurements are performed for all sizes m =2 k with k \u2208[0, k m ]. The value of k m has to be chosen large enough to cover any non-linearity caused by the tested software layer. In our experiments, we used k m =18 to cover all changes in send modes of the assessed MPI implementation (MPICH). After measuring the initial set of message sizes, we check whether the gap per byte ( g ( m )/ m ) has stabilized for large m . If this is not the case, sending larger messages may achieve lower gaps (and hence higher throughput). So k m is incremented and the next message size is tested. This process is performed until g (2 k m ) is close (within \u03f5 ) to the value linearly extrapolated from g (2 k m \u22122 ) and g (2 k m \u22121 ). So far, the \u201cinteresting\u201d range of message sizes has been determined. Finally, possible non-linear behavior remains to be detected. For any size m k , we check whether the measured values for os ( m k ), or ( m k ), and g ( m k ) are consistent with the corresponding predicted values for size m k , extrapolated from the measurements of the previous two (smaller) message sizes, m k \u22121 and m k \u22122 . If the difference is larger than \u03f5 , we do new measurements for m =( m k \u22121 + m k )/2, and repeat halving the intervals until either the extrapolation matches the measurements, or until m k \u2212m k\u22121 \u2a7d max (32 B ,\u03f5\u00b7m k ) . The measurement procedure described above assumes that network links are symmetrical, such that sending from measure to mirror has the same parameters as for the reverse direction. However, this assumption may not always be true. On wide-area networks, for example, the achievable bandwidth (the gap) and/or the network latency may be different in both directions, due to possibly asymmetric routing behavior or link speed. Furthermore, if the machines running the measure and mirror processes are different (like a fast and a slow workstation), then also the overhead for sending and receiving may depend on the direction in which the message is sent. In such cases, the parameters os , or , and g may be measured by performing our procedure twice, while switching the roles of measure and mirror in between. Asymmetric latency can only be measured by sending a message with a timestamp t s , and letting the receiver derive the latency from t r \u2212 t s , where t r is the receive time. This requires clock synchronization between sender and receiver. Without external clock synchronization (like using GPS receivers or specialized software like the network time protocol , NTP), clocks can be synchronized either by statistical time estimation or by simple message exchange protocols. Statistical time estimation [32] is highly intrusive to the network and thus not feasible for WANs. Simple message exchange protocols allow synchronization only up to a granularity of the roundtrip time between two hosts [34] , which is useless for measuring network latency. Unfortunately, as we cannot generally assume the clocks of (possibly widely) distributed hosts to be tightly synchronized, asymmetric network latencies cannot be measured, neither with our framework nor with previous benchmarks [10,23] . 3 Performance-aware collective communication We now illustrate how the P \u2212 Log P model can be used to optimize collective communication. We discuss four important operations: broadcast, scatter, gather, and allgather. With broadcast, a single process (called the root ) sends a message to all other processes. Scatter is also known as personalized broadcast. Here, the root splits a large message vector and sends individual messages to the other processes. Gather is the inverse operation of scatter. Here, the root collects messages from its peer processes into a large message vector. Finally, allgather is defined like a gather operation, except that all processes receive the result, instead of just the root. For the exchange of data this implies that for allgather, all processes conceptually have to broadcast a message. 3.1 Broadcast With broadcast, a single process (called the root) sends a message of size M to all other ( P \u22121) processes. Optimal broadcast algorithms use tree-shaped communication graphs, so every process receives the message exactly once [25] . MagPIe's original broadcast algorithm was optimized for wide-area networks by sending the message only once to each cluster and by avoiding transmission paths that contain more than one wide-area link. In MagPIe's algorithm, one of the application processes is selected in each cluster to act as a so-called coordinator node. The root process acts as coordinator of its own cluster. First, the root sends the message to the other coordinator nodes, forming a flat tree in the WAN. As soon as a coordinator receives the message, it forwards it to the other nodes of its cluster, using a binomial tree shape in the LANs. A disadvantage of MagPIe's original algorithm is that it forwards complete messages down the spanning tree [30] . For large messages, this leads to poor link utilization. As large messages have a high send overhead, the root can only send over one WAN link at a time. Our goal for the optimized algorithms is to use the accumulated bandwidth of several (or all) available WAN links. Because a cluster gateway decouples LAN and WAN packets, the root can quasi-simultaneously send small packets over up to n =\u230a g w ( m )/ g l ( m )\u230b WAN links. We use message segmentation to achieve such a better link utilization. Instead of forwarding complete messages down the spanning tree, we split each message into k segments of size m (where k =\u2308 M / m \u2309) and forward each incoming segment down all links. In this way, there will be much more overlap in communication over different links. In addition to message segmentation, we optimize the shape of the spanning tree by trading send overhead for latency. We try to minimize the total completion time of the broadcast (i.e., the time when the last receiver has obtained the complete message). This optimization problem can be stated more accurately as follows: Given a network N and the message size M , our goal is to find a tree shape and a segment size m that together minimize the completion time. We approach this problem in two steps. First, we develop a single-layer broadcast algorithm and its performance model, assuming that all links have the same speed. This algorithm can be used to optimize either communication within a cluster (i.e., all links are fast local networks) or communication between nodes in different clusters (i.e., all links are slow wide-area networks). Next, we develop a two-layer algorithm for the hierarchical model of metacomputers described in Section 1 . Two-layer algorithms are more complicated to model, but are more efficient for broadcast operations. 3.1.1 Single-layer broadcast The optimal tree shape depends on the network parameters, as well as on M and m . In general, we characterize a tree by two parameters, its height h and its degree d . h is the longest path from the root to any of the other nodes, determined by counting edges. d is the maximum number of successor nodes of any node in the graph. For example, MagPIe's flat WAN tree has d = P \u22121 and h =1, where P in this case denotes P w , the number of clusters. Depending on the actual network parameters, the optimal tree shape (yielding the minimal completion time) can have any d , h \u2208[1\u2026 P \u22121]. Fortunately, d and h of the optimal tree are related to each other, and we can compute the minimal height of a tree with P nodes and degree d as the smallest h \u2a7e1 for which \u2211 i =0 h d i \u2a7e P is true. Similar to the parameters for latency L and gap g ( m ) for a single message send, we define latency \u03bb ( m ) and gap \u03b3 ( m ) of a broadcast tree. Here, \u03bb ( m ) denotes the time at which a message segment has been received by all nodes, after the root process started sending it. \u03b3 ( m ) is the time interval between the sending of two consecutive segments. ( \u03b3 ( m ) hence indicates the throughput of a broadcast tree.) We compute the completion time T of a broadcast algorithm with k message segments of size m as: T= (k\u22121)\u00b7\u03b3(m)+\u03bb(m). To illustrate this formula, we can express the values for \u03b3 ( m ) and \u03bb ( m ) for MagPIe's flat WAN tree as: \u03b3(m)= max g(m),(P\u22121)\u00b7s(m) , \u03bb(m)=(P\u22122)\u00b7s(m)+r(m). Here, \u03b3 ( m ) is the maximum of the gap between two segments of size m sent on the same link and the time the root needs for sending ( P \u22121) times the same segment on disjoint links. The corresponding value for \u03bb ( m ) is the time at which a message segment is sent to the last node, plus the time until it is received. For a general tree shape, upper bounds for both parameters can be expressed depending on the degree d and height h of a broadcast tree: \u03b3(m)\u2a7d max g(m),or(m)+d\u00b7s(m) , \u03bb(m)\u2a7dh\u00b7 (d\u22121)\u00b7s(m)+r(m) . \u03b3 ( m ) is the maximum of the gap caused by the network, and the time a node needs to process the message. For intermediate nodes, this is the time to receive the message plus the time to forward it to d successor nodes. (For the root and for leaf nodes, it is either one of both.) The exact value of \u03bb ( m ) depends on the order in which the root process and all intermediate nodes send to their successor nodes and which path leads to the node that receives the message last. For the rest of this paper, we approximate \u03b3 ( m ) and \u03bb ( m ) by their upper bounds, given by the inequalities. The optimal broadcast tree depends on the number of processors P and the message size M . Both values are parameters of MPI _ Bcast . This implies that the optimal broadcast tree and segment size have to be computed at runtime for each invocation of MPI _ Bcast . Optimization at runtime has to be very fast, so it does not outweigh the performance improvement of applying the optimized algorithm. Therefore, we avoid communication between processes and we avoid doing an exhaustive search over the complete search space with m \u2208[1\u2026 M ] and d \u2208[1\u2026 P \u22121]. Communication is avoided by replicating the network performance information over all application processes. When calling MPI _ Bcast , all processes simultaneously compute the optimal tree and segment size. We apply heuristics to reduce the number of segment sizes and tree shapes to be investigated. In general, several segment sizes are tried out, and for each size the optimal tree shape is computed. For a given segment size m , we investigate only the following tree shapes: 1. d \u2208[\u230a g ( m )/ s ( m )\u230b\u2026 P \u22121]. Lower values for d can only lead to higher completion times, because less accumulated bandwidth would be used. 2. Starting with d =\u230a g ( m )/ s ( m )\u230b, we increase d while only investigating those values of d for which the height h will be reduced. Values for d in between would increase \u03b3 ( m ) but would not decrease \u03bb ( m ), and could thus not improve completion time. For the segment size, we only evaluate \u201cuseful\u201d values. The segment size must be a multiple of the size of the basic data type to be transmitted (in MPI terms, the extent) and it must split the initial message into k equal segments. The segment size m is determined in two steps: 1. In a binary search, segment sizes m=M/2 i , i\u2208[0\u2026 log 2 M] are investigated. For each value for m , the degree d with minimal completion time is determined. 2. Starting from the best value m \u2032 found in step 1, a local hill-climbing strategy is performed. k \u2032 =\u2308 M / m \u2032 \u2309 is the so-far best known number of segments. With the respectively best value for d , the completion times are computed for k \u2032 \u22125, k \u2032 \u22121, k \u2032 +1, and k \u2032 +5. We then replace k \u2032 by the value with the best completion time. This is performed until no further improvement can be found. The simultaneous look-ahead of 1 and 5 steps helps overcoming local minima. Furthermore, it speeds up the process when a minimum is far away from the starting point computed in step 1. Having identical links inside a network, the actual broadcast tree can be constructed from d in O ( P ) time by keeping track of each node's degree while assigning receivers to senders. 3.1.2 Two-layer broadcast So far, we have optimized broadcast for single-layer networks. Given optimized broadcast trees for the WAN and for the LANs, a simple way to obtain a two-layer algorithm is the sequential composition, as performed in the original MagPIe library. Here, the coordinator nodes first participate in the wide-area broadcast. Then, they forward the message inside their clusters. This leads to a total completion time of: T=T w +T l =(k w \u22121)\u00b7\u03b3 w (m w )+\u03bb w (m w )+(k l \u22121)\u00b7\u03b3 l (m l )+\u03bb l (m l ). A better integration into a two-layer broadcast can be achieved by pipelining both phases. Here, the coordinator nodes immediately forward segments, first to other coordinators, and then to their successor nodes in the LAN tree structure. We use the same segment size for the WAN and for the LANs, so coordinators do not need to re-assemble segments. This leads to: T=(k\u22121)\u00b7\u03b3(m)+\u03bb w (m)+\u03bb l (m). To reflect the double forwarding in the coordinator nodes, we use: \u03b3(m)\u2a7d max g w (m),or w (m)+d w \u00b7s w (m)+d l \u00b7s l (m) . The optimization of the two-layer broadcast for a given m additionally requires taking care of \u03bb l ( m ) and \u03b3 \u2032( m ), the latter denoting the fraction of \u03b3 ( m ) that can be used for the LAN without slowing down the WAN. We then have to investigate the LAN trees with d \u2208[1\u2026\u230a \u03b3 \u2032( m )/ s l ( m )\u230b] with: \u03b3 \u2032 (m)= max g w (m)\u2212or w (m)\u2212d w \u00b7s w (m),s l (m) . 3.1.3 Performance evaluation We have implemented a two-layer broadcast as described so far as part of the MagPIe library. Fig. 5 compares the completion times of this new algorithm with MagPIe's original broadcast, measured on the DAS experimentation system described in Section 2 . The new algorithm does message segmentation, and pipelines WAN and LAN forwarding. The original algorithm sends complete messages, and sequentially combines WAN and LAN forwarding. We measured configurations with four clusters (with 1 or 16 CPUs) and eight clusters (with 1 or 8 CPUs). The number of CPUs per cluster has hardly any effect on the overall completion time, causing the respective pairs of lines in Fig. 5 to be almost the same. This, and the fact that for short messages the original algorithm performs approximately as fast as the segmented broadcast, both support our original design goals for MagPIe. For larger messages, however, the new algorithm is much faster than the original one and achieves much higher aggregate wide-area bandwidth. For example, with eight clusters of 1 CPU each, broadcasting a 1 MB message takes 1079 ms with segmentation, and 7425 ms without. This corresponds to an aggregate bandwidth of 6.65 Mb/s for segmentation. This is 95% of the actually available bandwidth which is 7\u00d71 Mb/s. Without segmentation, the wide-area links are used sequentially, resulting in an aggregate bandwidth of only 0.96 Mb/s, or 14% of what is available. With fragmentation, the completion times for four and eight clusters are about the same (for 1 MB messages, 1072 ms with four clusters and 1079 ms with eight clusters), whereas the original algorithm takes much longer for eight clusters than for four clusters (for 1 MB messages, 3200 ms with four clusters and 7425 ms with eight clusters). This clearly shows that segmentation allows us to use all available wide-area links in parallel as long as the local-area network has enough bandwidth to feed them all. Although hard to see from the graphs, the pipelined message forwarding further reduces the overall completion time. With the original algorithm, the sequential combination of WAN and LAN broadcast adds some additional time. With 4 MB messages and four clusters, for example, the forwarding takes 13224.3\u221212723.3=501 ms. This overhead disappears with pipelining. We also investigated the quality of our theoretical estimates. Fig. 6 shows the estimation error, namely the difference between estimated and measured completion times. For short messages it is up to 4%, whereas for large messages the deviation is less than 1%. In general, this is a very close match between our performance model and the implementation. The slightly larger difference for short messages is mostly due to the fact that our measurements also include the time for computing the optimized tree. We also compared the difference between the best heuristically found completion times and the global optima, which were computed offline by an exhaustive search. Our heuristics found the global optimum in almost all test cases, in the few exceptional cases differences were always below 1%. 3.2 Scatter The second collective operation we discuss is scatter, which is also called personalized broadcast. With scatter, the root process holds M \u00b7 P data items which it equally distributes across the P processes, including itself. With homogeneous networks, optimal scatter algorithms use flat trees as communication graphs. (Because each processor has to receive a personalized message, forwarding does not help improving the completion time.) Only for very small messages, message combining and forwarding via coordinator nodes may improve performance. However, we do not investigate message combining here, because we focus on large messages. Adding message combining to our performance model would be straightforward, by using segment sizes m \u2208[ M \u2026 M \u00b7 P ] over the WAN and a two-level algorithm, as for broadcast. With a fixed communication graph (a flat tree encompassing all nodes in all clusters), our analytical model combines both WAN and LAN parameters. We optimize scatter algorithms by finding a message segment size that yields the shortest overall completion time. P w denotes the number of clusters and P l the (maximum) number of processes per cluster. Analogous to broadcast, we model T =( k \u22121)\u00b7 \u03b3 ( m )+ \u03bb ( m ) with: \u03b3(m)\u2a7dP l \u00b7 max (g w (m),(P w \u22121)\u00b7s w (m)+s l (m))+or l (m), \u03bb(m)\u2a7d(P l \u22121)\u00b7 max (g w (m),(P w \u22121)\u00b7s w (m)+s l (m)) +(P w \u22121)\u00b7s w (m)+r w (m). \u03b3 ( m ) models the time spent at the root process, because for other processes, the gap per segment is much smaller, namely or ( m ). The model assumes that the message segments are first sent to the first processor in each cluster, then to the second processor in each cluster, etc., using the wide-area links in a round-robin fashion. Before the root process can send out the next segment, it has to receive the message segment it just sent to itself. \u03bb ( m ) denotes the time at which the last message of a segment round is sent, plus its receiving time. This sending time adds the time for sending the segment to all but one processors in each cluster, and the time for sending to the last processor in all but one clusters. \u03bb ( m ) equals its upper bound when the last message is sent to a process in a remote cluster. Otherwise it is somewhat less, depending on which message is actually received last. As with broadcast, we use the upper bounds for optimizing T and use the binary search with subsequent hill climbing for finding a near-optimal segment size. Fig. 7 compares the completion times of the new scatter algorithm with MagPIe's original scatter. The new algorithm does message segmentation whereas the original algorithm sends complete messages. We measured the same configurations as with broadcast. The new algorithm performs much better, because it achieves higher aggregate WAN bandwidth. For example, with eight clusters of 1 CPU each, sending a 1 MB message to each receiver takes 1078 ms with segmentation, and 7437 ms without. The completion times for large messages are proportional to g w ( M )\u00b7 P l , which indicates that almost all available WAN bandwidth can be utilized. Fig. 8 compares the theoretically estimated completion times with the ones actually measured on our system. For small messages, the estimation error is slightly worse than for broadcast (up to 14%), but for large messages it is only about 1%, denoting a very close match between theory and practice. As with broadcast, our search heuristics find the global optimum in almost all test cases, with a few negligible deviations (smaller than 1%). 3.3 Gather Gather is the inverse operation of scatter. Here, each process (including the root) holds M data items which are sent to the root. Analogous to scatter, optimal algorithms use flat trees. The completion time can be modeled as T =( k \u22121)\u00b7 \u03b3 ( m )+ \u03bb ( m ) with: \u03b3(m)= max P l \u00b7g w (m),P l \u00b7or l (m)+(P w \u22121)\u00b7P l \u00b7or w (m)+os l (m) , \u03bb(m)= max L w +P l \u00b7g w (m), max (L l ,os l (m))+P l \u00b7or l (m) +(P w \u22121)\u00b7P l \u00b7or w (m) . \u03b3 ( m ) and \u03bb ( m ) model the time at which the root process completes receiving the message segments. \u03b3 ( m ) is the maximum of the time the segments need to cross the WAN links, and the time the root needs to receive all messages of a segment round, plus sending its own segment to itself. \u03bb ( m ) is similar, but models the overlap of latency and receiving at the root. Fig. 9 shows the measured completion times for the same configurations as with broadcast and scatter. We compare MagPIe's original algorithm with the new segmenting variant. The results are similar to the ones achieved with MPI _ Scatter . This time, message segmentation helps simultaneously receiving on all connections, leading to better WAN bandwidth utilization. Fig. 10 compares the theoretically estimated completion times with the ones actually measured on our system. As with broadcast and scatter, the estimation errors are mostly negligible, except for a few cases with large messages and many simultaneously sending nodes. Here, some effects related to contention inside the receiving node cause somewhat higher completion times than expected by the theoretical model. As with broadcast and scatter, the search heuristics hardly ever miss the globally optimal configurations. 3.4 Allgather Allgather is like the gather operation, except that all data are delivered at every node, and not just at a single root. One simple way to implement allgather is as a sequence of gather operations, using every node as root. However, in this way the scarce bandwidth available at the WAN links is not used efficiently. An alternative, which is currently implemented in MagPIe, is to first let the coordinator of every cluster gather the data of all local nodes; next exchange that data with all other cluster coordinators; and finally let the coordinators broadcast the whole data vector locally in their clusters. This approach works particularly well when there is a big difference in performance in the LAN and WAN networks, since pipelining issues (as encountered in the optimization of the broadcast implementation) are much less important than the efficient use of the slow WAN links. The completion time of this implementation can be modeled as the sum of local gather, wide-area allgather (between the coordinator nodes), and the local broadcast. The time for gather and broadcast has been outlined above; in the following we only deal with the allgather operation between the coordinator nodes. Its completion time is T =( k \u22121)\u00b7 \u03b3 ( m )+ \u03bb ( m ) with: \u03b3(m)= max (P w \u22121)\u00b7(s w (m)+ max g l (m),or w (m) ,g w (m) , \u03bb(m)=(P w \u22122)\u2217s w (m)+g w (m)+L w . The segmenting allgather consists of k rounds in which each node sends a segment to all its peers and receives one segment per peer. \u03b3 ( m ) thus is the maximum of the wide-area gap and the processing time per segment, the latter is the number of peers multiplied by the sum of the completion time for send and receive. The receive completion time is the maximum of the local gap and the receive overhead. \u03bb ( m ) is, as usual, the time when a node sends the last message, plus the time to deliver it to the respective receiver. We compared MagPIe's original implementation of MPI _ Allgather with one that uses segmentation in the central exchange phase between the cluster coordinators. The results for four and eight clusters are shown in Fig. 11 . For eight clusters, and using a segment size smaller than 64 K, the optimized version avoids performance loss due to flow control issues for message sizes between 64 and 128 K. However, for most other message sizes, and for four clusters, the results are very similar, since the original MagPIe implementation is already keeping the WAN links well occupied. Finally, Fig. 12 compares (for the central phase between the coordinator nodes) the theoretically estimated completion times with the ones actually measured on our system. For a few configurations, the estimation error is up to 16% which can be explained by contention inside the nodes due to simultaneously receiving from multiple peers. 4 Experimental results on the real wide-area system The results presented so far have been obtained using the wide-area simulator of our Panda communication sublayer. The simulator allows to perform measurements on a real parallel machine with only the wide-area links being simulated, by adding delay loops in the Panda gateway nodes. With the simulator, we were able to investigate our collective communication operations in a clean environment without interference of network traffic caused by other users. This allowed us to quantitatively analyze our results. In this section, we present experimental results from the real system, the four DAS clusters located at Vrije Universiteit Amsterdam (VU), at Universiteit Leiden, the University of Amsterdam (UvA), and Delft University of Technology. The clusters are connected via the Dutch academic Internet backbone (SURFnet). Table 2 summarizes the average TCP-level bandwidth (in Mb/s) and one-way latency (in ms) between the DAS clusters. As can be seen in the table, the latency between the clusters varies from 1.5 to 4.0 ms. Bandwidth ranges from 3.0 to 28.0 Mb/s. These values for bandwidth and latency indicate that, with the DAS, wide-area communication is somewhat faster than in our simulated environment. This can be attributed to the rather small physical distances (up to 50 km) between the clusters. Additionally, the network parameters vary over time due to concurrent network traffic. Furthermore, the wide-area links are asymmetric due to the routing setup. Therefore, the purpose of the experiments presented in this section is to qualitatively verify our results from Section 3 on a real wide-area system. The collective communication operations presented in Section 3 are based on P \u2212 Log P parameters for both local-area and wide-area networks. Fig. 13 shows send overhead and gap on four of the 12 links between the DAS clusters. The two graphs on the right-hand side of Fig. 13 (VU to UvA and VU to Delft) closely resemble the measurements on the simulated WAN in Fig. 2 . However, the sender can transmit messages larger than 64 KB without stalling while waiting for flow-control information from the receiver. This is due to the lower wide-area latency. Unlike our clean simulated environment, the Leiden cluster is connected to the rest of the DAS via a very lossy link. The frequent packet losses not only degrade bandwidth, they also lead to various artefacts with the measured gap values. But even with such a lossy link, the shape of the measured curves qualitatively matches the results from the simulated environment. The collective algorithms presented in Section 3 assume a homogeneous wide-area network with identical P \u2212 Log P parameters for all links. However, as shown in Table 2 , the wide-area links of the DAS perform somewhat differently from each other. For measuring the collective operations on the real wide-area system, we have used P \u2212 Log P parameters according to the slowest available link (from Leiden to Delft). With this conservative approach, message segmentation and tree shape are determined according to the \u201cbottleneck\u201d WAN link. Fig. 14 shows the completion times of the four collective operations (broadcast, scatter, gather, and allgather) across the four DAS clusters. As in Section 3 , we measured completion times for four clusters with 1 and with 16 processors each. The results qualitatively confirm the simulator measurements. Due to the somewhat faster wide-area links, however, the effects are less pronounced. But still, starting with a message size between 16 and 64 KB, the implementation based on message segmentation shows better performance than MagPIe's original implementation. 5 Related work Log P [9] and Log GP [1] are direct precursors of parameterized Log P. Having constant values for overhead and gap, Log P is restricted to short messages whereas Log GP adds the gap per byte for long messages, assuming linear behavior. Neither of them handles overhead for medium-sized to long messages correctly, nor do they model hierarchical networks. Within their limitations, they have been used to study collective communication [1,4,9,25] . The parameterized communication model [33] has two parameters which also depend on message size, resembling P \u2212 Log P 's sender and receiver completion times. Unfortunately, the model fails to adequately capture receiver overhead, as needed by the collective operations discussed in this paper. Bruck et al. [7] studied broadcast and allreduce in homogeneous networks using measured machine parameters and the postal model. Santos [35] studies optimality of k -item broadcast algorithms in a theoretical, simplified Log P variant. The problem of a k -item broadcast comes close to broadcasting large messages split into k smaller segments as presented in this paper. Multicast based on packetization is studied in [26] where the underlying network determines the size of the data items being delivered to multiple receivers. In a previous paper, we focussed on optimizing MPI's broadcast operation by message segmentation and tree shape optimization [27] . Van de Geijn et al. studied efficient pipelined implementations of broadcast [37] and reduction [36] on homogeneous one-level networks, like meshes and hypercubes. Some work has been performed on optimizing single collective operations (e.g., broadcast) for clusters of SMPs which (like wide-area networks) also exhibit hierarchical structures [16,22] , however without using message segmentation to accommodate hierarchical networks. Others study collective operations in networks of heterogeneous workstations rather than hierarchically structured systems [3,31] . Compositions of collective operations are optimized in [17] , but performance is studied only in the homogeneous case. Our work currently makes simplifying assumptions about the wide-area networks. Karonis et al. [24] apply optimizations to MPI's broadcast for hierarchical systems with more than two layers. The authors show performance improvements compared to the non-segmenting version of MagPIe for a system with three hierarchy layers. However, identifying a hierarchical system representation seems to be a challenging problem for the general case of Internet-based metacomputing platforms. Several metacomputing projects are currently building the infrastructure on top of which our MagPIe library may utilize distributed computing capacity [11,12,14,19] . The Interoperable MPI Protocol (IMPI) [15] specifies collective communication algorithms for clustered systems while focusing on interoperability rather than performance. 6 Conclusions Earlier research has shown that many parallel applications can be optimized to run efficiently on a hierarchical wide-area system and that collective communication is a useful abstraction to do some of these optimizations transparently. In this paper, we described two important optimizations for such wide-area collective operations. First, we use message segmentation to split messages into smaller units that can be sent concurrently over different wide-area links, resulting in better link utilization. Second, we determine the tree shape for the collective operations based on properties of the underlying system (such as the LAN and WAN performance and the processor to cluster mapping), instead of using a fixed shape. Both optimizations reduce the completion time for large messages. For both optimizations, we need a performance model that can accurately estimate the completion time of collective operations. Most existing (Log P-based) models are inaccurate for collective operations on hierarchical systems with fast local networks and slow wide-area networks. We described a new model, the P \u2212 Log P (parameterized Log P) model, which has different sets of Log P parameters for both networks. Also, our model makes these parameters a function of the message size, and uses measured values as input. We have used the P \u2212 Log P model to optimize four important operations in our MagPIe library (broadcast, scatter, gather, and allgather). The new library computes a near-optimal segment size and tree shape at runtime, based on various system properties. The optimization algorithms use heuristics to prune the search space, so they are efficient and could be used with dynamic information (such as produced by NWS [38] ) as input, although our current implementation uses static network performance data. We have empirically validated our approach. Experiments show that the new algorithms significantly improve collective performance for large messages. In the controlled simulator environment, our experiments show that the performance model accurately predicts the completion times. Our algorithms improve performance also on the real wide-area system where performance information is imprecise. We think the new algorithms are an important step towards a convenient, easy-to-use infrastructure for parallel programming on wide-area systems like computational grids. The current library can be used for programming hierarchical systems with similar network performance on the wide-area links, like our DAS system. Other possible target platforms are clusters of SMP nodes that have a similar network hierarchy. The techniques developed here also are a good basis for an MPI library that adapts itself dynamically to changing conditions in the networks. Building such a library is the next step in our research. Acknowledgements This work is supported in part by a USF grant from the Vrije Universiteit. The wide-area DAS system is an initiative of the Advanced School for Computing and Imaging (ASCI). We thank Peter Merz (University of Siegen) for his valuable advice on fast optimization techniques and Gr\u00e9gory Mouni\u00e9 for his contributions to this paper. Finally, we thank John Romein for keeping the DAS in good shape. Appendix A Symbols used in analytical modeling M total message size m message segment size k number of message segments, k= M/m L network latency os(m) send overhead for message of size m or(m) receive overhead for message of size m g(m) gap between two messages of size m P number of processors N network, N=(L,os,or,g,P) T completion time of a collective operation s(m) sender completion time for message of size m r(m) receiver completion time for message of size m RTT(m) round-trip time for message of size m (empty reply message) h height of a broadcast tree d degree (fan out) of a broadcast tree \u03bb(m) latency of a broadcast tree with message of size m \u03b3(m) gap of a broadcast tree with message of size m l subscript, used to identify LAN parameters w subscript, used to identify WAN parameters References [1] A. Alexandrov, M.F. Ionescu, K.E. Schauser, C. Scheiman, Log GP: incorporating long messages into the Log P model \u2013 one step closer towards a realistic model for parallel computation, in: Proceedings of the Symposium on Parallel Algorithms and Architectures (SPAA), Santa Barbara, CA, July 1995, pp. 95\u2013105 [2] H. Bal R. Bhoedjang R. Hofman C. Jacobs K. Langendoen T. R\u00fchl F. Kaashoek Performance evaluation of the Orca shared object system ACM Trans. Comput. Syst. 16 1 1998 1 40 [3] M. Banikazemi, V. Moorthy, D. Panda, Efficient collective communication on heterogeneous networks of workstations, in: International Conference on Parallel Processing, Minneapolis, MN, August 1998, pp. 460\u2013467 [4] M. Bernaschi G. Iannello Collective communication operations: experimental results vs. theory Concurrency: Practice and Experience 10 5 1998 359 386 [5] R. Bhoedjang T. R\u00fchl H. Bal User-Level network interface protocols IEEE Comput. 31 11 1998 53 60 [6] N. Boden D. Cohen R. Felderman A. Kulawik C. Seitz J. Seizovic W. Su Myrinet: a gigabit-per-second local area network IEEE Micro 15 1 1995 29 36 [7] J. Bruck L.D. Coster C.-T. Ho R. Lauwereins On the design and implementation of broadcast and global combine operations using the postal model IEEE Trans. Parallel Distrib. Syst. 7 3 1996 256 265 [8] C. Catlett L. Smarr Metacomputing Commun. ACM 35 1992 44 52 [9] D. Culler, R. Karp, D. Patterson, A. Sahay, K.E. Schauser, E. Santos, R. Subramonian, T. von Eicken, Log P: towards a realistic model of parallel computation, in: Proceedings of the Symposium on Principles and Practice of Parallel Programming (PPoPP), San Diego, CA, May 1993, pp. 1\u201312 [10] D.E. Culler L.T. Liu R.P. Martin C.O. Yoshikawa Assessing fast network interfaces IEEE Micro 16 1 1996 35 43 [11] G.E. Fagg, K.S. London, J.J. Dongarra, MPI_Connect: managing heterogeneous MPI applications interoperation and process control, in: Proceedings of the 5th European PVM/MPI Users' Group Meeting, number 1497 in LNCS, Liverpool, UK, 1998, pp. 93\u201396 [12] I. Foster C. Kesselman Globus: a metacomputing infrastructure toolkit Int. J. Supercomput. Appl. 11 2 1997 115 128 [13] I. Foster, C. Kesselman (Eds.), The GRID: Blueprint for a New Computing Infrastructure, Morgan Kaufmann, Los Altos, MA, 1998 [14] E. Gabriel, M. Resch, T. Beisel, R. Keller, Distributed computing in a heterogeneous computing environment, in: Proceedings of the 5th European PVM/MPI Users' Group Meeting number 1497 in LNCS, Liverpool, UK, 1998, pp. 180\u2013187 [15] W. George, J. Hagedorn, J. Devaney, Status report on the development of the interoperable MPI protocol, in: Proceedings of MPIDC'99, Message Passing Interface Developer's and User's Conference, Atlanta, GA, March 1999, pp. 7\u201313 [16] M. Go\u0142ebiewski, R. Hempel, J.L. Tr\u00e4ff, Algorithms for collective communication operations on SMP clusters, in: The 1999 Workshop on Cluster-Based Computing, held in conjunction with 13th ACM-SIGARCH International Conference on Supercomputing (ICS'99), 1999, pp. 11\u201315 [17] S. Gorlatch, C. Wedler, C. Lengauer, Optimization rules for programming with collective operations, in: Proceedings of the 13th International Parallel Processing Symposium & 10th Symposium on Parallel and Distributed Processing (IPPS/SPDP'99), 1999, pp. 492\u2013499 [18] J.-P. Goux, S. Kulkarni, J. Linderoth, M. Yoder, An enabling framework for master\u2013worker applications on the computatinal grid, in: Proceedings of the High Performance Distributed Computing (HPDC 2000), Pittsburgh, PA, August 2000, pp. 43\u201350 [19] A.S. Grimshaw, W.A. Wulf, and the Legion team, The legion vision of a worldwide virtual computer, Commun. ACM 40 (1) (1997) 39\u201345 [20] W. Gropp E. Lusk N. Doss A. Skjellum A high-performance, portable implementation of the MPI message passing interface standard Parallel Comput. 22 6 1996 789 828 [21] W.D. Gropp, E. Lusk, D. Swider, Improving the performance of MPI derived datatypes, in: Proceedings of MPIDC'99, Message Passing Interface Developer's and User's Conference, Atlanta, GA, March 1999, pp. 25\u201330 [22] P. Husbands, J.C. Hoe, MPI-StarT: delivering network performance to numerical applications, in: Proceedings of SC'98, November 1998; Online at http://www.supercomp.org/sc98/proceedings/ [23] G. Iannello, M. Lauria, S. Mercolino, Cross-platform analysis of fast messages for Myrinet, in: Proceedings of the Workshop on CANPC'98, Lecture Notes in Computer Science, vol. 1362, Las Vegas, Nevada, Springer, Berlin, January 1998, pp. 217\u2013231 [24] N.T. Karonis, B.R. de Supinski, I. Foster, W. Gropp, E. Lusk, J. Bresnahan, Exploiting hierarchy in parallel computer networks to optimize collective operation performance, in: Proceedings of the International Parallel and Distributed Processing Symposium (IPDPS 2000), Cancun, Mexico, IEEE, New York, May 2000, pp. 377\u2013384 [25] R.M. Karp, A. Sahay, E.E. Santos, K.E. Schauser, Optimal broadcast and summation in the Log P model, in: Proceedings of the Symposium on Parallel Algorithms and Architectures (SPAA), Velen, Germany, June 1993, pp. 142\u2013153 [26] R. Kesavan, D.K. Panda, Optimal multicast with packetization and network interface support, in: Proceedings of the International Conference on Parallel Processing, IEEE, New York, August 1997, pp. 370\u2013377 [27] T. Kielmann, H.E. Bal, S. Gorlatch, Bandwidth-efficient collective communication for clustered wide area systems, in: Proceedings of the International Parallel and Distributed Processing Symposium (IPDPS 2000), Cancun, Mexico, IEEE, New York, May 2000, pp. 492\u2013499 [28] T. Kielmann, H.E. Bal, K. Verstoep, Fast measurement of Log P parameters for message passing platforms, in: 4th Workshop on Runtime Systems for Parallel Programming (RTSPP), Lecture Notes in Computer Science, vol. 1800, Cancun, Mexico, Springer, Berlin, May 2000, pp. 1176\u20131183 [29] T. Kielmann, R.F.H. Hofman, H.E. Bal, A. Plaat, R.A.F. Bhoedjang, MPI's reduction operations in clustered wide area systems, in: Proceedings of MPIDC'99, Message Passing Interface Developer's and User's Conference, Atlanta, GA, March 1999, pp. 43\u201352 [30] T. Kielmann, R.F.H. Hofman, H.E. Bal, A. Plaat, R.A.F. Bhoedjang, MagPIe: MPI's collective communication operations for clustered wide area systems, in: Proceedings of the Symposium on Principles and Practice of Parallel Programming (PPoPP), Atlanta, GA, May 1999, pp. 131\u2013140 [31] B. Lowekamp, A. Beguelin, ECO: efficient collective operations for communication on heterogeneous networks, in: International Parallel Processing Symposium (IPPS), Honolulu, HI, 1996, pp. 399\u2013405 [32] E. Maillet C. Tron On efficiently implementing global time for performance evaluation on multiprocessor systems J. Parallel Distrib. Comput. 28 1995 84 93 [33] J.-Y.L. Park, H.-A. Choi, N. Nupairoj, L.M. Ni, Construction of optimal multicast trees based on the parameterized communication model, in: Proceedings of the International Conference on Parallel Processing (ICPP), vol. I, 1996, pp. 180\u2013187 [34] V. Paxson, On calibrating measurements of packet transit times, in: Proceedings of SIGMETRICS'98/PERFORMANCE'98, Madison, Wisconsin, June 1998, pp. 11\u201321 [35] E.E. Santos Optimal and near-optimal algorithms for k -item broadcast J. Parallel Distrib. Comput. 57 1999 121 139 [36] R. van de Geijn On global combine operations J. Parallel Distrib. Comput. 22 1994 324 328 [37] J. Watts R. Van de Geijn A pipelined broadcast for multidimensional meshes Parallel Process. Lett. 5 2 1995 281 292 [38] R. Wolski, Forecasting network performance to support dynamic scheduling using the network weather service, in: Proceedings of the High-Performance Distributed Computing (HPDC-6), Portland, OR, August 1997, pp. 316\u2013325; the network weather service is at http://nws.npaci.edu/\"",
        "title: \"Synthetic Coordinates for Disjoint Multipath Routing\" with abstract: \"We address the problem of routing packets on multiple, router-disjoint, paths in the Internet using large-scale overlay networks. Multipath routing can improve Internet QoS, by routing around congestions. This can benefit interactive and other real-time applications. One of the main problems with practically achieving router-disjoint multipath routing is the scalability limitation on the number of participating nodes in such an overlay network, caused by the large number of (expensive) topology probes required to discover relay nodes that provide high router-level path disjointness. To address this problem, we propose a novel, synthetic coordinates-based approach. We evaluate our method against alternative strategies for finding router-level disjoint alternative paths. Additionally, we empirically evaluate the distribution of path diversity in the Internet.\"",
        "1 is \"Bringing skeletons out of the closet: a pragmatic manifesto for skeletal parallel programming\", 2 is \"Performance of Firefly RPC\".",
        "\nGiven above information, for an author who has written the paper with the title \"A Component-Based Integrated Toolkit\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0176": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'On mixed and componentwise condition numbers for Moore-Penrose inverse and linear least squares problems':",
        "title: \"Safe Recursion Over an Arbitrary Structure: PAR, PH and DPH\" with abstract: \"Considering the Blum, Shub, and Smale computational model for real numbers, extended by Poizat to general structures, classical complexity can be considered as the restriction to finite structures of a more general notion of computability and complexity working over arbitrary structures.\"",
        "title: \"On sparseness and Turing reducibility over the reals\" with abstract: \"We prove some results about existence of NP-complete and NP-hard (for Turing reductions) sparse sets on different settings over the real numbers.\"",
        "title: \"Counting complexity classes for numeric computations II: algebraic and semialgebraic sets\" with abstract: \"We define counting classes #PR and #PC in the Blum-Shub-Smale setting of computations over the real or complex numbers, respectively. The problems of counting the number of solutions of systems of polynomial inequalities over R, or of systems of polynomial equalities over C, respectively, turn out to be natural complete problems in these classes. We investigate to what extent the new counting classes capture the complexity of computing basic topological invariants of semialgebraic sets (over R) and algebraic sets (over C). We prove that the problem to compute the (modified) Euler characteristic of semialgebraic sets is FPR#P RR-complete, and that the problem to compute the geometric degree of complex algebraic sets is FPR#PCC-complete. We also define new counting complexity classes GCR and GCC in the classical Turing model via taking Boolean parts of the classes above, and show that the problems to compute the Euler characteristic and the geometric degree of (semi)algebraic sets given by integer polynomials are complete in these classes. We complement the results in the Turing model by proving, for all k \u2208 N, the FPSPACE-hardness of the problem of computing the kth Betti number of the set of real zeros of a given integer polynomial. This holds with respect to the singular homology as well as for the Borel-Moore homology.\"",
        "title: \"Computing the homology of basic semialgebraic sets in weak exponential time.\" with abstract: \"We describe and analyze an algorithm for computing the homology (Betti numbers and torsion coefficients) of basic semialgebraic sets which works in weak exponential time. That is, out of a set of exponentially small measure in the space of data the cost of the algorithm is exponential in the size of the data. All algorithms previously proposed for this problem have a complexity which is doubly exponential (and this is so for almost all data).\"",
        "title: \"Adversarial smoothed analysis\" with abstract: \"The purpose of this note is to extend the results on uniform smoothed analysis of condition numbers from Burgisser et al. (2008) [1] to the case where the perturbation follows a radially symmetric probability distribution. In particular, we will show that the bounds derived in [1] still hold in the case of distributions whose density has a singularity at the center of the perturbation, which we call adversarial.\"",
        "1 is \"A Survey of Russian Approaches to Perebor (Brute-Force Searches) Algorithms\", 2 is \"Perturbation bound of singular linear systems\".",
        "\nGiven above information, for an author who has written the paper with the title \"On mixed and componentwise condition numbers for Moore-Penrose inverse and linear least squares problems\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0177": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Managing Control Asynchrony on SIMD Machines\u2014a Survey':",
        "title: \"Relaxing Causal Constraints in PDES\" with abstract: \"One of the major overheads that prohibits the wide spread deployment of parallel discrete event simulation (PDES) is the need to synchronize the distributed processes in the simulation. Considerable investigations have been conducted to analyze and optimize the two widely used synchronization strategies, namely the conservative and the optimistic simulation paradigms. However, little attention has been focussed on the definition and strictness of causality. Does causality need to be preserved in all types of simulations? Previously, we had suggested an answer to this question. We had argued that significant performance gains can be achieved by reconsidering this definition to decide if the parallel simulation really needs to subscribe to the preservation of causality. In this paper, we investigate this issue even more closely. An in depth analysis using several example simulation models is presented in this paper. In addition, a comparative analysis between unsynchronized and Time Warp simulation is presented.\"",
        "title: \"Predicting Performance Impacts due to Resolution Changes in Parallel Simulations\" with abstract: \"Multi-resolution models are often used to accelerate simulation-based analysis without significantly impacting the fidelity of the simulations. We have developed a web-enabled, component-based, multi-resolution modeling and Time Warp synchronized parallel simulation environment called WESE (Web-Enabled Simulation Environment). WESE uses a methodology called Dynamic Component Substitution (DCS) to enable abstractions or refinements to a given model. However, effectively utilizing abstractions, whether they are DCS-based or not, is a complex and time-consuming task. The complexity arises because not all abstractions improve simulation performance due to a myriad of factors related to model characteristics, synchronization protocol overheads and simulation-platform configuration. The overheads involved in identifying optimal model resolution have been exacerbating effective use of multi-resolution simulations, including our DCS-based approach. In an endeavor to minimize the time taken to identify performance impacts of resolution changes, this study proposes a DCS Performance Prediction Methodology (DCSPPM). It predicts simulation performance changes due to DCS transformations via static analysis of the model. Static analysis uses platform-specific performance characteristics of components constituting the model. DCSPPM yields quantitative estimates of performance impacts which are used by the modeler to select appropriate transformations. This article presents DCSPPM, its implementation in WESE and its empirical evaluation. The inferences drawn from the experiments prove that DCSPPM estimates have errors of less than 5% for a variety of models. Furthermore, DCSPPM executes orders of magnitude faster than corresponding shortest test simulations. Note that applicability of DCSPPM is not restricted to WESE but can be extended to other Time Warp synchronized simulators.\"",
        "title: \"Quantitative Driven Optimization of a Time Warp Kernel.\" with abstract: \"The set of events available for execution in a Parallel Discrete Event Simulation (PDES) are known as the pending event set. In a Time Warp synchronized simulation engine, these pending events are scheduled for execution in an aggressive manner that does not strictly enforce the causal relations between events. One of the key principles of Time Warp is that this relaxed causality will result in the processing of events in a manner that implicitly satisfies their causal order without paying the overhead costs of a strict enforcement of their causal order. On a shared memory platform the event scheduler generally attempts to schedule all available events in their Least TimeStamp First (LTSF) order to facilitate event processing in their causal order. By following an LTSF scheduling policy, a Time Warp scheduler can generally process events so that: (i) the critical path of the event timestamps is scheduled as early as possible, and (ii) causal violations occur infrequently. While this works effectively to minimize rollback (triggered by causal violations), as the number of parallel threads increases, the contention to the shared data structures holding the pending events can have significant negative impacts on overall event processing throughput. This work examines the application of profile data taken from Discrete-Event Simulation (DES) models to drive the simulation kernel optimization process. In particular, we take profile data about events in the schedule pool from three DES models to derive alternate scheduling possibilities in a Time Warp simulation kernel. Profile data from the studied DES models suggests that in many cases each Logical Process (LP) in a simulation will have multiple events that can be dequeued and executed as a set. In this work, we review the profile data and implement group event scheduling strategies based on this profile data. Experimental results show that event group scheduling can help alleviate contention and improve performance. However, the size of the event groups matters, small groupings can improve performance, larger groupings can trigger more frequent causal violations and actually slow the parallel simulation.\"",
        "title: \"A distributed method to bound rollback lengths for fossil collection in time warp simulators\" with abstract: \"The calculation of GVT has been a requirement to identify fossilized state and event space during Time Warp simulations. This paper outlines methods that use observations of past behavior to estimate future behavior for the purposes of fossil reclamation. More precisely, predictions of future rollback behavior are used to determine a probability that a particular item of saved state or event information is no longer needed. This probability is compared against a user-defined risk factor to decide if the space can be reclaimed and reused. This method is called optimistic fossil collection and it is fully distributed, not requiring the global estimate of GVT for operation.\"",
        "title: \"Causality representation and cancellation mechanism in time warp simulations\" with abstract: \"The Time Warp synchronization protocol allows causality errors and then recovers from them with the assistance of a cancellation mechanism. Cancellation can cause the rollback of several other simulation objects that may trigger a cascading rollback situation where the rollback cycles back to the original simulation object. These cycles of rollback can cause the simulation to enter a unstable (or thrashing) state where little real forward simulation progress is achieved. To address this problem, knowledge of causal relations between events can be used during cancellation to avoid cascading rollbacks and to initiate early recovery operations from causality errors. In this paper, we describe a logical time representation for Time Warp simulations that is used to disseminate causality information. The new timestamp representation, called Total Clocks, has two components: (i) a virtual time component, and (ii) a vector of event counters similar to Vector clocks. The virtual time component provides a one dimensional global simulation time, and the vector of event counters records event processing rates by the simulation objects. This time representation allows us to disseminate causality information during event execution that can be used to allow early recovery during cancellation. We propose a cancellation mechanism using Total Clocks that avoids cascading rollbacks in Time Warp simulations that have FIFO communication channels.\"",
        "1 is \"Efficient warp execution in presence of divergence with collaborative context collection\", 2 is \"A study of time warp rollback mechanisms\".",
        "\nGiven above information, for an author who has written the paper with the title \"Managing Control Asynchrony on SIMD Machines\u2014a Survey\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0178": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Design and implementation of a multi-standard event-driven energy management system for smart buildings':",
        "title: \"A Design Approach For Power-Optimized Fully Reconfigurable Delta Sigma A/D Converter For 4g Radios\" with abstract: \"This paper presents a novel design approach for fully reconfigurable low-voltage delta-sigma analog-digital converters for next-generation wireless applications. This approach guides us to find the power-optimal solution corresponding to the specifications of various wireless standards by exploring single-loop feedback and feedforward topologies with different filter order, number of quantizer bits, and oversampling ratios. Unlike previous multimode designs, this approach provides a better power efficiency. Based on this approach, a system-level design of a digitally programmable delta-sigma modulator for 4G radios is presented.\"",
        "title: \"Efficient DDD-based symbolic analysis of large linear analog circuits\" with abstract: \"A new technique for generating approaximate symbolic expressions for network functions in linear(ized) analog circuits is presented. It is based on the compact determinant decision diagram (DDD) representation of the circuit. An implementation of a term generation algorithm is given and its performance is compared to a matroid-based algorithm. Experimental results indicate that our approach is the fastest reported algorithm so far for this application.\"",
        "title: \"Automation in mixed-signal design: challenges and solutions in the wake of the nano era\" with abstract: \"The use of CMOS nanometer technologies at 65 nm and below will pose serious challenges on the design of mixed-signal integrated systems in the very near future. Rising design complexities, tightening time-to-market constraints, leakage power, increasing technology tolerances, and reducing supply voltages are key challenges that designers face. Novel types of devices, new process materials and new reliability issues are next on the horizon. We discuss new design methodologies and EDA tools that are being or need to be developed to address the problems of designing such mixed-signal integrated systems.\"",
        "title: \"A fast analog circuit yield estimation method for medium and high dimensional problems\" with abstract: \"Yield estimation for analog integrated circuits remains a time-consuming operation in variation-aware sizing. State-of-the-art statistical methods such as ranking-integrated Quasi-Monte-Carlo (QMC), suffer from performance degradation if the number of effective variables is large (as typically is the case for realistic analog circuits). To address this problem, a new method, called AYLeSS, is proposed to estimate the yield of analog circuits by introducing Latin Supercube Sampling (LSS) technique from the computational statistics field. Firstly, a partitioning method is proposed for analog circuits, whose purpose is to appropriately partition the process variation variables into low-dimensional sub-groups fitting for LSS sampling. Then, randomized QMC is used in each sub-group. In addition, the way to randomize the run order of samples in Latin Hypercube Sampling (LHS) is used for the QMC sub-groups. AYLeSS is tested on 4 designs of 2 example circuits in 0.35\u03bcm and 90nm technologies with yield from about 50% to 90%. Experimental results show that AYLeSS has approximately a 2 times speed enhancement compared with the best state-of-the-art method.\"",
        "title: \"Automatic generation of autonomous built-in observability structures for analog circuits\" with abstract: \"In this paper a new method is presented to automatically generate a Design-for-Testability infrastructure which increases the observability of defects in integrated circuits. An algorithm is proposed to detect circuit locations to which small detection blocks can be added. Those are coupled to an oscillator and the triggering of this oscillator in case of detected defects leaves traces in the power consumption. Therefore, the detection of a defective circuit can directly be transmitted to the Automated Test Equipment without requiring a special routing of the signals on the chip and extra test pins. Simulations on an industrial circuit show a 86 percent fault coverage of the hard-to-detect faults for an area increase of less than a percent.\"",
        "1 is \"Parameterized model order reduction of nonlinear dynamical systems\", 2 is \"Symbolic computation of logic implications for technology-dependent low-power synthesis\".",
        "\nGiven above information, for an author who has written the paper with the title \"Design and implementation of a multi-standard event-driven energy management system for smart buildings\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0179": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'A Traffic-Based Routing Algorithm By Using Mobile Agents':",
        "title: \"Assessing service protocol adaptability based on protocol reduction and graph search\" with abstract: \"Given the inherent autonomy, heterogeneity, and continuous evolution of Web services, mismatches usually exist between service protocols. Adapters are typically used to reconcile these mismatches. Before synthesizing an adapter, a service requestor is often willing to assess whether her expected interactions can be conducted or not. The effort of synthesizing an adapter is beneficial only if the result of this assessment is positive. Previous effort analyzing service interactions focused on either (i) compatibility analysis for checking whether interactions can be conducted in a direct manner or (ii) adapter synthesization for reconciling mismatches. In this paper we propose a new kind of adaptability assessment that (i) determines whether two service protocols of a requestor and a provider are adaptable, (ii) computes an adaptation degree, and (iii) identifies conditions that determine when these two service protocols can be adapted. This adaptability assessment provides complementary criteria to the service requestor for selecting a suitable service protocol from a set of functionally equivalent candidates according to her requirements. Copyright \u00a9 2010 John Wiley & Sons, Ltd.\"",
        "title: \"Exploring an epidemic in an e-science environment\" with abstract: \"Rules of bio-epidemic and e-epidemic inspire scientists to create a live, scalable interconnected environment for effectively managing situations in nature, society, and the digital virtual world.\"",
        "title: \"The schema theory for semantic link network\" with abstract: \"The Semantic Link Network (SLN) is a loosely coupled semantic data model for managing Web resources. Its nodes can be any type of resource. Its edges can be any semantic relation. Potential semantic links can be derived out according to reasoning rules on semantic relations. This paper proposes the schema theory for the SLN, including the concepts, rule-constraint normal forms, and relevant algorithms. The theory provides the basis for normalized management of semantic link network. A case study demonstrates the proposed theory.\"",
        "title: \"Semantics, Knowledge and Grids on Big Data.\" with abstract: \"Semantics, knowledge and Grids represent three spaces where people interact, understand, learn and create. Grids represent the advanced cyber-infrastructures and evolution. Big data influence the evolution of semantics, knowledge and Grids. Exploring semantics, knowledge and Grids on big data helps accelerate the shift of scientific paradigm, the fourth industrial revolution, and the transformational innovation of technologies.\"",
        "title: \"The open and autonomous interconnection semantics\" with abstract: \"Semantics is the meaning expressed by various languages and the study of meaning. Human society co-evolves with diverse semantic spaces holding commonsense, culture, and knowledge of sciences and technologies. By surveying the origin and development of general semantics, this paper studies the semantics of future interconnection environment, proposes a new notion of open and autonomous interconnection semantics, explores relevant principles and rules, and demonstrates a cultural application.\"",
        "1 is \"Towards an Iterative Reinforcement Approach for Simultaneous Document Summarization and Keyword Extraction\", 2 is \"Adaptive Multicast Topology Inference\".",
        "\nGiven above information, for an author who has written the paper with the title \"A Traffic-Based Routing Algorithm By Using Mobile Agents\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0180": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Markov model fuzzy-reasoning based algorithm for fast block motion estimation':",
        "title: \"Agglomerative-based flip-flop merging with signal wirelength optimization\" with abstract: \"In this paper, an optimization methodology using agglomerative-based clustering for number of flip-flop reduction and signal wirelength minimization is proposed. Comparing to previous works on flip-flop reduction, our method can obtain an optimal tradeoff curve between flip-flop number reduction and increase in signal wirelength. Our proposed methodology outperforms [1] and [12] in both reducing number of flip-flops and minimizing increase in signal wirelength. In comparison with [9], our methodology obtains a tradeoff of 15.8% reduction in flip-flop's signal wirelength with 16.9% additional flip-flops. Due to the nature of agglomerative clustering, when relocating flip-flops, our proposed method minimizes total displacement by an average of 5.9%, 8.0%, 181.4% in comparison with [12], [1] and [9] respectively.\"",
        "title: \"Clock planning for multi-voltage and multi-mode designs\" with abstract: \"Low power demand drives the development of lower power design architectures, among which multiple supply voltage is one of the state-of-the-art techniques to achieve low power. In addition, dynamic voltage frequency scaling and adaptive voltage scaling are popular power saving techniques during chip operation to provide different modes for various performance requirements. It is therefore very challenging to generate a clock tree for different operation modes. This paper proposes several implementations on this important issue, one of which can provide smallest clock latency and minimum clock skew on average of required operation modes in multi-voltage designs.\"",
        "title: \"Efficient analog layout prototyping by layout reuse with routing preservation\" with abstract: \"To strive for better circuit performance on analog design, layout generation heavily relies on experienced analog designers' effort. Other than general analog constraints such as symmetry and wire-matching are commonly embraced in many proposed works, analog circuit performance is also sensitive to routing behavior. This paper presents a CDT-based layout extraction to preserve routing behavior of the reference layout. Furthermore, a generalized layout prototyping methodology is proposed based on the layout extraction to achieve routing reuse. The proposed layout prototyping is applied to a variable-gain amplifier and a folded-cascode operational amplifier for both migration and prototypes generation. Experimental results show that our approach effectively reduces design cycle time and simultaneously produces reasonable performance.\n\n\"",
        "title: \"Package routability- and IR-drop-aware finger/pad assignment in chip-package co-design\" with abstract: \"Due to increasing complexity of design interactions between the chip, package and PCB, it is essential to consider them at the same time. Specifically the finger/pad locations affect the performance of the chip and the package significantly. In this paper, we have developed techniques in chip-package codesign to decide the locations of fingers/pads for package routability and signal integrity concerns in chip core design. Our finger/pad assignment is a two-step method: first we optimize the wire congestion problem in package routing, and then we try to minimize the IR-drop violation with finger/pad solution refinement. The experimental results are encouraging. Compared with the randomly optimized methods, our approaches reduce in average 42% and 68% of the maximum density in package and 10.61% of IR-drop for test circuits.\"",
        "title: \"Routability-driven bump assignment for chip-package co-design\" with abstract: \"In current chip and package designs, it is a bottleneck to simultaneously optimize both pin assignment and pin routing for different design domains (chip, package, and board). Usually the whole process costs a huge manual effort and multiple iterations thus reducing profit margin. Therefore, we propose a fast heuristic chip-package co-design algorithm in order to automatically obtain a bump assignment which introduces high routability both in RDL routing and substrate routing (100% in our real case). Experimental results show that the proposed method (inspired by board escape routing algorithms) automatically finishes bump assignment, RDL routing and substrate routing in a short time, while the traditional co-design flow requires weeks even months.\"",
        "1 is \"A block-based gradient descent search algorithm for block motion estimation in video coding\", 2 is \"An interference-cancellation scheme for carrier frequency offsets correction in OFDMA systems\".",
        "\nGiven above information, for an author who has written the paper with the title \"Markov model fuzzy-reasoning based algorithm for fast block motion estimation\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0181": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Moving Beyond e-Health and the Quantified Self: The Role of CSCW in Collaboration, Community and Practice for Technologically-Supported Proactive Health and Wellbeing':",
        "title: \"Bursting your (filter) bubble: strategies for promoting diverse exposure\" with abstract: \"Broadcast media are declining in their power to decide which issues and viewpoints will reach large audiences. But new information filters are appearing, in the guise of recommender systems, aggregators, search engines, feed ranking algorithms, and the sites we bookmark and the people and organizations we choose to follow on Twitter. Sometimes we explicitly choose our filters; some we hardly even notice. Critics worry that, collectively, these filters will isolate people in information bubbles only partly of their own choosing, and that the inaccurate beliefs they form as a result may be difficult to correct. But should we really be worried, and, if so, what can we do about it? Our panelists will review what scholars know about selectivity of exposure preferences and actual exposure and what we in the CSCW field can do to develop and test ways of promoting diverse exposure, openness to the diversity we actually encounter, and deliberative discussion.\"",
        "title: \"Motivating and enabling organizational memory with a workgroup wiki\" with abstract: \"Workgroups can struggle with remembering past projects and sharing this information with other groups in the organization. In a case study of the deployment of MediaWiki as a publishing tool for building organizational memory, group members' motivation to document past projects increased. A browsable collection of past projects allowed for discovery of past work, building the reputation of individuals and the workgroup, and development of transactive memory within the workgroup. The \"anyone can edit\" feature, frequently touted as the main benefit of wikis, had both benefits and drawbacks in this implementation. Group members did not feel comfortable making substantial edits to others' content but did occasionally use the wiki to coauthor content and also categorize and link to others' content and fix typos, particularly when asked to help.\"",
        "title: \"Maytag: a multi-staged approach to identifying complex events in textual data\" with abstract: \"We present a novel application of NLP and text mining to the analysis of financial documents. In particular, we describe an implemented prototype, Maytag, which combines information extraction and subject classification tools in an interactive exploratory framework. We present experimental results on their performance, as tailored to the financial domain, and some forward-looking extensions to the approach that enables users to specify classifications on the fly.\"",
        "title: \"When Personal Tracking Becomes Social: Examining the Use of Instagram for Healthy Eating.\" with abstract: \"Many people appropriate social media and online communities in their pursuit of personal health goals, such as healthy eating or increased physical activity. However, people struggle with impression management, and with reaching the right audiences when they share health information on these platforms. Instagram, a popular photo-based social media platform, has attracted many people who post and share their food photos. We aim to inform the design of tools to support healthy behaviors by understanding how people appropriate Instagram to track and share food data, the benefits they obtain from doing so, and the challenges they encounter. We interviewed 16 women who consistently record and share what they eat on Instagram. Participants tracked to support themselves and others in their pursuit of healthy eating goals. They sought social support for their own tracking and healthy behaviors and strove to provide that support for others. People adapted their personal tracking practices to better receive and give this support. Applying these results to the design of health tracking tools has the potential to help people better access social support.\"",
        "title: \"Social Cues and Interest in Reading Political News Stories.\" with abstract: \"People tend to prefer information sources that agree with their viewpoints, as predicted by the selective exposure theory, and to associate with people who are like them, a process known as homophily. Scholars raise fears that the combination of these factors can limit the diversity of viewpoints to which people are exposed, particularly when people find news through social network sites. In this study, we evaluate whether we can use annotations showing that a story was shared by people who are in some way similar to encourage people to read articles that may challenge their viewpoints. Most annotations (shared city, employer, music tastes, liked organizations, and friendship) had no discernable effect on reading interest compared to no annotation. Shared job type, though, led to decreased interest in reading an article. Although people consider themselves similar to others sharing news articles, this predominantly does not change their reading interest.\"",
        "1 is \"Finding \", 2 is \" Correlations in Multi-Faceted Personal Informatics Systems.\".",
        "\nGiven above information, for an author who has written the paper with the title \"Moving Beyond e-Health and the Quantified Self: The Role of CSCW in Collaboration, Community and Practice for Technologically-Supported Proactive Health and Wellbeing\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0182": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Automatically Exploring Hypotheses About Fault Prediction: A Comparative Study Of Inductive Logic Programming Methods':",
        "title: \"Learning the Classic Description Logic: Theoretical and Experimental Results\" with abstract: \" We present a series of theoretical and experimental results on the learnability of description logics. We first extend previous formal learnability results on simple description logics to C-Classic, a description logic expressive enough to be practically useful. We then experimentally evaluate two extensions of a learning algorithm suggested by the formal analysis. The first extension learns C-Classic descriptions from individuals. (The formal results assume that examples are themselves... \"",
        "title: \"Learning by teaching SimStudent: an initial classroom baseline study comparing with cognitive tutor\" with abstract: \"This paper describes an application of a machine-learning agent, SimStudent, as a teachable peer learner that allows a student to learn by teaching. SimStudent has been integrated into APLUS (Artificial Peer Learning environment Using SimStudent), an on-line game-like learning environment. The first classroom study was conducted in local public high schools to test the effectiveness of APLUS for learning linear algebra equations. In the study, learning by teaching (i.e., APLUS) was compared with learning by tutored-problem solving (i.e., Cognitive Tutor). The results show that the prior knowledge has a strong influence on tutor learning - for students with insufficient training on the target problems, learning by teaching may have limited benefits compared to learning by tutored problem solving. It was also found that students often use inappropriate problems to tutor SimStudent that did not effectively facilitate the tutor learning.\"",
        "title: \"Inductive Specification Recovery: Understanding Software by Learning from Example Behaviors\" with abstract: \"We describe a technique for extracting specifications from software using machine learning techniques. In our proposed technique, instrumented code is run on a number of representative test cases, generating examples of its behavior. Inductive learning techniques are then used to generalize these examples, forming a general description of some aspect of the system's behavior. A case study is presented in which this \u201cinductive specification recovery\u201d method is used to find Datalog specifications forC code that implements database views, in the context of a large real-world software system. It is demonstrated that off-the-shelf inductive logic programming methods can be successfully used for specification recovery in this domain, but that these methods can be substantially improved by adapting them more closely to the task at hand.\"",
        "title: \"Learning to match and cluster large high-dimensional data sets for data integration\" with abstract: \"Part of the process of data integration is determining which sets of identifiers refer to the same real-world entities. In integrating databases found on the Web or obtained by using information extraction methods, it is often possible to solve this problem by exploiting similarities in the textual names used for objects in different databases. In this paper we describe techniques for clustering and matching identifier names that are both scalable and adaptive, in the sense that they can be trained to obtain better performance in a particular domain. An experimental evaluation on a number of sample datasets shows that the adaptive method sometimes performs much better than either of two non-adaptive baseline systems, and is nearly always competitive with the best baseline system.\"",
        "title: \"Community-based classification of noun phrases in twitter\" with abstract: \"Many event monitoring systems rely on counting known keywords in streaming text data to detect sudden spikes in frequency. But the dynamic and conversational nature of Twitter makes it hard to select known keywords for monitoring. Here we consider a method of automatically finding noun phrases (NPs) as keywords for event monitoring in Twitter. Finding NPs has two aspects, identifying the boundaries for the subsequence of words which represent the NP, and classifying the NP to a specific broad category such as politics, sports, etc. To classify an NP, we define the feature vector for the NP using not just the words but also the author's behavior and social activities. Our results show that we can classify many NPs by using a sample of training data from a knowledge-base.\"",
        "1 is \"Reasoning with Memory Augmented Neural Networks for Language Comprehension.\", 2 is \"Scalable access control for distributed object systems\".",
        "\nGiven above information, for an author who has written the paper with the title \"Automatically Exploring Hypotheses About Fault Prediction: A Comparative Study Of Inductive Logic Programming Methods\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0183": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Algorithms for Inverse Reinforcement Learning':",
        "title: \"Selecting Receptive Fields in Deep Networks.\" with abstract: \"Recent deep learning and unsupervised feature learning systems that learn from unlabeled data have achieved high performance in benchmarks by using extremely large architectures with many features (hidden units) at each layer. Unfortunately, for such large architectures the number of parameters usually grows quadratically in the width of the network, thus necessitating hand-coded \"local receptive fields\" that limit the number of connections from lower level features to higher ones (e.g., based on spatial locality). In this paper we propose a fast method to choose these connections that may be incorporated into a wide variety of unsupervised training methods. Specifically, we choose local receptive fields that group together those low-level features that are most similar to each other according to a pairwise similarity metric. This approach allows us to harness the advantages of local receptive fields (such as improved scalability, and reduced data requirements) when we do not know how to specify such receptive fields by hand or where our unsupervised training algorithm has no obvious generalization to a topographic setting. We produce results showing how this method allows us to use even simple unsupervised training algorithms to train successful multi-layered etworks that achieve state-of-the-art results on CIFAR and STL datasets: 82.0% and 60.1% accuracy, respectively.\"",
        "title: \"Make3D: Learning 3D Scene Structure from a Single Still Image\" with abstract: \"We consider the problem of estimating detailed 3D structure from a single still image of an unstructured environment. Our goal is to create 3D models that are both quantitatively accurate as well as visually pleasing. For each small homogeneous patch in the image, we use a Markov random field (MRF) to infer a set of \"plane parametersrdquo that capture both the 3D location and 3D orientation of the patch. The MRF, trained via supervised learning, models both image depth cues as well as the relationships between different parts of the image. Other than assuming that the environment is made up of a number of small planes, our model makes no explicit assumptions about the structure of the scene; this enables the algorithm to capture much more detailed 3D structure than does prior art and also give a much richer experience in the 3D flythroughs created using image-based rendering, even for scenes with significant nonvertical structure. Using this approach, we have created qualitatively correct 3D models for 64.9 percent of 588 images downloaded from the Internet. We have also extended our model to produce large-scale 3D models from a few images.\"",
        "title: \"Distance Metric Learning with Application to Clustering with Side-Information\" with abstract: \"Many algorithms rely critically on being given a good metric over their inputs. For instance, data can often be clustered in many \"plausible\" ways, and if a clustering algorithm such as K-means initially fails to find one that is meaningful to a user, the only recourse may be for the user to manually tweak the metric until sufficiently good clusters are found. For these and other applications requiring good metrics, it is desirable that we provide a more systematic way for users to indicate what they con- sider \"similar.\" For instance, we may ask them to provide examples. In this paper, we present an algorithm that, given examples of similar (and, if desired, dissimilar) pairs of points in , learns a distance metric over that respects these relationships. Our method is based on posing met- ric learning as a convex optimization problem, which allows us to give efficient, local-optima-free algorithms. We also demonstrate empirically that the learned metrics can be used to significantly improve clustering performance.\"",
        "title: \"Unsupervised learning of hierarchical representations with convolutional deep belief networks\" with abstract: \"There has been much interest in unsupervised learning of hierarchical generative models such as deep belief networks (DBNs); however, scaling such models to full-sized, high-dimensional images remains a difficult problem. To address this problem, we present the convolutional deep belief network, a hierarchical generative model that scales to realistic image sizes. This model is translation-invariant and supports efficient bottom-up and top-down probabilistic inference. Key to our approach is probabilistic max-pooling, a novel technique that shrinks the representations of higher layers in a probabilistically sound way. Our experiments show that the algorithm learns useful high-level visual features, such as object parts, from unlabeled images of objects and natural scenes. We demonstrate excellent performance on several visual recognition tasks and show that our model can perform hierarchical (bottom-up and top-down) inference over full-sized images.\"",
        "title: \"Autonomous operation of novel elevators for robot navigation\" with abstract: \"Although robot navigation in indoor environments has achieved great success, robots are unable to fully navigate these spaces without the ability to operate elevators, including those which the robot has not seen before. In this paper, we focus on the key challenge of autonomous interaction with an unknown elevator button panel. A number of factors, such as lack of useful 3D features, variety of elevator panel designs, variation in lighting conditions, and small size of elevator buttons, render this goal quite difficult. To address the task of detecting, localizing, and labeling the buttons, we use state-of-the-art vision algorithms along with machine learning techniques to take advantage of contextual features. To verify our approach, we collected a dataset of 150 pictures of elevator panels from more than 60 distinct elevators, and performed extensive offline testing. On this very diverse dataset, our algorithm succeeded in correctly localizing and labeling 86.2% of the buttons. Using a mobile robot platform, we then validate our algorithms in experiments where, using only its on-board sensors, the robot autonomously interprets the panel and presses the appropriate button in elevators never seen before by the robot. In a total of 14 trials performed on 3 different elevators, our robot succeeded in localizing the requested button in all 14 trials and in pressing it correctly in 13 of the 14 trials.\"",
        "1 is \"Solving SAT and SAT Modulo Theories: From an abstract Davis--Putnam--Logemann--Loveland procedure to DPLL(T)\", 2 is \"Interactive Teaching Of A Mobile Robot\".",
        "\nGiven above information, for an author who has written the paper with the title \"Algorithms for Inverse Reinforcement Learning\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0184": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Finite nondeterministic automata: simulation and minimality':",
        "title: \"Algebraic Constraints, Automata, and Regular Languages\" with abstract: \"A class of decision problems is Boolean if it is closed under the set-theoretic operations of union, intersection and complementation. The paper introduces new Boolean classes of decision problems based on algebraic constraints imposed on transitions of finite automata. We discuss issues related to specifications of these classes from algebraic, computational and proof-theoretic points of view.\"",
        "title: \"Update Networks and Their Routing Strategies\" with abstract: \"We introduce the notion of update networks to model communication networks with infinite duration. In our formalization we use bipartite finite graphs and game-theoretic terminology as an underlying structure. For these networks we exhibit a simple routing procedure to update information throughout the nodes of the network. We also introduce an hierarchy for the class of all update networks and discuss the complexity of some natural problems.\"",
        "title: \"Games with Unknown Past\" with abstract: \"We define a new type of two player game occurring on a tree. The tree may have no root and may have arbitrary degrees of nodes. These games extend the class of games considered by Gurevich-Harrington in [5]. We prove that in the game one of the players has a winning strategy which depends on finite bounded information about the past part of a play and on future of each play that is isomorphism types of tree nodes. This result extends further the Gurevich-Harrington determinacy theorem from [5].\"",
        "title: \"Decision Problems For Finite Automata Over Infinite Algebraic Structures\" with abstract: \"We introduce the concept of finite automata over algebraic structures. We address the classical emptiness problem and its various refinements in our setting. In particular, we prove several decidability and undecidability results. We also explain the way our automata model connects with the existential first order theory of algebraic structures.\"",
        "title: \"Relaxed update and partition network games\" with abstract: \"In this paper, we study the complexity of deciding which player has a winning strategy in certain types of McNaughton games. These graph games can be used as models for computational problems and processes of infinite duration. We consider the cases (1) where the first player wins when vertices in a specified set are visited infinitely often and vertices in another specified set are visited finitely often, (2) where the first player wins when exactly those vertices in one of a number of specified disjoint sets are visited infinitely often, and (3) a generalization of these first two cases. We give polynomial time algorithms to determine which player has a winning strategy in each of the games considered.\"",
        "1 is \"The effects of transparency on trust in and acceptance of a content-based art recommender\", 2 is \"Depth-first search and linear grajh algorithms\".",
        "\nGiven above information, for an author who has written the paper with the title \"Finite nondeterministic automata: simulation and minimality\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0185": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'ROARS: a robust object archival system for data intensive scientific computing':",
        "title: \"ROARS: a robust object archival system for data intensive scientific computing\" with abstract: \"As scientific research becomes more data intensive, there is an increasing need for scalable, reliable, and high performance storage systems. Such data repositories must provide both data archival services and rich metadata, and cleanly integrate with large scale computing resources. ROARS is a hybrid approach to distributed storage that provides both large, robust, scalable storage and efficient rich metadata queries for scientific applications. In this paper, we present the design and implementation of ROARS, focusing primarily on the challenge of maintaining data integrity across long time scales. We evaluate the performance of ROARS on a storage cluster, comparing to the Hadoop distributed file system and a centralized file server. We observe that ROARS has read and write performance that scales with the number of storage nodes, and integrity checking that scales with the size of the largest node. We demonstrate the ability of ROARS to function correctly through multiple system failures and reconfigurations. ROARS has been in production use for over three years as the primary data repository for a biometrics research lab at the University of Notre Dame.\"",
        "title: \"Recognition of free-form objects in dense range data using local features\" with abstract: \"This article describes a system for recognizing free-form 3D objects in dense range data employing local features and object-centered geometric models. Local features are extracted from range images and object models using curvature analysis, and variability in feature size is accommodated by decomposition of features into sub-features. Shape indices and other attributes provide a basis for correspondence between compatible image and model features and subfeatures, as well as pruning of invalid correcpondences. A verification step provides a final ranking of object identity and pose hypotheses. The evaluation system contained 10 free-form objects and was tested using 10 range images with two objects from the database in each image. Comments address strengths of the proposed technique as well as areas for future improvement.\"",
        "title: \"Personal Identification Utilizing Finger Surface Features\" with abstract: \"In this paper we present a novel approach for personal identification which utilizes finger surface features as a biometric identifier. Using dense range data images of the hand, we calculate the curvature-based surface representation, shape index, for the index, middle, and ring fingers. This representation is used for comparisons to determine subject similarity. Our experiments involve the use of a large data set of range images collected over time. We examine the performance of individual finger surfaces as a biometric identifier as well as the performance when using the three finger surfaces in conjunction. The results of our experiments are presented, which indicate that this approach performs well for a first-of-its-kind biometric technique.\"",
        "title: \"Degradation of iris recognition performance due to non-cosmetic prescription contact lenses\" with abstract: \"Many iris recognition systems operate under the assumption that non-cosmetic contact lenses have no or minimal effect on iris biometrics performance and convenience. In this paper we show results of a study of 12,003 images from 87 contact-lens-wearing subjects and 9697 images from 124 non-contact-lens-wearing subjects. We visually classified the contact lens images into four categories according to the type of lens effects observed in the image. Our results show different degradations in performance for different types of contact lenses. Lenses that produce larger artifacts on the iris yield more degraded performance. This is the first study to document degraded iris biometrics performance with non-cosmetic contact lenses.\"",
        "title: \"Textured mesh generation of extracted regions from urban range-scanned LIDAR data\" with abstract: \"LIDAR range scanners are a popular tool for data acquisition in the field of urban modeling, archaeological preservation, city planning, and more. However, range scanners output a series of discrete distance samples as disconnected points, providing a fundamentally incomplete representation of the underlying structure in a scene. Data from a single LIDAR scan of a region can be trivially triangulated, but when multiple scanners are involved, or when the acquisition platform is mobile, those inter-point relationships are lost. We propose a technique to triangulate such data by identifying logical surfaces within the data and triangulating those surfaces individually. The resulting triangulations are simplified dramatically by using information about the shape of the regions, and texture is applied from camera imagery on the scan vehicle. The result is a high fidelity representation of a scene which is more efficiently rendered on modern hardware than point sets, which occupies less space in memory, and which brings us closer to a solid representation of the true scanned scene.\"",
        "1 is \"Hardness-Aware Truth Discovery in Social Sensing Applications\", 2 is \"Point Signatures: A New Representation for 3D Object Recognition\".",
        "\nGiven above information, for an author who has written the paper with the title \"ROARS: a robust object archival system for data intensive scientific computing\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0186": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Designing Fast and Scalable XACML Policy Evaluation Engines':",
        "title: \"Fast and Accurate Tracking of Population Dynamics in RFID Systems\" with abstract: \"RFID systems have been widely deployed for various applications such as supply chain management, indoor localization, inventory control, and access control. This paper deals with the fundamental problem of estimating the number of arriving and departing tags between any two time instants in dynamically changing RFID tag populations, which is needed in many applications such as warehouse monitoring and privacy sensitive RFID systems. In this paper, we propose a dynamic tag estimation scheme, namely DTE, that can achieve arbitrarily high required reliability, is compliant with the C1G2 standard, and works in single as well as multiple-reader environment. DTE uses the standardized frame slotted Aloha protocol and utilizes the number of slots that change their values in corresponding Aloha frames at the two time instants to estimate the number of arriving and departing tags. It is easy to deploy because it neither requires modification to tags nor to the communication protocol between tags and readers. We have extensively evaluated and compared DTE with the only prior scheme, ZDE, that can estimate the number of arriving and departing tags. Unfortunately, ZDE can not achieve arbitrarily high required reliability. In contrast, our proposed scheme always achieves the required reliability. For example, for a tag population containing 10 4 tags, a required reliability of 95%, and a required confidence interval of 5%, DTE takes 5.12 seconds to achieve the required reliability whereas ZDE achieves a reliability of only 66% in the same amount of time.\"",
        "title: \"A Distributed Algorithm for Identifying Information Hubs in Social Networks\" with abstract: \"This paper addresses the problem of identifying the top-k information hubs in a social network. Identifying top-k information hubs is crucial for many applications such as advertising in social networks where advertisers are interested in identifying hubs to whom free samples can be given. Existing solutions are centralized and require time stamped information about pair-wise user interactions and can only be used by social network owners as only they have access to such data. Existing distributed algorithms suffer from poor accuracy. In this paper, we propose a new algorithm to identify information hubs that preserves user privacy. Our method can identify hubs without requiring a central entity to access the complete friendship graph. We achieve this by fully distributing the computation using the Kempe-McSherry algorithm, while addressing user privacy concerns. We evaluate the effectiveness of our proposed technique using three real-world data set; The first two are Facebook data sets containing about 6 million users and more than 40 million friendship links. The third data set is from Twitter and comprises of a little over 2 million users. The results of our analysis show that our algorithm is up to 50% more accurate than existing algorithms. Results also show that the proposed algorithm can estimate the rank of the top-k information hubs users more accurately than existing approaches.\"",
        "title: \"Dynamic camouflage event based malicious node detection architecture\" with abstract: \"Compromised sensor nodes may collude to segregate a specific region of the sensor network preventing event reporting packets in this region from reaching the basestation. Additionally, they can cause skepticism over all data collected. Identifying and segregating such compromised nodes while identifying the type of attack with a certain confidence level is critical to the smooth functioning of a sensor network. Existing work specializes in preventing or identifying a specific type of attack and lacks a unified architecture to identify multiple attack types. Dynamic Camouflage Event-Based Malicious Node Detection Architecture (D-CENDA) is a proactive architecture that uses camouflage events generated by mobile-nodes to detect malicious nodes while identifying the type of attack. We exploit the spatial and temporal information of camouflage event while analyzing the packets to identify malicious activity. We have simulated D-CENDA to compare its performance with other techniques that provide protection against individual attack types and the results show marked improvement in malicious node detection while having significantly less false positive rate. Moreover, D-CENDA can identify the type of attack and is flexible to be configured to include other attack types in future.\"",
        "title: \"Fast range query processing with strong privacy protection for cloud computing\" with abstract: \"Privacy has been the key road block to cloud computing as clouds may not be fully trusted. This paper concerns the problem of privacy preserving range query processing on clouds. Prior schemes are weak in privacy protection as they cannot achieve index indistinguishability, and therefore allow the cloud to statistically estimate the values of data and queries using domain knowledge and history query results. In this paper, we propose the first range query processing scheme that achieves index indistinguishability under the indistinguishability against chosen keyword attack (IND-CKA). Our key idea is to organize indexing elements in a complete binary tree called PBtree, which satisfies structure indistinguishability (i.e., two sets of data items have the same PBtree structure if and only if the two sets have the same number of data items) and node indistinguishability (i.e., the values of PBtree nodes are completely random and have no statistical meaning). We prove that our scheme is secure under the widely adopted IND-CKA security model. We propose two algorithms, namely PBtree traversal width minimization and PBtree traversal depth minimization, to improve query processing efficiency. We prove that the worse case complexity of our query processing algorithm using PBtree is O(|R| log n), where n is the total number of data items and R is the set of data items in the query result. We implemented and evaluated our scheme on a real world data set with 5 million items. For example, for a query whose results contain ten data items, it takes only 0.17 milliseconds.\"",
        "title: \"Device-Free Human Activity Recognition Using Commercial WiFi Devices.\" with abstract: \"Since human bodies are good reflectors of wireless signals, human activities can be recognized by monitoring changes in WiFi signals. However, existing WiFi-based human activity recognition systems do not build models that can quantify the correlation between WiFi signal dynamics and human activities. In this paper, we propose a Channel State Information (CSI)-based human Activity Recognition and Monitoring system (CARM). CARM is based on two theoretical models. First, we propose a CSI-speed model that quantifies the relation between CSI dynamics and human movement speeds. Second, we propose a CSI-activity model that quantifies the relation between human movement speeds and human activities. Based on these two models, we implemented the CARM on commercial WiFi devices. Our experimental results show that the CARM achieves recognition accuracy of 96% and is robust to environmental changes.\"",
        "1 is \"MuJava: an automated class mutation system\", 2 is \"An improved algorithm to accelerate regular expression evaluation\".",
        "\nGiven above information, for an author who has written the paper with the title \"Designing Fast and Scalable XACML Policy Evaluation Engines\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0187": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Influence Of Selective Pressure On Quality Of Solutions And Speed Of Evolutionary Mastermind':",
        "title: \"Evolution of artificial terrains for video games based on accessibility\" with abstract: \"Diverse methods have been developed to generate terrains under constraints to control terrain features, but most of them use strict restrictions. However, there are situations were more flexible restrictions are sufficient, such as ensuring that terrains have enough accessible area, which is an important trait for video games. The Genetic Terrain Program technique, based on genetic programming, was used to automatically evolve Terrain Programs (TPs - which are able to generate terrains procedurally) for the desired accessibility parameters. Results showed that the accessibility parameters have negligible influence on the evolutionary system and that the terminal set has a major role on the terrain look. TPs produced this way are already being used on Chapas video game.\"",
        "title: \"Embedding Branch and Bound within Evolutionary Algorithms\" with abstract: \"A framework for hybridizing evolutionary algorithms with the branch-and-bound algorithm (B&B) is presented in this paper. This framework is based on using B&B as an operator embedded in the evolutionary algorithm. The resulting hybrid operator will intelligently explore the dynastic potential (possible children) of the solutions being recombined, providing the best combination of formae (generalized schemata) that can be constructed without introducing implicit mutation. As a basis for studying this operator, the general functioning of transmitting recombination is considered. Two important concepts are introduced, compatibility sets, and granularity of the representation. These concepts are studied in the context of different kinds of representation: orthogonal, non-orthogonal separable, and non-separable.The results of an extensive experimental evaluation are reported. It is shown that this model can be useful when problem knowledge is available in the form of an optimistic evaluation function. Scalability issues are also considered. A control mechanism is proposed to alleviate the increasing computational cost of the algorithm for highly multidimensional problems.\"",
        "title: \"Design of emergent and adaptive virtual players in a war RTS game\" with abstract: \"Basically, in (one-player) war Real Time Strategy (wRTS) games a human player controls, in real time, an army consisting of a number of soldiers and her aim is to destroy the opponent's assets where the opponent is a virtual (i.e., non-human player controlled) player that usually consists of a pre-programmed decision-making script. These scripts have usually associated some well-known problems (e.g., predictability, non-rationality, repetitive behaviors, and sensation of artificial stupidity among others). This paper describes a method for the automatic generation of virtual players that adapt to the player skills; this is done by building initially a model of the player behavior in real time during the game, and further evolving the virtual player via this model in-between two games. The paper also shows preliminary results obtained on a oneplayer wRTS game constructed specifically for experimentation.\"",
        "title: \"A memetic cooperative optimization schema and its application to the tool switching problem\" with abstract: \"This paper describes a generic (meta-)cooperative optimization schema in which several agents endowed with an optimization technique (whose nature is not initially restricted) cooperate to solve an optimization problem. These agents can use a wide set of optimization techniques, including local search, population-based methods, and hybrids thereof, hence featuring multilevel hybridization. This optimization approach is here deployed on the Tool Switching Problem (ToSP), a hard combinatorial optimization problem in the area of flexible manufacturing. We have conducted an ample experimental analysis involving a comparison of a wide number of algorithms or a large number of instances. This analysis indicates that some meta-cooperative instances perform significantly better than the rest of the algorithms, including a memetic algorithm that was the previous incumbent for this problem.\"",
        "title: \"An Analysis of a Selecto-Lamarckian Model of Multimemetic Algorithms with Dynamic Self-organized Topology.\" with abstract: \"Multimemetic algorithms (MMAs) are memetic algorithms that explicitly represent and evolve memes (computational representations of problem solving methods) as a part of solutions. We use an idealized selecto-Lamarckian model of MMAs in order to analyze the propagation of memes in spatially structured populations. To this end, we focus on the use of dynamic self-organized spatial structures, based on the stimergic communication among solutions, and compare these with regular static lattices and unstructured (panmictic) populations. An empirical analysis indicates that these dynamic lattices are capable of promoting memetic diversity and provide better results in terms of survival of high-quality memes.\"",
        "1 is \"Automating XML document structure transformations\", 2 is \"Edge sets: an effective evolutionary coding of spanning trees\".",
        "\nGiven above information, for an author who has written the paper with the title \"Influence Of Selective Pressure On Quality Of Solutions And Speed Of Evolutionary Mastermind\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0188": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'A New Class of Sequences With Zero or Low Correlation Zone Based on Interleaving Technique':",
        "title: \"HBC entity authentication for low-cost pervasive devices\" with abstract: \"The HB-like entity authentication protocols for low-cost pervasive devices have attracted a great deal of attention because of their simplicity, computational efficiency and solid security foundation on a well-studied hard problem-learning parity with noise. By far, the most efficient protocol is HB#, which is provably resistant to the GRS attack under the conjecture that it is secure in the DET-model. However, in order to achieve 80-bit security, a typical HB# authentication key comprises over 1000 bits, which imposes considerable storage burdens on resource-constrained devices. In this study, the authors propose a new HB-like protocol: HBC. The protocol makes use of a special type of circulant matrix, in contrast to the Toeplitz matrix in HB#, to significantly reduce storage consumption and overcome a subtle security proof inefficacy in HB#. In addition, the authors introduce a masking technique that substantially increases noise level from an adversary's standpoint, and thus improves protocol performance. The authors demonstrate that 613-bit authentication key suffices for 80-bit security in the HBC protocol, which is quite competitive and more appealing for low-cost devices.\"",
        "title: \"A new algorithm to compute remote terms in special types of characteristic sequences\" with abstract: \"This paper proposes a new algorithm, called the Diagonal Double-Add (DDA) algorithm, to compute the k-th term of special kinds of characteristic sequences. We show that this algorithm is faster than Fiduccia's algorithm, the current standard for computation of general sequences, for fourth- and fifth-order sequences.\"",
        "title: \"Generating Large Instances of the Gong-Harn Cryptosystem\" with abstract: \"In 1999, Gong and Harn proposed a new cryptosystem based on third-order characteristic sequences over finite fields. This paper gives an efficient method to generate instances of this cryptosystem over large finite fields. The method first finds a \"good\" prime p to work with and then constructs the sequence to ensure that it has the desired period. This method has been implemented in C++ using NTL [7] and so timing results are presented.\"",
        "title: \"A 32-bit RC4-like Keystream Generator\" with abstract: \"Abstract: In this paper we propose a new 32-bit RC4 like keystreamgenerator. The proposed generator produces 32 bits in each iteration andcan be implemented in software with reasonable memory requirements.\"",
        "title: \"Constructions Of Multiple Shift-Distinct Signal Sets With Low Correlation\" with abstract: \"In this paper, we introduce a concept of correlation of multiple shift-distinct signal sets, and make a connection between constructions of multiple binary signal sets with low maximum correlation and constructions of a binary signal set with larger size and low maximum correlation. We then present one construction for multiple shift-distinct binary signal sets with low maximum correlation using Kasami (small) signal sets (or generalized Kasami signals sets). We show that the constructed m Kasami signal sets, in which each sequence has period N = 2(2m) - 1, satisfies the tth-order shift-distinct property, which is a new concept introduced in this paper. As a by-product, this construction also yields some new pairs of m-sequences with different periods and three-valued crosscorrelation.\"",
        "1 is \"A 440-nA True Random Number Generator for Passive RFID Tags\", 2 is \"A class of three-weight cyclic codes.\".",
        "\nGiven above information, for an author who has written the paper with the title \"A New Class of Sequences With Zero or Low Correlation Zone Based on Interleaving Technique\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0189": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'A Hierarchical Signature Scheme for Robust Video Authentication using Secret Sharing':",
        "title: \"Coopetitive visual surveillance using model predictive control\" with abstract: \"Active cooperative sensing with multiple sensors is being actively researched in visual surveillance. However, active cooperative sensing often suffers from the delay in information exchange among the sensors and also from sensor reaction delays. This is because simplistic control strategies like Proportional Integral Differential (PID), that do not employ the look-ahead strategy, often fail to counterbalance these delays at real time. Hence, there is a need for more sophisticated interaction and control mechanisms that can overcome the delay problems. In this paper, we propose a coopetitive framework using Model Predictive Control (MPC) which allows the sensors to not only 'compete' as well as 'cooperate' with each other to perform the designated task in the best possible manner but also to dynamically swap their roles and sub-goals rather than just the parameters. MPC is used as a feedback control mechanism to allow sensors to react not only based on past observations but also on possible future events. We demonstrate the utility of our framework in a dual camera surveillance setup with the goal of capturing the high resolution images of intruders in the surveyed rectangular area e.g. an ATM lobby or a museum. The results are promising and clearly establish the efficacy of coopetition as an effective form of interaction between sensors and MPC as a superior feedback mechanism than the PID.\"",
        "title: \"Privacy modeling for video data publication\" with abstract: \"Video cameras are being extensively used in many applications. Huge amounts of video are being recorded and stored everyday by surveillance systems. Any proposed application of this data raises severe privacy concerns. An assessment of privacy loss is necessary before any potential application of the data. In traditional methods of privacy modeling, researchers have focused on explicit means of identity leakage like facial information, etc. However, other implicit inference channels through which individual's an identity can be learned have not been considered. For example, an adversary can observe the behavior, look at the places visited and combine that with the temporal information to infer the identity of the person in the video. In this work, we thoroughly investigate privacy issues involved with the video data considering both implicit and explicit channels. We first establish an analogy with the statistical databases and then propose a model to calculate the privacy loss that might occur due to publication of the video data. The experimental results demonstrate the utility of the proposed model.\"",
        "title: \"Toward a Remote-Controlled Weapon-Equipped Camera Surveillance System\" with abstract: \"Camera surveillance systems have proved useful for public safety. The main disadvantage is that since the camera views are monitored in a remote control room, it is often difficult for security officers to reach the crime-scene in time. During this time, the assailant(s) have likely caused sufficient damage and threaten many lives. To overcome this problem, this paper proposes to take a standard surveillance system, augment it with simple weaponry for the purpose of disabling potential assailants, and use mathematical models to develop decision criteria for selecting the safest and most effective weapon in a given situation. The feasibility of the proposed system is examined using simulation results which also validate the utility of the proposed decision models.\"",
        "title: \"Secret sharing approach for securing cloud-based pre-classification volume ray-casting\" with abstract: \"With the evolution in cloud computing, cloud-based volume rendering, which outsources data rendering tasks to cloud datacenters, is attracting interest. Although this new rendering technique has many advantages, allowing third-party access to potentially sensitive volume data raises security and privacy concerns. In this paper, we address these concerns for cloud-based pre-classification volume ray-casting by using Shamir's (k, n) secret sharing and its variant (l, k, n) ramp secret sharing, which are homomorphic to addition and scalar multiplication operations, to hide color information of volume data/images in datacenters. To address the incompatibility issue of the modular prime operation used in secret sharing technique with the floating point operations of ray-casting, we consider excluding modular prime operation from secret sharing or converting the floating number operations of ray-casting to fixed point operations --- the earlier technique degrades security and the later degrades image quality. Both these techniques, however, result in significant data overhead. To lessen the overhead at the cost of high security, we propose a modified ramp secret sharing scheme that uses the three color components in one secret sharing polynomial and replaces the shares in floating point with smaller integers.\"",
        "title: \"Determining trust in media-rich websites using semantic similarity\" with abstract: \"Significant growth of multimedia content on the World Wide Web (or simply `Web') has made it an essential part of peoples lives. The web provides enormous amount of information, however, it is very important for the users to be able to gauge the trustworthiness of web information. Users normally access content from the first few links provided to them by search engines such as Google or Yahoo!. This is assuming that these search engines provide factual information, which may be popular due to criteria such as page rank but may not always be trustworthy from the factual aspects. This paper presents a mechanism to determine trust of websites based on the semantic similarity of their multimedia content with already established and trusted websites. The proposed method allows for dynamic computation of the trust level of websites of different domains and hence overcomes the dependency on traditional user feedback methods for determining trust. In fact, our method attempts to emulate the evolving process of trust that takes place in a user's mind. The experimental results have been provided to demonstrate the utility and practicality of the proposed method.\"",
        "1 is \"Recursive Least Squares Dictionary Learning Algorithm\", 2 is \"A design methodology for selection and placement of sensors in multimedia surveillance systems\".",
        "\nGiven above information, for an author who has written the paper with the title \"A Hierarchical Signature Scheme for Robust Video Authentication using Secret Sharing\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0190": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Computational highlight holography':",
        "title: \"Spectral sampling of manifolds\" with abstract: \"A central problem in computer graphics is finding optimal sampling conditions for a given surface representation. We propose a new method to solve this problem based on spectral analysis of manifolds which results in faithful reconstructions and high quality isotropic samplings, is efficient, out-of-core, feature sensitive, intuitive to control and simple to implement. We approach the problem in a novel way by utilizing results from spectral analysis, kernel methods, and matrix perturbation theory. Change in a manifold due to a single point is quantified by a local measure that limits the change in the Laplace-Beltrami spectrum of the manifold. Hence, we do not need to explicitly compute the spectrum or any global quantity, which makes our algorithms very efficient. Although our main focus is on sampling surfaces, the analysis and algorithms are general and can be applied for simplifying and resampling point clouds lying near a manifold of arbitrary dimension.\"",
        "title: \"Analysis and synthesis of point distributions based on pair correlation\" with abstract: \"Analyzing and synthesizing point distributions are of central importance for a wide range of problems in computer graphics. Existing synthesis algorithms can only generate white or blue-noise distributions with characteristics dictated by the underlying processes used, and analysis tools have not been focused on exploring relations among distributions. We propose a unified analysis and general synthesis algorithms for point distributions. We employ the pair correlation function as the basis of our methods and design synthesis algorithms that can generate distributions with given target characteristics, possibly extracted from an example point set, and introduce a unified characterization of distributions by mapping them to a space implied by pair correlations. The algorithms accept example and output point sets of different sizes and dimensions, are applicable to multi-class distributions and non-Euclidean domains, simple to implement and run in O(n) time. We illustrate applications of our method to real world distributions.\"",
        "title: \"Computational highlight holography\" with abstract: \"Computational highlight holography converts three-dimensional computer models into mechanical \"holograms\" fabricated on (specular) reflective or refractive materials. The surface consists of small grooves with patches of paraboloids or hyperboloids, each of which produces a highlight when illuminated by a directional light. Each highlight appears in different places for different view directions, with the correct binocular and motion parallax corresponding to a virtual 3D point position. Our computational pipeline begins with a 3D model and desired view position, samples the model to generate points that depict its features accurately, and computes a maximal set of non-overlapping patches to be embedded in the surface. We provide a preview of the hologram for the user, then fabricate the surface using a computer-controlled engraving machine. We show a variety of different fabricated holograms: reflective, transmissive, and holograms with color and proper shading. We also present extensions to stationary and animated 2D stippled images.\"",
        "title: \"Deforming meshes that split and merge\" with abstract: \"We present a method for accurately tracking the moving surface of deformable materials in a manner that gracefully handles topological changes. We employ a Lagrangian surface tracking method, and we use a triangle mesh for our surface representation so that fine features can be retained. We make topological changes to the mesh by first identifying merging or splitting events at a particular grid resolution, and then locally creating new pieces of the mesh in the affected cells using a standard isosurface creation method. We stitch the new, topologically simplified portion of the mesh to the rest of the mesh at the cell boundaries. Our method detects and treats topological events with an emphasis on the preservation of detailed features, while simultaneously simplifying those portions of the material that are not visible. Our surface tracker is not tied to a particular method for simulating deformable materials. In particular, we show results from two significantly different simulators: a Lagrangian FEM simulator with tetrahedral elements, and an Eulerian grid-based fluid simulator. Although our surface tracking method is generic, it is particularly well-suited for simulations that exhibit fine surface details and numerous topological events. Highlights of our results include merging of viscoplastic materials with complex geometry, a taffy-pulling animation with many fold and merge events, and stretching and slicing of stiff plastic material.\"",
        "title: \"Spatio-temporal geometry fusion for multiple hybrid cameras using moving least squares surfaces\" with abstract: \"Multi-view reconstruction aims at computing the geometry of a scene observed by a set of cameras. Accurate 3D reconstruction of dynamic scenes is a key component for a large variety of applications, ranging from special effects to telepresence and medical imaging. In this paper we propose a method based on Moving Least Squares surfaces which robustly and efficiently reconstructs dynamic scenes captured by a calibrated set of hybrid color+depth cameras. Our reconstruction provides spatio-temporal consistency and seamlessly fuses color and geometric information. We illustrate our approach on a variety of real sequences and demonstrate that it favorably compares to state-of-the-art methods.\"",
        "1 is \"Efficient Dense Stereo with Occlusions for New View-Synthesis by Four-State Dynamic Programming\", 2 is \"Seeing people in different light--joint shape, motion, and reflectance capture.\".",
        "\nGiven above information, for an author who has written the paper with the title \"Computational highlight holography\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0191": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Symmetry-Aware Nonrigid Matching of Incomplete 3D Surfaces':",
        "title: \"Estimating fluid simulation parameters from videos\" with abstract: \"Recently, a video-based high quality 3D shape and motion modeling methods for fluid are proposed. [Huamin et al. 2009] However, this approach only aims to capture and generate original fluid action as it is.\"",
        "title: \"Effective Nearest Neighbor Search for Aligning and Merging Range Images\" with abstract: \"This paper describes a novel method which extends the search algorithm of a k-d tree for aligning and merging range images. If the nearest neighbor point is far from a query, many of the leaf nodes must be examined during the search, which actually will not finish in logarithmic time. However such a distant point is not as important as the nearest neighbor in many, applications, such as aligning and merging range images; the reason for this is either because it is not consequently used or because its weight becomes very small. Thus, in this paper we propose a new algorithm that does not search strictly by pruning branches if the nearest neighbor point lies beyond a certain threshold. We call the technique the Bounds-Overlap-Threshold (BOT) test. The BOT test can be applied without re-creating the k-d tree if the threshold value changes. Then, we describe how we applied our new method to three applications in order to analyze its performance. Finally, we discuss the method's effectiveness.\"",
        "title: \"A Probabilistic Method for Aligning and Merging Range Images with Anisotropic Error Distribution\" with abstract: \"This paper describes a probabilistic method of aligning and merging range images. We formulate these issues as problems of estimating the maximum likelihood. By examining the error distribution of a range finder, we model it as a normal distribution along the line of sight. To align range images, our method estimates the parameters based on the ExpectationMaximization (EM) approach. By assuming the error model, the algorithm is implemented as an extension of the Iterative Closest Point (ICP) method. For merging range images, our method computes the signed distances by finding the distances of maximum likelihood. Since our proposed method uses multiple correspondences for each vertex of the range images, errors after aligning and merging range images are less than those of earlier methods that use one-to-one correspondences. Finally, we tested and validated the efficiency of our method by simulation and on real range images .\"",
        "title: \"Adaptive-Scale robust estimator using distribution model fitting\" with abstract: \"We propose a new robust estimator for parameter estimation in highly noisy data with multiple structures and without prior information on the noise scale of inliers This is a diagnostic method that uses random sampling like RANSAC, but adaptively estimates the inlier scale using a novel adaptive scale estimator The residual distribution model of inliers is assumed known, such as a Gaussian distribution Given a putative solution, our inlier scale estimator attempts to extract a distribution for the inliers from the distribution of all residuals This is done by globally searching a partition of the total distribution that best fits the Gaussian distribution Then, the density of the residuals of estimated inliers is used as the score in the objective function to evaluate the putative solution The output of the estimator is the best solution that gives the highest score Experiments with various simulations and real data for line fitting and fundamental matrix estimation are carried out to validate our algorithm, which performs better than several of the latest robust estimators.\"",
        "title: \"One-shot range scanner using coplanarity constraints\" with abstract: \"Methods for scanning dynamic scenes are important in many applications and many systems using structured light have been proposed. Many of these systems use either mul- tiple patterns projected rapidly or a single pattern. Although the former allows dense reconstruction with a sufficient num- ber of patterns, it has difficulty in capturing objects in rapid motion. The latter technique uses only a single pattern and have no such difficulties, however, they often have stability problems and their result tend to have low resolution. In this paper, we develop a system to achieve dense and accurate 3D measurement from only a single image. The proposed sys- tem also has the advantage of being robust in terms of image processing.\"",
        "1 is \"Shape reconstruction from cast shadows using coplanarities and metric constraints\", 2 is \"Path planning for mobile manipulators for multiple task execution\".",
        "\nGiven above information, for an author who has written the paper with the title \"Symmetry-Aware Nonrigid Matching of Incomplete 3D Surfaces\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0192": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Learning Tag Embeddings And Tag-Specific Composition Functions In Recursive Neural Network':",
        "title: \"A Generative Probabilistic Model for Multi-label Classification\" with abstract: \"Traditional discriminative classification method makes little attempt to reveal the probabilistic structure and the correlation within both input and output spaces. In the scenario of multi-label classification, most of the classifiers simply assume the predefined classes are independently distributed, which would definitely hinder the classification performance when there are intrinsic correlations between the classes. In this article, we propose a generative probabilistic model, the Correlated Labeling Model (CoL Model), to formulate the correlation between different classes. The CoL model is presented to capture the correlation between classes and the underlying structures via the latent random variables in a supervised manner. We develop a variational procedure to approximate the posterior distribution and employ the EM algorithm for the empirical Bayes parameter estimation. In our evaluations, the proposed model achieved promising results on various data sets.\"",
        "title: \"Recognizing biomedical named entities using skip-chain conditional random fields\" with abstract: \"Linear-chain Conditional Random Fields (CRF) has been applied to perform the Named Entity Recognition (NER) task in many biomedical text mining and information extraction systems. However, the linear-chain CRF cannot capture long distance dependency, which is very common in the biomedical literature. In this paper, we propose a novel study of capturing such long distance dependency by defining two principles of constructing skip-edges for a skip-chain CRF: linking similar words and linking words having typed dependencies. The approach is applied to recognize gene/protein mentions in the literature. When tested on the BioCreAtIvE II Gene Mention dataset and GENIA corpus, the approach contributes significant improvements over the linear-chain CRF. We also present in-depth error analysis on inconsistent labeling and study the influence of the quality of skip edges on the labeling performance.\"",
        "title: \"Story Ending Generation with Incremental Encoding and Commonsense Knowledge.\" with abstract: \"Generating a reasonable ending for a given story context, i.e., story ending generation, is a strong indication of story comprehension. This task requires not only to understand the context clues which play an important role in planning the plot, but also to handle implicit knowledge to make a reasonable, coherent story. In this paper, we devise a novel model for story ending generation. The model adopts an incremental encoding scheme to represent context clues which are spanning in the story context. In addition, commonsense knowledge is applied through multi-source attention to facilitate story comprehension, and thus to help generate coherent and reasonable endings. Through building context clues and using implicit knowledge, the model is able to produce reasonable story endings. Automatic and manual evaluation shows that our model can generate more reasonable story endings than state-of-the-art baselines.(1)\"",
        "title: \"A Novel Method for Dialogue Management Based on The Finite State Automaton\" with abstract: \"Recently spoken dialogue systems in a range of domains have been developed, but there are still no commonly acceptable methods for the dialogue management. In this paper, we propose a novel scheme for it, in which a finite state automaton based on slot-feature is designed to establish the whole dialogue management structure. Two functions are used to implement the strategy control in dialogue session and proved to work pretty well and be efficient. Furthermore, intention-layered trees are proposed and applied to the topic detection and switch, which is quite intuitive and functionary in the multi-topics applications. Me experimental results show that our scheme is promising.\"",
        "title: \"Exploring weakly supervised latent sentiment explanations for aspect-level review analysis\" with abstract: \"In sentiment analysis, aspect-level review analysis has been an important task because it can catalogue, aggregate, or summarize various opinions according to a product's properties. In this paper, we explore a new concept for aspect-level review analysis, latent sentiment explanations, which are defined as a set of informative aspect-specific sentences whose polarities are consistent with that of the review. In other words, sentiment explanations best represent a review in terms of both aspect and polarity. We formulate the problem as a structure learning problem, and sentiment explanations are modeled with latent variables. Training samples are automatically identified through a set of pre-defined aspect signature terms (i.e., without manual annotation on samples), which we term the way weakly supervised. Our major contributions lie in two folds: first, we formalize the use of aspect signature terms as weak supervision in a structural learning framework, which remarkably promotes aspect-level analysis; second, the performance of aspect analysis and document-level sentiment classification are mutually enhanced through joint modeling. The proposed method is evaluated on restaurant and hotel reviews respectively, and experimental results demonstrate promising performance in both document-level and aspect-level sentiment analysis.\"",
        "1 is \"Joint sentiment/topic model for sentiment analysis\", 2 is \"A Tree Sequence Alignment-based Tree-to-Tree Translation Model\".",
        "\nGiven above information, for an author who has written the paper with the title \"Learning Tag Embeddings And Tag-Specific Composition Functions In Recursive Neural Network\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0193": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'An introduction to string re-writing kernel':",
        "title: \"A QoS Multicast Routing Protocol with Mobile Prediction Based on MAODV in MANETs\" with abstract: \"Qos multicast routing is the process for upbuilding a tree which is rooted from the source node, contains all the multicast destinations and satisfies the QoS constrains. This paper introduces QoS routing problem and several mobile prediction strategies. It depicts QoS multicast model and proposes a QoS multicast routing protocol with mobile prediction based on MAODV (MPMRPQ). Simulation results demonstrate that the scheme is an effective approach to multicast routing decision with multiple QoS constrains. The proposed scheme can reduce multicast packet loss, network overload and optimize the delivery path.\"",
        "title: \"Recognizing biomedical named entities using skip-chain conditional random fields\" with abstract: \"Linear-chain Conditional Random Fields (CRF) has been applied to perform the Named Entity Recognition (NER) task in many biomedical text mining and information extraction systems. However, the linear-chain CRF cannot capture long distance dependency, which is very common in the biomedical literature. In this paper, we propose a novel study of capturing such long distance dependency by defining two principles of constructing skip-edges for a skip-chain CRF: linking similar words and linking words having typed dependencies. The approach is applied to recognize gene/protein mentions in the literature. When tested on the BioCreAtIvE II Gene Mention dataset and GENIA corpus, the approach contributes significant improvements over the linear-chain CRF. We also present in-depth error analysis on inconsistent labeling and study the influence of the quality of skip edges on the labeling performance.\"",
        "title: \"Verification Code Recognition Based on Active and Deep Learning\" with abstract: \"A verification code is an automated test method used to distinguish between humans and computers. Humans can easily identify verification codes, whereas machines cannot. With the development of convolutional neural networks, automatically recognizing a verification code is now possible for machines. However, the advantages of convolutional neural networks depend on the data used by the training classifier, particularly the size of the training set. Therefore, identifying a verification code using a convolutional neural network is difficult when training data are insufficient. This study proposes an active and deep learning strategy to obtain new training data on a special verification code set without manual intervention. A feature learning model for a scene with less training data is presented in this work, and the verification code is identified by the designed convolutional neural network. Experiments show that the method can considerably improve the recognition accuracy of a neural network when the amount of initial training data is small.\"",
        "title: \"Promoting diversity in recommendation by entropy regularizer\" with abstract: \"We study the problem of diverse promoting recommendation task: selecting a subset of diverse items that can better predict a given user's preference. Recommendation techniques primarily based on user or item similarity can suffer from the risk that users cannot get expected information from the over-specified recommendation lists. In this paper, we propose an entropy regularizer to capture the notion of diversity. The entropy regularizer has good properties in that it satisfies monotonicity and submodularity, such that when we combine it with a modular rating set function, we get submodular objective function, which can be maximized approximately by efficient greedy algorithm, with provable constant factor guarantee of optimality. We apply our approach on the top-K prediction problem and evaluate its performance on Movie-Lens data set, which is a standard database containing movie rating data collected from a popular online movie recommender system. We compare our model with the state-of-the-art recommendation algorithms. Our experiments show that entropy regularizer effectively captures diversity and hence improves the performance of recommendation task.\"",
        "title: \"Learning Tag Embeddings And Tag-Specific Composition Functions In Recursive Neural Network\" with abstract: \"Recursive neural network is one of the most successful deep learning models for natural language processing due to the compositional nature of text. The model recursively composes the vector of a parent phrase from those of child words or phrases, with a key component named composition function. Although a variety of composition functions have been proposed, the syntactic information has not been fully encoded in the composition process. We propose two models, Tag Guided RNN (TG-RNN for short) which chooses a composition function according to the part-of-speech tag of a phrase, and Tag Embedded RNN/RNTN (TE-RNN/RNTN for short) which learns tag embeddings and then combines tag and word embeddings together. In the fine-grained sentiment classification, experiment results show the proposed models obtain remarkable improvement: TG-RNN/TE-RNN obtain remarkable improvement over baselines, TE-RNTN obtains the second best result among all the top performing models, and all the proposed models have much less parameters/complexity than their counter-parts.\"",
        "1 is \"Privacy-Preserving Matching of DNA Profiles\", 2 is \"Tri-Training: Exploiting Unlabeled Data Using Three Classifiers\".",
        "\nGiven above information, for an author who has written the paper with the title \"An introduction to string re-writing kernel\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0194": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'RBN-World: a sub-symbolic artificial chemistry':",
        "title: \"On the Advantages of Variable Length GRNs for the Evolution of Multicellular Developmental Systems\" with abstract: \"Biological genomes have evolved over a period of millions of years and comprise thousands of genes, even for the simplest organisms. However, in nature, only 1\u20132% of the genes play an active role in creating and maintaining the organism, while the majority are evolutionary fossils. This raises the question of whether a considerably larger number of (partly redundant) genes are required in order to effectively build a functional developmental system, of which, in the final system only a fraction is required for the latter to function. This paper investigates different approaches to creating artificial developmental systems (ADSs) based on variable length gene regulatory networks (GRNs). The GRNs are optimized using an evolutionary algorithm (EA). A comparison is made between the different variable length representations and fixed length representations. It is shown that variable length GRNs can achieve both reducing computational effort during optimization and increasing speed and compactness of the resulting ADS, despite the higher complexity of the encoding required. The results may also improve the understanding of how to effectively model GRN based developmental systems. Taking results of all experiments into account makes it possible to create an overall ranking of the different patterns used as a testbench in terms of their complexity. This ranking may aid to compare related work against. In addition, this allows a detailed assessment of the ADS used and enables the identification of missing mechanisms.\"",
        "title: \"Self Modifying Cartesian Genetic Programming: Fibonacci, Squares, Regression and Summing\" with abstract: \"Self Modifying CGP (SMCGP) is a developmental form of Cartesian Genetic Programming(CGP). It is able to modify its own phenotype during execution of the evolved program. This is done by the inclusion of modification operators in the function set. Here we present the use of the technique on several different sequence generation and regression problems.\"",
        "title: \"A biological development model for the design of robust multiplier\" with abstract: \"A biologically inspired developmental model targeted at hardware implementation (off-shelf FPGA) is proposed which exhibits extremely robust transient fault-tolerant capability. All cells in this model have identical genotype (physical structures), and only differ in internal states. In a 3x3 cell digital organism, some individuals which implement a 2-bit multiplier were discovered using evolution that have the ability to \u201crecover\u201d themselves from almost any kinds of transient faults. An intrinsic evolvable hardware platform based on FPGA was realized to speed up the evolution process.\"",
        "title: \"A model for intrinsic artificial development featuring structural feedback and emergent growth\" with abstract: \"A model for intrinsic artificial development is introduced in this paper. The proposed model features a novel mechanism where growth emerges, rather than being triggered by a single action. Different types of cell signalling ensure that breaking symmetries is rather the norm than an exception, and gene activity is regulated on two layers: first, by the proteins that are produced by the gene regulatory network (GRN). Second, through structural feedback by second messenger molecules, which are not directly produced through gene expression, but are produced by sensor proteins, which take the cell's structure into account. The latter feedback mechanism is a novel approach, intended to enable adaptivity and environment coupling in real-world applications. The model is implemented in hardware, and is designed to run autonomously in resource limited embedded systems. Initial experiments are carried out to measure long-term stability, dynamics, adaptivity and scalability of the new approach. Furthermore the ability of the GRN to produce patterns of different symmetries is examined.\"",
        "title: \"Intrinsic evolvable hardware implementation of a robust biological development model for digital systems\" with abstract: \"An intrinsic evolvable hardware platform was realized to accelerate the evolutionary search process of a biologically inspired developmental model targeted at off-shelf FPGA implementation. The model has the capability of exhibiting very large transient fault-tolerance. The evolved circuits make up a digital \"organism\" from identical cells which only differ in internal states. Organisms implementing a 2-bit multiplier were evolved that can \"recover\" from almost any kinds of transient faults. This paper focuses on the design concerns and details of the evolvable hardware system, including the digital organism/cell and the intrinsic FPGA-based evolvable hardware platform.\"",
        "1 is \"Sharp Retrenchment, Modulated Refinement and Simulation.\", 2 is \"The arcade learning environment: an evaluation platform for general agents\".",
        "\nGiven above information, for an author who has written the paper with the title \"RBN-World: a sub-symbolic artificial chemistry\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0195": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'A comparison of methods for sketch-based 3D shape retrieval':",
        "title: \"Chosen Ciphertext Security via Point Obfuscation.\" with abstract: \"In this paper, we show two new constructions of chosen ciphertext secure (CCA secure) public key encryption (PKE) from general assumptions. The key ingredient in our constructions is an obfuscator for point functions with multi-bit output (MBPF obfuscators, for short), that satisfies some (average-case) indistinguishability-based security, which we call AIND security, in the presence of hard-to-invert auxiliary input. Specifically, our first construction is based on a chosen plaintext secure PKE scheme and an MBPF obfuscator satisfying the AIND security in the presence of computationally hard-to-invert auxiliary input. Our second construction is based on a lossy encryption scheme and an MBPF obfuscator satisfying the AIND security in the presence of statistically hard-to-invert auxiliary input. To clarify the relative strength of AIND security, we show the relations among security notions for MBPF obfuscators, and show that AIND security with computationally (resp. statistically) hard-to-invert auxiliary input is implied by the average-case virtual black-box (resp. virtual grey-box) property with the same type of auxiliary input. Finally, we show that a lossy encryption scheme can be constructed from an obfuscator for point functions (point obfuscator) that satisfies re-randomizability and a weak form of composability in the worst-case virtual grey-box sense. This result, combined with our second generic construction and several previous results on point obfuscators and MBPF obfuscators, yields a CCA secure PKE scheme that is constructed solely from a re-randomizable and composable point obfuscator. We believe that our results make an interesting bridge that connects CCA secure PKE and program obfuscators, two seemingly isolated but important cryptographic primitives in the area of cryptography.\"",
        "title: \"Relations between constrained and bounded chosen ciphertext security for key encapsulation mechanisms\" with abstract: \"In CRYPTO 2007, Hofheinz and Kiltz formalized a security notion for key encapsulation mechanisms (KEMs), called constrained chosen ciphertext (CCCA) security, which is strictly weaker than ordinary chosen ciphertext (CCA) security, and showed a new composition paradigm for CCA secure hybrid encryption. Thus, CCCA security of a KEM turned out to be quite useful. However, since the notion is relatively new and its definition is slightly complicated, relations among CCCA security and other security notions have not been clarified well. In this paper, in order to better understand CCCA security and the construction of CCCA secure KEMs, we study relations between CCCA and bounded CCA security, where the latter notion considers security against adversaries that make a-priori bounded number of decapsulation queries, and is also strictly weaker than CCA security. Specifically, we show that in most cases there are separations between these notions, while there is some unexpected implication from (a slightly stronger version of) CCCA security to a weak form of 1-bounded CCA security. We also revisit the construction of a KEM from a hash proof system (HPS) with computational security properties, and show that the HPS-based KEM, which was previously shown CCCA secure, is actually 1-bounded CCA secure as well. This result, together with the above general implication, suggests that 1-bounded CCA security can be essentially seen as a \u2018\u2018necessary\" condition for a CCCA secure KEM.\"",
        "title: \"Recognition of Layout-Free Characters on Complex Background\" with abstract: \"Recognizing characters in a scene is a challenging and unsolved problem. In this demonstration, we show an effective approach to cope with the problems: recognizing Japanese characters including complex characters such as Kanji (Chinese characters), which may not be aligned on a straight line and may be printed on a complex background. In the demo, our recognition method is applied to image sequences captured with a web camera. The recognition method is based on local features and their alignment. In addition, using a tracking method, recognition results and extracted features are accumulated so as to increase recognition accuracy as time goes on. The demo runs about 1 fps on a standard laptop computer.\"",
        "title: \"Performance analysis of TCP fairness between wired and wireless sessions\" with abstract: \"A significant amount of wireless traffic will be car- ried in the Internet, and wireless connections need to share the network resources with wired connections. However, in a wire- less network environment, TCP, one of the most important trans- port protocol of TCP/IP, suffers from significant throughput degra- dation due to the lossy characteristics of a wireless link. There- fore, in order to design the next generation mobile networks, it is necessary to know how much the wireless connection suf- fers from the degradation in comparison to the wired connec- tion. In this paper, we discuss the fairness issue between TCP connections over wireless and wired links, and theoretically an- alyze throughput fairness between TCP over wireless link with ARQ(Automatic Repeat reQuest)-based link layer error recov- ery and TCP over error-free wired link. We validate our anal- ysis by comparing numerical results obtained from the analysis with computer simulation ones.\"",
        "title: \"Impact of round trip delay self-similarity on TCP performance\" with abstract: \"Previous measurement showed that self-similar nature is found not only in network traffic volume but also round trip packet delay. In this paper, we discuss three issues of the self-similarity of round trip time (RTT), which is one of the most important parameters to determine TCP throughput performance. First, we discuss the origin of the packet delay self-similarity. One study anticipated that the queueing delay of self-similar traffic is the reason for packet delay self-similarity. With computer simulation, we evaluate the correlation between traffic and RTT self-similarity. Next, we investigate the impact of RTT self-similarity on TCP throughput performance. Computer simulation results show that RTT self-similarity gives high variability to file transfer time. Finally, we investigate the impact of RTT self-similarity on RTO (retransmission time out). We discover that the bigger the Hurst parameter of the RTT is, the more frequent unnecessary timeouts occurs. Furthermore, we propose a new RTO calculation algorithm to improve these unnecessary timeouts\"",
        "1 is \"Improved Non-committing Encryption Schemes Based on a General Complexity Assumption\", 2 is \"A user study on visualizing directed edges in graphs\".",
        "\nGiven above information, for an author who has written the paper with the title \"A comparison of methods for sketch-based 3D shape retrieval\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0196": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Traffic Anomaly Detection Based On Robust Principal Component Analysis Using Periodic Traffic Behavior':",
        "title: \"Bounds for the tail distribution in a queue with a superposition of general periodic Markov sources: theory and application\" with abstract: \"An efficient yet accurate estimation of the tail distribution of the queue length has been considered as one of the most important issues in call admission and congestion controls in ATM networks. The arrival process in ATM networks is essentially a superposition of sources which are typically bursty and periodic either due to their origin or their periodic slot occupation after traffic shaping. In this paper, we consider a discrete-time queue where the arrival process is a superposition of general periodic Markov sources. The general periodic Markov source is rather general since it is assumed only to be irreducible, stationary and periodic. Note also that the source model can represent multiple time-scale correlations in arrivals. For this queue, we obtain upper and lower bounds for the asymptotic tail distribution of the queue length by bounding the asymptotic decay constant. The formulas can be applied to a queue having a huge number of states describing the arrival process. To show this, we consider an MPEG-like source which is a special case of general periodic Markov sources. The MPEG-like source has three time-scale correlations: peak rate, frame length and a group of pictures. We then apply our bound formulas to a queue with a superposition of MPEG-like sources, and provide some numerical examples to show the numerical feasibility of our bounds. Note that the number of states in a Markov chain describing the superposed arrival process is more than 1.4 \u00d7 10^{88}. Even for such a queue, the numerical examples show that the order of the magnitude of the tail distribution can be readily obtained.\"",
        "title: \"MAP/M/c and M/PH/c queues with constant impatience times\" with abstract: \"This paper considers stationary MAP/M/c and M/PH/c queues with constant impatience times. In those queues, waiting customers leave the system without receiving their services if their elapsed waiting times exceed a predefined deterministic threshold. For the MAP/M/c queue with constant impatience times, Choi et al. (Math Oper Res 29:309\u2013325, ) derive the virtual waiting time distribution, from which the loss probability and the actual waiting time distribution are obtained. We first refine their result for the virtual waiting time and then derive the stationary queue length distribution. We also discuss the computational procedure for performance measures of interest. Next we consider the stationary M/PH/c queue with constant impatience times and derive the loss probability, the waiting time distribution, and the queue length distribution. Some numerical results are also provided.\"",
        "title: \"Nonlinear Integer Programming Formulation For Quasi-Optimal Grouping Of Clusters In Ferry-Assisted Dtns\" with abstract: \"Communication among isolated networks (clusters) in delay tolerant networks (DTNs) can be supported by a message ferry, which collects bundles from clusters and delivers them to a sink node. When there are lots of distant static clusters, multiple message ferries and sink nodes will be required. In this paper, we aim to make groups, each of which consists of physically close clusters, a sink node, and a message ferry. Our objective is minimizing the overall mean delivery delay of bundles in consideration of both the offered load of clusters and distances between clusters and their sink nodes. Based on existing work, we first model this problem as a nonlinear integer programming. Using a commercial nonlinear solver, we obtain a quasi-optimal grouping. Through numerical evaluations, we show the fundamental characteristics of grouping, the impact of location limitation of base clusters, and the relationship between delivery delay and the number of base clusters.\"",
        "title: \"A generalization of the decomposition property in the M/G/1 queue with server vacations\" with abstract: \"This paper considers the M/G/1 queueing systems with server vacations. For a very general class of such systems, Fuhrmann and Cooper have shown that the stationary queue length at a random point in time is distributed as the sum of two independent random variables, one of which is the stationary queue length at a random point in time in the corresponding standard M/G/1 queue. This property is called the stochastic decomposition in the M/G/1 queue with server vacations. In this paper, we show that this decomposition property is also valid for the joint probability distribution of the queue length and the forward recurrence service time.\"",
        "title: \"Mean Buffer Contents in Discrete-Time Single-Server Queues with Heterogeneous Sources\" with abstract: \"We consider discrete-time single-server queues fed by independent, heterogeneous sources with geometrically distributed idle periods. While being active, each source generates some cells depending on the state of the underlying Markov chain. We first derive a general and explicit formula for the mean buffer contents in steady state when the underlying Markov chain of each source has finite states. Next we show the applicability of the general formula to queues fed by independent sources with infinite-state underlying Markov chains and discrete phase-type active periods. We then provide explicit formulas for the mean buffer contents in queues with Markovian autoregressive sources and greedy sources. Further we study two limiting cases in general settings, one is that the lengths of active periods of each source are governed by an infinite-state absorbing Markov chain, and the other is the model obtained by the limit such that the number of sources goes to infinity under an appropriate normalizing condition. As you will see, the latter limit leads to a queue with (generalized) M/G/\u9a74 input sources. We provide sufficient conditions under which the general formula is applicable to these limiting cases.\"",
        "1 is \"Some (in)sufficient conditions for secure hybrid encryption\", 2 is \"Generalized guaranteed rate scheduling algorithms: a framework\".",
        "\nGiven above information, for an author who has written the paper with the title \"Traffic Anomaly Detection Based On Robust Principal Component Analysis Using Periodic Traffic Behavior\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0197": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'BorderGuard: detecting cold potatoes from peers':",
        "title: \"Midas: An Impact Scale For Ddos Attacks\" with abstract: \"We usually have well-defined classification scales to estimate the intensity and impact of natural disasters. Prominent examples are the Richter and the Fujita scales for measuring earthquakes and tornadoes respectively. In this paper, we apply similar ideas to estimate the impact of distributed denial of service (DDoS) attacks from the perspective of network operators. Devising such a classification scale improves our understanding of DDoS attacks by assessing the actual damage incurred from an ISP's perspective, and allows comparison of various mitigation strategies. We have designed MIDAS, a DDoS impact scale, based on the economic impact of a DDoS attack, calculated using economic and network data. We then present an approximation of the MIDAS scale that relies only on network measurements for ease of computation. To demonstrate the usefulness of the scale, we perform sensitivity analysis to qualitatively validate the magnitude of the scale value for diverse attacks.\"",
        "title: \"Ensemble: Community-Based Anomaly Detection for Popular Applications.\" with abstract: \"A major challenge in securing end-user systems is the risk of popular applications being hijacked at run-time. Traditional measures do not prevent such threats because the code itself is unmodified and local anomaly detectors are difficult to tune for correct thresholds due to insufficient training data. Given that the target of attackers are often popular applications for communication and social networking, we propose Ensemble, a novel, automated approach based on a trusted community of users contributing system-call level local behavioral profiles of their applications to a global profile merging engine. The trust can be assumed in cases such as enterprise environments and can be further policed by reputation systems, e.g., by exploiting trust relationships inherently associated with social networks. The generated global profile can be used by all community users for local anomaly detection or prevention. Evaluation results based on a malware pool of 57 exploits demonstrate that Ensemble is an effective defense technique for communities of about 300 or more users as in enterprise environments.\"",
        "title: \"SocialWatch: detection of online service abuse via large-scale social graphs\" with abstract: \"In this paper, we present a framework, SocialWatch, to detect attacker-created accounts and hijacked accounts for online services at a large scale. SocialWatch explores a set of social graph properties that effectively model the overall social activity and connectivity patterns of online users, including degree, PageRank, and social affinity features. These features are hard to mimic and robust to attacker counter strategies. We evaluate SocialWatch using a large, real dataset with more than 682 million users and over 5.75 billion directional relationships. SocialWatch successfully detects 56.85 million attacker-created accounts with a low false detection rate of 0.75% and a low false negative rate of 0.61%. In addition, SocialWatch detects 1.95 million hijacked accounts---among which 1.23 million were not detected previously---with a low false detection rate of 2%. Our work demonstrates the practicality and effectiveness of using large social graphs with billions of edges to detect real attacks.\"",
        "title: \"COMET: code offload by migrating execution transparently\" with abstract: \"In this paper we introduce a runtime system to allow unmodified multi-threaded applications to use multiple machines. The system allows threads to migrate freely between machines depending on the workload. Our prototype, COMET (Code Offload by Migrating Execution Transparently), is a realization of this design built on top of the Dalvik Virtual Machine. COMET leverages the underlying memory model of our runtime to implement distributed shared memory (DSM) with as few interactions between machines as possible. Making use of a new VM-synchronization primitive, COMET imposes little restriction on when migration can occur. Additionally, enough information is maintained so one machine may resume computation after a network failure. We target our efforts towards augmenting smartphones or tablets with machines available in the network. We demonstrate the effectiveness of COMET on several real applications available on Google Play. These applications include image editors, turn-based games, a trip planner, and math tools. Utilizing a server-class machine, COMET can offer significant speed-ups on these real applications when run on a modern smartphone. With WiFi and 3G networks, we observe geometric mean speed-ups of 2.88\u00d7 and 1.27\u00d7 relative to the Dalvik interpreter across the set of applications with speed-ups as high as 15\u00d7 on some applications.\"",
        "title: \"Locating internet routing instabilities\" with abstract: \"This paper presents a methodology for identifying the autonomous system (or systems) responsible when a routing change is observed and propagated by BGP. The origin of such a routing instability is deduced by examining and correlating BGP updates for many prefixes gathered at many observation points. Although interpreting BGP updates can be perplexing, we find that we can pinpoint the origin to either a single AS or a session between two ASes in most cases. We verify our methodology in two phases. First, we perform simulations on an AS topology derived from actual BGP updates using routing policies that are compatible with inferred peering/customer/provider relationships. In these simulations, in which network and router behavior are \"ideal\", we inject inter-AS link failures and demonstrate that our methodology can effectively identify most origins of instability. We then develop several heuristics to cope with the limitations of the actual BGP update propagation process and monitoring infrastructure, and apply our methodology and evaluation techniques to actual BGP updates gathered at hundreds of observation points. This approach of relying on data from BGP simulations as well as from measurements enables us to evaluate the inference quality achieved by our approach under ideal situations and how it is correlated with the actual quality and the number of observation points.\"",
        "1 is \"An implementation and analysis of the virtual interface architecture\", 2 is \"Exposing private information by timing web applications\".",
        "\nGiven above information, for an author who has written the paper with the title \"BorderGuard: detecting cold potatoes from peers\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0198": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'A clean slate 4D approach to network control and management':",
        "title: \"Dynamic scheduling of network updates\" with abstract: \"We present Dionysus, a system for fast, consistent network updates in software-defined networks. Dionysus encodes as a graph the consistency-related dependencies among updates at individual switches, and it then dynamically schedules these updates based on runtime differences in the update speeds of different switches. This dynamic scheduling is the key to its speed; prior update methods are slow because they pre-determine a schedule, which does not adapt to runtime conditions. Testbed experiments and data-driven simulations show that Dionysus improves the median update speed by 53--88% in both wide area and data center networks compared to prior methods.\"",
        "title: \"A new approach to interdomain routing based on secure multi-party computation\" with abstract: \"Interdomain routing involves coordination among mutually distrustful parties, leading to the requirements that BGP provide policy autonomy, flexibility, and privacy. BGP provides these properties via the distributed execution of policy-based decisions during the iterative route computation process. This approach has poor convergence properties, makes planning and failover difficult, and is extremely difficult to change. To rectify these and other problems, we propose a radically different approach to interdomain-route computation, based on secure multi-party computation (SMPC). Our approach provides stronger privacy guarantees than BGP and enables the deployment of new policy paradigms. We report on an initial exploration of this idea and outline future directions for research.\"",
        "title: \"BGP safety with spurious updates\" with abstract: \"We explore BGP safety, the question of whether a BGP system converges to a stable routing, in light of several BGP implementation features that have not been fully included in the previous theoretical analyses. We show that Route Flap Damping, MRAI timers, and other intra-router features can cause a router to briefly send \u201cspurious\u201d announcements of less-preferred routes. We demonstrate that, even in simple configurations, this short-term spurious behavior may cause long-term divergence in global routing. We then present DPVP, a general model that unifies these sources of spurious announcements in order to examine their impact on BGP safety. In this new, more robust model of BGP behavior, we derive a necessary and sufficient condition for safety, which furthermore admits an efficient algorithm for checking BGP safety in most practical circumstances - two complementary results that have been elusive in the past decade's worth of classical studies of BGP convergence in more simple models. We also consider the implications of spurious updates for well-known results on dispute wheels and safety under filtering.\"",
        "title: \"Continuous in-network round-trip time monitoring\" with abstract: \"Round-trip time (RTT) is a central metric that influences end-user QoE and can expose traffic-interception attacks. Many popular RTT monitoring techniques either send active probes (that do not capture application-level RTTs) or passively monitor only the TCP handshake (which can be inaccurate, especially for long-lived flows). High-speed programmable switches present a unique opportunity to monitor the RTTs continuously and react in real time to improve performance and security. In this paper, we present Dart, an inline, real-time, and continuous RTT measurement system that can enable automated detection of network events and adapt (e.g., routing, scheduling, marking, or dropping traffic) inside the network. However, designing Dart is fraught with challenges, due to the idiosyncrasies of the TCP protocol and the resource constraints in high-speed switches. Dart overcomes these challenges by strategically limiting the tracking of packets to only those that can generate useful RTT samples, and by identifying the synergy between per-flow state and per-packet state for efficient memory use. We present a P4 prototype of Dart for the Tofino switch, as well our experiments on a campus testbed and simulations using anonymized campus traces. Dart, running in real time and with limited data-plane memory, is able to collect 99% of the RTT samples of an offline, software baseline---a variant of the popular tcptrace tool that has access to unlimited memory.\"",
        "title: \"Toward Software-Defined Cellular Networks\" with abstract: \"Existing cellular networks suffer from inflexible and expensive equipment, complex control-plane protocols, and vendor-specific configuration interfaces. In this position paper, we argue that software defined networking (SDN) can simplify the design and management of cellular data networks, while enabling new services. However, supporting many subscribers, frequent mobility, fine-grained measurement and control, and real-time adaptation introduces new scalability challenges that future SDN architectures should address. As a first step, we propose extensions to controller platforms, switches, and base stations to enable controller applications to (i) express high-level policies based on subscriber attributes, rather than addresses and locations, (ii) apply real-time, fine-grained control through local agents on the switches, (iii)perform deep packet inspection and header compression on packets, and (iv)remotely manage shares of base-station resources.\"",
        "1 is \"Design, implementation, and evaluation of the linear road bnchmark on the stream processing core\", 2 is \"Disruption Free Topology Reconfiguration in OSPF Networks\".",
        "\nGiven above information, for an author who has written the paper with the title \"A clean slate 4D approach to network control and management\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0199": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Finding a needle in a haystack: pinpointing significant BGP routing changes in an IP network':",
        "title: \"Secure Optimization Computation Outsourcing in Cloud Computing: A Case Study of Linear Programming\" with abstract: \"Cloud computing enables an economically promising paradigm of computation outsourcing. However, how to protect customers confidential data processed and generated during the computation is becoming the major security concern. Focusing on engineering computing and optimization tasks, this paper investigates secure outsourcing of widely applicable linear programming (LP) computations. Our mechanism design explicitly decomposes LP computation outsourcing into public LP solvers running on the cloud and private LP parameters owned by the customer. The resulting flexibility allows us to explore appropriate security/efficiency tradeoff via higher-level abstraction of LP computation than the general circuit representation. Specifically, by formulating private LP problem as a set of matrices/vectors, we develop efficient privacy-preserving problem transformation techniques, which allow customers to transform the original LP into some random one while protecting sensitive input/output information. To validate the computation result, we further explore the fundamental duality theorem of LP and derive the necessary and sufficient conditions that correct results must satisfy. Such result verification mechanism is very efficient and incurs close-to-zero additional cost on both cloud server and customers. Extensive security analysis and experiment results show the immediate practicability of our mechanism design.\"",
        "title: \"TAU 2013 variation aware timing analysis contest\" with abstract: \"Timing analysis is a key component of any integrated circuit (IC) chip design-closure flow, and is employed at various stages of the flow including pre/post-route timing optimization and timing signoff. While accurate timing analysis is important, the run-time of the analysis is equally critical with growing chip design sizes and complexity (for example, increasing number of clocks domains, voltage islands, etc.). In addition, the increasing significance of variability in the chip manufacturing process as well as environmental variability necessitates use of variation aware techniques (e.g. statistical, multi-corner) for chip timing analysis which significantly impacts the analysis run-time. The aim of the TAU 2013 variation aware timing contest is to seek novel ideas for fast variation aware timing analysis, by means of the following: (a) increase awareness of variation aware timing analysis and provide insight into some challenging aspects of the analysis, (b) encourage novel parallelization techniques (including multi-threading) for timing analysis, and (c) facilitate creation of a publicly available variation aware timing analysis framework and benchmarks to further advance research in this area.\"",
        "title: \"OPTWALL: A Hierarchical Traffic-Aware Firewall\" with abstract: \"The overall efficiency, reliability, and availability of a firewall is crucial in enforcing and administrating securit y, especially when the network is under attack. The continuous growth of th e Internet, coupled with the increasing sophistication of th e attacks, is placing stringent demands on firewall performance. These ch allenges require new designs, architecture and algorithms to optimi ze firewalls. In this paper, we propose OPTWALL, an adaptive hierarchical firewall optimization framework aimed at reducing operatio nal cost of firewalls. The main features of the proposed approach are t he hierarchical design, splitting techniques, an online traf fic adaptation mechanism, and a strong reactive scheme to counter maliciou s attacks (e.g. Denial-of-Service (DoS) attacks). To the best of our k nowledge, this work is the first of its kind to use traffic characteristics in the design of an adaptive hierarchical firewall optimization fr amework. To study the performance of OPTWALL, a set of experiments are conducted on Linux ipchains. The performance evaluation st udy uses a large set of firewall policies and traffic traces managed by a Tier- 1 ISP and provides security access for the ISP network from/t o its business partners. Results show the high potential of OPTWA LL to reduce the operational cost of firewalls. In particular, theresults show that a performance improvement of nearly 35% can been achiev ed in a heavily loaded network environment.\"",
        "title: \"Statistical timing verification for transparently latched circuits through structural graph traversal\" with abstract: \"Level-sensitive transparent latches are widely used in high-performance sequential circuit designs. Under process variations, the timing of a transparently latched circuit will adapt random delays at runtime due to time borrowing. The central problem to determine the timing yield is to compute the probability of the presence of a positive cycle in the latest latch timing graph. Existing algorithms are either optimistic since cycles are omitted or require iterations that cannot be polynomially bounded. In this paper, we present the first algorithm to compute such probability based on block-based statistical timing analysis that, first, covers all cycles through a structural graph traversal, and second, terminates within a polynomial number of statistical \u00c2\u00bfsum\u00c2\u00bf and \u00c2\u00bfmax\u00c2\u00bf operations. Experimental results confirm that the proposed approach is effective and efficient.\"",
        "title: \"A light-weight distributed scheme for detecting ip prefix hijacks in real-time\" with abstract: \"As more and more Internet IP prefix hijacking incidents are being reported, the value of hijacking detection services has become evident. Most of the current hijacking detection approaches monitor IP prefixes on the control plane and detect inconsistencies in route advertisements and route qualities. We propose a different approach that utilizes information collected mostly from the data plane. Our method is motivated by two key observations: when a prefix is not hijacked, 1) the hop count of the path from a source to this prefix is generally stable; and 2) the path from a source to this prefix is almost always a super-path of the path from the same source to a reference point along the previous path, as long as the reference point is topologically close to the prefix. By carefully selecting multiple vantage points and monitoring from these vantage points for any departure from these two observations, our method is able to detect prefix hijacking with high accuracy in a light-weight, distributed, and real-time fashion. Through simulations constructed based on real Internet measurement traces, we demonstrate that our scheme is accurate with both false positive and false negative ratios below 0.5%.\"",
        "1 is \"Compressed Bloom filters\", 2 is \"On the optimal placement of web proxies in the Internet\".",
        "\nGiven above information, for an author who has written the paper with the title \"Finding a needle in a haystack: pinpointing significant BGP routing changes in an IP network\", which reference is related? Just choose 1 or 2! No explanation."
    ]
}