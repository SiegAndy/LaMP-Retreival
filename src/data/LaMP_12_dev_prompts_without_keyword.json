{
    "010": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Visual-audio integration for user authentication system of partner robots':",
        "title: \"Adaptive fuzzy logic controller for vehicle active suspensions with interval type-2 fuzzy membership functions\" with abstract: \"Elicited from the least means squares optimal algorithm (LMS), an adaptive fuzzy logic controller (AFC) based on interval type-2 fuzzy sets is proposed for vehicle non-linear active suspension systems. The interval membership functions (IMF2s) are utilized in the AFC design to deal with not only non-linearity and uncertainty caused from irregular road inputs and immeasurable disturbance, but also the potential uncertainty of expertpsilas knowledge and experience. The adaptive strategy is designed to self-tune the active force between the lower bounds and upper bounds of interval fuzzy outputs. A case study based on a quarter active suspension model has demonstrated that the proposed type-2 fuzzy controller significantly outperforms conventional fuzzy controllers of an active suspension and a passive suspension.\"",
        "title: \"Asymmetric LSH (ALSH) for Sublinear Time Maximum Inner Product Search (MIPS).\" with abstract: \"We present the first provably sublinear time hashing algorithm for approximate Maximum Inner Product Search (MIPS). Searching with (un-normalized) inner product as the underlying similarity measure is a known difficult problem and finding hashing schemes for MIPS was considered hard. While the existing Locality Sensitive Hashing (LSH) framework is insufficient for solving MIPS, in this paper we extend the LSH framework to allow asymmetric hashing schemes. Our proposal is based on a key observation that the problem of finding maximum inner products, after independent asymmetric transformations, can be converted into the problem of approximate near neighbor search in classical settings. This key observation makes efficient sublinear hashing scheme for MIPS possible. Under the extended asymmetric LSH (ALSH) framework, this paper provides an example of explicit construction of provably fast hashing scheme for MIPS. Our proposed algorithm is simple and easy to implement. The proposed hashing scheme leads to significant computational savings over the two popular conventional LSH schemes: (i) Sign Random Projection (SRP) and (ii) hashing based on p-stable distributions for L-2 norm (L2LSH), in the collaborative filtering task of item recommendations on Netflix and Movielens (10M) datasets.\"",
        "title: \"Training Logistic Regression and SVM on 200GB Data Using b-Bit Minwise Hashing and Comparisons with Vowpal Wabbit (VW)\" with abstract: \"  We generated a dataset of 200 GB with 10^9 features, to test our recent b-bit minwise hashing algorithms for training very large-scale logistic regression and SVM. The results confirm our prior work that, compared with the VW hashing algorithm (which has the same variance as random projections), b-bit minwise hashing is substantially more accurate at the same storage. For example, with merely 30 hashed values per data point, b-bit minwise hashing can achieve similar accuracies as VW with 2^14 hashed values per data point.   We demonstrate that the preprocessing cost of b-bit minwise hashing is roughly on the same order of magnitude as the data loading time. Furthermore, by using a GPU, the preprocessing cost can be reduced to a small fraction of the data loading time.   Minwise hashing has been widely used in industry, at least in the context of search. One reason for its popularity is that one can efficiently simulate permutations by (e.g.,) universal hashing. In other words, there is no need to store the permutation matrix. In this paper, we empirically verify this practice, by demonstrating that even using the simplest 2-universal hashing does not degrade the learning performance. \"",
        "title: \"Partial Hard Thresholding: Towards A Principled Analysis of Support Recovery.\" with abstract: \"In machine learning and compressed sensing, it is of central importance to understand when a tractable algorithm recovers the support of a sparse signal from its compressed measurements. In this paper, we present a principled analysis on the support recovery performance for a family of hard thresholding algorithms. To this end, we appeal to the partial hard thresholding (PHT) operator proposed recently by Jain et al. [IEEE Trans. Information Theory, 2017]. We show that under proper conditions, PHT recovers an arbitrary s-sparse signal within O(s kappa log kappa) iterations where kappa is an appropriate condition number. Specifying the PHT operator, we obtain the best known results for hard thresholding pursuit and orthogonal matching pursuit with replacement. Experiments on the simulated data complement our theoretical findings and also illustrate the effectiveness of PHT.\"",
        "title: \"Logician: A Unified End-to-End Neural Approach for Open-Domain Information Extraction.\" with abstract: \"In this paper, we consider the problem of open information extraction (OIE) for extracting entity and relation level intermediate structures from sentences in open-domain. We focus on four types of valuable intermediate structures (Relation, Attribute, Description, and Concept), and propose a unified knowledge expression form, SAOKE, to express them. We publicly release a data set which contains 48,248 sentences and the corresponding facts in the SAOKE format labeled by crowdsourcing. To our knowledge, this is the largest publicly available human labeled data set for open information extraction tasks. Using this labeled SAOKE data set, we train an end-to-end neural model using the sequence-to-sequence paradigm, called Logician, to transform sentences into facts. For each sentence, different to existing algorithms which generally focus on extracting each single fact without concerning other possible facts, Logician performs a global optimization over all possible involved facts, in which facts not only compete with each other to attract the attention of words, but also cooperate to share words. An experimental study on various types of open domain relation extraction tasks reveals the consistent superiority of Logician to other states-of-the-art algorithms. The experiments verify the reasonableness of SAOKE format, the valuableness of SAOKE data set, the effectiveness of the proposed Logician model, and the feasibility of the methodology to apply end-to-end learning paradigm on supervised data sets for the challenging tasks of open information extraction.\n\n\"",
        "1 is \"Pronunciation Modeling for Improved Spelling Correction\", 2 is \"Path following algorithm for highly redundant manipulators\".",
        "\nGiven above information, for an author who has written the paper with the title \"Visual-audio integration for user authentication system of partner robots\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "011": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Perspectives on Cognitive Informatics and Cognitive Computing':",
        "title: \"Big Data Analytics: A Cognitive Perspectives.\" with abstract: \"Big data are pervasively generated by human cognitive processes, formal inferences, and system quantifications. This paper presents the cognitive foundations of big data systems towards big data science. The key perceptual model of big data systems is the recursively typed hyperstructure RTHS. The RTHS model reveals the inherited complexities and unprecedented difficulty in big data engineering. This finding leads to a set of mathematical and computational models for efficiently processing big data systems. The cognitive relationship between data, information, knowledge, and intelligence is formally described.\"",
        "title: \"The Cognitive Informatics Theory And Mathematical Models Of Visual Information Processing In The Brain\" with abstract: \"It is recognized that the internal mechanisms for visual information processing are based on semantic inferences where visual information is represented and processed as visual semantic objects rather than direct images or episode pictures in the long-term memory. This article presents a cognitive informatics theory of visual information and knowledge processing in the brain. A set of cognitive principles of visual perception is reviewed particularly the classic gestalt principles, the cognitive informatics principles, and the hypercolumn theory. A visual frame theory is developed to explain the visual information processing mechanisms of human vision, where the size of a unit visual frame is tested and calibrated based on vision experiments. The framework of human visual information processing is established in order to elaborate mechanisms of visual information processing and the compatibility of internal representations between visual and abstract information and knowledge in the brain. [Article copies are available for purchase from InfoSci-on-Demand.com]\"",
        "title: \"The Cognitive Process Of Decision Making\" with abstract: \"Decision making is one of the basic cognitive processes of human behaviors by which a preferred option or a course of actions is chosen from among a set of alternatives based on certain criteria. Decision theories are widely applied in many disciplines encompassing cognitive informatics, computer science, management science, economics, sociology, psychology, political science, and statistics. A number of decision strategies have been proposed from different angles and application domains such as the maximum expected utility and Bayesian method. However, there is still a lack of a fundamental and mathematical decision model and a rigorous cognitive process for decision making. This article presents a fundamental cognitive decision making process and its mathematical model, which is described as a sequence of Cartesian-product based selections. A rigorous description of the decision process in real-time process algebra (RTPA) is provided. Real-world decisions are perceived as a repetitive application of the fundamental cognitive process. The result shows that all categories of decision strategies fit in the formally described decision process. The cognitive process of decision making may be applied in a wide range of decision-based systems such as cognitive informatics, software agent systems, expert systems, and decision support systems.\"",
        "title: \"A Web Knowledge Discovery Engine Based on Concept Algebra\" with abstract: \"On-line knowledge discovery is an important area of knowledge engineering. This paper develops a visualized concept network explorer and a semantic analyzer to locate, capture, and refine queries based on concept algebra. A graphical interface is built using concept and semantic models to refine users' query structures. This tool kit can generate a structured XML query package that accurately express users' information needs for on-line searching and knowledge acquisition.\"",
        "title: \"Simulation and Visualization of Concept Algebra in MATLAB\" with abstract: \"Concept algebra (CA) is a denotational mathematics for formal knowledge manipulation and natural language processing. In order to explicitly demonstrate the mathematical models of formal concepts and their algebraic operations in CA, a simulation and visualization software is developed in the MATLAB environment known as the Visual Simulator of Concept Algebra (VSCA). This paper presents the design and implementation of VSCA and the theories underpinning its development. Visual simulations for the sets of reproductive and compositional operations of CA are demonstrated by real-world examples throughout the elaborations of CA and VSCA.\"",
        "1 is \"Learning Discriminative And Shareable Features For Scene Classification\", 2 is \"Using design patterns to develop reusable object-oriented communication software\".",
        "\nGiven above information, for an author who has written the paper with the title \"Perspectives on Cognitive Informatics and Cognitive Computing\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "012": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Using Magnetic RAM to Build Low-Power and Soft Error-Resilient L1 Cache':",
        "title: \"Quasi-nonvolatile SSD: Trading flash memory nonvolatility to improve storage system performance for enterprise applications\" with abstract: \"This paper advocates a quasi-nonvolatile solid-state drive (SSD) design strategy for enterprise applications. The basic idea is to trade data retention time of NAND flash memory for other system performance metrics including program/erase (P/E) cycling endurance and memory programming speed, and meanwhile use explicit internal data refresh to accommodate very short data retention time (e.g., few weeks or even days). We also propose SSD scheduling schemes to minimize the impact of internal data refresh on normal I/O requests. Based upon detailed memory cell device modeling and SSD system modeling, we carried out simulations that clearly show the potential of using this simple quasi-nonvolatile SSD design strategy to improve system cycling endurance and speed performance. We also performed detailed energy consumption estimation, which shows the energy consumption overhead induced by data refresh is negligible.\"",
        "title: \"Architecting high-performance energy-efficient soft error resilient cache under 3D integration technology\" with abstract: \"Radiation-induced soft error has become an emerging reliability threat to high performance microprocessor design. As the size of on chip cache memory steadily increased for the past decades, resilient techniques against soft errors in cache are becoming increasingly important for processor reliability. However, conventional soft error resilient techniques have significantly increased the access latency and energy consumption in cache memory, thereby resulting in undesirable performance and energy efficiency degradation. The emerging 3D integration technology provides an attractive advantage, as the 3D microarchitecture exhibits heterogeneous soft error resilient characteristics due to the shielding effect of die stacking. Moreover, the 3D shielding effect can offer several inner dies that are inherently invulnerable to soft error, as they are implicitly protected by the outer dies. To exploit the invulnerability benefit, we propose a soft error resilient 3D cache architecture, in which data blocks on the soft error invulnerable dies have no protection against soft error, therefore, access to the data block on the soft error invulnerable die incurs a considerably reduced access latency and energy. Furthermore, we propose to maximize the access on the soft error invulnerable dies by dynamically moving data blocks among different dies, thereby achieving further performance and energy efficiency improvement. Simulation results show that the proposed 3D cache architecture can reduce the power consumption by up to 65% for the L1 instruction cache, 60% for the L1 data cache and 20% for the L2 cache, respectively. In general, the overall IPC performance can be improved by 5% on average.\"",
        "title: \"Improving STT MRAM storage density through smaller-than-worst-case transistor sizing\" with abstract: \"This paper presents a technique to improve the storage density of spin-torque transfer (STT) magnetoresistive random access memory (MRAM) in the presence of significant magnetic tunneling junction (MTJ) write current threshold variability. In conventional design practice, the nMOS transistor within each memory cell is sized to be large enough to carry a current larger than the worst-case MTJ write current threshold, leading to an increasing storage density penalty as the technology scales down. To mitigate such variability-induced storage density penalty, this paper presents a smaller-than-worst-case transistor sizing approach with the underlying theme of jointly considering memory cell transistor sizing and defect tolerance. Its effectiveness is demonstrated using 256 Mb STT MRAM design at 45 nm node as a test vehicle. Results show that, under a normalized write current threshold deviation of 20%, the overall memory die size can be reduced by more than 20% compared with the conventional worst-case transistor sizing design practice.\"",
        "title: \"Exploiting memory device wear-out dynamics to improve NAND flash memory system performance\" with abstract: \"This paper advocates a device-aware design strategy to improve various NAND flash memory system performance metrics. It is well known that NAND flash memory program/erase (PE) cycling gradually degrades memory device raw storage reliability, and sufficiently strong error correction codes (ECC) must be used to ensure the PE cycling endurance. Hence, memory manufacturers must fabricate enough number of redundant memory cells geared to the worst-case device reliability at the end of memory lifetime. Given the memory device wear-out dynamics, the existing worst-case oriented ECC redundancy is largely under-utilized over the entire memory lifetime, which can be adaptively traded for improving certain NAND flash memory system performance metrics. This paper explores such device-aware adaptive system design space from two perspectives, including (1) how to improve memory program speed, and (2) how to improve memory defect tolerance and hence enable aggressive fabrication technology scaling. To enable quantitative evaluation, we for the first time develop a NAND flash memory device model to capture the effects of PE cycling from the system level. We carry out simulations using the DiskSim-based SSD simulator and a variety of traces, and the results demonstrate up to 32% SSD average response time reduction. We further demonstrate that the potential on achieving very good defect tolerance, and finally show that these two design approaches can be readily combined together to noticeably improve SSD average response time even in the presence of high memory defect rates.\"",
        "title: \"Impacts Of Though-Dram Vias In 3d Processor-Dram Integrated Systems\" with abstract: \"As a promising option to address the memory wall problem, 3D processor-DRAM integration has recently received many attentions. Since DRAM tiers must be stacked between the processor tier and package substrate, we must fabricate a large number of through-DRAM through-silicon vias (TSVs) to connect the processor tier and package for power and I/O signal delivery. Although such through-DRAM TSVs will inevitably Interfere with DRAM design and induce non-negligible power consumption overhead, little research has been done to study how to allocate these TSVs on the DRAM tiers and analyze their impacts. To address this open issue, this paper first presents a through-DRAM TSV allocation strategy that fits well to the regular DRAM architecture. To demonstrate this design strategy and evaluate trade-offs involved, we develop a CACTI-based modeling tool to carry out extensive simulations over a wide range of design parameters.\"",
        "1 is \"Severless Search and Authentication Protocols for RFID\", 2 is \"Dynamic Backlight Scaling Optimization: A Cloud-Based Energy-Saving Service for Mobile Streaming Applications\".",
        "\nGiven above information, for an author who has written the paper with the title \"Using Magnetic RAM to Build Low-Power and Soft Error-Resilient L1 Cache\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "013": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Voronoi diagrams for direction-sensitive distances':",
        "title: \"Parallel algorithms for maximum matching in interval graphs\" with abstract: \"Given a set of n intervals representing an interval graph, the problem of finding a maximum matching between pairs of disjoint (nonintersecting) intervals has been considered in the sequential model. We present parallel algorithms for computing maximum cardinality matchings among pairs of disjoint intervals in interval graphs an the EREW PRAM and hypercube models. For the general case of the problem, our algorithms compute a maximum matching in O(log/sup 3/ n) time using O(n/log/sup 2/ n) processors on the EREW PRAM and using O(n) processors on the hypercubes. For the case of proper interval graphs, our algorithm runs in O(log n) time using O(n) processors if the input intervals are not given already sorted and using O(n/log n) processors otherwise, on the EREW PRAM. On n-processor hypercubes, our algorithm for this case takes O(log n loglog n) time for unsorted input and O(log n) time for sorted input. Our parallel results also lead to optimal sequential algorithms for computing maximum matchings among disjoint intervals. We also present an improved parallel algorithm for maximum matching between overlapping intervals in proper interval graphs.\"",
        "title: \"On approximating the maximum simple sharing problem\" with abstract: \"In the maximum simple sharing problem (MSS), we want to compute a set of node-disjoint simple paths in an undirected bipartite graph covering as many nodes as possible of one layer of the graph, with the constraint that all paths have both endpoints in the other layer. This is a variation of the maximum sharing problem (MS) that finds important applications in the design of molecular quantum-dot cellular automata (QCA) circuits and physical synthesis in VLSI. It also generalizes the maximum weight node-disjoint path cover problem. We show that MSS is NP-complete, present a polynomial-time $5\\over 3$-approximation algorithm, and show that it cannot be approximated with a factor better than $740\\over 739$ unless P = NP.\"",
        "title: \"Scheduling for power reduction in a real-time system\" with abstract: \"This paper describes how, through a combination of scheduling and buffer insertion, real-time systems may be optimized for power consumption while maintaining deadlines. Beginning with simple examples (components that have no internal pipelines and in which the only design freedoms are buffer insertion and scheduling), we? illustrate the effect of adjusting the time at which data are processed on power consumption. Algorithms for optimizing the energy saving are proposed for several real-time system implementations including non-pipelined and pipelined. We also discuss extension to this preliminary work including selection of alternate processing units in order to reduce power consumption while maintaining deadlines.\"",
        "title: \"Planar Spanners and Approximate Shortest Path Queries among Obstacles in the Plane\" with abstract: \"We consider the problem of finding an obstacle-avoiding path between two points s and t in the plane, amidst a set of disjoint polyg- onal obstacles with a total of n vertices. The length of this path should be within a small constant factor c of the length of the shortest possible obstacle-avoiding s-t path measured in the Lv-metric. Such an approxi- mate shortest path is called a c-short path, or a short path with stretch )actor c. The goal is to preprocess the obstacle-scattered plane by creat- ing an efficient data structure that enables fast reporting of a c-short path (or its length). In this paper, we give a family of algorithms for the above problem that achieve an interesting trade-off between the stretch factor, the query time and the preprocessing bounds. Our main results are al- gorithms that achieve logarithmic length query time, after subquadratic time and space preprocessing.\"",
        "title: \"Biomedical Image Segmentation Using Fully Convolutional Networks on TrueNorth\" with abstract: \"With the rapid growth of medical and biomedical image data, energy-efficient solutions for analyzing such image data that can be processed fast and accurately on platforms with low power budget are highly desirable. This paper uses segmenting glial cells in brain microscopy images as a case study to demonstrate how to achieve biomedical image segmentation with significant energy saving and minimal comprise in accuracy. Specifically, we design, train, implement, and evaluate Fully Convolutional Networks (FCNs) for biomedical image segmentation on IBM's neurosynaptic DNN processor - TrueNorth (TN). Comparisons in terms of accuracy and energy dissipation of TN with that of a low power NVIDIA TX2 mobile GPU platform have been conducted. Experimental results show that TN can offer at least two orders of magnitude improvement in energy efficiency when compared to TX2 GPU for the same workload.\"",
        "1 is \"Fast Algorithms for Finding Maximum-Density Segments of a Sequence with Applications to Bioinformatics\", 2 is \"Power minimization in IC design: principles and applications\".",
        "\nGiven above information, for an author who has written the paper with the title \"Voronoi diagrams for direction-sensitive distances\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "014": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'QuickSense: Fast and energy-efficient channel sensing for dynamic spectrum access networks':",
        "title: \"Fast and Accurate Performance Analysis of LTE Radio Access Networks.\" with abstract: \"An increasing amount of analytics is performed on data that is procured in a real-time fashion to make real-time decisions. Such tasks include simple reporting on streams to sophisticated model building. However, the practicality of such analyses are impeded in several domains because they are faced with a fundamental trade-off between data collection latency and analysis accuracy. In this paper, we study this trade-off in the context of a specific domain, Cellular Radio Access Networks (RAN). Our choice of this domain is influenced by its commonalities with several other domains that produce real-time data, our access to a large live dataset, and their real-time nature and dimensionality which makes it a natural fit for a popular analysis technique, machine learning (ML). We find that the latency accuracy trade-off can be resolved using two broad, general techniques: intelligent data grouping and task formulations that leverage domain characteristics. Based on this, we present CellScope, a system that addresses this challenge by applying a domain specific formulation and application of Multi-task Learning (MTL) to RAN performance analysis. It achieves this goal using three techniques: feature engineering to transform raw data into effective features, a PCA inspired similarity metric to group data from geographically nearby base stations sharing performance commonalities, and a hybrid online-offline model for efficient model updates. Our evaluation of CellScope shows that its accuracy improvements over direct application of ML range from 2.5x to 4.4x while reducing the model update overhead by up to 4.8x. We have also used CellScope to analyze a live LTE consisting of over 2 million subscribers for a period of over 10 months, where it uncovered several problems and insights, some of them previously unknown.\"",
        "title: \"Stable Egress Route Selection for Interdomain Traffic Engineering: Model and Analysis\" with abstract: \"We present a general model of interdomain route selection to study interdomain traffic engineering. In this model, the routing of multiple destinations can be coordinated. Thus the model can capture general traffic engineering behaviors such as load balancing and link capacity constraints. We first identify potential routing instability and inefficiency of interdomain traffic engineering. We then derive a sufficient condition to guarantee convergence. We also show that the constraints on local policies imposed by business considerations in the Internet can guarantee stability without global coordination. Using realistic Internet topology, we evaluate the extent to which routing instability of interdomain traffic engineering can happen when the constraints are violated.\"",
        "title: \"Large-scale IP traceback in high-speed internet: practical techniques and information-theoretic foundation\" with abstract: \"Tracing attack packets to their sources, known as IP traceback, is an important step to counter distributed denial-of-service (DDoS) attacks. In this paper, we propose a novel packet logging based (i.e., hash-based) traceback scheme that requires an order of magnitude smaller processing and storage cost than the hash-based scheme proposed by Snoeren et al. [1], thereby being able to scalable to much higher link speed (e.g., OC-768). The base-line idea of our approach is to sample and log a small percentage (e.g., 3.3%) of packets. The challenge of this low sampling rate is that much more sophisticated techniques need to be used for traceback. Our solution is to construct the attack tree using the correlation between the attack packets sampled by neighboring routers. The scheme using naive independent random sampling does not perform well due to the low correlation between the packets sampled by neighboring routers. We invent a sampling scheme that improves this correlation and the overall efficiency significantly. Another major contribution of this work is that we introduce a novel information-theoretic framework for our traceback scheme to answer important questions on system parameter tuning and the fundamental tradeoff between the resource used for traceback and the traceback accuracy. Simulation results based on real-world network topologies (e.g., Skitter) match very well with results from the information-theoretic analysis. The simulation results also demonstrate that our traceback scheme can achieve high accuracy, and scale very well to a large number of attackers (e.g., 5000+).\"",
        "title: \"Time-evolving graph processing at scale.\" with abstract: \"Time-evolving graph-structured big data arises naturally in many application domains such as social networks and communication networks. However, existing graph processing systems lack support for efficient computations on dynamic graphs. In this paper, we represent most computations on time evolving graphs into (1) a stream of consistent and resilient graph snapshots, and (2) a small set of operators that manipulate such streams of snapshots. We then introduce GraphTau, a time-evolving graph processing framework built on top of Apache Spark, a widely used distributed dataflow system. GraphTau quickly builds fault-tolerant graph snapshots as each small batch of new data arrives. GraphTau achieves high performance and fault tolerant graph stream processing via a number of optimizations. GraphTau also unifies data streaming and graph streaming processing. Our preliminary evaluations on two representative datasets show promising results. Besides performance benefit, GraphTau API relieves programmers from handling graph snapshot generation, windowing operators and sophisticated differential computation mechanisms.\"",
        "title: \"Coordination mechanisms for selfish scheduling\" with abstract: \"In machine scheduling, a set of n jobs must be scheduled on a set of m machines. Each job i incurs a processing time of pij on machine j and the goal is to schedule jobs so as to minimize some global objective function, such as the maximum makespan of the schedule considered in this paper. Often in practice, each job is controlled by an independent selfish agent who chooses to schedule his job on machine which minimizes the (expected) completion time of his job. This scenario can be formalized as a game in which the players are job owners; the strategies are machines; and the disutility to each player in a strategy profile is the completion time of his job in the corresponding schedule (a player\u2019s objective is to minimize his disutility). The equilibria of these games may result in larger-than-optimal overall makespan. The ratio of the worst-case equilibrium makespan to the optimal makespan is called the price of anarchy of the game. In this paper, we design and analyze scheduling policies, or coordination mechanisms, for machines which aim to minimize the price of anarchy (restricted to pure Nash equilibria) of the corresponding game. We study coordination mechanisms for four classes of multiprocessor machine scheduling problems and derive upper and lower bounds for the price of anarchy of these mechanisms. For several of the proposed mechanisms, we also are able to prove that the system converges to a pure Nash equilibrium in a linear number of rounds. Finally, we note that our results are applicable to several practical problems arising in networking.\"",
        "1 is \"A first look at cellular network performance during crowded events\", 2 is \"On the complexity of scheduling in wireless networks\".",
        "\nGiven above information, for an author who has written the paper with the title \"QuickSense: Fast and energy-efficient channel sensing for dynamic spectrum access networks\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "015": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Argos: practical many-antenna base stations':",
        "title: \"Uniformly optimal 3-D fan filters for optical moving target detection\" with abstract: \"Broad-band (e.g. white) noise is sometimes the limiting factor in the detection of moving targets in a sequence of images. If the target velocity is known (and if the target intensity distribution is known) then the 3-D matched filter is optimal with respect to signal to noise ratio (SNR), however it suffers from velocity mismatch if the velocity is unknown. A bank of 3-D matched filters, each tuned to a different velocity, has been proposed for dealing with this problem. Alternatively one can use a new type of 3-D fan filter which passes undistorted all targets over a specified velocity set while providing maximum noise attenuation, thereby avoiding velocity mismatch. This paper obtains analytical expressions for the SNR improvement of these filters which facilitate the design of banks of these filters as well as performance trade-off studies.\"",
        "title: \"Performance of Conjugate and Zero-Forcing Beamforming in Large-Scale Antenna Systems\" with abstract: \"Large-Scale Antenna Systems (LSAS) is a form of multi-user MIMO technology in which unprecedented numbers of antennas serve a significantly smaller number of autonomous terminals. We compare the two most prominent linear pre-coders, conjugate beamforming and zero-forcing, with respect to net spectral-efficiency and radiated energy-efficiency in a simplified single-cell scenario where propagation is governed by independent Rayleigh fading, and where channel-state information (CSI) acquisition and data transmission are both performed during a short coherence interval. An effective-noise analysis of the pre-coded forward channel yields explicit lower bounds on net capacity which account for CSI acquisition overhead and errors as well as the sub-optimality of the pre-coders. In turn the bounds generate trade-off curves between radiated energy-efficiency and net spectral-efficiency. For high spectral-efficiency and low energy-efficiency zero-forcing outperforms conjugate beamforming, while at low spectral-efficiency and high energy-efficiency the opposite holds. Surprisingly, in an optimized system, the total LSAS-critical computational burden of conjugate beamforming may be greater than that of zero-forcing. Conjugate beamforming may still be preferable to zero-forcing because of its greater robustness, and because conjugate beamforming lends itself to a de-centralized architecture and de-centralized signal processing.\"",
        "title: \"Cell-Free Massive MIMO: Uniformly great service for everyone\" with abstract: \"We consider the downlink of Cell-Free Massive MIMO systems, where a very large number of distributed access points (APs) simultaneously serve a much smaller number of users. Each AP uses local channel estimates obtained from received uplink pilots and applies conjugate beamforming to transmit data to the users. We derive a closed-form expression for the achievable rate. This expression enables us to design an optimal max-min power control scheme that gives equal quality of service to all users. We further compare the performance of the Cell-Free Massive MIMO system to that of a conventional small-cell network and show that the throughput of the Cell-Free system is much more concentrated around its median compared to that of the smallcell system. The Cell-Free Massive MIMO system can provide an almost 20-fold increase in 95%-likely per-user throughput, compared with the small-cell system. Furthermore, Cell-Free systems are more robust to shadow fading correlation than smallcell systems.\"",
        "title: \"Cell-Free Massive Mimo Systems\" with abstract: \"Cell-Free Massive MIMO systems comprise a large number of distributed, low cost, and low power access point antennas, connected to a network controller. The number of antennas is significantly larger than the number of users. The system is not partitioned into cells and each user is served by all access point antennas simultaneously.In this paper, we define cell-free systems and analyze algorithms for power optimization and linear pre-coding. Compared with the conventional small-cell scheme, Cell-Free Massive MIMO can yield more than ten-fold improvement in terms of 5%-outage rate.\"",
        "title: \"Performance of cell-free massive MIMO systems with MMSE and LSFD receivers\" with abstract: \"Cell-Free Massive MIMO comprises a large number of distributed single-antenna access points (APs) serving a much smaller number of users. There is no partitioning into cells and each user is served by all APs. In this paper, the uplink performance of cell-free systems with minimum mean squared error (MMSE) and large scale fading decoding (LSFD) receivers is investigated. The main idea of LSFD receiver is to maximize achievable throughput using only large scale fading coefficients between APs and users. Capacity lower bounds for MMSE and LSFD receivers are derived. An asymptotic approximation for signal-to-interference-plus-noise ratio (SINR) of MMSE receiver is derived as a function of large scale fading coefficients only. The obtained approximation is accurate even for a small number of antennas. MMSE and LSFD receivers demonstrate five-fold and two-fold gains respectively over matched filter (MF) receiver in terms of 5%-outage rate.\"",
        "1 is \"Joint Beamforming and Broadcasting in Massive MIMO.\", 2 is \"Power modeling of graphical user interfaces on OLED displays\".",
        "\nGiven above information, for an author who has written the paper with the title \"Argos: practical many-antenna base stations\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "016": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Power estimation for cycle-accurate functional descriptions of hardware':",
        "title: \"Interconnect-aware high-level synthesis for low power\" with abstract: \"Interconnects (wires, buffers, clock distribution networks, multiplexers and buses) consume a significant fraction of total circuit power. In this work, we demonstrate the importance of optimizing on-chip interconnects for power during high-level synthesis. We present a methodology to integrate interconnect power optimization into high-level synthesis. Our binding algorithm not only reduces power consumption in functional units and registers in the resultant register-transfer level (RTL) architecture, but also optimizes interconnects for power. We take physical design information into account for this purpose. To estimate interconnect power consumption accurately for deep sub-micron (DSM) technologies, wire coupling capacitance is taken into. consideration. We observed that there is significant spurious (i.e., unnecessary) switching activity in the interconnects and propose techniques to reduce it. Compared to interconnect-unaware power-optimized circuits, our experimental results show that interconnect power can be reduced by 53.1% on an average, while reducing overall power by an average of 26.8% with 0.5% area overhead. Compared to area-optimized circuits, the interconnect power reduction is 72.9% and overall power reduction is 56.0% with 44.4% area overhead.\"",
        "title: \"Demo: Rio: a system solution for sharing I/O between mobile systems\" with abstract: \"A user nowadays owns a variety of mobile systems, including smartphones, tablets, smart glasses, and smart watches, each equipped with a plethora of I/O devices, such as cameras, speakers, microphones, sensors, and cellular modems. There are many interesting use cases in which an application running on one mobile system accesses I/O on another system, for three fundamental reasons. (i) Mobile systems can be in different physical locations or orientations. For example, one can control a smartphone's high-resolution camera from a tablet camera application to more easily capture a self-portrait. (ii) Mobile systems can serve different users. For example, one can a play music for another user if one's smartphone can access the other device's speaker. (iii) Certain mobile systems have unique I/O devices due to their distinct form factor and targeted use cases. For example, a user can make a phone call from her tablet using the modem and SIM card in her smartphone. Solutions exist for sharing I/O devices, e.g., for camera [1], speaker [2], and modem (for messaging) [3]. However, these solutions have three limitations. (i) They do not support unmodified applications. (ii) They do not expose all the functionality of an I/O device for sharing. (iii) They are I/O class-specific, requiring significant engineering effort to support new I/O devices. We demonstrate Rio (Remote I/O), an I/O sharing solution for mobile systems that overcomes all three aforementioned limitations. Rio adopts a split-stack I/O sharing model, in which the I/O stack is split between the two mobile systems at a certain boundary. All communications that cross this boundary are intercepted on the mobile system hosting the application and forwarded to the mobile system with the I/O device, where they are served by the rest of the I/O stack. Rio uses device files as its boundary of choice. Device files are used in Unix-like OSes, such as Android and iOS, to abstract many classes of I/O devices, providing an I/O class-agnostic boundary. The device file boundary supports I/O sharing for unmodified applications, as it is transparent to the application layer. It also exposes the full functionality of each I/O device to other mobile systems by allowing processes in one system to directly communicate with the device drivers in another. Rio is not the first system to exploit the device file boundary; our previous work, Paradice [5], uses device files as the boundary for I/O virtualization inside a single system. However, Rio faces a different set of challenges regarding how to properly exploit this boundary, as explained in the full paper [6]. In this demo, we use a prototype implementation of Rio for Android systems. Our implementation supports four important I/O classes: camera, audio devices such as speaker and microphone, sensors such as accelerometer, and cellular modem (for phone calls and SMS). It consists of about 7100 lines of code, of which less than 500 are specific to I/O classes. Rio also supports I/O sharing between heterogeneous mobile systems, including tablets and smartphones. See [4] for a video of the demo.\"",
        "title: \"Power signal processing: a new perspective for power analysis and optimization\" with abstract: \"To address the productivity bottlenecks in power analysis and optimization of modern systems, we propose to treat power as a signal and leverage the rich set of signal processing techniques. We first investigate the power signal properties of digital systems and analyze their limitations. We then study signal processing techniques to detect temporal and structural correlations of power signals. Finally, we employ these techniques to accelerate the simulation of an architecture-level power simulator. Our experiments with the SPEC2000 benchmark suite show that it is possible to accelerate power simulation by 100X without introducing significant errors at various resolution levels.\"",
        "title: \"Video: Rio: a system solution for sharing i/o between mobile systems\" with abstract: \"Modern mobile systems are equipped with a diverse collection of I/O devices, including cameras, microphones, various sensors, and cellular modem. There exist many novel use cases for allowing an application on one mobile system to utilize I/O devices from another. This video demonstrates Rio, an I/O sharing solution that supports unmodified applications and realizes many of these novel use cases. Rio's design is common to many classes of I/O devices, significantly reducing the engineering effort to support new I/O devices. Moreover, it supports all the functionalities of an I/O device for sharing. Rio also supports I/O sharing between mobile systems of different form factors, including smartphones and tablets.\"",
        "title: \"Data broadcasting using mobile FM radio: design, realization and application\" with abstract: \"In this work, we offer a novel system, MicroStation (\u03bcStation) that allows ubiquitous data broadcasting applications using the FM radio on mobile devices such as smartphones. \u03bcStation includes two key modules to enable data broadcasting based on existing mobile FM radio hardware. Channel Selector assigns different FM channels to neighboring \u03bcStation broadcasters to avoid collision and guides \u03bcStation listeners to find their broadcasting of interest. Data Codec realizes bit-level communication between mobile devices through existing FM radio hardware. We describe an implementation of \u03bcStation on the Nokia N900 smartphone, and provide low-level APIs and services to support application development. We also demonstrate two representative applications: Facebook-FM and Sync-Flash. These applications demonstrate the capability of \u03bcStation to readily enable a new class of ubiquitous data broadcasting applications on mobile devices.\"",
        "1 is \"Medium Access Control Protocols using Directional Antennas in Ad Hoc Networks\", 2 is \"Dynamic state and objective learning for sequential circuit automatic test generation using recomposition equivalence\".",
        "\nGiven above information, for an author who has written the paper with the title \"Power estimation for cycle-accurate functional descriptions of hardware\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "017": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Tag recommendations in social bookmarking systems':",
        "title: \"Geo_ML @ MediaEval Placing Task 2015.\" with abstract: \"We participated in the MediaEval Benchmarking whose goal is to concentrate on the multimodal geo-location prediction on the Yahoo! Flickr Creative Commons 100M dataset - the placing task. It challenges participants to develop models and/or techniques to estimate the geographic locations of the Flickr resources based on textual metadata, e.g. titles, descriptions and tags. We aim to nd a procedure that is conceptual to understand, simple to implement and exible to integrate dierent techniques. In this paper, we present a three-step approach to tackle the locale-based sub-task.\"",
        "title: \"Hybrid Matrix Factorization Update for Progress Modeling in Intelligent Tutoring Systems.\" with abstract: \"Intelligent Tutoring Systems often profit of intelligent components, which allow to personalize the proposed contents' characteristics and sequence. Adaptive sequencing, in particular, requires either a detrimental data collection for users or extensive domain information provided by experts of the educational area. In this paper we propose an efficient domain independent method to model student progress that can be later used to sequence tasks in large commercial systems. The developed method is based on the integration of domain independent Matrix Factorization Performance Prediction with Kalman Filters state modeling abilities. Our solution not only reduces the prediction error, but also possesses a more computationally efficient model update. Finally, we give hints about a potential interpretability of student's state computed by Matrix Factorization, that, because of its implicit modeling, did not allow human experts, to monitor user's knowledge acquisition.\"",
        "title: \"Hyperparameter Search Space Pruning \u2013 A New Component for Sequential Model-Based Hyperparameter Optimization\" with abstract: \"The optimization of hyperparameters is often done manually or exhaustively but recent work has shown that automatic methods can optimize hyperparameters faster and even achieve better final performance. Sequential model-based optimization SMBO is the current state of the art framework for automatic hyperparameter optimization. Currently, it consists of three components: a surrogate model, an acquisition function and an initialization technique. We propose to add a fourth component, a way of pruning the hyperparameter search space which is a common way of accelerating the search in many domains but yet has not been applied to hyperparameter optimization. We propose to discard regions of the search space that are unlikely to contain better hyperparameter configurations by transferring knowledge from past experiments on other data sets as well as taking into account the evaluations already done on the current data set. Pruning as a new component for SMBO is an orthogonal contribution but nevertheless we compare it to surrogate models that learn across data sets and extensively investigate the impact of pruning with and without initialization for various state of the art surrogate models. The experiments are conducted on two newly created meta-data sets which we make publicly available. One of these meta-data sets is created on 59 data sets using 19 different classifiers resulting in a total of about 1.3 million experiments. This is by more than four times larger than all the results collaboratively collected by OpenML.\"",
        "title: \"Combining Multi-Distributed Mixture Models and Bayesian Networks for Semi-Supervised Learning\" with abstract: \"In many real world scenarios, mixture models have successfully been used for analyzing features in data ([11, 13, 21]). Usually, multivariate Gaussian distributions for continuous data ([2, 8, 4]) or Bayesian networks for nominal data ([15, 16]) are applied. In this paper, we combine both approaches in a family of Bayesian models for continuous data that are able to handle univariate as well as multivariate nodes, different types of distributions, e.g. Gaussian as well as Poisson distributed nodes, and dependencies between nodes. The models we introduce can be used for unsupervised, semi-supervised as well as for fully supervised learning tasks. We evaluate our models empirically on generated synthetic data and on public datasets thereby showing that they outperform classifiers such as SVMs and logistic regression on mixture data.\"",
        "title: \"Ultra-Fast Shapelets for Time Series Classification.\" with abstract: \"  Time series shapelets are discriminative subsequences and their similarity to a time series can be used for time series classification. Since the discovery of time series shapelets is costly in terms of time, the applicability on long or multivariate time series is difficult. In this work we propose Ultra-Fast Shapelets that uses a number of random shapelets. It is shown that Ultra-Fast Shapelets yield the same prediction quality as current state-of-the-art shapelet-based time series classifiers that carefully select the shapelets by being by up to three orders of magnitudes. Since this method allows a ultra-fast shapelet discovery, using shapelets for long multivariate time series classification becomes feasible.   A method for using shapelets for multivariate time series is proposed and Ultra-Fast Shapelets is proven to be successful in comparison to state-of-the-art multivariate time series classifiers on 15 multivariate time series datasets from various domains. Finally, time series derivatives that have proven to be useful for other time series classifiers are investigated for the shapelet-based classifiers. It is shown that they have a positive impact and that they are easy to integrate with a simple preprocessing step, without the need of adapting the shapelet discovery algorithm. \"",
        "1 is \"An Instance-based Approach for Identifying Candidate Ontology Relations within a Multi-Agent System\", 2 is \"Spreading Activation Models for Trust Propagation\".",
        "\nGiven above information, for an author who has written the paper with the title \"Tag recommendations in social bookmarking systems\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "018": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Online Robust Low-Rank Tensor Modeling for Streaming Data Analysis.':",
        "title: \"Deep Recurrent Regression for Facial Landmark Detection.\" with abstract: \"We propose a novel end-to-end deep architecture for face landmark detection, based on a deep convolutional and deconvolutional network followed by carefully designed recurrent network structures. The pipeline of this architecture consists of three parts. Through the first part, we encode an input face image to resolution-preserved deconvolutional feature maps via a deep network with stacked convol...\"",
        "title: \"Exact Low Tubal Rank Tensor Recovery from Gaussian Measurements.\" with abstract: \"The recent proposed Tensor Nuclear Norm (TNN) [Lu et al., 2016; 2018a] is an interesting convex penalty induced by the tensor SVD [Kilmer and Martin, 2011]. It plays a similar role as the matrix nuclear norm which is the convex surrogate of the matrix rank. Considering that the TNN based Tensor Robust PCA [Lu et al., 2018a] is an elegant extension of Robust PCA with a similar tight recovery bound, it is natural to solve other low rank tensor recovery problems extended from the matrix cases. However, the extensions and proofs are generally tedious. The general atomic norm provides a unified view of low-complexity structures induced norms, e.g., the $ell_1$-norm and nuclear norm. The sharp estimates of the required number of generic measurements for exact recovery based on the atomic norm are known in the literature. In this work, with a careful choice of the atomic set, we prove that TNN is a special atomic norm. Then by computing the Gaussian width of certain cone which is necessary for the sharp estimate, we achieve a simple bound for guaranteed low tubal rank tensor recovery from Gaussian measurements. Specifically, we show that by solving a TNN minimization problem, the underlying tensor of size $n_1times n_2times n_3$ with tubal rank $r$ can be exactly recovered when the given number of Gaussian measurements is $O(r(n_1+n_2-r)n_3)$. It is order optimal when comparing with the degrees of freedom $r(n_1+n_2-r)n_3$. Beyond the Gaussian mapping, we also give the recovery guarantee of tensor completion based on the uniform random mapping by TNN minimization. Numerical experiments verify our theoretical results.\"",
        "title: \"Image Classification by Selective Regularized Subspace Learning\" with abstract: \"Feature learning is an intensively studied research topic in image classification. Although existing methods like sparse coding, locality-constrained linear coding, fisher vector encoding, etc., have shown their effectiveness in image representation, most of them overlook a phenomenon called thesmall sample size problem, where the number of training samples is relatively smaller than the dimensionality of the features, which may limit the predictive power of the classifier. Subspace learning is a strategy to mitigate this problem by reducing the dimensionality of the features. However, most conventional subspace learning methods attempt to learn a global subspace to discriminate all the classes, which proves to be difficult and ineffective in multi-class classification task. To this end, we propose to learn a local subspace for each sample instead of learning a global subspace for all samples. Our key observation is that, in multi-class image classification, the label of each testing sample is only confused by a few classes which have very similar visual appearance to it. Thus, in this work, we propose a coarse-to-fine strategy, which first picks out such classes, and then conducts a local subspace learning to discriminate them. As the subspace learning method is regularized and conducted within some selected classes, we term it selective regularized subspace learning (SRSL), and we term our classification pipeline selective regularized subspace learning based multi-class image classification (SRSL_MIC). Experimental results on four representative datasets (Caltech-101, Indoor-67, ORL Faces and AR Faces) demonstrate the effectiveness of the proposed method.\"",
        "title: \"Diversified Visual Attention Networks for Fine-Grained Object Classification.\" with abstract: \"Fine-grained object classification attracts increasing attention in multimedia applications. However, it is a quite challenging problem due to the subtle interclass difference and large intraclass variation. Recently, visual attention models have been applied to automatically localize the discriminative regions of an image for better capturing critical difference, which have demonstrated promising performance. Unfortunately, without consideration of the diversity in attention process, most of existing attention models perform poorly in classifying fine-grained objects. In this paper, we propose a diversified visual attention network (DVAN) to address the problem of fine-grained object classification, which substantially relieves the dependency on strongly supervised information for learning to localize discriminative regions com-pared with attention-less models. More importantly, DVAN explicitly pursues the diversity of attention and is able to gather discriminative information to the maximal extent. Multiple attention canvases are generated to extract convolutional features for attention. An LSTM recurrent unit is employed to learn the attentiveness and discrimination of attention canvases. The proposed DVAN has the ability to attend the object from coarse to fine granularity, and a dynamic internal representation for classification is built up by incrementally combining the information from different locations and scales of the image. Extensive experiments conducted on CUB-2011, Stanford Dogs, and Stanford Cars datasets have demonstrated that the pro-posed DVAN achieves competitive performance compared to the state-of-the-art approaches, without using any prior knowledge, user interaction, or external resource in training and testing.\"",
        "title: \"Robust multiperson detection and tracking for mobile service and social robots.\" with abstract: \"This paper proposes an efficient system which integrates multiple vision models for robust multiperson detection and tracking for mobile service and social robots in public environments. The core technique is a novel maximum likelihood (ML)-based algorithm which combines the multimodel detections in mean-shift tracking. First, a likelihood probability which integrates detections and similarity to local appearance is defined. Then, an expectation-maximization (EM)-like mean-shift algorithm is derived under the ML framework. In each iteration, the E-step estimates the associations to the detections, and the M-step locates the new position according to the ML criterion. To be robust to the complex crowded scenarios for multiperson tracking, an improved sequential strategy to perform the mean-shift tracking is proposed. Under this strategy, human objects are tracked sequentially according to their priority order. To balance the efficiency and robustness for real-time performance, at each stage, the first two objects from the list of the priority order are tested, and the one with the higher score is selected. The proposed method has been successfully implemented on real-world service and social robots. The vision system integrates stereo-based and histograms-of-oriented-gradients-based human detections, occlusion reasoning, and sequential mean-shift tracking. Various examples to show the advantages and robustness of the proposed system for multiperson tracking from mobile robots are presented. Quantitative evaluations on the performance of multiperson tracking are also performed. Experimental results indicate that significant improvements have been achieved by using the proposed method.\"",
        "1 is \"Non-homogeneous Content-driven Video-retargeting\", 2 is \"Exploring Context with Deep Structured models for Semantic Segmentation.\".",
        "\nGiven above information, for an author who has written the paper with the title \"Online Robust Low-Rank Tensor Modeling for Streaming Data Analysis.\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "019": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'A framework for ETH-tight algorithms and lower bounds in geometric intersection graphs.':",
        "title: \"Covering Many or Few Points with Unit Disks\" with abstract: \"Let P be a set of n weighted points. We study approximation algorithms for the following two continuous facility-location problems. In the first problem we want to place m unit disks, for a given constant m\u22651, such that the total weight of the points from P inside the union of the disks is maximized. We present a deterministic algorithm that can compute, for any \u03b50, a (1\u2212\u03b5)-approximation to the optimal solution in O(n logn + \u03b5$^{{\\rm -4}{\\it m}}$log$^{\\rm 2{\\it m}}$ (1/\u03b5)) time. In the second problem we want to place a single disk with center in a given constant-complexity region X such that the total weight of the points from P inside the disk is minimized. Here we present an algorithm that can compute, for any \u03b50, with high probability a (1+\u03b5)-approximation to the optimal solution in O(n (log3n + \u03b5\u22124 log2n )) expected time.\"",
        "title: \"Treemaps with bounded aspect ratio\" with abstract: \"Treemaps are a popular technique to visualize hierarchical data. The input is a weighted tree T where the weight of each node is the sum of the weights of its children. A treemap for T is a hierarchical partition of a rectangle into simply connected regions, usually rectangles. Each region represents a node of T and its area is proportional to the weight of the corresponding node. An important quality criterion for treemaps is the aspect ratio of its regions. One cannot bound the aspect ratio if the regions are restricted to be rectangles. In contrast, polygonal partitions, that use convex polygons, can have bounded aspect ratio. We are the first to obtain convex partitions with optimal aspect ratio O(depth(T)). However, depth(T) still depends on the input tree. Hence we introduce a new type of treemaps, namely orthoconvex treemaps, where regions representing leaves are rectangles, L-, and S-shapes, and regions representing internal nodes are orthoconvex polygons. We prove that any input tree, irrespective of the weights of the nodes and the depth of the tree, admits an orthoconvex treemap of constant aspect ratio. We also obtain several specialized results for single-level treemaps, that is, treemaps where the input tree has depth 1.\"",
        "title: \"On The Design Of Genetic Algorithms For Geographical Applications\" with abstract: \"In many geographical optimization problems, the linkage (which determines the structure of building blocks) is determined by the spatial relationships between the components of a solution. Therefore the linkage can be identified easily, unlike in most other problems. Based ox this observation, we develop a hybrid GA that uses a geometrically local optimiser-one that computes good solutions to subproblems that are local in the geometric sense. One of the main advantages of our method is that it leads to GA's that are easily adapted to slight changes in the problem definition, without the need to tune many parameters in the fitness function. We apply our method to the map labeling problem, (placing as many names as possible on a map, without overlap), where it leads to good results.\"",
        "title: \"Improved Bounds for the Union of Locally Fat Objects in the Plane.\" with abstract: \"We show that, for any gamma > 0, the combinatorial complexity of the union of n locally gamma-fat objects of constant complexity in the plane is n/gamma(4)2(O(log* n)). For the special case of gamma-fat triangles, the bound improves to O(n log* n + n/gamma log(2) 1/gamma).\"",
        "title: \"Delineating imprecise regions via shortest-path graphs\" with abstract: \"An imprecise region, also called a vernacular region, is a region without a precise or administrative boundary. We present a new method to delineate imprecise regions from a set of points that are likely to lie inside the region. We use shortest-path graphs based on the squared Euclidean distance which capture the shape of region boundaries well. Shortest-path graphs naturally adapt to point sets of varying density, and they are always connected. As opposed to neighborhood graphs, they use a non-local criterion to determine which points to connect. Furthermore, shortest-path graphs can easily be extended to take geographic context into account by modeling context as \"soft\" obstacles. We present efficient algorithms to compute shortest-path graphs with or without geographic context. We experimentally evaluate the quality of the imprecise regions computed with our method. To fairly compare our results to those obtained by the common KDE approach, we also show how to integrate context into KDE by again using soft obstacles.\"",
        "1 is \"Decomposition of domains based on the micro-structure of finite constraint-satisfaction problems\", 2 is \"Improved algorithms for fully dynamic geometric spanners and geometric routing\".",
        "\nGiven above information, for an author who has written the paper with the title \"A framework for ETH-tight algorithms and lower bounds in geometric intersection graphs.\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0110": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'A Framework For Actively Selecting Viewpoints In Object Recognition':",
        "title: \"Learning with few examples for binary and multiclass classification using regularization of randomized trees\" with abstract: \"The human visual system is often able to learn to recognize difficult object categories from only a single view, whereas automatic object recognition with few training examples is still a challenging task. This is mainly due to the human ability to transfer knowledge from related classes. Therefore, an extension to Randomized Decision Trees is introduced for learning with very few examples by exploiting interclass relationships. The approach consists of a maximum a posteriori estimation of classifier parameters using a prior distribution learned from similar object categories. Experiments on binary and multiclass classification tasks show significant performance gains\"",
        "title: \"Online Next-Best-View Planning for Accuracy Optimization Using an Extended E-Criterion\" with abstract: \"Next-best-view (NBV) planning is an important aspect for three-dimensional (3D) reconstruction within controlled environments, such as a camera mounted on a robotic arm. NBV methods aim at a purposive 3D reconstruction sustaining predefined goals and limitations. Up to now, literature mainly presents NBV methods for range sensors, model-based approaches or algorithms that address the reconstruction of a finite set of primitives. For this work, we use an intensity camera without active illumination. We present a novel combined online approach comprising feature tracking, 3D reconstruction, and NBV planning that addresses arbitrary unknown objects. In particular we focus on accuracy optimization based on the reconstruction uncertainty. To this end we introduce an extension of the statistical E-criterion to model directional uncertainty, and we present a closed-form, optimal solution to this NBV planning problem. Our experimental evaluation demonstrates the effectivity of our approach using an absolute error measure.\"",
        "title: \"A comparison of nearest neighbor search algorithms for generic object recognition\" with abstract: \"The nearest neighbor (NN) classifier is well suited for generic object recognition. However, it requires storing the complete training data, and classification time is linear in the amount of data. There are several approaches to improve runtime and/or memory requirements of nearest neighbor methods: Thinning methods select and store only part of the training data for the classifier. Efficient query structures reduce query times. In this paper, we present an experimental comparison and analysis of such methods using the ETH-80 database. We evaluate the following algorithms. Thinning: condensed nearest neighbor, reduced nearest neighbor, Baram's algorithm, the Baram-RNN hybrid algorithm, Gabriel and GSASH thinning. Query structures: kd-tree and approximate nearest neighbor. For the first four thinning algorithms, we also present an extension to k-NN which allows tuning the trade-off between data reduction and classifier degradation. The experiments show that most of the above methods are well suited for generic object recognition.\"",
        "title: \"Semantic segmentation with millions of features: integrating multiple cues in a combined random forest approach\" with abstract: \"In this paper, we present a new combined approach for feature extraction, classification, and context modeling in an iterative framework based on random decision trees and a huge amount of features. A major focus of this paper is to integrate different kinds of feature types like color, geometric context, and auto context features in a joint, flexible and fast manner. Furthermore, we perform an in-depth analysis of multiple feature extraction methods and different feature types. Extensive experiments are performed on challenging facade recognition datasets, where we show that our approach significantly outperforms previous approaches with a performance gain of more than 15% on the most difficult dataset.\"",
        "title: \"Tracking and reconstruction in a combined optimization approach.\" with abstract: \"We present a novel approach to the structure-from-motion problem which combines the search for correspondences and geometric reconstruction, rather than treating these as separate steps. Through the combination of the two steps, we achieve an implicit feedback of 3D information to aid the correspondence search, and at the same time we avoid an explicit model for tracking errors. The reconstruction results are therefore optimal in case of, for example, Gaussian noise on image intensities. We also present an efficient online framework for structure-from-motion with our combined approach, thoroughly evaluate the method in experiments and compare the results to state-of-the-art methods.\"",
        "1 is \"Active Search for Real-Time Vision\", 2 is \"The Random Subspace Method for Constructing Decision Forests\".",
        "\nGiven above information, for an author who has written the paper with the title \"A Framework For Actively Selecting Viewpoints In Object Recognition\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0111": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Model-Driven web form validation with UML and OCL':",
        "title: \"Animation Can Show Only the Presence of Errors, Never Their Absence\" with abstract: \"Abstract: A formal specification animator executes and interprets traces on a specification. Similar to software testing, animation can only show the presence of errors, never their absence. However, animation is a powerful means of finding errors, and it is important that we adequately exercise a specification when we animate it. This paper outlines a systematic approach to the animation of formal specifications. We demonstrate the method on a small example, and then discuss its application to a non-trivial, system-level specification. Our aim is to provide a method for planned, documented and maintainable animation of specifications, so that we can achieve a high level of coverage, evaluate the adequacy of the animation, and repeat the process at a later time.\"",
        "title: \"Specification-Based Class Testing: A Case Study\" with abstract: \"This paper contains a case study demonstrating a complete process for specification-based class testing. The process starts with an abstract specification written in Object-Z and concludes by exercising an implementation with test cases and evaluating the results. The test cases are derived using the Test Template Framework for each individual operation. They are analysed to generate a finite state machine that can execute test sequences within the ClassBench framework. An oracle is also derived from the Object-Z specification. The case study demonstrates how a formal specification contributes to the development of practical tests that can be executed by a testing tool. It also shows how a test oracle can be derived from a specification and used by the same testing tool to evaluate test results.\"",
        "title: \"Refinement of higher-order logic programs\" with abstract: \"A refinement calculus provides a method for transforming specifications to executable code, maintaining the correctness of the code with respect to its specification. In this paper we extend the refinement calculus for logic programs to include higher-order programming capabilities in specifications and programs, such as procedures as terms and lambda abstraction. We use a higher-order type and term system to describe programs, and provide a semantics for the higher-order language and refinement. The calculus is illustrated by refinement examples.\"",
        "title: \"Grammar-based test generation with YouGen\" with abstract: \"Grammars are traditionally used to recognize or parse sentences in a language, but they can also be used to generate sentences. In grammar-based test generation (GBTG), context-free grammars are used to generate sentences that are interpreted as test cases. A generator reads a grammar G and generates L(G), the language accepted by the grammar. Often L(G) is so large that it is not practical to execute all of the generated cases. Therefore, GBTG tools support \u2018tags\u2019: extra-grammatical annotations which restrict the generation. Since its introduction in the early 1970s, GBTG has become well established: proven on industrial projects and widely published in academic venues. Despite the demonstrated effectiveness, the tool support is uneven; some tools target specific domains, e.g. compiler testing, while others are proprietary. The tools can be difficult to use and the precise meaning of the tags are sometimes unclear. As a result, while many testing practitioners and researchers are aware of GBTG, few have detailed knowledge or experience. We present YouGen, a new GBTG tool supporting many of the tags provided by previous tools. In addition, YouGen incorporates covering-array tags, which support a generalized form of pairwise testing. These tags add considerable power to GBTG tools and have been available only in limited form in previous GBTG tools. We provide semantics for the YouGen tags using parse trees and a new construct, generation trees. We illustrate YouGen with both simple examples and a number of industrial case studies. Copyright \u00a9 2010 John Wiley & Sons, Ltd.\"",
        "title: \"A state-of-practice questionnaire on verification and validation for concurrent programs\" with abstract: \"Research in verification and validation (V&V) for concurrent programs can be guided by practitioner information. A survey was therefore run to gain state-of-practice information in this context. The survey presented in this paper collected state-of-practice information on V&V technology in concurrency from 35 respondents. The results of the survey can help refine existing V&V technology by providing a better understanding of the context of V&V technology usage. Responses to questions regarding the motivation for selecting V&V technologies can help refine a systematic approach to V&V technology selection.\"",
        "1 is \"Impartiality, Justice and Fairness: The Ethics of Concurrent Termination\", 2 is \"Software product lines: a case study\".",
        "\nGiven above information, for an author who has written the paper with the title \"Model-Driven web form validation with UML and OCL\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0112": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Probabilistic Roadmaps for Path Planning in High-Dimensional Configuration Spaces':",
        "title: \"Natural Motion Generation for Humanoid Robots\" with abstract: \"This paper presents a method of generating natural-looking motion primitives for humanoid robots. An optimization-based approach is used to generate these primitives, but the objective function is tailored to each one and complexity is reduced by identifying relevant degrees of freedom. Several examples are shown in simulation: for an arm movement to reach an object, it is better to minimize the acceleration of key parts of the robot over its entire trajectory; for a single step on flat ground, it is better to minimize the torque and instantaneous angular momentum at every posture. The primitives are precomputed off-line, but might be used by on-line planner either to provide a fixed set of maneuvers or to bias a probabilistic, sample-based search for motions.\"",
        "title: \"Landmark-Based Robot Navigation\" with abstract: \" Achieving goals despite uncertainty in control and sensing may require robots toperform complicated motion planning and execution monitoring. This paper describesa reduced version of the general planning problem in the presence of uncertainty anda complete polynomial algorithm solving it. The planner computes a guaranteed plan(for given uncertainty bounds) by backchaining omnidirectional backprojections of thegoal until one fully contains the set of possible initial positions of the robot. ... \"",
        "title: \"Motion Strategies For Maintaining Visibility Of A Moving Target\" with abstract: \"We introduce the problem of computing robot motion strategies that maintain visibility of a moving target in a cluttered workspace. Both motion constraints(as considered in standard motion planning) and visibility constraints (as considered in visual tracking) must be satisfied. Additional criteria, such as the total distance traveled, can be optimized. The general problem is divided into two categories, on the basis of whether the target is predictable. For the predictable cease, an algorithm that computes optimal, numerical solutions is presented. For the more challenging case of a partially-predictable target, two on-line algorithms are presented that each attempt to maintain future visibility with limited prediction. One strategy maximizes the probability that the target will remain in view in a subsequent time step, and the other maximizes the minimum time in which the target could escape the visibility region. We additionally discuss issues resulting from our implementation and experiments on a mobile robot system.\"",
        "title: \"Modeling Structural Heterogeneity in Proteins from X-Ray Data\" with abstract: \"In a crystallographic experiment, a protein is precipitated to obtain a crystalline sample (crystal) containing many copies of the molecule. An electron density map (EDM) is calculated from diffraction images obtained from focusing X-rays through the sample at different angles. This involves iterative phase determination and density calculation. The protein conformation is modeled by placing the atoms in 3-D space to best match the electron density. In practice, the copies of a protein in a crystal are not exactly in the same conformation. Consequently the obtained EDM, which corresponds to the cumulative distribution of atomic positions over all conformations, is blurred. Existing modeling methods compute an \"average\" protein conformation by maximizing its fit with the EDM and explain structural heterogeneity in the crystal with a harmonic distribution of the position of each atom. However, proteins undergo coordinated conformational variations leading to substantial correlated changes in atomic positions. These variations are biologically important. This paper presents a sample-select approach to model structural heterogeneity by computing an ensemble of conformations (along with occupancies) that, collectively, provide a near-optimal explanation of the EDM. The focus is on deformable protein fragments, mainly loops and side-chains. Tests were successfully conducted on simulated and experimental EDMs.\"",
        "title: \"Planning motions with intentions\" with abstract: \"We apply manipulation planning to computer animation. A new path planner is presented that automatically computes the collision-free trajectories for several cooperating arms to manipulate a movable object between two configurations. This implemented planner is capable of dealing with complicated tasks where regrasping is involved. In addition, we present a new inverse kinematics algorithm for the human arms. This algorithm is utilized by the planner for the generation of realistic human arm motions as they manipulate objects. We view our system as a tool for facilitating the production of animation.\"",
        "1 is \"A probabilistic roadmap approach for systems with closed kinematic chains\", 2 is \"Geometric And Computational Aspects Of Gravity Casting\".",
        "\nGiven above information, for an author who has written the paper with the title \"Probabilistic Roadmaps for Path Planning in High-Dimensional Configuration Spaces\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0113": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'The effect of memory capacity on Time Warp performance':",
        "title: \"Parallel simulation techniques for large-scale networks\" with abstract: \"Simulation has always been an indispensable tool in the design and analysis of telecommunication networks. Due to performance limitations of the majority of simulators, usually network simulations have been done for rather small network models and for short timescales. In contrast, many difficult design problems facing today's network engineers concern the behavior of very large hierarchical multihop networks carrying millions of multiprotocol flows over long timescales. Examples include scalability and stability of routing protocols, packet losses in core routers, of long-lasting transient behavior due to observed self-similarity of traffic patterns. Simulation of such systems would greatly benefit from application of parallel computing technologies, especially now that multiprocessor workstations and servers have become commonly available. However, parallel simulation has not yet been widely embraced by the telecommunications community due to a number of difficulties. Based on our accumulated experience in parallel network simulation projects, we believe that parallel simulation technology has matured to the point that it is ready to be used in industrial practice of network simulation. This article highlights work in parallel simulations of networks and their promise\"",
        "title: \"Energy consumption of HLA data distribution management approaches\" with abstract: \"Energy and power aware data distribution methods are essential for using these approaches in energy constrained devices and environments. Data Distribution Management (DDM) is a set of services defined in the High Level Architecture (HLA) that aims to efficiently propagate distributed simulation state information. This paper describes an empirical study of the energy consumption of computation and communication components of region based and grid based DDM approaches in mobile devices. Experimental data illustrate that region based approaches tend to consume more energy than grid based approaches for computations, but less for communications. These results also show that the choice of grid cell size and grid cell constraints on publication regions can play an important role in the energy efficiency of grid based approaches.\n\n\"",
        "title: \"Next Generation Real-Time RTI Software\" with abstract: \"Abstract: This paper describes the recent changes to the RTI-Kit and changes being studied to develop new paradigms for real-time distributed simulation execution. We discuss design options for hard real-time (HRT) extensions to the High-Level Architecture and current efforts to adapt the high-performance RTI-Kit modules to a HRT RTI. We give some preliminary results supporting the efficacy of a hard real-time RTI implementation and discuss the impact of its availability.\n\n\"",
        "title: \"Scalable simulation of electromagnetic hybrid codes\" with abstract: \"New discrete-event formulations of physics simulation models are emerging that can outperform models based on traditional time-stepped techniques. Detailed simulation of the Earth's magnetosphere, for example, requires execution of sub-models that are at widely differing timescales. In contrast to time-stepped simulation which requires tightly coupled updates to entire system state at regular time intervals, the new discrete event simulation (DES) approaches help evolve the states of sub-models on relatively indepen-dent timescales. However, parallel execution of DES-based models raises challenges with respect to their scalability and performance. One of the key challenges is to improve the computation granularity to offset synchronization and communication overheads within and across processors. Our previous work was limited in scalability and runtime performance due to the parallelization challenges. Here we report on optimizations we performed on DES-based plasma simulation models to improve parallel performance. The mapping of model to simulation processes is optimized via aggregation techniques, and the parallel runtime engine is optimized for communication and memory efficiency. The net result is the capability to simulate hybrid particle-in-cell (PIC) models with over 2 billion ion particles using 512 processors on supercomputing platforms.\"",
        "title: \"Real-time data driven arterial simulation for performance measures estimation\" with abstract: \"Transportation professionals are increasingly exploring multi-pronged solutions to alleviate traffic congestion. Real-time information systems for travelers and facility managers are one approach that has been the focus of many recent efforts. Real-time performance information can facilitate more efficient roadway usage and operations. Toward this end, a dynamic data driven simulation based system for estimating and predicting performance measures along arterial streets in real-time is described that uses microscopic traffic simulations, driven by point sensor data. Current practices of real-time estimation of roadway performance measures are reviewed. The proposed real-time data driven arterial simulation methodology to estimate performance measures along arterials is presented as well as preliminary field results that provide evidence to validate this approach.\"",
        "1 is \"Using Web services to integrate heterogeneous simulations in a grid environment\", 2 is \"Gathering correlated data in sensor networks\".",
        "\nGiven above information, for an author who has written the paper with the title \"The effect of memory capacity on Time Warp performance\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0114": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'A Frame-Level Rate Control Scheme Based on Texture and Nontexture Rate Models for High Efficiency Video Coding':",
        "title: \"Motion-decision based spatiotemporal saliency for video sequences\" with abstract: \"An adaptive spatiotemporal saliency algorithm for video attention detection using motion vector decision is proposed, motivated by the importance of motion information in video sequences for human visual system. This novel system can detect the saliency regions quickly by using only part of the classic saliency features in each iteration. Motion vectors calculated by block matching and optical flow are used to determine the decision condition. When significant motion contrast occurs (decision condition is satisfied), the saliency area is detected by motion and intensity features. Otherwise, when motion contrast is low, color and orientation features are added to form a more detailed saliency map. Experimental results show that the proposed algorithm can detect salient objects and actions in video sequences robustly and efficiently.\"",
        "title: \"An effective example-based learning method for denoising of medical images corrupted by heavy Gaussian noise and poisson noise\" with abstract: \"Denoising is an essential application to improve image quality, especially in medical imaging. This paper introduces an example and patch-based learning method for reducing Gaussian noise and Poisson noise which often appear in medical imaging modalities using ionizing radiation. In the proposed method, denoising is performed by learning the regression model based on a set of the nearest neighbors of a given noisy patch, with the help of a given set of standard images. The method is evaluated and compared to several state-of-the-art denoising methods. The obtained results confirm its efficiency, especially for heavy noise.\"",
        "title: \"Robust tracking and mapping with a handheld RGB-D camera\" with abstract: \"In this paper, we propose a robust method for camera tracking and surface mapping using a handheld RGB-D camera which is effective in challenging situations such as fast camera motion or geometrically featureless scenes. The main contributions are threefold. First, we introduce a robust orientation estimation based on quaternion method for initial sparse estimation. By using visual feature points detection and matching, no prior or small movement assumption is required to estimate a rigid transformation between frames. Second, a weighted ICP (Iterative Closest Point) method for better rate of convergence in optimization and accuracy in resulting trajectory is proposed. While the conventional ICP fails when there is no 3D features in the scene, our approach achieves robustness by emphasizing the influence of points that contain more geometric information of the scene. Finally, we show quantitative results on an RGB-D benchmark dataset. The experiments on an RGB-D trajectory benchmark dataset demonstrate that our method is able to track camera pose accurately.\"",
        "title: \"Accelerating GMM-based patch priors for image restoration: Three ingredients for a 100\u00d7 speed-up.\" with abstract: \"Image restoration methods aim to recover the underlying clean image from corrupted observations. The expected patch log-likelihood (EPLL) algorithm is a powerful image restoration method that uses a Gaussian mixture model (GMM) prior on the patches of natural images. Although it is very effective for restoring images, its high runtime complexity makes the EPLL ill-suited for most practical applica...\"",
        "title: \"Vector sparse representation of color image using quaternion matrix analysis.\" with abstract: \"Traditional sparse image models treat color image pixel as a scalar, which represents color channels separately or concatenate color channels as a monochrome image. In this paper, we propose a vector sparse representation model for color images using quaternion matrix analysis. As a new tool for color image representation, its potential applications in several image-processing tasks are presented, including color image reconstruction, denoising, inpainting, and super-resolution. The proposed model represents the color image as a quaternion matrix, where a quaternion-based dictionary learning algorithm is presented using the K-quaternion singular value decomposition (QSVD) (generalized K-means clustering for QSVD) method. It conducts the sparse basis selection in quaternion space, which uniformly transforms the channel images to an orthogonal color space. In this new color space, it is significant that the inherent color structures can be completely preserved during vector reconstruction. Moreover, the proposed sparse model is more efficient comparing with the current sparse models for image restoration tasks due to lower redundancy between the atoms of different color channels. The experimental results demonstrate that the proposed sparse image model avoids the hue bias issue successfully and shows its potential as a general and powerful tool in color image analysis and processing domain.\"",
        "1 is \"Monte-Carlo sure: a black-box optimization of regularization parameters for general denoising algorithms.\", 2 is \"On Distributed Systems and Social Engineering\".",
        "\nGiven above information, for an author who has written the paper with the title \"A Frame-Level Rate Control Scheme Based on Texture and Nontexture Rate Models for High Efficiency Video Coding\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0115": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Efficient and adaptive proportional share I/O scheduling':",
        "title: \"Scheduling multiple flows on parallel disks\" with abstract: \"We examine the problem of scheduling concurrent independent flows on multiple-disk I/O storage systems. Two models are considered: in the shared buffer model the memory buffer is shared among all the flows, while in the partitioned buffer model each flow has a private buffer. For the parallel disk model with d 1 disks it is shown that the problem of minimizing the schedule length of n 2 concurrent flows is NP-complete for both buffer models. A randomized scheduling algorithm for the partitioned buffer model is analyzed and probabilistic bounds on the schedule length are presented. Finally a heuristic based on static buffer allocation for the shared buffer model is discussed.\"",
        "title: \"Statistical workload shaping for storage systems\" with abstract: \"Data center computing is gaining popularity due to the cost efficiencies of consolidation, centralized management, and high reliability. The unpredictable bursty nature of typical workloads where the instantaneous arrival rates can significantly exceed the average long-term rate, requires the server to significantly over provision resources in order to meet response-time service level agreements (SLAs), resulting in low resource utilization and higher costs. In this paper we consider a statistical on-off model for bursty workloads to explore the relationship between burst size, frequency, capacity, and response time distribution. We analyze the performance of a workload shaping method to reduce capacity requirements by providing a graduated two-level SLA. Statistics of the request arrival to the overflow queue are characterized in terms of the underlying Markov chain passage times, and used to estimate its capacity.\"",
        "title: \"Lexicographic QoS scheduling for parallel I/O\" with abstract: \"High-end shared storage systems serving multiple independent workloads must assure that concurrently executing clients will receive a fair or agreed-upon share of system I/O resources. In a parallel I/O system an application makes requests for specific disks at different steps of its computation depending on the data layout and its computational state. Different applications contend for disk access making the problem of maintaining fair allocation challenging.We propose a model for differentiated disk bandwidth allocation based on lexicographic minimization, and provide new efficient scheduling algorithms to allocate the I/O bandwidth fairly among contending applications. A major contribution of our model is its ability to handle multiple parallel disks and contention for disks among the concurrent applications. Analysis and simulation-based evaluation shows that our algorithms provide performance isolation, weighted allocation of resources, and are work conserving. The solutions are also applicable to other shared resource environments dealing with non-uniform heterogeneous servers.\"",
        "title: \"Optimal Read-Once Parallel Disk Scheduling\" with abstract: \"An optimal prefetching and I/O scheduling algorithm L-OPT, for parallel I/O systems, using a read-once model of block references is presented. The algorithm uses knowledge of the next $L$ references, $L$-block lookahead, to create a minimal-length I/O schedule. For a system with $D$ disks and a buffer of capacity $m$ blocks, we show that the competitive ratio of L-OPT is $\\Theta(\\sqrt{mD/L})$ when $L \\geq m$, which matches the lower bound of any prefetching algorithm with $L$-block lookahead. Tight bounds for the remaining ranges of lookahead are also presented. In addition we show that L-OPT is the optimal offline algorithm: when the lookahead consists of the entire reference string, it performs the absolute minimum possible number of I/Os. Finally, we show that L-OPT is comparable with the best online algorithm with the same amount of lookahead; the ratio of the length of its schedule to the length of the optimal schedule is always within a constant factor.\"",
        "title: \"Brief Announcement: Hardware Transactional Storage Class Memory.\" with abstract: \"Emerging persistent memory technologies (generically referred to as Storage Class Memory or SCM) hold tremendous promise for accelerating popular data-management applications like in-memory databases. However, programmers now need to deal with ensuring the atomicity of transactions on SCM-resident data and maintaining consistency between the persistent and in-memory execution orders of concurrent transactions. The problem is specially challenging when high-performance isolation mechanisms like Hardware Transaction Memory (HTM) are used for concurrency control. In this work we show how SCM-based HTM transactions can be ordered correctly using existing CPU instructions, without requiring any changes to existing processor cache hardware or HTM protocols. We describe a method that employs HTM for concurrency control and enforces atomic persistence and consistency with a novel software protocol and back-end external memory controller. In contrast, previous approaches require significant hardware changes to existing processor microarchitectures.\"",
        "1 is \"Scalable Distributed Communication Architectures to Support Advanced Metering Infrastructure in Smart Grid\", 2 is \"Optimal parallel merging and sorting without memory conflicts\".",
        "\nGiven above information, for an author who has written the paper with the title \"Efficient and adaptive proportional share I/O scheduling\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0116": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Emergent Semantics and Cooperation in Multi-knowledge Communities: the ESTEEM Approach':",
        "title: \"An algorithm for implementing BFT registers in distributed systems with bounded churn\" with abstract: \"Distributed storage service is one of the main abstractions provided to the developers of distributed applications due to its capability to hide the complexity generated by the messages exchanged between processes. Many protocols have been proposed to build byzantine-faulttolerant storage services on top of a message-passing system, but they do not consider the possibility to have servers joining and leaving the computation (churn phenomenon). This phenomenon, if not properly mastered, can either block protocols or violate the safety of the storage. In this paper, we address the problem of building of a safe register storage resilient to byzantine failures in a distributed system affected from churn. A protocol implementing a safe register in an eventually synchronous system is proposed and some feasibility constraints on the arrival and departure of the processes are given. The protocol is proved to be correct under the assumption that the constraint on the churn is satisfied.\"",
        "title: \"An Index-Based Checkpointing Algorithm for Autonomous Distributed Systems\" with abstract: \"This paper presents an index-based checkpointing algorithm for distributed systems with the aim of reducing the total number of checkpoints while ensuring that each checkpoint belongs to at least one consistent global checkpoint (or recovery line). The algorithm is based on an equivalence relation defined between pairs of successive checkpoints of a process which allows us, in some cases, to advance the recovery line of the computation without forcing checkpoints in other processes. The algorithm is well-suited for autonomous and heterogeneous environments, where each process does not know any private information about other processes and private information of the same type of distinct processes is not related (e.g., clock granularity, local checkpointing strategy, etc.). We also present a simulation study which compares the checkpointing-recovery overhead of this algorithm to the ones of previous solutions.\"",
        "title: \"Direct Dependency-Based Determination Of Consistent Global Checkpoints\" with abstract: \"Building consistent global checkpoints that contain a given set of local checkpoints has been usually handled by using transitive dependency tracking. This imply the usage of a vector of integers piggybacked on each message of the computation (the vector size being given by the number of processes). In this paper we address the problem to get consistent global checkpoints including a given subset of local checkpoints tracking just direct dependencies. In that case application messages are required to piggyback one integer. An algorithm is proposed that takes a set of local checkpoints as an input and returns the minimum consistent global checkpoint, if any, including that set. Otherwise it returns the first consistent global checkpoint that follows this subset. Among the applications of the algorithm there are rollback-recovery and global predicate detection.\"",
        "title: \"From Crash Fault-Tolerance to Arbitrary-Fault Tolerance: Towards a Modular Approach\" with abstract: \"The design of protocols able to cope with processes exhibiting an arbitrary faulty behavior is a real practical challenge due to malicious attacks or unexpected software errors. Nowadays, there are many protocols able to cope with process crashes, but, unfortunately, a process crash represents only a particular faulty behavior. Then, a good engineering argument would be to take a protocol resilient to process crashes and to transform it into one resilient to arbitrary failures. This paper presents a generic methodology to perform the previous transformation in the case where processes run the same text and regularly exchange messages (i.e., the case of round-based protocols). This modular approach encapsulates the detection of arbitrary failures in specific modules. Such a methodology can be the starting point for designing tools that allow automatic transformation. We show an application of this methodology to the case of consensus.\"",
        "title: \"On the Deterministic Tracking of Moving Objects with a Binary Sensor Network\" with abstract: \"This paper studies the problem of associating deterministically a track revealed by a binary sensor network with the trajectory of a unique moving anonymous object, namely the Multiple Object Tracking and Identification(MOTI) problem. In our model, the network is represented by a sparse connected graph where each vertex represents a binary sensor and there is an edge between two sensors if an object can pass from a sensed region to another without activating any other remaining sensor. The difficulty of MOTI lies in the fact that trajectories of two or more objects can be so close (track merging) that the corresponding tracks on the sensor network can no longer be distinguished, thus confusing the deterministic association between an object trajectory and a track.The paper presents several results. We first show that MOTI cannot be solved on a general graph of ideal binary sensors even by an omniscient external observer if all the objects can freely move on the graph. Then, we describe some restrictions that can be imposed a priorieither on the graph, on the object movements or both, to make MOTI problem always solvable. We also discuss the consequences of our results and present some related open problems.\"",
        "1 is \"Sample Evaluation of Ontology-Matching Systems\", 2 is \"Efficient Byzantine-resilient reliable multicast on a hybrid failure model\".",
        "\nGiven above information, for an author who has written the paper with the title \"Emergent Semantics and Cooperation in Multi-knowledge Communities: the ESTEEM Approach\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0117": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Multimedia indexing and retrieval: ever great challenges':",
        "title: \"Evolutionary artificial neural networks by multi-dimensional particle swarm optimization.\" with abstract: \"In this paper, we propose a novel technique for the automatic design of Artificial Neural Networks (ANNs) by evolving to the optimal network configuration(s) within an architecture space. It is entirely based on a multi-dimensional Particle Swarm Optimization (MD PSO) technique, which re-forms the native structure of swarm particles in such a way that they can make inter-dimensional passes with a dedicated dimensional PSO process. Therefore, in a multidimensional search space where the optimum dimension is unknown, swarm particles can seek both positional and dimensional optima. This eventually removes the necessity of setting a fixed dimension a priori, which is a common drawback for the family of swarm optimizers. With the proper encoding of the network configurations and parameters into particles, MD PSO can then seek the positional optimum in the error space and the dimensional optimum in the architecture space. The optimum dimension converged at the end of a MD PSO process corresponds to a unique ANN configuration where the network parameters (connections, weights and biases) can then be resolved from the positional optimum reached on that dimension. In addition to this, the proposed technique generates a ranked list of network configurations, from the best to the worst. This is indeed a crucial piece of information, indicating what potential configurations can be alternatives to the best one, and which configurations should not be used at all for a particular problem. In this study, the architecture space is defined over feed-forward, fully-connected ANNs so as to use the conventional techniques such as back-propagation and some other evolutionary methods in this field. The proposed technique is applied over the most challenging synthetic problems to test its optimality on evolving networks and over the benchmark problems to test its generalization capability as well as to make comparative evaluations with the several competing techniques. The experimental results show that the MD PSO evolves to optimum or near-optimum networks in general and has a superior generalization capability. Furthermore, the MD PSO naturally favors a low-dimension solution when it exhibits a competitive performance with a high dimension counterpart and such a native tendency eventually yields the evolution process to the compact network configurations in the architecture space rather than the complex ones, as long as the optimality prevails.\"",
        "title: \"Personalized long-term ECG classification: A systematic approach\" with abstract: \"This paper presents a personalized long-term electrocardiogram (ECG) classification framework, which addresses the problem within a long-term ECG signal, known as Holter register, recorded from an individual patient. Due to the massive amount of ECG beats in a Holter register, visual inspection is quite difficult and cumbersome, if not impossible. Therefore, the proposed system helps professionals to quickly and accurately diagnose any latent heart disease by examining only the representative beats (the so-called master key-beats) each of which is automatically extracted from a time frame of homogeneous (similar) beats. We tested the system on a benchmark database where beats of each Holter register have been manually labeled by cardiologists. The selection of the right master key-beats is the key factor for achieving a highly accurate classification and thus we used exhaustiveK-means clustering in order to find out (near-) optimal number of key-beats as well as the master key-beats. The classification process produced results that were consistent with the manual labels with over 99% average accuracy, which basically shows the efficiency and the robustness of the proposed system over massive data (feature) collections in high dimensions.\"",
        "title: \"A Dynamic Content-Based Indexing Method For Multimedia Databases: Hierarchical Cellular Tree\" with abstract: \"This paper presents a novel indexing technique, Hierarchical Cellular Tree, which is designed to bring an effective solution especially for indexing on large-scale multimedia databases. A pre-emptive cell search mechanism is introduced in order to prevent the corruption of large multimedia item collections due to the limited discrimination obtained from the visual and aural descriptors. In addition to this, the similar items are focused within appropriate cellular structures, which will be the subject to mitosis operations when the dissimilarity emerges as a result of irrelevant item insertions. Mitosis operations ensure to keep the cells in a focused and compact form and yet the cells can grow into any dimension as long as the compactness prevails. The proposed indexing scheme is then optimized for a novel query method, the Progressive Query, in order to maximize the retrieval efficiency for the user point of view. Experimental results show that the speed of the retrievals is significantly improved.\"",
        "title: \"The visual goodness evaluation of color-based retrieval processes\" with abstract: \"A content-based image retrieval system helps users to find suitable images or videos from large databases for their purposes. In this paper we present the results of analyses of visual outputs from our image retrieval system, based on color matching. These analyses are based on human evaluation of color content similarity. Computer-based simulation results have been analyzed visually with the opinion scores method, i.e. how good the results are according to the human visual system. Tests have shown that there is no superior retrieval method that is best in every case. The goodness results of each retrieval method will be presented in this paper. The usability and usefulness of each system will be presented as well as a possible utilization domain.\"",
        "title: \"Semi-Supervised Learning for Ill-Posed Polarimetric SAR Classification\" with abstract: \"In recent years, the interest in semi-supervised learning has increased, combining supervised and unsupervised learning approaches. This is especially valid for classification applications in remote sensing, while the data acquisition rate in current systems has become fairly large considering high-and very-high resolution data; yet on the other hand, the process of obtaining the ground truth data may be cumbersome for such large repositories. In this paper, we investigate the application of semi-supervised learning approaches and particularly focus on the small sample size problem. To that extend, we consider two basic unsupervised approaches by enlarging the initial labeled training set as well as an ensemble-based self-training method. We propose different strategies within self-training on how to select more reliable candidates from the pool of unlabeled samples to speed-up the learning process and to improve the classification performance of the underlying classifier ensemble. We evaluate the effectiveness of the proposed semi-supervised learning approach over polarimetric SAR data. Results show that the proposed self-training approach using an ensemble-based classifier that is initially trained over a small training set can achieve a similar performance level of a fully supervised learning approach where the training is performed over significantly larger labeled data. Considering the difficulties of the manual data labeling in such massive volumes of SAR repositories, this is indeed a promising accomplishment for semi-supervised SAR classification.\"",
        "1 is \"Lossless plenoptic image compression using adaptive block differential prediction\", 2 is \"MSLD: A robust descriptor for line matching\".",
        "\nGiven above information, for an author who has written the paper with the title \"Multimedia indexing and retrieval: ever great challenges\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0118": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Novel Inter-file Coded Placement and D2D Delivery for a Cache-aided Fog-RAN Architecture.':",
        "title: \"A Survey of Physical Layer Security Techniques for 5G Wireless Networks and Challenges Ahead.\" with abstract: \"Physical layer security which safeguards data confidentiality based on the information-theoretic approaches has received significant research interest recently. The key idea behind physical layer security is to utilize the intrinsic randomness of the transmission channel to guarantee the security in physical layer. The evolution toward 5G wireless communications poses new challenges for physical l...\"",
        "title: \"Order-Optimal Rate of Caching and Coded Multicasting with Random Demands.\" with abstract: \"We consider the canonical shared link caching network formed by a source node, hosting a library of $m$ information messages (files), connected via a noiseless multicast link to $n$ user nodes, each equipped with a cache of size $M$ files. Users request files independently at random according to an a-priori known demand distribution q. A coding scheme for this network consists of two phases: cache placement and delivery. The cache placement is a mapping of the library files onto the user caches that can be optimized as a function of the demand statistics, but is agnostic of the actual demand realization. After the user demands are revealed, during the delivery phase the source sends a codeword (function of the library files, cache placement, and demands) to the users, such that each user retrieves its requested file with arbitrarily high probability. The goal is to minimize the average transmission length of the delivery phase, referred to as rate (expressed in channel symbols per file). In the case of deterministic demands, the optimal min-max rate has been characterized within a constant multiplicative factor, independent of the network parameters. The case of random demands was previously addressed by applying the order-optimal min-max scheme separately within groups of files requested with similar probability. However, no complete characterization of order-optimality was previously provided for random demands under the average rate performance criterion. In this paper, we consider the random demand setting and, for the special yet relevant case of a Zipf demand distribution, we provide a comprehensive characterization of the order-optimal rate for all regimes of the system parameters, as well as an explicit placement and delivery scheme achieving order-optimal rates. We present also numerical results that confirm the superiority of our scheme with respect to previously proposed schemes for the same setting.\"",
        "title: \"Allocations for heterogenous distributed storage\" with abstract: \"We study the problem of storing a data object in a set of data nodes that fail independently with given probabilities. Our problem is a natural generalization of a homogenous storage allocation problem where all the nodes had the same reliability and is naturally motivated for peer-to-peer and cloud storage systems with different types of nodes. Assuming optimal erasure coding (MDS), the goal is to find a storage allocation (i.e, how much to store in each node) to maximize the probability of successful recovery. This problem turns out to be a challenging combinatorial optimization problem. In this work we introduce an approximation framework based on large deviation inequalities and convex optimization. We propose two approximation algorithms and study the asymptotic performance of the resulting allocations.\"",
        "title: \"Interference alignment, carrier pairing, and lattice decoding\" with abstract: \"We discuss how to take advantage of the discreteness of signal constellations at the detector side, in various interference alignment schemes for which linear zero-forcing of the interference has been traditionally considered. We show that lattice decoding, with possible \u201cgeneralized decision feedback equalization\u201d preprocessing can achieve significant gains in symbol error rate at the detector output, at finite SNR. Furthermore, we discuss the effect of channel coefficient dynamics on the Cadambe and Jafar interference alignment precoding scheme, and point out the need of careful carrier pairing in order to limit these dynamics.\"",
        "title: \"Optimal tradeoff of secure PUF-based authentication\" with abstract: \"We consider a problem of secure PUF-based authentication under a privacy constraint on the PUF responses. Given a randomly chosen challenge, an adversary who has access to the (publicly) stored helper data and side information correlated with the challenge -response pairs may try to mimic the corresponding PUF response in order to get authenticated. We characterize the optimal tradeoff between rate of the compressed helper data, information leakage rate of the PUF response at the adversary, and the adversary's maximum false acceptance probability exponent.. The result captures a tradeoff between authentication performance and security aspects of PUF-based authentication such as authentication security and unpredictability of PUF when used in authentication applications.\"",
        "1 is \"Gaussian multiaccess channels with ISI: capacity region and multiuser water-filling\", 2 is \"Coverage and Rate Analysis for Millimeter-Wave Cellular Networks\".",
        "\nGiven above information, for an author who has written the paper with the title \"Novel Inter-file Coded Placement and D2D Delivery for a Cache-aided Fog-RAN Architecture.\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0119": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Multi-cell MIMO cooperative networks: a new look at interference':",
        "title: \"Memory-Based Opportunistic Multi-User Beamforming\" with abstract: \"A scheme exploiting memory in opportunistic multiuser beamforming is proposed. The scheme builds on recent advances realized in [1] in the area of multi-user downlink precoding and scheduling based on partial transmitter channel state information (CSIT). Although the precoding and scheduling done in [1] is optimal within the set of unitary precoders, it is only so asymptotically for large number of users. Secondly, this scheme is unable to exploit potential time correlation of the channel. In this paper, we show 1- that exploiting memory in the transmitter allows to fill the gap to optimality for fixed (even low) number of users for time correlated channels, 2- how such schemes ([1] and ours) can be extended to take fairness into account in the proportional fair sense.\"",
        "title: \"A Semi-Blind Approach To Structured Channel Equalization\" with abstract: \"This paper describes a direct equalization approach for channels with some underlying structure. A semi-blind approach is taken here where a snail amount of training symbols is available. A family of MMSE equalizers is obtained that includes some prior information about the channel structure. The channel structure assumed in this paper is that the channel vector lies approximately in the subspace of a matric associated with the samples of the transmit pulse shape. Blind identifiability issues of the structured equalizer are also addressed. Numerical results using experimental indoor channel data indicate that these structured equalizers can achieve bit error rates that are significantly lower than traditional non-blind MMSE equalizers.\"",
        "title: \"Interference alignment in the partially connected K-user MIMO interference channel\" with abstract: \"We consider interference alignment in the partially connected K-user MIMO interference channel (IC). Conversely to the fully-connected case, we show that interference alignment can be achievable for an arbitrary number of users K in the network, while the per-user signaling dimension remains fixed, provided that the number of interference links per user is bounded. For this class of channels, which we denote by L-interfering K-user MIMO IC, we provide a criterion applicable to symmetric systems for the system of IA equations to be proper, according to the framework introduced earlier by Yetis et al. Properness is a necessary condition for IA to be feasible. Interestingly, this criterion is independent from the number of users K. Furthermore, we propose an iterative algorithm to solve the alignment problem for this class of channels.\"",
        "title: \"Some Systems Aspects Regarding Compressive Relaying with Wireless Infrastructure Links\" with abstract: \"In this paper, we consider single-cell cellular networks assisted with fixed relay station (RS), used by mobile stations (MS) to access the base station (BTS) via a relaying strategy. The RSs are positioned around the BTS, in such a way that wireless channels on the relay link (from RSs to the BTS) are line-of-sight, we analyze the achievable sum-of-rates for up-link communications. We compare two relaying strategies at the RSs, namely amplify-and-forward (AF) and compress-and-forward (CF). It is assumed that mobile signals and relay signals are emitted on orthogonal bands (FDD), with the possibility of having a larger bandwidth (BW) on the relay-to-base links. We predict the system gains bought by relays, in comparison with two other reference systems. One reference is an ideal relay-based system where the relays enjoy noiseless communications to the BTSs, i.e. a so-called distributed antenna system (DAS). The second reference is offered by a conventional cellular systems without relays, but same number of overall infrastructure antennas. In this paper, it is demonstrated the surprising result that with a relay bandwidth just twice that of the mobile's bandwidth, the system capacity approaches that of an ideal distributed antenna system, (while probably being much superior in practice in terms of ease of deployment and cost). The capacity gains of the relay-assisted network over a conventional network are also analyzed.\"",
        "title: \"Rate loss analysis of transmitter cooperation with distributed CSIT\" with abstract: \"We consider in this work the problem of determining the number of feedback bits which should be used to quantize the channel state information (CSI) in a broadcast channel (BC) with K transmit antennas (or equivalently K single-antenna transmitters (TXs)) and K single-antenna receivers (RXs). We focus on an extension of the conventional centralized CSI at the TX (CSIT) model, where instead of having a single channel estimate, or quantized version, perfectly shared by all the TX antennas, each TX receives its own estimate of the global multiuser channel. This CSIT configuration, denoted as distributed CSIT, is particularly suited to model the joint transmission from TXs which are not colocated. With centralized CSIT, a very important design guideline for the feedback link was provided by Jindal [Trans. Inf. Theory 2006] by providing a sufficient feedback rate to ensure that the rate loss stays below a maximum value. In the distributed CSIT setting, additional errors occur and the design guidelines for the centralized case are no longer valid. Consequently, we obtain a new relation between the rate loss and the number of feedback bits. Interestingly, the feedback rate derived in the distributed CSIT setting is roughly K log2(K) bits larger than its counterpart in the centralized case. This highlights the critical impact of the CSIT distributedness over the performance.\"",
        "1 is \"Scaling Laws and Techniques in Decentralized Processing of Interfered Gaussian Channels\", 2 is \"Transmit power adaptation for multiuser OFDM systems\".",
        "\nGiven above information, for an author who has written the paper with the title \"Multi-cell MIMO cooperative networks: a new look at interference\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0120": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Deciding stability and mortality of piecewise affine dynamical systems':",
        "title: \"Safe Recursion Over an Arbitrary Structure: PAR, PH and DPH\" with abstract: \"Considering the Blum, Shub, and Smale computational model for real numbers, extended by Poizat to general structures, classical complexity can be considered as the restriction to finite structures of a more general notion of computability and complexity working over arbitrary structures.\"",
        "title: \"Reachability Problems For One-Dimensional Piecewise Affine Maps\" with abstract: \"Piecewise affine maps (PAMs) are frequently used as a reference model to discuss the frontier between known and open questions about the decidability for reachability questions. In particular, the reachability problem for one-dimensional PAM is still an open problem, even if restricted to only two intervals. As the main contribution of this paper we introduce new techniques for solving reachability problems based on p-adic norms and weights as well as showing decidability for two classes of maps. Then we show the connections between topological properties for PAM's orbits, reachability problems and representation of numbers in a rational base system. Finally we construct an example where the distribution properties of well studied sequences can be significantly disrupted by taking fractional parts after regular shifts. The study of such sequences could help with understanding similar sequences generated in PAMs or in well known Mahler's 3/2 problem.\"",
        "title: \"Computability over an arbitrary structure: sequential and parallel polynomial time\" with abstract: \"We provide several machine-independent characterizations of deterministic complexity classes in the model of computation proposed by L. Blum, M. Shub and S. Smale. We provide a characterization of partial recursive functions over any arbitrary structure. We show that polynomial time computable functions over any arbitrary structure can be characterized in term of safe recursive functions. We show that polynomial parallel time decision problems over any arbitrary structure can be characterized in terms of safe recursive functions with substitutions.\"",
        "title: \"Real recursive functions and real extensions of recursive functions\" with abstract: \"Recently, functions over the reals that extend elementarily computable functions over the integers have been proved to correspond to the smallest class of real functions containing some basic functions and closed by composition and linear integration. We extend this result to all computable functions: functions over the reals that extend total recursive functions over the integers are proved to correspond to the smallest class of real functions containing some basic functions and closed by composition, linear integration and a very natural unique minimization schema.\"",
        "title: \"A Framework for Algebraic Characterizations in Recursive Analysis.\" with abstract: \"Algebraic characterizations of the computational aspects of functions defined over the real numbers provide very effective tool to understand what computability and complexity over the reals, and generally over continuous spaces, mean. This is relevant for both communities of computer scientists and mathematical analysts, particularly the latter who do not understand (and/or like) the language of machines and string encodings. Recursive analysis can be considered the most standard framework of computation over continuous spaces, it is however defined in a very machine specific way which does not leave much to intuitiveness. Recently several characterizations, in the form of function algebras, of recursively computable functions and some sub-recursive classes were introduced. These characterizations shed light on the hidden behavior of recursive analysis as they convert complex computational operations on sequences of real objects to simple intuitive mathematical operations such as integration or taking limits. The authors previously presented a framework for obtaining algebraic characterizations at the complexity level over compact domains. The current paper presents a comprehensive extension to that framework. Though we focus our attention in this paper on functions defined over the whole real line, the framework, and accordingly the obtained results, can be easily extended to functions defined over arbitrary domains.\"",
        "1 is \"Rapid Object Indexing and Recognition Using Enhanced Geometric Hashing\", 2 is \"Factoring multivariate polynomials via partial differential equations\".",
        "\nGiven above information, for an author who has written the paper with the title \"Deciding stability and mortality of piecewise affine dynamical systems\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0121": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Tree exploration with little memory':",
        "title: \"Epidemic Algorithms and Processes: From Theory to Applications (Dagstuhl Seminar 13042).\" with abstract: \"This report documents the program and the outcomes of Dagstuhl Seminar 13042 \"Epidemic Algorithms and Processes: From Theory to Applications\", which took place from January 20 to 25, 2013 at Schloss Dagstuhl - Leibniz Center for \r\nInformatics. Several research topics were covered by the seminar participants, including scientists working in Theoretical Computer Science, as well as researchers from the more practical area of Computer Systems. Most of the participants presented their recent results on the topic of the seminar, as well as some challenging new directions and open problems. The presentations contained a description of the main research area for a wide audience. During the seminar, ample time was reserved for informal discussions between participants working on different topics. In our executive summary, we describe the main field of the seminar, as well as our goals in general. Then, we present the abstracts of the presentations given during the seminar.\"",
        "title: \"Redundancy in Distributed Proofs.\" with abstract: \"Distributed proofs are mechanisms enabling the nodes of a network to collectivity and efficiently check the correctness of Boolean predicates on the structure of the network, or on data-structures distributed over the nodes (e.g., spanning trees or routing tables). We consider mechanisms consisting of two components: a emph{prover} assigning a emph{certificate} to each node, and a distributed algorithm called emph{verifier} that is in charge of verifying the distributed proof formed by the collection of all certificates.  this paper, we show that many network predicates have distributed proofs offering a high level of redundancy, explicitly or implicitly. We use this remarkable property of distributed proofs for establishing perfect tradeoffs between the emph{size of the certificate} stored at every node, and the emph{number of rounds} of the verification protocol. If we allow every node to communicate to distance at most $t$, one might expect that the certificate sizes can be reduced by a multiplicative factor of at least~$t$. In trees, cycles and grids, we show that such tradeoffs can be established for emph{all} network predicates, i.e., it is always possible to linearly decrease the certificate size. In arbitrary graphs, we show that any part of the certificates common to all nodes can be evenly redistributed among these nodes, achieving even a better tradeoff: this common part of the certificate can be reduced by the size of a smallest ball of radius $t$ in the network.  addition to these general results, we establish several upper and lower bounds on the certificate sizes used for distributed proofs for spanning trees, minimum-weight spanning trees, diameter, additive and multiplicative spanners, and more, improving and generalizing previous results from the literature.\"",
        "title: \"On the searchability of small-world networks with arbitrary underlying structure\" with abstract: \"Revisiting the \"small-world\" experiments of the '60s, Kleinberg observed that individuals are very effective at constructing short chains of acquaintances between any two people, and he proposed a mathematical model of this phenomenon. In this model, individuals are the nodes of a base graph, the square grid, capturing the underlying structure of the social network; and this base graph is augmented with additional edges from each node to a few long-range contacts of this node, chosen according to some natural distance-based distribution. In this augmented graph, a greedy search algorithm takes only a polylogarithmic number of steps in the graph size. Following this work, several papers investigated the correlations between underlying structure and long-range connections that yield efficient decentralized search, generalizing Kleinberg's results to broad classes of underlying structures, such as metrics of bounded doubling dimension, and minor-excluding graphs. We focus on the case of arbitrary base graphs. We show that for a simple long-range contact distribution consistent with empirical observations on social networks, a slight variation of greedy search, where the next hop is to a distant node only if it yields sufficient progress towards the target, requires no(1) steps, where $n$ is the number of nodes. Precisely, the expected number of steps for any source-target pair is at most 2(log n)1/2+o(1). This bound almost matches the best known lower bound of \u03a9(2\u221alog n) steps, which applies to a general class of search algorithms. In the context of social networks, our result could be interpreted as: individuals may well be able to construct short chains between people regardless of the underlying structure of the social network.\"",
        "title: \"Eclecticism shrinks even small worlds\" with abstract: \"We consider small world graphs as defined by Kleinberg (2000), i.e., graphs obtained from a d-dimensional mesh by adding links chosen at random according to the d-harmonic distribution. This model aims at giving formal support to the \"six degrees of separation\" between individuals experienced by Milgram (1967),and verified recently by Dodds, Muhamad, and Watts (2003). In particular, Kleinberg shows that greedy routing performs in O(log2n) expected number of steps in d-dimensional augmented meshes, with O(log2n) bits of topological awareness per node, for any constant d \u2265 1. We show that giving O(log2n) bits of topological awareness per node decreases the expected number of steps of greedy routing to O(log1+1/dn) in d-dimensional augmented meshes. We also show that, independently of the amount of topological awareness given to the nodes, greedy routing performs in \u03a9(log1+1/dn) expected number of steps. In particular, augmenting the topological awareness above this optimum of O(log2n) bits would drastically decrease the performances of greedy routing. Moreover, our model demonstrates that the efficiency of greedy routing is sensible to the \"world's dimension\", in the sense that high dimensional worlds enjoy faster greedy routing than low dimensional ones. This could not be observed in Kleinberg's model. In addition to bringing new light to Milgram's experiment, our protocol presents several desirable properties. In particular, it is totally oblivious i.e., there is no header modification along the path from the source to the target, and the routing decision depends only on the target, and on information stored locally at each node. Finally, our protocol can obviously be used for the design of DHTs, in the same spirit as Symphony (2003).\"",
        "title: \"Survey of Distributed Decision.\" with abstract: \"We survey the recent distributed computing literature on checking whether a given distributed system configuration satisfies a given boolean predicate, i.e., whether the configuration is legal or illegal w.r.t. that predicate. We consider classical distributed computing environments, including mostly synchronous fault-free network computing (LOCAL and CONGEST models), but also asynchronous crash-prone shared-memory computing (WAIT-FREE model), and mobile computing (FSYNC model).\"",
        "1 is \"Models and algorithms for coscheduling compute-intensive taks on a network of workstations\", 2 is \"Trading Space for Time in Undirected $s-t$ Connectivity\".",
        "\nGiven above information, for an author who has written the paper with the title \"Tree exploration with little memory\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0122": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Leakage Current: Moore's Law Meets Static Power':",
        "title: \"High Performance and Energy Efficient Serial Prefetch Architecture\" with abstract: \"Energy efficient architecture research has flourished recently, in an attempt to address packaging and cooling concerns of current microprocessor designs, as well as battery life for mobile computers. Moreover, architects have become increasingly concerned with the complexity of their designs in the face of scalability, verification, and manufacturing concerns.In this paper, we propose and evaluate a high performance, energy and complexity efficient front-end prefetch architecture. This design, called Serial Prefetching, combines a high fetch bandwidth branch prediction and efficient instruction prefetching architecture with a low-energy instruction cache. Serial Prefetching explores the benefit of decoupling the tag component of the cache from the data component. Cache blocks are first verified by the tag component of the cache, and then the accesses are put into a queue to be consumed by the data component of the instruction cache. Energy is saved by only accessing the correct way of the data component specified by the tag lookup in a previous cycle. The tag component does not stall on a I-cache miss, only the data component. The accesses that miss in the tag component are speculatively brought in from lower levels of the memory hierarchy. This in effect performs a prefetch, while the access migrates through the queue to be consumed by the data component.\"",
        "title: \"What input-language is the best choice for high level synthesis (HLS)?\" with abstract: \"As of 2010, over 30 of the world's top semiconductor / systems companies have adopted HLS. In 2009, SOCs tape-outs containing IPs developed using HLS exceeded 50 for the first time. Now that the practicality and value of HLS is established, engineers are turning to the question of \"what input-language works best?\" The answer is critical because it drives key decisions regarding the tool/methodology infrastructure companies will create around this new flow. ANSI-C/C++ advocates cite ease-of-learning, simulation speed. SystemC advocates make similar claims, and point to SystemC's hardware-oriented features. Proponents of BSV (Bluespec SystemVerilog) claim that language enhances architectural transparency and control. To maximize the benefits of HLS, companies must consider many factors and tradeoffs.\"",
        "title: \"Architecting a reliable CMP switch architecture\" with abstract: \"As silicon technologies move into the nanometer regime, transistor reliability is expected to wane as devices become subject to extreme process variation, particle-induced transient errors, and transistor wear-out. Unless these challenges are addressed, computer vendors can expect low yields and short mean-times-to-failure. In this article, we examine the challenges of designing complex computing systems in the presence of transient and permanent faults. We select one small aspect of a typical chip multiprocessor (CMP) system to study in detail, a single CMP router switch. Our goal is to design a BulletProof CMP switch architecture capable of tolerating significant levels of various types of defects. We first assess the vulnerability of the CMP switch to transient faults. To better understand the impact of these faults, we evaluate our CMP switch designs using circuit-level timing on detailed physical layouts. Our infrastructure represents a new level of fidelity in architectural-level fault analysis, as we can accurately track faults as they occur, noting whether they manifest or not, because of masking in the circuits, logic, or architecture. Our experimental results are quite illuminating. We find that transient faults, because of their fleeting nature, are of little concern for our CMP switch, even within large switch fabrics with fast clocks. Next, we develop a unified model of permanent faults, based on the time-tested bathtub curve. Using this convenient abstraction, we analyze the reliability versus area tradeoff across a wide spectrum of CMP switch designs, ranging from unprotected designs to fully protected designs with on-line repair and recovery capabilities. Protection is considered at multiple levels from the entire system down through arbitrary partitions of the design. We find that designs are attainable that can tolerate a larger number of defects with less overhead than na\u00efve triple-modular redundancy, using domain-specific techniques, such as end-to-end error detection, resource sparing, automatic circuit decomposition, and iterative diagnosis and reconfiguration.\"",
        "title: \"MEVBench: A mobile computer vision benchmarking suite\" with abstract: \"The growth in mobile vision applications, coupled with the performance limitations of mobile platforms, has led to a growing need to understand computer vision applications. Computationally intensive mobile vision applications, such as augmented reality or object recognition, place significant performance and power demands on existing embedded platforms, often leading to degraded application quality. With a better understanding of this growing application space, it will be possible to more effectively optimize future embedded platforms. In this work, we introduce and evaluate a custom benchmark suite for mobile embedded vision applications named MEVBench. MEVBench provides a wide range of mobile vision applications such as face detection, feature classification, object tracking and feature extraction. To better understand mobile vision processing characteristics at the architectural level, we analyze single and multithread implementations of many algorithms to evaluate performance, scalability, and memory characteristics. We provide insights into the major areas where architecture can improve the performance of these applications in embedded systems.\"",
        "title: \"Scalable hybrid verification of complex microprocessors\" with abstract: \"We introduce a new verification methodology for modern micro-processors that uses a simple checker processor to validate the exe-cution of a companion high-performance processor. The checker can be viewed as an at-speed emulator that is formally verified to be compliant to an ISA specification. This verification approach en-ables the practical deployment of formal methods without impact-ing overall performance.\"",
        "1 is \"Using data compression in an MPSoC architecture for improving performance\", 2 is \"Trace scheduling: a technique for global microcode compaction\".",
        "\nGiven above information, for an author who has written the paper with the title \"Leakage Current: Moore's Law Meets Static Power\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0123": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'From Fingerprint to Footprint: Revealing Physical World Privacy Leakage by Cyberspace Cookie Logs.':",
        "title: \"Fundamental trade-offs in aggregate packet scheduling\" with abstract: \"We investigate the fundamental trade-offs in aggregate packet scheduling for the support of guaranteed delay service. Besides the simple FIFO packet scheduling algorithm, we consider two new classes of aggregate packet scheduling algorithms: the static earliest time first (SETF) and dynamic earliest time first (DETF). Through these two classes of aggregate packet scheduling, we show that, with additional time stamp information encoded in the packet header for scheduling purpose, we can significantly increase the maximum allowable network utilization level, while at the same time reducing the worst-case edge-to-edge delay bound. Furthermore, we demonstrate how the number of the bits used to encode the time stamp information affects the trade-off between the maximum allowable network utilization level and the worst-case edge-to-edge delay bound. In addition, the more complex DETF algorithms have far better performance than the simpler SETF algorithms. These results illustrate the fundamental trade-offs in aggregate packet scheduling algorithms and shed light on their provisioning power in support of guaranteed delay service.\"",
        "title: \"Coverage-Aware Proxy Placement for Dynamic Content Management over the Internet\" with abstract: \"In an effort to differentiate service quality, service providers have resorted to employing Content Distribution Networks (CDNs) over the Internet CDNs deploy geographically distributed proxy servers which manage content on behalf of the service provider's servers for better performance and enhanced availability. In this paper we explore the proxy placement problem for content distribution over the Internet. Its goal is to strategically place a number of proxies in the network to optimize certain criteria which improve performance of proxies. We motivate the various necessary factors and constraints that need to be taken into account for a good placement of proxies over the Internet which reflect real world scenario more accurately and which we claim hitherto has not been completely addressed. We introduce a novel concept of host coverage characterizing every Autonomous Systems (AS) and use this stable, coarse grained measure as a long-term estimate of the load being serviced by the proxy system. We then pose an optimal formulation of the proxy placement problem taking into consideration all the relevant factors. We propose a couple of proxy placement algorithms that solve the above problem and analyze their behavior. Finally we present the performance of those algorithms against the optimal solution and other schemes proposed in literature. We also study the stability of the proposed algorithms through a variety of experiments.\"",
        "title: \"MTBF: an efficient multicast group aggregation scheme for the global area multicast\" with abstract: \"IP Multicast is an important enabling service for the current and future Internet. With the explosive, growth of the Internet, a challenging issue facing IP multicast is scalability, in particular, the problem of multicast forwarding state and control explosion. In this paper, we propose a new methodology to address the multicast scalability problem for backbone domains Multicast Tunneling with Branch Filtering (MTBF). This multicast group aggregation scheme is designed on top of the inter-domain protocol architecture such as MASC/BGMP, and is independent of any underlying intra-domain multicast protocols. It aggregates multicast groups by constructing Border Router (BR)-based multicast routing trees and forwards data by using an encapsulation technique called Multicast Tunneling (MT). To minimize excess traffic due to aggregate multicast address based data forwarding, an efficient Dynamic Filtering Point Selection (DFPS) algorithm is used. The feasibility and performance of our scheme is demonstrated through analysis and simulations\"",
        "title: \"Protocol independent multicast group aggregation scheme for the global area multicast\" with abstract: \"IP multicast is an important enabling service for the current and future Internet. With the explosive growth of the Internet, a challenging issue facing IP multicast is scalability, in particular, the problem of multicast forwarding state and control explosion. In this paper, we propose a new methodology to address the multicast scalability problem for backbone domains-multicast tunneling with branch filtering (MTBF). This multicast group aggregation scheme is designed on top of the inter-domain protocol architecture such as MASC/BGMF, and is independent of any underlying intra-domain multicast protocols. It aggregates multicast groups by constructing bolder router (BR)-based multicast routing trees and forwards data by using an encapsulation technique called multicast tunneling (MT). The feasibility and performance of our scheme is demonstrated through analysis and simulations\"",
        "title: \"Profiling users in a 3g network using hourglass co-clustering\" with abstract: \"With widespread popularity of smart phones, more and more users are accessing the Internet on the go. Understanding mobile user browsing behavior is of great significance for several reasons. For example, it can help cellular (data) service providers (CSPs) to improve service performance, thus increasing user satisfaction. It can also provide valuable insights about how to enhance mobile user experience by providing dynamic content personalization and recommendation, or location-aware services. In this paper, we try to understand mobile user browsing behavior by investigating whether there exists distinct \"behavior patterns\" among mobile users. Our study is based on real mobile network data collected from a large 3G CSP in North America. We formulate this user behavior profiling problem as a \"co-clustering\" problem, i.e., we group both users (who share similar browsing behavior), and browsing profiles (of like-minded users) simultaneously. We propose and develop a scalable co-clustering methodology, Phantom, using a novel hourglass model. The proposed hourglass model first reduces the dimensions of the input data and performs divisive hierarchical co-clustering on the lower dimensional data; it then carries out an expansion step that restores the original dimensions. Applying Phantom to the mobile network data, we find that there exists a number of prevalent and distinct behavior patterns that persist over time, suggesting that user browsing behavior in 3G cellular networks can be captured using a small number of co-clusters. For instance, behavior of most users can be classified as either homogeneous (users with very limited set of browsing interests) or heterogeneous (users with very diverse browsing interests), and such behavior profiles do not change significantly at either short (30-min) or long (6 hour) time scales.\"",
        "1 is \"Conflict classification and analysis of distributed firewall policies\", 2 is \"A policy-aware switching layer for data centers\".",
        "\nGiven above information, for an author who has written the paper with the title \"From Fingerprint to Footprint: Revealing Physical World Privacy Leakage by Cyberspace Cookie Logs.\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0124": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Characteristics of YouTube network traffic at a campus network - Measurements, models, and implications':",
        "title: \"A visual progranmming approach to manufacturing modeling\" with abstract: \"The RESearch Queueing Package Modeling Environment (RESQME) is an interactive, graphics-oriented workstation environment for iteratively constructing, running and analyzing models of resource contention systems. It is built on top of the RESearch Queueing Package (RESQ) which provides the functionality to evaluate extended queueing networks. In this paper we describe how to create a \u201cpull\u201d system, a manufacturing line with finite buffers, using the graphical interface and hierarchical modeling capability. This serves as an example of the kind of manufacturing submodels that can be created in this system and then used for higher-level modeling.\"",
        "title: \"On managing quality of experience of multiple video streams in wireless networks.\" with abstract: \"Managing the Quality-of-Experience (QoE) of video streaming for wireless clients is becoming increasingly important due to the rapid growth of video traffic on wireless networks. The inherent variability of the wireless channel as well as the Variable Bit Rate (VBR) of the compressed video streams make QoE management a challenging problem. Prior work has studied this problem in the context of transmitting a single video stream. In this paper, we investigate multiplexing schemes to transmit multiple video streams from a base station to mobile clients that use number of playout stalls as a performance metric.In this context, we present an epoch-by-epoch framework to fairly allocate wireless transmission slots to streaming videos. In each epoch our scheme essentially reduces the vulnerability to stalling by allocating slots to videos in a way that maximizes the minimum 'playout lead' across all videos. Next, we show that the problem of allocating slots fairly is NP-complete even for a constant number of videos. We then present a fast lead-aware greedy algorithm for the problem. Our choice of greedy algorithm is motivated by the fact that this algorithm is optimal when the channel quality of a user remains unchanged within an epoch (but different users may experience different channel quality). Moreover, our experimental results based on public MPEG-4 video traces and wireless channel traces that we collected from a WiMAX test-bed show that the lead-aware greedy approach performs a fair distribution of stalls across the clients when compared to other algorithms, while still maintaining similar or lower average number of stalls per client.\"",
        "title: \"A study of proactive hybrid FEC/ARQ and scalable feedback techniques for reliable, real-time multicast\" with abstract: \"In this paper, we examine the performance benefits of using parity encoding for reliable delivery of data to multiple receivers, where the receivers have heterogeneous loss and delay characteristics. First, we examine the effect of sending parity loss repairs proactively to receivers before it is known whether or not the repairs are needed to repair losses. We show that in many cases, proactive repair can reduce feedback implosion and the expected delay of reliable delivery without increasing bandwidth usage. Next, we examine how proactive repairs can be used to meet end-to-end deadlines to within a tunable probability. Last, we examine several receiver feedback mechanisms for parity repairs in a heterogeneous networking environment, and find that the various mechanisms offer different levels of robustness. Our examinations take into account temporally correlated (bursty) loss, as well as loss of feedback messages from receivers.\"",
        "title: \"The effectiveness of affinity-based scheduling in multiprocessor network protocol processing (extended version)\" with abstract: \"Techtdques for avoiding the high memory overheads found on many modern shared-memory mtdtiprocessors are of increasing importance in the development of high-performance mtdtiprocessor protocol implementations. One such technique is processor-cache aflirdty schedtding, which can significantly lower packet latency and substantially increase protocol processing throughput (30). In tlds paper, we evaluate severtd aspects of the effectiveness of affinity-based scheduling in mtdtiproeessor network protocol processing, under packet-level and connection- level parallelization approaches Speeifieally, we evaluate the performance of the scheduling technique 1) when a large number of streams are concurrently supported, 2) when processing in- cludes copying of uncached packet data, 3) as applied to send-side protocol processing, and 4) in the presence of stream burstiness and source locality, two well-known properties of network t-c. We find that ~ty-based schedtding performs well under these conditions, emphasizing its rubustneatsand general effectiveness in multiprocessor network processing. In addition, we explore a technique which improves the cachhsg behavior and available packet-level concurrency under connection-level parallelism, and find performance improves dramatically.\"",
        "title: \"Online smoothing of variable-bit-rate streaming video\" with abstract: \"Bandwidth smoothing techniques for stored video perform end to end workahead transmission of frames into the client playback buffer, in advance of their display times. Such techniques are very effective in reducing the burstiness of the bandwidth requirements for transmitting compressed, stored video. This paper addresses online bandwidth smoothing for a growing number of streaming video applications such as newscasts, sportscasts, and distance learning, where many clients may be willing to tolerate a playback delay of a few seconds in exchange for a smaller bandwidth requirement. The smoothing can be performed at either the source of the videocast or at special smoothing server(s) (e.g., proxies or gateways) within the network. In contrast to previous work on stored video, the online smoothing server has limited knowledge of frame sizes and access to only a segment of the video at a time. This is either because the feed is live or because it is streaming past the server. We formulate an online smoothing model which incorporates playback delay, client and server buffer sizes, server processing capacity, and frame size prediction techniques. Our model can accommodate an arbitrary arrival process. Using techniques for smoothing stored video at the source as a starting point, we develop an online, window-based smoothing algorithm for delay tolerant applications. Extensive experiments with MPEG-1 and M-JPEG video traces demonstrate that online smoothing significantly reduces the peak rate, coefficient of variation, and effective bandwidth of variable-bit-rate video streams. These reductions can be achieved with modest playback delays of a few seconds to a few tens of seconds and moderate client buffer sizes, and closely approximate the performance of optimal offline smoothing of stored video. In addition, we show that frame size prediction can offer further reduction in resource requirements, though prediction becomes relatively less important for longer playback delays. However, the ability to predict future frame sizes affects the appropriate division of buffer space between the server and client sites. Our experiments show that the optimal buffer allocation shifts to placing more memory at the server as the server has progressively less information about future frame sizes\"",
        "1 is \"An end-to-end adaptation protocol for layered video multicast using optimal rate allocation\", 2 is \"Random early detection gateways for congestion avoidance\".",
        "\nGiven above information, for an author who has written the paper with the title \"Characteristics of YouTube network traffic at a campus network - Measurements, models, and implications\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0125": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Improving Random Walk Estimation Accuracy with Uniform Restarts':",
        "title: \"Analysis and design of controllers for AQM routers supporting TCP flows\" with abstract: \"In active queue management (AQM), core routers signal transmission control protocol (TCP) sources with the ob- jective of managing queue utilization and delay. It is essentially a feedback control problem. Based on a recently developed dynamic model of TCPs congestion-avoidance mode, this paper does three things. First, it relates key network parameters such as the number of TCP sessions, link capacity and round-trip time to the under- lying feedback control problem. Second, it analyzes the present de facto AQM standard: random early detection (RED) and de- termines that REDs queue-averaging is not beneficial. Finally, it recommends alternative AQM schemes which amount to classical proportional and proportional-integral control. We illustrate our results using ns simulations and demonstrate the practical impact of proportional-integral control on managing queue utilization and delay.\"",
        "title: \"Optimal scheduling policies for a class of queues with customer deadlines to the beginning of service\" with abstract: \"Many problems can be modeled as single-server queues with impatient customers. An example is that of the transmission of voice packets over a packet-switched network. If the voice packets do not reach their destination within a certain time interval of their transmission, they are useless to the receiver and considered lost. It is therefore desirable to schedule the customers such that the fraction of customers served within their respective deadlines is maximized. For this measure of performance, it is shown that the shortest time to extinction (STE) policy is optimal for a class of continuous and discrete time nonpreemptive M/G/1 queues that do not allow unforced idle times. When unforced idle times are allowed, the best policies belong to the class of shortest time to extinction with inserted idle time (STEI) policies. An STEI policy requires that the customer closest to his or her deadline be scheduled whenever it schedules a customer. It also has the choice of inserting idle times while the queue is nonempty. It is also shown that the STE policy is optimal for the discrete time G/D/1 queue where all customers receive one unit of service. The paper concludes with a comparison of the expected customer loss using an STE policy with that of the first-come, first-served (FCFS) scheduling policy for one specific queue.\"",
        "title: \"Joint Data Compression and Caching: Approaching Optimality with Guarantees.\" with abstract: \"We consider the problem of optimally compressing and caching data across a communication network. Given the data generated at edge nodes and a routing path, our goal is to determine the optimal data compression ratios and caching decisions across the network in order to minimize average latency, which can be shown to be equivalent to maximizing the compression and caching gain  under an energy consumption constraint. We show that this problem is NP-hard in general and the hardness is caused by the caching decision subproblem, while the compression sub-problem is polynomial-time solvable. We then propose an approximation algorithm that achieves a $(1-1/e)$-approximation solution to the optimum in strongly polynomial time. We show that our proposed algorithm achieve the near-optimal performance in synthetic-based evaluations. In this paper, we consider a tree-structured network as an illustrative example, but our results easily extend to general network topology at the expense of more complicated notations.\n\n\"",
        "title: \"Minfer: A Method Of Inferring Motif Statistics From Sampled Edges\" with abstract: \"Characterizing motif (i.e., locally connected subgraph patterns) statistics is important for understanding complex networks such as online social networks and communication networks. Previous work made the strong assumption that the graph topology of interest is known in advance. In practice, sometimes researchers have to deal with the situation where the graph topology is unknown because it is expensive to collect and store all topological and meta information. Hence, typically what is available to researchers is only a snapshot of the graph, i.e., a subgraph of the graph. Crawling methods such as breadth first sampling can be used to generate the snapshot. However, these methods fail to sample a streaming graph represented as a high speed stream of edges. Therefore, graph mining applications such as network traffic monitoring use random edge sampling (i.e., sample each edge with a fixed probability) to collect edges and generate a sampled graph, which we called a \" RE Sampled graph \". Clearly, a RESampled graph's motif statistics may be quite different from those of the underlying original graph. To resolve this, we propose a framework and implement a system called Minfer, which takes the given RESampled graph and accurately infers the underlying graph's motif statistics. We also apply Fisher information to bound the errors of our estimates. Experiments using large scale datasets show the accuracy and efficiency of our method.\"",
        "title: \"Covert communications on Poisson packet channels.\" with abstract: \"Consider a channel where authorized transmitter Jack sends packets to authorized receiver Steve according to a Poisson process with rate lambda packets per second for a time period T. Suppose that covert transmitter Alice wishes to communicate information to covert receiver Bob on the same channel without being detected by a watchful adversary Willie. We consider two scenarios. In the first scenario, we assume that warden Willie cannot look at packet contents but rather can only observe packet timings, and Alice must send information by inserting her own packets into the channel. We show that the number of packets that Alice can covertly transmit to Bob is on the order of the square root of the number of packets that Jack transmits to Steve; conversely, if Alice transmits more than that, she will be detected by Willie with high probability. In the second scenario, we assume that Willie can look at packet contents but that Alice can communicate across an M / M / 1 queue to Bob by altering the timings of the packets going from Jack to Steve. First, Alice builds a codebook, with each codeword consisting of a sequence of packet timings to be employed for conveying the information associated with that codeword. However, to successfully employ this codebook, Alice must always have a packet to send at the appropriate time. Hence, leveraging our result from the first scenario, we propose a construction where Alice covertly slows down the packet stream so as to buffer packets to use during a succeeding codeword transmission phase. Using this approach, Alice can covertly and reliably transmit O (lambda T) covert bits to Bob in time period T over an M / M / 1 queue with service rate mu > e.lambda.\"",
        "1 is \"Service differentiation and guarantees for TCP-based elastic traffic\", 2 is \"Trade-offs in optimizing the cache deployments of CDNs\".",
        "\nGiven above information, for an author who has written the paper with the title \"Improving Random Walk Estimation Accuracy with Uniform Restarts\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0126": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'An Economically-Principled Generative Model of AS Graph Connectivity':",
        "title: \"Self-correcting sampling-based dynamic multi-unit auctions\" with abstract: \"We exploit methods of sample-based stochastic optimization for the purpose of strategyproof dynamic, multi-unit auctions. There are no analytic characterizations of optimal policies for this domain and thus a heuristic approach, such as that proposed here, seems necessary in practice. Following the suggestion of Parkes and Duong [17], we perform sensitivity analysis on the allocation decisions of an online algorithm for stochastic optimization, and correct the decisions to enable a strategyproof auction. In applying this approach to the allocation of non-expiring goods, the technical problem that we must address is related to achieving strategyproofness for reports of departure. This cannot be achieved through self-correction without canceling many allocation decisions, and must instead be achieved by first modifying the underlying algorithm. We introduce the NowWait method for this purpose, prove its successful interfacing with sensitivity analysis and demonstrate good empirical performance. Our method is quite general, requiring a technical property of uncertainty independence, and that values are not too positively correlated with agent patience. We also show how to incorporate \"virtual valuations\" in order to increase the seller's revenue.\"",
        "title: \"Expressive banner ad auctions and model-based online optimization for clearing\" with abstract: \"We present the design of a banner advertising auction which is considerably more expressive than current designs. We describe a general model of expressive ad contract/bidding and an allocation model that can be executed in real time through the assignment of fractions of relevant ad channels to specific advertiser contracts. The uncertainty in channel supply and demand is addresscd by the formulation of a stochastic combinatorial optimization problem for channel allocation that is rerun periodically. We solve this in two different ways: fast deterministic optimization with respect to expectations; and a novel online sample-based stochastic optimization method-- that can be applied to continuous decision spaces--which exploits the deterministic optimization as a black box. Experiments demonstrate the importance of expressive bidding and the value of stochastic optimization.\"",
        "title: \"Hybrid transitive trust mechanisms\" with abstract: \"Establishing trust amongst agents is of central importance to the development of well-functioning multi-agent systems. For example, the anonymity of transactions on the Internet can lead to inefficiencies; e.g., a seller on eBay failing to ship a good as promised, or a user free-riding on a file-sharing network. Trust (or reputation) mechanisms can help by aggregating and sharing trust information between agents. Unfortunately these mechanisms can often be manipulated by strategic agents. Existing mechanisms are either very robust to manipulation (i.e., manipulations are not beneficial for strategic agents), or they are very informative (i.e., good at aggregating trust data), but never both. This paper explores this trade-off between these competing desiderata. First, we introduce a metric to evaluate the informativeness of existing trust mechanisms. We then show analytically that trust mechanisms can be combined to generate new hybrid mechanisms with intermediate robustness properties. We establish through simulation that hybrid mechanisms can achieve higher overall efficiency in environments with risky transactions and mixtures of agent types (some cooperative, some malicious, and some strategic) than any previously known mechanism.\"",
        "title: \"Why markets could (but don't currently) solve resource allocation problems in systems\" with abstract: \"Using market mechanisms for resource allocation in distributed systems is not a new idea, nor is it one that has caught on in practice or with a large body of computer science research. Yet, projects that use markets for distributed resource allocation recur every few years [1, 2, 3], and a new generation of research is exploring market-based resource allocation mechanisms [4, 5, 6, 7, 8] for distributed environments such as Planetlab, Netbed, and computational grids. This paper has three goals. The first goal is to explore why markets can be appropriate to use for allocation, when simpler allocation mechanisms exist. The second goal is to demonstrate why a new look at markets for allocation could be timely, and not a re-hash of previous research. The third goal is to point out some of the thorny problems inherent in market deployment and to suggest action items both for market designers and for the greater research community. We are optimistic about the power of market design, but we also believe that key challenges exist for a markets/systems integration that must be overcome for market-based computer resource allocation systems to succeed.\"",
        "title: \"Online mechanism design for electric vehicle charging\" with abstract: \"Plug-in hybrid electric vehicles are expected to place a considerable strain on local electricity distribution networks, requiring charging to be coordinated in order to accommodate capacity constraints. We design a novel online auction protocol for this problem, wherein vehicle owners use agents to bid for power and also state time windows in which a vehicle is available for charging. This is a multi-dimensional mechanism design domain, with owners having non-increasing marginal valuations for each subsequent unit of electricity. In our design, we couple a greedy allocation algorithm with the occasional \"burning\" of allocated power, leaving it unallocated, in order to adjust an allocation and achieve monotonicity and thus truthfulness. We consider two variations: burning at each time step or on-departure. Both mechanisms are evaluated in depth, using data from a real-world trial of electric vehicles in the UK to simulate system dynamics and valuations. The mechanisms provide higher allocative efficiency than a fixed price system, are almost competitive with a standard scheduling heuristic which assumes non-strategic agents, and can sustain a substantially larger number of vehicles at the same per-owner fuel cost saving than a simple random scheme.\"",
        "1 is \"Solving transition independent decentralized Markov decision processes\", 2 is \"In search of database consistency\".",
        "\nGiven above information, for an author who has written the paper with the title \"An Economically-Principled Generative Model of AS Graph Connectivity\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0127": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Converging to approximated max-min flow fairness in logarithmic time':",
        "title: \"Multipath routing slice experiments in federated testbeds\" with abstract: \"The Internet today consist of many heterogeneous infrastructures, owned and maintained by separate and potentially competing administrative authorities. On top of this a wide variety of applications has different requirements with regard to quality, reliability and security from the underlying networks. The number of stakeholders who participate in provisioning of network and services is growing. More demanding applications (like eGovernment, eHealth, critical and emergency infrastructures) are on the rise. Therefore we assume that these two basic characteristics, a) multiple authorities and b) applications with very diverse demands, are likely to stay or even increase in the Internet of the future. In such an environment federation and virtualization of resources are key features that should be supported in a future Internet. The ability to form slices across domains that meet application specific requirements enables many of the desired features in future networks. In this paper, we present a Multipath Routing Slice experiment that we performed over multiple federated testbeds. We combined capabilities from different experimental facilities, since one single testbed did not offer all the required capabilities. This paper summarizes the conducted experiment, our experience with the usability of federated testbeds and our experience with the use of advanced measurement technologies within experimental facilities. We believe that this experiment provides a good example use case for the future Internet itself because we assume that the Internet will consist of multiple different infrastructures that have to be combined in application specific overlays or routing slices, very much like the experimental facilities we used in this experiment. We also assume that the growing demands will push towards a much better measurement instrumentation of the future Internet. The tools used in our experiment can provide a starting point for this.\"",
        "title: \"Topological trends of internet content providers\" with abstract: \"The Internet is constantly changing, and its hierarchy was recently shown to become flatter. Recent studies of inter-domain traffic showed that large content providers drive this change by bypassing tier-1 networks and reaching closer to their users, enabling them to save transit costs and reduce reliance of transit networks as new services are being deployed, and traffic shaping is becoming increasingly popular. In this paper we take a first look at the evolving connectivity of large content provider networks, from a topological point of view of the autonomous systems (AS) graph. We perform a 5-year longitudinal study of the topological trends of large content providers, by analyzing several large content providers and comparing these trends to those observed for large tier-1 networks. We study trends in the connectivity of the networks, neighbor diversity and geographical spread, their hierarchy, the adoption of IXPs as a convenient method for peering, and their centrality. Our observations indicate that content providers gradually increase and diversify their connectivity, enabling them to improve their centrality in the Internet, while tier-1 networks lose dominance over time.\"",
        "title: \"A practical revocation scheme for broadcast encryption using smartcards\" with abstract: \"We present an anti-pirate revocation scheme for broadcast encryption systems (e.g., pay TV), in which the data is encrypted to ensure payment by users. In the systems we consider, decryption of keys is done on smartcards and key management is done in-band. Our starting point is a scheme of Naor and Pinkas. Their basic scheme uses secret sharing to remove up to t parties, is information-theoretic secure against coalitions of size t, and is capable of creating a new group key. However, with current smartcard technology, this scheme is only feasible for small system parameters, allowing up to about 100 pirates to be revoked before all the smartcards need to be replaced. We first present a novel implementation method of their basic scheme that distributes the work among the smartcard, set-top terminal, and center. Based on this, we construct several improved schemes for many revocation rounds that scale to realistic system sizes. We allow up to about 10,000 pirates to be revoked using current smartcard technology before recarding is needed. The transmission lengths of our constructions are on par with those of the best tree-based schemes. However, our constructions have much lower smartcard CPU complexity: only O(1) smartcard operations per revocation round (a single 10-byte field multiplication and addition), as opposed to the complexity of the best tree-based schemes, which is polylogarithmic in the number of users. We evaluate the system behavior via an exhaustive simulation study coupled with a queueing theory analysis. Our simulations show that with mild assumptions on the piracy discovery rate, our constructions can perform effective pirate revocation for realistic broadcast encryption scenarios.\"",
        "title: \"Topology aggregation for directed graphs\" with abstract: \"This paper addresses the problem of aggregating the topology of a sub-network in a compact way with minimum distortion. The problem arises from networks that have a hierarchical structure, where each sub-network must advertise the cost of routing between each pair of its border nodes. The straight-forward solution of advertising the exact cost for each pair has a quadratic cost which is not practical. We look at the realistic scenario of networks where all links are bidirectional, but their cost (or distance) in the opposite directions might differ significantly. The paper presents a solution with distortion that is bounded by the logarithm of the number of border nodes and the square-root of the asymmetry in the cost of a link. This is the first time that a theoretical bound is given to an undirected graph. We show how to apply our solution to PNNI, and suggest some other heuristics that are tested to perform better than the provenly bounded solution\"",
        "title: \"Approximation and heuristic algorithms for minimum-delay application-layer multicast trees\" with abstract: \"In this paper we investigate the problem of finding minimum-delay application-layer multicast trees, such as the trees constructed in overlay networks. It is accepted that shortest path trees are not a good solution for the problem since such trees can have nodes with very large degree, termed high-load nodes. The load on these nodes makes them a bottleneck in the distribution tree, due to computation load and access link bandwidth constraints. Many previous solutions limited the maximum degree of the nodes by introducing arbitrary constraints. In this work, we show how to directly map the node load to the delay penalty at the application host, and create a new model that captures the trade offs between the desire to select shortest path trees and the need to constrain the load on the hosts. In this model the problem is shown to be NP-hard. We therefore present an approximation algorithm and an alternative heuristic algorithm. Our heuristic algorithm is shown by simulations to be scalable for large group sizes, and produces results that are very close to optimal.\"",
        "1 is \"A One-Pass Algorithm for Accurately Estimating Quantiles for Disk-Resident Data\", 2 is \"Greedy dynamic routing on arrays\".",
        "\nGiven above information, for an author who has written the paper with the title \"Converging to approximated max-min flow fairness in logarithmic time\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0128": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Computing the success factors in consistent acquisition and recognition of objects in color digital images by explicit preconditioning':",
        "title: \"Extraction of mid-level semantics from gesture videos using a Bayesian network\" with abstract: \"A method for extraction of mid-level semantics from sign language videos is proposed, by employing high level domain knowledge. The semantics concern labelling of the depicted head and hands as well as the occlusion events, which are essential for interpretation and for higher level semantic indexing. A Bayesian network is employed to bridge in a probabilistic fashion the gap between the high level knowledge about the valid spatiotemporal configurations of the human body and the extractor. The approach is applied here in sign language videos, but it can be generalised to any case, where semantically rich information can be derived from gesture.\"",
        "title: \"GOrevenge: A Novel Generic Reverse Engineering Method for the Identification of Critical Molecular Players, Through the Use of Ontologies\" with abstract: \"The ever-increasing use of ontologies in modern biological analysis and interpretation facilitates the understanding of the cellular procedures, their hierarchical organization, and their potential interactions at a system's level. Currently, the gene ontology serves as a paradigm, where through the annotation of whole genomes of certain organisms, genes subsets selected, either from high-throughput experiments or with an established pivotal role regarding the probed disease, can act as a starting point for the exploration of their underlying functional interconnections. This may also aid the elucidation of hidden regulatory mechanisms among genes. Reverse engineering the functional relevance of genes to specific cellular pathways and vice versa, through the exploitation of the inner structure of the ontological vocabularies, may help impart insight regarding the identification and prioritization of the critical role of specific genes. The proposed graph-theoretical method is showcased in a pancreatic cancer and a T-cell acute lymphoblastic leukemia gene set, incorporating edge and Resnik semantic similarity metrics, and systematically evaluated regarding its performance.\"",
        "title: \"Inference Of A Robust Diagnostic Signature In The Case Of Melanoma: Gene Selection By Information Gain And Gene Ontology Tree Exploration\" with abstract: \"Integrated datasets originating from multi-modal data can be used towards the identification of causal biological actions that through a systems level process trigger the development of a disease. We use, here, an integrated dataset related to cutaneous melanoma that comes from two separate sets (microarray and imaging) and the application of data imputation methods. Our goal is to select a subset of genes that comprise candidate biomarkers and compare these to imaging features, that characterize disease at a macroscopic level. Using information gain ratio measurements and exploration of Gene Ontology (GO) tree, we identified a set of 33 genes both highly correlated to the disease status and with a central role in regulatory mechanisms. Selected genes were used to train various classifiers that could generalize well when discriminating malignant from benign melanoma samples. Results showed that classifiers performed better when selected genes were used as input, rather than imaging features selected by information gain measurements. Thus, genes in the backstage of low-level biological processes showed to carry higher information content than the macroscopic imaging features.\"",
        "title: \"On-Line Fall Detection Via Mobile Accelerometer Data\" with abstract: \"Mobile devices have entered our daily life in several forms, such as tablets, smartphones, smartwatches and wearable devices, in general. The majority of those devices have built-in several motion sensors, such as accelerometers, gyroscopes, orientation and rotation sensors. The activity recognition or emergency event detection in cases of falls or abnormal activity conduce a challenging task, especially for elder people living independently in their homes. In this work, we present a methodology capable of performing real time fall detect, using data from a mobile accelerometer sensor. To this end, data taken from the 3-axis accelerometer is transformed using the Incremental Principal Components Analysis methodology. Next, we utilize the cumulative sum algorithm, which is capable of detecting changes using devices having limited CPU power and memory resources. Our experimental results are promising and indicate that using the proposed methodology, real time fall detection is feasible.\"",
        "title: \"Electronic Roads in Historical Documents: A Student Oriented Approach\" with abstract: \"The new educational approaches, as far as teaching of history in secondary education is concerned, are characterized by a shift away from sterile memorization and towards a critical approach of historical facts and phenomena; the aim is to contribute to both the development of students' historical concept and conscience and the promotion of critical thought. These teaching goals are pursued by promoting initiative of the students through self study assignments in the form of projects; students can be greatly supported in such tasks by computer-based applications, which can offer access to vast amounts of historical texts and data to be used next to the main scholar textbook and be analyzed by students. Still, existing applications seem to be quite inadequate for this purpose, as they require that the student be already informed on a matter, before the initiation of a quest for data. In this paper, we describe an intelligent information system that is designed to facilitate browsing of educational material and historical sources, thus allowing students to efficiently retrieve information on topics that are not yet known to them and expand in this way their historical knowledge. This can help in fulfilling the aforementioned teaching goals. Our system relies on the notion of the Electronic Road.\"",
        "1 is \"Automated semantic web service discovery with OWLS-MX\", 2 is \"Model-based articulated hand motion tracking for gesture recognition\".",
        "\nGiven above information, for an author who has written the paper with the title \"Computing the success factors in consistent acquisition and recognition of objects in color digital images by explicit preconditioning\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0129": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'A General Framework for Hardware Trojan Detection in Digital Circuits by Statistical Learning Algorithms.':",
        "title: \"ESE: Efficient Speech Recognition Engine with Sparse LSTM on FPGA.\" with abstract: \"Long Short-Term Memory (LSTM) is widely used in speech recognition. In order to achieve higher prediction accuracy, machine learning scientists have built increasingly larger models. Such large model is both computation intensive and memory intensive. Deploying such bulky model results in high power consumption and leads to a high total cost of ownership (TCO) of a data center. To speedup the prediction and make it energy efficient, we first propose a load-balance-aware pruning method that can compress the LSTM model size by 20x (10x from pruning and 2x from quantization) with negligible loss of the prediction accuracy. The pruned model is friendly for parallel processing. Next, we propose a scheduler that encodes and partitions the compressed model to multiple PEs for parallelism and schedule the complicated LSTM data flow. Finally, we design the hardware architecture, named Efficient Speech Recognition Engine (ESE) that works directly on the sparse LSTM model. Implemented on Xilinx KU060 FPGA running at 200MHz, ESE has a performance of 282 GOPS working directly on the sparse LSTM network, corresponding to 2.52 TOPS on the dense one, and processes a full LSTM for speech recognition with a power dissipation of 41 Watts. Evaluated on the LSTM for speech recognition benchmark, ESE is 43x and 3x faster than Core i7 5930k CPU and Pascal Titan X GPU implementations. It achieves 40x and 11.5x higher energy efficiency compared with the CPU and GPU respectively.\"",
        "title: \"Improving energy efficiency of write-asymmetric memories by log style write\" with abstract: \"The significant scaling challenges of conventional memories, i.e., SRAM and DRAM, motivated the research on emerging memory technologies. Many promising memory technology candidates, however, suffer from a common issue in their write operations: the switching processes at different write operations (i.e., 0 \u2192 1 and 1 \u2192 0) are asymmetric. Using a pessimistic design corner to cover the worst case of a write operation incurs large power and performance cost in the existing emerging memory technology designs. In this work, we propose a universal log style write methodology to mitigate this asymmetry issue by operating two switching processes in separate stages. The dedicated design optimizations are allowed on either switching process. The simulation results on the spin-transfer-torque random access memory based last-level cache show that our technique can improve the system performance by 4% while receiving 35% power reduction on average1.\"",
        "title: \"Training itself: Mixed-signal training acceleration for memristor-based neural network\" with abstract: \"The artificial neural network (ANN) is among the most widely used methods in data processing applications. The memristor-based neural network further demonstrates a power efficient hardware realization of ANN. Training phase is the critical operation of memristor-based neural network. However, the traditional training method for memristor-based neural network is time consuming and energy inefficient. Users have to first work out the parameters of memristors through digital computing systems and then tune the memristor to the corresponding state. In this work, we introduce a mixed-signal training acceleration framework, which realizes the self-training of memristor-based neural network. We first modify the original stochastic gradient descent algorithm by approximating calculations and designing an alternative computing method. We then propose a mixed-signal acceleration architecture for the modified training algorithm by equipping the original memristor-based neural network architecture with the copy crossbar technique, weight update units, sign calculation units and other assistant units. The experiment on the MNIST database demonstrates that the proposed mixed-signal acceleration is 3 orders of magnitude faster and 4 orders of magnitude more energy efficient than the CPU implementation counterpart at the cost of a slight decrease of the recognition accuracy (<; 5%).\"",
        "title: \"FPGA-based acceleration of neural network for ranking in web search engine with a streaming architecture\" with abstract: \"Web search engine companies are intensively running learning to rank algorithms to improve the search relevance. Neural network (NN)-based approaches, such as LambdaRank, can significantly increase the ranking quality. While, their training is very slow on a single computer and inherent coarse-grained parallelism could be hardly utilized by computer clusters. Thus an efficient implementation is necessary to timely generate acceptable NN models on frequently updated training datasets. This paper presents our work in accelerator. A SIMD streaming architecture is proposed to i) efficiently map the query-level NN computation and data structure to FPGA, ii) fully exploit the inherent fine-grained parallelism, and iii) provide scalability to large scale datasets. The accelerator shows up to 17.9X speedup over the software implementation on datasets from a commercial search engine.\"",
        "title: \"An evaluation method for video semantic models\" with abstract: \"The development of video technology and video-related applications demands strong support in semantic data models. To meet such a requirement, many video semantic data models have been proposed. The semantic model plays a key role in providing query capability and other features for a video database. However, to our knowledge, the criteria for a good semantic model remain open at present. As a result, people lack the rules for evaluating an existing model and the guidelines for the design of a new data model when necessary. To address this issue, this paper proposes twenty one properties as the criteria for video semantic models, and gives the evaluation result of eleven existing rich semantic models according to these properties. It shows that these models mostly concentrate on basic expressive power and query capability, and fulfill users' primary requirements. But in some advanced features such as expressive power, acquisition and analysis of semantic information, and query capability etc., there are rooms for further enhancement. The paper concludes by indicating some research directions for video semantic models.\"",
        "1 is \"A nondestructive self-reference scheme for Spin-Transfer Torque Random Access Memory (STT-RAM)\", 2 is \"VideoGraph: a graphical object-based model for representing and querying video data\".",
        "\nGiven above information, for an author who has written the paper with the title \"A General Framework for Hardware Trojan Detection in Digital Circuits by Statistical Learning Algorithms.\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0130": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Generalized network design polyhedra':",
        "title: \"The general routing polyhedron: A unifying framework\" with abstract: \"It is shown how to transform the General Routing Problem (GRP) into a variant of the Graphical Travelling Salesman Problem (GTSP). This transformation yields a projective characterisation of the GRP polyhedron. Using this characterisation, it is shown how to convert facets of the GTSP polyhedron into valid inequalities for the GRP polyhedron. The resulting classes of inequalities subsume several previously published classes.\"",
        "title: \"Small bipartite subgraph polytopes\" with abstract: \"We compute a complete linear description of the bipartite subgraph polytope, for up to seven nodes, and a conjectured complete description for eight nodes. We then show how these descriptions were used to compute the integrality ratio of various relaxations of the max-cut problem, again for up to eight nodes.\"",
        "title: \"New techniques for cost sharing in combinatorial optimization games\" with abstract: \"Combinatorial optimization games form an important subclass of cooperative games. In recent years, increased attention has been given to the issue of finding good cost shares for such games. In this paper, we define a very general class of games, called integer minimization games, which includes the combinatorial optimization games in the literature as special cases. We then present new techniques, based on row and column generation, for computing good cost shares for these games. To illustrate the power of these techniques, we apply them to traveling salesman and vehicle routing games. Our results generalize and unify several results in the literature. The main underlying idea is that suitable valid inequalities for the associated combinatorial optimization problems can be used to derive improved cost shares.\"",
        "title: \"Exploiting sparsity in pricing routines for the capacitated arc routing problem\" with abstract: \"The capacitated arc routing problem (CARP) is a well-known and fundamental vehicle routing problem. A promising exact solution approach to the CARP is to model it as a set covering problem and solve it via branch-cut-and-price. The bottleneck in this approach is the pricing (column generation) routine. In this paper, we note that most CARP instances arising in practical applications are defined on sparse graphs. We show how to exploit this sparsity to yield faster pricing routines. Extensive computational results are given.\"",
        "title: \"Gap inequalities for non-convex mixed-integer quadratic programs.\" with abstract: \"Laurent and Poljak introduced a very general class of valid linear inequalities, called gap inequalities, for the max-cut problem. We show that an analogous class of inequalities can be defined for general non-convex mixed-integer quadratic programs. These inequalities dominate some inequalities arising from a natural semidefinite relaxation.\"",
        "1 is \"On the boolean-quadric packinguncapacitated facility-location polytope\", 2 is \"{0, 1/2}-Chv\u00e1tal-Gomory cuts\".",
        "\nGiven above information, for an author who has written the paper with the title \"Generalized network design polyhedra\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0131": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Exponential Structures for Efficient Cache-Oblivious Algorithms':",
        "title: \"Priority Queues: Small, Monotone and Trans-dichotomous\" with abstract: \"We consider two data-structuring problems which involve performing priority queue (pq) operations on a set of integers in the range 0..2w\u22121 on a unit-cost RAM with word size \u03c9 bits.\"",
        "title: \"Fast Deterministic Selection on Mesh-Connected Processor Arrays\" with abstract: \"We present a deterministic algorithm for selecting the element of rank k among N=n2 elements, 1kN, on an n\u00d7n mesh-connected processor array in (1.44+ parallel computation steps, for any constant >0, using constant sized queues. This is a considerable improvement over the best previous deterministic algorithm, which was based upon sorting and required 3n steps. Our algorithm can be generalized to solve the problem of selection on higher dimensional meshes, achieving time bounds better than the known results in each case.\"",
        "title: \"Optimal trade-offs for succinct string indexes\" with abstract: \"Let s be a string whose symbols are solely available through access(i), a read-only operation that probes s and returns the symbol at position i in s. Many compressed data structures for strings, trees, and graphs, require two kinds of queries on s: select(c, j), returning the position in s containing the jth occurrence of c, and rank(c, p), counting how many occurrences of c are found in the first p positions of s. We give matching upper and lower bounds for this problem. The main contribution is to introduce a general technique for proving lower bounds on succinct data structures, that is based on the access patterns of the supported operations, abstracting from the particular operations at hand.\"",
        "title: \"Engineering succinct DOM\" with abstract: \"We describe the engineering of Succinct DOM (SDOM), a DOM implementation, written in C++, which is suitable for in-memory representation of large static XML documents. SDOM avoids the use of pointers, and is based upon succinct data structures, which use an information-theoretically minimum amount of space to represent an object. SDOM gives a space-efficient in-memory representation, with stable and predictable memory usage. The space used by SDOM is an order of magnitude less than that used by a standard C++ DOM representation such as Xerces, but SDOM is extremely fast: navigation is in some cases faster than for a pointer-based representation such as Xerces (even for moderate-sized documents which can comfortably be loaded into main memory by Xerces). A variant, SDOM-CT, applies bzip-based compression to textual and attribute data, and its space usage is comparable with \"queryable\" XML compressors. Some of these compressors support navigation and/or querying (e.g. subpath queries) of the compressed file. SDOM-CT does not support querying directly, but remains extremely fast: it is several orders of magnitude faster for navigation than queryable XML compressors that support navigation (and only a few times slower than say Xerces).\"",
        "title: \"Random access to grammar-compressed strings\" with abstract: \"Let S be a string of length N compressed into a context-free grammar S of size n. We present two representations of S achieving O(log N) random access time, and either O(n \u00b7 \u03b1k(n)) construction time and space on the pointer machine model, or O(n) construction time and space on the RAM. Here, \u03b1k(n) is the inverse of the kth row of Ackermann's function. Our representations also efficiently support decompression of any substring in S: we can decompress any substring of length m in the same complexity as a single random access query and additional O(m) time. Combining these results with fast algorithms for uncompressed approximate string matching leads to several efficient algorithms for approximate string matching on grammar-compressed strings without decompression. For instance, we can find all approximate occurrences of a pattern P with at most k errors in time O(n(min{|P|k, k4 +|P|} +log N) + occ), where occ is the number of occurrences of P in S. Finally, we are able to generalize our results to navigation and other operations on grammar-compressed trees. All of the above bounds significantly improve the currently best known results. To achieve these bounds, we introduce several new techniques and data structures of independent interest, including a predecessor data structure, two \"biased\" weighted ancestor data structures, and a compact representation of heavy-paths in grammars.\"",
        "1 is \"On RAM Priority Queues\", 2 is \"Triangulating a polygon in parallel\".",
        "\nGiven above information, for an author who has written the paper with the title \"Exponential Structures for Efficient Cache-Oblivious Algorithms\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0132": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Statistical Eigenmode Transmission Over Jointly Correlated MIMO Channels':",
        "title: \"Energy Efficient Link Adaptation for Downlink Transmission of LTE/LTE-A Systems\" with abstract: \"As a large percentage of the energy required by a cellular network is consumed at base station (BS) sites, how to reduce the energy consumption at BS sites has recently received much attention. In this paper, we propose an energy efficient link adaptation scheme to improve the BS's energy efficiency (EE) for long term evalution (LTE) systems. Based on the traditional spectral- efficiency centered link adaptation scheme, the proposed scheme treats transmit power as a new feedback parameter, aiming to maximize the BS's EE under a certain block error rate constraint. In addition, in order to reduce the transmit power adjustment frequency, a semi-static power control scheme is presented. Simulation results indicate that our proposed schemes can improve the BS's EE significantly.\"",
        "title: \"Limited feedback unitary precoding using improved Euclidean distance metrics for spatial multiplexing systems\" with abstract: \"This paper presents a new codeword selection criterion based on the Euclidean distance in the limited feedback unitary precoding systems. Because of the non-uniqueness of singular value decomposition (SVD), various precoding matrices may result in the same performance. We take the rotation and permutation invariance property into the definition of distortion function, then bring forward a new codeword selection criterion and distance metric derived from the improved Euclidean distance. A codebook design method based on the generalized Lloyd algorithm (GLA) is proposed thereafter. For the traditional subspace distance on the Grassmann manifold, the transmit data streams must be less than the transmit antennas, otherwise all the precoding matrices result in the same distortion results, however, our new distance metric can overcome this limitation. Simulation results show that our proposed distance metric can work properly with the unitary precoding codebook, apart from this, it can also acquire excellent performance under other spatial multiplexing environments.\"",
        "title: \"A Minimum Error Probability NOMA Design\" with abstract: \"Non-orthogonal multiple access (NOMA) enables massive connectivity and achieves high spectral efficiency. The vast majority of the NOMA literature has adopted the ideal information rate as performance metric assuming perfect successive interference cancellation (SIC) without any error propagation, which, however, may lead to NOMA designs adverse to SIC. In this paper, we take into account imperfec...\"",
        "title: \"Analysis and comparison of different feedback schemes for coordinated cellular networks\" with abstract: \"This paper compares several feedback schemes of a limited feedback system in a coordinated cellular networks. In a coordinated system one user can be served by different base stations (BSs) simultaneously. Conventional codebook is intended for single BS, its aim is to maximize the amplitude of the receive signal. When multiple BSs are equipped, phase discrimination among different BSs at the receive side may lead to performance degradation. Brute-force search through the entire BSs is a heavy burden especially when the BS number is large. In order to mitigate the negative effects, we present one solution dedicated to the coordinated system in this paper. When the BS number is small, we put forward a codebook design scheme based on the Lloyd codebook. When the BS number is large, we propose phase criterion instead of amplitude criterion in choosing the beamforming vector. Both schemes work well under their operating region. With our solution we can effectively reduce performance degradation in a coordinated cellular networks while still maintain a low complexity burden. Simulation results verify the effectiveness of our proposed schemes.\"",
        "title: \"Statistical 3-D Beamforming for Large-Scale MIMO Downlink Systems Over Rician Fading Channels.\" with abstract: \"In this paper, we investigate a downlink transmission algorithm for single-cell multiuser systems with two-dimensional (2-D) large-scale antenna array at the base station (BS) over Rician fading channels. We first derive some properties of the channel&#39;s line-of-sight (LOS) component in the large-scale antenna array scenario. Next, based on these properties and under the assumption of only statisti...\"",
        "1 is \"On scheduling and power allocation over multiuser MIMO-OFDMA: Fundamental design and performance evaluation in WiMAX systems\", 2 is \"Efficient resource allocation algorithm for cognitive OFDM systems\".",
        "\nGiven above information, for an author who has written the paper with the title \"Statistical Eigenmode Transmission Over Jointly Correlated MIMO Channels\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0133": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Nonadiabatic Ab initio surface-hopping dynamics calculation in a grid environment - first experiences':",
        "title: \"Native Double Precision LINPACK Implementation on a Hybrid Reconfigurable CPU\" with abstract: \"Applications requiring double precision (DP) arithmetic executed on embedded CPUs without native DP support suffer from prohibitively low performance and power efficiency. Hybrid reconfigurable CPUs, allowing for reconfiguration of the instruction set at runtime, appear as a viable computing platform for applications requiring instructions not supported by existing fixed architectures. Our experiments on a Stretch S6 as prototypical platform show that limited reconfigurable resources on such architectures are sufficient for providing native support of DP arithmetic. Our design using a DP fused multiply-accumulate (FMA) extension instruction achieves a peak performance of 200~MFlop/s and a sustained performance of 22.7~MFlop/s at a clock frequency of 100~MHz. It outperforms LINPACK using software-emulated DP floating-point arithmetic on the S6 by a factor of 5.7 while achieving slightly higher numerical accuracy. In single precision, multiple floating-point operators can be implemented in parallel on the S6.\"",
        "title: \"Effcient Solution of Evolution Models for Virus Populations\" with abstract: \"The computation of the quasispecies in Eigen's quasispecies model requires the solution of a very large scale eigenvalue problem. Since the problem dimension is of an exponential growing nature the well known methods for dealing with such a problem run out of resources already far away from practically relevant cases. We propose the use of an implicit matrix vector product using the special problem structure as building block for eigenvalue solvers to partially overcome the exponential growth, which let us reach unexpected large problem sizes. As we will show our implicit matrix vector product is a prime example for an algorithm perfectly matching the requirements of GPU computing since it has low space and high parallel computation requirements. Therefore we will also present an GPU implementation delivering a speedup factor of about 100 compared to a standard implementation.\"",
        "title: \"Energy consumption vs. latency in a new boundary identification method for WSNs with a mobile sink\" with abstract: \"In a recent paper we introduced the boundary identification scheme MoSBoD for wireless sensor networks. This method exploits a mobile sink equipped with a directional antenna to accomplish the desired task. In this paper we provide a careful analysis of the MoSBoD scheme in terms of energy efficiency and completion time (latency). Based on the results, we extend our previous work along two fronts. On the one hand, in order to improve the quality of the boundary identified using MoSBoD, we use a more restrictive definition of the boundary, which leads to reduced completion time for the boundary identification as well as to reduced energy consumption from the network. On the other hand, we propose a modification of the MoSBoD method which further reduces the completion time based on a selective increase of the duty cycles of sensor nodes. This strategy also increases the energy consumption. It is therefore required to carefully balance the expected reduction in the completion time with the increase in energy consumption. Thus, we also provide a deterministic model based analysis for energy consumption and completion time of the new boundary identification scheme, comparing it with the state-of-the-art.\"",
        "title: \"Mining agile DNS traffic using graph analysis for cybercrime detection.\" with abstract: \"We consider the analysis of network traffic data for identifying highly agile DNS patterns which are widely considered indicative for cybercrime. In contrast to related approaches, our methodology is capable of explicitly distinguishing between the individual, inherent agility of benign Internet services and criminal sites. Although some benign services use a large number of addresses, they are confined to a subset of IP addresses, due to operational requirements and contractual agreements with certain Content Distribution Networks. We discuss DNSMap, a system which analyzes observed DNS traffic, and continuously learns which FQDNs are hosted on which IP addresses. Any significant changes over time are mapped to bipartite graphs, which are then further pruned for cybercrime activity. Graph analysis enables the detection of transitive relations between FQDNs and IPs, and reveals clusters of malicious FQDNs and IP addresses hosting them. We developed a prototype system which is designed for realtime analysis, requires no costly classifier retraining, and no excessive whitelisting. We evaluate our system using large data sets from an ISP with several 100,000 customers, and demonstrate that even moderately agile criminal sites can be detected reliably and almost immediately.\"",
        "title: \"Efficient scheduling of sporadic tasks for real-time wireless sensor networks\" with abstract: \"Industrial automation requires hard real-time delivery of data that can be of periodic or sporadic in nature. It is challenging to ensure hard real-time delivery of periodic and sporadic data to a multi-hop away destination in a bandwidth constrained environment, such as wireless sensor networks. In this regard, research has been done for joint scheduling of periodic and sporadic data delivery using the IEEE 802.15.4 standard. However, delivery to a destination that is multiple hops away has not been handled. Moreover, the IEEE 802.15.4 standard compliance is not fully taken care of. This study proposes a novel communication protocol that is IEEE 802.15.4 standard compliant, for real-time delivery of sporadic and periodic data to a destination that is multiple hops away. Implicit decisions at various nodes result in the minimal increase in the control traffic and reduced energy dissipation. The proposed protocol is tested using the OPNET simulator and the correctness is proven through a multitude of simulation results.\"",
        "1 is \"Harmony: an execution model and runtime for heterogeneous many core systems\", 2 is \"Boundary estimation in sensor networks: theory and methods\".",
        "\nGiven above information, for an author who has written the paper with the title \"Nonadiabatic Ab initio surface-hopping dynamics calculation in a grid environment - first experiences\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0134": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Semi-supervised discriminative classification with application to tumorous tissues segmentation of MR brain images':",
        "title: \"Towards Model-Agnostic Post-Hoc Adjustment for Balancing Ranking Fairness and Algorithm Utility\" with abstract: \"ABSTRACTBipartite ranking, which aims to learn a scoring function that ranks positive individuals higher than negative ones from labeled data, is widely adopted in various applications where sample prioritization is needed. Recently, there have been rising concerns on whether the learned scoring function can cause systematic disparity across different protected groups defined by sensitive attributes. While there could be trade-off between fairness and performance, in this paper we propose a model agnostic post-processing framework for balancing them in the bipartite ranking scenario. Specifically, we maximize a weighted sum of the utility and fairness by directly adjusting the relative ordering of samples across groups. By formulating this problem as the identification of an optimal warping path across different protected groups, we propose a non-parametric method to search for such an optimal path through a dynamic programming process. Our method is compatible with various classification models and applicable to a variety of ranking fairness metrics. Comprehensive experiments on a suite of benchmark data sets and two real-world patient electronic health record repositories show that our method can achieve a great balance between the algorithm utility and ranking fairness. Furthermore, we experimentally verify the robustness of our method when faced with the fewer training samples and the difference between training and testing ranking score distributions.\"",
        "title: \"Believe It Today or Tomorrow? Detecting Untrustworthy Information from Dynamic Multi-Source Data.\" with abstract: \"A vast ocean of data is collected every day, and numerous applications call for the extraction of actionable insights from data. One important task is to detect untrustworthy information because such information usually indicates critical, unusual, or suspicious activities. In this paper, we study the important problem of detecting untrustworthy information from a novel perspective of correlating and comparing multiple sources that describe the same set of items. Different from existing work, we recognize the importance of time dimension in modeling the commonalities among multiple sources. We represent dynamic multi-source data as tensors and develop a joint non-negative tensor factorization approach to capture the common patterns across sources. We then conduct a comparison between source input and common patterns to identify inconsistencies as an indicator of untrustworthiness. An incremental factorization approach is developed to improve the computational efficiency on dynamically arriving data. We also propose a method to handle data sparseness. Experiments are conducted on hotel rating, network traffic flow, and weather forecast data that are collected from multiple sources. Results demonstrate the advantages of the proposed approach in detecting inconsistent and untrustworthy information.\"",
        "title: \"Predicting Seizures from Electroencephalography Recordings: A Knowledge Transfer Strategy\" with abstract: \"Epilepsy, a brain disorder afflicts nearly 1% of the world's population, is characterized by the occurrence of spontaneous seizures. For most epilepsy patients, the drugs are either not effective or produce severe side-effects. Seizure forecasting systems have the potential to help patients with epilepsy lead more normal lives. Recently multi-center clinical studies showed evidence of premonitory symptoms in 6.2% of 500 patients with epilepsy, and some interviews of epilepsy patients also found that a certain amount of patients felt \"auras\". All these are promising signs suggesting that seizure might be predictable. In this paper, we will study the application of deep learning techniques for seizure prediction with EEG signals. Deep learning methods have been shown to be very effective on exploring the latent structures from continuous signals and they have achieved state-of-the-art performance on speech analysis. One potential requirement for deep learning algorithms to work is a huge training set, which could be difficult for a specific medical problem. Therefore we specifically investigated a transfer learning strategy: we performed the major seizure prediction task on the data from American Epilepsy Society Seizure Prediction Challenge1, and we adopted another 6 publicly available EEG datasets2, which are not directly related to seizure prediction, as auxiliary information to pre-train the deep neural network for getting a good initial point. Our results show that with those auxiliary information, the prediction performance can be boosted. This observation is validated with different predictive models, which opens another gate for effective integration and utilization of medical data resources.\"",
        "title: \"Smoothness Maximization via Gradient Descents.\" with abstract: \"The recent years have witnessed a surge of interest in graph based semi-supervised learning. However, despite its extensive research, there has been little work on graph construction. In this study, employing the idea of gradient descent, we propose a novel method called Iterative Smoothness Maximization (ISM), to learn an optimal graph automatically for a semi-supervised learning task. The main procedure of ISM is to minimize the upper bound of semi-supervised classification error through an iterative gradient descent approach. We also prove the convergence of ISM theoretically, and finally experimental results on two real-world data sets are provided to demonstrate the effectiveness of ISM. \u00a9 2007 IEEE.\"",
        "title: \"Towards heterogeneous temporal clinical event pattern discovery: a convolutional approach\" with abstract: \"Large collections of electronic clinical records today provide us with a vast source of information on medical practice. However, the utilization of those data for exploratory analysis to support clinical decisions is still limited. Extracting useful patterns from such data is particularly challenging because it is longitudinal, sparse and heterogeneous. In this paper, we propose a Nonnegative Matrix Factorization (NMF) based framework using a convolutional approach for open-ended temporal pattern discovery over large collections of clinical records. We call the method One-Sided Convolutional NMF (OSC-NMF). Our framework can mine common as well as individual shift-invariant temporal patterns from heterogeneous events over different patient groups, and handle sparsity as well as scalability problems well. Furthermore, we use an event matrix based representation that can encode quantitatively all key temporal concepts including order, concurrency and synchronicity. We derive efficient multiplicative update rules for OSC-NMF, and also prove theoretically its convergence. Finally, the experimental results on both synthetic and real world electronic patient data are presented to demonstrate the effectiveness of the proposed method.\"",
        "1 is \"Vision and the Atmosphere\", 2 is \"Localized Supervised Metric Learning on Temporal Physiological Data\".",
        "\nGiven above information, for an author who has written the paper with the title \"Semi-supervised discriminative classification with application to tumorous tissues segmentation of MR brain images\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0135": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Analysis of classification margin for classification accuracy with applications':",
        "title: \"Pseudo relevance feedback based on iterative probabilistic one-class SVMs in web image retrieval\" with abstract: \"To improve the precision of top-ranked images returned by a web image search engine, we propose in this paper a novel pseudo relevance feedback method named iterative probabilistic one-class SVMs to re-rank the retrieved images. By assuming that most top-ranked images are relevant to the query, we iteratively train one-class SVMs, and convert the outputs to probabilities so as to combine the decision from different image representation. The effectiveness of our method is validated by systematic experiments even if the assumption is not well satisfied.\"",
        "title: \"Visual object recognition using probabilistic kernel subspace similarity\" with abstract: \"Probabilistic subspace similarity-based face matching is an efficient face recognition algorithm proposed by Moghaddam et al. It makes one basic assumption: the intra-class face image set spans a linear space. However, there are yet no rational geometric interpretations of the similarity under that assumption. This paper investigates two subjects. First, we present one interpretation of the intra-class linear subspace assumption from the perspective of manifold analysis, and thus discover the geometric nature of the similarity. Second, we also note that the linear subspace assumption does not hold in some cases, and generalize it to nonlinear cases by introducing kernel tricks. The proposed model is named probabilistic kernel subspace similarity (PKSS). Experiments on synthetic data and real visual object recognition tasks show that PKSS can achieve promising performance, and outperform many other current popular object recognition algorithms.\"",
        "title: \"Optimizing Top-k Multiclass SVM via Semismooth Newton Algorithm.\" with abstract: \"Top-k performance has recently received increasing attention in large data categories. Advances, like a top-k multiclass support vector machine (SVM), have consistently improved the top-k accuracy. However, the key ingredient in the state-of-the-art optimization scheme based upon stochastic dual coordinate ascent relies on the sorting method, which yields O(d log d) complexity. In this paper, we l...\"",
        "title: \"Regularized clustering for documents\" with abstract: \"In recent years, document clustering has been receiving more and more attentions as an important and fundamental technique for unsupervised document organization, automatictopic extraction, and fast information retrieval or filtering. In this paper, we propose a novel method for clustering documents using regularization. Unlike traditional globally regularized clustering methods, our method first construct a local regularized linear label predictor for each document vector, and then combine all those local regularizers with a global smoothness regularizer. So we call our algorithm Clustering with Local and Global Regularization (CLGR). We will show that the cluster memberships of the documents can be achieved by eigenvalue decomposition of a sparse symmetric matrix, which can be efficiently solved by iterative methods. Finally our experimental evaluations on several datasets are presented to show the superiorities of CLGR over traditional document clustering methods.\"",
        "title: \"Semi-Supervised Classification with Universum\" with abstract: \"The Universum data, deflned as a collection of \"non- examples\" that do not belong to any class of inter- est, have been shown to encode some prior knowledge by representing meaningful concepts in the same do- main as the problem at hand. In this paper, we ad- dress a novel semi-supervised classiflcation problem, called semi-supervised Universum, that can simultane- ously utilize the labeled data, unlabeled data and the Universum data to improve the classiflcation perfor- mance. We propose a graph based method to make use of the Universum data to help depict the prior infor- mation for possible classiflers. Like conventional graph based semi-supervised methods, the graph regulariza- tion is also utilized to favor the consistency between the labels. Furthermore, since the proposed method is a graph based one, it can be easily extended to the multi- class case. The empirical experiments on the USPS and MNIST datasets are presented to show that the pro- posed method can obtain superior performances over conventional supervised and semi-supervised methods.\"",
        "1 is \"High-performing feature selection for text classification\", 2 is \"YouSlow: a performance analysis tool for adaptive bitrate video streaming\".",
        "\nGiven above information, for an author who has written the paper with the title \"Analysis of classification margin for classification accuracy with applications\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0136": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Architecting high-performance energy-efficient soft error resilient cache under 3D integration technology':",
        "title: \"An Immune Multi-agent System for Network Intrusion Detection\" with abstract: \"Inspired by the immune theory and multi-agent systems, an immune multi-agent system for network intrusion detection is established. The concept of immune agent is introduced. And its logical structure and running mechanism are established. This model implements the multi-layer and distributed mechanism for network intrusion detection. The experimental results show that the new model not only reduces the False-Negative rate and False-Positive rate effectively but also has the feature to adapt to continuous changing network environments.\"",
        "title: \"Mining logs files for data-driven system management\" with abstract: \"With advancement in science and technology, computing systems are becoming increasingly more complex with an increasing variety of heterogeneous software and hardware components. They are thus becoming increasingly more difficult to monitor, manage and maintain. Traditional approaches to system management have been largely based on domain experts through a knowledge acquisition process that translates domain knowledge into operating rules and policies. This has been well known and experienced as a cumber-some, labor intensive, and error prone process. In addition, this process is difficult to keep up with the rapidly changing environments. There is thus a pressing need for automatic and efficient approaches to monitor and manage complex computing systems.A popular approach to system management is based on analyzing system log files. However, some new aspects of the log files have been less emphasized in existing methods from data mining and machine learning community. The various formats and relatively short text messages of log files, and temporal characteristics in data representation pose new challenges. In this paper, we will describe our research efforts on mining system log files for automatic management. In particular, we apply text mining techniques to categorize messages in log files into common situations, improve categorization accuracy by considering the temporal characteristics of log messages, and utilize visualization tools to evaluate and validate the interesting temporal patterns for system management.\"",
        "title: \"Personalized Recommendation via Parameter-Free Contextual Bandits\" with abstract: \"Personalized recommendation services have gained increasing popularity and attention in recent years as most useful information can be accessed online in real-time. Most online recommender systems try to address the information needs of users by virtue of both user and content information. Despite extensive recent advances, the problem of personalized recommendation remains challenging for at least two reasons. First, the user and item repositories undergo frequent changes, which makes traditional recommendation algorithms ineffective. Second, the so-called cold-start problem is difficult to address, as the information for learning a recommendation model is limited for new items or new users. Both challenges are formed by the dilemma of exploration and exploitation. In this paper, we formulate personalized recommendation as a contextual bandit problem to solve the exploration/exploitation dilemma. Specifically in our work, we propose a parameter-free bandit strategy, which employs a principled resampling approach called online bootstrap, to derive the distribution of estimated models in an online manner. Under the paradigm of probability matching, the proposed algorithm randomly samples a model from the derived distribution for every recommendation. Extensive empirical experiments on two real-world collections of web data (including online advertising and news recommendation) demonstrate the effectiveness of the proposed algorithm in terms of the click-through rate. The experimental results also show that this proposed algorithm is robust in the cold-start situation, in which there is no sufficient data or knowledge to tune the hyper-parameters.\"",
        "title: \"Workload Characterization of Bioinformatics Applications\" with abstract: \"The exponential growth in the amount of genomic information has spurred growing interest in large scale analysis of genetic data. Bioinformatics applications represent the increasingly important workloads. However, very few results on the behavior of these applications running on the state-of-the-art microprocessor and systems have been published. This paper proposes a suite of widely used bioinformatics applications and studies the execution characteristics of these benchmarks on a representative architecture - the Intel Pentium 4. To understand the impacts and implications of bioinformatics workloads on the microprocessor designs, we contrast the characteristics of bioinformatics workloads and the widely used SPEC 2000 integer benchmarks. The proposed bioinformatics benchmark suite as well as the input datasets can be downloaded from the following website: http://www.ideal.ece.ufl.edu/BioInfoMark.\"",
        "title: \"A comparative study of feature selection and multiclass classification methods for tissue classification based on gene expression.\" with abstract: \"This paper studies the problem of building multiclass classifiers for tissue classification based on gene expression. The recent development of microarray technologies has enabled biologists to quantify gene expression of tens of thousands of genes in a single experiment. Biologists have begun collecting gene expression for a large number of samples. One of the urgent issues in the use of microarray data is to develop methods for characterizing samples based on their gene expression. The most basic step in the research direction is binary sample classification, which has been studied extensively over the past few years. This paper investigates the next step-multiclass classification of samples based on gene expression. The characteristics of expression data (e.g. large number of genes with small sample size) makes the classification problem more challenging. The process of building multiclass classifiers is divided into two components: (i) selection of the features (i.e. genes) to be used for training and testing and (ii) selection of the classification method. This paper compares various feature selection methods as well as various state-of-the-art classification methods on various multiclass gene expression datasets. Our study indicates that multiclass classification problem is much more difficult than the binary one for the gene expression datasets. The difficulty lies in the fact that the data are of high dimensionality and that the sample size is small. The classification accuracy appears to degrade very rapidly as the number of classes increases. In particular, the accuracy was very low regardless of the choices of the methods for large-class datasets (e.g. NCI60 and GCM). While increasing the number of samples is a plausible solution to the problem of accuracy degradation, it is important to develop algorithms that are able to analyze effectively multiple-class expression data for these special datasets.\"",
        "1 is \"The Skyline operator\", 2 is \"Wave-pipelining: a tutorial and research survey\".",
        "\nGiven above information, for an author who has written the paper with the title \"Architecting high-performance energy-efficient soft error resilient cache under 3D integration technology\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0137": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Hybrid Filter Banks With Fractional Delays: Minimax Design and Application to Multichannel Sampling':",
        "title: \"Joint Optimization of Source Power Allocation and Cooperative Beamforming for SC-FDMA Multi-User Multi-Relay Networks\" with abstract: \"This paper is concerned with design problems of joint source power allocation and relay beamforming in multi-user multi-relay networks that use single-carrier frequency division multiple access (SC-FDMA) and amplify-and-forward relaying. Examined are the joint programs of (i) maximizing the minimum signal-to-interference-plus-noise ratio (SINR) under various transmitted power constraints, and (ii) minimizing the total transmitted power subject to prescribed SINR thresholds of users. Although these optimization problems are highly nonconvex and have large dimensions, by exploiting their partial convexities and making elegant nonlinear variable changes, they are recast as d.c. (difference of two convex) programs. Efficient d.c. iterative procedures are then developed to find the solutions. Simplified joint programs under the two cases of equal source power and equal relay beamforming weights, respectively, are also considered. Branch-and-bound algorithms of deterministic global optimization are then proposed for solving the simplified joint programs. Simulation results confirm the excellent performance and computational efficiency of all the proposed solutions.\"",
        "title: \"Orthogonal affine precoding and decoding for channel stimation and source detection in MIMO frequency-selective fading channels\" with abstract: \"A new affine precoding and decoding method for multiple-input multiple-output (MIMO) frequency-selective fading channels is proposed. The optimal design of the affine precoder consists of a linear precoder and a training sequence, which is superimposed on the linearly precoded data in an orthogonal manner. The channel is estimated independently of the data, while the training sequence is completely nulled out in the source data detection. Furthermore, the optimal power allocation between the data and training signals is also analytically derived. Simulation results show that the proposed method is significantly better than other affine precoding methods with regard to source detection performance and computational complexity.\"",
        "title: \"Relay Beamforming Designs in Multi-User Wireless Relay Networks Based on Throughput Maximin Optimization.\" with abstract: \"Beamforming design for multi-user wireless relay networks under the criterion of maximin information throughput is an important but also very hard optimization problem due to its nonconvex nature. The existing approach to reformulate the design as a matrix rank-one constrained optimization problem is highly inefficient. This paper exploits the d.c. (difference of two convex functions) structure of the objective function and the convex structure of the constraints in such a global optimization problem to develop efficient iterative algorithms of very low complexity to find the solutions. Both cases of concurrent and orthogonal transmissions from sources to relays are considered. Numerical results indicate that the proposed algorithms provide solutions that are very close to the upper bound on the solution of the non-orthogonal source transmissions case and are almost equal to the optimal solution of the orthogonal source transmissions case. This demonstrates the ability of the developed algorithms to locate approximations close to the global optimal solutions in a few iterations. Moreover, the proposed methods are superior to other methods in both performance and computation complexity.\"",
        "title: \"Joint Optimization of Source Precoding and Relay Beamforming in Wireless MIMO Relay Networks.\" with abstract: \"This paper considers joint linear processing at multi-antenna sources and one multiple-input multiple-output (MIMO) relay station for both one-way and two-way relay-assisted wireless communications. The one-way relaying is applicable in the scenario of downlink transmission by a multi-antenna base station to multiple single-antenna users with the help of one MIMO relay. In such a scenario, the objective of join linear processing is to maximize the information throughput to users. The design problem is equivalently formulated as the maximization of the worst signal-to-interference-plus-noise ratio (SINR) among all users subject to various transmission power constraints. Such a program of nonconvex objective minimization under nonconvex constraints is transformed to a canonical d.c. (difference of convex functions/sets) program of d.c. function optimization under convex constraints through nonconvex duality with zero duality gap. An efficient iterative algorithm is then applied to solve this canonical d.c program. For the scenario of using one MIMO relay to assist two sources exchanging their information in two-way relying manner, the joint linear processing aims at either minimizing the maximum mean square error (MSE) or maximizing the total information throughput of the two sources. By applying tractable optimization for the linear minimum MSE estimator and d.c. programming, an iterative algorithm is developed to solve these two optimization problems. Extensive simulation results demonstrate that the proposed methods substantially outperform previously-known joint optimization methods.\"",
        "title: \"A Novel and Efficient Mapping of 32-QAM Constellation for BICM-ID Systems\" with abstract: \"Bit-interleaved coded modulation with iterative decoding (BICM-ID) is a bandwidth-efficient technique for both additive white Gaussian noise and fading channels. The asymptotic performance of BICM-ID is strongly determined by how the coded bits are mapped to the symbols of the signal constellation. In this paper an explicit mapping method is presented for 32-QAM using two criteria: (i) maximization of the minimum Euclidean distance between the symbols with Hamming distance one, and (ii) minimizing the number of symbols which have jointly the minimum Hamming distance and the minimum Euclidean distance from each other. Our method is much simpler than the previously-known methods. Compared to previously-known best mapping, the mapping obtained by our method performs significantly better in a BICM-ID system implemented with hard-decision feedback, while its asymptotic performance is almost the same in a BICM-ID system using soft-decision feedback.\"",
        "1 is \"Local Layering for Joint Motion Estimation and Occlusion Detection\", 2 is \"Differential unitary space-time modulation\".",
        "\nGiven above information, for an author who has written the paper with the title \"Hybrid Filter Banks With Fractional Delays: Minimax Design and Application to Multichannel Sampling\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0138": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'LMI characterization for the convex hull of trigonometric curves and applications':",
        "title: \"Joint Optimization of Source Power Allocation and Cooperative Beamforming for SC-FDMA Multi-User Multi-Relay Networks\" with abstract: \"This paper is concerned with design problems of joint source power allocation and relay beamforming in multi-user multi-relay networks that use single-carrier frequency division multiple access (SC-FDMA) and amplify-and-forward relaying. Examined are the joint programs of (i) maximizing the minimum signal-to-interference-plus-noise ratio (SINR) under various transmitted power constraints, and (ii) minimizing the total transmitted power subject to prescribed SINR thresholds of users. Although these optimization problems are highly nonconvex and have large dimensions, by exploiting their partial convexities and making elegant nonlinear variable changes, they are recast as d.c. (difference of two convex) programs. Efficient d.c. iterative procedures are then developed to find the solutions. Simplified joint programs under the two cases of equal source power and equal relay beamforming weights, respectively, are also considered. Branch-and-bound algorithms of deterministic global optimization are then proposed for solving the simplified joint programs. Simulation results confirm the excellent performance and computational efficiency of all the proposed solutions.\"",
        "title: \"Precoder Design for Signal Superposition in MIMO-NOMA Multicell Networks.\" with abstract: \"The throughput of users with poor channel conditions, such as those at a cell edge, is a bottleneck in wireless systems. A major part of the power budget must be allocated to serve these users in guaranteeing their quality-of-service (QoS) requirements, hampering QoS for other users, and thus compromising the system reliability. In non-orthogonal multiple access (NOMA), the message intended for a user with a poor channel condition is decoded by itself and by another user with a better channel condition. The message intended for the latter is then successively decoded by itself after canceling the interference of the former. The overall information throughput is thus improved by this particular successive decoding and interference cancellation. This paper aims to design linear precoders/beamformers for signal superposition at the base stations of NOMA multiple-input multiple-output multi-cellular systems to maximize the overall sum throughput subject to the users' QoS requirements, which are imposed independently on the users' channel conditions. This design problem is formulated as the maximization of a highly nonlinear and nonsmooth function subject to nonconvex constraints, which is very computationally challenging. Path-following algorithms for its solution, which invoke only a simple convex problem of moderate dimension at each iteration, are developed. Generating a sequence of improved points, these algorithms converge at least to a local optimum. Extensive numerical simulations are then provided to demonstrate their merit.\"",
        "title: \"Robust control via concave minimization local and global algorithms\" with abstract: \"This paper is concerned with the robust control problem of linear fractional representation (LFT) uncertain systems depending on a time-varying parameter uncertainty. Our main result exploits a linear matrix inequality (LMI) characterization involving scalings and Lyapunov variables subject to an additional essentially nonconvex algebraic constraint. The nonconvexity enters the problem in the form of a rank deficiency condition or matrix inverse relation on the scalings only. It is shown that such problems, but also more generally rank inequalities and bilinear constraints, can be formulated as the minimization of a concave functional subject to LMI constraints. First of all, a local Frank and Wolfe (1956) feasible direction algorithm is introduced in this context to tackle this hard optimization problem. Exploiting the attractive concavity structure of the problem, several efficient global concave programming methods are then introduced and combined with the local feasible direction method to secure and certify global optimality of the solutions. Computational experiments indicate the viability of our algorithms, and in the worst case, they require the solution of a few LMI programs\"",
        "title: \"Optimum multi-user detection by nonsmooth optimization\" with abstract: \"The optimum multiuser detection (OMD) is a discrete (binary) optimization. The previously developed approaches often relax it by a semi-definite program (SDP) and then employ randomization for searching the optimal solution around the solution of this relaxed SDP. In this paper, we show the limited capacity of this SDP program, which at the end cannot give a better solution than the simple linear minimum mean square error detector (LMMSE). Our departure point is to express the problem as quadratic minimization over quadratic equality constraint (QMQE) or concave quadratic minimization over a box of continuous optimization (CQOB). The QMQE allows us to develop a nonsmooth optimization algorithm to locate the global optimal solution of OMD, while CQOB facilities effective confirmation of the solutions found by QMQE. Our intensive simulation clearly shows that the algorithm outperforms all previously developed algorithms while the computational burden is essentially reduced.\"",
        "title: \"MPC-Based UAV Navigation for Simultaneous Solar-Energy Harvesting and Two-Way Communications\" with abstract: \"The paper is the first work that considers a constrained feedback control strategy to navigate an unmanned aerial vehicle (UAV) from a given starting point to a given terminal point while harvesting solar energy and providing a wireless communication service for ground users. Wireless communication channels are stochastic and cannot be known off-line, making the problem of off-line UAV path planni...\"",
        "1 is \"L2 optimal filter reduction: a closed-loop approach\", 2 is \"Real-time spatiotemporal stereo matching using the dual-cross-bilateral grid\".",
        "\nGiven above information, for an author who has written the paper with the title \"LMI characterization for the convex hull of trigonometric curves and applications\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0139": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'A two-ball mouse affords three degrees of freedom':",
        "title: \"Fitts' throughput and the speed-accuracy tradeoff\" with abstract: \"We describe an experiment to test the hypothesis that Fitts' throughput is independent of the speed-accuracy tradeoff. Eighteen participants used a mouse in performing a total of 5,400 target selection trials. Comparing nominal, speed-emphasis, and accuracy-emphasis conditions, significant main effects were found on movement time (ms) and error rate (%), but not on throughput (bits/s). In the latter case, failure to reject the null hypothesis of \"no significant difference\" (i.e., .05\"",
        "title: \"An empirical comparison of \"wiimote\" gun attachments for pointing tasks\" with abstract: \"We evaluated and compared four input methods using the Nintendo Wii Remote for pointing tasks. The methods used (i) the \"A\" button on top of the device, (ii) the \"B\" button on the bottom of the device, (iii) the Intec Wii Combat Shooter attachment and (iv) the Nintendo Wii Zapper attachment. Fitts' throughput for all four input methods was calculated for both button-up and button-down events. Results indicate that the throughput of the Wii Remote using the A button is 2.85 bps for button-down events. Performance with the Intec Wii Combat Shooter attachment was significantly worse than with the other input methods, likely due to the trigger mechanism. Throughput for button-down target selection using the B button was highest at 2.93 bps.\"",
        "title: \"slab: smart labeling of family photos through an interactive interface\" with abstract: \"A novel technique for semi-automatic photo annotation is proposed and evaluated. The technique, sLab, uses face processing algorithms and a simplified user interface for labeling family photos. A user study compared our system with two others. One was Adobe Photoshop Element. The other was an in-house implementation of a face clustering interface recently proposed in the research community. Nine participants performed an annotation task with each system on faces extracted from a set of 150 images from their own family photo albums. As the faces were all well known to participants, accuracy was near perfect with all three systems. On annotation time, sLab was 25% faster than Photoshop Element and 16% faster than the face clustering interface.\"",
        "title: \"Graffiti vs. unistrokes: an empirical comparison\" with abstract: \"Unistrokes and Graffiti are stylus-based text entry techniques. While Unistrokes is recognized in academia, Graffiti is commercially prevalent in PDAs. Though numerous studies have investigated the usability of Graffiti, none exists to compare its long-term performance with that of Unistrokes. This paper presents a longitudinal study comparing entry speed, correction rate, stroke duration, and preparation (i.e., inter-stroke) time of these two techniques. Over twenty fifteen-phrase sessions, performance increased from 4.0 wpm to 11.4 wpm for Graffiti and from 4.1 wpm to 15.8 wpm for Unistrokes. Correction rates were high for both techniques. However, rates for Graffiti remained relatively consistent at 26%, while those for Unistrokes decreased from 43% to 16%.\"",
        "title: \"Testing pointing device performance and user assessment with the ISO 9241, Part 9 standard\" with abstract: \"The IS0 9241, Part 9 Draft International Standard for testingcomputer pointing devices proposes an evaluation of performance andcomfort. In this paper we evaluate the scientific validity andpracticality of these dimensions for two pointing devices forlaptop computers, a finger-controlled isometric joystick and atouchpad. Using a between-subjects design, evaluation ofperformance using the measure of throughput was done forone-direction and multi-directional pointing and selecting. Resultsshow a significant difference in throughput for themulti-directional task, with the joystick 27% higher; results forthe one-direction task were non-significant. After the experiment,participants rated the device for comfort, including operation,fatigue, and usability. The questionnaire showed no overalldifference in the responses, and a significant statisticaldifference in only the question concerning force required tooperate the device - the joystick requiring slightly more force.The paper concludes with a discussion of problems in implementingthe IS0 standard and recommendations for improvement.\"",
        "1 is \"Model-based and empirical evaluation of multimodal interactive error correction\", 2 is \"Probabilistic Fusion of Stereo with Color and Contrast for Bi-Layer Segmentation\".",
        "\nGiven above information, for an author who has written the paper with the title \"A two-ball mouse affords three degrees of freedom\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0140": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'User Elicitation on Single-hand Microgestures.':",
        "title: \"Perceptual grouping: selection assistance for digital sketching\" with abstract: \"Modifying a digital sketch may require multiple selections before a particular editing tool can be applied. Especially on large interactive surfaces, such interactions can be fatiguing. Accordingly, we propose a method, called Suggero, to facilitate the selection process of digital ink. Suggero identifies groups of perceptually related drawing objects. These \"perceptual groups\" are used to suggest possible extensions in response to a person's initial selection. Two studies were conducted. First, a background study investigated participant's expectations of such a selection assistance tool. Then, an empirical study compared the effectiveness of Suggero with an existing manual technique. The results revealed that Suggero required fewer pen interactions and less pen movement, suggesting that Suggero minimizes fatigue during digital sketching.\"",
        "title: \"Novel user interfaces for diagram versioning and differencing\" with abstract: \"Easily available software for diagram creation does not support the comparison of different versions and the merging of such versions. We present new methods and techniques for easy versioning of general two-dimensional diagrams. Multiple novel versioning methods for diagram versioning are compared to each other as well as to previous work in a user study. Participants in a user study preferred the Translucency View and Master Diagram Scenario to the other investigated methods and scenarios.\"",
        "title: \"Modeling learning effects in mobile texting\" with abstract: \"No work on mobile text messaging so far has taken into account the effect of learning on the change in visual exploration behavior as users progress from non-expert to expert level. We discuss within the domain of multi-tap texting on mobile phone and address the process of searching versus selecting a letter on the keypad interface. We develop a simulation model that forecasts the probability of letter location recall by non-expert users and thereby models learning, as the user acquires expertise in recalling, with practice, session after session. We then plugin this probability within a model of visual strategy that combines the effect of different ways visual exploration: non-expert users search for a letter while expert users select a letter. The observed non-expert non-motor time preceding a key press (for a letter) correlates extremely well with the simulation results.\"",
        "title: \"Laser Pointers As Collaborative Pointing Devices\" with abstract: \"Single Display Groupware (SDG) is a research area that focuses oil providing collaborative computing environments. Traditionally, most hardware platforms for SDG support only one person interacting at any given time, which limits collaboration. In this paper, we present laser pointers as input devices that can provide concurrent input streams ideally required to the SDG environment.First, we discuss several issues related to utilization of laser pointers and present the new concept of computer controlled laser pointers. Then we briefly present a performance evaluation of laser pointers as input devices and a baseline comparison with the mouse according to the ISO 9241-9 standard.Finally, we describe a new system that uses multiple computer controlled laser pointers as interaction devices for one or more displays. Several alternatives for distinguishing between different laser pointers arc presented, and ail implementation of one of them is demonstrated with SDG applications.\"",
        "title: \"A cognitive simulation model for novice text entry on cell phone keypads\" with abstract: \"Motivation -- To create a cognitive simulation model that predicts text entry performance and learning on cell phone keypads by novice users. Research approach -- A programmable cognitive architecture, ACT-R, is used to execute the simulation model. Part of the simulation result is compared with the result of a previous user study. Findings/Design -- The proposed model is an a priori model (not tuned to any real user data) that predicts the amount of time spent in finding a key on the keypad and pressing it repeatedly. The predicted amount of time in finding a key differs by 6% and the time between two repeated key-presses of the same key by 27% compared to the results of a previous user study. The model also captures the learning of keypad layout by novice users. Memorization of keypad layout is simulated using task repetition. Research limitations/Implications -- This research has several limitations described towards the end of this paper. An important one among them is that the work does not model the impact of visual distracters in the field of view (frontal surface of the handset) on user performance. Originality/Value -- This is the first cognitive simulation model of novice user's text entry performance and learning on cell phone keypads. Take away message -- This work introduces an a priori congnitive model of text entry by novice users. This forms a basis for systematic exploration of keypad designs for cell phones in shorter time and lower cost.\"",
        "1 is \"Feeling bumps and holes without a haptic interface: the perception of pseudo-haptic textures\", 2 is \"VisPorter: facilitating information sharing for collaborative sensemaking on multiple displays\".",
        "\nGiven above information, for an author who has written the paper with the title \"User Elicitation on Single-hand Microgestures.\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0141": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'GenDeR: A Generic Diversified Ranking Algorithm.':",
        "title: \"Classification of digital photos taken by photographers or home users\" with abstract: \"In this paper, we address a specific image classification task, i.e. to group images according to whether they were taken by photographers or home users. Firstly, a set of low-level features explicitly related to such high-level semantic concept are investigated together with a set of general-purpose low-level features. Next, two different schemes are proposed to find out those most discriminative features and feed them to suitable classifiers: one resorts to boosting to perform feature selection and classifier training simultaneously; the other makes use of the information of the label by Principle Component Analysis for feature re-extraction and feature de-correlation; followed by Maximum Marginal Diversity for feature selection and Bayesian classifier or Support Vector Machine for classification. In addition, we show an application in No-Reference holistic quality assessment as a natural extension of such image classification. Experimental results demonstrate the effectiveness of our methods.\"",
        "title: \"Function-on-Function Regression with Mode-Sparsity Regularization.\" with abstract: \"Functional data is ubiquitous in many domains, such as healthcare, social media, manufacturing process, sensor networks, and so on. The goal of function-on-function regression is to build a mapping from functional predictors to functional response. In this article, we propose a novel function-on-function regression model based on mode-sparsity regularization. The main idea is to represent the regression coefficient function between predictor and response as the double expansion of basis functions, and then use a mode-sparsity regularization to automatically filter out irrelevant basis functions for both predictors and responses. The proposed approach is further extended to the tensor version to accommodate multiple functional predictors. While allowing the dimensionality of the regression weight matrix or tensor to be relatively large, the mode-sparsity regularized model facilitates the multi-way shrinking of basis functions for each mode. The proposed mode-sparsity regularization covers a wide spectrum of sparse models for function-on-function regression. The resulting optimization problem is challenging due to the non-smooth property of the mode-sparsity regularization. We develop an efficient algorithm to solve the problem, which works in an iterative update fashion, and converges to the global optimum. Furthermore, we analyze the generalization performance of the proposed method and derive an upper bound for the consistency between the recovered function and the underlying true function. The effectiveness of the proposed approach is verified on benchmark functional datasets in various domains.\n\n\"",
        "title: \"On the Connectivity of Multi-layered Networks: Models, Measures and Optimal Control\" with abstract: \"Networks appear naturally in many high-impact real-world applications. In an increasingly connected and coupled world, the networks arising from many application domains are often collected from different channels, forming the so-called multi-layered networks, such as cyber-physical systems, organization-level collaboration platforms, critical infrastructure networks and many more. Compared with single-layered networks, multi-layered networks are more vulnerable as even a small disturbance on one supporting layer/network might cause a ripple effect to all the dependent layers, leading to a catastrophic/cascading failure of the entire system. The state-of-the-art has been largely focusing on modeling and manipulating the cascading effect of two-layered interdependent network systems for some specific type of network connectivity measure. This paper generalizes the challenge to multiple dimensions. First, we propose a new data model for multi-layered networks MULAN, which admits an arbitrary number of layers with a much more flexible dependency structure among different layers, beyond the current pair-wise dependency. Second, we unify a wide range of classic network connectivity measures SUBLINE. Third, we show that for any connectivity measure in the SUBLINE family, it enjoys the diminishing returns property which in turn lends itself to a family of provable near-optimal control algorithms with linear complexity. Finally, we conduct extensive empirical evaluations on real network data, to validate the effectiveness of the proposed algorithms.\"",
        "title: \"GenDeR: A Generic Diversified Ranking Algorithm.\" with abstract: \"Diversified ranking is a fundamental task in machine learning. It is broadly applicable in many real world problems, e.g., information retrieval, team assembling, product search, etc. In this paper, we consider a generic setting where we aim to diversify the top-k ranking list based on an arbitrary relevance function and an arbitrary similarity function among all the examples. We formulate it as an optimization problem and show that in general it is NP-hard. Then, we show that for a large volume of the parameter space, the proposed objective function enjoys the diminishing returns property, which enables us to design a scalable, greedy algorithm to find the near-optimal solution. Experimental results on real data sets demonstrate the effectiveness of the proposed algorithm.\"",
        "title: \"Feature co-shrinking for co-clustering.\" with abstract: \"\u2022We propose a novel non-negative matrix tri-factorization model based on cosparsity regularization to enable the co-feature-selection for co-clustering. It aims to learn the inter-correlation among the multi-way features while co-shrinking the irrelevant ones by encouraging the co-sparsity of the model parameters.\u2022We propose an efficient algorithm to solve the non-smooth optimization problem. It works in an iteratively update fashion, and is guaranteed to converge.\u2022Experimental results on various data sets show the effectiveness of the proposed approach.\"",
        "1 is \"Approximating the Expansion Profile and Almost Optimal Local Graph Clustering\", 2 is \"Urban sensing systems: opportunistic or participatory?\".",
        "\nGiven above information, for an author who has written the paper with the title \"GenDeR: A Generic Diversified Ranking Algorithm.\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0142": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Fast and Flexible Top-k Similarity Search on Large Networks':",
        "title: \"Pseudo relevance feedback based on iterative probabilistic one-class SVMs in web image retrieval\" with abstract: \"To improve the precision of top-ranked images returned by a web image search engine, we propose in this paper a novel pseudo relevance feedback method named iterative probabilistic one-class SVMs to re-rank the retrieved images. By assuming that most top-ranked images are relevant to the query, we iteratively train one-class SVMs, and convert the outputs to probabilities so as to combine the decision from different image representation. The effectiveness of our method is validated by systematic experiments even if the assumption is not well satisfied.\"",
        "title: \"Structured Low-Rank Matrix Factorization with Missing and Grossly Corrupted Observations.\" with abstract: \"  Recovering low-rank and sparse matrices from incomplete or corrupted observations is an important problem in machine learning, statistics, bioinformatics, computer vision, as well as signal and image processing. In theory, this problem can be solved by the natural convex joint/mixed relaxations (i.e., l_{1}-norm and trace norm) under certain conditions. However, all current provable algorithms suffer from superlinear per-iteration cost, which severely limits their applicability to large-scale problems. In this paper, we propose a scalable, provable structured low-rank matrix factorization method to recover low-rank and sparse matrices from missing and grossly corrupted data, i.e., robust matrix completion (RMC) problems, or incomplete and grossly corrupted measurements, i.e., compressive principal component pursuit (CPCP) problems. Specifically, we first present two small-scale matrix trace norm regularized bilinear structured factorization models for RMC and CPCP problems, in which repetitively calculating SVD of a large-scale matrix is replaced by updating two much smaller factor matrices. Then, we apply the alternating direction method of multipliers (ADMM) to efficiently solve the RMC problems. Finally, we provide the convergence analysis of our algorithm, and extend it to address general CPCP problems. Experimental results verified both the efficiency and effectiveness of our method compared with the state-of-the-art methods. \"",
        "title: \"Joint voting prediction for questions and answers in CQA\" with abstract: \"Community Question Answering (CQA) sites have become valuable repositories that host a massive volume of human knowledge. How can we detect a high-value answer which clears the doubts of many users? Can we tell the user if the question s/he is posting would attract a good answer? In this paper, we aim to answer these questions from the perspective of the voting outcome by the site users. Our key observation is that the voting score of an answer is strongly positively correlated with that of its question, and such correlation could be in turn used to boost the prediction performance. Armed with this observation, we propose a family of algorithms to jointly predict the voting scores of questions and answers soon after they are posted in the CQA sites. Experimental evaluations demonstrate the effectiveness of our approaches.\"",
        "title: \"Manifold-ranking-based keyword propagation for image retrieval\" with abstract: \"A novel keyword propagation method is proposed for image retrieval based on a recently developed manifold-ranking algorithm. In contrast to existing methods which train a binary classifier for each keyword, our keyword model is constructed in a straight forward manner by exploring the relationship among all images in the feature space in the learning stage. In relevance feedback, the feedback information can be naturally incorporated to refine the retrieval result by additional propagation processes. In order to speed up the convergence of the query concept, we adopt two active learning schemes to select images during relevance feedback. Furthermore, by means of keyword model update, the system can be self-improved constantly. The updating procedure can be performed online during relevance feedback without extra offline training. Systematic experiments on a general-purpose image database consisting of 5000 Corel images validate the effectiveness of the proposed method.\"",
        "title: \"A Local Algorithm for Structure-Preserving Graph Cut\" with abstract: \"Nowadays, large-scale graph data is being generated in a variety of real-world applications, from social networks to co-authorship networks, from protein-protein interaction networks to road traffic networks. Many existing works on graph mining focus on the vertices and edges, with the first-order Markov chain as the underlying model. They fail to explore the high-order network structures, which are of key importance in many high impact domains. For example, in bank customer personally identifiable information (PII) networks, the star structures often correspond to a set of synthetic identities; in financial transaction networks, the loop structures may indicate the existence of money laundering. In this paper, we focus on mining user-specified high-order network structures and aim to find a structure-rich subgraph which does not break many such structures by separating the subgraph from the rest. A key challenge associated with finding a structure-rich subgraph is the prohibitive computational cost. To address this problem, inspired by the family of local graph clustering algorithms for efficiently identifying a low-conductance cut without exploring the entire graph, we propose to generalize the key idea to model high-order network structures. In particular, we start with a generic definition of high-order conductance, and define the high-order diffusion core, which is based on a high-order random walk induced by user-specified high-order network structure. Then we propose a novel High-Order Structure-Preserving LOcal Cut (HOSPLOC) algorithm, which runs in polylogarithmic time with respect to the number of edges in the graph. It starts with a seed vertex and iteratively explores its neighborhood until a subgraph with a small high-order conductance is found. Furthermore, we analyze its performance in terms of both effectiveness and efficiency. The experimental results on both synthetic graphs and real graphs demonstrate the effectiveness and efficiency of our proposed HOSPLOC algorithm.\"",
        "1 is \"DEX: Deep EXpectation of apparent age from a single image\", 2 is \"Modeling dynamic behavior in large evolving graphs\".",
        "\nGiven above information, for an author who has written the paper with the title \"Fast and Flexible Top-k Similarity Search on Large Networks\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0143": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'About the lossless reduction of the minimal generator family of a context':",
        "title: \"Constructing Iceberg Lattices from Frequent Closures Using Generators\" with abstract: \"Frequent closures (FCIs) and generators (FGs) as well as the precedence relation on FCIs are key components in the definition of a variety of association rule bases. Although their joint computation has been studied in concept analysis, no scalable algorithm exists for the task at present. We propose here to reverse a method from the latter field using a fundamental property of hypergraph theory. The goal is to extract the precedence relation from a more common mining output, i.e. closures and generators. The resulting order computation algorithm proves to be highly efficient, benefiting from peculiarities of generator families in typical mining datasets. Due to its genericity, the new algorithm fits an arbitrary FCI/FG-miner.\"",
        "title: \"Generating frequent itemsets incrementally: two novel approaches based on Galois lattice theory\" with abstract: \"Galois (concept) lattice theory has been successfully applied in data mining for the resolution of the association rule problem. In particular, structural results about lattices have been used in the design of efficient procedures for mining the frequent patterns (itemsets) in transaction databases. Since such databases are often dynamic, we propose a detailed study of the incremental aspects in lattice construction to support effective procedures for incremental mining of frequent closed itemsets (FCIs). Based on a set of descriptive results about lattice substructures involved in incremental updates, the paper presents a novel algorithm for lattice construction that explores only limited parts of a lattice for updating. Two new methods for incremental FCI mining are studied: the first inherits its extensive search strategy from a classical lattice method, whereas the second applies the new lattice construction strategy to the itemset mining context. Unlike batch techniques based on FCIs, both methods avoid rebuilding the FCI family from scratch whenever new transactions are added to the database and/or when the minimal support is changed.\"",
        "title: \"T-GOWler: Discovering Generalized Process Models Within Texts.\" with abstract: \"Contemporary workflow management systems are driven by explicit process models specifying the interdependencies between tasks. Creating these models is a challenging and timeconsuming task. Existing approaches to mining concrete workflows into models tackle design aspects related to the diverging abstraction levels of the tasks. Concrete workflow logs represent tasks and cases of concrete events-partially or totally ordered-grounding hidden multilevel (abstract) semantics and contexts. Relevant generalized events could be rediscovered within these processes. We propose, in this article, an ontology-based workflow mining systemto generate patterns fromsequences of events that are themselves extracted from texts. Our system T-GOWler (Generalized Ontology-basedWorkfLow minER within Texts) is based on two ontology-basedmodules: a workflow extractor and a patternminer. To this end, it uses two different ontologies: a domain one (to support workflow extraction from texts) and a processual one (to mine generalized patterns from extracted workflows).\"",
        "title: \"On the Assessment of Concept Relevance in FCA-Based Ontology Restructuring\" with abstract: \"Along their lif-cycle, Ontologies typically undergo a number of structural operations that might deplete their structural quality. Ontology restructuring is a process of improving that quality by reconsidering the way the knowledge is spread across the class and property hierarchies. This often leads to the discovery of new abstractions whose relevance to the ontology must be assessed. We address the relevance assessment within a context where it is crucial: Our restructuring method performs a enhanced concept analysis on the initial ontology that outputs all abstractions of a predefined language. Thus, the key step is the selection of the abstractions to include into the restructured ontology. We propose a set of relevance metrics and a rule-based algorithm for combining them into a single filtering criterion. Their effectiveness is evaluated using a collection of ontologies that have been analyzed beforehand to provide ground truth.\"",
        "title: \"Formal Concept Analysis for Knowledge Discovery and Data Mining: The New Challenges\" with abstract: \"Data mining (DM) is the extraction of regularities from raw data, which are further transformed within the wider process of knowledge discovery in databases (KDD) into non-trivial facts intended to support decision making. Formal concept analysis (FCA) offers an appropriate framework for KDD, whereby our focus here is on its potential for DM support. A variety of mining methods powered by FCA have been published and the figures grow steadily, especially in the association rule mining (ARM) field. However, an analysis of current ARM practices suggests the impact of FCA has not reached its limits, i.e., appropriate FCA-based techniques could successfully apply in a larger set of situations. As a first step in the projected FCA expansion, we discuss the existing ARM methods, provide a set of guidelines for the design of novel ones, and list some open algorithmic issues on the FCA side. As an illustration, we propose two on-line methods computing the minimal generators of a closure system.\"",
        "1 is \"Frequent Itemset Mining from Databases Including One Evidential Attribute\", 2 is \"Developing a robust part-of-speech tagger for biomedical text\".",
        "\nGiven above information, for an author who has written the paper with the title \"About the lossless reduction of the minimal generator family of a context\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0144": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'On The Discovery of Semantically Enhanced Sequential Patterns':",
        "title: \"Distributed and Parallel Computation of the Canonical Direct Basis.\" with abstract: \"Mining association rules, including implications, is an important topic in Knowledge Discovery research area and in Formal Concept Analysis (FCA). In this paper, we present a novel algorithm that computes in a parallel way the canonical direct unit basis of a formal context in FCA. To that end, the algorithm first performs a horizontal split of the initial context into subcontexts and then exploits the notion of minimal dual transversal to merge the canonical direct unit bases generated from subcontexts.\"",
        "title: \"Group disappearance in social networks with communities\" with abstract: \"The purpose of this paper is to handle the disappearance of a group of nodes in a social network. The quality of the information flow is used as a key performance indicator to conduct network changes after group disappearance. Nodes as well as node sets are first classified into categories (critical and non-critical nodes, and scattered, contiguous and hybrid groups) and then analyzed according to two distinct perspectives: the network as a whole or its identified communities. Finally, algorithms are devised to manage group disappearance according to different cases. New links are added in a parsimonious way and a possible substitute for a leaving group is found based on the adage \u201cbirds of a feather flock together\u201d and the homophily principle. This means that new links (e.g., relationships) and a potential substitute are found only between individuals that share common characteristics such as beliefs, values, and education, i.e., individuals that are more likely neighbors of the leaving node or group. To validate our approach, an empirical study is conducted using various kinds of data sets and a set of criteria. The results show the benefits of our solution in terms of response time, number of added links and metrics of the overall network topology.\"",
        "title: \"Conceptual modeling for data and knowledge management\" with abstract: \"In order to exploit knowledge embedded in databases and to migrate from data to knowledge management environments, conceptual modeling languages must offer more expressiveness than traditional modeling languages. This paper proposes the conceptual graph formalism as such a modeling language. It shows through an example and a comparison with Telos, a semantically rich knowledge modeling language, that it is suited for that purpose. The conceptual graph formalism offers simplicity of use through its graphical components and small set of constructs and operators. It allows easy migration from database to knowledge base environments. Thus, this paper advocates its use. (C) 2000 Elsevier Science B.V. All rights reserved.\"",
        "title: \"Formal Concept Analysis for Knowledge Discovery and Data Mining: The New Challenges\" with abstract: \"Data mining (DM) is the extraction of regularities from raw data, which are further transformed within the wider process of knowledge discovery in databases (KDD) into non-trivial facts intended to support decision making. Formal concept analysis (FCA) offers an appropriate framework for KDD, whereby our focus here is on its potential for DM support. A variety of mining methods powered by FCA have been published and the figures grow steadily, especially in the association rule mining (ARM) field. However, an analysis of current ARM practices suggests the impact of FCA has not reached its limits, i.e., appropriate FCA-based techniques could successfully apply in a larger set of situations. As a first step in the projected FCA expansion, we discuss the existing ARM methods, provide a set of guidelines for the design of novel ones, and list some open algorithmic issues on the FCA side. As an illustration, we propose two on-line methods computing the minimal generators of a closure system.\"",
        "title: \"Computing Implications with Negation from a Formal Context\" with abstract: \"The objective of this article is to define an approach towards generating implications with (or without) negation when only a formal context K = (G, M, I) is provided. To that end, we define a two-step procedure which first (i) computes implications whose premise is a key in the context K | $\\tilde{\\rm K}$ representing the apposition of the context K and its complementary $\\tilde{\\rm K}$ with attributes in $\\tilde{\\rm M}$ (negative attributes), and then (ii) uses an inference axiom we have defined to produce the whole set of implications.\"",
        "1 is \"Improving pattern quality in web usage mining by using semantic information\", 2 is \"Semantic Query Optimization for Object Databases\".",
        "\nGiven above information, for an author who has written the paper with the title \"On The Discovery of Semantically Enhanced Sequential Patterns\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0145": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Finding large degree-anonymous subgraphs is hard.':",
        "title: \"Satisfactory graph partition, variants, and generalizations\" with abstract: \"The Satisfactory Partition problem asks for deciding if a given graph has a partition of its vertex set into two nonempty parts such that each vertex has at least as many neighbors in its part as in the other part. This problem was introduced by Gerber and Kobler [M. Gerber, D. Kobler, Algorithmic approach to the satisfactory graph partitioning problem, European Journal of Operational Research 125 (2000) 283\u2013291] and studied further by other authors. In this paper we first review some applications and related problems. Then, we survey structural, complexity, and approximation results obtained for Satisfactory Partition and for some of its variants and generalizations. A list of open questions concludes this survey.\"",
        "title: \"On the Complexity Landscape of the Domination Chain.\" with abstract: \"In this paper, we survey and supplement the complexity landscape of the domination chain parameters as a whole, including classifications according to approximability and parameterised complexity. Moreover, we provide clear pointers to yet open questions. As this posed the majority of hitherto unsettled problems, we focus on Upper Irredundance and Lower Irredundance that correspond to finding the largest irredundant set and resp. the smallest maximal irredundant set. The problems are proved NP-hard even for planar cubic graphs. While Lower Irredundance is proved not $$c\\\\log n$$-approximable in polynomial time unless $$\\\\mathrm {NP}\\\\subseteq \\\\mathrm {DTIME}n^{\\\\log \\\\log n}$$, no such result is known for Upper Irredundance. Their complementary versions are constant-factor approximable in polynomial time. All these four versions are APX-hard even on cubic graphs.\"",
        "title: \"An efficient implementation for the 0-1 multi-objective Knapsack problem\" with abstract: \"In this paper, we present an approach, based on dynamic programming, for solving 0-1 multi-objective knapsack problems. The main idea of the approach relies on the use of several complementary dominance relations to discard partial solutions that cannot lead to new nondominated criterion vectors. This way, we obtain an efficient method that outperforms the existing methods both in terms of CPU time and size of solved instances. Extensive numerical experiments on various types of instances are reported. A comparison with other exact methods is also performed. In addition, for the first time to our knowledge, we present experiments in the three-objective case.\"",
        "title: \"Parameterized complexity of firefighting.\" with abstract: \"The Firefighter problem is to place firefighters on the vertices of a graph to prevent a fire with known starting point from lighting up the entire graph. In each time step, a firefighter may be placed on an unburned vertex, permanently protecting it, and the fire spreads to all neighboring unprotected vertices of burning vertices. The goal is to let as few vertices burn as possible. In this paper, we consider a generalization of this problem, where at each time step b\u2a7e1 firefighters can be deployed. Our results answer several open questions raised by Cai et al. [8]. We show that this problem is W[1]-hard when parameterized by the number of saved vertices, protected vertices, and burned vertices. We also investigate several combined parameterizations for which the problem is fixed-parameter tractable. Some of our algorithms improve on previously known algorithms. We also establish lower bounds to polynomial kernelization.\"",
        "title: \"Min-max and min-max regret versions of combinatorial optimization problems: A survey\" with abstract: \"Min\u2013max and min\u2013max regret criteria are commonly used to define robust solutions. After motivating the use of these criteria, we present general results. Then, we survey complexity results for the min\u2013max and min\u2013max regret versions of some combinatorial optimization problems: shortest path, spanning tree, assignment, min cut, min s\u2013t cut, knapsack. Since most of these problems are NP-hard, we also investigate the approximability of these problems. Furthermore, we present algorithms to solve these problems to optimality.\"",
        "1 is \"Approximability of maximum splitting of k-sets and some other Apx-complete problems\", 2 is \"Kernelization: New Upper and Lower Bound Techniques\".",
        "\nGiven above information, for an author who has written the paper with the title \"Finding large degree-anonymous subgraphs is hard.\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0146": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Bounds on the game transversal number in hypergraphs.':",
        "title: \"Graphs with no induced C4 and 2K2\" with abstract: \"We characterize the structure of graphs containing neither the 4-cycle nor its complement as an induced subgraph. This self-complementary class G of graphs includes split graphs, which are graphs whose vertex set is the union of a clique and an independent set. In the members of G , the number of cliques (as well as the number of maximal independent sets) cannot exceed the number of vertices. Moreover, these graphs are almost extremal to the theorem of Nordhaus and Gaddum (1956).\"",
        "title: \"New models of graph-bin packing.\" with abstract: \"In Bujt\u00e1s et al. (2011) 4 the authors introduced a very general problem called Graph-Bin Packing (GBP). It requires a mapping \u00b5 : V ( G ) \u017a V ( H ) from the vertex set of an input graph G into a fixed host graph H, which, among other conditions, satisfies that for each pair u , v of adjacent vertices the distance of \u00b5 ( u ) and \u00b5 ( v ) in H is between two prescribed bounds. In this paper we propose two online versions of the Graph-Bin Packing problem. In both cases the vertices can arrive in an arbitrary order where each new vertex is adjacent to some of the previous ones. One version is a Maker-Breaker game whose rules are defined by the packing conditions. A subclass of Maker-win input graphs is what we call 'well-packable'; it means that a packing of G is obtained whenever the mapping \u00b5 ( u ) is generated by selecting an arbitrary feasible vertex of the host graph for the next vertex of G in each step. The other model is connected-online packing where we are looking for an online algorithm which can always find a feasible packing. In both models we present some sufficient and some necessary conditions for packability. In the connected-online version we also give bounds on the size of used part of the host graph.\"",
        "title: \"Characterization of (m,1)-transitive and (3,2)-transitive semi-complete directed graphs\" with abstract: \"A directed graph is called ( m , k )-transitive if for every directed path x 0 x 1 \u2026 x m there is a directed path y 0 y 1 \u2026 y k such that x 0 = y 0 , x m = y k , and { y i |0\u2a7d i \u2a7d k } \u2282{ x i |0\u2a7d i \u2a7d m }. We describe the structure of those ( m , 1)-transitive and (3,2)-transitive directed graphs in which each pair of vertices is adjacent by an arc in at least one direction, and present an algorithm with running time O( n 2 ) that tests ( m, k )-transitivity in such graphs on n vertices for every m and k =1, and for m =3 and k =2.\"",
        "title: \"Complexity of most vital nodes for independent set in graphs related to tree structures\" with abstract: \"Given an undirected graph with weights on its vertices, the k most vital nodes independent set problem consists of determining a set of k vertices whose removal results in the greatest decrease in the maximum weight of independent sets. We also consider the complementary problem, minimum node blocker independent set that consists of removing a subset of vertices of minimum size such that the maximum weight of independent sets in the remaining graph is at most a specified value. We show that these problems are NP-hard on bipartite graphs but polynomial-time solvable on unweighted bipartite graphs. Furthermore, these problems are polynomial also on graphs of bounded treewidth and cographs. A result on the non-existence of a ptas is presented, too.\"",
        "title: \"On the Existence and Determination of Satisfactory Partitions in a Graph\" with abstract: \"The SATISFACTORY PARTITION problem consists in deciding if a given graph has a partition of its vertex set into two nonempty sets V-1, V-2 such that for each vertex upsilon, if upsilon is an element of V-i then dv(i) (upsilon) greater than or equal to S(upsilon), where s(upsilon) less than or equal to d(less than or equal to) is a given integer-valued function. This problem was introduced by Gerber and Kobler [EJOR 125 (2000), 283-291] for s = [d/2]. In this paper we study the complexity of this problem for different values of s.\"",
        "1 is \"On diameter 2-critical graphs\", 2 is \"The size of minimum 3-trees\".",
        "\nGiven above information, for an author who has written the paper with the title \"Bounds on the game transversal number in hypergraphs.\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0147": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Bi-objective matchings with the triangle inequality.':",
        "title: \"Efficient computation of the search region in multi-objective optimization.\" with abstract: \"\u2022We define a neighborhood relation on search zones.\u2022We develop an algorithm to update the search region that uses this relation.\u2022We show by numerical experiments that it enables a much more efficient exploration of the search region than earlier approaches.\"",
        "title: \"Discrete representation of the non-dominated set for multi-objective optimization problems using kernels.\" with abstract: \"\u2022We give algorithms which produce discrete representations of the non-dominated set.\u2022These representations satisfy conditions of covering, spacing, and minimum size.\u2022We introduce the concept of kernel for this purpose and study its properties.\u2022In the bi-objective case we give exact and approximate algorithms to build kernels.\u2022For more than two objectives we give some positive and negative results.\"",
        "title: \"On the number of non-dominated points of a multicriteria optimization problem\" with abstract: \"This work proposes an upper bound on the maximal number of non-dominated points of a multicriteria optimization problem. Assuming that the number of values taken on each criterion is known, the criterion space corresponds to a comparability graph or a product of chains. Thus, the upper bound can be interpreted as the stability number of a comparability graph or, equivalently, as the width of a product of chains. Standard approaches or formulas for computing these numbers are impractical. We develop a practical formula which only depends on the number of criteria. We also investigate the tightness of this upper bound and the reduction of this bound when feasible, possibly efficient, solutions are known.\"",
        "title: \"Efficient determination of the k most vital edges for the minimum spanning tree problem\" with abstract: \"We study in this paper the problem of finding in a graph a subset of k edges whose deletion causes the largest increase in the weight of a minimum spanning tree. We propose for this problem an explicit enumeration algorithm whose complexity, when compared to the current best algorithm, is better for general k but very slightly worse for fixed k. More interestingly, unlike in the previous algorithms, we can easily adapt our algorithm so as to transform it into an implicit enumeration algorithm based on a branch and bound scheme. We also propose a mixed integer programming formulation for this problem. Computational results show a clear superiority of the implicit enumeration algorithm both over the explicit enumeration algorithm and the mixed integer program.\"",
        "title: \"A unified framework for multiple criteria auction mechanisms\" with abstract: \"Multi-attribute auctions allow negotiations over multiple attributes besides price. Multiple criteria English reverse auction mechanisms differ regarding the aggregation model used to represent the buyer's preferences and the feedback information provided to bidders during the auction. In this paper, we present a unified framework, called MERA in order to integrate existing mechanisms and guide the design of new ones. Framework MERA both provides an automated buyer agent that manages auctions and a formal model that describes an auction mechanism. This model includes a class of preference relations used to express the buyer's preference relation, a class of request relations used to formulate the feedback, and a class of constraints used to express the initial requirements on the purchased item. We study efficiency for multiple criteria English reverse auction processes, and we show that any process derived from framework MERA is efficient.\"",
        "1 is \"Database merging strategy based on logistic regression\", 2 is \"Approximation results for the weighted P4 partition problem\".",
        "\nGiven above information, for an author who has written the paper with the title \"Bi-objective matchings with the triangle inequality.\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0148": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Uniform intersecting families with covering number four':",
        "title: \"Extremal k-edge-hamiltonian hypergraphs\" with abstract: \"Abstract An r -uniform hypergraph is k -edge-hamiltonian iff it still contains a hamiltonian chain after deleting any k edges of the hypergraph. What is the minimum number of edges in such a hypergraph? We give lower and upper bounds for this question for several values of r and k.\"",
        "title: \"The Erdos-Ko-Rado Theorem for Integer Sequences\" with abstract: \"For positive integers n,q,t we determine the maximum number of integer sequences (a1,...,an) which satisfy 1 ai q for 1 i n, and any two sequences agree in at least t positions. The result gives an armative answer to a conjecture of Frankl and Furedi.\"",
        "title: \"Two-colorings with many monochromatic cliques in both colors\" with abstract: \"Color the edges of the n-vertex complete graph in red and blue, and suppose that red k-cliques are fewer than blue k-cliques. We show that the number of red k-cliques is always less than c\"kn^k, where c\"k@?(0,1) is the unique root of the equation z^k=(1-z)^k+kz(1-z)^k^-^1. On the other hand, we construct a coloring in which there are at least c\"kn^k-O(n^k^-^1) red k-cliques and at least the same number of blue k-cliques.\"",
        "title: \"Linear independence, a unifying approach to shadow theorems.\" with abstract: \"The intersection shadow theorem of Katona is an important tool in extremal set theory. The original proof is purely combinatorial. The aim of the present paper is to show how it is using linear independence latently.\"",
        "title: \"A size-sensitive inequality for cross-intersecting families.\" with abstract: \"Two families A and B of k-subsets of an n-set are called cross-intersecting if AB0 for all AA,BB. Strengthening the classical ErdsKoRado theorem, Pyber proved that |A||B|n1k12 holds for n2k. In the present paper we sharpen this inequality. We prove that assuming |B|n1k1+niki+1 for some 3ik+1 the stronger inequality |A||B|(n1k1+niki+1)(n1k1nik1) holds. These inequalities are best possible. We also present a new short proof of Pybers inequality and a short computation-free proof of an inequality due to Frankl and Tokushige (1992).\"",
        "1 is \"The pagenumber of genus g graphs is O(g)\", 2 is \"Perfect matching in 3 uniform hypergraphs with large vertex degree\".",
        "\nGiven above information, for an author who has written the paper with the title \"Uniform intersecting families with covering number four\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0149": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'TWOPRIME: A Fast Stream Ciphering Algorithm':",
        "title: \"Authentication Schemes from Highly Nonlinear Functions\" with abstract: \"We construct two families of authentication schemes using highly nonlinear functions on finite fields of characteristic 2. This leads to improvements on an earlier construction by Ding and Niederreiter if one chooses, for instance, an almost bent function as the highly nonlinear function\"",
        "title: \"Maximal arcs and extended cyclic codes\" with abstract: \"It is proved that for every \\(d\\ge 2\\) such that \\(d-1\\) divides \\(q-1\\), where q is a power of 2, there exists a Denniston maximal arc A of degree d in \\({\\mathrm {PG}}(2,q)\\), being invariant under a cyclic linear group that fixes one point of A and acts regularly on the set of the remaining points of A. Two alternative proofs are given, one geometric proof based on Abatangelo\u2013Larato\u2019s characterization of Denniston arcs, and a second coding-theoretical proof based on cyclotomy and the link between maximal arcs and two-weight codes.\"",
        "title: \"Cyclotomic Linear Codes Of Order 3\" with abstract: \"In this correspondence, two classes of cyclotomic linear codes over GF (q) of order 3 are constructed and their weight distributions are determined. The two classes are two-weight codes and contain optimal codes. They are not equivalent to irreducible cyclic codes in general when q > 2.\"",
        "title: \"The Bose and Minimum Distance of a Class of BCH Codes\" with abstract: \"Cyclic codes are an interesting class of linear codes due to their efficient encoding and decoding algorithms. Bose-Ray-Chaudhuri-Hocquenghem (BCH) codes form a subclass of cyclic codes and are very important in both theory and practice as they have good error-correcting capability and are widely used in communication systems, storage devices, and consumer electronics. However, the dimension and minimum distance of BCH codes are not known in general. The objective of this paper is to determine the Bose and minimum distances of a class of narrow-sense primitive BCH codes.\"",
        "title: \"Cyclotomic optical orthogonal codes of composite lengths\" with abstract: \"Optical orthogonal codes (OOCs) have applications in optical code-division multiple-access communications systems and other wideband code-division multiple environments. They can also be used to construct protocol sequences for multiuser collision channel without feedback, and constant-weight codes for error detection and correction. We have given a cyclotomic construction of several classes of (2m-1,w,2) OOCs recently. The purpose of this paper is to present five classes of (q-1,w,2) OOCs, and thus five classes of binary constant-weight cyclic codes, where q is a power of an odd prime.\"",
        "1 is \"State complexity of some operations on binary regular languages\", 2 is \"New cyclic difference sets with Singer parameters\".",
        "\nGiven above information, for an author who has written the paper with the title \"TWOPRIME: A Fast Stream Ciphering Algorithm\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0150": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Power-Distortion Optimization For Wireless Image/Video Softcast By Transform Coefficients Energy Modeling With Adaptive Chunk Division':",
        "title: \"Fast Image Super-Resolution via Local Adaptive Gradient Field Sharpening Transform.\" with abstract: \"This paper proposes a single-image super-resolution scheme by introducing a gradient field sharpening transform that converts the blurry gradient field of upsampled low-resolution (LR) image to a much sharper gradient field of original high-resolution (HR) image. Different from the existing methods that need to figure out the whole gradient profile structure and locate the edge points, we derive a...\"",
        "title: \"Fully Connected Network-Based Intra Prediction for Image Coding.\" with abstract: \"This paper proposes a deep learning method for intra prediction. Different from traditional methods utilizing some fixed rules, we propose using a fully connected network to learn an end-to-end mapping from neighboring reconstructed pixels to the current block. In the proposed method, the network is fed by multiple reference lines. Compared with traditional single line-based methods, more contextu...\"",
        "title: \"Compressive gradient based scalable image SoftCast\" with abstract: \"In wireless visual communication systems, it is crucial to effectively utilize channel power and bandwidth in the pursue of optimal performance, and it is worthwhile to adapt the transmission scheme to human vision system (HVS) so as to achieve perceptually appealing results. Inspired by observations that visual quality of an image is closely related to the gradient data, this paper proposes to convey visual information by random projection measurements of image gradients in an analog framework. Since HVS is more sensitive to luminance variations of image contents, which are contained in the gradient data, the proposed scheme achieves better perceptual quality than conventional analog uncoded schemes like SoftCast. Besides, the gradient transform removes the low and medium frequency components of the image hence substantially reduces the power of the signal transmitted in the analog channel, thus evidently improves the power-distortion performance of the system. Furthermore, by applying random projection to the gradients, the number of transmitted data can be adjusted according to bandwidth conditions. Another contribution of this paper is to develop an effective optimization scheme for the compressive gradient based reconstruction problem. Experimental results validate the effectiveness of the proposed transmission and reconstruction scheme under different channel signal-to-noise ratio and bandwidth conditions.\"",
        "title: \"WaveCast: Wavelet based wireless video broadcast using lossy transmission\" with abstract: \"Wireless video broadcasting is a popular application of mobile network. However, the traditional approaches have limited supports to the accommodation of users with diverse channel conditions. The newly emerged Softcast approach provides smooth multicast performance but is not very efficient in inter frame compression. In this work, we propose a new video multicast approach: WaveCast. Different from softcast, WaveCast utilizes motion compensated temporal filter (MCTF) to exploit inter frame redundancy, and utilizes conventional framework to transmit motion information such that the MVs can be reconstructed losslessly. Meanwhile, WaveCast transmits the transform coefficients in lossy mode and performs gracefully in multicast. In experiments, Wave-Cast outperforms softcast 2dB in video PSNR at low channel SNR, and outperforms H.264 based framework up to 8dB in broadcast.\"",
        "title: \"Hybridcast: A wireless image/video SoftCast scheme using layered representation and hybrid digital-analog modulation\" with abstract: \"The recently proposed SoftCast scheme employs analog-like transmission for wireless visual communication, providing graceful reconstruction quality degradation for drastically changing channel conditions. However, the transmission in SoftCast is not always efficient in terms of power usage. In this paper, we propose a wireless image/video SoftCast scheme which employs layered representation with hybrid digital-analog modulation. In this scheme, a coarse approximation of the image is coded in a base layer in digital ways, while the residual image details are delivered in an enhancement layer in analog-like way. The outputs from the two layers are superimposed for transmission, using a hybrid digital-analog modulation scheme. Since a major part of the signal is handled by the base layer, the power efficiency of the SoftCast layer is significantly improved. Experimental results show that the proposed scheme outperforms the original SoftCast remarkably, while still preserving the smooth quality degradation characteristic of the SoftCast scheme.\"",
        "1 is \"Image Denoising by Sparse 3-D Transform-Domain Collaborative Filtering\", 2 is \"Low-Complexity Encoder Framework For Window-Level Rate Control Optimization\".",
        "\nGiven above information, for an author who has written the paper with the title \"Power-Distortion Optimization For Wireless Image/Video Softcast By Transform Coefficients Energy Modeling With Adaptive Chunk Division\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0151": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Adaptive Lzma-Based Coding For Screen Content':",
        "title: \"Weighted Rate-Distortion Optimization for Screen Content Coding.\" with abstract: \"Unlike camera-captured video, screen content (SC) often contains a lot of repeating patterns, which makes some blocks used as references much more important than others. However, conventional rate-distortion optimization (RDO) schemes in video coding do not consider the dependence among image blocks, which often leads to a locally optimal parameter selection, especially for SC. In this paper, we p...\"",
        "title: \"Rate-distortion optimization with adaptive weighted distortion in high Efficiency Video Coding\" with abstract: \"This paper presents an adaptive weighted distortion optimization algorithm used in the Rate-Distortion Optimization (RDO) process of the High Efficiency Video Coding (HEVC). RDO is an important tool to improve the coding efficiency. Usually the distortion weights of different color components are equal or predetermined. In this paper, an adaptive weighted distortion optimization algorithm is introduced to improve the coding efficiency. The distortion weight is estimated according to the previous coded pictures belonging to the same temporal level, such that encoding complexity is almost unchanged. With the proposed adaptive weighted distortion optimization method, on average about 3.3% and up to 10.6% bit-saving are obtained based on the latest HEVC reference software, HM-8.0 and the corresponding common test conditions. The proposed algorithm can also be applied to other coding schemes such as H.264/MPEG-4 AVC.\"",
        "title: \"A Cross-Resolution Leaky Prediction Scheme for In-Band Wavelet Video Coding With Spatial Scalability\" with abstract: \"In most existing in-band wavelet video coding schemes, over-complete wavelet transform is used for the motion-compensated temporal filtering (MCTF) of each spatial subband. It can overcome the shift-variance of critical sampling wavelet transform and improve the coding efficiency of the in-band scheme. However, a dilemma exists in the current implementations of in-band MCTF (IBMCTF), which is whether or not to exploit the spatial highpass subbands in motion compensation of the spatial lowpass subband. The absence of the spatial highpass subbands will result in significant quality loss in the reconstructed full-resolution video, whereas the presence of the spatial highpass subbands may bring serious mismatch error in the decoded low-resolution video since the corresponding highpass subbands may be unavailable at the decoder. In this paper, we first analyze the mismatch error propagation in decoding the low-resolution video. Based on our analysis, we then propose a frame-based cross-resolution leaky prediction scheme for IBMCTF. It can make a good tradeoff between alleviating the low-resolution mismatch and improving the full-resolution coding efficiency. Experimental results show that the proposed scheme can dramatically reduce the mismatch error by 0.3-2.5 dB for low resolution, while the performance loss is marginal for high resolution.\"",
        "title: \"Directional filtering transform\" with abstract: \"This paper proposes the directional filtering transform (dFT, in order to distinguish from the common usage on DFT) to better exploit intra-frame correlation in H.264 intra-frame coding. It consists of a directional filtering and an optional DCT transform. In the proposed directional filtering, there are two different approaches. One is the uni-directional filtering (UDF) that is similar to H.264 directional intra prediction. In this approach, only samples from neighboring blocks can be used in prediction. Another is bidirectional filtering (BDF) that exploits the correlations among samples from not only neighboring blocks but also the current block. The prediction structure in this approach is hierarchical multi-layer. In this paper, we present mathematical analyses on UDF and BDF and show the advantage to combine them together. The proposed dFT is integrated into H.264 intra-frame coding too. The preliminary experimental results in H.264 demonstrate its superiority.\"",
        "title: \"Intra-Predictive Transforms for Block-Based Image Coding\" with abstract: \"This paper presents the theory and the design of intra-predictive transforms, which unify the inter-block prediction and block-based transforms in block-based image coding. Motivated by interpreting inter-block prediction as a transform with a larger size, we derive the concept of intra-predictive transforms. Conventional predictions and transforms can be viewed as special cases of intra-predictive transforms. Intra-predictive transforms are able to exploit both inter and intra-block correlations. We derive the tight upper bound of the coding gain of intra-predictive transforms for stationary Gaussian sources. It turns out that the coding gain can be greater than that of conventional transforms. The optimal intra-predictive transform that achieves the upper bound is also derived. We also design a practical intra-predictive transform using frequency-domain prediction that can achieve better performance in image coding while exhibiting low computational complexity. Experimental results confirm the effectiveness of the proposed intra-predictive transforms in block-based image coding systems and show the improvements over the current design.\"",
        "1 is \"Immune K-SVD algorithm for dictionary learning in speech denoising.\", 2 is \"A New Compressive Video Sensing Framework for Mobile Broadcast\".",
        "\nGiven above information, for an author who has written the paper with the title \"Adaptive Lzma-Based Coding For Screen Content\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0152": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'WaveCast: Wavelet based wireless video broadcast using lossy transmission':",
        "title: \"Wyner\u2013Ziv Switching Scheme for Multiple Bit-Rate Video Streaming\" with abstract: \"This paper proposes a Wyner-Ziv (WZ) switching scheme for multiple bit-rate (MBR) video streaming over networks. Identical video content is encoded into a set of normal streams, which are generated by conventional hybrid video coding with multiple bit rates, so that streaming can dynamically switch among these normal streams according to available bandwidth. At encoder side, the WZ codec generates a switching stream by compressing the reconstructed frames of a certain normal stream that will be switched to, no matter which normal stream it switches from. At decoder side, for switching to the same frame, the same WZ switching stream is used to reconstruct the switching-to frame by taking the switching-from frame as the side information. The number of required WZ bits depends on the inherent mutual correlation between two frames switching to and from. Since the WZ switching streams are generated independently of the normal switching-from streams, given normal streams that can switch from any one to another, the proposed scheme reduces the number of switching streams from to . Furthermore, switching streams do not deteriorate the coding efficiency of normal streams when no switching occurs. However a big problem here, similar to requesting bits in distributed video coding, is how many WZ bits should be transmitted when a switching happens because the streaming scenario does not tolerate too much extra delay caused by the requests back and forth. Therefore, a Laplacian model, which is proved in the simplified case, is proposed to characterize the correlation between switching-to and switching-from frames. It can be used to accurately estimate the number of WZ bits at the server side.\"",
        "title: \"Seamless rate adaptation for wireless networking\" with abstract: \"This paper aims at designing a Seamless Rate Adaptation for wireless networking which achieves smooth rate adjustment in a broad dynamic range of channel conditions. Conventional rate adaptation can only achieve a stair-case rate adjustment. Even when combining with hybrid ARQ, it suffers from an irreconcilable conflict between throughput and dynamic range. We tackle this problem from a new perspective by relying on modulation, instead of channel coding, for rate adaptation. We propose rate compatible modulation (RCM), in which modulation signals are incrementally generated from information bits through weighted mapping. Rate adaptation is achieved through varying the number of modulated signals. As more signals are transmitted, information bits gradually accumulate energy. The weights in bit-to-symbol mapping are delicately designed to ensure fine-grained energy accumulation so that smoothness and efficiency can both be achieved. We design and implement a rate adaptation system, called SRA and evaluate its performance through a software radio testbed. Results show that, under highly dynamic channel conditions, SRA achieves over 80% throughput gain over 802.11a adaptive modulation and coding, and achieves 28.8% and 43.8% gain over HARQ systems implemented with Turbo code and Raptor code. We believe that the concept of rate compatible modulation opens up a fresh research avenue toward the wireless rate adaptation problem.\"",
        "title: \"Design and Analysis of Compressive Data Persistence in Large-Scale Wireless Sensor Networks\" with abstract: \"This paper addresses the data persistence problem in wireless sensor networks (WSNs) where static sinks are not present and the sensed data have to be temporarily but resiliently stored in the network. Based on the observation that sensor readings are correlated, we propose compressive data persistence (CDP) scheme that makes use of the compressive sensing (CS) theory. Each sensor node independently computes and stores a random projection of the sensed data, such that a mobile sink can recover the data with high probability after visiting a small and random portion of the network. As a prerequisite of distributed CS encoding, sensor readings from all nodes are disseminated within the network through random walk. Therefore, the CS measurement matrix depends heavily on how the random walk is performed. In this paper, we present an in-depth analysis on the interplay between random walk parameters and sensing data characteristics, and derive the conditions in successful CS data recovery. In addition, we discover that there is a tradeoff between the number of random walk instances and steps in order to achieve the required data persistence performance. Experiments using real sensor data verify that the proposed CDP scheme achieves much lower decoding ratio than the state-of-theart Fountain code based schemes or the decentralized erasure codes based schemes, and demonstrate that there exist energyoptimized random walk parameters for CDP.\"",
        "title: \"Off-Line Motion Description For Fast Video Stream Generation In Mpeg-4 Avc/H.264\" with abstract: \"The rate-distortion optimal mode decision as well as motion estimation adopted in H.264 brings a big challenge to real-time encoding and transcoding duo to the high computation complexity. In this paper, we propose a hierarchical motion description mode I to present the motion data of each macroblock (MB) from coarsely to finely. A preprocessing approach is developed to estimate the motion data for each MB at each quality level with regard to its reference quality, its adjacent MBs and the target bit-rate. The resulting motion data can be coded and stored as metadata in a media file or a stream. Moreover, we propose a method to readily extract the specific motion data from the model for each MB at given bit-rates. Experimental results have shown the effectiveness of our proposed motion description model in terms of coding efficiency as well as fast bit-rate adaptation in comparison with that of H.264.\"",
        "title: \"Compressive cooperation for gaussian half-duplex relay channel.\" with abstract: \"Motivated by the compressive sensing (CS) theory and its close relationship with low-density parity-check code, we propose compressive transmission which utilizes CS as the channel code and directly transmits multi-level CS random projections through amplitude modulation. This article focuses on the compressive cooperation strategies in a relay channel. Four decode-and-forward (DF) strategies, namely receiver diversity, code diversity, successive decoding and concatenated decoding, are analyzed and their achievable rates in a three-terminal half-duplex Gaussian relay channel are quantified. The comparison among the four schemes is made through both numerical calculation and simulation experiments. In addition, we compare compressive cooperation with a separate source channel coding scheme for transmitting sparse sources. Results show that compressive cooperation has great potential in both transmission efficiency and its adaptation capability to channel variations.\"",
        "1 is \"Content adaptive prediction unit size decision algorithm for HEVC intra coding\", 2 is \"Characterization of signals from multiscale edges\".",
        "\nGiven above information, for an author who has written the paper with the title \"WaveCast: Wavelet based wireless video broadcast using lossy transmission\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0153": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'On network bandwidth sharing for transporting rate-adaptive packet video using feedback.':",
        "title: \"Variable Bit-Rate Video Transmission In The Broad-Band Isdn Environment\" with abstract: \"While the ATM-based broadband ISDN network gives the possibility to support source coding at the variable bit rate (VBR), it also benefits from the VBR coding. The advantages of VBR coding include consistent picture quality, bandwidth savings, and delay reduction. However, one of the major drawbacks of ATM networks is the cell loss and consequent aggravated picture degradation in case of VBR coding. Many compensative measures have been proposed recently to make the cell loss subjectively imperceptible. These schemes include the simple ARQ scheme, error concealment by command refreshment, appropriate queuing discipline and priority switching design, layered source coding schemes, and other signal processing techniques. This paper intends to summarize these efforts and, in particular, elaborate on different layered source coding schemes. Four types of signal priority classification schemes are identified: bit-plane separation (BPS), frequency-domain separation (FDS) combined bit-plane-frequency separation (CBFS), and feature plane separation (FPS). Different layered coding techniques are discussed and compared. Some open questions and recommendations are also given for further research and study.\"",
        "title: \"Network-Adaptive Rate Control With Tcp-Friendly Protocol For Multiple Video Objects\" with abstract: \"Delivering visual content over the Internet with acceptable quality is an important and challenging task. In order to maintain good quality of service (QoS) in the current best-effort Internet, bandwidth adaptation to the varying network conditions is desired for streaming video over the internet. In this paper, we propose a new rate control scheme for streaming MPEG4 video containing multiple video objects in conjunction with a TCP-friendly transport protocol that dynamically estimates available network bandwidth on the fly. Furthermore, network packet loss rate is taken into account in rate adaptation so that the end-to-end distortion is minimized. Simulation results demonstrate the effectiveness of our proposed scheme.\"",
        "title: \"Scalable video transport over wireless IP networks\" with abstract: \"There has been great interest in transporting real-time video over wireless IP networks from both industry and academia. Real-time video applications have quality-of-service (QoS) requirements. However, the fluctuations of wireless channel conditions pose many challenges to providing QoS for video transmission over wireless IP networks. It has been shown that scalable video coding and adaptive services are viable solutions under a time-varying wireless environment. We propose an adaptive framework to support quality video communication over wireless IP networks. The adaptive framework includes: (1) scalable video representations, (2) network-aware video applications, and (3) adaptive services. Under this framework, as wireless channel conditions change, the mobile terminal and network elements can scale the video streams and transport the scaled video streams to receivers with acceptable perceptual quality. The key advantages of the adaptive framework are: (1) perceptual quality is degraded gracefully under severe channel conditions; (2) network resources are efficiently utilized; and (3) the resources are shared in a fair manner\"",
        "title: \"Channel-Adaptive Unequal Error Protection For Scalable Video Transmission Over Wireless Channel\" with abstract: \"Scalable video delivery over wireless link is a very challenging task due to the time-varying characteristics of wireless channels. This paper proposes a channel-adaptive error control scheme for efficiently video delivery, which consists of dynamically channel estimation and channel-adaptive Unequal Error Protection (UEP). In our proposed channel-adaptive UEP scheme, a bit allocation algorithm is presented to periodically allocate the available bits among different video layers based on varying channel conditions so as to minimize the end-to-end distortion. Simulation results show that our proposed scheme is efficient under various channel conditions.\"",
        "title: \"On the compression of image based rendering scene\" with abstract: \"In image based rendering (IBR), a 3D scene is recorded through a set of photos, and a novel view is rendered by assembling data from the photo set. Compression is essential to reduce the huge data amount of IBR. We examine three categories of IBR compression algorithms: the block coder, the reference coder and the high dimensional transform (wavelet) coder. It is observed that the block coder consumes the least computation resource, however, its compression ratio is low. The reference coder achieves a good compression ratio with reasonable computation complexity. The high dimensional wavelet coder achieves the best compression ratio, however, it is also the most complex.\"",
        "1 is \"Congestion avoidance in computer networks with a connectionless network layer\", 2 is \"Statistics of video signals for viewphone-type pictures\".",
        "\nGiven above information, for an author who has written the paper with the title \"On network bandwidth sharing for transporting rate-adaptive packet video using feedback.\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0154": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of '1.2 V BiCMOS Sinh-Domain Filters':",
        "title: \"Simple Multi-Function Fractional-Order Filter Designs\" with abstract: \"A multi-function fractional-order filter topology, constructed from 2 MOS transistors only, is introduced in this paper. The required time-constants are realized through the utilization of the small-signal transconductance parameter of the MOS transistor, offering electronic adjustment of the frequency characteristics of the derived filters. The main attractive offered benefit, in comparison with the literature, is the significant reduction of MOS transistor count and, consequently, the reduced power dissipation. The behavior of the filters is evaluated using the Cadence software and MOS transistor models provided by the 0.35\u03bcm Austria Mikro Systeme CMOS process.\"",
        "title: \"Transient and Steady-State Response of a Fractional-Order Dynamic PV Model Under Different Loads.\" with abstract: \"In this paper, a fractional-order dynamic model of the photovoltaic (PV) solar module is introduced. Dynamic modeling of PV solar modules is useful when used in switching circuits and grid-connected situations. The dynamic elements of the proposed model are a fractional-order inductor and capacitor of two independent orders which allow for two extra degrees of freedom over the conventional dynamic model. The step response and transfer function of the load current are investigated for different orders under resistive and supercapacitor loading conditions. Closed-form expressions for the time response of the load current at equal orders of capacitor and inductor are derived. Stability analysis of the load current transfer function is carried out for different orders and loading conditions. The regions for pure real and pure imaginary input admittance scenarios are calculated numerically for both resistive and supercapacitor load cases. It is found that the order of the inductor has a dominant effect on the responses. As a proof of concept, the model is fitted to experimental data to show its flexibility in regenerating the actual response. The fitted fractional-order model response is compared to optimized integer-order ones from literature showing noticeable improvement.\"",
        "title: \"Design of square-root domain filters by substituting the passive elements of the prototype filter by their equivalents\" with abstract: \"A novel technique for designing square-root domain (SRD) filters is introduced in this paper. The concept of the proposed method is based on the substitution of the passive elements of the corresponding prototype filter by their SRD equivalents. The signal processing performed by the proposed SRD equivalents achieves that the voltage at each terminal of the SRD equivalent is the compressed version of the voltage at the corresponding terminal of the passive element, and that the current that flows through the SRD equivalent is the same as that flows through the passive element. The main attractive characteristic of the proposed method is that a quick procedure for designing SRD filters is offered. The validity of the proposed technique was verified by studying the behaviour of a 5th-order SRD low-pass filter. In order to demonstrate the benefits offered by the proposed technique, a SRD leapfrog filter was also designed and its performance is compared with that of the active filter that topologically simulates the same prototype filter. Copyright \u00a9 2007 John Wiley & Sons, Ltd.\"",
        "title: \"Log-domain wave filters\" with abstract: \"A systematic method for designing log-domain wave filters is presented. Wave filters simulate topologically and functionally passive doubly terminated LC ladder prototype filters of low sensitivity. The design in the log-domain is based on a transposition of the signal flow graph (SFG) that corresponds to the wave equivalent of elementary two-port blocks in the linear domain, to the corresponding log-domain SFG. This is achieved by using an appropriate set of complementary operators, in order to preserve the linear operation of the whole circuit. Simulation results of a fifth-order low-pass and a fourth-order bandpass log-domain wave filter are given, using HSPICE. The proposed circuits are suitable for low-voltage operation and in high-frequency applications.\"",
        "title: \"Electronically controlled multiphase sinusoidal oscillators using current amplifiers\" with abstract: \"A novel current-mode multiphase oscillator topology is introduced in this letter. This is realized by employing current amplifiers and only grounded capacitors. Attractive characteristics offered by the new topology are the electronic adjustment of the oscillation frequency, the absence of passive resistors, and the requirement of only grounded capacitors. Comparison with the corresponding already published current follower based structure shows that the proposed topology has better performance in terms of the number of required active elements, the employment of passive resistors, and the ability for electronic adjustment of the oscillation frequency. Copyright \u00a9 2008 John Wiley & Sons, Ltd.\"",
        "1 is \"Graphics recognition - from re-engineering to retrieval\", 2 is \"Diffusion process modeling by using fractional-order models.\".",
        "\nGiven above information, for an author who has written the paper with the title \"1.2 V BiCMOS Sinh-Domain Filters\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0155": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'CMC: a pragmatic approach to model checking real code':",
        "title: \"Model checking large network protocol implementations\" with abstract: \"Network protocols must work. The effects of protocol specification or implementation errors range from reduced performance, to security breaches, to bringing down entire networks. However, network protocols are difficult to test due to the exponential size of the state space they define. Ideally, a protocol implementation must be validated against all possible events (packet arrivals, packet losses, timeouts, etc.) in all possible protocol states. Conventional means of testing can explore only a minute fraction of these possible combinations. This paper focuses on how to effectively find errors in large network protocol implementations using model checking, a formal verification technique. Model checking involves a systematic exploration of the possible states of a system, and is well-suited to finding intricate errors lurking deep in exponential state spaces. Its primary limitation has been the effort needed to use it on software. The primary contribution of this paper are novel techniques that allow us to model check complex, real-world, well-tested protocol implementations with reasonable effort. We have implemented these techniques in CMC, a C model checker [30] and applied the result to the Linux TCP/IP implementation, finding four errors in the protocol implementation.\"",
        "title: \"Model checking system software with CMC\" with abstract: \"Complex systems have errors that involve mishandled corner cases in intricate sequences of events. Conventional testing techniques usually miss these errors. In recent years, formal verification techniques such as [5] have gained popularity in checking a property in all possible behaviors of a system. However, such techniques involve generating an abstract model of the system. Such an abstraction process is unreliable, difficult and miss a lot of implementation errors.CMC is a framework for model checking a broad class of software written in the C programming language. CMC runs the software implementation directly without deriving an abstract model of the code. We used CMC to model check an existing implementation of AODV (Ad Hoc On Demand Distance Vector) routing protocol and found a total of 29 bugs in two implementations [7],[6] of the protocol. One of them is a bug in the actual specification of the AODV protocol [3]. We also used CMC on the IP Fragmentation module in the Linux TCP/IPv4 stack and verified its correctness for up to 4 fragments per packet.\"",
        "title: \"RWset: attacking path explosion in constraint-based test generation\" with abstract: \"Recent work has used variations of symbolic execution to automatically generate high-coverage test inputs [3, 4, 7, 8, 14]. Such tools have demonstrated their ability to find very subtle errors. However, one challenge they all face is how to effectively handle the exponential number of paths in checked code. This paper presents a new technique for reducing the number of traversed code paths by discarding those that must have side-effects identical to some previously explored path. Our results on a mix of open source applications and device drivers show that this (sound) optimization reduces the numbers of paths traversed by several orders of magnitude, often achieving program coverage far out of reach for a standard constraint-based execution system.\"",
        "title: \"Static Analysis versus Software Model Checking for Bug Finding\" with abstract: \"Abstract: This paper describes experiences with software model checking after several years of using static analysis to find errors. We initially thought that the trade-off between the two was clear: static analysis was easy but would mainly find shallow bugs, while model checking would require more work but would be strictly better - it would find more errors, the errors would be deeper, and the approach would be more powerful. These expectations were often wrong.\"",
        "title: \"Static analysis versus model checking for bug finding\" with abstract: \"This talk tries to distill several years of experience using both model checking and static analysis to find errors in large software systems. We initially thought that the tradeoffs between the two was clear: static analysis was easy but would mainly find shallow bugs, while model checking would require more work but would be strictly better -- it would find more errors, the errors would be deeper and the approach would be more powerful. These expectations were often wrong. This talk will describe some of the sharper tradeoffs between the two, as well as a detailed discussion of one domain -- finding errors in file systems code -- where model checking seems to work very well.\"",
        "1 is \"Auto-parallelizing stateful distributed streaming applications\", 2 is \"Factor graphs and the sum-product algorithm\".",
        "\nGiven above information, for an author who has written the paper with the title \"CMC: a pragmatic approach to model checking real code\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0156": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'A rule engine for relevance assessment in a contextualized information delivery system':",
        "title: \"On the relationship between workflow models and document types\" with abstract: \"The best practice in information system development is to model the business processes that have to be supported and the database of the information system separately. This is inefficient because they are closely related. Therefore we present a framework in which it is possible to derive one from the other. To this end we introduce a special class of Petri nets, called Jackson nets, to model the business processes, and a document type, called Jackson types, to model the database. We show that there is a one-to-one correspondence between Jackson nets and Jackson types. We illustrate the use of the framework by an example.\"",
        "title: \"Towards a Calculus for Collection-Oriented Scientific Workflows with Side Effects\" with abstract: \"In this paper we propose a calculus that can be used to describe the semantics of collection-oriented scientific workflow systems such as the Taverna workbench. Typically such systems focus on the specification and execution of workflows with a relatively simple control flow and a more complex data flow that involves large nested collections of data. An essential operation in such workflows is the instantiation of a certain nested workflow for each element of a collection. We argue that if such workflows call external services, their semantics must be described not only in terms of input-output behavior but also take side effects into account. Based on this assumption a trace semantics is defined that corresponds to the observational equivalence of two workflow specifications. We show that under such a semantics a relatively small calculus with a structural semantics can be defined and used to describe such workflows. This is demonstrated by giving a translation of Taverna workflows in terms of this calculus.\"",
        "title: \"PG-Keys: Keys for Property Graphs\" with abstract: \"ABSTRACTWe report on a community effort between industry and academia to shape the future of property graph constraints. The standardization for a property graph query language is currently underway through the ISO Graph Query Language (GQL) project. Our position is that this project should pay close attention to schemas and constraints, and should focus next on key constraints. The main purposes of keys are enforcing data integrity and allowing the referencing and identifying of objects. Motivated by use cases from our industry partners, we argue that key constraints should be able to have different modes, which are combinations of basic restriction that require the key to be exclusive, mandatory, and singleton. Moreover, keys should be applicable to nodes, edges, and properties since these all can represent valid real-life entities. Our result is PG-Keys, a flexible and powerful framework for defining key constraints, which fulfills the above goals. PG-Keys is a design by the Linked Data Benchmark Council's Property Graph Schema Working Group, consisting of members from industry, academia, and ISO GQL standards group, intending to bring the best of all worlds to property graph practitioners. PG-Keys aims to guide the evolution of the standardization efforts towards making systems more useful, powerful, and expressive.\"",
        "title: \"Report from the first workshop on scalable workflow enactment engines and technology (SWEET'12)\" with abstract: \"This report summarizes the presentations and discussions of SWEET 2012, the First InternationalWorkshop on ScalableWorkflow Enactment Engines and Technologies. SWEET was held in conjunction with the 2012 SIGMOD conference in Scottsdale, Arizona, USA on May 20th, 2012. The goal of the workshop was to bring together researchers and practitioners to explore the state of the art in workflow-based programming for data-intensive applications, and the potential of cloud-based computing in this area. The program featured two very well attended invited talks by Pawel Garbacki from Google and Jimmy Lin from the University of Maryland, on leave at Twitter at the time, as well as a tutorial on Oozie, Yahoo's workflow engine based on Hadoop, by Mohammad Islam from Yahoo/Cloudera.\"",
        "title: \"On generating *-sound nets with substitution\" with abstract: \"We present a method for hierarchically generating sound workflow nets by substitution of nets with multiple inputs and outputs. We show that this method is correct and generalizes the class of nets generated by other hierarchical approaches. The method involves a new notion of soundness which is preserved by the generalized type of substitution that is presented in this paper. We show that this notion is better suited than @?-soundness for use with the presented type of generalized substitution, since @?-soundness is not preserved by it. It is moreover shown that it is in some sense the optimal notion of soundness for the purpose of generating sound nets by the presented type of substitution.\"",
        "1 is \"What is Twitter, a social network or a news media?\", 2 is \"A probabilistic computational model of cross-situational word learning.\".",
        "\nGiven above information, for an author who has written the paper with the title \"A rule engine for relevance assessment in a contextualized information delivery system\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0157": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Music2Share - Copyright-Compliant Music Sharing in P2P Systems':",
        "title: \"When Game Becomes Life: The Creators and Spectators of Online Game Replays and Live Streaming.\" with abstract: \"Online gaming franchises such as World of Tanks, Defense of the Ancients, and StarCraft have attracted hundreds of millions of users who, apart from playing the game, also socialize with each other through gaming and viewing gamecasts. As a form of User Generated Content (UGC), gamecasts play an important role in user entertainment and gamer education. They deserve the attention of both industrial partners and the academic communities, corresponding to the large amount of revenue involved and the interesting research problems associated with UGC sites and social networks. Although previous work has put much effort into analyzing general UGC sites such as YouTube, relatively little is known about the gamecast sharing sites. In this work, we provide the first comprehensive study of gamecast sharing sites, including commercial streaming-based sites such as Amazon\u2019s Twitch.tv and community-maintained replay-based sites such as WoTreplays. We collect and share a novel dataset on WoTreplays that includes more than 380,000 game replays, shared by more than 60,000 creators with more than 1.9 million gamers. Together with an earlier published dataset on Twitch.tv, we investigate basic characteristics of gamecast sharing sites, and we analyze the activities of their creators and spectators. Among our results, we find that (i) WoTreplays and Twitch.tv are both fast-consumed repositories, with millions of gamecasts being uploaded, viewed, and soon forgotten; (ii) both the gamecasts and the creators exhibit highly skewed popularity, with a significant heavy tail phenomenon; and (iii) the upload and download preferences of creators and spectators are different: while the creators emphasize their individual skills, the spectators appreciate team-wise tactics. Our findings provide important knowledge for infrastructure and service improvement, for example, in the design of proper resource allocation mechanisms that consider future gamecasting and in the tuning of incentive policies that further help player retention.\"",
        "title: \"Reducing the history in decentralized interaction-based reputation systems\" with abstract: \"In decentralized interaction-based reputation systems, nodes store information about the past interactions of other nodes. Based on this information, they compute reputations in order to take decisions about future interactions. Computing the reputations with the complete history of interactions is inefficient due to its resource requirements. Furthermore, the complete history of interactions accumulates old information, which may impede the nodes from capturing the dynamic behavior of the system when computing reputations. In this paper, we propose a scheme for reducing the amount of history maintained in decentralized interaction-based reputation systems based on elements such as the age of nodes, and we explore its effect on the computed reputations showing its effectiveness in both synthetic and real-world graphs.\"",
        "title: \"Correlating Topology and Path Characteristics of Overlay Networks and the Internet\" with abstract: \"Real-world IP applications such as peer-to-peer file-sharing are now able to benefit from network and location awareness. It is therefore crucial to understand the relation between underlay and overlay networks and to characterize the behavior of real users with regard to the Internet. For this purpose, we have designed and implemented MULTI-PROBE, a framework for large-scale P2P file-sharing measurements. Using this framework, we have performed measurements of BitTorrent, which is currently the P2P file sharing network with the largest amount of Internet traffic. We analyze and correlate these measurements to provide new insights into the topology, the connectivity, and the path characteristics of the Internet parts underlying P2P networks, as well as to present unique information on the BitTorrent throughput and connectivity\"",
        "title: \"An Empirical Performance Evaluation of GPU-Enabled Graph-Processing Systems\" with abstract: \"Graph processing is increasingly used in knowledge economies and in science, in advanced marketing, social networking, bioinformatics, etc. A number of graph-processing systems, including the GPU-enabled Medusa and Totem, have been developed recently. Understanding their performance is key to system selection, tuning, and improvement. Previous performance evaluation studies have been conducted for CPU-based graph-processing systems, such as Graph and GraphX. Unlike them, the performance of GPU-enabled systems is still not thoroughly evaluated and compared. To address this gap, we propose an empirical method for evaluating GPU-enabled graph-processing systems, which includes new performance metrics and a selection of new datasets and algorithms. By selecting 9 diverse graphs and 3 typical graph-processing algorithms, we conduct a comparative performance study of 3 GPU-enabled systems, Medusa, Totem, and MapGraph. We present the first comprehensive evaluation of GPU-enabled systems with results giving insight into raw processing power, performance breakdown into core components, scalability, and the impact on performance of system-specific optimization techniques and of the GPU generation. We present and discuss many findings that would benefit users and developers interested in GPU acceleration for graph processing.\"",
        "title: \"The distributed ASCI Supercomputer project\" with abstract: \"The Distributed ASCI Supercomputer (DAS) is a homogeneous wide-area distributed system consisting of four cluster computers at different locations. DAS has been used for research on communication software, parallel languages and programming systems, schedulers, parallel applications, and distributed applications. The paper gives a preview of the most interesting research results obtained so far in the DAS project.\"",
        "1 is \"A Model for Usage Policy-Based Resource Allocation in Grids\", 2 is \"FIRE: FInding Rogue nEtworks\".",
        "\nGiven above information, for an author who has written the paper with the title \"Music2Share - Copyright-Compliant Music Sharing in P2P Systems\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0158": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'HPS-HDS: High Performance Scheduling for Heterogeneous Distributed Systems.':",
        "title: \"When Game Becomes Life: The Creators and Spectators of Online Game Replays and Live Streaming.\" with abstract: \"Online gaming franchises such as World of Tanks, Defense of the Ancients, and StarCraft have attracted hundreds of millions of users who, apart from playing the game, also socialize with each other through gaming and viewing gamecasts. As a form of User Generated Content (UGC), gamecasts play an important role in user entertainment and gamer education. They deserve the attention of both industrial partners and the academic communities, corresponding to the large amount of revenue involved and the interesting research problems associated with UGC sites and social networks. Although previous work has put much effort into analyzing general UGC sites such as YouTube, relatively little is known about the gamecast sharing sites. In this work, we provide the first comprehensive study of gamecast sharing sites, including commercial streaming-based sites such as Amazon\u2019s Twitch.tv and community-maintained replay-based sites such as WoTreplays. We collect and share a novel dataset on WoTreplays that includes more than 380,000 game replays, shared by more than 60,000 creators with more than 1.9 million gamers. Together with an earlier published dataset on Twitch.tv, we investigate basic characteristics of gamecast sharing sites, and we analyze the activities of their creators and spectators. Among our results, we find that (i) WoTreplays and Twitch.tv are both fast-consumed repositories, with millions of gamecasts being uploaded, viewed, and soon forgotten; (ii) both the gamecasts and the creators exhibit highly skewed popularity, with a significant heavy tail phenomenon; and (iii) the upload and download preferences of creators and spectators are different: while the creators emphasize their individual skills, the spectators appreciate team-wise tactics. Our findings provide important knowledge for infrastructure and service improvement, for example, in the design of proper resource allocation mechanisms that consider future gamecasting and in the tuning of incentive policies that further help player retention.\"",
        "title: \"The Failure Trace Archive: Enabling Comparative Analysis of Failures in Diverse Distributed Systems\" with abstract: \"With the increasing functionality and complexity of distributed systems, resource failures are inevitable. While numerous models and algorithms for dealing with failures exist, the lack of public trace data sets and tools has prevented meaningful comparisons. To facilitate the design, validation, and comparison of fault-tolerant models and algorithms, we have created the Failure Trace Archive (FTA) as an online public repository of availability traces taken from diverse parallel and distributed systems. Our main contributions in this study are the following. First, we describe the design of the archive, in particular the rationale of the standard FTA format, and the design of a toolbox that facilitates automated analysis of trace data sets. Second, applying the toolbox, we present a uniform comparative analysis with statistics and models of failures in nine distributed systems. Third, we show how different interpretations of these data sets can result in different conclusions. This emphasizes the critical need for the public availability of trace data and methods for their analysis.\"",
        "title: \"Analyzing Implicit Social Networks In Multiplayer Online Games\" with abstract: \"Understanding the social structures that people implicitly form when playing networked games helps developers create innovative gaming services to benefit both players and operators. But how can we extract and analyze this implicit social structure? The authors' proposed formalism suggests various ways to map interactions to social structure. Applying this formalism to real-world data collected from three game genres reveals the implications of the mappings on in-game and gaming-related services, ranging from network and socially aware player matchmaking to an investigation of social network robustness against player departure.\"",
        "title: \"How are Real Grids Used? The Analysis of Four Grid Traces and Its Implications\" with abstract: \"The Grid computing vision promises to provide the needed platform for a new and more demanding range of applications. For this promise to become true, a number of hurdles, including the design and deployment of adequate resource management and information services, need to be overcome. In this context, understanding the characteristics of real Grid workloads is a crucial step for improving the quality of existing Grid services, and in guiding the design of new solutions. Towards this goal, in this work we present the characteristics of traces of four real Grid environments, namely LCG, Grid3, and TeraGrid, which are among the largest production Grids currently deployed, and the DAS, which is a research Grid. We focus our analysis on virtual organizations, on users, and on individual jobs characteristics. We further attempt to quantify the evolution and the performance of the Grid systems from which our traces originate. Finally, given the scarcity of the information available for analysis purposes, we discuss the requirements of a new format for Grid traces, and we propose the establishment of a virtual center for workload-based Grid benchmarking data: The Grid Workloads Archive.\"",
        "title: \"A model for space-correlated failures in large-scale distributed systems\" with abstract: \"Distributed systems such as grids, peer-to-peer systems, and even Internet DNS servers have grown significantly in size and complexity in the last decade. This rapid growth has allowed distributed systems to serve a large and increasing number of users, but has also made resource and system failures inevitable. Moreover, perhaps as a result of system complexity, in distributed systems a single failure can trigger within a short time span several more failures, forming a group of time-correlated failures. To eliminate or alleviate the significant effects of failures on performance and functionality, the techniques for dealing with failures require good failure models. However, not many such models are available, and the available models are valid for few or even a single distributed system. In contrast, in this work we propose a model that considers groups of time-correlated failures and is valid for many types of distributed systems. Our model includes three components, the group size, the group inter-arrival time, and the resource downtime caused by the group. To validate this model, we use failure traces corresponding to fifteen distributed systems. We find that space-correlated failures are dominant in terms of resource downtime in seven of the fifteen studied systems. For each of these seven systems, we provide a set of model parameters that can be used in research studies or for tuning distributed systems. Last, as a result of our work six of the studied traces have been made available through the Failure Trace Archive (http://fta.inria.fr).\"",
        "1 is \"Who plays, how much, and why? Debunking the stereotypical gamer profile\", 2 is \"Reality mining: sensing complex social systems\".",
        "\nGiven above information, for an author who has written the paper with the title \"HPS-HDS: High Performance Scheduling for Heterogeneous Distributed Systems.\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0159": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Community Detection In Multi-Relational Social Networks':",
        "title: \"hPSD: A Hybrid PU-Learning-Based Spammer Detection Model for Product Reviews.\" with abstract: \"Spammers, who manipulate online reviews to promote or suppress products, are flooding in online commerce. To combat this trend, there has been a great deal of research focused on detecting review spammers, most of which design diversified features and thus develop various classifiers. The widespread growth of crowdsourcing platforms has created large-scale deceptive review writers who behave more like normal users, that the way they can more easily evade detection by the classifiers that are purely based on fixed characteristics. In this paper, we propose a hybrid semisupervised learning model titled hybrid PU-learning-based spammer detection (hPSD) for spammer detection to leverage both the users\u2019 characteristics and the user\u2013product relations. Specifically, the hPSD model can iteratively detect multitype spammers by injecting different positive samples, and allows the construction of classifiers in a semisupervised hybrid learning framework. Comprehensive experiments on movie dataset with shilling injection confirm the superior performance of hPSD over existing baseline methods. The hPSD is then utilized to detect the hidden spammers from real-life Amazon data. A set of spammers and their underlying employers (e.g., book publishers) are successfully discovered and validated. These demonstrate that hPSD meets the real-world application scenarios and can thus effectively detect the potentially deceptive review writers.\"",
        "title: \"Hybrid Collaborative Filtering algorithm for bidirectional Web service recommendation.\" with abstract: \"Web service recommendation has become a hot yet fundamental research topic in service computing. The most popular technique is the Collaborative Filtering (CF) based on a user-item matrix. However, it cannot well capture the relationship between Web services and providers. To address this issue, we first design a cube model to explicitly describe the relationship among providers, consumers and Web services. And then, we present a Standard Deviation based Hybrid Collaborative Filtering (SD-HCF) for Web Service Recommendation (WSRec) and an Inverse consumer Frequency based User Collaborative Filtering (IF-UCF) for Potential Consumers Recommendation (PCRec). Finally, the decision-making process of bidirectional recommendation is provided for both providers and consumers. Sets of experiments are conducted on real-world data provided by Planet-Lab. In the experiment phase, we show how the parameters of SD-HCF impact on the prediction quality as well as demonstrate that the SD-HCF is much better than extant methods on recommendation quality, including the CF based on user, the CF based on item and general HCF. Experimental comparison between IF-UCF and UCF indicates the effectiveness of adding inverse consumer frequency to UCF. \u00a9 2012 Springer-Verlag London.\"",
        "title: \"CAMAS: A cluster-aware multiagent system for attributed graph clustering.\" with abstract: \"Abstract   Attributed graphs describe nodes via attribute vectors and also relationships between different nodes via edges. To partition nodes into clusters with tighter correlations, an effective way is applying clustering techniques on attributed graphs based on various criteria such as node connectivity and/or attribute similarity. Even though clusters typically form around nodes with tight edges and similar attributes, existing methods have only focused on one of these two data modalities. In this paper, we comprehend each node as an autonomous agent and develop an accurate and scalable multiagent system for extracting overlapping clusters in attributed graphs. First, a kernel function with a tunable bandwidth factor  \u03b4  is introduced to measure the influence of each agent, and those agents with highest local influence can be viewed as the \u201cleader\u201d agents. Then, a novel local expansion strategy is proposed, which can be applied by each leader agent to absorb the most relevant followers in the graph. Finally, we design the cluster-aware multiagent system (CAMAS), in which agents communicate with each other freely under an efficient communication mechanism. Using the proposed multiagent system, we are able to uncover the optimal overlapping cluster configuration, i.e. nodes within one cluster are not only connected closely with each other but also with similar attributes. Our method is highly efficient, and the computational time is shown that nearly linearly dependent on the number of edges when  \u03b4  \u2208 [0.5, 1). Finally, applications of the proposed method on a variety of synthetic benchmark graphs and real-life attributed graphs are demonstrated to verify the systematic performance.\"",
        "title: \"A generalized game theoretic framework for mining communities in complex networks.\" with abstract: \"\u2022We introduce a game theoretic framework to model community formation in networks.\u2022We deduce many commonly used objective functions as potential functions in the model.\u2022The proposed framework has been proved to exist fixed (equilibrium) point.\u2022A synchronous learning mechanism has be proposed to reach the local equilibrium.\"",
        "title: \"Dynamic advance reservation for grid system using resource pools\" with abstract: \"Dynamic behavior of resources is a non-negligible feature in grid system, and most research efforts on advance reservation cannot effectively deal with the negative effect resulted from the dynamic feature. In this paper, a new grid system architecture using resource pool is proposed firstly. Theoretical analysis demonstrates that resource pool can well adapt to dynamic behavior of resources. Secondly, Quality of Service (QoS) distance computation method for hybrid variable types is presented. Then, k-set Availability Prediction Admission Control (kAPAC) algorithm is described in detail. Experimental results show that kAPAC can significantly increase success ratio of reservation, resource utilization and stability of grid system.\"",
        "1 is \"A novel and better fitness evaluation for rough set based minimum attribute reduction problem\", 2 is \"Exploring the Capabilities of Mobile Agents in Distributed Data Mining\".",
        "\nGiven above information, for an author who has written the paper with the title \"Community Detection In Multi-Relational Social Networks\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0160": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Mechanisms and Evaluation of Cross-Layer Fault-Tolerance for Supercomputing':",
        "title: \"Analyzing Behavior Specialized Acceleration.\" with abstract: \"Hardware specialization has become a promising paradigm for overcoming the inefficiencies of general purpose microprocessors. Of significant interest are Behavioral Specialized Accelerators (BSAs), which are designed to efficiently execute code with only certain properties, but remain largely configurable or programmable. The most important strength of BSAs -- their ability to target a wide variety of codes -- also makes their interactions and analysis complex, raising the following questions: can multiple BSAs be composed synergistically, what are their interactions with the general purpose core, and what combinations favor which workloads? From a methodological standpoint, BSAs are also challenging, as they each require ISA development, compiler and assembler extensions, and either simulator or RTL models.   To study the potential of BSAs, we propose a novel modeling technique called the Transformable Dependence Graph (TDG) - a higher level alternative to the time-consuming traditional compiler+simulator approach, while still enabling detailed microarchitectural models for both general cores and accelerators. We then propose a multi-BSA organization, called ExoCore, which we model and study using the TDG. A design space exploration reveals that an ExoCore organization can push designs beyond the established energy-performance frontiers for general purpose cores. For example, a 2-wide OOO processor with three BSAs matches the performance of a conventional 6-wide OOO core, has 40% lower area, and is 2.6x more energy efficient.\"",
        "title: \"Universal Mechanisms for Data-Parallel Architectures\" with abstract: \"Data-parallel programs are both growing in importanceand increasing in diversity, resulting in specialized processorstargeted at specific classes of these programs. This paperpresents a classification scheme for data-parallelprogram attributes, and proposes micro-architecturalmechanisms to support applications with diverse behaviorusing a single reconfigurable architecture. We focuson the following four broad kinds of data-parallel programs- DSP/multimedia, scientific, networking, andreal-time graphics workloads. While all of these programsexhibit high computational intensity, coarse-grainregular control behavior, and some regular memory accessbehavior, they show wide variance in the computationrequirements, fine grain control behavior, and the frequencyof other types of memory accesses. Based onthis study of application attributes, this paper proposesa set of general micro-architectural mechanismsthat enable a baseline architecture to be dynamically tailoredto the demands of a particular application. Thesemechanisms provide efficient execution across a spectrumof data-parallel applications and can be applied todiverse architectures ranging from vector cores to conventionalsuperscalar cores. Our results using a baselineTRIPS processor show that the configurability of the architectureto the application demands provides harmonicmean performance improvement of 5%-55% over scalableyet less flexible architectures, and performs competitivelyagainst specialized architectures.\"",
        "title: \"Implementing Signatures for Transactional Memory\" with abstract: \"Transactional Memory (TM) systems must track the read and write sets--items read and written during a transaction--to detect conflicts among concurrent trans- actions. Several TMs use signatures, which summarize unbounded read/write sets in bounded hardware at a per- formance cost of false positives (conflicts detected when none exists). This paper examines different organizations to achieve hardware-efficient and accurate TM signatures. First, we find that implementing each signature with a single k-hash- function Bloom filter (True Bloom signature) is inefficient, as it requires multi-ported SRAMs. Instead, we advocate using k single-hash-function Bloom filters in parallel (Par- allel Bloom signature), using area-efficient single-ported SRAMs. Our formal analysis shows that both organiza- tions perform equally well in theory and our simulation- based evaluation shows this to hold approximately in prac- tice. We also show that by choosing high-quality hash func- tions we can achieve signature designs noticeably more ac- curate than the previously proposed implementations. Fi- nally, we adapt Pagh and Rodler's cuckoo hashing to im- plement Cuckoo-Bloom signatures. While this representa- tion does not support set intersection, it mitigates false pos- itives for the common case of small read/write sets and per- forms like a Bloom filter for large sets.\"",
        "title: \"A general constraint-centric scheduling framework for spatial architectures\" with abstract: \"Specialized execution using spatial architectures provides energy efficient computation, but requires effective algorithms for spatially scheduling the computation. Generally, this has been solved with architecture-specific heuristics, an approach which suffers from poor compiler/architect productivity, lack of insight on optimality, and inhibits migration of techniques between architectures. Our goal is to develop a scheduling framework usable for all spatial architectures. To this end, we expresses spatial scheduling as a constraint satisfaction problem using Integer Linear Programming (ILP). We observe that architecture primitives and scheduler responsibilities can be related through five abstractions: placement of computation, routing of data, managing event timing, managing resource utilization, and forming the optimization objectives. We encode these responsibilities as 20 general ILP constraints, which are used to create schedulers for the disparate TRIPS, DySER, and PLUG architectures. Our results show that a general declarative approach using ILP is implementable, practical, and typically matches or outperforms specialized schedulers.\"",
        "title: \"Performance evaluation of a DySER FPGA prototype system spanning the compiler, microarchitecture, and hardware implementation\" with abstract: \"Specialization and accelerators are being proposed as an effective way to address the slowdown of Dennard scaling. DySER is one such accelerator, which dynamically synthesizes large compound functional units to match program regions, using a co-designed compiler and microarchitecture. We have completed a full prototype implementation of DySER integrated into the OpenSPARC processor (called SPARC-DySER), a co-designed compiler in LLVM, and a detailed performance evaluation on an FPGA system, which runs an Ubuntu Linux distribution and full applications. Through the prototype, this paper evaluates the fundamental principles of DySER acceleration. Our two key findings are: i) the DySER execution model and microarchitecture provides energy efficient speedups and the integration of DySER does not introduce overheads ??? overall, DySER???s performance improvement to OpenSPARC is 6X, consuming only 200mW ; ii) on the compiler side, the DySER compiler is effective at extracting computationally intensive regular and irregular code. For non-computationally intense irregular code, two control flow shapes curtail the compiler???s effectiveness, and we identify potential adaptive mechanisms. Finally, our experience of bringing up an end-to-end prototype of an ISA-exposed accelerator has made clear that two particular artifacts are greatly needed to perform this type of design more quickly and effectively: 1) Open-source implementations of high-performance baseline processors, and 2) Declarative tools for quickly specifying combinations of known compiler transforms.\"",
        "1 is \"Accountable internet protocol (aip)\", 2 is \"An Algebra for Cross-Experiment Performance Analysis\".",
        "\nGiven above information, for an author who has written the paper with the title \"Mechanisms and Evaluation of Cross-Layer Fault-Tolerance for Supercomputing\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0161": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'On the Outage Probability of Device-to-Device-Communication-Enabled Multichannel Cellular Networks: An RSS-Threshold-Based Perspective':",
        "title: \"Learning bundle manifold by double neighborhood graphs\" with abstract: \"In this paper, instead of the ordinary manifold assumption, we introduced the bundle manifold assumption that imagines data points lie on a bundle manifold Under this assumption, we proposed an unsupervised algorithm, named as Bundle Manifold Embedding (BME), to embed the bundle manifold into low dimensional space In BME, we construct two neighborhood graphs that one is used to model the global metric structure in local neighborhood and the other is used to provide the information of subtle structure, and then apply the spectral graph method to obtain the low-dimensional embedding Incorporating some prior information, it is possible to find the subtle structures on bundle manifold in an unsupervised manner Experiments conducted on benchmark datasets demonstrated the feasibility of the proposed BME algorithm, and the difference compared with ISOMAP, LLE and Laplacian Eigenmaps.\"",
        "title: \"2D projective transformation based active shape model for facial feature location.\" with abstract: \"Active shape model statistically represents a shape by a set of well-defined landmark points and can model object variations using principal component analysis. However, the face modeling becomes degenerate when the test images are taken from different viewpoints other than frontal. In this paper, we present a 2D projective transformation based active shape model to address this problem. First, a 2D projective transformation based alignment algorithm is proposed to model pose variations. Then, projective local profiles are calculated, which make the intensity profiles be scale independent. We evaluate our approach on two different datasets containing both frontal facial images and images with a large range of variations in pose and expression. Experimental results demonstrate the efficiency and effectiveness of the proposed approach. \u00a9 2011 IEEE.\"",
        "title: \"Emulating biological strategies for uncontrolled face recognition\" with abstract: \"Face recognition technology is of great significance for applications involving national security and crime prevention. Despite enormous progress in this field, machine-based system is still far from the goal of matching the versatility and reliability of human face recognition. In this paper, we show that a simple system designed by emulating biological strategies of human visual system can largely surpass the state-of-the-art performance on uncontrolled face recognition. In particular, the proposed system integrates dual retinal texture and color features for face representation, an incremental robust discriminant model for high level face coding, and a hierarchical cue-fusion method for similarity qualification. We demonstrate the strength of the system on the large-scale face verification task following the evaluation protocol of the Face Recognition Grand Challenge (FRGC) version 2 Experiment 4. The results are surprisingly well: Its modules significantly outperform their state-of-the-art counterparts, such as Gabor image representation, local binary patterns, and enhanced Fisher linear discriminant model. Furthermore, applying the integrated system to the FRGC version 2 Experiment 4, the verification rate at the false acceptance rate of 0.1 percent reaches to 93.12 percent.\"",
        "title: \"Statistical Color Model Based Adult Video Filter\" with abstract: \"This paper, guided by Statistical Color Models, proposes a real-time Adult Video detector to filter the adult content in the video. A generic color model is constructed by statistical analysis of the sample images containing adult pixels. We fully utilize the video continuity characteristics, i.e. preceding and following N frames considered in the classification. Our method, through experimental, displays a satisfactory performance for detecting adult content. The reminder of the paper addresses the application of real-time adult video filter that blocks adult content from kids.\"",
        "title: \"Support Vector Machines Optimization Based Wavelet Transform Liner Equalizer and Decision Directed Combined Blind Equalization Algorithm.\" with abstract: \"Aiming at the defects of time varying and multipath fading in underwater acoustic channel, support vector machines optimization based wavelet transform liner equalizer and decision directed combined blind equalization algorithm is proposed under the standard of statistical properties. The proposed algorithm improves the convergence speed through normalized orthogonal wavelet transform, and makes the weight vectors converge to global optimal points by using support vector machines. In order to correct signals' phase rotation and reduce steady-state error, the proposed algorithm integrate the advantages of decision directed algorithm and soft decision. Experiment simulations in underwater acoustic channel indicate that the proposed algorithm has ability to improve the convergence rate and to reduce the mean square error. especially to compensate phase rotation.\"",
        "1 is \"Falling asleep with Angry Birds, Facebook and Kindle: a large scale study on mobile application usage\", 2 is \"Stastical Estimation of the Intrinsic Dimensionality of a Noisy Signal Collection\".",
        "\nGiven above information, for an author who has written the paper with the title \"On the Outage Probability of Device-to-Device-Communication-Enabled Multichannel Cellular Networks: An RSS-Threshold-Based Perspective\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0162": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Two web-based approaches for noun sense disambiguation':",
        "title: \"Gated Multimodal Units for Information Fusion.\" with abstract: \"This paper presents a novel model for multimodal learning based on gated neural networks. The Gated Multimodal Unit (GMU) model is intended to be used as an internal unit in a neural network architecture whose purpose is to find an intermediate representation based on a combination of data from different modalities. The GMU learns to decide how modalities influence the activation of the unit using multiplicative gates. It was evaluated on a multilabel scenario for genre classification of movies using the plot and the poster. The GMU improved the macro f-score performance of single-modality approaches and outperformed other fusion strategies, including mixture of experts models. Along with this work, the MM-IMDb dataset is released which, to the best of our knowledge, is the largest publicly available multimodal dataset for genre prediction on movies.\"",
        "title: \"Comparaci\u00f3n de algoritmos de aprendizaje para identificaci\u00f3n del usuario a trav\u00e9s de la voz\" with abstract: \"En este trabajo presentamos una comparaci\u00f3n entre cuatro algoritmos de aprendizaje autom\u00e1tico para identificaci\u00f3n del hablante. El estudio hace hincapi\u00e9 en la simplificaci\u00f3n de la caracterizaci\u00f3n de la se\u00f1al de voz al no usar reconocimiento fon\u00e9tico. Los resultados hasta ahora alcanzados nos brindan elementos para preferir el algoritmo de M\u00e1quinas de Vectores de Soporte (SVM).\"",
        "title: \"Question answering for spanish supported by lexical context annotation\" with abstract: \"This paper describes the prototype developed by the Language Technologies Laboratory at INAOE for Spanish monolingual QA evaluation task at CLEF 2004. Our approach is centered on the use of context at a lexical level in order to identify possible answers to factoid questions. This method is supported by an alternative one based on pattern recognition in order to identify candidate answers to definition questions. We describe the methods applied at different stages of the system and our prototype architecture for question answering. The paper shows and discusses the results we achieved with this approach.\"",
        "title: \"Applying dependency trees and term density for answer selection reinforcement\" with abstract: \"This paper describes the experiments performed for the QA@CLEF- 2006 within the joint participation of the eLing Division at VEng and the Language Technologies Laboratory at INAOE. The aim of these experiments was to observe and quantify the improvements in the final step of the Question Answering prototype when some syntactic features were included into the decision process. In order to reach this goal, a shallow approach to answer ranking based on the term density measure has been integrated into the weighting schema. This approach has shown an interesting improvement against the same prototype without this module. The paper discusses the results achieved, the conclusions and further directions within this research.\"",
        "title: \"INAOE at CLEF 2006: Experiments in Spanish Question Answering.\" with abstract: \"This paper describes the system developed by the Language Technologies Lab at INAOE for the Spanish Question Answering task at CLEF 2006. The presented system is centered in a full data- driven architecture that uses machine learning and text mining techniques to identify the most probable answers to factoid and definition questions respectively. Its major quality is that it mainly relies on the use of lexical information and avoids applying any complex language processing re- source such as named entity classifiers, parsers or ontologies. Experimental results show that the proposed architecture can be a practical solution for monolingual question answering reaching an answer precision as high as 51%.\"",
        "1 is \"Uncovering the Semantics of Wikipedia Pagelinks.\", 2 is \"Optimization models of sound systems using genetic algorithms\".",
        "\nGiven above information, for an author who has written the paper with the title \"Two web-based approaches for noun sense disambiguation\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0163": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'R2D2 at GeoCLEF 2006: a combined approach':",
        "title: \"The UPV at GeoCLEF 2008: The GeoWorSE System.\" with abstract: \"This year our system was complemented with a map-based lter. During the indexing phase, all places are disambiguated and assigned their coordinates on the map. These coordinates are stored in a separate index. The search process is carried out in two phases: in the rst one, we search the collection with the same method applied in 2007, which exploits the expansion of index terms by means of WordNet synonyms and holonyms. The next phase consists in a re-ranking of the results of the previous phase depending on the distance of document toponyms from the toponyms in the query, or depending on the fact that the document contains toponyms that are included in an area dened by the query. The area is calculated from the toponyms in the query and their meronyms. This is the rst attempt to use GeoWordNet, a resource that includes the geographical coordinates of the places listed in WordNet, for the Geographical Information Retrieval task. The results show that map-based ltering allows to improve the results obtained by the base system, based only on the textual information.\"",
        "title: \"Borda-based voting schemes for semantic role labeling\" with abstract: \"In this article, we have studied the possibility of applying Borda and Fuzzy Borda voting schemes to combine semantic role labeling systems. To better select the correct semantic role, among those provided by different experts, we have introduced two measures: the first one calculates the overlap between labeled sentences, whereas the second one adds different scoring levels depending on the verbs that have been parsed.\"",
        "title: \"On the relative hardness of clustering corpora\" with abstract: \"Clustering is often considered the most important unsupervised learning problem and several clustering algorithms have been proposed over the years. Many of these algorithms have been tested on classical clustering corpora such as Reuters and 20 Newsgroups in order to determine their quality. However, up to now the relative hardness of those corpora has not been determined. The relative clustering hardness of a given corpus may be of high interest, since it would help to determine whether the usual corpora used to benchmark the clustering algorithms are hard enough. Moreover, if it is possible to find a set of features involved in the hardness of the clustering task itself, specific clustering techniques may be used instead of general ones in order to improve the quality of the obtained clusters. In this paper, we are presenting a study of the specific feature of the vocabulary overlapping among documents of a given corpus. Our preliminary experiments were carried out on three different corpora: the train and test version of the R8 subset of the Reuters collection and a reduced version of the 20 Newsgroups (Mini20Newsgroups). We figured out that a possible relation between the vocabulary overlapping and the F-Measure may be introduced.\"",
        "title: \"Detecting Deceptive Opinions: Intra And Cross-Domain Classification Using An Efficient Representation\" with abstract: \"Online opinions play an important role for customers and companies because of the increasing use they do to make purchase and business decisions. A consequence of that is the growing tendency to post fake reviews in order to change purchase decisions and opinions about products and services. Therefore, it is really important to filter out deceptive comments from the retrieved opinions. In this paper we propose the character n-grams in tokens, an efficient and effective variant of the traditional character n-grams model, which we use to obtain a low dimensionality representation of opinions. A Support Vector Machines classifier was used to evaluate our proposal on available corpora with reviews of hotels, doctors and restaurants. In order to study the performance of our model, we make experiments with intra and cross-domain cases. The aim of the latter experiment is to evaluate our approach in a realistic cross-domain scenario where deceptive opinions are available in a domain but not in another one. After comparing our method with state-of-the-art ones we may conclude that using character n-grams in tokens allows to obtain competitive results with a low dimensionality representation.\"",
        "title: \"POS tagging in Amazighe using support vector machines and conditional random fields\" with abstract: \"The aim of this paper is to present the first Amazighe POS tagger. Very few linguistic resources have been developed so far for Amazighe and we believe that the development of a POS tagger tool is the first step needed for automatic text processing. The used data have been manually collected and annotated. We have used state-of-art supervised machine learning approaches to build our POS-tagging models. The obtained accuracy achieved 92.58% and we have used the 10-fold technique to further validate our results.\"",
        "1 is \"Subjectivity Word Sense Disambiguation\", 2 is \"A survey of modern authorship attribution methods\".",
        "\nGiven above information, for an author who has written the paper with the title \"R2D2 at GeoCLEF 2006: a combined approach\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0164": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'A mapping between classifiers and training conditions for WSD':",
        "title: \"A text mining approach for definition question answering\" with abstract: \"This paper describes a method for definition question answering based on the use of surface text patterns. The method is specially suited to answer questions about person\u2019s positions and acronym\u2019s descriptions. It considers two main steps. First, it applies a sequence-mining algorithm to discover a set of definition-related text patterns from the Web. Then, using these patterns, it extracts a collection of concept-description pairs from a target document database, and applies the sequence-mining algorithm to determine the most adequate answer to a given question. Experimental results on the Spanish CLEF 2005 data set indicate that this method can be a practical solution for answering this kind of definition questions, reaching a precision as high as 84%.\"",
        "title: \"The Use of Lexical Context in Question Answering for Spanish\" with abstract: \"This paper describes the prototype developed by the Language Technologies Laboratory at INAOE for Spanish monolingual QA evaluation task at CLEF 2004. Our approach is centered in the use of context at a lexical level in order to identify possible answers to factoid questions. Such method is supported by an alternative one based on pattern recognition in order to identify candidate answers to definition questions. The methods applied at different stages of the system and prototype architecture for question answering are described. The paper shows and discusses the results achieved with this approach.\"",
        "title: \"Fusing Affective Dimensions and Audio-Visual Features from Segmented Video for Depression Recognition: INAOE-BUAP's Participation at AVEC'14 Challenge\" with abstract: \"Depression is a disease that affects a considerable portion of the world population. Severe cases of depression interfere with the common live of patients, for those patients a strict monitoring is necessary in order to control the progress of the disease and to prevent undesired side effects. A way to keep track of patients with depression is by means of online monitoring via human-computer-interaction. The AVEC'14 challenge aims at developing technology towards the online monitoring of depression patients. This paper describes an approach to depression recognition from audiovisual information in the context of the AVEC'14 challenge. The proposed method relies on an effective voice segmentation procedure, followed by segment-level feature extraction and aggregation. Finally, a meta-model is trained to fuse mono-modal information. The main novel features of our proposal are that (1) we use affective dimensions for building depression recognition models; (2) we extract visual information from voice and silence segments separately; (3) we consolidate features and use a meta-model for fusion. The proposed methodology is evaluated, experimental results reveal the method is competitive.\"",
        "title: \"A Web-Based Self-training Approach for Authorship Attribution\" with abstract: \"As any other text categorization task, authorship attribution requires a large number of training examples. These examples, which are easily obtained for most of the tasks, are particularly difficult to obtain for this case. Based on this fact, in this paper we investigate the possibility of using Web-based text mining methods for the identification of the author of a given poem. In particular, we propose a semi-supervised method that is specially suited to work with justfew training examples in order to tackle the problem of the lack of data with the same writing style. The method considers the automatic extraction of the unlabeled examples from the Web and its iterative integration into the training data set. To the knowledge of the authors, a semi-supervised method which makes use of the Web as support lexical resource has not been previously employed in this task. The results obtained on poem categorization show that this method may improve the classification accuracy and it is appropriate to handle the attribution of short documents.\"",
        "title: \"DIMEx100: A New Phonetic and Speech Corpus for Mexican Spanish\" with abstract: \"In this paper the phonetic and speech corpus DIMEx100 for Mexican Spanish is presented. We discuss both the linguistic motivation and the computational tools employed for the design, collection and transcription of the corpus. The phonetic transcription methodology is based on recent empirical studies proposing a new basic set of allophones and phonological rules for the dialect of the central part of Mexico. These phonological rules have been implemented in a visualization tool that provides the expected phonetic representation of a text, and also a default temporal alignment between the spoken corpus and its phonetic representation. The tools are also used to compute the properties of the corpus and compare these figures with previous work.\"",
        "1 is \"Computing semantic relatedness using Wikipedia-based explicit semantic analysis\", 2 is \"Proposal of acoustic measures for automatic detection of vocal fry\".",
        "\nGiven above information, for an author who has written the paper with the title \"A mapping between classifiers and training conditions for WSD\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0165": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Rate bounds for MIMO relay channels':",
        "title: \"Work Capacity of Regulated Freelance Platforms: Fundamental Limits and Decentralized Schemes.\" with abstract: \"Crowdsourcing of jobs to online freelance platforms is rapidly gaining popularity. Most crowdsourcing platforms are uncontrolled and offer freedom to customers and freelancers to choose each other. This works well for unskilled jobs (e.g., image classification) with no specific quality requirement since freelancers are functionally identical. For skilled jobs (e.g., software development) with spec...\"",
        "title: \"Network Utility Maximization: A Rate-Distortion Perspective\" with abstract: \"Network utility maximization (NUM) represents a vast and growing body of\nliterature in optimizing network operation such as throughput and fairness,\ngiven a set of constraints. This framework has resulted in a better\nunderstanding of optimal operation of and interaction among layers of the\nprotocol stack, including congestion control, routing, access and physical\nlayer transmission. However, traditional NUM optimization does not incorporate\nlossy compression (rate-distortion) into its formulation - data is assumed\npre-compressed and packetized prior to analysis. Since rate-distortion has a\nsubstantial impact on end-user experience (for example, in video/multimedia\ndelivery), this paper generalizes the traditional NUM framework to include\ncompression control. It develops a distributed compression control for binary\nsources, and solves the coupled NUM problem in special cases to illustrate\nimportant aspects of compression control. Finally, this paper discusses a\nstochastic framework that includes compression control, and provide insights on\nadaptive control of networks.\"",
        "title: \"Adaptive turbo-coded modulation for flat-fading channels\" with abstract: \"We consider a turbo-coded system employed on a flat-fading channel where the transmitter and receiver adapt the encoder, decoder, modulation scheme, and transmit power to the state of the channel. Assuming instantaneous and error-free channel gain and phase knowledge at the transmitter and the receiver, we determine the optimal adaptation strategy that maximizes the throughput of this system, whil...\"",
        "title: \"On the Capacity of a Class of MIMO Cognitive Radios\" with abstract: \"Cognitive radios have been studied recently as a means to utilize spectrum in a more efficient manner. This paper focuses on the fundamental limits of operation of a MIMO cognitive radio network with a single licensed user and a single cognitive user. The channel setting is equivalent to an interference channel with degraded message sets (with the cognitive user having access to the licensed user's message). An achievable region and an outer bound is derived for such a network setting. It is shown that the achievable region is optimal for a portion of the capacity region that includes sum capacity.\"",
        "title: \"Distributed Rate Allocation for Wireless Networks\" with abstract: \"This paper develops a distributed algorithm for rate allocation in wireless networks that achieves the same throughput region as optimal centralized algorithms. This cross-layer algorithm jointly performs medium access control and physical-layer rate adaptation. The paper establishes that this algorithm is throughput-optimal for general rate regions. In contrast to on-off scheduling, rate allocation enables optimal utilization of physical-layer schemes by scheduling multiple rate levels. The algorithm is based on local queue-length information, and thus the algorithm is of significant practical value. An important application of this algorithm is in multiple-band multiple-radio throughput-optimal distributed scheduling for white-space networks.\"",
        "1 is \"A vector-perturbation technique for near-capacity multiantenna multiuser communication-part I: channel inversion and regularization\", 2 is \"An 11b 3.6GS/s time-interleaved SAR ADC in 65nm CMOS\".",
        "\nGiven above information, for an author who has written the paper with the title \"Rate bounds for MIMO relay channels\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0166": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Using The Future To ''Sort Out'' The Present: Rankprop And Multitask Learning For Medical Risk Evaluation':",
        "title: \"Learning \"forgiving\" hash functions: algorithms and large scale tests\" with abstract: \"The problem of efficiently finding similar items in a large corpus of high-dimensional data points arises in many real-world tasks, such as music, image, and video retrieval. Beyond the scaling difficulties that arise with lookups in large data sets, the complexity in these domains is exacerbated by an imprecise definition of similarity. In this paper, we describe a method to learn a similarity function from only weakly labeled positive examples. Once learned, this similarity function is used as the basis of a hash function to severely constrain the number of points considered for each lookup. Tested on a large real-world audio dataset, only a tiny fraction of the points (\u223c0.27%) are ever considered for each lookup. To increase efficiency, no comparisons in the original high-dimensional space of points are required. The performance far surpasses, in terms of both efficiency and accuracy, a state-of-the-art Locality-Sensitive-Hashing based technique for the same problem and data set.\"",
        "title: \"Audio Fingerprinting: Combining Computer Vision & Data Stream Processing\" with abstract: \"In this paper, we present waveprint, a novel system for audio identification. Waveprint uses a combination of computer-vision techniques and large-scale-data-stream processing algorithms to create compact fingerprints of audio data that can be efficiently matched. The resulting system has excellent identification capabilities for small snippets of audio that have been degraded in a variety of manners, including competing noise, poor recording quality, and cell-phone playback. We measure the tradeoffs between performance, memory usage, and computation through extensive experimentation. The system is more efficient in terms of memory usage and computation, while being more accurate, when compared with previous state of the art systems.\"",
        "title: \"Using a priori knowledge to create probabilistic models for optimization\" with abstract: \"Recent studies have examined the effectiveness of using probabilistic models to guide the sample generation process for searching high dimensional spaces. Although the simplest models, which do not account for parameter interdependencies, often perform well on many problems, they may perform poorly when used on problems that have a high degree of interdependence between parameters. More complex dependency networks that can account for the interactions between parameters are required. However, building these networks may necessitate enormous amounts of sampling. In this paper, we demonstrate how a priori knowledge of parameter dependencies, even incomplete knowledge, can be incorporated to efficiently obtain accurate models that account for parameter interdependencies. This is achieved by effectively putting priors on the network structures that are created. These more accurate models yield improved results when used to guide the sample generation process for search and also when used to initialize the starting points of other search algorithms.\"",
        "title: \"The Virtues of Peer Pressure: A Simple Method for Discovering High-Value Mistakes\" with abstract: \"Much of the recent success of neural networks can be attributed to the deeper architectures that have become prevalent. However, the deeper architectures often yield unintelligible solutions, require enormous amounts of labeled data, and still remain brittle and easily broken. In this paper, we present a method to efficiently and intuitively discover input instances that are misclassified by well-trained neural networks. As in previous studies, we can identify instances that are so similar to previously seen examples such that the transformation is visually imperceptible. Additionally, unlike in previous studies, we can also generate mistakes that are significantly different from any training sample, while, importantly, still remaining in the space of samples that the network should be able to classify correctly. This is achieved by training a basket of N \\\"peer networks\\\" rather than a single network. These are similarly trained networks that serve to provide consistency pressure on each other. When an example is found for which a single network, S, disagrees with all of the other $$N-1$$ networks, which are consistent in their prediction, that example is a potential mistake for S. We present a simple method to find such examples and demonstrate it on two visual tasks. The examples discovered yield realistic images that clearly illuminate the weaknesses of the trained models, as well as provide a source of numerous, diverse, labeled-training samples.\"",
        "title: \"Evolution-Based Methods for Selecting Point Data for Object Localization: Applications to Computer-Assisted Surgery\" with abstract: \"Object localization has applications in many areas of engineeringand science. The goal is to spatially locate an arbitrarily shaped object.In many applications, it is desirable to minimize the number of measurementscollected while ensuring sufficient localization accuracy. In surgery, forexample, collecting a large number of localization measurements may eitherextend the time required to perform a surgical procedure or increase theradiation dosage to which a patient is exposed.Localization accuracy is a function of the spatial distribution ofdiscrete measurements over an object when measurement noise is present. Inprevious work (J. of Image Guided Surgery, Simon et al., 1995), metrics werepresented to evaluate the information available from a set of discreteobject measurements. In this study, new approaches to the discrete pointdata selection problem are described. These include hillclimbing, geneticalgorithms (GAs), and Population-Based Incremental Learning (PBIL).Extensions of the standard GA and PBIL methods that employ multipleparallel populations are explored. The results of extensive empiricaltesting are provided. The results suggest that a combination of PBIL andhillclimbing result in the best overall performance. A computer-assistedsurgical system that incorporates some of the methods presented in thispaper is currently being evaluated in cadaver trials.\"",
        "1 is \"Neural Based Steganography\", 2 is \"The Efficient Learning Of Multiple Task Sequences\".",
        "\nGiven above information, for an author who has written the paper with the title \"Using The Future To ''Sort Out'' The Present: Rankprop And Multitask Learning For Medical Risk Evaluation\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0167": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Well Structured Transition Systems With History':",
        "title: \"Automatic Verification of Parameterized Cache Coherence Protocols\" with abstract: \" . We propose a new method for the verication of parameterized cache coherence protocols. Cache coherence protocols are used tomaintain data consistency in multiprocessor systems equipped with localfast caches. In our approach we use arithmetic constraints to model possiblyinnite sets of global states of a multiprocessor system with manyidentical caches. In preliminary experiments using symbolic model checkersfor innite-state systems based on real arithmetics (HyTech [HHW97]and... \"",
        "title: \"Parameterized Tree Systems\" with abstract: \"Several recent works have considered parameterized verification, i.e. automatic verification of systems consisting of an arbitrary number of finite-state processes organized in a linear array. The aim of this paper is to extend these works by giving a simple and efficient method to prove safety properties for systems\n with tree-like architectures. A process in the system is a finite-state automaton and a transition is performed jointly by a process and\n its parent and children processes. The method derives an over-approximation of the induced transition system, which allows\n the use of finite trees as symbolic representations of infinite sets of configurations. Compared to traditional methods for\n parameterized verification of systems with tree topologies, our method does not require the manipulation of tree transducers,\n hence its simplicity and efficiency. We have implemented a prototype which works well on several nontrivial tree-based protocols.\n \"",
        "title: \"A Biologically Inspired Model with Fusion and Clonation of Membranes\" with abstract: \"P-systems represent an important class of biologically inspired computational models. In this paper, we study computational properties of a variation of P-systems with rules that model in an abstract way fusion and clonation of membranes. We focus our attention on extended P-systems with an interleaving semantics and symbol objects and we investigate decision problems like reachability of a configuration, boundedness (finiteness of the state space), and coverability (verification of safety properties). In particular we use the theory of well-structured transition systems to prove that both the coverability and the boundedness problems are decidable for PB systems with fusion and clonation. Our results represent a preliminary step towards the development of automated verification procedures for concurrent systems with biologically inspired operations like fusion and clonation.\"",
        "title: \"Parameterized verification of time-sensitive models of ad hoc network protocols.\" with abstract: \"We study decidability and undecidability results for parameterized verification of a formal model of timed Ad Hoc network protocols. The communication topology is defined by an undirected graph and the behaviour of each node is defined by a timed automaton communicating with its neighbours via broadcast messages. We consider parameterized verification problems formulated in terms of reachability. In particular we are interested in searching for an initial configuration from which an individual node can reach an error state. We study the problem for dense and discrete time and compare the results with those obtained for (fully connected) networks of timed automata.\"",
        "title: \"A Linear Logic Calculus Objects\" with abstract: \" This paper presents a linear logic programming language, called O \\Gammaffi , that gives a complete account of an object-oriented calculus with inheritance and override. This language is best understood as a logical counterpart the object and record extensions of functional programming that have recently been proposed in the literature. From these proposals, O \\Gammaffi inherits the representation of objects as composite data structures, with attribute and method fields, as well as their... \"",
        "1 is \"Program analysis via graph reachability\", 2 is \"Detecting conflicts in commitments\".",
        "\nGiven above information, for an author who has written the paper with the title \"Well Structured Transition Systems With History\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0168": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Predictive Absolute Moment Block Truncation Coding For Image Compression':",
        "title: \"A backward segmented-block matching algorithm for video compression\" with abstract: \"In this paper, we present a segmentation-based block matching algorithm for video compression. First, the decoded frame is segmented into background and foreground using a two-step predictive procedure. This segmentation is then used in estimating the motion in the next frame. Being a backward procedure, the segmentation information is not transmitted to the receiver. Experimental results show that blocks at the leading edges of moving objects get better compensated with improved subjective quality of the motion compensated frame.\"",
        "title: \"Video object error coding method based on compressive sensing\" with abstract: \"The recently emerged theory of compressive sensing (CS) has a remarkable result that signals having sparse representations in some known basis can be represented (with high probability) by taking a few random projection measurements of the signals. In this paper, we study some CS sparse reconstruction methods and propose a video object error coding method based on CS theory. The proposed system first assumes the moving objects have been segmented from background image and object-based motion compensated from the previous reconstruction frame, and then the resulting object error is encoded by using CS random matrix projection. Finally the coded measurements can be quantized to store or transmit. Experimental results demonstrate the object error blocks can be effectively recovered by using CS sparse reconstruction algorithms. This proposed method would be widely used in the object-based video compression fields.\"",
        "title: \"Signal Recovery from Random Measurements via Extended Orthogonal Matching Pursuit\" with abstract: \"Orthogonal Matching Pursuit (OMP) and Basis Pursuit (BP) are two well-known recovery algorithms in compressed sensing. To recover a -dimensional -sparse signal with high probability, OMP needs number of measurements, whereas BP needs only number of measurements. In contrary, OMP is a practically more appealing algorithm due to its superior execution speed. In this piece of work, we have proposed a scheme that brings the required number of measurements for OMP closer to BP. We have termed this scheme as , which runs OMP for -iterations instead of -iterations, by choosing a value of . It is shown that guarantees a high probability signal recovery with number of measurements. Another limitation of OMP unlike BP is that it requires the knowledge of . In order to overcome this limitation, we have extended the idea of to illustrate another recovery scheme called , which runs OMP until th- signal residue vanishes. It is shown that can achieve a close to -norm recovery without any knowledge of like BP.\"",
        "title: \"Kernel-Based Spatial-Color Modeling for Fast Moving Object Tracking.\" with abstract: \"Visual tracking has been a challenging problem in computer vision over the decades. The applications of Visual Tracking are far-reaching, ranging from surveillance and monitoring to smart rooms. Mean- shift (MS) tracker, which gained more attention recently, is known for tracking objects in a cluttered environment and its low compu- tational complexity. The major problem encountered in histogram- based MS is its inability to track rapidly moving objects. In order to track fast moving objects, we propose a new robust mean-shift tracker that uses both spatial similarity measure and color histogram- based similarity measure. The inability of MS tracker to handle large displacements is circumvented by the spatial similarity-based track- ing module, which lacks robustness to object's appearance change. The performance of the proposed tracker is better than the individual trackers for tracking fast-moving objects with better accuracy.\"",
        "title: \"On the closeness of the space spanned by the lattice structures for a class of linear phase perfect reconstruction filter banks\" with abstract: \"The incompleteness of the existing lattice structures has been well established for M-channel FIR linear phase perfect reconstruction filter banks (LPPRFBs) with filter length L2M in the literature, and even the nonexistence of complete order-one lattice has been reported recently. Thus, a question arises naturally as to how large the space spanned by the existing lattice structure is, and about its closeness over some polynomial transformations. The study for such issue can reveal what sense of optimality the lattice based design for LPPRFBs possesses. Inspired from this perspective, this paper firstly studies the closeness of the space spanned by the existing lattice structures under the polynomial transformations for arbitrary equal-length LPPRFBs. We have shown that this space is closed under the popular polynomial transforms widely used in FB design, which establishes the suboptimality of the lattice based design methods for LPPRFBs. Furthermore, the explicit relationship between the lattice parameters before and after transformations has been shown for describing the closeness of the space spanned by those lattice structures.\"",
        "1 is \"A novel and efficient design of multidimensional PR two-channel filter banks with hourglass-shaped passband support\", 2 is \"Parametric prediction of heap memory requirements\".",
        "\nGiven above information, for an author who has written the paper with the title \"Predictive Absolute Moment Block Truncation Coding For Image Compression\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0169": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'A study on two-sided linear prediction approach for land mine detection':",
        "title: \"TDOA Positioning Irrespective of Source Range.\" with abstract: \"TDOA localization requires the knowledge if the source is in the near-field or far-field, for the purpose to decide using the curved wavefront model that enables point positioning or the linear wavefront model that provides only the DOA. Such prior knowledge is often not available in practice. The far-field model can cause a considerable amount of DOA bias if the source is not sufficiently distant from the sensor array. This paper proposes a unified model to locate a source irrespective of whether it is in the near field, the far field or in between. The proposed model represents the source location by the direction and the inverse-range. It yields the unique coordinate if the source is near or the DOA if it is distant. We developed the Maximumm Likelihood Estimator for the proposed model through the Gauss\u2013Newton iteration and semidefinite relaxation. We analyze the proposed model using the Hybrid Bhattacharyya\u2013Barankin bound and show that the proposed model does not have the thresholding effect as the source range increases, validating that there is no need to resort to the far-field model even if the source range is large. We also perform bias analysis and elaborate a benefit of the proposed approach in reducing the DOA bias as compared to the far-field model.\"",
        "title: \"Adaptive time-delay estimation in nonstationary signal and/or noise power environments\" with abstract: \"A model for an adaptive time-delay estimator is proposed to improve its performance in estimating the difference in arrival time of a bandlimited random signal received by two spatially separated sensors in an environment where the signal and noise power are time varying. The system comprises two adaptive units: a filter to compensate time shift between the two receiver channels and a gain control to provide Wiener filtering. Both the filter coefficients and the variable gain are adjusted simultaneously by using modifications from the stochastic mean-square-error gradient in the traditional adaptive least-mean-square time-delay estimation (LMSTDE) method. The convergence characteristics of the proposed system are analyzed in detail and compared with those obtained by the traditional technique. Theoretical results show that, unlike the LMSTDE configuration, this arrangement can decouple the adaptation of time shift from the changing signal and/or noise power, which in turn gives rise to better convergence behavior of the delay estimate. Simulation results are included to illustrate the effectiveness of the new model and corroborate the theoretical developments\"",
        "title: \"An iterative algorithm for two-scale wavelet decomposition\" with abstract: \"This correspondence proposes an iterative method to decompose an arbitrary mother wavelet into a bandpass filter and a lowpass filter, where the filter pair will reproduce the mother wavelet through the two-scale equations. This problem is not straightforward because the two-scale relationship between the filter pair and the mother wavelet is nonlinear. The method finds the filter pair by minimizing an objective function with respect to one filter in an iteration and the other in the next iteration until convergence. The algorithm requires least-squares minimization only and is computationally efficient. The performance of the proposed algorithm is supported by simulations\"",
        "title: \"Efficient closed-form estimators for multistatic sonar localization\" with abstract: \"This paper addresses the multistatic sonar localization problem using time measurements only or with bearings in the presence of Gaussian transmitter positions, receiver positions, and propagation speed variations. We develop efficient algebraic solutions to this nonlinear estimation problem through parameter transformation and multistage processing. Both situations of known and unknown statistical distribution of the propagation speed are considered. Analysis supports their performance in reaching the hybrid Cramer-Rao lower bound (CRLB) over the small error region.\"",
        "title: \"Accurate Localization of a Rigid Body Using Multiple Sensors and Landmarks\" with abstract: \"This paper develops estimators for locating a rigid body using the time measurements, and the Doppler as well if it is moving, between the sensors in the rigid body and a few landmarks outside. The challenge of rigid body localization is that in addition to the position, we are also interested in obtaining the rotation parameters of the rigid body that must belong to the special orthogonal group. ...\"",
        "1 is \"A Unified Convergence Analysis of Block Successive Minimization Methods for Nonsmooth Optimization.\", 2 is \"A study of cross-validation and bootstrap for accuracy estimation and model selection\".",
        "\nGiven above information, for an author who has written the paper with the title \"A study on two-sided linear prediction approach for land mine detection\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0170": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Multiresolution analysis for meshes with appearance attributes':",
        "title: \"An Active Vision System for Obtaining High Resolution Depth Information\" with abstract: \"A low-cost active vision head with ten degrees of freedom is presented that has been build from off-the-shelf parts. To obtain high resolution depth information of fixated objects in the scene a general purpose calibration procedure is proposed which estimates intrinsic and extrinsic camera parameters including the vergence axes of both cameras. To produce enhanced dense depth maps a hierarchical block matching procedure is presented that employs color information. To simplify the development of controlling strategies for the head a modular hierarchy is applied that distributes various tasks among different levels employing basic capabilities of the components of the head.\"",
        "title: \"Linking Feature Lines on 3D Triangle Meshes with Artificial Potential Fields\" with abstract: \"We propose artificial potential fields as a support theory for a feature linking algorithm. This algorithm operates on 3D triangle meshes derived from multiple range scans of an object, and the features of interest are curvature extrema on the object's surface. A problem that arises with detecting these features is that results from standard algorithms are often incomplete in that feature lines are broken and discontinuous. Our novel linking algorithm closes these broken feature lines to form a more complete feature description. The main contribution of this algorithm is the use of arti- ficial potential fields to govern the linking process. In this paper, we discuss the feature detection process itself and then define the linking procedure in the context of potential fields. We present results for both synthetic and scanned models.\"",
        "title: \"Studies on the Effectiveness of Multispectral Images for Face Recognition: Comparative Studies and New Approaches\" with abstract: \"In this paper, we investigate face recognition in unconstrained illumination conditions. A twofold contribution is proposed: First, three state of the art algorithms, namely Multiblock Local Binary Pattern (MBLBP), Histogram of Gabor Phase Patterns (HGPP) and Local Gabor Binary Pattern Histogram Sequence (LGBPHS) are challenged against the IRIS-M3 multispectral face data base to evaluate their robustness against high illumination variation. Second, we propose to enhance the Performance of the three mentioned algorithms, which has been drastically decreased because of the non-monotonic illumination variation that distinguishes the IRIS-M3 face database. Instead of the usual braod band images, we use narrow band sub spectral images selected from the visible spectrum. Selection of best spectral bands is formulated as a pursuit optimization problem wherein the vector of weights determining the importance of each visible spectral band is supposed to be sparse, and hence can be determined by minimizing its L1-norm. The results highlight further the still challenging problem of face recognition in conditions with high illumination variation, as well as the effectiveness of our sub spectral images based approach to increase the accuracy of the studied algorithms by at least 14% upon the proposed database.\"",
        "title: \"A kernelized sparsity-based approach for best spectral bands selection for face recognition\" with abstract: \"We study face recognition in unconstrained illumination conditions. A twofold contribution is proposed: First, the robustness of four state-of-the-art algorithms, namely Multi-block Local Binary Pattern (MBLBP), Histogram of Gabor Phase Patterns (HGPP), Local Gabor Binary Pattern Histogram Sequence (LGBPHS) and Patterns of Oriented Edge Magnitudes (POEM-WPCA) against high illumination variation is studied. Second, we propose to enhance the performance of the four mentioned algorithms, which has been drastically decreased upon the day lighted face images provided by IRIS-M3 face database. For this purpose, we use visible narrow band subspectral images selected from the mentioned database. We formulate best spectral bands selection as a pursuit optimization problem wherein the vector of weights determining the importance of each visible spectral band is supposed to be sparse, and hence can be determined by minimizing its L1-norm. Several fusing approaches are then applied on selected best spectral bands using multi-scale and multi-orientation Gabor wavelets. The results highlight further the still challenging problem of face recognition in conditions with high illumination variation, as well as the effectiveness of our subspectral images based approach with its two components; bands selection and bands fusion, to increase the accuracy of the studied algorithms by at least 14 % upon the proposed database.\"",
        "title: \"Multiresolution analysis for meshes with appearance attributes\" with abstract: \"We present a new multiresolution analysis framework for ir- regular meshes with attributes based on the lifting scheme. We introduce a surface prediction operator to compute the detail coefcients for the geometry and the attributes of the model. Attribute analysis gives appearance information to complete the geometrical analysis of the model. A set of experimental results are given to show the efcienc y of our framework. We present two applications to adaptive visual- ization and denoising.\"",
        "1 is \"SURE-Based Non-Local Means.\", 2 is \"Uncertainty Modeling and Model Selection for Geometric Inference\".",
        "\nGiven above information, for an author who has written the paper with the title \"Multiresolution analysis for meshes with appearance attributes\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0171": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Optical flow-based real-time object tracking using non-prior training active feature model':",
        "title: \"Multifocus Image Fusion by Establishing Focal Connectivity\" with abstract: \"Multifocus fusion is the process of unifying focal information from a set of input images acquired with limited depth of field. In this effort, we present a general purpose multifocus fusion algorithm, which can be applied to varied applications ranging from microscopic to long range scenes. The main contribution in this paper is the segmentation of the input images into partitions based on focal connectivity. Focal connectivity is established by isolating regions in an input image that fall on the same focal plane. Our method uses focal connectivity and does not rely on physical properties like edges directly for segmentation. Our method establishes sharpness maps to the input images, which are used to isolate and attribute image partitions to input images. The partitions are mosaiced seamlessly to form the fused image. Illustrative examples of multifocus fusion using our method are shown. Comparisons against existing methods are made and the results are discussed.\"",
        "title: \"Camera Handoff with Adaptive Resource Management for Multi-camera Multi-target Surveillance\" with abstract: \"Camera handoff is a crucial step to generate a continuously tracked and consistently labeled trajectory of the object of interest in multi-camera surveillance systems. Most existing camera handoff algorithms concentrate on data association, namely consistent labeling, where images of the same object are matched across different cameras. However, most real-time object tracking systems see a decrease in the system's frame rate as the number of tracked objects increases. To address this issue, we propose to incorporate an adaptive resource management mechanism into camera handoff. In so doing, cameras\u00c2\u00bf resources can be dynamically allocated to multiple objects according to their priorities and hence the required minimum frame rate can be maintained. Experimental results illustrate that the proposed camera handoff algorithm is capable of maintaining a constant frame rate and of achieving a substantially improved handoff success rate by approximately 20% in comparison with the algorithm presented by Khan and Shah.\"",
        "title: \"Triangle mesh-based edge detection and its application to surface segmentation and adaptive surface smoothing\" with abstract: \"Triangle meshes are widely used in representing surfaces in com- puter vision and computer graphics. Although 2D image processing- based edge detection techniques have been popular in many appli- cation areas, they are not well developed for surfaces represented by triangle meshes. This paper proposes a robust edge detection algorithm for triangle meshes and its applications to surface seg- mentation and adaptive surface smoothing. The proposed edge de- tection technique is based on eigen analysis of the surface normal vector field in a geodesic window. To compute the edge strength of a certain vertex, the neighboring vertices in a specified geodesic distance are involved. Edge information are used further to seg- ment the surfaces with watershed algorithm and to achieve edge- preserved, adaptive surface smoothing. The proposed algorithm is novel in robustly detecting edges on triangle meshes against noise. The 3D watershed algorithm is an extension from previous work. Experimental results on surfaces reconstructed from multi-view real range images are presented.\"",
        "title: \"Integration of multiple range and intensity image pairs using a volumetric method to create textured three-dimensional models\" with abstract: \"We present a volumetric approach to three-dimensional (3D) object modeling that differs from previous techniques in that both object texture and geometry are considered in the reconstruction process. The motivation for the research is the simulation of a thermal tire inspection station. integrating 3D geometry information with two-dimensional thermal images permits the thermal information to be displayed as a texture map on the tire structure, enhancing analysis capabilities. Additionally, constructing the fire geometry during the inspection process allows the tire to be examined for structural defects that might be missed if the thermal data were textured onto a predefined model. Experimental results demonstrate the efficacy of the proposed approach and quantitative experiments indicate that the volumetric integration technique compares favorably to a state-of-the-art, mesh-based integration approach in terms of geometrical accuracy. Future research goals are also noted. (C) 2001 SPIE and IS&T.\"",
        "title: \"Impact Of Intensity Edge Map On Segmentation Of Noisy Range Images\" with abstract: \"In this paper, we investigate the impact of intensity edge maps (IEMs) on the segmentation of noisy range images. Two edge-based segmentation algorithms are considered. The first is a watershed-based segmentation technique and the other is the scan-line grouping technique. Each of these algorithms is implemented in two different forms. In the first form, an IEM is fused with the range edge map prior to segmentation. In the second form, the range edge map alone is used. The performance of each algorithm, with and without the use of the IEM information, is evaluated and reported in terms of correct segmentation rate. For our experiments, two sets of real range images are used. The first set comprises inherently noisy images. The other set is composed of images with varying levels of artificial, additive Gaussian noise. The experimental results indicate that the use of IEMs can significantly improve edge-based segmentation of noisy range images. Considering these results, it seems that segmentation tasks involving range images captured by noisy scanners would benefit from the use of IEM information. Additionally, the experiments indicate that higher quality edge information dan be obtained by fusing range and intensity edge information.\"",
        "1 is \"Fast Image Restoration for Reducing Block Artifacts Based on Adaptive Constrained Optimization\", 2 is \"Detection and Tracking of Moving Objects from Overlapping EO and IR Sensors\".",
        "\nGiven above information, for an author who has written the paper with the title \"Optical flow-based real-time object tracking using non-prior training active feature model\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0172": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Synchronization In Oscillator Networks: Switching Topologies And Non-Homogeneous Delays':",
        "title: \"Using economic Model Predictive Control to design sustainable policies for mitigating climate change\" with abstract: \"Reducing greenhouse gas emissions is now an important and pressing matter. Systems control theory, and in particular feedback control, can contribute to the design of policies that achieve sustainable levels of emissions of CO2 (and other greenhouse gases) while minimizing the impact on the economy, and at the same time explicitly addressing the high levels of uncertainty associated with predictions of future emissions. In this paper, preliminary results are described for an approach where economic Model Predictive Control (MPC) is applied to a Regional dynamic Integrated model of Climate and the Economy (RICE model) as a test bed to design savings rates and global carbon tax for greenhouse gas emissions. Using feedback control, the policies are updated on the basis of the observed emissions, rather than on the predicted level of emissions. The basic structure and principle of the RICE model is firstly introduced and some key equations are described. The idea of introducing feedback control is then explained and economic MPC is applied to design policies for CO2 emissions. Simulation results are presented to demonstrate the effectiveness of the proposed method for two different scenarios. Feedback control design provides a degree of robustness against disturbances and model uncertainties, which is illustrated through a simulation study with two particular types of uncertainties. The results obtained in this paper illustrate the strength of the proposed design approach and form the basis for future research on using systems control theory to design optimal sustainable policies.\"",
        "title: \"Analysis of aircraft pitch axis stability augmentation system using sum of squares optimization\" with abstract: \"The region of attraction of a trim point for the pitch axis of a nonlinear modeled aircraft modulated via linear dynamic inversion based controller was determined numerically. The model incorporates uncertainty in the position of center of gravity along the X-body axis. The stability regions are computed using SOSTOOLS, which converts the required sum of squares conditions to an appropriate semi definite program that is then solved using SeDuMi.\"",
        "title: \"Stochastic processes and feedback-linearisation for online identification and Bayesian adaptive control of fully-actuated mechanical systems.\" with abstract: \"  This work proposes a new method for simultaneous probabilistic identification and control of an observable, fully-actuated mechanical system. Identification is achieved by conditioning stochastic process priors on observations of configurations and noisy estimates of configuration derivatives. In contrast to previous work that has used stochastic processes for identification, we leverage the structural knowledge afforded by Lagrangian mechanics and learn the drift and control input matrix functions of the control-affine system separately. We utilise feedback-linearisation to reduce, in expectation, the uncertain nonlinear control problem to one that is easy to regulate in a desired manner. Thereby, our method combines the flexibility of nonparametric Bayesian learning with epistemological guarantees on the expected closed-loop trajectory. We illustrate our method in the context of torque-actuated pendula where the dynamics are learned with a combination of normal and log-normal processes. \"",
        "title: \"Structured Sum Of Squares For Networked Systems Analysis\" with abstract: \"In this paper we introduce a structured Sum of Squares technique that enables Sum of Squares programming to be applied to networked systems analysis. By taking the structure of the network into account, we limit the site and number of decision variables in the LMI representation of the Sum of Squares, which improves the scalability of the technique for networked systems beyond taking advantage of symmetry and sparsity. We apply the technique to test non-negativity of fourth order structured polynomials in many variables and show that for these problems the technique has improved scalability over existing Sum of Squares techniques.\"",
        "title: \"Improving the Performance of Network Congestion Control Algorithms\" with abstract: \"This technical note describes a redesign framework for fluid-flow models of network congestion control algorithms. Motivated by the augmented Lagrangian method, we introduce extra dynamics to algorithms resulting from traditional primal-dual methods to improve their performance while guaranteeing stability. We use our method to redesign the primal-dual, primal and dual algorithms for network flow control. In particular, we investigate the influence of the gains resulting from the extra dynamics on system stability and robustness to time delays. We provide a method to improve the transient performance and delay robustness of the overall system by tuning these gains.\"",
        "1 is \"On the Evaluation Complexity of Cubic Regularization Methods for Potentially Rank-Deficient Nonlinear Least-Squares Problems and Its Relevance to Constrained Nonlinear Optimization.\", 2 is \"Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming\".",
        "\nGiven above information, for an author who has written the paper with the title \"Synchronization In Oscillator Networks: Switching Topologies And Non-Homogeneous Delays\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0173": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Embedding Digitized Fibre Fields In Finite Element Models Of Muscles':",
        "title: \"Intensity-based registration of prostate brachytherapy implants and ultrasound\" with abstract: \"Purpose: In prostate brachytherapy, determining the 3D location of the seeds relative to surrounding structures is necessary for calculating dosimetry. Ultrasound imaging provides the ability to visualize soft tissues, and implanted seeds can be reconstructed from C-arm fluoroscopy. Registration between these two complementary modalities would allow us to make immediate provisions for dosimetric deviation from the optimal implant plan. Methods: We propose intensity- based registration between ultrasound and a reconstructed model of seeds from fluoroscopy. The ultrasound images are pre-processed with recursive thresholding and phase congruency. Then a 3D ultrasound volume is reconstructed and registered to the implant model using mutual information. Results: A standard training phantom was implanted with 49 seeds. Average registration error between corresponding seeds relative to the ground truth is 0.09 mm. The effect of false positives in ultrasound was investigated by masking seeds from the fluoroscopy reconstructed model. The registration error remained below 0.5 mm at a rate of 30% false positives. Conclusion: Our method promises to be clinically adequate, where requirements for registration is 1.5 mm.\"",
        "title: \"Real-time ultrasound image classification for spine anesthesia using local directional Hadamard features\" with abstract: \"Injection therapy is a commonly used solution for back pain management. This procedure typically involves percutaneous insertion of a needle between or around the vertebrae, to deliver anesthetics near nerve bundles. Most frequently, spinal injections are performed either blindly using palpation or under the guidance of fluoroscopy or computed tomography. Recently, due to the drawbacks of the ionizing radiation of such imaging modalities, there has been a growing interest in using ultrasound imaging as an alternative. However, the complex spinal anatomy with different wave-like structures, affected by speckle noise, makes the accurate identification of the appropriate injection plane difficult. The aim of this study was to propose an automated system that can identify the optimal plane for epidural steroid injections and facet joint injections.\"",
        "title: \"Using Hidden Markov Models to capture temporal aspects of ultrasound data in prostate cancer\" with abstract: \"Recent studies highlight temporal ultrasound data as highly promising in differentiating between malignant and benign tissues in prostate cancer patients. Since Hidden Markov Models can be used for capturing order and patterns in time varying signals, we employ them to model temporal aspects of ultrasound data that are typically not incorporated in existing models. By comparing order-preserving and orderaltering models, we demonstrate that the order encoded in the series is necessary to model the variability in ultrasound data of prostate tissues. In future studies, we will investigate the influence of order on the differentiation between malignant and benign tissues.\"",
        "title: \"Information processing in computer-assisted interventions: 5th international conference, 2014\" with abstract: \"Information processing is an increasingly important tool for surgical interventions. In computer-assisted surgery and interventional radiology, computer systems present valuable information during procedures which is used to help clinical decision-making at the point of treatment.With the paradigm shift towards minimally invasive surgery, where the surgeon or interventionalist has an inherently restricted view of the operating field, this information has a significant influence on how procedures are performed. In fact, surgical instrumentation and technology is increasing in complexity in\"",
        "title: \"A New Approach for Creating Customizable Cytoarchitectonic Probabilistic Maps without a Template\" with abstract: \"We present a novel technique for creating template-free probabilistic maps of the cytoarchitectonic areas using a groupwise registration. We use the technique to transform 10 human post-mortem structural MR data sets, together with their corresponding cytoarchitectonic information, to a common space. We have targeted the cytoarchitectonically defined subregions of the primary auditory cortex. Thanks to the template-free groupwise registration, the created maps are not macroanatomically biased towards a specific geometry/topology. The advantage of the groupwise versus pairwise registration in avoiding such anatomical bias is better revealed in studies with small number of subjects and a high degree of variability among the individuals such as the post-mortem data. A leave-one-out cross-validation method was used to compare the sensitivity, specificity and positive predictive value of the proposed and published maps. We observe a significant improvement in localization of cytoarchitectonically defined subregions in primary auditory cortex using the proposed maps. The proposed maps can be tailored to any subject space by registering the subject image to the average of the groupwise-registered post-mortem images.\"",
        "1 is \"Synthesizing sounds from rigid-body simulations\", 2 is \"Nonrigid liver registration for image-guided surgery using partial surface data: a novel iterative approach\".",
        "\nGiven above information, for an author who has written the paper with the title \"Embedding Digitized Fibre Fields In Finite Element Models Of Muscles\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0174": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Probabilistic Explanation Based Learning':",
        "title: \"A model for mining relevant and non-redundant information\" with abstract: \"We propose a relatively simple yet powerful model for choosing relevant and non-redundant pieces of information. The model addresses data mining or information retrieval settings where relevance is measured with respect to a set of key or query objects, either specified by the user or obtained by a data mining step. The problem addressed is not only to identify other relevant objects, but also ensure that they are not related to possible negative query objects, and that they are not redundant with respect to each other. The model proposed here only assumes a similarity or distance function for the objects. It has simple parameterization to allow for different behaviors with respect to query objects. We analyze the model and give two efficient, approximate methods. We illustrate and evaluate the proposed model on different applications: linguistics and social networks. The results indicate that the model and methods are useful in finding a relevant and non-redundant set of results. While this area has been a popular topic of research, our contribution is to provide a simple, generic model that covers several related approaches while providing a systematic model for taking account of positive and negative query objects as well as non-redundancy of the output.\"",
        "title: \"Contrasting Subgroup Discovery\" with abstract: \"Subgroup discovery methods find interesting subsets of objects of a given class. Motivated by an application in bioinformatics, we first define a generalized subgroup discovery problem. In this setting, a subgroup is interesting if its members are characteristic for their class, even if the classes are not identical. Then we further refine this setting for the case where subsets of objects, for example, subsets of objects that represent different time points or different phenotypes, are contrasted. We show that this allows finding subgroups of objects that could not be found with classical subgroup discovery. To find such subgroups, we propose an approach that consists of two subgroup discovery steps and an intermediate, contrast set definition step. This approach is applicable in various application areas. An example is biology, where interesting subgroups of genes are searched by using gene expression data. We address the problem of finding enriched gene sets that are specific for virus-infected samples for a specific time point or a specific phenotype. We report on experimental results on a time series dataset for virus-infected Solanum tuberosum (potato) plants. The results on S. tuberosum's response to virus-infection revealed new research hypotheses for plant biologists.\"",
        "title: \"Unobtrusive Online Monitoring Of Sleep At Home\" with abstract: \"We describe an online sleep monitoring service, based on unobtrusive ballistocardiography (BCG) measurement in an ordinary bed. The novelty of the system is that the sleep tracking web application is based on measurements from a fully unobtrusive sensor. The BCG signal is measured with a piezoelectric film sensor under the mattress topper, and sent to the web server for analysis. Heart rate and respiratory variation, activity, sleep stages, and stress reactions are inferred based on the signal. The sleep information is presented to the user along with measurements of the sleeping environment (temperature, noise, luminosity) and user-logged tags (e.g. stress, alcohol, exercise). The approach is designed for long-term use at home, allowing users to follow the development of their sleep over months and years. The service has also a medical use, as sleep disorder patients can be measured for long periods before and after interventions.\"",
        "title: \"Mining Relaxed Graph Properties in Internet\" with abstract: \"Many real world datasets are represented in the form of graphs. The classical graph properties found in the data, like cliques or independent sets, can reveal new interesting information in the data. However, such properties can be either too rare or too trivial in the given context. By relaxing the criteria of the classical properties, we can find more and totally new patterns in the data. In this paper, we define relaxed graph properties and study their use in analyzing and processing graph-based data. Especially, we consider the problem of finding self-referring groups in WWW, and give a general algorithm for mining all such patterns from a collection of WWW pages. We suggest that such self-referring groups can reveal web communities or other clusterings in WWW and also help in compression of graph-formed data.\"",
        "title: \"SegMine workflows for semantic microarray data analysis in Orange4WS.\" with abstract: \"In experimental data analysis, bioinformatics researchers increasingly rely on tools that enable the composition and reuse of scientific workflows. The utility of current bioinformatics workflow environments can be significantly increased by offering advanced data mining services as workflow components. Such services can support, for instance, knowledge discovery from diverse distributed data and knowledge sources (such as GO, KEGG, PubMed, and experimental databases). Specifically, cutting-edge data analysis approaches, such as semantic data mining, link discovery, and visualization, have not yet been made available to researchers investigating complex biological datasets.We present a new methodology, SegMine, for semantic analysis of microarray data by exploiting general biological knowledge, and a new workflow environment, Orange4WS, with integrated support for web services in which the SegMine methodology is implemented. The SegMine methodology consists of two main steps. First, the semantic subgroup discovery algorithm is used to construct elaborate rules that identify enriched gene sets. Then, a link discovery service is used for the creation and visualization of new biological hypotheses. The utility of SegMine, implemented as a set of workflows in Orange4WS, is demonstrated in two microarray data analysis applications. In the analysis of senescence in human stem cells, the use of SegMine resulted in three novel research hypotheses that could improve understanding of the underlying mechanisms of senescence and identification of candidate marker genes.Compared to the available data analysis systems, SegMine offers improved hypothesis generation and data interpretation for bioinformatics in an easy-to-use integrated workflow environment.\"",
        "1 is \"Link mining: a new data mining challenge\", 2 is \"Mining Association Rules in Multiple Relations\".",
        "\nGiven above information, for an author who has written the paper with the title \"Probabilistic Explanation Based Learning\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0175": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'A Component-Based Integrated Toolkit':",
        "title: \"Kangaroo: A Tenant-Centric Software-Defined Cloud Infrastructure\" with abstract: \"Applications on cloud infrastructures acquire virtual machines (VMs) from providers when necessary. The current interface for acquiring VMs from most providers, however, is too limiting for the tenants, in terms of granularity in which VMs can be acquired (e.g., small, medium, large, etc.), while giving very limited control over their placement. The former leads to VM underutilization, and the latter has performance implications, both translating into higher costs for the tenants. In this work, we leverage nested virtualization and a networking overlay to tackle these problems. We present Kangaroo, an Open Stack-based virtual infrastructure provider, and IPOPsm, a virtual networking switch for communication between nested VMs over different infrastructure VMs. In addition, we design and implement Skippy, the realization of our proposed virtual infrastructure API for programming Kangaroo. Our benchmarks show that through careful mapping of nested VMs to infrastructure VMs, Kangaroo achieves up to an order of magnitude better performance, with only half the cost on Amazon EC2. Further, Kangaroo's unified Open Stack API allows us to migrate an entire application between Amazon EC2 and our local Open Nebula deployment within a few minutes, without any downtime or modification to the application code.\"",
        "title: \"The Case For Smartphones As An Urgent Computing Client Platform\" with abstract: \"The computing world is now populated with smartphones which combine the features of a phone with a general purpose computer and come loaded with sensors including digital cameras, global positioning system (GPS) receivers, accelerometers and many more. In this paper we argue that these devices are an ideal platform for collecting data for use in urgent computing simulations. We describe how these devices will have far reaching impacts on how people connect with and organize their communities and discuss how this coincides with the rise of community driven response to disasters and the need for decentralized command and control that is discussed in disaster management literature. We also that smartphones, by providing technology which can network people without the use of centralized infrastructure, and which are carried, used, and maintained as part of daily life, are a promising platform for building distributed disaster management applications, which could be part of the inputs provided to an urgent computing simulation. In this paper we describe not only the potential of the platform, but also analyze the challenges it faces in order to realize that potential, and discuss how our middleware is designed to meet these challenges and bring about the future of disaster management applications for smartphones using urgent computing.\"",
        "title: \"Coordination Requirements for Open Distributed Systems\" with abstract: \" ion).Inter--agent actions have to be cleanly separated from intra--agent actions in order todistinguish between the concerns of coordination on one hand and of computationson the other (Separation of Concerns).Last but not least, a coordination model should be based on a rigorous formal semanticsin order to allow reasoning about specifications.3. Commercially available systems.The RM--ODP model conceptually provides the basis for commercially available systems.In this model,... \"",
        "title: \"Network performance-aware collective communication for clustered wide-area systems\" with abstract: \"Metacomputing infrastructures couple multiple clusters (or MPPs) via wide-area networks. A major problem in programming parallel applications for such platforms is their hierarchical network structure: latency and bandwidth of WANs often are orders of magnitude worse than those of local networks. Our goal is to optimize MPI's collective operations for such platforms. We use two techniques: selecting suitable communication graph shapes, and splitting messages into multiple segments that are sent in parallel over different WAN links. To optimize graph shape and segment size at runtime, we introduce a performance model called Parameterized Log P ( P \u2212 Log P ) , a hierarchical extension of the Log P model that covers messages of arbitrary length. An experimental performance evaluation shows that the newly implemented collective operations have significantly improved performance for large messages, and that there is a close match between the theoretical model and the measured completion times. Keywords Grid computing MPI Collective communication for clustered wide-area systems 1 Introduction Research on global computational infrastructures has raised considerable interest in running parallel applications on wide-area distributed systems, often called metacomputers or computational grids [8,13] . Communication insensitive applications like the ones based on the master-worker paradigm can easily be deployed in grid environments [18] . However, writing applications with frequently communicating processes is much more difficult when targeting at grids rather than traditional parallel machines, due to the presence of different (local and wide-area) networks. As the wide-area links are orders of magnitude slower than the interconnects within clusters (or MPPs), metacomputers have a hierarchical structure. In earlier work [29,30] , we discussed how collective communication libraries can be used to simplify wide-area parallel programming. We implemented an MPI-compatible library, called MagPIe, which optimizes MPI's collective operations for wide-area systems. MagPIe exploits the hierarchical structure, resulting in much less wide-area communication than other MPI libraries [30] . MagPIe's existing implementation efficiently deals with the high WAN latency but has suboptimal performance with long messages which are more sensitive to WAN bandwidth. Collective communication with long messages needs a more detailed model, including latency and bandwidth of the local and wide-area networks, the number of clusters, the number of processors in each cluster, and the length of the messages. Therefore, we introduce a new performance model for wide-area collective communication, called the parameterized Log P model ( P \u2212 Log P ), which extends the Log P model [9] . We use this model to optimize collective communication using message segmentation and tree shape determination. We optimized four important collective operations: broadcast, scatter, gather, and allgather. We further describe heuristics with which the optimizations can be performed dynamically (at runtime), based on measured model parameters of the computing platform. We currently make some simplifying assumptions about the networks, as we use regular topologies and constant latencies and bandwidths. Assuming latency and bandwidth to be constant is certainly realistic for the duration of a single collective operation. In general, MagPIe's network performance data can be updated regularly during an application run. Our fast, heuristic optimizations are designed to cope with such dynamically changing information. Assuming a regular wide-area topology allows us to focus on the impact of network performance on the design of optimized collective operations. We have evaluated our model and the optimizations on an experimental wide-area testbed, both with simulated WAN links and with a real WAN, using clusters located at four universities in The Netherlands. Our wide-area simulator allows us to study the impact of wide-area performance quantitatively and in detail. Our experiments on the real system confirmed the simulator results qualitatively. Our experimental results are encouraging to further pursue our approach. For example, while using the simulator-based system for broadcasting messages to 7 remote clusters with a 1 Mb/sB/s link each, we achieved an aggregate bandwidth of 6.65 Mb/s, compared to 0.96 Mb/s without message segmentation. Furthermore, we compared theoretically estimated and actually measured completion times and found close matches; for example, with broadcast, the difference is between 1% and 4%. Comparing our optimization heuristics with offline, exhaustive searching revealed that the heuristics missed the global optima only in a few exceptional cases. In the broadcast example, differences were always below 1%. Our ultimate goal, however, is to develop an MPI library that does not have the mentioned limitations and that adapts at runtime to changing network conditions. We intend to use our performance model in combination with dynamic information about topology and network performance, the latter as provided by the Network Weather Service [38] . To allow such runtime decisions, our optimization algorithms themselves also are efficient and are executed on-line, as part of the MagPIe library. This paper is organized as follows. In Section 2 , we introduce our parameterized Log P model. In Section 3 , we apply it to optimize collective operations in hierarchical systems. We experimentally verify our approach using a real wide-area system in Section 4 . Related work will be discussed in Section 5 . Section 6 presents our conclusions. Appendix A summarizes the symbols used throughout the paper. 2 Modeling message-passing performance To motivate our performance model, we first analyze the communication behavior of the MPI implementation on our experimentation platform, called the DAS system. DAS consists of four cluster computers, each containing Pentium Pros that are locally connected by Myrinet [6] . The clusters are located at four Dutch universities and are connected by the Dutch academic Internet backbone, SURFnet. The system is described in detail on http://www.cs.vu.nl/das/ and in [30] . Fig. 1 illustrates the general system structure we assume throughout this paper, consisting of multiple clusters with fully connected local networks and a fully connected WAN. Each cluster has a gateway that is connected both to the LAN and to the WAN. MagPIe re-implements the collective operations of a given MPI implementation on top of MPI's point-to-point communication. We run MagPIe on top of MPICH [20] , a widely used public MPI implementation, which we have ported to the wide-area DAS system. Our MPICH port uses the Panda communication sublayer [2] and implements a Panda-specific MPICH device. Panda gives access to IP and Myrinet. On Myrinet, Panda uses the LFC [5] control program. One of the DAS clusters has 128 CPUs, and has been set up to allow easy experimentation with different WAN latencies and bandwidths, by adding delay loops to the networking subsystem [30] . This wide-area simulation is part of Panda and thus is transparent to software layers on top of it, like MPI. We use this simulation system for the performance experiments reported throughout Sections 2 and 3 . We use a varying number of clusters, and simulate realistic values for wide-area latency (10 ms), and for wide-area bandwidth (1 Mb/s). In comparison, the latency over Myrinet is about 20 \u03bc s and the MPI-level throughput is about 50 Mb/s, so there is almost two orders of magnitude difference between the local and the simulated wide-area network. We emphasize that our simulation results have been measured on a real parallel machine, using the same software on the compute nodes as in the real wide-area system; only the wide-area links are simulated over some of the Myrinet links. To achieve this, the software on the gateway uses a different option of Panda. In Section 4 , we verify our simulator-based results with experiments performed on the real wide-area DAS system. Throughout this paper, we assume all message data to be in contiguous data buffers. The MPI standard also allows so-called derived data types, which may have non-contiguous memory layout, possibly causing higher processing overhead for sender and receiver. However, this additional overhead depends on the data types in use, preventing a generally applicable performance model. The influence of derived data types on communication performance is a topic of its own [21] and not covered here. The granularity of message segmentation is affected by the layering of MagPIe on top of MPI's point-to-point communication. According to MPI's philosophy, MagPIe treats messages as vectors of elements of a base type. MagPIe constructs message segments from multiple vector elements. Hence, the granularity of message segments is the size of a single vector element. This is sufficient for common-case applications using data types of small size, like vectors of floating point numbers. However, derived data types might exceptionally reach sizes larger than a suitable message segment. In such a case, MagPIe has to use suboptimal segment sizes. This problem could only be resolved by implementing MagPIe's algorithms as an integral part of an MPI library. In that case, however, MagPIe could no longer be used across multiple heterogeneous systems using their respective native MPI implementations. Our MagPIe library uses point-to-point messages provided by the underlying MPI implementation. Thus, to obtain a realistic performance model for collective operations on a clustered wide-area system, we first study the performance of point-to-point communication by looking at the Log P parameters for a local network and a wide-area network. Fig. 2 shows send overhead and gap , measured for the MPI _ Isend routine for various message sizes on both networks, Myrinet and WAN. In analogy to the Log P model, the send overhead o is the completion time of MPI _ Isend for a given message size, while the gap g is the minimum time interval between consecutive calls to MPI _ Isend . We measured those parameters with the method described in Section 2.2 . The MPI standard prescribes that the non-blocking MPI _ Isend only initiates the send operation. The actual implementation is free to perform as much of the sending task as is convenient, as long as it guarantees that it will never wait for the receiving process. The application can later check whether the transfer has actually been completed. Our MPICH port to Panda sends until it reaches a point at which it would have to block, and then it returns. For short messages, this usually means that the entire message has been sent when MPI _ Isend returns. In fact, Fig. 2 shows that over Myrinet, MPI _ Isend behaves exactly like this up to a message size of 128 KB. At this size, MPICH switches to rendezvous mode; as MPI _ Isend would now have to wait for a reply message from the receiver, it returns immediately in this case. When a slow wide-area network is used instead of the fast Myrinet, the behavior is similar, except that between 64 and 128 KB, MPI _ Isend returns as soon as it would block waiting for flow-control information from the receiver. The corresponding gap values are as expected. They start rather \u201cflat\u201d for short messages and get into a linear increase for sufficiently large messages. On the WAN, there is another non-linearity at 128 KB when the send mode changes. We can draw several conclusions from these measurements, which are useful for developing a realistic performance model. On Myrinet, the difference between overhead o and gap g is small (up to 128 KB), indicating that the end-to-end bandwidth is limited by the host computers and not by the network. On the wide-area link, on the other hand, the difference between o and g is two orders of magnitude for small messages. With this huge difference, Log P's assumption that a sender cannot transmit a message faster than g time units after a preceding message is much too pessimistic and yields misleading results for collective communication. This assumption is only true if the next message follows the same network links. With collective operations, a sender typically sends messages to different destinations in a row, so the next send can already start after o time units, but not earlier than g time units as constrained by the local area network. Another important observation from Fig. 2 is that o and g depend not only on message size and network bandwidth, but also on the behavior of the underlying MPI implementation. Collective operations thus have to be optimized carefully. Neither the assumption that MPI _ Isend will always return \u201cimmediately\u201d nor linear approximations for o and g in general give realistic performance models. 2.1 Parameterized Log P ( P \u2212 Log P ) Based on these observations, we now present the parameterized Log P model ( P \u2212 Log P ) . For a single-layer network, it defines five parameters. P is the number of processors. L is the end-to-end latency from process to process, combining all contributing factors such as copying data to and from network interfaces and the transfer over the physical network. os ( m ), or ( m ), and g ( m ) are send overhead, receive overhead, and gap. They are defined as functions of the message size m . os ( m ) and or ( m ) are the times the CPUs on both sides are busy sending and receiving a message of size m . For sufficiently long messages, receiving may already start while the sender is still busy, so os and or may overlap. The gap g ( m ) is the minimum time interval between consecutive message transmissions or receptions along the same link or connection. It is the reciprocal value of the end-to-end bandwidth from process to process for messages of a given size m . Like L , g ( m ) covers all contributing factors. From g ( m ) covering os ( m ) and or ( m ), follows g ( m )\u2a7e os ( m ) and g ( m )\u2a7e or ( m ). A network N is characterized as N =( L , os , or , g , P ). To illustrate how the parameters are used, we introduce s ( m ) and r ( m ), the times for sending and receiving a message of size m when both sender and receiver simultaneously start their operations. s ( m )= g ( m ) is the time at which the sender is ready to send the next message. Whenever the network itself is the transmission bottleneck, os ( m )< g ( m ), and the sender may continue computing after os ( m ) time. But because g ( m ) models the time a message \u201coccupies\u201d the network, the next message cannot be sent before g ( m ) time. r ( m )= L + g ( m ) is the time at which the receiver has received the message. The latency L can be seen as the time it takes for the first bit of a message to travel from sender to receiver. The message gap adds the time after the first bit has been received until the last bit of the message has been received. Fig. 3 illustrates this model. When a sender transmits several messages in a row, the latency will contribute only once to the receiver completion time but the gap values of all messages sum up. This can be expressed as r ( m 1 ,\u2026, m n )= L + g ( m 1 )+\u22ef+ g ( m n ). For completeness, we show that parameterized Log P subsumes the original model Log P [9] and its version for long messages, Log GP [1] . In Table 1 , Log GP's parameters are expressed in terms of parameterized Log P. We use 1 byte as the size for short messages; any other reasonable \u201cshort\u201d size may as well be used instead. Note that neither Log P nor Log GP distinguishes between os and or . For short messages, they use r = o + L + o to relate the L parameter to receiver completion time which gives L a slightly different meaning compared to parameterized Log P. We use this equation to derive Log P's L from our own parameters. For clustered wide-area systems, we use two parameter sets, identified by a subscript l for the LAN and w for the WAN. For example, L l denotes the latency within a cluster and L w is the latency when the sender and receiver are in different clusters. For a local network we get: s l (m)=g l (m), r l (m)=L l +g l (m). A wide-area transmission takes three steps: the sender forwards the message to its local gateway, which in turn sends the message to the gateway of the receiver's cluster, which finally forwards the message to the receiving node. The value of r w always depends on the wide-area bandwidth and can be expressed in analogy to r l . The value of s w may either be determined by wide-area overhead os w ( m ) or local-area gap g l ( m ), whichever is higher: the sender cannot send the next message before g l ( m ) time, but it might have to wait even longer, for example while waiting for flow-control information from its wide-area peer. Unlike the local-area case, the sender is decoupled from the wide-area gap. This gives us the following equations for the wide-area case: s w (m)= max g l (m),os w (m) , r w (m)=L w +g w (m). 2.2 Efficient parameter measurement for P \u2212 Log P An important issue is how to measure the P \u2212 Log P parameters described above on an actual wide-area system. Previous Log P micro benchmarks [10,23] measure the gap values by saturating the link for each message size. However, this approach is too intrusive to be feasible for wide-area links which are typically shared with many other users. Our method has to use saturation only for obtaining g (0), which can be done fast and thus is hardly intrusive. In [28] we showed that our measurement procedure, compared to the saturation-based method, yields the same results while reducing the link utilization by 90\u201395%. Below, we summarize this method. As we use g (0) for deriving other values, we measure it first using two processes, measure and mirror (see Fig. 4 ). We measure the time RTT n (0) for a roundtrip consisting of n messages sent in a row by measure, and a single, empty reply message sent back by mirror. We take the time measured (after subtracting RTT 1 (0)/2 for the reply) as n \u00b7 g (0). The procedure first uses n =1 and then n =10. From this value on n is doubled until the gap per message changes only by \u03f5 =1%. At this point, saturation is assumed to be reached and we compute the gap from the measurement with the so-far largest n . We start with a small value for n (to speed up the measurement) and double it until the roundtrip time is dominated by bandwidth rather than latency, namely until RTT 1 (0)< \u03f5 \u00b7 RTT n (0) holds in addition to the saturation test. By waiting for a reply we enforce that the messages are really sent to mirror instead of just being buffered locally. All other parameters can be determined by the procedure shown in Fig. 4 . It starts with a synchronization message by which the mirror process indicates being ready. For each size m , two message roundtrips are necessary from measure to mirror and back. (We use RTT ( m )= RTT 1 ( m ).) In the first roundtrip, measure sends an m -bytes message and in turn receives a zero-bytes message. We measure the time for just sending and for the complete roundtrip. The send time directly yields os ( m ). g ( m ) and L can be determined by solving the equations for RTT (0) and RTT ( m ), according to the timing breakdown in Fig. 3 : RTT(0)=2(L+g(0)), RTT(m)=L+g(m)+L+g(0), g(m)=RTT(m)\u2212RTT(0)+g(0), L=(RTT(0)\u22122g(0))/2. In the second roundtrip, the measure process sends a 0-bytes message, waits for \u0394 > RTT ( m ) time, and then receives an m -bytes message. Measuring the receive operation now yields or ( m ), because after \u0394 > RTT ( m ) time, the message from mirror is available at measure for immediate receiving. For each message size, the roundtrip tests are initially run a small number of times. As long as the variance of measurements is too high, we successively increase the number of roundtrips until a sufficiently small (90%) confidence interval is obtained, or until an upper bound on the total number of iterations is reached (60 for small messages, 15 for large messages). In the latter case, we trade accuracy for non-intrusiveness. Initially, measurements are performed for all sizes m =2 k with k \u2208[0, k m ]. The value of k m has to be chosen large enough to cover any non-linearity caused by the tested software layer. In our experiments, we used k m =18 to cover all changes in send modes of the assessed MPI implementation (MPICH). After measuring the initial set of message sizes, we check whether the gap per byte ( g ( m )/ m ) has stabilized for large m . If this is not the case, sending larger messages may achieve lower gaps (and hence higher throughput). So k m is incremented and the next message size is tested. This process is performed until g (2 k m ) is close (within \u03f5 ) to the value linearly extrapolated from g (2 k m \u22122 ) and g (2 k m \u22121 ). So far, the \u201cinteresting\u201d range of message sizes has been determined. Finally, possible non-linear behavior remains to be detected. For any size m k , we check whether the measured values for os ( m k ), or ( m k ), and g ( m k ) are consistent with the corresponding predicted values for size m k , extrapolated from the measurements of the previous two (smaller) message sizes, m k \u22121 and m k \u22122 . If the difference is larger than \u03f5 , we do new measurements for m =( m k \u22121 + m k )/2, and repeat halving the intervals until either the extrapolation matches the measurements, or until m k \u2212m k\u22121 \u2a7d max (32 B ,\u03f5\u00b7m k ) . The measurement procedure described above assumes that network links are symmetrical, such that sending from measure to mirror has the same parameters as for the reverse direction. However, this assumption may not always be true. On wide-area networks, for example, the achievable bandwidth (the gap) and/or the network latency may be different in both directions, due to possibly asymmetric routing behavior or link speed. Furthermore, if the machines running the measure and mirror processes are different (like a fast and a slow workstation), then also the overhead for sending and receiving may depend on the direction in which the message is sent. In such cases, the parameters os , or , and g may be measured by performing our procedure twice, while switching the roles of measure and mirror in between. Asymmetric latency can only be measured by sending a message with a timestamp t s , and letting the receiver derive the latency from t r \u2212 t s , where t r is the receive time. This requires clock synchronization between sender and receiver. Without external clock synchronization (like using GPS receivers or specialized software like the network time protocol , NTP), clocks can be synchronized either by statistical time estimation or by simple message exchange protocols. Statistical time estimation [32] is highly intrusive to the network and thus not feasible for WANs. Simple message exchange protocols allow synchronization only up to a granularity of the roundtrip time between two hosts [34] , which is useless for measuring network latency. Unfortunately, as we cannot generally assume the clocks of (possibly widely) distributed hosts to be tightly synchronized, asymmetric network latencies cannot be measured, neither with our framework nor with previous benchmarks [10,23] . 3 Performance-aware collective communication We now illustrate how the P \u2212 Log P model can be used to optimize collective communication. We discuss four important operations: broadcast, scatter, gather, and allgather. With broadcast, a single process (called the root ) sends a message to all other processes. Scatter is also known as personalized broadcast. Here, the root splits a large message vector and sends individual messages to the other processes. Gather is the inverse operation of scatter. Here, the root collects messages from its peer processes into a large message vector. Finally, allgather is defined like a gather operation, except that all processes receive the result, instead of just the root. For the exchange of data this implies that for allgather, all processes conceptually have to broadcast a message. 3.1 Broadcast With broadcast, a single process (called the root) sends a message of size M to all other ( P \u22121) processes. Optimal broadcast algorithms use tree-shaped communication graphs, so every process receives the message exactly once [25] . MagPIe's original broadcast algorithm was optimized for wide-area networks by sending the message only once to each cluster and by avoiding transmission paths that contain more than one wide-area link. In MagPIe's algorithm, one of the application processes is selected in each cluster to act as a so-called coordinator node. The root process acts as coordinator of its own cluster. First, the root sends the message to the other coordinator nodes, forming a flat tree in the WAN. As soon as a coordinator receives the message, it forwards it to the other nodes of its cluster, using a binomial tree shape in the LANs. A disadvantage of MagPIe's original algorithm is that it forwards complete messages down the spanning tree [30] . For large messages, this leads to poor link utilization. As large messages have a high send overhead, the root can only send over one WAN link at a time. Our goal for the optimized algorithms is to use the accumulated bandwidth of several (or all) available WAN links. Because a cluster gateway decouples LAN and WAN packets, the root can quasi-simultaneously send small packets over up to n =\u230a g w ( m )/ g l ( m )\u230b WAN links. We use message segmentation to achieve such a better link utilization. Instead of forwarding complete messages down the spanning tree, we split each message into k segments of size m (where k =\u2308 M / m \u2309) and forward each incoming segment down all links. In this way, there will be much more overlap in communication over different links. In addition to message segmentation, we optimize the shape of the spanning tree by trading send overhead for latency. We try to minimize the total completion time of the broadcast (i.e., the time when the last receiver has obtained the complete message). This optimization problem can be stated more accurately as follows: Given a network N and the message size M , our goal is to find a tree shape and a segment size m that together minimize the completion time. We approach this problem in two steps. First, we develop a single-layer broadcast algorithm and its performance model, assuming that all links have the same speed. This algorithm can be used to optimize either communication within a cluster (i.e., all links are fast local networks) or communication between nodes in different clusters (i.e., all links are slow wide-area networks). Next, we develop a two-layer algorithm for the hierarchical model of metacomputers described in Section 1 . Two-layer algorithms are more complicated to model, but are more efficient for broadcast operations. 3.1.1 Single-layer broadcast The optimal tree shape depends on the network parameters, as well as on M and m . In general, we characterize a tree by two parameters, its height h and its degree d . h is the longest path from the root to any of the other nodes, determined by counting edges. d is the maximum number of successor nodes of any node in the graph. For example, MagPIe's flat WAN tree has d = P \u22121 and h =1, where P in this case denotes P w , the number of clusters. Depending on the actual network parameters, the optimal tree shape (yielding the minimal completion time) can have any d , h \u2208[1\u2026 P \u22121]. Fortunately, d and h of the optimal tree are related to each other, and we can compute the minimal height of a tree with P nodes and degree d as the smallest h \u2a7e1 for which \u2211 i =0 h d i \u2a7e P is true. Similar to the parameters for latency L and gap g ( m ) for a single message send, we define latency \u03bb ( m ) and gap \u03b3 ( m ) of a broadcast tree. Here, \u03bb ( m ) denotes the time at which a message segment has been received by all nodes, after the root process started sending it. \u03b3 ( m ) is the time interval between the sending of two consecutive segments. ( \u03b3 ( m ) hence indicates the throughput of a broadcast tree.) We compute the completion time T of a broadcast algorithm with k message segments of size m as: T= (k\u22121)\u00b7\u03b3(m)+\u03bb(m). To illustrate this formula, we can express the values for \u03b3 ( m ) and \u03bb ( m ) for MagPIe's flat WAN tree as: \u03b3(m)= max g(m),(P\u22121)\u00b7s(m) , \u03bb(m)=(P\u22122)\u00b7s(m)+r(m). Here, \u03b3 ( m ) is the maximum of the gap between two segments of size m sent on the same link and the time the root needs for sending ( P \u22121) times the same segment on disjoint links. The corresponding value for \u03bb ( m ) is the time at which a message segment is sent to the last node, plus the time until it is received. For a general tree shape, upper bounds for both parameters can be expressed depending on the degree d and height h of a broadcast tree: \u03b3(m)\u2a7d max g(m),or(m)+d\u00b7s(m) , \u03bb(m)\u2a7dh\u00b7 (d\u22121)\u00b7s(m)+r(m) . \u03b3 ( m ) is the maximum of the gap caused by the network, and the time a node needs to process the message. For intermediate nodes, this is the time to receive the message plus the time to forward it to d successor nodes. (For the root and for leaf nodes, it is either one of both.) The exact value of \u03bb ( m ) depends on the order in which the root process and all intermediate nodes send to their successor nodes and which path leads to the node that receives the message last. For the rest of this paper, we approximate \u03b3 ( m ) and \u03bb ( m ) by their upper bounds, given by the inequalities. The optimal broadcast tree depends on the number of processors P and the message size M . Both values are parameters of MPI _ Bcast . This implies that the optimal broadcast tree and segment size have to be computed at runtime for each invocation of MPI _ Bcast . Optimization at runtime has to be very fast, so it does not outweigh the performance improvement of applying the optimized algorithm. Therefore, we avoid communication between processes and we avoid doing an exhaustive search over the complete search space with m \u2208[1\u2026 M ] and d \u2208[1\u2026 P \u22121]. Communication is avoided by replicating the network performance information over all application processes. When calling MPI _ Bcast , all processes simultaneously compute the optimal tree and segment size. We apply heuristics to reduce the number of segment sizes and tree shapes to be investigated. In general, several segment sizes are tried out, and for each size the optimal tree shape is computed. For a given segment size m , we investigate only the following tree shapes: 1. d \u2208[\u230a g ( m )/ s ( m )\u230b\u2026 P \u22121]. Lower values for d can only lead to higher completion times, because less accumulated bandwidth would be used. 2. Starting with d =\u230a g ( m )/ s ( m )\u230b, we increase d while only investigating those values of d for which the height h will be reduced. Values for d in between would increase \u03b3 ( m ) but would not decrease \u03bb ( m ), and could thus not improve completion time. For the segment size, we only evaluate \u201cuseful\u201d values. The segment size must be a multiple of the size of the basic data type to be transmitted (in MPI terms, the extent) and it must split the initial message into k equal segments. The segment size m is determined in two steps: 1. In a binary search, segment sizes m=M/2 i , i\u2208[0\u2026 log 2 M] are investigated. For each value for m , the degree d with minimal completion time is determined. 2. Starting from the best value m \u2032 found in step 1, a local hill-climbing strategy is performed. k \u2032 =\u2308 M / m \u2032 \u2309 is the so-far best known number of segments. With the respectively best value for d , the completion times are computed for k \u2032 \u22125, k \u2032 \u22121, k \u2032 +1, and k \u2032 +5. We then replace k \u2032 by the value with the best completion time. This is performed until no further improvement can be found. The simultaneous look-ahead of 1 and 5 steps helps overcoming local minima. Furthermore, it speeds up the process when a minimum is far away from the starting point computed in step 1. Having identical links inside a network, the actual broadcast tree can be constructed from d in O ( P ) time by keeping track of each node's degree while assigning receivers to senders. 3.1.2 Two-layer broadcast So far, we have optimized broadcast for single-layer networks. Given optimized broadcast trees for the WAN and for the LANs, a simple way to obtain a two-layer algorithm is the sequential composition, as performed in the original MagPIe library. Here, the coordinator nodes first participate in the wide-area broadcast. Then, they forward the message inside their clusters. This leads to a total completion time of: T=T w +T l =(k w \u22121)\u00b7\u03b3 w (m w )+\u03bb w (m w )+(k l \u22121)\u00b7\u03b3 l (m l )+\u03bb l (m l ). A better integration into a two-layer broadcast can be achieved by pipelining both phases. Here, the coordinator nodes immediately forward segments, first to other coordinators, and then to their successor nodes in the LAN tree structure. We use the same segment size for the WAN and for the LANs, so coordinators do not need to re-assemble segments. This leads to: T=(k\u22121)\u00b7\u03b3(m)+\u03bb w (m)+\u03bb l (m). To reflect the double forwarding in the coordinator nodes, we use: \u03b3(m)\u2a7d max g w (m),or w (m)+d w \u00b7s w (m)+d l \u00b7s l (m) . The optimization of the two-layer broadcast for a given m additionally requires taking care of \u03bb l ( m ) and \u03b3 \u2032( m ), the latter denoting the fraction of \u03b3 ( m ) that can be used for the LAN without slowing down the WAN. We then have to investigate the LAN trees with d \u2208[1\u2026\u230a \u03b3 \u2032( m )/ s l ( m )\u230b] with: \u03b3 \u2032 (m)= max g w (m)\u2212or w (m)\u2212d w \u00b7s w (m),s l (m) . 3.1.3 Performance evaluation We have implemented a two-layer broadcast as described so far as part of the MagPIe library. Fig. 5 compares the completion times of this new algorithm with MagPIe's original broadcast, measured on the DAS experimentation system described in Section 2 . The new algorithm does message segmentation, and pipelines WAN and LAN forwarding. The original algorithm sends complete messages, and sequentially combines WAN and LAN forwarding. We measured configurations with four clusters (with 1 or 16 CPUs) and eight clusters (with 1 or 8 CPUs). The number of CPUs per cluster has hardly any effect on the overall completion time, causing the respective pairs of lines in Fig. 5 to be almost the same. This, and the fact that for short messages the original algorithm performs approximately as fast as the segmented broadcast, both support our original design goals for MagPIe. For larger messages, however, the new algorithm is much faster than the original one and achieves much higher aggregate wide-area bandwidth. For example, with eight clusters of 1 CPU each, broadcasting a 1 MB message takes 1079 ms with segmentation, and 7425 ms without. This corresponds to an aggregate bandwidth of 6.65 Mb/s for segmentation. This is 95% of the actually available bandwidth which is 7\u00d71 Mb/s. Without segmentation, the wide-area links are used sequentially, resulting in an aggregate bandwidth of only 0.96 Mb/s, or 14% of what is available. With fragmentation, the completion times for four and eight clusters are about the same (for 1 MB messages, 1072 ms with four clusters and 1079 ms with eight clusters), whereas the original algorithm takes much longer for eight clusters than for four clusters (for 1 MB messages, 3200 ms with four clusters and 7425 ms with eight clusters). This clearly shows that segmentation allows us to use all available wide-area links in parallel as long as the local-area network has enough bandwidth to feed them all. Although hard to see from the graphs, the pipelined message forwarding further reduces the overall completion time. With the original algorithm, the sequential combination of WAN and LAN broadcast adds some additional time. With 4 MB messages and four clusters, for example, the forwarding takes 13224.3\u221212723.3=501 ms. This overhead disappears with pipelining. We also investigated the quality of our theoretical estimates. Fig. 6 shows the estimation error, namely the difference between estimated and measured completion times. For short messages it is up to 4%, whereas for large messages the deviation is less than 1%. In general, this is a very close match between our performance model and the implementation. The slightly larger difference for short messages is mostly due to the fact that our measurements also include the time for computing the optimized tree. We also compared the difference between the best heuristically found completion times and the global optima, which were computed offline by an exhaustive search. Our heuristics found the global optimum in almost all test cases, in the few exceptional cases differences were always below 1%. 3.2 Scatter The second collective operation we discuss is scatter, which is also called personalized broadcast. With scatter, the root process holds M \u00b7 P data items which it equally distributes across the P processes, including itself. With homogeneous networks, optimal scatter algorithms use flat trees as communication graphs. (Because each processor has to receive a personalized message, forwarding does not help improving the completion time.) Only for very small messages, message combining and forwarding via coordinator nodes may improve performance. However, we do not investigate message combining here, because we focus on large messages. Adding message combining to our performance model would be straightforward, by using segment sizes m \u2208[ M \u2026 M \u00b7 P ] over the WAN and a two-level algorithm, as for broadcast. With a fixed communication graph (a flat tree encompassing all nodes in all clusters), our analytical model combines both WAN and LAN parameters. We optimize scatter algorithms by finding a message segment size that yields the shortest overall completion time. P w denotes the number of clusters and P l the (maximum) number of processes per cluster. Analogous to broadcast, we model T =( k \u22121)\u00b7 \u03b3 ( m )+ \u03bb ( m ) with: \u03b3(m)\u2a7dP l \u00b7 max (g w (m),(P w \u22121)\u00b7s w (m)+s l (m))+or l (m), \u03bb(m)\u2a7d(P l \u22121)\u00b7 max (g w (m),(P w \u22121)\u00b7s w (m)+s l (m)) +(P w \u22121)\u00b7s w (m)+r w (m). \u03b3 ( m ) models the time spent at the root process, because for other processes, the gap per segment is much smaller, namely or ( m ). The model assumes that the message segments are first sent to the first processor in each cluster, then to the second processor in each cluster, etc., using the wide-area links in a round-robin fashion. Before the root process can send out the next segment, it has to receive the message segment it just sent to itself. \u03bb ( m ) denotes the time at which the last message of a segment round is sent, plus its receiving time. This sending time adds the time for sending the segment to all but one processors in each cluster, and the time for sending to the last processor in all but one clusters. \u03bb ( m ) equals its upper bound when the last message is sent to a process in a remote cluster. Otherwise it is somewhat less, depending on which message is actually received last. As with broadcast, we use the upper bounds for optimizing T and use the binary search with subsequent hill climbing for finding a near-optimal segment size. Fig. 7 compares the completion times of the new scatter algorithm with MagPIe's original scatter. The new algorithm does message segmentation whereas the original algorithm sends complete messages. We measured the same configurations as with broadcast. The new algorithm performs much better, because it achieves higher aggregate WAN bandwidth. For example, with eight clusters of 1 CPU each, sending a 1 MB message to each receiver takes 1078 ms with segmentation, and 7437 ms without. The completion times for large messages are proportional to g w ( M )\u00b7 P l , which indicates that almost all available WAN bandwidth can be utilized. Fig. 8 compares the theoretically estimated completion times with the ones actually measured on our system. For small messages, the estimation error is slightly worse than for broadcast (up to 14%), but for large messages it is only about 1%, denoting a very close match between theory and practice. As with broadcast, our search heuristics find the global optimum in almost all test cases, with a few negligible deviations (smaller than 1%). 3.3 Gather Gather is the inverse operation of scatter. Here, each process (including the root) holds M data items which are sent to the root. Analogous to scatter, optimal algorithms use flat trees. The completion time can be modeled as T =( k \u22121)\u00b7 \u03b3 ( m )+ \u03bb ( m ) with: \u03b3(m)= max P l \u00b7g w (m),P l \u00b7or l (m)+(P w \u22121)\u00b7P l \u00b7or w (m)+os l (m) , \u03bb(m)= max L w +P l \u00b7g w (m), max (L l ,os l (m))+P l \u00b7or l (m) +(P w \u22121)\u00b7P l \u00b7or w (m) . \u03b3 ( m ) and \u03bb ( m ) model the time at which the root process completes receiving the message segments. \u03b3 ( m ) is the maximum of the time the segments need to cross the WAN links, and the time the root needs to receive all messages of a segment round, plus sending its own segment to itself. \u03bb ( m ) is similar, but models the overlap of latency and receiving at the root. Fig. 9 shows the measured completion times for the same configurations as with broadcast and scatter. We compare MagPIe's original algorithm with the new segmenting variant. The results are similar to the ones achieved with MPI _ Scatter . This time, message segmentation helps simultaneously receiving on all connections, leading to better WAN bandwidth utilization. Fig. 10 compares the theoretically estimated completion times with the ones actually measured on our system. As with broadcast and scatter, the estimation errors are mostly negligible, except for a few cases with large messages and many simultaneously sending nodes. Here, some effects related to contention inside the receiving node cause somewhat higher completion times than expected by the theoretical model. As with broadcast and scatter, the search heuristics hardly ever miss the globally optimal configurations. 3.4 Allgather Allgather is like the gather operation, except that all data are delivered at every node, and not just at a single root. One simple way to implement allgather is as a sequence of gather operations, using every node as root. However, in this way the scarce bandwidth available at the WAN links is not used efficiently. An alternative, which is currently implemented in MagPIe, is to first let the coordinator of every cluster gather the data of all local nodes; next exchange that data with all other cluster coordinators; and finally let the coordinators broadcast the whole data vector locally in their clusters. This approach works particularly well when there is a big difference in performance in the LAN and WAN networks, since pipelining issues (as encountered in the optimization of the broadcast implementation) are much less important than the efficient use of the slow WAN links. The completion time of this implementation can be modeled as the sum of local gather, wide-area allgather (between the coordinator nodes), and the local broadcast. The time for gather and broadcast has been outlined above; in the following we only deal with the allgather operation between the coordinator nodes. Its completion time is T =( k \u22121)\u00b7 \u03b3 ( m )+ \u03bb ( m ) with: \u03b3(m)= max (P w \u22121)\u00b7(s w (m)+ max g l (m),or w (m) ,g w (m) , \u03bb(m)=(P w \u22122)\u2217s w (m)+g w (m)+L w . The segmenting allgather consists of k rounds in which each node sends a segment to all its peers and receives one segment per peer. \u03b3 ( m ) thus is the maximum of the wide-area gap and the processing time per segment, the latter is the number of peers multiplied by the sum of the completion time for send and receive. The receive completion time is the maximum of the local gap and the receive overhead. \u03bb ( m ) is, as usual, the time when a node sends the last message, plus the time to deliver it to the respective receiver. We compared MagPIe's original implementation of MPI _ Allgather with one that uses segmentation in the central exchange phase between the cluster coordinators. The results for four and eight clusters are shown in Fig. 11 . For eight clusters, and using a segment size smaller than 64 K, the optimized version avoids performance loss due to flow control issues for message sizes between 64 and 128 K. However, for most other message sizes, and for four clusters, the results are very similar, since the original MagPIe implementation is already keeping the WAN links well occupied. Finally, Fig. 12 compares (for the central phase between the coordinator nodes) the theoretically estimated completion times with the ones actually measured on our system. For a few configurations, the estimation error is up to 16% which can be explained by contention inside the nodes due to simultaneously receiving from multiple peers. 4 Experimental results on the real wide-area system The results presented so far have been obtained using the wide-area simulator of our Panda communication sublayer. The simulator allows to perform measurements on a real parallel machine with only the wide-area links being simulated, by adding delay loops in the Panda gateway nodes. With the simulator, we were able to investigate our collective communication operations in a clean environment without interference of network traffic caused by other users. This allowed us to quantitatively analyze our results. In this section, we present experimental results from the real system, the four DAS clusters located at Vrije Universiteit Amsterdam (VU), at Universiteit Leiden, the University of Amsterdam (UvA), and Delft University of Technology. The clusters are connected via the Dutch academic Internet backbone (SURFnet). Table 2 summarizes the average TCP-level bandwidth (in Mb/s) and one-way latency (in ms) between the DAS clusters. As can be seen in the table, the latency between the clusters varies from 1.5 to 4.0 ms. Bandwidth ranges from 3.0 to 28.0 Mb/s. These values for bandwidth and latency indicate that, with the DAS, wide-area communication is somewhat faster than in our simulated environment. This can be attributed to the rather small physical distances (up to 50 km) between the clusters. Additionally, the network parameters vary over time due to concurrent network traffic. Furthermore, the wide-area links are asymmetric due to the routing setup. Therefore, the purpose of the experiments presented in this section is to qualitatively verify our results from Section 3 on a real wide-area system. The collective communication operations presented in Section 3 are based on P \u2212 Log P parameters for both local-area and wide-area networks. Fig. 13 shows send overhead and gap on four of the 12 links between the DAS clusters. The two graphs on the right-hand side of Fig. 13 (VU to UvA and VU to Delft) closely resemble the measurements on the simulated WAN in Fig. 2 . However, the sender can transmit messages larger than 64 KB without stalling while waiting for flow-control information from the receiver. This is due to the lower wide-area latency. Unlike our clean simulated environment, the Leiden cluster is connected to the rest of the DAS via a very lossy link. The frequent packet losses not only degrade bandwidth, they also lead to various artefacts with the measured gap values. But even with such a lossy link, the shape of the measured curves qualitatively matches the results from the simulated environment. The collective algorithms presented in Section 3 assume a homogeneous wide-area network with identical P \u2212 Log P parameters for all links. However, as shown in Table 2 , the wide-area links of the DAS perform somewhat differently from each other. For measuring the collective operations on the real wide-area system, we have used P \u2212 Log P parameters according to the slowest available link (from Leiden to Delft). With this conservative approach, message segmentation and tree shape are determined according to the \u201cbottleneck\u201d WAN link. Fig. 14 shows the completion times of the four collective operations (broadcast, scatter, gather, and allgather) across the four DAS clusters. As in Section 3 , we measured completion times for four clusters with 1 and with 16 processors each. The results qualitatively confirm the simulator measurements. Due to the somewhat faster wide-area links, however, the effects are less pronounced. But still, starting with a message size between 16 and 64 KB, the implementation based on message segmentation shows better performance than MagPIe's original implementation. 5 Related work Log P [9] and Log GP [1] are direct precursors of parameterized Log P. Having constant values for overhead and gap, Log P is restricted to short messages whereas Log GP adds the gap per byte for long messages, assuming linear behavior. Neither of them handles overhead for medium-sized to long messages correctly, nor do they model hierarchical networks. Within their limitations, they have been used to study collective communication [1,4,9,25] . The parameterized communication model [33] has two parameters which also depend on message size, resembling P \u2212 Log P 's sender and receiver completion times. Unfortunately, the model fails to adequately capture receiver overhead, as needed by the collective operations discussed in this paper. Bruck et al. [7] studied broadcast and allreduce in homogeneous networks using measured machine parameters and the postal model. Santos [35] studies optimality of k -item broadcast algorithms in a theoretical, simplified Log P variant. The problem of a k -item broadcast comes close to broadcasting large messages split into k smaller segments as presented in this paper. Multicast based on packetization is studied in [26] where the underlying network determines the size of the data items being delivered to multiple receivers. In a previous paper, we focussed on optimizing MPI's broadcast operation by message segmentation and tree shape optimization [27] . Van de Geijn et al. studied efficient pipelined implementations of broadcast [37] and reduction [36] on homogeneous one-level networks, like meshes and hypercubes. Some work has been performed on optimizing single collective operations (e.g., broadcast) for clusters of SMPs which (like wide-area networks) also exhibit hierarchical structures [16,22] , however without using message segmentation to accommodate hierarchical networks. Others study collective operations in networks of heterogeneous workstations rather than hierarchically structured systems [3,31] . Compositions of collective operations are optimized in [17] , but performance is studied only in the homogeneous case. Our work currently makes simplifying assumptions about the wide-area networks. Karonis et al. [24] apply optimizations to MPI's broadcast for hierarchical systems with more than two layers. The authors show performance improvements compared to the non-segmenting version of MagPIe for a system with three hierarchy layers. However, identifying a hierarchical system representation seems to be a challenging problem for the general case of Internet-based metacomputing platforms. Several metacomputing projects are currently building the infrastructure on top of which our MagPIe library may utilize distributed computing capacity [11,12,14,19] . The Interoperable MPI Protocol (IMPI) [15] specifies collective communication algorithms for clustered systems while focusing on interoperability rather than performance. 6 Conclusions Earlier research has shown that many parallel applications can be optimized to run efficiently on a hierarchical wide-area system and that collective communication is a useful abstraction to do some of these optimizations transparently. In this paper, we described two important optimizations for such wide-area collective operations. First, we use message segmentation to split messages into smaller units that can be sent concurrently over different wide-area links, resulting in better link utilization. Second, we determine the tree shape for the collective operations based on properties of the underlying system (such as the LAN and WAN performance and the processor to cluster mapping), instead of using a fixed shape. Both optimizations reduce the completion time for large messages. For both optimizations, we need a performance model that can accurately estimate the completion time of collective operations. Most existing (Log P-based) models are inaccurate for collective operations on hierarchical systems with fast local networks and slow wide-area networks. We described a new model, the P \u2212 Log P (parameterized Log P) model, which has different sets of Log P parameters for both networks. Also, our model makes these parameters a function of the message size, and uses measured values as input. We have used the P \u2212 Log P model to optimize four important operations in our MagPIe library (broadcast, scatter, gather, and allgather). The new library computes a near-optimal segment size and tree shape at runtime, based on various system properties. The optimization algorithms use heuristics to prune the search space, so they are efficient and could be used with dynamic information (such as produced by NWS [38] ) as input, although our current implementation uses static network performance data. We have empirically validated our approach. Experiments show that the new algorithms significantly improve collective performance for large messages. In the controlled simulator environment, our experiments show that the performance model accurately predicts the completion times. Our algorithms improve performance also on the real wide-area system where performance information is imprecise. We think the new algorithms are an important step towards a convenient, easy-to-use infrastructure for parallel programming on wide-area systems like computational grids. The current library can be used for programming hierarchical systems with similar network performance on the wide-area links, like our DAS system. Other possible target platforms are clusters of SMP nodes that have a similar network hierarchy. The techniques developed here also are a good basis for an MPI library that adapts itself dynamically to changing conditions in the networks. Building such a library is the next step in our research. Acknowledgements This work is supported in part by a USF grant from the Vrije Universiteit. The wide-area DAS system is an initiative of the Advanced School for Computing and Imaging (ASCI). We thank Peter Merz (University of Siegen) for his valuable advice on fast optimization techniques and Gr\u00e9gory Mouni\u00e9 for his contributions to this paper. Finally, we thank John Romein for keeping the DAS in good shape. Appendix A Symbols used in analytical modeling M total message size m message segment size k number of message segments, k= M/m L network latency os(m) send overhead for message of size m or(m) receive overhead for message of size m g(m) gap between two messages of size m P number of processors N network, N=(L,os,or,g,P) T completion time of a collective operation s(m) sender completion time for message of size m r(m) receiver completion time for message of size m RTT(m) round-trip time for message of size m (empty reply message) h height of a broadcast tree d degree (fan out) of a broadcast tree \u03bb(m) latency of a broadcast tree with message of size m \u03b3(m) gap of a broadcast tree with message of size m l subscript, used to identify LAN parameters w subscript, used to identify WAN parameters References [1] A. Alexandrov, M.F. Ionescu, K.E. Schauser, C. Scheiman, Log GP: incorporating long messages into the Log P model \u2013 one step closer towards a realistic model for parallel computation, in: Proceedings of the Symposium on Parallel Algorithms and Architectures (SPAA), Santa Barbara, CA, July 1995, pp. 95\u2013105 [2] H. Bal R. Bhoedjang R. Hofman C. Jacobs K. Langendoen T. R\u00fchl F. Kaashoek Performance evaluation of the Orca shared object system ACM Trans. Comput. Syst. 16 1 1998 1 40 [3] M. Banikazemi, V. Moorthy, D. Panda, Efficient collective communication on heterogeneous networks of workstations, in: International Conference on Parallel Processing, Minneapolis, MN, August 1998, pp. 460\u2013467 [4] M. Bernaschi G. Iannello Collective communication operations: experimental results vs. theory Concurrency: Practice and Experience 10 5 1998 359 386 [5] R. Bhoedjang T. R\u00fchl H. Bal User-Level network interface protocols IEEE Comput. 31 11 1998 53 60 [6] N. Boden D. Cohen R. Felderman A. Kulawik C. Seitz J. Seizovic W. Su Myrinet: a gigabit-per-second local area network IEEE Micro 15 1 1995 29 36 [7] J. Bruck L.D. Coster C.-T. Ho R. Lauwereins On the design and implementation of broadcast and global combine operations using the postal model IEEE Trans. Parallel Distrib. Syst. 7 3 1996 256 265 [8] C. Catlett L. Smarr Metacomputing Commun. ACM 35 1992 44 52 [9] D. Culler, R. Karp, D. Patterson, A. Sahay, K.E. Schauser, E. Santos, R. Subramonian, T. von Eicken, Log P: towards a realistic model of parallel computation, in: Proceedings of the Symposium on Principles and Practice of Parallel Programming (PPoPP), San Diego, CA, May 1993, pp. 1\u201312 [10] D.E. Culler L.T. Liu R.P. Martin C.O. Yoshikawa Assessing fast network interfaces IEEE Micro 16 1 1996 35 43 [11] G.E. Fagg, K.S. London, J.J. Dongarra, MPI_Connect: managing heterogeneous MPI applications interoperation and process control, in: Proceedings of the 5th European PVM/MPI Users' Group Meeting, number 1497 in LNCS, Liverpool, UK, 1998, pp. 93\u201396 [12] I. Foster C. Kesselman Globus: a metacomputing infrastructure toolkit Int. J. Supercomput. Appl. 11 2 1997 115 128 [13] I. Foster, C. Kesselman (Eds.), The GRID: Blueprint for a New Computing Infrastructure, Morgan Kaufmann, Los Altos, MA, 1998 [14] E. Gabriel, M. Resch, T. Beisel, R. Keller, Distributed computing in a heterogeneous computing environment, in: Proceedings of the 5th European PVM/MPI Users' Group Meeting number 1497 in LNCS, Liverpool, UK, 1998, pp. 180\u2013187 [15] W. George, J. Hagedorn, J. Devaney, Status report on the development of the interoperable MPI protocol, in: Proceedings of MPIDC'99, Message Passing Interface Developer's and User's Conference, Atlanta, GA, March 1999, pp. 7\u201313 [16] M. Go\u0142ebiewski, R. Hempel, J.L. Tr\u00e4ff, Algorithms for collective communication operations on SMP clusters, in: The 1999 Workshop on Cluster-Based Computing, held in conjunction with 13th ACM-SIGARCH International Conference on Supercomputing (ICS'99), 1999, pp. 11\u201315 [17] S. Gorlatch, C. Wedler, C. Lengauer, Optimization rules for programming with collective operations, in: Proceedings of the 13th International Parallel Processing Symposium & 10th Symposium on Parallel and Distributed Processing (IPPS/SPDP'99), 1999, pp. 492\u2013499 [18] J.-P. Goux, S. Kulkarni, J. Linderoth, M. Yoder, An enabling framework for master\u2013worker applications on the computatinal grid, in: Proceedings of the High Performance Distributed Computing (HPDC 2000), Pittsburgh, PA, August 2000, pp. 43\u201350 [19] A.S. Grimshaw, W.A. Wulf, and the Legion team, The legion vision of a worldwide virtual computer, Commun. ACM 40 (1) (1997) 39\u201345 [20] W. Gropp E. Lusk N. Doss A. Skjellum A high-performance, portable implementation of the MPI message passing interface standard Parallel Comput. 22 6 1996 789 828 [21] W.D. Gropp, E. Lusk, D. Swider, Improving the performance of MPI derived datatypes, in: Proceedings of MPIDC'99, Message Passing Interface Developer's and User's Conference, Atlanta, GA, March 1999, pp. 25\u201330 [22] P. Husbands, J.C. Hoe, MPI-StarT: delivering network performance to numerical applications, in: Proceedings of SC'98, November 1998; Online at http://www.supercomp.org/sc98/proceedings/ [23] G. Iannello, M. Lauria, S. Mercolino, Cross-platform analysis of fast messages for Myrinet, in: Proceedings of the Workshop on CANPC'98, Lecture Notes in Computer Science, vol. 1362, Las Vegas, Nevada, Springer, Berlin, January 1998, pp. 217\u2013231 [24] N.T. Karonis, B.R. de Supinski, I. Foster, W. Gropp, E. Lusk, J. Bresnahan, Exploiting hierarchy in parallel computer networks to optimize collective operation performance, in: Proceedings of the International Parallel and Distributed Processing Symposium (IPDPS 2000), Cancun, Mexico, IEEE, New York, May 2000, pp. 377\u2013384 [25] R.M. Karp, A. Sahay, E.E. Santos, K.E. Schauser, Optimal broadcast and summation in the Log P model, in: Proceedings of the Symposium on Parallel Algorithms and Architectures (SPAA), Velen, Germany, June 1993, pp. 142\u2013153 [26] R. Kesavan, D.K. Panda, Optimal multicast with packetization and network interface support, in: Proceedings of the International Conference on Parallel Processing, IEEE, New York, August 1997, pp. 370\u2013377 [27] T. Kielmann, H.E. Bal, S. Gorlatch, Bandwidth-efficient collective communication for clustered wide area systems, in: Proceedings of the International Parallel and Distributed Processing Symposium (IPDPS 2000), Cancun, Mexico, IEEE, New York, May 2000, pp. 492\u2013499 [28] T. Kielmann, H.E. Bal, K. Verstoep, Fast measurement of Log P parameters for message passing platforms, in: 4th Workshop on Runtime Systems for Parallel Programming (RTSPP), Lecture Notes in Computer Science, vol. 1800, Cancun, Mexico, Springer, Berlin, May 2000, pp. 1176\u20131183 [29] T. Kielmann, R.F.H. Hofman, H.E. Bal, A. Plaat, R.A.F. Bhoedjang, MPI's reduction operations in clustered wide area systems, in: Proceedings of MPIDC'99, Message Passing Interface Developer's and User's Conference, Atlanta, GA, March 1999, pp. 43\u201352 [30] T. Kielmann, R.F.H. Hofman, H.E. Bal, A. Plaat, R.A.F. Bhoedjang, MagPIe: MPI's collective communication operations for clustered wide area systems, in: Proceedings of the Symposium on Principles and Practice of Parallel Programming (PPoPP), Atlanta, GA, May 1999, pp. 131\u2013140 [31] B. Lowekamp, A. Beguelin, ECO: efficient collective operations for communication on heterogeneous networks, in: International Parallel Processing Symposium (IPPS), Honolulu, HI, 1996, pp. 399\u2013405 [32] E. Maillet C. Tron On efficiently implementing global time for performance evaluation on multiprocessor systems J. Parallel Distrib. Comput. 28 1995 84 93 [33] J.-Y.L. Park, H.-A. Choi, N. Nupairoj, L.M. Ni, Construction of optimal multicast trees based on the parameterized communication model, in: Proceedings of the International Conference on Parallel Processing (ICPP), vol. I, 1996, pp. 180\u2013187 [34] V. Paxson, On calibrating measurements of packet transit times, in: Proceedings of SIGMETRICS'98/PERFORMANCE'98, Madison, Wisconsin, June 1998, pp. 11\u201321 [35] E.E. Santos Optimal and near-optimal algorithms for k -item broadcast J. Parallel Distrib. Comput. 57 1999 121 139 [36] R. van de Geijn On global combine operations J. Parallel Distrib. Comput. 22 1994 324 328 [37] J. Watts R. Van de Geijn A pipelined broadcast for multidimensional meshes Parallel Process. Lett. 5 2 1995 281 292 [38] R. Wolski, Forecasting network performance to support dynamic scheduling using the network weather service, in: Proceedings of the High-Performance Distributed Computing (HPDC-6), Portland, OR, August 1997, pp. 316\u2013325; the network weather service is at http://nws.npaci.edu/\"",
        "title: \"Synthetic Coordinates for Disjoint Multipath Routing\" with abstract: \"We address the problem of routing packets on multiple, router-disjoint, paths in the Internet using large-scale overlay networks. Multipath routing can improve Internet QoS, by routing around congestions. This can benefit interactive and other real-time applications. One of the main problems with practically achieving router-disjoint multipath routing is the scalability limitation on the number of participating nodes in such an overlay network, caused by the large number of (expensive) topology probes required to discover relay nodes that provide high router-level path disjointness. To address this problem, we propose a novel, synthetic coordinates-based approach. We evaluate our method against alternative strategies for finding router-level disjoint alternative paths. Additionally, we empirically evaluate the distribution of path diversity in the Internet.\"",
        "1 is \"Bringing skeletons out of the closet: a pragmatic manifesto for skeletal parallel programming\", 2 is \"Performance of Firefly RPC\".",
        "\nGiven above information, for an author who has written the paper with the title \"A Component-Based Integrated Toolkit\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0176": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'On mixed and componentwise condition numbers for Moore-Penrose inverse and linear least squares problems':",
        "title: \"Safe Recursion Over an Arbitrary Structure: PAR, PH and DPH\" with abstract: \"Considering the Blum, Shub, and Smale computational model for real numbers, extended by Poizat to general structures, classical complexity can be considered as the restriction to finite structures of a more general notion of computability and complexity working over arbitrary structures.\"",
        "title: \"On sparseness and Turing reducibility over the reals\" with abstract: \"We prove some results about existence of NP-complete and NP-hard (for Turing reductions) sparse sets on different settings over the real numbers.\"",
        "title: \"Counting complexity classes for numeric computations II: algebraic and semialgebraic sets\" with abstract: \"We define counting classes #PR and #PC in the Blum-Shub-Smale setting of computations over the real or complex numbers, respectively. The problems of counting the number of solutions of systems of polynomial inequalities over R, or of systems of polynomial equalities over C, respectively, turn out to be natural complete problems in these classes. We investigate to what extent the new counting classes capture the complexity of computing basic topological invariants of semialgebraic sets (over R) and algebraic sets (over C). We prove that the problem to compute the (modified) Euler characteristic of semialgebraic sets is FPR#P RR-complete, and that the problem to compute the geometric degree of complex algebraic sets is FPR#PCC-complete. We also define new counting complexity classes GCR and GCC in the classical Turing model via taking Boolean parts of the classes above, and show that the problems to compute the Euler characteristic and the geometric degree of (semi)algebraic sets given by integer polynomials are complete in these classes. We complement the results in the Turing model by proving, for all k \u2208 N, the FPSPACE-hardness of the problem of computing the kth Betti number of the set of real zeros of a given integer polynomial. This holds with respect to the singular homology as well as for the Borel-Moore homology.\"",
        "title: \"Computing the homology of basic semialgebraic sets in weak exponential time.\" with abstract: \"We describe and analyze an algorithm for computing the homology (Betti numbers and torsion coefficients) of basic semialgebraic sets which works in weak exponential time. That is, out of a set of exponentially small measure in the space of data the cost of the algorithm is exponential in the size of the data. All algorithms previously proposed for this problem have a complexity which is doubly exponential (and this is so for almost all data).\"",
        "title: \"Adversarial smoothed analysis\" with abstract: \"The purpose of this note is to extend the results on uniform smoothed analysis of condition numbers from Burgisser et al. (2008) [1] to the case where the perturbation follows a radially symmetric probability distribution. In particular, we will show that the bounds derived in [1] still hold in the case of distributions whose density has a singularity at the center of the perturbation, which we call adversarial.\"",
        "1 is \"A Survey of Russian Approaches to Perebor (Brute-Force Searches) Algorithms\", 2 is \"Perturbation bound of singular linear systems\".",
        "\nGiven above information, for an author who has written the paper with the title \"On mixed and componentwise condition numbers for Moore-Penrose inverse and linear least squares problems\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0177": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Managing Control Asynchrony on SIMD Machines\u2014a Survey':",
        "title: \"Relaxing Causal Constraints in PDES\" with abstract: \"One of the major overheads that prohibits the wide spread deployment of parallel discrete event simulation (PDES) is the need to synchronize the distributed processes in the simulation. Considerable investigations have been conducted to analyze and optimize the two widely used synchronization strategies, namely the conservative and the optimistic simulation paradigms. However, little attention has been focussed on the definition and strictness of causality. Does causality need to be preserved in all types of simulations? Previously, we had suggested an answer to this question. We had argued that significant performance gains can be achieved by reconsidering this definition to decide if the parallel simulation really needs to subscribe to the preservation of causality. In this paper, we investigate this issue even more closely. An in depth analysis using several example simulation models is presented in this paper. In addition, a comparative analysis between unsynchronized and Time Warp simulation is presented.\"",
        "title: \"Predicting Performance Impacts due to Resolution Changes in Parallel Simulations\" with abstract: \"Multi-resolution models are often used to accelerate simulation-based analysis without significantly impacting the fidelity of the simulations. We have developed a web-enabled, component-based, multi-resolution modeling and Time Warp synchronized parallel simulation environment called WESE (Web-Enabled Simulation Environment). WESE uses a methodology called Dynamic Component Substitution (DCS) to enable abstractions or refinements to a given model. However, effectively utilizing abstractions, whether they are DCS-based or not, is a complex and time-consuming task. The complexity arises because not all abstractions improve simulation performance due to a myriad of factors related to model characteristics, synchronization protocol overheads and simulation-platform configuration. The overheads involved in identifying optimal model resolution have been exacerbating effective use of multi-resolution simulations, including our DCS-based approach. In an endeavor to minimize the time taken to identify performance impacts of resolution changes, this study proposes a DCS Performance Prediction Methodology (DCSPPM). It predicts simulation performance changes due to DCS transformations via static analysis of the model. Static analysis uses platform-specific performance characteristics of components constituting the model. DCSPPM yields quantitative estimates of performance impacts which are used by the modeler to select appropriate transformations. This article presents DCSPPM, its implementation in WESE and its empirical evaluation. The inferences drawn from the experiments prove that DCSPPM estimates have errors of less than 5% for a variety of models. Furthermore, DCSPPM executes orders of magnitude faster than corresponding shortest test simulations. Note that applicability of DCSPPM is not restricted to WESE but can be extended to other Time Warp synchronized simulators.\"",
        "title: \"Quantitative Driven Optimization of a Time Warp Kernel.\" with abstract: \"The set of events available for execution in a Parallel Discrete Event Simulation (PDES) are known as the pending event set. In a Time Warp synchronized simulation engine, these pending events are scheduled for execution in an aggressive manner that does not strictly enforce the causal relations between events. One of the key principles of Time Warp is that this relaxed causality will result in the processing of events in a manner that implicitly satisfies their causal order without paying the overhead costs of a strict enforcement of their causal order. On a shared memory platform the event scheduler generally attempts to schedule all available events in their Least TimeStamp First (LTSF) order to facilitate event processing in their causal order. By following an LTSF scheduling policy, a Time Warp scheduler can generally process events so that: (i) the critical path of the event timestamps is scheduled as early as possible, and (ii) causal violations occur infrequently. While this works effectively to minimize rollback (triggered by causal violations), as the number of parallel threads increases, the contention to the shared data structures holding the pending events can have significant negative impacts on overall event processing throughput. This work examines the application of profile data taken from Discrete-Event Simulation (DES) models to drive the simulation kernel optimization process. In particular, we take profile data about events in the schedule pool from three DES models to derive alternate scheduling possibilities in a Time Warp simulation kernel. Profile data from the studied DES models suggests that in many cases each Logical Process (LP) in a simulation will have multiple events that can be dequeued and executed as a set. In this work, we review the profile data and implement group event scheduling strategies based on this profile data. Experimental results show that event group scheduling can help alleviate contention and improve performance. However, the size of the event groups matters, small groupings can improve performance, larger groupings can trigger more frequent causal violations and actually slow the parallel simulation.\"",
        "title: \"A distributed method to bound rollback lengths for fossil collection in time warp simulators\" with abstract: \"The calculation of GVT has been a requirement to identify fossilized state and event space during Time Warp simulations. This paper outlines methods that use observations of past behavior to estimate future behavior for the purposes of fossil reclamation. More precisely, predictions of future rollback behavior are used to determine a probability that a particular item of saved state or event information is no longer needed. This probability is compared against a user-defined risk factor to decide if the space can be reclaimed and reused. This method is called optimistic fossil collection and it is fully distributed, not requiring the global estimate of GVT for operation.\"",
        "title: \"Causality representation and cancellation mechanism in time warp simulations\" with abstract: \"The Time Warp synchronization protocol allows causality errors and then recovers from them with the assistance of a cancellation mechanism. Cancellation can cause the rollback of several other simulation objects that may trigger a cascading rollback situation where the rollback cycles back to the original simulation object. These cycles of rollback can cause the simulation to enter a unstable (or thrashing) state where little real forward simulation progress is achieved. To address this problem, knowledge of causal relations between events can be used during cancellation to avoid cascading rollbacks and to initiate early recovery operations from causality errors. In this paper, we describe a logical time representation for Time Warp simulations that is used to disseminate causality information. The new timestamp representation, called Total Clocks, has two components: (i) a virtual time component, and (ii) a vector of event counters similar to Vector clocks. The virtual time component provides a one dimensional global simulation time, and the vector of event counters records event processing rates by the simulation objects. This time representation allows us to disseminate causality information during event execution that can be used to allow early recovery during cancellation. We propose a cancellation mechanism using Total Clocks that avoids cascading rollbacks in Time Warp simulations that have FIFO communication channels.\"",
        "1 is \"Efficient warp execution in presence of divergence with collaborative context collection\", 2 is \"A study of time warp rollback mechanisms\".",
        "\nGiven above information, for an author who has written the paper with the title \"Managing Control Asynchrony on SIMD Machines\u2014a Survey\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0178": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Design and implementation of a multi-standard event-driven energy management system for smart buildings':",
        "title: \"A Design Approach For Power-Optimized Fully Reconfigurable Delta Sigma A/D Converter For 4g Radios\" with abstract: \"This paper presents a novel design approach for fully reconfigurable low-voltage delta-sigma analog-digital converters for next-generation wireless applications. This approach guides us to find the power-optimal solution corresponding to the specifications of various wireless standards by exploring single-loop feedback and feedforward topologies with different filter order, number of quantizer bits, and oversampling ratios. Unlike previous multimode designs, this approach provides a better power efficiency. Based on this approach, a system-level design of a digitally programmable delta-sigma modulator for 4G radios is presented.\"",
        "title: \"Efficient DDD-based symbolic analysis of large linear analog circuits\" with abstract: \"A new technique for generating approaximate symbolic expressions for network functions in linear(ized) analog circuits is presented. It is based on the compact determinant decision diagram (DDD) representation of the circuit. An implementation of a term generation algorithm is given and its performance is compared to a matroid-based algorithm. Experimental results indicate that our approach is the fastest reported algorithm so far for this application.\"",
        "title: \"Automation in mixed-signal design: challenges and solutions in the wake of the nano era\" with abstract: \"The use of CMOS nanometer technologies at 65 nm and below will pose serious challenges on the design of mixed-signal integrated systems in the very near future. Rising design complexities, tightening time-to-market constraints, leakage power, increasing technology tolerances, and reducing supply voltages are key challenges that designers face. Novel types of devices, new process materials and new reliability issues are next on the horizon. We discuss new design methodologies and EDA tools that are being or need to be developed to address the problems of designing such mixed-signal integrated systems.\"",
        "title: \"A fast analog circuit yield estimation method for medium and high dimensional problems\" with abstract: \"Yield estimation for analog integrated circuits remains a time-consuming operation in variation-aware sizing. State-of-the-art statistical methods such as ranking-integrated Quasi-Monte-Carlo (QMC), suffer from performance degradation if the number of effective variables is large (as typically is the case for realistic analog circuits). To address this problem, a new method, called AYLeSS, is proposed to estimate the yield of analog circuits by introducing Latin Supercube Sampling (LSS) technique from the computational statistics field. Firstly, a partitioning method is proposed for analog circuits, whose purpose is to appropriately partition the process variation variables into low-dimensional sub-groups fitting for LSS sampling. Then, randomized QMC is used in each sub-group. In addition, the way to randomize the run order of samples in Latin Hypercube Sampling (LHS) is used for the QMC sub-groups. AYLeSS is tested on 4 designs of 2 example circuits in 0.35\u03bcm and 90nm technologies with yield from about 50% to 90%. Experimental results show that AYLeSS has approximately a 2 times speed enhancement compared with the best state-of-the-art method.\"",
        "title: \"Automatic generation of autonomous built-in observability structures for analog circuits\" with abstract: \"In this paper a new method is presented to automatically generate a Design-for-Testability infrastructure which increases the observability of defects in integrated circuits. An algorithm is proposed to detect circuit locations to which small detection blocks can be added. Those are coupled to an oscillator and the triggering of this oscillator in case of detected defects leaves traces in the power consumption. Therefore, the detection of a defective circuit can directly be transmitted to the Automated Test Equipment without requiring a special routing of the signals on the chip and extra test pins. Simulations on an industrial circuit show a 86 percent fault coverage of the hard-to-detect faults for an area increase of less than a percent.\"",
        "1 is \"Parameterized model order reduction of nonlinear dynamical systems\", 2 is \"Symbolic computation of logic implications for technology-dependent low-power synthesis\".",
        "\nGiven above information, for an author who has written the paper with the title \"Design and implementation of a multi-standard event-driven energy management system for smart buildings\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0179": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'A Traffic-Based Routing Algorithm By Using Mobile Agents':",
        "title: \"Assessing service protocol adaptability based on protocol reduction and graph search\" with abstract: \"Given the inherent autonomy, heterogeneity, and continuous evolution of Web services, mismatches usually exist between service protocols. Adapters are typically used to reconcile these mismatches. Before synthesizing an adapter, a service requestor is often willing to assess whether her expected interactions can be conducted or not. The effort of synthesizing an adapter is beneficial only if the result of this assessment is positive. Previous effort analyzing service interactions focused on either (i) compatibility analysis for checking whether interactions can be conducted in a direct manner or (ii) adapter synthesization for reconciling mismatches. In this paper we propose a new kind of adaptability assessment that (i) determines whether two service protocols of a requestor and a provider are adaptable, (ii) computes an adaptation degree, and (iii) identifies conditions that determine when these two service protocols can be adapted. This adaptability assessment provides complementary criteria to the service requestor for selecting a suitable service protocol from a set of functionally equivalent candidates according to her requirements. Copyright \u00a9 2010 John Wiley & Sons, Ltd.\"",
        "title: \"Exploring an epidemic in an e-science environment\" with abstract: \"Rules of bio-epidemic and e-epidemic inspire scientists to create a live, scalable interconnected environment for effectively managing situations in nature, society, and the digital virtual world.\"",
        "title: \"The schema theory for semantic link network\" with abstract: \"The Semantic Link Network (SLN) is a loosely coupled semantic data model for managing Web resources. Its nodes can be any type of resource. Its edges can be any semantic relation. Potential semantic links can be derived out according to reasoning rules on semantic relations. This paper proposes the schema theory for the SLN, including the concepts, rule-constraint normal forms, and relevant algorithms. The theory provides the basis for normalized management of semantic link network. A case study demonstrates the proposed theory.\"",
        "title: \"Semantics, Knowledge and Grids on Big Data.\" with abstract: \"Semantics, knowledge and Grids represent three spaces where people interact, understand, learn and create. Grids represent the advanced cyber-infrastructures and evolution. Big data influence the evolution of semantics, knowledge and Grids. Exploring semantics, knowledge and Grids on big data helps accelerate the shift of scientific paradigm, the fourth industrial revolution, and the transformational innovation of technologies.\"",
        "title: \"The open and autonomous interconnection semantics\" with abstract: \"Semantics is the meaning expressed by various languages and the study of meaning. Human society co-evolves with diverse semantic spaces holding commonsense, culture, and knowledge of sciences and technologies. By surveying the origin and development of general semantics, this paper studies the semantics of future interconnection environment, proposes a new notion of open and autonomous interconnection semantics, explores relevant principles and rules, and demonstrates a cultural application.\"",
        "1 is \"Towards an Iterative Reinforcement Approach for Simultaneous Document Summarization and Keyword Extraction\", 2 is \"Adaptive Multicast Topology Inference\".",
        "\nGiven above information, for an author who has written the paper with the title \"A Traffic-Based Routing Algorithm By Using Mobile Agents\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0180": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Markov model fuzzy-reasoning based algorithm for fast block motion estimation':",
        "title: \"Agglomerative-based flip-flop merging with signal wirelength optimization\" with abstract: \"In this paper, an optimization methodology using agglomerative-based clustering for number of flip-flop reduction and signal wirelength minimization is proposed. Comparing to previous works on flip-flop reduction, our method can obtain an optimal tradeoff curve between flip-flop number reduction and increase in signal wirelength. Our proposed methodology outperforms [1] and [12] in both reducing number of flip-flops and minimizing increase in signal wirelength. In comparison with [9], our methodology obtains a tradeoff of 15.8% reduction in flip-flop's signal wirelength with 16.9% additional flip-flops. Due to the nature of agglomerative clustering, when relocating flip-flops, our proposed method minimizes total displacement by an average of 5.9%, 8.0%, 181.4% in comparison with [12], [1] and [9] respectively.\"",
        "title: \"Clock planning for multi-voltage and multi-mode designs\" with abstract: \"Low power demand drives the development of lower power design architectures, among which multiple supply voltage is one of the state-of-the-art techniques to achieve low power. In addition, dynamic voltage frequency scaling and adaptive voltage scaling are popular power saving techniques during chip operation to provide different modes for various performance requirements. It is therefore very challenging to generate a clock tree for different operation modes. This paper proposes several implementations on this important issue, one of which can provide smallest clock latency and minimum clock skew on average of required operation modes in multi-voltage designs.\"",
        "title: \"Efficient analog layout prototyping by layout reuse with routing preservation\" with abstract: \"To strive for better circuit performance on analog design, layout generation heavily relies on experienced analog designers' effort. Other than general analog constraints such as symmetry and wire-matching are commonly embraced in many proposed works, analog circuit performance is also sensitive to routing behavior. This paper presents a CDT-based layout extraction to preserve routing behavior of the reference layout. Furthermore, a generalized layout prototyping methodology is proposed based on the layout extraction to achieve routing reuse. The proposed layout prototyping is applied to a variable-gain amplifier and a folded-cascode operational amplifier for both migration and prototypes generation. Experimental results show that our approach effectively reduces design cycle time and simultaneously produces reasonable performance.\n\n\"",
        "title: \"Package routability- and IR-drop-aware finger/pad assignment in chip-package co-design\" with abstract: \"Due to increasing complexity of design interactions between the chip, package and PCB, it is essential to consider them at the same time. Specifically the finger/pad locations affect the performance of the chip and the package significantly. In this paper, we have developed techniques in chip-package codesign to decide the locations of fingers/pads for package routability and signal integrity concerns in chip core design. Our finger/pad assignment is a two-step method: first we optimize the wire congestion problem in package routing, and then we try to minimize the IR-drop violation with finger/pad solution refinement. The experimental results are encouraging. Compared with the randomly optimized methods, our approaches reduce in average 42% and 68% of the maximum density in package and 10.61% of IR-drop for test circuits.\"",
        "title: \"Routability-driven bump assignment for chip-package co-design\" with abstract: \"In current chip and package designs, it is a bottleneck to simultaneously optimize both pin assignment and pin routing for different design domains (chip, package, and board). Usually the whole process costs a huge manual effort and multiple iterations thus reducing profit margin. Therefore, we propose a fast heuristic chip-package co-design algorithm in order to automatically obtain a bump assignment which introduces high routability both in RDL routing and substrate routing (100% in our real case). Experimental results show that the proposed method (inspired by board escape routing algorithms) automatically finishes bump assignment, RDL routing and substrate routing in a short time, while the traditional co-design flow requires weeks even months.\"",
        "1 is \"A block-based gradient descent search algorithm for block motion estimation in video coding\", 2 is \"An interference-cancellation scheme for carrier frequency offsets correction in OFDMA systems\".",
        "\nGiven above information, for an author who has written the paper with the title \"Markov model fuzzy-reasoning based algorithm for fast block motion estimation\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0181": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Moving Beyond e-Health and the Quantified Self: The Role of CSCW in Collaboration, Community and Practice for Technologically-Supported Proactive Health and Wellbeing':",
        "title: \"Bursting your (filter) bubble: strategies for promoting diverse exposure\" with abstract: \"Broadcast media are declining in their power to decide which issues and viewpoints will reach large audiences. But new information filters are appearing, in the guise of recommender systems, aggregators, search engines, feed ranking algorithms, and the sites we bookmark and the people and organizations we choose to follow on Twitter. Sometimes we explicitly choose our filters; some we hardly even notice. Critics worry that, collectively, these filters will isolate people in information bubbles only partly of their own choosing, and that the inaccurate beliefs they form as a result may be difficult to correct. But should we really be worried, and, if so, what can we do about it? Our panelists will review what scholars know about selectivity of exposure preferences and actual exposure and what we in the CSCW field can do to develop and test ways of promoting diverse exposure, openness to the diversity we actually encounter, and deliberative discussion.\"",
        "title: \"Motivating and enabling organizational memory with a workgroup wiki\" with abstract: \"Workgroups can struggle with remembering past projects and sharing this information with other groups in the organization. In a case study of the deployment of MediaWiki as a publishing tool for building organizational memory, group members' motivation to document past projects increased. A browsable collection of past projects allowed for discovery of past work, building the reputation of individuals and the workgroup, and development of transactive memory within the workgroup. The \"anyone can edit\" feature, frequently touted as the main benefit of wikis, had both benefits and drawbacks in this implementation. Group members did not feel comfortable making substantial edits to others' content but did occasionally use the wiki to coauthor content and also categorize and link to others' content and fix typos, particularly when asked to help.\"",
        "title: \"Maytag: a multi-staged approach to identifying complex events in textual data\" with abstract: \"We present a novel application of NLP and text mining to the analysis of financial documents. In particular, we describe an implemented prototype, Maytag, which combines information extraction and subject classification tools in an interactive exploratory framework. We present experimental results on their performance, as tailored to the financial domain, and some forward-looking extensions to the approach that enables users to specify classifications on the fly.\"",
        "title: \"When Personal Tracking Becomes Social: Examining the Use of Instagram for Healthy Eating.\" with abstract: \"Many people appropriate social media and online communities in their pursuit of personal health goals, such as healthy eating or increased physical activity. However, people struggle with impression management, and with reaching the right audiences when they share health information on these platforms. Instagram, a popular photo-based social media platform, has attracted many people who post and share their food photos. We aim to inform the design of tools to support healthy behaviors by understanding how people appropriate Instagram to track and share food data, the benefits they obtain from doing so, and the challenges they encounter. We interviewed 16 women who consistently record and share what they eat on Instagram. Participants tracked to support themselves and others in their pursuit of healthy eating goals. They sought social support for their own tracking and healthy behaviors and strove to provide that support for others. People adapted their personal tracking practices to better receive and give this support. Applying these results to the design of health tracking tools has the potential to help people better access social support.\"",
        "title: \"Social Cues and Interest in Reading Political News Stories.\" with abstract: \"People tend to prefer information sources that agree with their viewpoints, as predicted by the selective exposure theory, and to associate with people who are like them, a process known as homophily. Scholars raise fears that the combination of these factors can limit the diversity of viewpoints to which people are exposed, particularly when people find news through social network sites. In this study, we evaluate whether we can use annotations showing that a story was shared by people who are in some way similar to encourage people to read articles that may challenge their viewpoints. Most annotations (shared city, employer, music tastes, liked organizations, and friendship) had no discernable effect on reading interest compared to no annotation. Shared job type, though, led to decreased interest in reading an article. Although people consider themselves similar to others sharing news articles, this predominantly does not change their reading interest.\"",
        "1 is \"Finding \", 2 is \" Correlations in Multi-Faceted Personal Informatics Systems.\".",
        "\nGiven above information, for an author who has written the paper with the title \"Moving Beyond e-Health and the Quantified Self: The Role of CSCW in Collaboration, Community and Practice for Technologically-Supported Proactive Health and Wellbeing\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0182": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Automatically Exploring Hypotheses About Fault Prediction: A Comparative Study Of Inductive Logic Programming Methods':",
        "title: \"Learning the Classic Description Logic: Theoretical and Experimental Results\" with abstract: \" We present a series of theoretical and experimental results on the learnability of description logics. We first extend previous formal learnability results on simple description logics to C-Classic, a description logic expressive enough to be practically useful. We then experimentally evaluate two extensions of a learning algorithm suggested by the formal analysis. The first extension learns C-Classic descriptions from individuals. (The formal results assume that examples are themselves... \"",
        "title: \"Learning by teaching SimStudent: an initial classroom baseline study comparing with cognitive tutor\" with abstract: \"This paper describes an application of a machine-learning agent, SimStudent, as a teachable peer learner that allows a student to learn by teaching. SimStudent has been integrated into APLUS (Artificial Peer Learning environment Using SimStudent), an on-line game-like learning environment. The first classroom study was conducted in local public high schools to test the effectiveness of APLUS for learning linear algebra equations. In the study, learning by teaching (i.e., APLUS) was compared with learning by tutored-problem solving (i.e., Cognitive Tutor). The results show that the prior knowledge has a strong influence on tutor learning - for students with insufficient training on the target problems, learning by teaching may have limited benefits compared to learning by tutored problem solving. It was also found that students often use inappropriate problems to tutor SimStudent that did not effectively facilitate the tutor learning.\"",
        "title: \"Inductive Specification Recovery: Understanding Software by Learning from Example Behaviors\" with abstract: \"We describe a technique for extracting specifications from software using machine learning techniques. In our proposed technique, instrumented code is run on a number of representative test cases, generating examples of its behavior. Inductive learning techniques are then used to generalize these examples, forming a general description of some aspect of the system's behavior. A case study is presented in which this \u201cinductive specification recovery\u201d method is used to find Datalog specifications forC code that implements database views, in the context of a large real-world software system. It is demonstrated that off-the-shelf inductive logic programming methods can be successfully used for specification recovery in this domain, but that these methods can be substantially improved by adapting them more closely to the task at hand.\"",
        "title: \"Learning to match and cluster large high-dimensional data sets for data integration\" with abstract: \"Part of the process of data integration is determining which sets of identifiers refer to the same real-world entities. In integrating databases found on the Web or obtained by using information extraction methods, it is often possible to solve this problem by exploiting similarities in the textual names used for objects in different databases. In this paper we describe techniques for clustering and matching identifier names that are both scalable and adaptive, in the sense that they can be trained to obtain better performance in a particular domain. An experimental evaluation on a number of sample datasets shows that the adaptive method sometimes performs much better than either of two non-adaptive baseline systems, and is nearly always competitive with the best baseline system.\"",
        "title: \"Community-based classification of noun phrases in twitter\" with abstract: \"Many event monitoring systems rely on counting known keywords in streaming text data to detect sudden spikes in frequency. But the dynamic and conversational nature of Twitter makes it hard to select known keywords for monitoring. Here we consider a method of automatically finding noun phrases (NPs) as keywords for event monitoring in Twitter. Finding NPs has two aspects, identifying the boundaries for the subsequence of words which represent the NP, and classifying the NP to a specific broad category such as politics, sports, etc. To classify an NP, we define the feature vector for the NP using not just the words but also the author's behavior and social activities. Our results show that we can classify many NPs by using a sample of training data from a knowledge-base.\"",
        "1 is \"Reasoning with Memory Augmented Neural Networks for Language Comprehension.\", 2 is \"Scalable access control for distributed object systems\".",
        "\nGiven above information, for an author who has written the paper with the title \"Automatically Exploring Hypotheses About Fault Prediction: A Comparative Study Of Inductive Logic Programming Methods\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0183": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Algorithms for Inverse Reinforcement Learning':",
        "title: \"Selecting Receptive Fields in Deep Networks.\" with abstract: \"Recent deep learning and unsupervised feature learning systems that learn from unlabeled data have achieved high performance in benchmarks by using extremely large architectures with many features (hidden units) at each layer. Unfortunately, for such large architectures the number of parameters usually grows quadratically in the width of the network, thus necessitating hand-coded \"local receptive fields\" that limit the number of connections from lower level features to higher ones (e.g., based on spatial locality). In this paper we propose a fast method to choose these connections that may be incorporated into a wide variety of unsupervised training methods. Specifically, we choose local receptive fields that group together those low-level features that are most similar to each other according to a pairwise similarity metric. This approach allows us to harness the advantages of local receptive fields (such as improved scalability, and reduced data requirements) when we do not know how to specify such receptive fields by hand or where our unsupervised training algorithm has no obvious generalization to a topographic setting. We produce results showing how this method allows us to use even simple unsupervised training algorithms to train successful multi-layered etworks that achieve state-of-the-art results on CIFAR and STL datasets: 82.0% and 60.1% accuracy, respectively.\"",
        "title: \"Make3D: Learning 3D Scene Structure from a Single Still Image\" with abstract: \"We consider the problem of estimating detailed 3D structure from a single still image of an unstructured environment. Our goal is to create 3D models that are both quantitatively accurate as well as visually pleasing. For each small homogeneous patch in the image, we use a Markov random field (MRF) to infer a set of \"plane parametersrdquo that capture both the 3D location and 3D orientation of the patch. The MRF, trained via supervised learning, models both image depth cues as well as the relationships between different parts of the image. Other than assuming that the environment is made up of a number of small planes, our model makes no explicit assumptions about the structure of the scene; this enables the algorithm to capture much more detailed 3D structure than does prior art and also give a much richer experience in the 3D flythroughs created using image-based rendering, even for scenes with significant nonvertical structure. Using this approach, we have created qualitatively correct 3D models for 64.9 percent of 588 images downloaded from the Internet. We have also extended our model to produce large-scale 3D models from a few images.\"",
        "title: \"Distance Metric Learning with Application to Clustering with Side-Information\" with abstract: \"Many algorithms rely critically on being given a good metric over their inputs. For instance, data can often be clustered in many \"plausible\" ways, and if a clustering algorithm such as K-means initially fails to find one that is meaningful to a user, the only recourse may be for the user to manually tweak the metric until sufficiently good clusters are found. For these and other applications requiring good metrics, it is desirable that we provide a more systematic way for users to indicate what they con- sider \"similar.\" For instance, we may ask them to provide examples. In this paper, we present an algorithm that, given examples of similar (and, if desired, dissimilar) pairs of points in , learns a distance metric over that respects these relationships. Our method is based on posing met- ric learning as a convex optimization problem, which allows us to give efficient, local-optima-free algorithms. We also demonstrate empirically that the learned metrics can be used to significantly improve clustering performance.\"",
        "title: \"Unsupervised learning of hierarchical representations with convolutional deep belief networks\" with abstract: \"There has been much interest in unsupervised learning of hierarchical generative models such as deep belief networks (DBNs); however, scaling such models to full-sized, high-dimensional images remains a difficult problem. To address this problem, we present the convolutional deep belief network, a hierarchical generative model that scales to realistic image sizes. This model is translation-invariant and supports efficient bottom-up and top-down probabilistic inference. Key to our approach is probabilistic max-pooling, a novel technique that shrinks the representations of higher layers in a probabilistically sound way. Our experiments show that the algorithm learns useful high-level visual features, such as object parts, from unlabeled images of objects and natural scenes. We demonstrate excellent performance on several visual recognition tasks and show that our model can perform hierarchical (bottom-up and top-down) inference over full-sized images.\"",
        "title: \"Autonomous operation of novel elevators for robot navigation\" with abstract: \"Although robot navigation in indoor environments has achieved great success, robots are unable to fully navigate these spaces without the ability to operate elevators, including those which the robot has not seen before. In this paper, we focus on the key challenge of autonomous interaction with an unknown elevator button panel. A number of factors, such as lack of useful 3D features, variety of elevator panel designs, variation in lighting conditions, and small size of elevator buttons, render this goal quite difficult. To address the task of detecting, localizing, and labeling the buttons, we use state-of-the-art vision algorithms along with machine learning techniques to take advantage of contextual features. To verify our approach, we collected a dataset of 150 pictures of elevator panels from more than 60 distinct elevators, and performed extensive offline testing. On this very diverse dataset, our algorithm succeeded in correctly localizing and labeling 86.2% of the buttons. Using a mobile robot platform, we then validate our algorithms in experiments where, using only its on-board sensors, the robot autonomously interprets the panel and presses the appropriate button in elevators never seen before by the robot. In a total of 14 trials performed on 3 different elevators, our robot succeeded in localizing the requested button in all 14 trials and in pressing it correctly in 13 of the 14 trials.\"",
        "1 is \"Solving SAT and SAT Modulo Theories: From an abstract Davis--Putnam--Logemann--Loveland procedure to DPLL(T)\", 2 is \"Interactive Teaching Of A Mobile Robot\".",
        "\nGiven above information, for an author who has written the paper with the title \"Algorithms for Inverse Reinforcement Learning\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0184": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Finite nondeterministic automata: simulation and minimality':",
        "title: \"Algebraic Constraints, Automata, and Regular Languages\" with abstract: \"A class of decision problems is Boolean if it is closed under the set-theoretic operations of union, intersection and complementation. The paper introduces new Boolean classes of decision problems based on algebraic constraints imposed on transitions of finite automata. We discuss issues related to specifications of these classes from algebraic, computational and proof-theoretic points of view.\"",
        "title: \"Update Networks and Their Routing Strategies\" with abstract: \"We introduce the notion of update networks to model communication networks with infinite duration. In our formalization we use bipartite finite graphs and game-theoretic terminology as an underlying structure. For these networks we exhibit a simple routing procedure to update information throughout the nodes of the network. We also introduce an hierarchy for the class of all update networks and discuss the complexity of some natural problems.\"",
        "title: \"Games with Unknown Past\" with abstract: \"We define a new type of two player game occurring on a tree. The tree may have no root and may have arbitrary degrees of nodes. These games extend the class of games considered by Gurevich-Harrington in [5]. We prove that in the game one of the players has a winning strategy which depends on finite bounded information about the past part of a play and on future of each play that is isomorphism types of tree nodes. This result extends further the Gurevich-Harrington determinacy theorem from [5].\"",
        "title: \"Decision Problems For Finite Automata Over Infinite Algebraic Structures\" with abstract: \"We introduce the concept of finite automata over algebraic structures. We address the classical emptiness problem and its various refinements in our setting. In particular, we prove several decidability and undecidability results. We also explain the way our automata model connects with the existential first order theory of algebraic structures.\"",
        "title: \"Relaxed update and partition network games\" with abstract: \"In this paper, we study the complexity of deciding which player has a winning strategy in certain types of McNaughton games. These graph games can be used as models for computational problems and processes of infinite duration. We consider the cases (1) where the first player wins when vertices in a specified set are visited infinitely often and vertices in another specified set are visited finitely often, (2) where the first player wins when exactly those vertices in one of a number of specified disjoint sets are visited infinitely often, and (3) a generalization of these first two cases. We give polynomial time algorithms to determine which player has a winning strategy in each of the games considered.\"",
        "1 is \"The effects of transparency on trust in and acceptance of a content-based art recommender\", 2 is \"Depth-first search and linear grajh algorithms\".",
        "\nGiven above information, for an author who has written the paper with the title \"Finite nondeterministic automata: simulation and minimality\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0185": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'ROARS: a robust object archival system for data intensive scientific computing':",
        "title: \"ROARS: a robust object archival system for data intensive scientific computing\" with abstract: \"As scientific research becomes more data intensive, there is an increasing need for scalable, reliable, and high performance storage systems. Such data repositories must provide both data archival services and rich metadata, and cleanly integrate with large scale computing resources. ROARS is a hybrid approach to distributed storage that provides both large, robust, scalable storage and efficient rich metadata queries for scientific applications. In this paper, we present the design and implementation of ROARS, focusing primarily on the challenge of maintaining data integrity across long time scales. We evaluate the performance of ROARS on a storage cluster, comparing to the Hadoop distributed file system and a centralized file server. We observe that ROARS has read and write performance that scales with the number of storage nodes, and integrity checking that scales with the size of the largest node. We demonstrate the ability of ROARS to function correctly through multiple system failures and reconfigurations. ROARS has been in production use for over three years as the primary data repository for a biometrics research lab at the University of Notre Dame.\"",
        "title: \"Recognition of free-form objects in dense range data using local features\" with abstract: \"This article describes a system for recognizing free-form 3D objects in dense range data employing local features and object-centered geometric models. Local features are extracted from range images and object models using curvature analysis, and variability in feature size is accommodated by decomposition of features into sub-features. Shape indices and other attributes provide a basis for correspondence between compatible image and model features and subfeatures, as well as pruning of invalid correcpondences. A verification step provides a final ranking of object identity and pose hypotheses. The evaluation system contained 10 free-form objects and was tested using 10 range images with two objects from the database in each image. Comments address strengths of the proposed technique as well as areas for future improvement.\"",
        "title: \"Personal Identification Utilizing Finger Surface Features\" with abstract: \"In this paper we present a novel approach for personal identification which utilizes finger surface features as a biometric identifier. Using dense range data images of the hand, we calculate the curvature-based surface representation, shape index, for the index, middle, and ring fingers. This representation is used for comparisons to determine subject similarity. Our experiments involve the use of a large data set of range images collected over time. We examine the performance of individual finger surfaces as a biometric identifier as well as the performance when using the three finger surfaces in conjunction. The results of our experiments are presented, which indicate that this approach performs well for a first-of-its-kind biometric technique.\"",
        "title: \"Degradation of iris recognition performance due to non-cosmetic prescription contact lenses\" with abstract: \"Many iris recognition systems operate under the assumption that non-cosmetic contact lenses have no or minimal effect on iris biometrics performance and convenience. In this paper we show results of a study of 12,003 images from 87 contact-lens-wearing subjects and 9697 images from 124 non-contact-lens-wearing subjects. We visually classified the contact lens images into four categories according to the type of lens effects observed in the image. Our results show different degradations in performance for different types of contact lenses. Lenses that produce larger artifacts on the iris yield more degraded performance. This is the first study to document degraded iris biometrics performance with non-cosmetic contact lenses.\"",
        "title: \"Textured mesh generation of extracted regions from urban range-scanned LIDAR data\" with abstract: \"LIDAR range scanners are a popular tool for data acquisition in the field of urban modeling, archaeological preservation, city planning, and more. However, range scanners output a series of discrete distance samples as disconnected points, providing a fundamentally incomplete representation of the underlying structure in a scene. Data from a single LIDAR scan of a region can be trivially triangulated, but when multiple scanners are involved, or when the acquisition platform is mobile, those inter-point relationships are lost. We propose a technique to triangulate such data by identifying logical surfaces within the data and triangulating those surfaces individually. The resulting triangulations are simplified dramatically by using information about the shape of the regions, and texture is applied from camera imagery on the scan vehicle. The result is a high fidelity representation of a scene which is more efficiently rendered on modern hardware than point sets, which occupies less space in memory, and which brings us closer to a solid representation of the true scanned scene.\"",
        "1 is \"Hardness-Aware Truth Discovery in Social Sensing Applications\", 2 is \"Point Signatures: A New Representation for 3D Object Recognition\".",
        "\nGiven above information, for an author who has written the paper with the title \"ROARS: a robust object archival system for data intensive scientific computing\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0186": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Designing Fast and Scalable XACML Policy Evaluation Engines':",
        "title: \"Fast and Accurate Tracking of Population Dynamics in RFID Systems\" with abstract: \"RFID systems have been widely deployed for various applications such as supply chain management, indoor localization, inventory control, and access control. This paper deals with the fundamental problem of estimating the number of arriving and departing tags between any two time instants in dynamically changing RFID tag populations, which is needed in many applications such as warehouse monitoring and privacy sensitive RFID systems. In this paper, we propose a dynamic tag estimation scheme, namely DTE, that can achieve arbitrarily high required reliability, is compliant with the C1G2 standard, and works in single as well as multiple-reader environment. DTE uses the standardized frame slotted Aloha protocol and utilizes the number of slots that change their values in corresponding Aloha frames at the two time instants to estimate the number of arriving and departing tags. It is easy to deploy because it neither requires modification to tags nor to the communication protocol between tags and readers. We have extensively evaluated and compared DTE with the only prior scheme, ZDE, that can estimate the number of arriving and departing tags. Unfortunately, ZDE can not achieve arbitrarily high required reliability. In contrast, our proposed scheme always achieves the required reliability. For example, for a tag population containing 10 4 tags, a required reliability of 95%, and a required confidence interval of 5%, DTE takes 5.12 seconds to achieve the required reliability whereas ZDE achieves a reliability of only 66% in the same amount of time.\"",
        "title: \"A Distributed Algorithm for Identifying Information Hubs in Social Networks\" with abstract: \"This paper addresses the problem of identifying the top-k information hubs in a social network. Identifying top-k information hubs is crucial for many applications such as advertising in social networks where advertisers are interested in identifying hubs to whom free samples can be given. Existing solutions are centralized and require time stamped information about pair-wise user interactions and can only be used by social network owners as only they have access to such data. Existing distributed algorithms suffer from poor accuracy. In this paper, we propose a new algorithm to identify information hubs that preserves user privacy. Our method can identify hubs without requiring a central entity to access the complete friendship graph. We achieve this by fully distributing the computation using the Kempe-McSherry algorithm, while addressing user privacy concerns. We evaluate the effectiveness of our proposed technique using three real-world data set; The first two are Facebook data sets containing about 6 million users and more than 40 million friendship links. The third data set is from Twitter and comprises of a little over 2 million users. The results of our analysis show that our algorithm is up to 50% more accurate than existing algorithms. Results also show that the proposed algorithm can estimate the rank of the top-k information hubs users more accurately than existing approaches.\"",
        "title: \"Dynamic camouflage event based malicious node detection architecture\" with abstract: \"Compromised sensor nodes may collude to segregate a specific region of the sensor network preventing event reporting packets in this region from reaching the basestation. Additionally, they can cause skepticism over all data collected. Identifying and segregating such compromised nodes while identifying the type of attack with a certain confidence level is critical to the smooth functioning of a sensor network. Existing work specializes in preventing or identifying a specific type of attack and lacks a unified architecture to identify multiple attack types. Dynamic Camouflage Event-Based Malicious Node Detection Architecture (D-CENDA) is a proactive architecture that uses camouflage events generated by mobile-nodes to detect malicious nodes while identifying the type of attack. We exploit the spatial and temporal information of camouflage event while analyzing the packets to identify malicious activity. We have simulated D-CENDA to compare its performance with other techniques that provide protection against individual attack types and the results show marked improvement in malicious node detection while having significantly less false positive rate. Moreover, D-CENDA can identify the type of attack and is flexible to be configured to include other attack types in future.\"",
        "title: \"Fast range query processing with strong privacy protection for cloud computing\" with abstract: \"Privacy has been the key road block to cloud computing as clouds may not be fully trusted. This paper concerns the problem of privacy preserving range query processing on clouds. Prior schemes are weak in privacy protection as they cannot achieve index indistinguishability, and therefore allow the cloud to statistically estimate the values of data and queries using domain knowledge and history query results. In this paper, we propose the first range query processing scheme that achieves index indistinguishability under the indistinguishability against chosen keyword attack (IND-CKA). Our key idea is to organize indexing elements in a complete binary tree called PBtree, which satisfies structure indistinguishability (i.e., two sets of data items have the same PBtree structure if and only if the two sets have the same number of data items) and node indistinguishability (i.e., the values of PBtree nodes are completely random and have no statistical meaning). We prove that our scheme is secure under the widely adopted IND-CKA security model. We propose two algorithms, namely PBtree traversal width minimization and PBtree traversal depth minimization, to improve query processing efficiency. We prove that the worse case complexity of our query processing algorithm using PBtree is O(|R| log n), where n is the total number of data items and R is the set of data items in the query result. We implemented and evaluated our scheme on a real world data set with 5 million items. For example, for a query whose results contain ten data items, it takes only 0.17 milliseconds.\"",
        "title: \"Device-Free Human Activity Recognition Using Commercial WiFi Devices.\" with abstract: \"Since human bodies are good reflectors of wireless signals, human activities can be recognized by monitoring changes in WiFi signals. However, existing WiFi-based human activity recognition systems do not build models that can quantify the correlation between WiFi signal dynamics and human activities. In this paper, we propose a Channel State Information (CSI)-based human Activity Recognition and Monitoring system (CARM). CARM is based on two theoretical models. First, we propose a CSI-speed model that quantifies the relation between CSI dynamics and human movement speeds. Second, we propose a CSI-activity model that quantifies the relation between human movement speeds and human activities. Based on these two models, we implemented the CARM on commercial WiFi devices. Our experimental results show that the CARM achieves recognition accuracy of 96% and is robust to environmental changes.\"",
        "1 is \"MuJava: an automated class mutation system\", 2 is \"An improved algorithm to accelerate regular expression evaluation\".",
        "\nGiven above information, for an author who has written the paper with the title \"Designing Fast and Scalable XACML Policy Evaluation Engines\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0187": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Influence Of Selective Pressure On Quality Of Solutions And Speed Of Evolutionary Mastermind':",
        "title: \"Evolution of artificial terrains for video games based on accessibility\" with abstract: \"Diverse methods have been developed to generate terrains under constraints to control terrain features, but most of them use strict restrictions. However, there are situations were more flexible restrictions are sufficient, such as ensuring that terrains have enough accessible area, which is an important trait for video games. The Genetic Terrain Program technique, based on genetic programming, was used to automatically evolve Terrain Programs (TPs - which are able to generate terrains procedurally) for the desired accessibility parameters. Results showed that the accessibility parameters have negligible influence on the evolutionary system and that the terminal set has a major role on the terrain look. TPs produced this way are already being used on Chapas video game.\"",
        "title: \"Embedding Branch and Bound within Evolutionary Algorithms\" with abstract: \"A framework for hybridizing evolutionary algorithms with the branch-and-bound algorithm (B&B) is presented in this paper. This framework is based on using B&B as an operator embedded in the evolutionary algorithm. The resulting hybrid operator will intelligently explore the dynastic potential (possible children) of the solutions being recombined, providing the best combination of formae (generalized schemata) that can be constructed without introducing implicit mutation. As a basis for studying this operator, the general functioning of transmitting recombination is considered. Two important concepts are introduced, compatibility sets, and granularity of the representation. These concepts are studied in the context of different kinds of representation: orthogonal, non-orthogonal separable, and non-separable.The results of an extensive experimental evaluation are reported. It is shown that this model can be useful when problem knowledge is available in the form of an optimistic evaluation function. Scalability issues are also considered. A control mechanism is proposed to alleviate the increasing computational cost of the algorithm for highly multidimensional problems.\"",
        "title: \"Design of emergent and adaptive virtual players in a war RTS game\" with abstract: \"Basically, in (one-player) war Real Time Strategy (wRTS) games a human player controls, in real time, an army consisting of a number of soldiers and her aim is to destroy the opponent's assets where the opponent is a virtual (i.e., non-human player controlled) player that usually consists of a pre-programmed decision-making script. These scripts have usually associated some well-known problems (e.g., predictability, non-rationality, repetitive behaviors, and sensation of artificial stupidity among others). This paper describes a method for the automatic generation of virtual players that adapt to the player skills; this is done by building initially a model of the player behavior in real time during the game, and further evolving the virtual player via this model in-between two games. The paper also shows preliminary results obtained on a oneplayer wRTS game constructed specifically for experimentation.\"",
        "title: \"A memetic cooperative optimization schema and its application to the tool switching problem\" with abstract: \"This paper describes a generic (meta-)cooperative optimization schema in which several agents endowed with an optimization technique (whose nature is not initially restricted) cooperate to solve an optimization problem. These agents can use a wide set of optimization techniques, including local search, population-based methods, and hybrids thereof, hence featuring multilevel hybridization. This optimization approach is here deployed on the Tool Switching Problem (ToSP), a hard combinatorial optimization problem in the area of flexible manufacturing. We have conducted an ample experimental analysis involving a comparison of a wide number of algorithms or a large number of instances. This analysis indicates that some meta-cooperative instances perform significantly better than the rest of the algorithms, including a memetic algorithm that was the previous incumbent for this problem.\"",
        "title: \"An Analysis of a Selecto-Lamarckian Model of Multimemetic Algorithms with Dynamic Self-organized Topology.\" with abstract: \"Multimemetic algorithms (MMAs) are memetic algorithms that explicitly represent and evolve memes (computational representations of problem solving methods) as a part of solutions. We use an idealized selecto-Lamarckian model of MMAs in order to analyze the propagation of memes in spatially structured populations. To this end, we focus on the use of dynamic self-organized spatial structures, based on the stimergic communication among solutions, and compare these with regular static lattices and unstructured (panmictic) populations. An empirical analysis indicates that these dynamic lattices are capable of promoting memetic diversity and provide better results in terms of survival of high-quality memes.\"",
        "1 is \"Automating XML document structure transformations\", 2 is \"Edge sets: an effective evolutionary coding of spanning trees\".",
        "\nGiven above information, for an author who has written the paper with the title \"Influence Of Selective Pressure On Quality Of Solutions And Speed Of Evolutionary Mastermind\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0188": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'A New Class of Sequences With Zero or Low Correlation Zone Based on Interleaving Technique':",
        "title: \"HBC entity authentication for low-cost pervasive devices\" with abstract: \"The HB-like entity authentication protocols for low-cost pervasive devices have attracted a great deal of attention because of their simplicity, computational efficiency and solid security foundation on a well-studied hard problem-learning parity with noise. By far, the most efficient protocol is HB#, which is provably resistant to the GRS attack under the conjecture that it is secure in the DET-model. However, in order to achieve 80-bit security, a typical HB# authentication key comprises over 1000 bits, which imposes considerable storage burdens on resource-constrained devices. In this study, the authors propose a new HB-like protocol: HBC. The protocol makes use of a special type of circulant matrix, in contrast to the Toeplitz matrix in HB#, to significantly reduce storage consumption and overcome a subtle security proof inefficacy in HB#. In addition, the authors introduce a masking technique that substantially increases noise level from an adversary's standpoint, and thus improves protocol performance. The authors demonstrate that 613-bit authentication key suffices for 80-bit security in the HBC protocol, which is quite competitive and more appealing for low-cost devices.\"",
        "title: \"A new algorithm to compute remote terms in special types of characteristic sequences\" with abstract: \"This paper proposes a new algorithm, called the Diagonal Double-Add (DDA) algorithm, to compute the k-th term of special kinds of characteristic sequences. We show that this algorithm is faster than Fiduccia's algorithm, the current standard for computation of general sequences, for fourth- and fifth-order sequences.\"",
        "title: \"Generating Large Instances of the Gong-Harn Cryptosystem\" with abstract: \"In 1999, Gong and Harn proposed a new cryptosystem based on third-order characteristic sequences over finite fields. This paper gives an efficient method to generate instances of this cryptosystem over large finite fields. The method first finds a \"good\" prime p to work with and then constructs the sequence to ensure that it has the desired period. This method has been implemented in C++ using NTL [7] and so timing results are presented.\"",
        "title: \"A 32-bit RC4-like Keystream Generator\" with abstract: \"Abstract: In this paper we propose a new 32-bit RC4 like keystreamgenerator. The proposed generator produces 32 bits in each iteration andcan be implemented in software with reasonable memory requirements.\"",
        "title: \"Constructions Of Multiple Shift-Distinct Signal Sets With Low Correlation\" with abstract: \"In this paper, we introduce a concept of correlation of multiple shift-distinct signal sets, and make a connection between constructions of multiple binary signal sets with low maximum correlation and constructions of a binary signal set with larger size and low maximum correlation. We then present one construction for multiple shift-distinct binary signal sets with low maximum correlation using Kasami (small) signal sets (or generalized Kasami signals sets). We show that the constructed m Kasami signal sets, in which each sequence has period N = 2(2m) - 1, satisfies the tth-order shift-distinct property, which is a new concept introduced in this paper. As a by-product, this construction also yields some new pairs of m-sequences with different periods and three-valued crosscorrelation.\"",
        "1 is \"A 440-nA True Random Number Generator for Passive RFID Tags\", 2 is \"A class of three-weight cyclic codes.\".",
        "\nGiven above information, for an author who has written the paper with the title \"A New Class of Sequences With Zero or Low Correlation Zone Based on Interleaving Technique\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0189": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'A Hierarchical Signature Scheme for Robust Video Authentication using Secret Sharing':",
        "title: \"Coopetitive visual surveillance using model predictive control\" with abstract: \"Active cooperative sensing with multiple sensors is being actively researched in visual surveillance. However, active cooperative sensing often suffers from the delay in information exchange among the sensors and also from sensor reaction delays. This is because simplistic control strategies like Proportional Integral Differential (PID), that do not employ the look-ahead strategy, often fail to counterbalance these delays at real time. Hence, there is a need for more sophisticated interaction and control mechanisms that can overcome the delay problems. In this paper, we propose a coopetitive framework using Model Predictive Control (MPC) which allows the sensors to not only 'compete' as well as 'cooperate' with each other to perform the designated task in the best possible manner but also to dynamically swap their roles and sub-goals rather than just the parameters. MPC is used as a feedback control mechanism to allow sensors to react not only based on past observations but also on possible future events. We demonstrate the utility of our framework in a dual camera surveillance setup with the goal of capturing the high resolution images of intruders in the surveyed rectangular area e.g. an ATM lobby or a museum. The results are promising and clearly establish the efficacy of coopetition as an effective form of interaction between sensors and MPC as a superior feedback mechanism than the PID.\"",
        "title: \"Privacy modeling for video data publication\" with abstract: \"Video cameras are being extensively used in many applications. Huge amounts of video are being recorded and stored everyday by surveillance systems. Any proposed application of this data raises severe privacy concerns. An assessment of privacy loss is necessary before any potential application of the data. In traditional methods of privacy modeling, researchers have focused on explicit means of identity leakage like facial information, etc. However, other implicit inference channels through which individual's an identity can be learned have not been considered. For example, an adversary can observe the behavior, look at the places visited and combine that with the temporal information to infer the identity of the person in the video. In this work, we thoroughly investigate privacy issues involved with the video data considering both implicit and explicit channels. We first establish an analogy with the statistical databases and then propose a model to calculate the privacy loss that might occur due to publication of the video data. The experimental results demonstrate the utility of the proposed model.\"",
        "title: \"Toward a Remote-Controlled Weapon-Equipped Camera Surveillance System\" with abstract: \"Camera surveillance systems have proved useful for public safety. The main disadvantage is that since the camera views are monitored in a remote control room, it is often difficult for security officers to reach the crime-scene in time. During this time, the assailant(s) have likely caused sufficient damage and threaten many lives. To overcome this problem, this paper proposes to take a standard surveillance system, augment it with simple weaponry for the purpose of disabling potential assailants, and use mathematical models to develop decision criteria for selecting the safest and most effective weapon in a given situation. The feasibility of the proposed system is examined using simulation results which also validate the utility of the proposed decision models.\"",
        "title: \"Secret sharing approach for securing cloud-based pre-classification volume ray-casting\" with abstract: \"With the evolution in cloud computing, cloud-based volume rendering, which outsources data rendering tasks to cloud datacenters, is attracting interest. Although this new rendering technique has many advantages, allowing third-party access to potentially sensitive volume data raises security and privacy concerns. In this paper, we address these concerns for cloud-based pre-classification volume ray-casting by using Shamir's (k, n) secret sharing and its variant (l, k, n) ramp secret sharing, which are homomorphic to addition and scalar multiplication operations, to hide color information of volume data/images in datacenters. To address the incompatibility issue of the modular prime operation used in secret sharing technique with the floating point operations of ray-casting, we consider excluding modular prime operation from secret sharing or converting the floating number operations of ray-casting to fixed point operations --- the earlier technique degrades security and the later degrades image quality. Both these techniques, however, result in significant data overhead. To lessen the overhead at the cost of high security, we propose a modified ramp secret sharing scheme that uses the three color components in one secret sharing polynomial and replaces the shares in floating point with smaller integers.\"",
        "title: \"Determining trust in media-rich websites using semantic similarity\" with abstract: \"Significant growth of multimedia content on the World Wide Web (or simply `Web') has made it an essential part of peoples lives. The web provides enormous amount of information, however, it is very important for the users to be able to gauge the trustworthiness of web information. Users normally access content from the first few links provided to them by search engines such as Google or Yahoo!. This is assuming that these search engines provide factual information, which may be popular due to criteria such as page rank but may not always be trustworthy from the factual aspects. This paper presents a mechanism to determine trust of websites based on the semantic similarity of their multimedia content with already established and trusted websites. The proposed method allows for dynamic computation of the trust level of websites of different domains and hence overcomes the dependency on traditional user feedback methods for determining trust. In fact, our method attempts to emulate the evolving process of trust that takes place in a user's mind. The experimental results have been provided to demonstrate the utility and practicality of the proposed method.\"",
        "1 is \"Recursive Least Squares Dictionary Learning Algorithm\", 2 is \"A design methodology for selection and placement of sensors in multimedia surveillance systems\".",
        "\nGiven above information, for an author who has written the paper with the title \"A Hierarchical Signature Scheme for Robust Video Authentication using Secret Sharing\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0190": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Computational highlight holography':",
        "title: \"Spectral sampling of manifolds\" with abstract: \"A central problem in computer graphics is finding optimal sampling conditions for a given surface representation. We propose a new method to solve this problem based on spectral analysis of manifolds which results in faithful reconstructions and high quality isotropic samplings, is efficient, out-of-core, feature sensitive, intuitive to control and simple to implement. We approach the problem in a novel way by utilizing results from spectral analysis, kernel methods, and matrix perturbation theory. Change in a manifold due to a single point is quantified by a local measure that limits the change in the Laplace-Beltrami spectrum of the manifold. Hence, we do not need to explicitly compute the spectrum or any global quantity, which makes our algorithms very efficient. Although our main focus is on sampling surfaces, the analysis and algorithms are general and can be applied for simplifying and resampling point clouds lying near a manifold of arbitrary dimension.\"",
        "title: \"Analysis and synthesis of point distributions based on pair correlation\" with abstract: \"Analyzing and synthesizing point distributions are of central importance for a wide range of problems in computer graphics. Existing synthesis algorithms can only generate white or blue-noise distributions with characteristics dictated by the underlying processes used, and analysis tools have not been focused on exploring relations among distributions. We propose a unified analysis and general synthesis algorithms for point distributions. We employ the pair correlation function as the basis of our methods and design synthesis algorithms that can generate distributions with given target characteristics, possibly extracted from an example point set, and introduce a unified characterization of distributions by mapping them to a space implied by pair correlations. The algorithms accept example and output point sets of different sizes and dimensions, are applicable to multi-class distributions and non-Euclidean domains, simple to implement and run in O(n) time. We illustrate applications of our method to real world distributions.\"",
        "title: \"Computational highlight holography\" with abstract: \"Computational highlight holography converts three-dimensional computer models into mechanical \"holograms\" fabricated on (specular) reflective or refractive materials. The surface consists of small grooves with patches of paraboloids or hyperboloids, each of which produces a highlight when illuminated by a directional light. Each highlight appears in different places for different view directions, with the correct binocular and motion parallax corresponding to a virtual 3D point position. Our computational pipeline begins with a 3D model and desired view position, samples the model to generate points that depict its features accurately, and computes a maximal set of non-overlapping patches to be embedded in the surface. We provide a preview of the hologram for the user, then fabricate the surface using a computer-controlled engraving machine. We show a variety of different fabricated holograms: reflective, transmissive, and holograms with color and proper shading. We also present extensions to stationary and animated 2D stippled images.\"",
        "title: \"Deforming meshes that split and merge\" with abstract: \"We present a method for accurately tracking the moving surface of deformable materials in a manner that gracefully handles topological changes. We employ a Lagrangian surface tracking method, and we use a triangle mesh for our surface representation so that fine features can be retained. We make topological changes to the mesh by first identifying merging or splitting events at a particular grid resolution, and then locally creating new pieces of the mesh in the affected cells using a standard isosurface creation method. We stitch the new, topologically simplified portion of the mesh to the rest of the mesh at the cell boundaries. Our method detects and treats topological events with an emphasis on the preservation of detailed features, while simultaneously simplifying those portions of the material that are not visible. Our surface tracker is not tied to a particular method for simulating deformable materials. In particular, we show results from two significantly different simulators: a Lagrangian FEM simulator with tetrahedral elements, and an Eulerian grid-based fluid simulator. Although our surface tracking method is generic, it is particularly well-suited for simulations that exhibit fine surface details and numerous topological events. Highlights of our results include merging of viscoplastic materials with complex geometry, a taffy-pulling animation with many fold and merge events, and stretching and slicing of stiff plastic material.\"",
        "title: \"Spatio-temporal geometry fusion for multiple hybrid cameras using moving least squares surfaces\" with abstract: \"Multi-view reconstruction aims at computing the geometry of a scene observed by a set of cameras. Accurate 3D reconstruction of dynamic scenes is a key component for a large variety of applications, ranging from special effects to telepresence and medical imaging. In this paper we propose a method based on Moving Least Squares surfaces which robustly and efficiently reconstructs dynamic scenes captured by a calibrated set of hybrid color+depth cameras. Our reconstruction provides spatio-temporal consistency and seamlessly fuses color and geometric information. We illustrate our approach on a variety of real sequences and demonstrate that it favorably compares to state-of-the-art methods.\"",
        "1 is \"Efficient Dense Stereo with Occlusions for New View-Synthesis by Four-State Dynamic Programming\", 2 is \"Seeing people in different light--joint shape, motion, and reflectance capture.\".",
        "\nGiven above information, for an author who has written the paper with the title \"Computational highlight holography\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0191": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Symmetry-Aware Nonrigid Matching of Incomplete 3D Surfaces':",
        "title: \"Estimating fluid simulation parameters from videos\" with abstract: \"Recently, a video-based high quality 3D shape and motion modeling methods for fluid are proposed. [Huamin et al. 2009] However, this approach only aims to capture and generate original fluid action as it is.\"",
        "title: \"Effective Nearest Neighbor Search for Aligning and Merging Range Images\" with abstract: \"This paper describes a novel method which extends the search algorithm of a k-d tree for aligning and merging range images. If the nearest neighbor point is far from a query, many of the leaf nodes must be examined during the search, which actually will not finish in logarithmic time. However such a distant point is not as important as the nearest neighbor in many, applications, such as aligning and merging range images; the reason for this is either because it is not consequently used or because its weight becomes very small. Thus, in this paper we propose a new algorithm that does not search strictly by pruning branches if the nearest neighbor point lies beyond a certain threshold. We call the technique the Bounds-Overlap-Threshold (BOT) test. The BOT test can be applied without re-creating the k-d tree if the threshold value changes. Then, we describe how we applied our new method to three applications in order to analyze its performance. Finally, we discuss the method's effectiveness.\"",
        "title: \"A Probabilistic Method for Aligning and Merging Range Images with Anisotropic Error Distribution\" with abstract: \"This paper describes a probabilistic method of aligning and merging range images. We formulate these issues as problems of estimating the maximum likelihood. By examining the error distribution of a range finder, we model it as a normal distribution along the line of sight. To align range images, our method estimates the parameters based on the ExpectationMaximization (EM) approach. By assuming the error model, the algorithm is implemented as an extension of the Iterative Closest Point (ICP) method. For merging range images, our method computes the signed distances by finding the distances of maximum likelihood. Since our proposed method uses multiple correspondences for each vertex of the range images, errors after aligning and merging range images are less than those of earlier methods that use one-to-one correspondences. Finally, we tested and validated the efficiency of our method by simulation and on real range images .\"",
        "title: \"Adaptive-Scale robust estimator using distribution model fitting\" with abstract: \"We propose a new robust estimator for parameter estimation in highly noisy data with multiple structures and without prior information on the noise scale of inliers This is a diagnostic method that uses random sampling like RANSAC, but adaptively estimates the inlier scale using a novel adaptive scale estimator The residual distribution model of inliers is assumed known, such as a Gaussian distribution Given a putative solution, our inlier scale estimator attempts to extract a distribution for the inliers from the distribution of all residuals This is done by globally searching a partition of the total distribution that best fits the Gaussian distribution Then, the density of the residuals of estimated inliers is used as the score in the objective function to evaluate the putative solution The output of the estimator is the best solution that gives the highest score Experiments with various simulations and real data for line fitting and fundamental matrix estimation are carried out to validate our algorithm, which performs better than several of the latest robust estimators.\"",
        "title: \"One-shot range scanner using coplanarity constraints\" with abstract: \"Methods for scanning dynamic scenes are important in many applications and many systems using structured light have been proposed. Many of these systems use either mul- tiple patterns projected rapidly or a single pattern. Although the former allows dense reconstruction with a sufficient num- ber of patterns, it has difficulty in capturing objects in rapid motion. The latter technique uses only a single pattern and have no such difficulties, however, they often have stability problems and their result tend to have low resolution. In this paper, we develop a system to achieve dense and accurate 3D measurement from only a single image. The proposed sys- tem also has the advantage of being robust in terms of image processing.\"",
        "1 is \"Shape reconstruction from cast shadows using coplanarities and metric constraints\", 2 is \"Path planning for mobile manipulators for multiple task execution\".",
        "\nGiven above information, for an author who has written the paper with the title \"Symmetry-Aware Nonrigid Matching of Incomplete 3D Surfaces\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0192": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Learning Tag Embeddings And Tag-Specific Composition Functions In Recursive Neural Network':",
        "title: \"A Generative Probabilistic Model for Multi-label Classification\" with abstract: \"Traditional discriminative classification method makes little attempt to reveal the probabilistic structure and the correlation within both input and output spaces. In the scenario of multi-label classification, most of the classifiers simply assume the predefined classes are independently distributed, which would definitely hinder the classification performance when there are intrinsic correlations between the classes. In this article, we propose a generative probabilistic model, the Correlated Labeling Model (CoL Model), to formulate the correlation between different classes. The CoL model is presented to capture the correlation between classes and the underlying structures via the latent random variables in a supervised manner. We develop a variational procedure to approximate the posterior distribution and employ the EM algorithm for the empirical Bayes parameter estimation. In our evaluations, the proposed model achieved promising results on various data sets.\"",
        "title: \"Recognizing biomedical named entities using skip-chain conditional random fields\" with abstract: \"Linear-chain Conditional Random Fields (CRF) has been applied to perform the Named Entity Recognition (NER) task in many biomedical text mining and information extraction systems. However, the linear-chain CRF cannot capture long distance dependency, which is very common in the biomedical literature. In this paper, we propose a novel study of capturing such long distance dependency by defining two principles of constructing skip-edges for a skip-chain CRF: linking similar words and linking words having typed dependencies. The approach is applied to recognize gene/protein mentions in the literature. When tested on the BioCreAtIvE II Gene Mention dataset and GENIA corpus, the approach contributes significant improvements over the linear-chain CRF. We also present in-depth error analysis on inconsistent labeling and study the influence of the quality of skip edges on the labeling performance.\"",
        "title: \"Story Ending Generation with Incremental Encoding and Commonsense Knowledge.\" with abstract: \"Generating a reasonable ending for a given story context, i.e., story ending generation, is a strong indication of story comprehension. This task requires not only to understand the context clues which play an important role in planning the plot, but also to handle implicit knowledge to make a reasonable, coherent story. In this paper, we devise a novel model for story ending generation. The model adopts an incremental encoding scheme to represent context clues which are spanning in the story context. In addition, commonsense knowledge is applied through multi-source attention to facilitate story comprehension, and thus to help generate coherent and reasonable endings. Through building context clues and using implicit knowledge, the model is able to produce reasonable story endings. Automatic and manual evaluation shows that our model can generate more reasonable story endings than state-of-the-art baselines.(1)\"",
        "title: \"A Novel Method for Dialogue Management Based on The Finite State Automaton\" with abstract: \"Recently spoken dialogue systems in a range of domains have been developed, but there are still no commonly acceptable methods for the dialogue management. In this paper, we propose a novel scheme for it, in which a finite state automaton based on slot-feature is designed to establish the whole dialogue management structure. Two functions are used to implement the strategy control in dialogue session and proved to work pretty well and be efficient. Furthermore, intention-layered trees are proposed and applied to the topic detection and switch, which is quite intuitive and functionary in the multi-topics applications. Me experimental results show that our scheme is promising.\"",
        "title: \"Exploring weakly supervised latent sentiment explanations for aspect-level review analysis\" with abstract: \"In sentiment analysis, aspect-level review analysis has been an important task because it can catalogue, aggregate, or summarize various opinions according to a product's properties. In this paper, we explore a new concept for aspect-level review analysis, latent sentiment explanations, which are defined as a set of informative aspect-specific sentences whose polarities are consistent with that of the review. In other words, sentiment explanations best represent a review in terms of both aspect and polarity. We formulate the problem as a structure learning problem, and sentiment explanations are modeled with latent variables. Training samples are automatically identified through a set of pre-defined aspect signature terms (i.e., without manual annotation on samples), which we term the way weakly supervised. Our major contributions lie in two folds: first, we formalize the use of aspect signature terms as weak supervision in a structural learning framework, which remarkably promotes aspect-level analysis; second, the performance of aspect analysis and document-level sentiment classification are mutually enhanced through joint modeling. The proposed method is evaluated on restaurant and hotel reviews respectively, and experimental results demonstrate promising performance in both document-level and aspect-level sentiment analysis.\"",
        "1 is \"Joint sentiment/topic model for sentiment analysis\", 2 is \"A Tree Sequence Alignment-based Tree-to-Tree Translation Model\".",
        "\nGiven above information, for an author who has written the paper with the title \"Learning Tag Embeddings And Tag-Specific Composition Functions In Recursive Neural Network\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0193": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'An introduction to string re-writing kernel':",
        "title: \"A QoS Multicast Routing Protocol with Mobile Prediction Based on MAODV in MANETs\" with abstract: \"Qos multicast routing is the process for upbuilding a tree which is rooted from the source node, contains all the multicast destinations and satisfies the QoS constrains. This paper introduces QoS routing problem and several mobile prediction strategies. It depicts QoS multicast model and proposes a QoS multicast routing protocol with mobile prediction based on MAODV (MPMRPQ). Simulation results demonstrate that the scheme is an effective approach to multicast routing decision with multiple QoS constrains. The proposed scheme can reduce multicast packet loss, network overload and optimize the delivery path.\"",
        "title: \"Recognizing biomedical named entities using skip-chain conditional random fields\" with abstract: \"Linear-chain Conditional Random Fields (CRF) has been applied to perform the Named Entity Recognition (NER) task in many biomedical text mining and information extraction systems. However, the linear-chain CRF cannot capture long distance dependency, which is very common in the biomedical literature. In this paper, we propose a novel study of capturing such long distance dependency by defining two principles of constructing skip-edges for a skip-chain CRF: linking similar words and linking words having typed dependencies. The approach is applied to recognize gene/protein mentions in the literature. When tested on the BioCreAtIvE II Gene Mention dataset and GENIA corpus, the approach contributes significant improvements over the linear-chain CRF. We also present in-depth error analysis on inconsistent labeling and study the influence of the quality of skip edges on the labeling performance.\"",
        "title: \"Verification Code Recognition Based on Active and Deep Learning\" with abstract: \"A verification code is an automated test method used to distinguish between humans and computers. Humans can easily identify verification codes, whereas machines cannot. With the development of convolutional neural networks, automatically recognizing a verification code is now possible for machines. However, the advantages of convolutional neural networks depend on the data used by the training classifier, particularly the size of the training set. Therefore, identifying a verification code using a convolutional neural network is difficult when training data are insufficient. This study proposes an active and deep learning strategy to obtain new training data on a special verification code set without manual intervention. A feature learning model for a scene with less training data is presented in this work, and the verification code is identified by the designed convolutional neural network. Experiments show that the method can considerably improve the recognition accuracy of a neural network when the amount of initial training data is small.\"",
        "title: \"Promoting diversity in recommendation by entropy regularizer\" with abstract: \"We study the problem of diverse promoting recommendation task: selecting a subset of diverse items that can better predict a given user's preference. Recommendation techniques primarily based on user or item similarity can suffer from the risk that users cannot get expected information from the over-specified recommendation lists. In this paper, we propose an entropy regularizer to capture the notion of diversity. The entropy regularizer has good properties in that it satisfies monotonicity and submodularity, such that when we combine it with a modular rating set function, we get submodular objective function, which can be maximized approximately by efficient greedy algorithm, with provable constant factor guarantee of optimality. We apply our approach on the top-K prediction problem and evaluate its performance on Movie-Lens data set, which is a standard database containing movie rating data collected from a popular online movie recommender system. We compare our model with the state-of-the-art recommendation algorithms. Our experiments show that entropy regularizer effectively captures diversity and hence improves the performance of recommendation task.\"",
        "title: \"Learning Tag Embeddings And Tag-Specific Composition Functions In Recursive Neural Network\" with abstract: \"Recursive neural network is one of the most successful deep learning models for natural language processing due to the compositional nature of text. The model recursively composes the vector of a parent phrase from those of child words or phrases, with a key component named composition function. Although a variety of composition functions have been proposed, the syntactic information has not been fully encoded in the composition process. We propose two models, Tag Guided RNN (TG-RNN for short) which chooses a composition function according to the part-of-speech tag of a phrase, and Tag Embedded RNN/RNTN (TE-RNN/RNTN for short) which learns tag embeddings and then combines tag and word embeddings together. In the fine-grained sentiment classification, experiment results show the proposed models obtain remarkable improvement: TG-RNN/TE-RNN obtain remarkable improvement over baselines, TE-RNTN obtains the second best result among all the top performing models, and all the proposed models have much less parameters/complexity than their counter-parts.\"",
        "1 is \"Privacy-Preserving Matching of DNA Profiles\", 2 is \"Tri-Training: Exploiting Unlabeled Data Using Three Classifiers\".",
        "\nGiven above information, for an author who has written the paper with the title \"An introduction to string re-writing kernel\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0194": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'RBN-World: a sub-symbolic artificial chemistry':",
        "title: \"On the Advantages of Variable Length GRNs for the Evolution of Multicellular Developmental Systems\" with abstract: \"Biological genomes have evolved over a period of millions of years and comprise thousands of genes, even for the simplest organisms. However, in nature, only 1\u20132% of the genes play an active role in creating and maintaining the organism, while the majority are evolutionary fossils. This raises the question of whether a considerably larger number of (partly redundant) genes are required in order to effectively build a functional developmental system, of which, in the final system only a fraction is required for the latter to function. This paper investigates different approaches to creating artificial developmental systems (ADSs) based on variable length gene regulatory networks (GRNs). The GRNs are optimized using an evolutionary algorithm (EA). A comparison is made between the different variable length representations and fixed length representations. It is shown that variable length GRNs can achieve both reducing computational effort during optimization and increasing speed and compactness of the resulting ADS, despite the higher complexity of the encoding required. The results may also improve the understanding of how to effectively model GRN based developmental systems. Taking results of all experiments into account makes it possible to create an overall ranking of the different patterns used as a testbench in terms of their complexity. This ranking may aid to compare related work against. In addition, this allows a detailed assessment of the ADS used and enables the identification of missing mechanisms.\"",
        "title: \"Self Modifying Cartesian Genetic Programming: Fibonacci, Squares, Regression and Summing\" with abstract: \"Self Modifying CGP (SMCGP) is a developmental form of Cartesian Genetic Programming(CGP). It is able to modify its own phenotype during execution of the evolved program. This is done by the inclusion of modification operators in the function set. Here we present the use of the technique on several different sequence generation and regression problems.\"",
        "title: \"A biological development model for the design of robust multiplier\" with abstract: \"A biologically inspired developmental model targeted at hardware implementation (off-shelf FPGA) is proposed which exhibits extremely robust transient fault-tolerant capability. All cells in this model have identical genotype (physical structures), and only differ in internal states. In a 3x3 cell digital organism, some individuals which implement a 2-bit multiplier were discovered using evolution that have the ability to \u201crecover\u201d themselves from almost any kinds of transient faults. An intrinsic evolvable hardware platform based on FPGA was realized to speed up the evolution process.\"",
        "title: \"A model for intrinsic artificial development featuring structural feedback and emergent growth\" with abstract: \"A model for intrinsic artificial development is introduced in this paper. The proposed model features a novel mechanism where growth emerges, rather than being triggered by a single action. Different types of cell signalling ensure that breaking symmetries is rather the norm than an exception, and gene activity is regulated on two layers: first, by the proteins that are produced by the gene regulatory network (GRN). Second, through structural feedback by second messenger molecules, which are not directly produced through gene expression, but are produced by sensor proteins, which take the cell's structure into account. The latter feedback mechanism is a novel approach, intended to enable adaptivity and environment coupling in real-world applications. The model is implemented in hardware, and is designed to run autonomously in resource limited embedded systems. Initial experiments are carried out to measure long-term stability, dynamics, adaptivity and scalability of the new approach. Furthermore the ability of the GRN to produce patterns of different symmetries is examined.\"",
        "title: \"Intrinsic evolvable hardware implementation of a robust biological development model for digital systems\" with abstract: \"An intrinsic evolvable hardware platform was realized to accelerate the evolutionary search process of a biologically inspired developmental model targeted at off-shelf FPGA implementation. The model has the capability of exhibiting very large transient fault-tolerance. The evolved circuits make up a digital \"organism\" from identical cells which only differ in internal states. Organisms implementing a 2-bit multiplier were evolved that can \"recover\" from almost any kinds of transient faults. This paper focuses on the design concerns and details of the evolvable hardware system, including the digital organism/cell and the intrinsic FPGA-based evolvable hardware platform.\"",
        "1 is \"Sharp Retrenchment, Modulated Refinement and Simulation.\", 2 is \"The arcade learning environment: an evaluation platform for general agents\".",
        "\nGiven above information, for an author who has written the paper with the title \"RBN-World: a sub-symbolic artificial chemistry\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0195": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'A comparison of methods for sketch-based 3D shape retrieval':",
        "title: \"Chosen Ciphertext Security via Point Obfuscation.\" with abstract: \"In this paper, we show two new constructions of chosen ciphertext secure (CCA secure) public key encryption (PKE) from general assumptions. The key ingredient in our constructions is an obfuscator for point functions with multi-bit output (MBPF obfuscators, for short), that satisfies some (average-case) indistinguishability-based security, which we call AIND security, in the presence of hard-to-invert auxiliary input. Specifically, our first construction is based on a chosen plaintext secure PKE scheme and an MBPF obfuscator satisfying the AIND security in the presence of computationally hard-to-invert auxiliary input. Our second construction is based on a lossy encryption scheme and an MBPF obfuscator satisfying the AIND security in the presence of statistically hard-to-invert auxiliary input. To clarify the relative strength of AIND security, we show the relations among security notions for MBPF obfuscators, and show that AIND security with computationally (resp. statistically) hard-to-invert auxiliary input is implied by the average-case virtual black-box (resp. virtual grey-box) property with the same type of auxiliary input. Finally, we show that a lossy encryption scheme can be constructed from an obfuscator for point functions (point obfuscator) that satisfies re-randomizability and a weak form of composability in the worst-case virtual grey-box sense. This result, combined with our second generic construction and several previous results on point obfuscators and MBPF obfuscators, yields a CCA secure PKE scheme that is constructed solely from a re-randomizable and composable point obfuscator. We believe that our results make an interesting bridge that connects CCA secure PKE and program obfuscators, two seemingly isolated but important cryptographic primitives in the area of cryptography.\"",
        "title: \"Relations between constrained and bounded chosen ciphertext security for key encapsulation mechanisms\" with abstract: \"In CRYPTO 2007, Hofheinz and Kiltz formalized a security notion for key encapsulation mechanisms (KEMs), called constrained chosen ciphertext (CCCA) security, which is strictly weaker than ordinary chosen ciphertext (CCA) security, and showed a new composition paradigm for CCA secure hybrid encryption. Thus, CCCA security of a KEM turned out to be quite useful. However, since the notion is relatively new and its definition is slightly complicated, relations among CCCA security and other security notions have not been clarified well. In this paper, in order to better understand CCCA security and the construction of CCCA secure KEMs, we study relations between CCCA and bounded CCA security, where the latter notion considers security against adversaries that make a-priori bounded number of decapsulation queries, and is also strictly weaker than CCA security. Specifically, we show that in most cases there are separations between these notions, while there is some unexpected implication from (a slightly stronger version of) CCCA security to a weak form of 1-bounded CCA security. We also revisit the construction of a KEM from a hash proof system (HPS) with computational security properties, and show that the HPS-based KEM, which was previously shown CCCA secure, is actually 1-bounded CCA secure as well. This result, together with the above general implication, suggests that 1-bounded CCA security can be essentially seen as a \u2018\u2018necessary\" condition for a CCCA secure KEM.\"",
        "title: \"Recognition of Layout-Free Characters on Complex Background\" with abstract: \"Recognizing characters in a scene is a challenging and unsolved problem. In this demonstration, we show an effective approach to cope with the problems: recognizing Japanese characters including complex characters such as Kanji (Chinese characters), which may not be aligned on a straight line and may be printed on a complex background. In the demo, our recognition method is applied to image sequences captured with a web camera. The recognition method is based on local features and their alignment. In addition, using a tracking method, recognition results and extracted features are accumulated so as to increase recognition accuracy as time goes on. The demo runs about 1 fps on a standard laptop computer.\"",
        "title: \"Performance analysis of TCP fairness between wired and wireless sessions\" with abstract: \"A significant amount of wireless traffic will be car- ried in the Internet, and wireless connections need to share the network resources with wired connections. However, in a wire- less network environment, TCP, one of the most important trans- port protocol of TCP/IP, suffers from significant throughput degra- dation due to the lossy characteristics of a wireless link. There- fore, in order to design the next generation mobile networks, it is necessary to know how much the wireless connection suf- fers from the degradation in comparison to the wired connec- tion. In this paper, we discuss the fairness issue between TCP connections over wireless and wired links, and theoretically an- alyze throughput fairness between TCP over wireless link with ARQ(Automatic Repeat reQuest)-based link layer error recov- ery and TCP over error-free wired link. We validate our anal- ysis by comparing numerical results obtained from the analysis with computer simulation ones.\"",
        "title: \"Impact of round trip delay self-similarity on TCP performance\" with abstract: \"Previous measurement showed that self-similar nature is found not only in network traffic volume but also round trip packet delay. In this paper, we discuss three issues of the self-similarity of round trip time (RTT), which is one of the most important parameters to determine TCP throughput performance. First, we discuss the origin of the packet delay self-similarity. One study anticipated that the queueing delay of self-similar traffic is the reason for packet delay self-similarity. With computer simulation, we evaluate the correlation between traffic and RTT self-similarity. Next, we investigate the impact of RTT self-similarity on TCP throughput performance. Computer simulation results show that RTT self-similarity gives high variability to file transfer time. Finally, we investigate the impact of RTT self-similarity on RTO (retransmission time out). We discover that the bigger the Hurst parameter of the RTT is, the more frequent unnecessary timeouts occurs. Furthermore, we propose a new RTO calculation algorithm to improve these unnecessary timeouts\"",
        "1 is \"Improved Non-committing Encryption Schemes Based on a General Complexity Assumption\", 2 is \"A user study on visualizing directed edges in graphs\".",
        "\nGiven above information, for an author who has written the paper with the title \"A comparison of methods for sketch-based 3D shape retrieval\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0196": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Traffic Anomaly Detection Based On Robust Principal Component Analysis Using Periodic Traffic Behavior':",
        "title: \"Bounds for the tail distribution in a queue with a superposition of general periodic Markov sources: theory and application\" with abstract: \"An efficient yet accurate estimation of the tail distribution of the queue length has been considered as one of the most important issues in call admission and congestion controls in ATM networks. The arrival process in ATM networks is essentially a superposition of sources which are typically bursty and periodic either due to their origin or their periodic slot occupation after traffic shaping. In this paper, we consider a discrete-time queue where the arrival process is a superposition of general periodic Markov sources. The general periodic Markov source is rather general since it is assumed only to be irreducible, stationary and periodic. Note also that the source model can represent multiple time-scale correlations in arrivals. For this queue, we obtain upper and lower bounds for the asymptotic tail distribution of the queue length by bounding the asymptotic decay constant. The formulas can be applied to a queue having a huge number of states describing the arrival process. To show this, we consider an MPEG-like source which is a special case of general periodic Markov sources. The MPEG-like source has three time-scale correlations: peak rate, frame length and a group of pictures. We then apply our bound formulas to a queue with a superposition of MPEG-like sources, and provide some numerical examples to show the numerical feasibility of our bounds. Note that the number of states in a Markov chain describing the superposed arrival process is more than 1.4 \u00d7 10^{88}. Even for such a queue, the numerical examples show that the order of the magnitude of the tail distribution can be readily obtained.\"",
        "title: \"MAP/M/c and M/PH/c queues with constant impatience times\" with abstract: \"This paper considers stationary MAP/M/c and M/PH/c queues with constant impatience times. In those queues, waiting customers leave the system without receiving their services if their elapsed waiting times exceed a predefined deterministic threshold. For the MAP/M/c queue with constant impatience times, Choi et al. (Math Oper Res 29:309\u2013325, ) derive the virtual waiting time distribution, from which the loss probability and the actual waiting time distribution are obtained. We first refine their result for the virtual waiting time and then derive the stationary queue length distribution. We also discuss the computational procedure for performance measures of interest. Next we consider the stationary M/PH/c queue with constant impatience times and derive the loss probability, the waiting time distribution, and the queue length distribution. Some numerical results are also provided.\"",
        "title: \"Nonlinear Integer Programming Formulation For Quasi-Optimal Grouping Of Clusters In Ferry-Assisted Dtns\" with abstract: \"Communication among isolated networks (clusters) in delay tolerant networks (DTNs) can be supported by a message ferry, which collects bundles from clusters and delivers them to a sink node. When there are lots of distant static clusters, multiple message ferries and sink nodes will be required. In this paper, we aim to make groups, each of which consists of physically close clusters, a sink node, and a message ferry. Our objective is minimizing the overall mean delivery delay of bundles in consideration of both the offered load of clusters and distances between clusters and their sink nodes. Based on existing work, we first model this problem as a nonlinear integer programming. Using a commercial nonlinear solver, we obtain a quasi-optimal grouping. Through numerical evaluations, we show the fundamental characteristics of grouping, the impact of location limitation of base clusters, and the relationship between delivery delay and the number of base clusters.\"",
        "title: \"A generalization of the decomposition property in the M/G/1 queue with server vacations\" with abstract: \"This paper considers the M/G/1 queueing systems with server vacations. For a very general class of such systems, Fuhrmann and Cooper have shown that the stationary queue length at a random point in time is distributed as the sum of two independent random variables, one of which is the stationary queue length at a random point in time in the corresponding standard M/G/1 queue. This property is called the stochastic decomposition in the M/G/1 queue with server vacations. In this paper, we show that this decomposition property is also valid for the joint probability distribution of the queue length and the forward recurrence service time.\"",
        "title: \"Mean Buffer Contents in Discrete-Time Single-Server Queues with Heterogeneous Sources\" with abstract: \"We consider discrete-time single-server queues fed by independent, heterogeneous sources with geometrically distributed idle periods. While being active, each source generates some cells depending on the state of the underlying Markov chain. We first derive a general and explicit formula for the mean buffer contents in steady state when the underlying Markov chain of each source has finite states. Next we show the applicability of the general formula to queues fed by independent sources with infinite-state underlying Markov chains and discrete phase-type active periods. We then provide explicit formulas for the mean buffer contents in queues with Markovian autoregressive sources and greedy sources. Further we study two limiting cases in general settings, one is that the lengths of active periods of each source are governed by an infinite-state absorbing Markov chain, and the other is the model obtained by the limit such that the number of sources goes to infinity under an appropriate normalizing condition. As you will see, the latter limit leads to a queue with (generalized) M/G/\u9a74 input sources. We provide sufficient conditions under which the general formula is applicable to these limiting cases.\"",
        "1 is \"Some (in)sufficient conditions for secure hybrid encryption\", 2 is \"Generalized guaranteed rate scheduling algorithms: a framework\".",
        "\nGiven above information, for an author who has written the paper with the title \"Traffic Anomaly Detection Based On Robust Principal Component Analysis Using Periodic Traffic Behavior\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0197": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'BorderGuard: detecting cold potatoes from peers':",
        "title: \"Midas: An Impact Scale For Ddos Attacks\" with abstract: \"We usually have well-defined classification scales to estimate the intensity and impact of natural disasters. Prominent examples are the Richter and the Fujita scales for measuring earthquakes and tornadoes respectively. In this paper, we apply similar ideas to estimate the impact of distributed denial of service (DDoS) attacks from the perspective of network operators. Devising such a classification scale improves our understanding of DDoS attacks by assessing the actual damage incurred from an ISP's perspective, and allows comparison of various mitigation strategies. We have designed MIDAS, a DDoS impact scale, based on the economic impact of a DDoS attack, calculated using economic and network data. We then present an approximation of the MIDAS scale that relies only on network measurements for ease of computation. To demonstrate the usefulness of the scale, we perform sensitivity analysis to qualitatively validate the magnitude of the scale value for diverse attacks.\"",
        "title: \"Ensemble: Community-Based Anomaly Detection for Popular Applications.\" with abstract: \"A major challenge in securing end-user systems is the risk of popular applications being hijacked at run-time. Traditional measures do not prevent such threats because the code itself is unmodified and local anomaly detectors are difficult to tune for correct thresholds due to insufficient training data. Given that the target of attackers are often popular applications for communication and social networking, we propose Ensemble, a novel, automated approach based on a trusted community of users contributing system-call level local behavioral profiles of their applications to a global profile merging engine. The trust can be assumed in cases such as enterprise environments and can be further policed by reputation systems, e.g., by exploiting trust relationships inherently associated with social networks. The generated global profile can be used by all community users for local anomaly detection or prevention. Evaluation results based on a malware pool of 57 exploits demonstrate that Ensemble is an effective defense technique for communities of about 300 or more users as in enterprise environments.\"",
        "title: \"SocialWatch: detection of online service abuse via large-scale social graphs\" with abstract: \"In this paper, we present a framework, SocialWatch, to detect attacker-created accounts and hijacked accounts for online services at a large scale. SocialWatch explores a set of social graph properties that effectively model the overall social activity and connectivity patterns of online users, including degree, PageRank, and social affinity features. These features are hard to mimic and robust to attacker counter strategies. We evaluate SocialWatch using a large, real dataset with more than 682 million users and over 5.75 billion directional relationships. SocialWatch successfully detects 56.85 million attacker-created accounts with a low false detection rate of 0.75% and a low false negative rate of 0.61%. In addition, SocialWatch detects 1.95 million hijacked accounts---among which 1.23 million were not detected previously---with a low false detection rate of 2%. Our work demonstrates the practicality and effectiveness of using large social graphs with billions of edges to detect real attacks.\"",
        "title: \"COMET: code offload by migrating execution transparently\" with abstract: \"In this paper we introduce a runtime system to allow unmodified multi-threaded applications to use multiple machines. The system allows threads to migrate freely between machines depending on the workload. Our prototype, COMET (Code Offload by Migrating Execution Transparently), is a realization of this design built on top of the Dalvik Virtual Machine. COMET leverages the underlying memory model of our runtime to implement distributed shared memory (DSM) with as few interactions between machines as possible. Making use of a new VM-synchronization primitive, COMET imposes little restriction on when migration can occur. Additionally, enough information is maintained so one machine may resume computation after a network failure. We target our efforts towards augmenting smartphones or tablets with machines available in the network. We demonstrate the effectiveness of COMET on several real applications available on Google Play. These applications include image editors, turn-based games, a trip planner, and math tools. Utilizing a server-class machine, COMET can offer significant speed-ups on these real applications when run on a modern smartphone. With WiFi and 3G networks, we observe geometric mean speed-ups of 2.88\u00d7 and 1.27\u00d7 relative to the Dalvik interpreter across the set of applications with speed-ups as high as 15\u00d7 on some applications.\"",
        "title: \"Locating internet routing instabilities\" with abstract: \"This paper presents a methodology for identifying the autonomous system (or systems) responsible when a routing change is observed and propagated by BGP. The origin of such a routing instability is deduced by examining and correlating BGP updates for many prefixes gathered at many observation points. Although interpreting BGP updates can be perplexing, we find that we can pinpoint the origin to either a single AS or a session between two ASes in most cases. We verify our methodology in two phases. First, we perform simulations on an AS topology derived from actual BGP updates using routing policies that are compatible with inferred peering/customer/provider relationships. In these simulations, in which network and router behavior are \"ideal\", we inject inter-AS link failures and demonstrate that our methodology can effectively identify most origins of instability. We then develop several heuristics to cope with the limitations of the actual BGP update propagation process and monitoring infrastructure, and apply our methodology and evaluation techniques to actual BGP updates gathered at hundreds of observation points. This approach of relying on data from BGP simulations as well as from measurements enables us to evaluate the inference quality achieved by our approach under ideal situations and how it is correlated with the actual quality and the number of observation points.\"",
        "1 is \"An implementation and analysis of the virtual interface architecture\", 2 is \"Exposing private information by timing web applications\".",
        "\nGiven above information, for an author who has written the paper with the title \"BorderGuard: detecting cold potatoes from peers\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0198": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'A clean slate 4D approach to network control and management':",
        "title: \"Dynamic scheduling of network updates\" with abstract: \"We present Dionysus, a system for fast, consistent network updates in software-defined networks. Dionysus encodes as a graph the consistency-related dependencies among updates at individual switches, and it then dynamically schedules these updates based on runtime differences in the update speeds of different switches. This dynamic scheduling is the key to its speed; prior update methods are slow because they pre-determine a schedule, which does not adapt to runtime conditions. Testbed experiments and data-driven simulations show that Dionysus improves the median update speed by 53--88% in both wide area and data center networks compared to prior methods.\"",
        "title: \"A new approach to interdomain routing based on secure multi-party computation\" with abstract: \"Interdomain routing involves coordination among mutually distrustful parties, leading to the requirements that BGP provide policy autonomy, flexibility, and privacy. BGP provides these properties via the distributed execution of policy-based decisions during the iterative route computation process. This approach has poor convergence properties, makes planning and failover difficult, and is extremely difficult to change. To rectify these and other problems, we propose a radically different approach to interdomain-route computation, based on secure multi-party computation (SMPC). Our approach provides stronger privacy guarantees than BGP and enables the deployment of new policy paradigms. We report on an initial exploration of this idea and outline future directions for research.\"",
        "title: \"BGP safety with spurious updates\" with abstract: \"We explore BGP safety, the question of whether a BGP system converges to a stable routing, in light of several BGP implementation features that have not been fully included in the previous theoretical analyses. We show that Route Flap Damping, MRAI timers, and other intra-router features can cause a router to briefly send \u201cspurious\u201d announcements of less-preferred routes. We demonstrate that, even in simple configurations, this short-term spurious behavior may cause long-term divergence in global routing. We then present DPVP, a general model that unifies these sources of spurious announcements in order to examine their impact on BGP safety. In this new, more robust model of BGP behavior, we derive a necessary and sufficient condition for safety, which furthermore admits an efficient algorithm for checking BGP safety in most practical circumstances - two complementary results that have been elusive in the past decade's worth of classical studies of BGP convergence in more simple models. We also consider the implications of spurious updates for well-known results on dispute wheels and safety under filtering.\"",
        "title: \"Continuous in-network round-trip time monitoring\" with abstract: \"Round-trip time (RTT) is a central metric that influences end-user QoE and can expose traffic-interception attacks. Many popular RTT monitoring techniques either send active probes (that do not capture application-level RTTs) or passively monitor only the TCP handshake (which can be inaccurate, especially for long-lived flows). High-speed programmable switches present a unique opportunity to monitor the RTTs continuously and react in real time to improve performance and security. In this paper, we present Dart, an inline, real-time, and continuous RTT measurement system that can enable automated detection of network events and adapt (e.g., routing, scheduling, marking, or dropping traffic) inside the network. However, designing Dart is fraught with challenges, due to the idiosyncrasies of the TCP protocol and the resource constraints in high-speed switches. Dart overcomes these challenges by strategically limiting the tracking of packets to only those that can generate useful RTT samples, and by identifying the synergy between per-flow state and per-packet state for efficient memory use. We present a P4 prototype of Dart for the Tofino switch, as well our experiments on a campus testbed and simulations using anonymized campus traces. Dart, running in real time and with limited data-plane memory, is able to collect 99% of the RTT samples of an offline, software baseline---a variant of the popular tcptrace tool that has access to unlimited memory.\"",
        "title: \"Toward Software-Defined Cellular Networks\" with abstract: \"Existing cellular networks suffer from inflexible and expensive equipment, complex control-plane protocols, and vendor-specific configuration interfaces. In this position paper, we argue that software defined networking (SDN) can simplify the design and management of cellular data networks, while enabling new services. However, supporting many subscribers, frequent mobility, fine-grained measurement and control, and real-time adaptation introduces new scalability challenges that future SDN architectures should address. As a first step, we propose extensions to controller platforms, switches, and base stations to enable controller applications to (i) express high-level policies based on subscriber attributes, rather than addresses and locations, (ii) apply real-time, fine-grained control through local agents on the switches, (iii)perform deep packet inspection and header compression on packets, and (iv)remotely manage shares of base-station resources.\"",
        "1 is \"Design, implementation, and evaluation of the linear road bnchmark on the stream processing core\", 2 is \"Disruption Free Topology Reconfiguration in OSPF Networks\".",
        "\nGiven above information, for an author who has written the paper with the title \"A clean slate 4D approach to network control and management\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "0199": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Finding a needle in a haystack: pinpointing significant BGP routing changes in an IP network':",
        "title: \"Secure Optimization Computation Outsourcing in Cloud Computing: A Case Study of Linear Programming\" with abstract: \"Cloud computing enables an economically promising paradigm of computation outsourcing. However, how to protect customers confidential data processed and generated during the computation is becoming the major security concern. Focusing on engineering computing and optimization tasks, this paper investigates secure outsourcing of widely applicable linear programming (LP) computations. Our mechanism design explicitly decomposes LP computation outsourcing into public LP solvers running on the cloud and private LP parameters owned by the customer. The resulting flexibility allows us to explore appropriate security/efficiency tradeoff via higher-level abstraction of LP computation than the general circuit representation. Specifically, by formulating private LP problem as a set of matrices/vectors, we develop efficient privacy-preserving problem transformation techniques, which allow customers to transform the original LP into some random one while protecting sensitive input/output information. To validate the computation result, we further explore the fundamental duality theorem of LP and derive the necessary and sufficient conditions that correct results must satisfy. Such result verification mechanism is very efficient and incurs close-to-zero additional cost on both cloud server and customers. Extensive security analysis and experiment results show the immediate practicability of our mechanism design.\"",
        "title: \"TAU 2013 variation aware timing analysis contest\" with abstract: \"Timing analysis is a key component of any integrated circuit (IC) chip design-closure flow, and is employed at various stages of the flow including pre/post-route timing optimization and timing signoff. While accurate timing analysis is important, the run-time of the analysis is equally critical with growing chip design sizes and complexity (for example, increasing number of clocks domains, voltage islands, etc.). In addition, the increasing significance of variability in the chip manufacturing process as well as environmental variability necessitates use of variation aware techniques (e.g. statistical, multi-corner) for chip timing analysis which significantly impacts the analysis run-time. The aim of the TAU 2013 variation aware timing contest is to seek novel ideas for fast variation aware timing analysis, by means of the following: (a) increase awareness of variation aware timing analysis and provide insight into some challenging aspects of the analysis, (b) encourage novel parallelization techniques (including multi-threading) for timing analysis, and (c) facilitate creation of a publicly available variation aware timing analysis framework and benchmarks to further advance research in this area.\"",
        "title: \"OPTWALL: A Hierarchical Traffic-Aware Firewall\" with abstract: \"The overall efficiency, reliability, and availability of a firewall is crucial in enforcing and administrating securit y, especially when the network is under attack. The continuous growth of th e Internet, coupled with the increasing sophistication of th e attacks, is placing stringent demands on firewall performance. These ch allenges require new designs, architecture and algorithms to optimi ze firewalls. In this paper, we propose OPTWALL, an adaptive hierarchical firewall optimization framework aimed at reducing operatio nal cost of firewalls. The main features of the proposed approach are t he hierarchical design, splitting techniques, an online traf fic adaptation mechanism, and a strong reactive scheme to counter maliciou s attacks (e.g. Denial-of-Service (DoS) attacks). To the best of our k nowledge, this work is the first of its kind to use traffic characteristics in the design of an adaptive hierarchical firewall optimization fr amework. To study the performance of OPTWALL, a set of experiments are conducted on Linux ipchains. The performance evaluation st udy uses a large set of firewall policies and traffic traces managed by a Tier- 1 ISP and provides security access for the ISP network from/t o its business partners. Results show the high potential of OPTWA LL to reduce the operational cost of firewalls. In particular, theresults show that a performance improvement of nearly 35% can been achiev ed in a heavily loaded network environment.\"",
        "title: \"Statistical timing verification for transparently latched circuits through structural graph traversal\" with abstract: \"Level-sensitive transparent latches are widely used in high-performance sequential circuit designs. Under process variations, the timing of a transparently latched circuit will adapt random delays at runtime due to time borrowing. The central problem to determine the timing yield is to compute the probability of the presence of a positive cycle in the latest latch timing graph. Existing algorithms are either optimistic since cycles are omitted or require iterations that cannot be polynomially bounded. In this paper, we present the first algorithm to compute such probability based on block-based statistical timing analysis that, first, covers all cycles through a structural graph traversal, and second, terminates within a polynomial number of statistical \u00c2\u00bfsum\u00c2\u00bf and \u00c2\u00bfmax\u00c2\u00bf operations. Experimental results confirm that the proposed approach is effective and efficient.\"",
        "title: \"A light-weight distributed scheme for detecting ip prefix hijacks in real-time\" with abstract: \"As more and more Internet IP prefix hijacking incidents are being reported, the value of hijacking detection services has become evident. Most of the current hijacking detection approaches monitor IP prefixes on the control plane and detect inconsistencies in route advertisements and route qualities. We propose a different approach that utilizes information collected mostly from the data plane. Our method is motivated by two key observations: when a prefix is not hijacked, 1) the hop count of the path from a source to this prefix is generally stable; and 2) the path from a source to this prefix is almost always a super-path of the path from the same source to a reference point along the previous path, as long as the reference point is topologically close to the prefix. By carefully selecting multiple vantage points and monitoring from these vantage points for any departure from these two observations, our method is able to detect prefix hijacking with high accuracy in a light-weight, distributed, and real-time fashion. Through simulations constructed based on real Internet measurement traces, we demonstrate that our scheme is accurate with both false positive and false negative ratios below 0.5%.\"",
        "1 is \"Compressed Bloom filters\", 2 is \"On the optimal placement of web proxies in the Internet\".",
        "\nGiven above information, for an author who has written the paper with the title \"Finding a needle in a haystack: pinpointing significant BGP routing changes in an IP network\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01100": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'MeanField analysis for the evaluation of gossip protocols':",
        "title: \"Ready to preorder: The case of weak process semantics\" with abstract: \"Recently, Aceto, Fokkink and Ingolfsdottir proposed an algorithm to turn any sound and ground-complete axiomatisation of any preorder listed in the linear time-branching time spectrum at least as coarse as the ready simulation preorder, into a sound and ground-complete axiomatisation of the corresponding equivalence-its kernel. Moreover, if the former axiomatisation is @w-complete, so is the latter. Subsequently, de Frutos Escrig, Gregorio Rodriguez and Palomino generalised this result, so that the algorithm is applicable to any preorder at least as coarse as the ready simulation preorder, provided it is initials preserving. The current paper shows that the same algorithm applies equally well to weak semantics: the proviso of initials preserving can be replaced by other conditions, such as weak initials preserving and satisfying the second @t-law. This makes it applicable to all 87 preorders surveyed in ''the linear time-branching time spectrum II'' that are at least as coarse as the ready simulation preorder. We also extend the scope of the algorithm to infinite processes, by adding recursion constants. As an application of both extensions, we provide a ground-complete axiomatisation of the CSP failures equivalence for BCCS processes with divergence.\"",
        "title: \"From chi-t to \u00b5CRL: Combining Performance and Functional Analysis\" with abstract: \"In this paper we first give short overviews of the mod- elling languages timed ( t) and CRL. Then we present a general translation scheme to translate t specifications to CRL specifications. As t targets performance anal- ysis and CRL targets functional analysis of systems, this translation scheme provides a way to perform both kinds of analysis on a given t system model. Finally, we give an example of a t system and show how the translation works on a concrete case study.\"",
        "title: \"A Cook's Tour of Equational Axiomatizations for Prefix Iteration\" with abstract: \" this paper, we continue this research programme bystudying axiomatic characterizations for more abstract semantics over this languagethan those based on variations of bisimulation. More precisely, we consider readysimulation, simulation, readiness, trace and language semantics, and provide complete(in)equational axiomatizations for each of these notions over BCCS with prefixiteration. All of the axiom systems we present are finite, if so is the set of atomicactions under consideration.... \"",
        "title: \"Turning GSOS Rules into Equations for Linear Time-Branching Time Semantics\" with abstract: \"An existing axiomatization strategy for process algebras modulo bisimulation semantics can be extended so that it can be applied to other behavioural semantics as well. We study term rewriting properties of the resulting axiomatizations.\"",
        "title: \"An equational axiomatization for multi-exit iteration\" with abstract: \"This paper presents an equational axiomatization of bisimulation equivalence over the language of Basic Process Algebra (BPA) with multi-exit iteration. Multi-exit iteration is a generalization of the standard binary Kleene star operation that allows for the specification of agents that, up to bisimulation equivalence, are solutions of systems of recursion equations of the form X 1 = def P 1 X 2 +Q 1 \u22ee X n = def P n X 1 +Q n , where n is a positive integer and the P i and the Q i are process terms. The addition of multi-exit iteration to BPA yields a more expressive language than that obtained by augmenting BPA with the standard binary Kleene star (BPA*). As a consequence, the proof of completeness of the proposed equational axiomatization for this language, although standard in its general structure, is much more involved than that for BPA*. An expressiveness hierarchy for the family of k -exit iteration operators proposed by Bergstra, Bethke, and Ponse is also offered.\"",
        "1 is \"Pi-I: A Symmetric Calculus Based on Internal Mobility\", 2 is \"Approximate performability and dependability analysis using generalized stochastic Petri nets\".",
        "\nGiven above information, for an author who has written the paper with the title \"MeanField analysis for the evaluation of gossip protocols\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01101": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Test-cost-sensitive attribute reduction':",
        "title: \"Transversal and function matroidal structures of covering-based rough sets\" with abstract: \"In many real world applications, information blocks form a covering of a universe. Covering-based rough set theory has been proposed to deal with this type of information. It is more general and complex than classical rough set theory, hence there is much need to develop sophisticated structures to characterize covering-based rough sets. Matroids are important tools for describing graphs and linear independence of matrix theory. This paper establishes two matroidal structures of covering-based rough sets. Firstly, the transversal matroidal structure of a family of subsets of a universe is constructed. We also prove that the family of subsets of a universe is a covering if and only if the constructed transversal matroid is a normal one. Secondly, the function matroidal structure is constructed through the upper approximation number. Moreover, the relationships between the two matroidal structures are studied. Specifically, when a covering of a universe is a partition, these two matroidal structures coincide with each other.\"",
        "title: \"Feature selection with test cost constraint\" with abstract: \"Feature selection is an important preprocessing step in machine learning and data mining. In real-world applications, costs, including money, time and other resources, are required to acquire the features. In some cases, there is a test cost constraint due to limited resources. We shall deliberately select an informative and cheap feature subset for classification. This paper proposes the feature selection with test cost constraint problem for this issue. The new problem has a simple form while described as a constraint satisfaction problem (CSP). Backtracking is a general algorithm for CSP, and it is efficient in solving the new problem on medium-sized data. As the backtracking algorithm is not scalable to large datasets, a heuristic algorithm is also developed. Experimental results show that the heuristic algorithm can find the optimal solution in most cases. We also redefine some existing feature selection problems in rough sets, especially in decision-theoretic rough sets, from the viewpoint of CSP. These new definitions provide insight to some new research directions.\"",
        "title: \"Obfuscate arrays by homomorphic functions\" with abstract: \"As various computers are connected into a world wide network, software protections becomes a more and more important issue for software users and developers. There are some technical measures for software protections, such as hardware-based protections and software-based techniques,(-) etc. Software obfuscation is one of these measures. It protects software from unauthorized modification by making software more obscure so that it is hard for the potential attacker to understand the obfuscated software. Chow et al. use residue number technique to software obfuscation by encoding variables in the original program to hide the true meaning of these variables [1]. There is some discussion about the division of residue numbers in [1], but, in order to lay a sound ground for this technique, we proposed homomorphic functions in [2] to deal with division by several constants in residue numbers.Data structures are important components of programme and they are key clues for people to understand codes. Obfuscating data structures of programme will make it very hard for an enemy to attack them. In this paper, we apply homomorphic functions to obfuscating the data structures of software.\"",
        "title: \"Mining Significant Granular Association Rules For Diverse Recommendation\" with abstract: \"Granular association rule is a new technique to build recommender systems. The quality of a rule is often evaluated by the confidence measure, namely the probability that users purchase or rate certain items. Unfortunately, the confidence-based approach tends to suggest popular items to users, and novel patterns are often ignored. In this paper, we propose to mine significant granular association rules for diverse and novel recommendation. Generally, a rule is significant if the recommended items favor respective users more than others; while a recommender is diverse if it recommends different items to different users. We define two sets of measures to evaluate the quality of a rule as well as a recommender. Then we propose a significance-based approach seeking top-k significant rules for each user. Results on the MovieLens dataset show that the new approach provides more significant and diverse recommendations than the confidence-based one.\"",
        "title: \"Optimal and adaptive pairwise DNA sequence correlation analysis in natural gradient\" with abstract: \"Besides scoring function methods, correlation analysis is another index to indicate homology of the sequences. In this paper, the statistical properties of correlation between two comparing sequences were analyzed. Natural gradient method was proposed to extract the most uncorrelated components of sequences in an adaptive way and the optimal Performance Index (PI) was to measure the correlation between two sequences. This can be accomplished in a computational effective way by a fastest natural gradient falling algorithm. Result of the method is compared to conventional scoring methods and it shows that PI values are consistent with their BLAST identity rate. Natural gradient method can bypass the selection of scoring function and gap penalty parameters, which is a key and difficult problem in traditional sequence alignment algorithms, such as BLAST and FASTA etc.\"",
        "1 is \"Manifold Adaptive Experimental Design for Text Categorization\", 2 is \"Decision rule mining using classification consistency rate\".",
        "\nGiven above information, for an author who has written the paper with the title \"Test-cost-sensitive attribute reduction\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01102": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Reconfigurable LUT: A Double Edged Sword for Security-Critical Applications':",
        "title: \"A synthesis of side-channel attacks on elliptic curve cryptography in smart-cards.\" with abstract: \"Elliptic curve cryptography in embedded systems is vulnerable to side-channel attacks. Those attacks exploit biases in various kinds of leakages, such as power consumption, electromagnetic emanation, execution time, .... The integration of countermeasures is required to thwart known attacks. No single countermeasure can cover the whole range of attacks; thus many of them shall be combined. However, as each of them has a non negligible cost, one cannot simply apply all of them. It is necessary to wisely select countermeasures, depending on the context and on the trade-off between security and performance. This paper summarizes the side-channel attacks and countermeasures on Elliptic Curve Cryptography. For each countermeasure, the cost in time and space is given. Some attacks are clarified such as the doubling attack; others are improved like the horizontal SVA, and new attacks are described like the horizontal attack against the unified formulae.\"",
        "title: \"Evaluation of delay PUFs on CMOS 65 nm technology: ASIC vs FPGA.\" with abstract: \"This paper presents a comparative study of delay Physically Unclonable Functions (PUFs) designed in CMOS-65nm technology platforms: ASIC and FPGA (Xilinx Virtex-5). The performances are analyzed for two types of silicon PUFs, namely the arbiter and the loop PUFs. For this purpose, a PUF has been specifically designed, the \"mixed PUF\", to allow a fair comparison between the two structures. The principle of the mixed PUF design consists on the use of the same delay chains for both PUFs. The analysis is based on PUF responses obtained at different operating conditions for 18 ASICs. Each one embeds 49 PUF instances. The comparison analysis reveals that overall the arbiter PUF structure has the worst performance when compared to the loop PUF, on both platforms.\"",
        "title: \"Towards different flavors of combined side channel attacks\" with abstract: \"Side Channel Attacks (SCA) have come a long way since first introduced. Extensive research has improved various aspects of SCA like acquisition techniques, processing of traces, choice of leakage model, choice of distinguishers etc. As a result, side-channel countermeasures have also improved. It is difficult to defeat such countermeasures and requires a huge number of traces. So far, only a few works studied the combination of SCA. In this paper, we put forward two methods to combine different attacks to accelerate SCA or to reduce the number of traces to attack. The first method is a combination of commonly used distinguishers. We provide a theoretical method and an empirical approach to combine Pearson and Spearman correlation coefficients. The second method suggests a combination of different measurements corresponding to the same activity. A metric to assess this combination using information theory is also given. Both methods are supported by application on real traces. The gain is expressed in terms of reduction in number of traces to attack. We report a gain of 50% for the first method and 45% for the second method.\"",
        "title: \"Hardware Trojan Horses in Cryptographic IP Cores.\" with abstract: \"Detecting hardware trojans is a difficult task in general. In this article we study hardware trojan horses insertion and detection in cryptographic intellectual property (IP) blocks. The context is that of a fabless design house that sells IP blocks as GDSII hard macros, and wants to check that final products have not been infected by trojans during the foundry stage. First, we show the efficiency of a medium cost hardware trojans detection method if the placement or the routing have been redone by the foundry. It consists in the comparison between optical microscopic pictures of the silicon product and the original view from a GDSII layout database reader. Second, we analyze the ability of an attacker to introduce a hardware trojan horse without changing neither the placement nor the routing of the cryptographic IP logic. On the example of an AES engine, we show that if the placement density is beyond 80%, the insertion is basically impossible. Therefore, this settles a simple design guidance to avoid trojan horses insertion in cryptographic IP blocks: have the design be compact enough, so that any functionally discreet trojan necessarily requires a complete replace and re-route, which is detected by mere optical imaging (and not complete chip reverse-engineering).\"",
        "title: \"High precision fault injections on the instruction cache of ARMv7-M architectures\" with abstract: \"Hardware and software of secured embedded systems are prone to physical attacks. In particular, fault injection attacks revealed vulnerabilities on the data and the control flow allowing an attacker to break cryptographic or secured algorithms implementations. While many research studies concentrated on successful attacks on the data flow, only a few targets the instruction flow. In this paper, we focus on electromagnetic fault injection (EMFI) on the control flow, especially on the instruction cache. We target the very widespread (smartphones, tablets, settop-boxes, health-industry monitors and sensors, etc.) ARMv7-M architecture. We describe a practical EMFI platform and present a methodology providing high control level and high reproducibility over fault injections. Indeed, we observe that a precise fault model occurs in up to 96% of the cases. We then characterize and exhibit this practical fault model on the cache that is not yet considered in the literature. We comprehensively describe its effects and show how it can be used to reproduce well known fault attacks. Finally, we describe how it can benefits attackers to mount new powerful attacks or simplify existing ones.\"",
        "1 is \"Order-preserving encryption revisited: improved security analysis and alternative solutions\", 2 is \"A fast algorithm for computing multiplicative inverses in GF(2m) using normal bases\".",
        "\nGiven above information, for an author who has written the paper with the title \"Reconfigurable LUT: A Double Edged Sword for Security-Critical Applications\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01103": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'An Efficient Algorithm for the Approximate Median Selection Problem':",
        "title: \"An Improved Image Re-indexing Technique by Self Organizing Motor Maps\" with abstract: \"The paper presents a novel Motor Map neural network for re-indexing color mapped images. The overall learning process is able to smooth the local spatial redundancy of the indexes of the input image. Differently than before, the proposed optimization process is specifically devoted to re-organize the matrix of differences of the indexes computed according to some predefined patterns. Experimental results show that the proposed approach achieves good performances both in terms of compression ratio and zero order entropy of local differences. Also its computational complexity is competitive with previous works in the field.\"",
        "title: \"Market basket analysis from egocentric videos.\" with abstract: \"\u2022We propose a new challenging application domain for egocentric vision.\u2022We introduce the problem of Visual Market Basket Analysis (VMBA).\u2022We introduce a dataset of videos acquired by cameras mounted on shopping carts.\u2022We propose a hierarchical representation to discriminate between 14 cart behaviors.\"",
        "title: \"First Quantization Matrix Estimation From Double Compressed JPEG Images\" with abstract: \"One of the most common problems in the image forensics field is the reconstruction of the history of an image or a video. The data related to the characteristics of the camera that carried out the shooting, together with the reconstruction of the (possible) further processing, allow us to have some useful hints about the originality of the visual document under analysis. For example, if an image has been subjected to more than one JPEG compression, we can state that the considered image is not the exact bitstream generated by the camera at the time of shooting. It is then useful to estimate the quantization steps of the first compression, which, in case of JPEG images edited and then saved again in the same format, are no more available in the embedded metadata. In this paper, we present a novel algorithm to achieve this goal in case of double JPEG compressed images. The proposed approach copes with the case when the second quantization step is lower than the first one, exploiting the effects of successive quantizations followed by dequantizations. To improve the results of the estimation, a proper filtering strategy together with a function devoted to find the first quantization step, have been designed. Experimental results and comparisons with the state-of-the-art methods, confirm the effectiveness of the proposed approach.\"",
        "title: \"On-board monitoring system for road traffic safety analysis.\" with abstract: \"This paper presents a framework for road traffic safety analysis. It is based on a stereo-vision system that, after being installed on-board of public transportation vehicles, collects data of what happens in front of a moving vehicle. The collected data are analysed throughout a process that acquire raw GPS information, video sequences and stereo-based depth maps to compute the surrogate safety measures. These measures are obtained by exploiting the Traffic Conflict Technique in conjunction with computer vision algorithms and a cascade of classifiers. The safety measures are then used for further analysis in order to identify dangerous locations in which an intervention is needed to improve the safety level and prevent accidents. Experiments performed in a real urban environment confirm the effectiveness of the framework.\"",
        "title: \"Food Recognition Using Consensus Vocabularies.\" with abstract: \"Food recognition is an interesting and challenging problem with applications in medical, social and anthropological research areas. The high variability of food images makes the recognition task difficult for current state-of-the-art methods. It has been proved that the exploitation of multiple features to capture complementary aspects of the image contents is useful to improve the discrimination of different food items. In this paper we exploit an image representation based on the consensus among visual vocabularies built on different feature spaces. Starting from a set of visual codebooks, a consensus clustering technique is used to build a consensus vocabulary used to represent food pictures with a Bag-of-Visual-Words paradigm. This new representation is employed together with a SVM for recognition purpose.\"",
        "1 is \"A parallel multistart algorithm for the closest string problem\", 2 is \"DAISY: an efficient dense descriptor applied to wide-baseline stereo.\".",
        "\nGiven above information, for an author who has written the paper with the title \"An Efficient Algorithm for the Approximate Median Selection Problem\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01104": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Feedback Approach To Design For Assembly By Evaluation Of Assembly Plan':",
        "title: \"Correlation-based Attribute Outlier Detection in XML\" with abstract: \"Compared to relational data models, the hierarchical structure of semi-structured data such as XML provides semantically meaningful neighbourhoods advancing data cleaning problems such as outlier detection. In this paper, we introduce the concept of correlated subspace that leverages on the hierarchical relationships between XML attributes to provide contextually informative neighbourhoods for attribute outlier detection. We also design two correlation-based attribute outlier metrics for XML, namely the xO-Measure and xQ-Measure. The effectiveness of our XML outlier detection approach is supported with experimental results.\"",
        "title: \"Rewriting queries for XML integration systems\" with abstract: \"A data integration system typically creates a target XML schema to represent an application domain and source schemas are mapped to the target schema. A user poses a query over the target schema, and the system rewrites the query into a set of queries over the data sources. Existing algorithms generate a set of static rules based on the target schema and mappings, and rewrite the target query using these rules. We design a flexible and dynamic approach that rewrites XML queries directly based on the mappings between the target and source schemas. Theoretical analysis and experiments on both synthetic and real-world datasets indicate that the proposed approach is efficient and scalable.\"",
        "title: \"Using Interval Association Rules to Identify Dubious Data Values\" with abstract: \"A hard-to-catch erroneous data is one whose value looks perfectly legitimate. Yet, if we examine this value in conjunction with other attribute values, the value appear questionable. Detecting such dubious values is a major problem in data cleaning. This paper presents a framework to automatically detect dubious data values in the datasets, Data is first pre-processed by data smoothing and mapping. Next, interval association rules are generated which involved data partitioning via clustering before the rules axe generated using an Apriori algorithm. Finally, these rules are used to identify data values that fall outside the expected intervals. Experiment results show that the proposed framework is able to accurately and efficiently dubious values in large datasets.\"",
        "title: \"Post-analysis of learned rules\" with abstract: \"Rule induction research implicitly assumes that after producing the rules from a dataset, these rules will be used directly by an expert system or a human user. In real-life applications, the situation may not be as simple as that, particularly, when the user of the rules is a human being. The human user almost always has some previous concepts or knowledge about the domain represented by the dataset. Naturally, he/she wishes to know how the new rules compare with his/her existing knowledge. In dynamic domains where the rules may change over time, it is important to know what the changes are. These aspects of research have largely been ignored in the past. With the increasing use of machine leaming tcclmiques in practical applications such as data mining, this issue of post analysis of rules warrants greater emphasis and attention. In this paper, we propose a technique to deal with this problem. A system has been implemented to perform the post analysis of classification rules genemted by systems such as C4.5. The proposed technique is general and highly interactive. It will be particularly useful in data mining and data analysis.\"",
        "title: \"Integrating Classification and Association Rule Mining\" with abstract: \"Classification rule mining aims to discover a small set of rules in the database that forms an accurate classifier. Association rule mining finds all the rules existing in the database that satisfy some minimum support and minimum confidence constraints. For association rule mining, the target of discovery is not pre-determined, while for classification rule mining there is one and only one pre-determined target. In this paper, we propose to integrate these two mining techniques. The integration is done by focusing on mining a special subset of association rules, called class association rules (CARs). An efficient algorithm is also given for building a classifier based on the set of discovered CARs. Experimental results show that the classifier built this way is, in general, more accurate than that produced by the state-of-the-art classification system C4.5. In addition, this integration helps to solve a number of problems that exist in the current classification systems.\"",
        "1 is \"MoteTrack: a robust, decentralized approach to RF-based location tracking\", 2 is \"Efficient Data Mining for Path Traversal Patterns\".",
        "\nGiven above information, for an author who has written the paper with the title \"Feedback Approach To Design For Assembly By Evaluation Of Assembly Plan\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01105": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'An evaluation of XML indexes for structural join':",
        "title: \"Correlation-based Attribute Outlier Detection in XML\" with abstract: \"Compared to relational data models, the hierarchical structure of semi-structured data such as XML provides semantically meaningful neighbourhoods advancing data cleaning problems such as outlier detection. In this paper, we introduce the concept of correlated subspace that leverages on the hierarchical relationships between XML attributes to provide contextually informative neighbourhoods for attribute outlier detection. We also design two correlation-based attribute outlier metrics for XML, namely the xO-Measure and xQ-Measure. The effectiveness of our XML outlier detection approach is supported with experimental results.\"",
        "title: \"Current Approaches to XML Management\" with abstract: \"The Extensible Markup Language has become the standard for informationinterchange on the Web. Developed primarily as a document markup language more powerful than HTML yet less complex than SGML, XML does not require content to adhere to structural rules. XML gives a single, human-readable syntax for representing data, including data in relational format. Hence XML appeals to both the document and thedatabase communities.\"",
        "title: \"An effective approach to detect lesions in color retinal images\" with abstract: \"Diabetic-related eye diseases are the most common cause of blindness in the world. So far the most effective treatment for these eye diseases is early detection through regular screening. To lower the cost of such screenings, we employ state-of-the-art image processing techniques to automatically detect the presence of abnormalities in the retinal images obtained during the screenings. The authors focus on one of the abnormal signs: the presence of exudates/lesions in the retinal images. We propose a novel approach that combines brightness adjustment procedure with statistical classification method and local-window-based verification strategy. Experimental results indicate that we are able to achieve 100% accuracy in terms of identifying all the retinal images with exudates while maintaining a 70% accuracy in correctly classifying the truly normal retinal images as normal. This translates to a huge amount of savings in terms of the number of retinal images that need to be manually reviewed by the medical professionals each year\"",
        "title: \"RRPJ: result-rate based progressive relational join\" with abstract: \"Progressive join algorithms are join algorithms that produce results incrementally as input data is available. Because they are nonblocking, they are particularly suitable for online processing of data streams. Reference algorithms of this family are the symmetric hash join, the X-join and more recently, the rate-based progressive join (RPJ). While the symmetric hash join introduces the idea of a symmetric processing of the input streams but assumes sufficient main memory, the X-Join suggests that the processing can scale to very large amounts of data if main memory is regularly flushed to disk, and a reactive/cleanup phase is triggered for disk-resident data. The X-join flushing strategy is based on a simple largest-first strategy, where the largest partition is flushed to disk. The recently proposed RPJ predicts the main memory tuples or partitions that should be flushed to disk in order to maximize throughput by computing their probabilities to contribute to a result. In this paper, we discuss the limitations of RPJ and propose a novel extension, called Result Rate-based Progressive Join (RRPJ), which addresses these limitations. Instead of computing the probabilities from statistics over the input data, RRPJ directly observes the output (result) statistics. This not only yields a better performance, but also simplifies the generalization of the algorithm to non-relational data such as multidimensional data and hierarchical data. We empirically show that RRPJ is effective and efficient and outperforms the state-of-art RPJ. We also investigate the relevance and performance of an adaptive version of these algorithms using amortization parameters.\"",
        "title: \"A Prime Number Labeling Scheme for Dynamic Ordered XML Trees\" with abstract: \"Efficient evaluation of XML queries requires thedetermination of whether a relationship exists betweentwo elements. A number of labeling schemes have beendesigned to label the element nodes such that therelationships between nodes can be easily determinedby comparing their labels. With the increasedpopularity of XML on the web, finding a labelingscheme that is able to support order-sensitive queriesin the presence of dynamic updates becomes urgent. Inthis paper, we propose a new labeling scheme thattakes advantage of the unique property of primenumbers to meet this need. The global order of thenodes can be captured by generating simultaneouscongruence values from the prime number node labels.Theoretical analysis of the label size requirements forthe various labeling schemes is given. Experimentresults indicate that the prime number labeling schemeis compact compared to existing dynamic labelingschemes, and provides efficient support to order-sensitivequeries and updates.\"",
        "1 is \"Retinal vessel segmentation using the 2-D Gabor wavelet and supervised classification.\", 2 is \"Study On Soft Decision Based Cooperative Sensing For Cognitive Radio Networks\".",
        "\nGiven above information, for an author who has written the paper with the title \"An evaluation of XML indexes for structural join\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01106": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Integrating User Reviews and Ratings for Enhanced Personalized Searching.':",
        "title: \"Mobile Agents for Secure Electronic Commerce Transactions with Privacy Protection of the Customers\" with abstract: \"It is believed that the mobile agent technology is going to play an important role in future electronic commerce due to the characteristics of mobility and autonomy of the agents, which make it ideal for electronic commerce applications in open network environment. However, a couple of security issues need to be tackled before we can employ mobile agents in real life commercial applications. Some schemes have been proposed to solve the security problems in mobile agent paradigm, such as the undetachable signature scheme, the secure agent scheme using proxy signature and others. Most of the current work, however, mainly focus on the protection of the private key of the customer from the malicious servers, while the other related security issues in electronic commerce, say, privacy protection of the customer, are seldom considered. In this paper, we summarize the security requirements for mobile agent-based electronic transactions and propose a new secure mobile agent scheme for electronic commercial transactions with privacy protection of the customer. With a security analysis, we show that the proposed scheme satisfies all security requirements.\"",
        "title: \"Existence of Risk Strategy Equilibrium in Games Having No Pure Strategy Nash Equilibrium\" with abstract: \"Two key properties defining an intelligent agent are reactive and pro-active. Before designing an intelligent agent for any multi-agent system, we need to first understand how agents should behave and interact in that particular application, which can be done by modelling the application as a game . To analyze these games and to understand how decision-makers interact, we can use a collection of analytical tools known as Game Theory . Risk strategies is a new kind of game-theoretic strategy. Simulations in previous work have shown that agents using risk strategies are reactive as well as pro-active and thus have better performance than agents using other models or strategies in various applications. However, research on risk strategies has been focusing on formalization, application, and games having pure strategy Nash equilibrium. In this paper, we analyze a game having no pure strategy Nash equilibrium. We find that risk strategy equilibrium may exist even the game does not have pure strategy Nash equilibrium. We then summarize general conditions for the existence of risk strategy equilibrium. Simulation shows that agents using risk strategies also have better performance than agents using other existing strategies in a game having no pure strategy Nash equilibrium.\"",
        "title: \"Adaptive Soft Bid Determination in Bidding Strategies for Continuous Double Auctions\" with abstract: \"There are several bidding strategies proposed in the literature for agents in Continuous Double Auctions (CDAs). For most bidding strategies, the asks or bids determined are hard and cannot be compromised. However, for human traders, we notice that the decisions are usually soft and adaptive in different situations. Therefore, we believe that integrating softness and adaptivity into the bidding strategies can enhance the performance of agents. Experimental results confirm that when agents using different bidding strategies make adaptive and soft compromise in various situations, their performance is improved significantly in general. In order to guide agents to adopt soft asks or bids in dynamic and unknown markets, an adaptive mechanism is proposed to adjust the degree of softness of soft asks or bids according to the realtime market context. Experiments results show that agents adopting the adaptive mechanism generally outperform the corresponding agents without the adaptive mechanism.\"",
        "title: \"Weighted/prioritised compensatory aggregation\" with abstract: \"Yager et al. (1996) first introduce compensatory operators. This paper further introduces a kind of weighted compensatory operators, and a kind of prioritised compensatory operators. The difference between these similar classes of operators are identified. In addition, the paper introduces the concepts of ordered weighted/prioritised compensatory aggregation\"",
        "title: \"Automatic privacy leakage detection for massive android apps via a novel hybrid approach\" with abstract: \"Android apps frequently leak private data off the device with or without intentions. Researchers have proposed a large number of methods, for example, static and dynamic analysis methods, to pick out the apps which tend to leak private data. However, they are only able to identify part of private data leakage vulnerabilities, due to the dynamic features in codes or code coverage problem. This paper presents a novel hybrid approach that can find out more private data leakages than the existing static or dynamic methods. The approach, realized in a tool, called HybriDroid, which employs both static and dynamic analysis methods to extract the models of each apps, and then refines the behavior model to a more adequate one according to the dynamic analysis result. As a consequence, HybriDroid inherits the advantages of both static and dynamic analysis methods, which not only achieves a high code coverage, but also can deal with the dynamic features in codes. The evaluation results show that HybriDroid is effective in detecting privacy leakages for both inter- and intra-app communication. Comparing with the existing methods, it can achieve considerable improvements in data leakage detection performance with a 97.8% precision and 90% recall on the selected apps from DroidBench 3.0 test suite.\"",
        "1 is \"A clustering particle swarm optimizer for dynamic optimization\", 2 is \"JADE: adaptive differential evolution with optional external archive\".",
        "\nGiven above information, for an author who has written the paper with the title \"Integrating User Reviews and Ratings for Enhanced Personalized Searching.\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01107": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Budget-Based Self-Optimized Incentive Search in Unstructured P2P Networks':",
        "title: \"Performance evaluation of a dataflow architecture\" with abstract: \"The formulation and validation of an analytical approach for the performance evaluation of the Manchester dataflow computer is discussed. The analytical approach is based on closed queuing network models. The average parallelism of the dataflow graph being executed on the dataflow architecture is shown to be related to the population of the closed network. The model of the dataflow computer is validated by comparing the analytical results to those obtained from the prototype Manchester dataflow computer and from simulation. The bottleneck centers in the prototype machine have been identified through the model, and various architectural modifications have been investigated from performance considerations.\"",
        "title: \"Mapping molecular dynamics computations on to hypercubes\" with abstract: \"We propose an approach for partitioning an irregular application problem in computational biology called Molecular Dynamics (MD) of Macromolecules . We model the application as a task graph which we call a compact MD graph . Such a modeling allows existing mapping heuristics to be applied to this problem. We then provide a parallel algorithm for this application, by using an efficient mapping heuristic called Allocation By Recursive Mincut (ARM) to map the compact MD graph to a hypercube connected parallel computer, the nCUBE 2S. A canonical model for executing parallel computations modeled as graphs is described. Thus, we attempt to provide the missing link between the mapping research and application implementation research, and demonstrate that the execution time can be sufficiently reduced by considering formal mapping techniques, while designing parallel programs for important applications.\"",
        "title: \"A Balanced Consistency Maintenance Protocol for Structured P2P Systems\" with abstract: \"A fundamental challenge of managing mutable data replication in a Peer-to-Peer (P2P) system is how to efficiently maintain consistency under various sharing patterns with heterogeneous resource capabilities. This paper presents a framework for balanced consistency maintenance (BCoM) in structured P2P systems. Replica nodes of each object are organized into a tree for disseminating updates, and a sliding window update protocol is developed to bound the consistency. The effect of window size in response to dynamic network conditions, workload updates and resource limits is analyzed through a queueing model. This enables us to balance availability, performance and consistency strictness for various application requirements. On top of the dissemination tree, two enhancements are proposed: a fast recovery scheme to strengthen the robustness against node and link failures; and a node migration policy to remove and prevent the bottleneck for better system performance. Simulations are conducted using P2PSim to evaluate BCoM in comparison to SCOPE. The experimental results demonstrate that BCoM significantly improves the availability of SCOPE by lowering the discard rate from almost 100% to 5% with slight increase in latency.\"",
        "title: \"The Impact of Link Arbitration on Switch Performance\" with abstract: \"Switch design for interconnection networks plays an important role in the overall performance of multiprocessors and computer networks. In this paper we study the impact of one parameter in the switch design space, link arbitration. We demonstrate that link arbitration can be a determining factor in the performance of current networks. Moreover, we expect increased research focus on arbitration techniques to become a trend in the future, as switch architectures evolve towards increasing the number of virtual channels and input ports.In the context of a state-of-the-art switch design we use both synthetic workload and execution driven simulations to compare several arbitration policies. Furthermore, we devise a new arbitration method, Look-Ahead arbitration. Under heavy traffic conditions the Look-Ahead policy provides important improvements over traditional arbitration schemes without a significant increase in hardware complexity. Also, we propose a priority based policy that is capable of reducing the execution time of parallel applications. Lastly, we enhance the arbitration policies by a supplemental mechanism, virtual channel reservation, intended to alleviate the hot-spot problem.\"",
        "title: \"P2P consistency support for large-scale interactive applications\" with abstract: \"Peer-to-Peer (P2P) systems have been widely used by networked interactive applications to relieve the drawback and reduce the reliance on well-provisioned servers. A core challenge is to provide consistency maintenance for a massive number of users in a P2P manner. This requires propagating updates on time by only using the uplink bandwidth from individual users instead of relying on dedicated servers. In this paper, we present a P2P system called PPAct to provide consistency maintenance for large-scale fast-interactive applications. We use massive multi-player online games as example applications to illustrate PPAct. The design can be directly applied to other interactive applications. We adopt the Area-of-Interest (AOI) filtering method, which is proposed in prior works [1,2], to reduce bandwidth consumption of update delivery. We solve the AOI's critical problem of bandwidth shortage in hot regions by dynamically balancing the workload of each region in a distributed way. We separate the roles of view discovery from consistency maintenance by assigning players as ''region hosts'' and ''object holders.'' A region host is responsible for tracking objects and players within a region, and an object holder is responsible for sending updates about an object to interested players. Lookup queries for view discovery are processed by region hosts, while consistency maintenance of objects is taken by object holders. Separating the roles not only alleviates the workload overflow in hot regions, but also speeds up view discovery and update delivery. Another key idea is that peers contribute spare bandwidth in a fully distributed way to forwarding updates about objects of interest. Thus popular, high-demand objects will have more peers forward updates. We also present how to select capable and reliable players for region hosts and object holders. A P2P network simulator is developed to evaluate PPAct on two major types of online games: role-playing games (RPGs) and first-person shooter (FPS) games. The results demonstrate that PPAct successfully supports 10,000 players in RPGs and 1500 players in FPS games. PPAct outperforms SimMud [2] in RPGs and Donnybrook [3] in FPS games by 40% and 30% higher successful update rates respectively.\"",
        "1 is \"Mapping the Gnutella network\", 2 is \"Load balancing for parallel forwarding\".",
        "\nGiven above information, for an author who has written the paper with the title \"Budget-Based Self-Optimized Incentive Search in Unstructured P2P Networks\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01108": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Internet of Tangible Things: Workshop on Tangible Interaction with the Internet of Things.':",
        "title: \"How HCI design influences web security decisions\" with abstract: \"Even though security protocols are designed to make computer communication secure, it is widely known that there is potential for security breakdowns at the human-machine interface. This paper reports on a diary study conducted in order to investigate what people identify as security decisions that they make while using the web. The study aimed to uncover how security is perceived in the individual's context of use. From this data, themes were drawn, with a focus on addressing security goals such as confidentiality and authentication. This study is the first study investigating users' web usage focusing on their self-documented perceptions of security and the security choices they made in their own environment.\"",
        "title: \"MyWord: Supporting the Interest-based Learning of Words through a Personal Visual Dictionary.\" with abstract: \"In this research we explore how interest-based interactive technologies can support learning, particularly for children for whom receptive language and the pace of the general curriculum can be challenging. We designed and iteratively developed MyWord, a visual dictionary tablet app, which supports the exploration of words through images that represent a child's interest. The prototype was derived from a parent's concept and a one-month deployment with her child. Early findings indicate that MyWord has the potential to support collaborative image curation, browsing and discovery of interests, and writing and spelling practices. This paper bridges knowledge between competency-based learning approaches and technology design. We conclude with reflections on early use of the MyWord app and pose questions to direct future work. \\\\\"",
        "title: \"A Noticeboard in \\\"Both Worlds\\\" Unsurprising Interfaces Supporting Easy Bi-Cultural Content Publication\" with abstract: \"We describe the design of a digital noticeboard to support communication within a remote Aboriginal community whose aspiration is to live in \\\"both worlds\\\", nurturing and extending their Aboriginal culture and actively participating in Western society and economy. Three bi-cultural aspects have emerged and are presented here: the need for a bi-lingual noticeboard to span both oral and written language traditions; the tension between perfunctory information exchange and social, embodied protocols of telling in person and the different ways in which time is represented in both cultures. The design approach, developed iteratively through consultation, demonstration and testing led to an \\\"unsurprising interface\\\" aimed at maximizing use and appropriation across cultures by unifying visual, text and spoken contents in both passive and interactive displays in a modeless manner.\"",
        "title: \"Designing for Sharing in Local Communities\" with abstract: \"The Sharing Economy has brought new attention to the everyday practice of sharing. Digital tools are changing both what we can do together across neighbourhoods and how we think about sharing our time, materials and skills. It is possible to design to boost resource management, economic wellbeing and social resilience by fostering sharing practices, but do different designs speak to different priorities in design for sharing?\"",
        "title: \"Reflections on a candidate design of the user-interface for a wireless vital-signs monitor\" with abstract: \"In this paper we present a case study of our candidate design for the user-interface of a wireless vital-signs monitor. We reflect on our design of the user-interface, and relate our design experience to theories of artefact design, evaluating from this case study how the theories apply to the broader design context of design for AR. Theories of `good design' in artefact design literature do not unilaterally apply to the design for an augmented reality device. In many cases, design in AR fields requires the designer to create new cultural conventions by virtue of the fact that the designer is immersing the user in an unfamiliar environment. Thus, the designer is often unable to utilise affordances and existing cultural conventions because the functions and/or use of the object expands the environment in which affordances and cultural conventions currently have meaning.\"",
        "1 is \"Tablet-Based Activity Schedule in Mainstream Environment for Children with Autism and Children with ID.\", 2 is \"Designing Acceptable Assisted Living Services for Elderly Users\".",
        "\nGiven above information, for an author who has written the paper with the title \"Internet of Tangible Things: Workshop on Tangible Interaction with the Internet of Things.\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01109": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Scheduling of dynamic dataflow programs based on state space analysis':",
        "title: \"Actor Merging for Dataflow Process Networks\" with abstract: \"Dataflow process networks provide a versatile model of computation for specifying signal processing applications in a platform independent fashion. This attractive feature of dataflow has lately been realized in dataflow programming tools that allow synthesizing the same application specification as both fixed hardware circuits and as software for programmable processors. However, in practice, the specification granularity of the dataflow program remains an arbitrary choice of the designer. Dataflow specifications of the same application with equivalent I/O behaviour can range from a single dataflow actor to a very fine grained network composed of elementary processing operations. A very fine grained dataflow specification might result into a high performance implementation when synthesized as hardware, but might perform poorly when executed on a programmable processor. This article presents actor merging as one solution for this performance portability problem of dataflow programs. In contrast to previous work around actor merging, this article presents a methodology that can merge also dynamic dataflow actors. To support these claims, results of experiments on several processing platforms and application examples ranging from telecommunications to video compression are reported.\"",
        "title: \"Exploring MPEG HEVC decoder parallelism for the efficient porting onto many-core platforms.\" with abstract: \"MPEG High Efficient Video Coding (HEVC) is likely to emerge as the video coding standard for HD and Ultra-HD TV resolutions. The two elements that push HEVC beyond the previous standards are a higher compression efficiency of about a factor of two, and the introduction of new coding tools, tiles and wavefront that are intended to ease the largely increased encoding complexity particularly for Ultra HD resolutions such as 4K and 8K. However, for HEVC decoder implementations, the achievement of the desired performance on massive parallel platforms cannot rely on the use of such optional (not enforced by MPEG profiles) tools. This paper reports results about the intrinsic parallelism of compliant HEVC decoding algorithms obtained by analyzing a dataflow implementation written using the standard language specified in ISO/IEC 23001-4 and structured attempting to maximize the algorithmic potential parallelism. The experimental results show what is the parallelism achieved by different dataflow architectures and how it can be further combined with the parallelism achieved by relying on tiles and wavefront, whenever they would be available, for porting a compliant HEVC decoder on massive parallel many-core platforms.\"",
        "title: \"Video decoder reconfigurations and AVS extensions in the new MPEG reconfigurable video coding framework\" with abstract: \"Multimedia devices are now required to support multiple coding standards. Supporting seamlessly both interoperability between standards and flexibility for application specific optimizations is a great challenge for current video coding technology. After a brief description of the new MPEG reconfigurable video coding (RVC) framework, this paper describes possible decoder reconfigurations within this framework. The essential idea behind this framework is to reuse as most as possible the algorithms or architectures which are common to several different standards and to reconfigure video decoders in a flexible way at the coding tool level. A coding tool is an encapsulated piece of algorithm. Reconfiguration can address specific optimization objectives such as improvement in colour reproduction or higher performance at high bitrate. These simple examples show that the tool level definition of the video tool library is flexible enough to support the incremental introduction of new coding algorithms, the usage of algorithms taken from different video standards (i.e. AVS is provided in one example), and the possibility of high level reconfigurations. Thus, this paper demonstrates that the RVC framework offers a great flexibility in selecting coding tools for decoder reconfigurations to satisfy a wide variety of different applications.\"",
        "title: \"A scalable and programmable architecture for 2-D DWT decoding\" with abstract: \"The compression of still images by means of the discrete wavelet transform (DWT), adopted in the JPEG-2000 and MPEG-4 standards, is becoming more and more widespread because it yields better performance than other compression methods, such as discrete cosine transform. The demand of efficient architectures for 2-D DWT coding and decoding for a variety of different applications and embedded systems is rapidly increasing. This paper presents the implementation of a 2-D DWT decoder for Mallat-tree decomposition, suitable for low power applications, such as portable devices. The decoder design has been synthesized and validated in 0.35-\u03bcm CMOS technology. The architecture is scalable according to the desired maximum image size, the maximum DWT kernel length, and arithmetic accuracy, and it is programmable at run-time to process different image sizes and use different DWT kernels.\"",
        "title: \"Secure computing with the MPEG RVC framework\" with abstract: \"Recently, ISO/IEC standardized a dataflow-programming framework called Reconfigurable Video Coding (RVC) for the specification of video codecs. The RVC framework aims at providing the specification of a system at a high abstraction level so that the functionality (or behavior) of the system become independent of implementation details. The idea is to specify a system so that only intrinsic features of the algorithms are explicitly expressed, whereas implementation choices can then be made only once specific target platforms have been chosen. With this system design approach, one abstract design can be used to automatically create implementations towards multiple target platforms. In this paper, we report our investigations on applying the methodology standardized by the MPEG RVC framework to develop secure computing in the domains of cryptography and multimedia security, leading to the conclusion that the RVC framework can successfully be applied as a general-purpose framework to other fields beyond multimedia coding. This paper also highlights the challenges we faced in conducting our study, and how our study helped the RVC and the secure computing communities benefited from each other. Our investigations started with the development of a Crypto Tools Library (CTL) based on RVC, which covers a number of widely used ciphers and cryptographic hash functions such as AES, Triple DES, ARC4 and SHA-2. Performance benchmarking results on the RVC-based AES and SHA-2 implementations in both C and Java revealed that the automatically generated implementations can achieve a comparable performance to some manually written reference implementations. We also demonstrated that the RVC framework can easily produce implementations with multi-core support without any change to the RVC code. A security protocol for mutual authentication was also implemented to demonstrate how one can build heterogeneous systems easily with RVC. By combining CTL with Video Tool Library (a standard library defined by the RVC standard), a non-standard RVC-based H.264/AVC encoder and a non-standard RVC-based JPEG codec, we further demonstrated the benefits of using RVC to develop different kinds of multimedia security applications, which include joint multimedia encryption-compression schemes, digital watermarking and image steganography in JPEG compressed domain. Our study has shown that RVC can be used as a general-purpose implementation-independent development framework for diverse data-driven applications with different complexities.\"",
        "1 is \"Semantic Space: An Infrastructure for Smart Spaces\", 2 is \"The Semantics of Simple Language for Parallel Programming.\".",
        "\nGiven above information, for an author who has written the paper with the title \"Scheduling of dynamic dataflow programs based on state space analysis\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01110": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Artificial intelligence in 2027.':",
        "title: \"Scan Matching Without Odometry Information\" with abstract: \"We present an algorithm for merging two partial maps obtained with a laser range scanner into a single map. The most unique original aspect of our algorithm is that it does not require any information on the position where the scans were collected but uses only geometrical features of the scans.\"",
        "title: \"No Fear: University Of Minnesota Robotics Day Camp Introduces Local Youth To Hands-On Technologies\" with abstract: \"Women and minorities are underrepresented in the IT field at the high school, university, and industry levels. Efforts to address this imbalance are often too late to solve underlying problems such as perceived ineptitude and actual inexperience. By designing and hosting a program for these underrepresented students in the middle grades, the Center for Distributed Robotics at the University of Minnesota hopes to establish a successful annual robotics day camp which will inspire both women and minorities to pursue careers in technology. Detailed accounts of the goals and methodology are provided. Initial survey results reveal a very positive response from the campers as well as strengths and weaknesses which will be useful in designing or refining similar camps.\"",
        "title: \"Real-Time Tactical and Strategic Sales Management for Intelligent Agents Guided by Economic Regimes\" with abstract: \"Many enterprises that participate in dynamic markets need to make product pricing and inventory resource utilization decisions in real time. We describe a family of statistical models that addresses these needs by combining characterization of the economic environment with the ability to predict future economic conditions to make tactical (short-term) decisions, such as product pricing, and strategic (long-term) decisions, such as level of finished goods inventories. Our models characterize economic conditions, called economic regimes, in the form of recurrent statistical patterns that have clear qualitative interpretations. We show how these models can be used to predict prices, price trends, and the probability of receiving a customer order at a given price. These \u201cregime\u201d models are developed using statistical analysis of historical data and are used in real time to characterize observed market conditions and predict the evolution of market conditions over multiple time scales. We evaluate our models using a testbed derived from the Trading Agent Competition for Supply Chain Management, a supply chain environment characterized by competitive procurement, sales markets, and dynamic pricing. We show how regime models can be used to inform both short-term pricing decisions and long-term resource allocation decisions. Results show that our method outperforms more traditional short-and long-term predictive modeling approaches.\"",
        "title: \"Performance of a distributed robotic system using shared communications channels\" with abstract: \"We have designed and built a set of miniature robots called Scouts and have developed a distributed software system to control them. This paper addresses the fundamental choices we made in the design of the control software, describes experimental results in a surveillance task, and analyzes the factors that affect robot performance. Space and power limitations on the Scouts severely restrict the computational power of their on-board computers, requiring a proxy-processing scheme in which the robots depend on remote computers for their computing needs. While this allows the robots to be autonomous, the fact that robots' behaviors are executed remotely introduces an additional complication\u2014sensor data and motion commands have to be exchanged using wireless commu- nications channels. Communications channels cannot always be shared, thus requiring the robots to obtain exclusive access to them. We present experimental results on a surveillance task in which multiple robots patrol an area and watch for motion. We discuss how the limited communications bandwidth affects robot perfor- mance in accomplishing the task, and analyze how performance depends on the number of robots that share the bandwidth.\"",
        "title: \"Label Correction and Event Detection for Electricity Disaggregation.\" with abstract: \"Electricity disaggregation focuses on identifying individual appliances from one or more aggregate signals. By reporting detailed appliance usage to consumers, disaggregation has the potential to significantly reduce electrical waste in residential and commercial sectors. However, application of existing methods is limited by two critical shortcomings. First, supervised learning methods implicitly assume error-free labels in training data, an unrealistic expectation for imperfectly-labeled consumer data. Second, supervised and unsupervised learning methods require parameters to be tuned to individual appliances and/or datasets, limiting widespread application. To address these limitations, this paper introduces the implementation of Bayesian changepoint detection (BCD) with necessary adaptations to electricity disaggregation. We introduce an algorithm to effectively apply BCD to automatically correct labels. We then apply BCD to event detection to identify transitions between appliances' on and off states. Performance is evaluated using 3 publicly available datasets containing over 250 appliances across 11 houses. Results show both BCD applications are competitive and in some cases outperform existing state-of-the-art methods without the need for parameter tuning, advancing disaggregation towards widespread, real-world deployment.\"",
        "1 is \"An asynchronous complete method for distributed constraint optimization\", 2 is \"Coordination specification in multi-agent systems: from requirements to architecture with the Tropos methodology\".",
        "\nGiven above information, for an author who has written the paper with the title \"Artificial intelligence in 2027.\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01111": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Document Categorization and Query Generation on the World Wide Web Using WebACE':",
        "title: \"A Recommendation Model Based on Latent Principal Factors in Web Navigation Data\" with abstract: \"Discovery of factors that lead to common navigational patterns can help in improving online infor- mation presentation as well as in providing personalized content to users. It is, therefore, necessary to develop techniques that can automatically characterize the users' underlying navigational objectives and to discover the hidden semantic relationships among users as well as between users and Web objects. Typical approaches to Web usage mining, such as clustering of user sessions, can discover usage patterns directly, but cannot identify the latent factors, intrinsic in users' navigational behavior, that lead to such patterns. In this paper, we propose an approach based on a latent variable model, called Iterative Prin- cipal Factor Analysis, to discover such hidden factors in Web usage data. The hidden factors are then used to create aggregate models of common user profiles which are, in turn, used to provide dynamic recommendations to users. Our experimental results, performed on real Web usage data, verify that the proposed principal factor approach results in better predictive user models, when compared to more traditional approaches such as clustering and principal component analysis.\"",
        "title: \"Recommendation by Example in Social Annotation Systems.\" with abstract: \"Recommendation by example is common in contemporary Internet applications providing resources similar to a user-selected example. In this paper this task is considered as a function available within a social annotation system offering new ways to model both users and resources. Using three real-world datasets we motivate several conclusions. First, a personalized approach outperforms non-personalized approaches suggesting that users perceive the similarity between resources differently. Second, the manner in which users interact with social annotation systems vary producing datasets with variable characteristics and requiring different recommendation strategies to best satisfy their needs. Third, a hybrid recommender constructed from several component recommenders can produce superior results by exploiting multiple dimensions of the data. The hybrid remains powerful, flexible and extensible despite the underlying characteristics of the data.\"",
        "title: \"Partitioning-based clustering for Web document categorization\" with abstract: \"Clustering techniques have been used by many intelligent software agents in order to retrieve, filter, and categorize documents available on the World Wide Web. Clustering is also useful in extracting salient features of related Web documents to automatically formulate queries and search for other similar documents on the Web. Traditional clustering algorithms either use a priori knowledge of document structures to define a distance or similarity among these documents, or use probabilistic techniques such as Bayesian classification. Many of these traditional algorithms, however, falter when the dimensionality of the feature space becomes high relative to the size of the document space. In this paper, we introduce two new clustering algorithms that can effectively cluster documents, even in the presence of a very high dimensional feature space. These clustering techniques, which are based on generalizations of graph partitioning, do not require pre-specified ad hoc distance functions, and are capable of automatically discovering document similarities or associations. We conduct several experiments on real Web data using various feature selection heuristics, and compare our clustering schemes to standard distance-based techniques, such as hierarchical agglomeration clustering , and Bayesian classification methods, such as AutoClass .\"",
        "title: \"Ontological User Profiles for Representing Context in Web Search\" with abstract: \"The goal of Web search personalization is to tailor search results to a particular user based on that user's interests and preferences. We present an approach to personalized search that involves building models of user context as ontological profiles by assigning implicitly derived interest scores to existing concepts in a domain ontology. Our experiments show that re-ranking the search results based on the interest scores and the semantic evidence in an ontological user profile is effective in presenting the most relevant results to the user.\"",
        "title: \"Effective personalization based on association rule discovery from web usage data\" with abstract: \"To engage visitors to a Web site at a very early stage (i.e., before registration or authentication), personalization tools must rely primarily on clickstream data captured in Web server logs. The lack of explicit user ratings as well as the sparse nature and the large volume of data in such a setting poses serious challenges to standard collaborative filtering techniques in terms of scalability and performance. Web usage mining techniques such as clustering that rely on offline pattern discovery from user transactions can be used to improve the scalability of collaborative filtering, however, this is often at the cost of reduced recommendation accuracy. In this paper we propose effective and scalable techniques for Web personalization based on association rule discovery from usage data. Through detailed experimental evaluation on real usage data, we show that the proposed methodology can achieve better recommendation effectiveness, while maintaining a computational advantage over direct approaches to collaborative filtering such as the k-nearest-neighbor strategy.\"",
        "1 is \"Modeling temporal relationships in large scale clinical associations.\", 2 is \"Conceptual User Tracking\".",
        "\nGiven above information, for an author who has written the paper with the title \"Document Categorization and Query Generation on the World Wide Web Using WebACE\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01112": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Guest Editorial Introducing Automation and Engineering for Ambient Intelligence':",
        "title: \"The Power Is in Your Hands: 3D Analysis of Hand Gestures in Naturalistic Video\" with abstract: \"We study natural human activity under difficult settings of cluttered background, volatile illumination, and frequent occlusion. To that end, a two-stage method for hand and hand-object interaction detection is developed. First, activity proposals are generated from multiple sub-regions in the scene. Then, these are integrated using a second-stage classifier. We study a set of descriptors for detection and activity recognition in terms of performance and speed. With the overarching goal of reducing 'lab setting bias', a case study is introduced with a publicly available annotated RGB and depth dataset. The dataset was captured using a Kinect under real-world driving settings. The approach is motivated by studying actions-as well as semantic elements in the scene and the driver's interaction with them-which may be used to infer driver inattentiveness. The proposed framework significantly outperforms a state-of-the-art baseline on our dataset for hand detection.\"",
        "title: \"Towards Semantic Understanding of Surrounding Vehicular Maneuvers: A Panoramic Vision-Based Framework for Real-World Highway Studies.\" with abstract: \"This paper proposes the use of multiple low-cost visual sensors to obtain a surround view of the ego-vehicle for semantic understanding. A multi-perspective view will assist the analysis of naturalistic driving studies (NDS), by automating the task of data reduction of the observed sequences into events. A user-centric vision-based framework is presented using a vehicle detector and tracker in each separate perspective. Multi-perspective trajectories are estimated and analyzed to extract 14 different events, including potential dangerous behaviors such as overtakes and cut-ins. The system is tested on ten sequences of real-world data collected on U.S. highways. The results show the potential use of multiple low-cost visual sensors for semantic understanding around the ego-vehicle.\"",
        "title: \"Audio-visual data association for face expression analysis\" with abstract: \"We present a novel facial expression recognition framework using audio-visual information analysis. In particular, we design a single good image representation of the image sequence by weighted sum of registered face images where the weights are derived using auditory features. We use a still image based technique for the expression recognition task. We performed experiments using eNTERFACE'05 audio-visual emotional database. The analysis shows that our framework can improve the recognition performance while significantly reducing the computational cost by avoiding redundant or insignificant frame processing by incorporating auditory information.\"",
        "title: \"How would surround vehicles move? A Unified Framework for Maneuver Classification and Motion Prediction.\" with abstract: \"Reliable prediction of surround vehicle motion is a critical requirement for path planning for autonomous vehicles. In this paper, we propose a unified framework for surround vehicle maneuver classification and motion prediction that exploits multiple cues, namely, the estimated motion of vehicles, an understanding of typical motion patterns of freeway traffic and intervehicle interaction. We repo...\"",
        "title: \"Omnidirectional image-based modeling: three approaches to approximated plenoptic representations\" with abstract: \"In this paper we present a set of novel methods for image-based modeling using omnidirectional vision sensors. The basic idea is to directly and efficiently acquire plenoptic representations by using omnidirectional vision sensors. The three methods, in order of increasing complexity, are direct memorization, discrete interpolation, and smooth interpolation . Results of these methods are compared visually with ground-truth images taken from a standard camera walking along the same path. The experimental results demonstrate that our methods are successful at generating high-quality virtual images. In particular, the smooth interpolation technique approximates the plenoptic function most closely. A comparative analysis of the computational costs associated with the three methods is also presented.\"",
        "1 is \"Towards Retro-Projected Robot Faces: An Alternative To Mechatronic And Android Faces\", 2 is \"Lie algebra approach for tracking and 3D motion estimation using monocular vision\".",
        "\nGiven above information, for an author who has written the paper with the title \"Guest Editorial Introducing Automation and Engineering for Ambient Intelligence\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01113": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Accelerating Key Establishment Protocols for Mobile Communication':",
        "title: \"Tamper resistant software by integrity-based encryption\" with abstract: \"There are many situations in which it is desirable to protect a piece of software from illegitimate tampering once it gets distributed to the users. Protecting the software code means some level of assurance that the program will execute as expected even if it encounters the illegitimated modifications. We provide the method of protecting software from unauthorized modification. One important technique is an integrity-based encryption, by which a program, while running, checks itself to verify that it has not been modified and conceals some privacy sensitive parts of program.\"",
        "title: \"Indirect Star-Type Networks for Large Multiprocessor Systems\" with abstract: \"The authors propose three indirect star-type networks, the indirect star networks I and II and the star-delta network, and investigate their properties. An indirect star-type network is obtained by unfolding the star graph. The star-delta network is obtained through an unfolding scheme based on the recursive property of the star graph, and has n-1 switching stages. The star-delta network has the advantage of being controlled by the destination tag routing scheme. The indirect star-type networks are to the star graph as the indirect cube-type networks are to the n-cube. The authors analyze the performance of the indirect star-type networks under uniform traffic to investigate their potential as an alternative to the indirect cube-type networks for the future high-performance large multiprocessor systems.\"",
        "title: \"Efficient Packet Routing in Highly Mobile Wireless Networks\" with abstract: \"Traditional mobile ad-hoc network (MANET) and delay tolerant network (DTN) routing schemes work properly only under the limited assumptions. MANET routing schemes are designed for densely populated networks while DTN routing schemes were developed for dealing with intermittently connected topologies. However, in a real environment, the assumption cannot be hold: the density of nodes constantly changes and a network can be separated due to node mobility even when it is composed of large number of nodes. We propose a routing scheme for maximizing the delivery rate in a network with varying density. In the proposed scheme, data packets are used to search an available path while they are delivered by DTN-based routing. If a path exists, subsequent packets are relayed along the path. Otherwise, packets are delivered continuously using DTN-based routing. Using simulation we show that the proposed scheme achieves high throughput and a low overhead in a dense part of the network, while it outperforms existing DTN routing schemes in an intermittently connected situation.\"",
        "title: \"Parallelized Scalar Multiplication on Elliptic Curves Defined over Optimal Extension Field\" with abstract: \"In this paper, we propose three algorithms to perform scalar multiplication on elliptic curves defined over higher characteristic finite fields such as the OEF (Optimal Ex- tension Field). First, we propose an efficient scalar multi- plication method in which the Frobenius expansion is used on an elliptic curve defined over OEF. Second, we pro- pose a new finite field multiplication algorithm. Third, we propose a particular polynomial squaring algorithm. We show that the proposed algorithms, when used together, accelerate the scalar multiplication on elliptic curves by two-fold.\"",
        "title: \"An improved algorithm for protocol validation by extended circular exploration\" with abstract: \"In this paper, to improve the efficiency of state exploration and to relieve the problem of state explosion, we propose another method of state exploration called the extended circular exploration (ECE) , which does not need to explore all the reachable global states and can be applied to N ( N \u22652)-party protocol with alternative routes, i.e. it is applicable to the protocol in which adaptive routing mechanism can be performed. Using this ECE, we can eliminate the topology restriction of the circular exploration (CE) through extending the notion of circular exploration. We explore only those global states which are reachable, provided that the participant processes of any group of transitions proceed at the same speed, and that they can be formed as a cycle. The state space thus explored is not exhaustive. The algorithm presented can detect deadlock error and unspecified reception error. It requires storage space and/or execution time much less than those of the conventional perturbation method. It can be viewed as a solution of the N -process collision and the interference mechanism.\"",
        "1 is \"Efficient Algorithms for Elliptic Curve Cryptosystems\", 2 is \"Buddy systems\".",
        "\nGiven above information, for an author who has written the paper with the title \"Accelerating Key Establishment Protocols for Mobile Communication\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01114": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Optimizing Spread of Influence in Weighted Social Networks via Partial Incentives.':",
        "title: \"A Layered Architecture for Open Data: design, implementation and experiences.\" with abstract: \"In order to increase transparency, Public Administrations (PAs) have their own portals to publish Open Data, resulting in more openness, reduced corruption and improved services. Open Data (OD) initiatives are achieving less citizensu0027 engagement than expected. Hence, the need to find other ways and services to obtain more engagement and extract value from OD. This paper introduces an architectural model to design software platforms with the objective to increase public value through collective participation of citizens, public administrations and key stakeholders. The architectural model originates from the Data-Information-Knowledge hierarchy, where greater value is at the top of the pyramid, in information and knowledge gathered from data. Thus, the architectural model adds the collaborative and presentation layers to the classical data layer; citizens, public administrations, and stakeholders form groups of interests to understand, reason and interpret Open Data to gather information and generate knowledge that will be communicated to the general audience over Internet, thus, increasing public value. The paper describes three platform instances: the ROUTE-TO-PA ecosystem, the data.world, and DKAN, which functionalities can be mapped onto the architectural model proposed in the paper. Furthermore, the paper describes HETOR, a concreate experience of exploitation of the architectural model and the ROUTE-TO-PA ecosystem with groups of students and associations of citizens, who collaborated together to ultimately generate new knowledge for Cultural Heritage to be communicated over Internet through blog posts.\"",
        "title: \"Area-maximizing schedules for series-parallel DAGs\" with abstract: \"Earlier work introduced a new optimization goal for DAG schedules: the \"AREA\" of the schedule. AREA-maximizing schedules are intended for computational environments--such as Internet-based computing and massively multicore computers--that benefit from DAG-schedules that produce executioneligible tasks as fast as possible. The earlier study of AREA-maximizing schedules showed how to craft such schedules efficiently for DAGs that have the structure of trees and other, less well-known, families of DAGs. The current paper extends the earlier work by showing how to efficiently craft AREA-maximizing schedules for series-parallel DAGs, a family that arises, e.g., in multi-threaded computations. The tools that produce the schedules for series-parallel DAGs promise to apply also to other large families of computationally significant DAGs.\"",
        "title: \"Brief Announcement: Active Information Spread in Networks.\" with abstract: \"Identifying the most influential spreaders is an important issue for the study of the dynamics of information diffusion in complex networks. In this paper we analyze the following spreading model. Initially, a few nodes know a piece of information and are active spreaders of it. At subsequent rounds, spreaders communicate the information to their neighbors. Upon receiving the information, a node becomes aware of it but does not necessarily become a spreader; it starts spreading only if it gets the information from a sufficiently large number of its neighbors. We study the problem of choosing a small set of initial spreaders so as to maximize the final number of nodes that become aware of the information.\"",
        "title: \"Degree-Optimal Routing for P2P Systems\" with abstract: \"We define a family of Distributed Hash Table systems whose aim is\n\tto combine the routing efficiency of randomized networks e.g. optimal\n\taverage path length O(log^2 n \\over delta log delta) with delta degree\n\t\n\t with the programmability and startup efficiency of a uniform overlay\n\tthat is, a deterministic system in which the overlay network is transitive\n\tand greedy routing is optimal. It is known that \\omega(log n) is\n\ta lower bound on the average path length for uniform overlays with\n\t=(log n) degree (Xu et al., IEEE J. Sel. Areas Commun. 22(1), 151\ufffd163,\n\t2004). \n\t\n\tOur work is inspired by neighbor-of-neighbor (NoN) routing, a recently\n\tintroduced variation of greedy routing that allows us to achieve\n\toptimal average path length in randomized networks. The advantage\n\tof our proposal is that of allowing the NoN technique to be implemented\n\twithout adding any overhead to the corresponding deterministic network.\n\t\n\tWe propose a family of networks parameterized with a positive integer\n\tc which measures the amount of randomness that is used. By varying\n\tthe value c, the system goes from the deterministic case (c=1) to\n\tan \ufffdalmost uniform\ufffd system. Increasing c to relatively low values\n\tallows for routing with asymptotically optimal average path length\n\twhile retaining most of the advantages of a uniform system, such\n\tas easy programmability and quick bootstrap of the nodes entering\n\tthe system.\n\t\n\tWe also provide a matching lower bound for the average path length\n\tof the routing schemes for any c.\"",
        "title: \"On Evaluating Graph Partitioning Algorithms for Distributed Agent Based Models on Networks.\" with abstract: \"Graph Partitioning is a key challenge problem with application in many scientific and technological fields. The problem is very well studied with a rich literature and is known to be NP-hard. Several heuristic solutions, which follow diverse approaches, have been proposed, they are based on different initial assumptions that make them difficult to compare. An analytical comparison was performed based on an Implementation Challenge [3], however being a multi-objective problem (two opposing goals are for instance load balancing and edge-cut size), the results are difficult to compare and it is hard to foresee what can be the impact of one solution, instead of another, in a real scenario. In this paper we analyze the problem in a real context: the development of a distributed agent-based simulation model on a network field (which for instance can model social interactions). We present an extensive evaluation of the most efficient and effective solutions for the balanced k-way partitioning problem. We evaluate several strategies both analytically and on real distributed simulation settings (D-Mason). Results show that, a good partitioning strategy strongly influences the performances of the distributed simulation environment. Moreover, we show that there is a strong correlation between the edge-cut size and the real performances. Analyzing the results in details we were also able to discover the parameters that need to be optimized for best performances on networks in ABMs.\"",
        "1 is \"Path coloring on the mesh\", 2 is \"Ray tracing on programmable graphics hardware\".",
        "\nGiven above information, for an author who has written the paper with the title \"Optimizing Spread of Influence in Weighted Social Networks via Partial Incentives.\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01115": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Hyperspectral and LiDAR Data Fusion: Outcome of the 2013 GRSS Data Fusion Contest':",
        "title: \"Utilization of Local and Global Hyperspectral Features via Wavelet Packets and Multiclassifiers for Robust Target Recognition\" with abstract: \"In this study, the authors investigate the combination of the wavelet packet decomposition (WPD) and multiclassifiers and decision fusion (MCDF) for a robust hyperspectral classification system. The authors investigate the use of the WPD multiresolution feature grouping and selection, forming groups of local and global spectral features, where each group is input to a classifier, resulting in local and global classifications. Then the labels are fused to form one class label. The classification system was applied to hyperspectral data for an agricultural application, namely the detection of different soybean rust infestation levels. The system was compared to current state-of-the-art hyperspectral analysis techniques to determine its comparative efficacy as compared to more conventional approaches, such as stepwise-linear discriminant analysis (LDA) or discriminant analysis feature extraction (DAFE) and current state-of-the art approaches, like spectral-domain multiclassifiers and decision fusion (MCDF). The proposed system had a classification accuracy which was approximately 40 % higher than the SLDA approach and approximately 15 % higher than MCDF.\"",
        "title: \"Manifold-Learning-Based Feature Extraction for Classification of Hyperspectral Data: A Review of Advances in Manifold Learning\" with abstract: \"Advances in hyperspectral sensing provide new capability for characterizing spectral signatures in a wide range of physical and biological systems, while inspiring new methods for extracting information from these data. HSI data often lie on sparse, nonlinear manifolds whose geometric and topological structures can be exploited via manifold-learning techniques. In this article, we focused on demonstrating the opportunities provided by manifold learning for classification of remotely sensed data. However, limitations and opportunities remain both for research and applications. Although these methods have been demonstrated to mitigate the impact of physical effects that affect electromagnetic energy traversing the atmosphere and reflecting from a target, nonlinearities are not always exhibited in the data, particularly at lower spatial resolutions, so users should always evaluate the inherent nonlinearity in the data. Manifold learning is data driven, and as such, results are strongly dependent on the characteristics of the data, and one method will not consistently provide the best results. Nonlinear manifold-learning methods require parameter tuning, although experimental results are typically stable over a range of values, and have higher computational overhead than linear methods, which is particularly relevant for large-scale remote sensing data sets. Opportunities for advancing manifold learning also exist for analysis of hyperspectral and multisource remotely sensed data. Manifolds are assumed to be inherently smooth, an assumption that some data sets may violate, and data often contain classes whose spectra are distinctly different, resulting in multiple manifolds or submanifolds that cannot be readily integrated with a single manifold representation. Developing appropriate characterizations that exploit the unique characteristics of these submanifolds for a particular data set is an open research problem for which hierarchical manifold structures appear to h- ve merit. To date, most work in manifold learning has focused on feature extraction from single images, assuming stationarity across the scene. Research is also needed in joint exploitation of global and local embedding methods in dynamic, multitemporal environments and integration with semisupervised and active learning.\"",
        "title: \"Hyperspectral image classification based on Dirichlet Process mixture models\" with abstract: \"In this work, we propose a new density estimation method for hyperspectral image data based on Dirichlet Process Gaussian mixture models (also known as infinite Gaussian mixture models - IGMMs), which successfully captures the complex multi-modal (potentially non-Gaussian) statistical structure of hyperspectral data. The mixture model we get from this will then be applied to the classification problem. This IGMM based approach is a non-parametric Bayesian method helping circumvent the problem of model selection, which is unavoidable and often difficult when employing traditional parametric Gaussian mixture models (GMM). Inference model based on Gibbs sampling employed during the inference of model parameters. As a preprocessing step, we use Local Fisher's Discriminant Analysis (LFDA) for dimension reduction since we expect it to preserve the multi-modal non-Gaussian structure of the hyperspectral data, which will benefit much in the aspect of computation cost. We compared our proposed IGMM based classification method to the existing state-of-the-art classification methods using popular hyperspectral imagery datasets. The results of our experiments show that the proposed LFDA-IGMM method and GMM method have almost the same performance (sometimes outperforming LFDA-GMM), and they outperform the other commonly used classification approaches when there is a sufficient number of training samples.\"",
        "title: \"A Machine Learning Framework for Detecting Landslides on Earthen Levees Using Spaceborne SAR Imagery\" with abstract: \"Earthen levees have a significant role in protecting large areas of inhabited and cultivated land in the United States from flooding. Failure of the levees can result in loss of life and property. Slough slides are among the problems which can lead to complete levee failure during a high water event. In this paper, we develop a method to detect such slides using X-band synthetic aperture radar (SAR) data. Our proposed methodology includes: 1) radiometric normalization of the TerraSAR image using high-resolution digital elevation map (DEM) data; 2) extraction of features including backscatter and texture features from the levee; 3) a feature selection method based on minimum redundancy maximum relevance (mRMR); and 4) training a support vector machine (SVM) classifier and testing on the area of interest. To validate the proposed methodology, ground-truth data are collected from slides and healthy areas of the levee. The study area is part of the levee system along the lower Mississippi River in the United States. The output classes are healthy and slide areas of the levee. The results show the average classification accuracies of approximately 0.92 and Cohen\u2019s kappa measures of 0.85 for both healthy and slide pixels using ten optimal features selected by mRMR with a sigmoid SVM. A comparison of the SVM performance to the maximum likelihood (ML) and back propagation neural network (BPNN) shows that the average accuracy of the SVM is superior to that of the BPNN and ML classifiers.\"",
        "title: \"Segmented Mixture-of-Gaussian Classification for Hyperspectral Image Analysis\" with abstract: \"The same high dimensionality of hyperspectral imagery that facilitates detection of subtle differences in spectral response due to differing chemical composition also hinders the deployment of traditional statistical pattern-classification procedures, particularly when relatively few training samples are available. Traditional approaches to addressing this issue, which typically employ dimensionality reduction based on either projection or feature selection, are at best suboptimal for hyperspectral classification tasks. A divide-and-conquer algorithm is proposed to exploit the high correlation between successive spectral bands and the resulting block-diagonal correlation structure to partition the hyperspectral space into approximately independent subspaces. Subsequently, dimensionality reduction based on a graph-theoretic locality-preserving discriminant analysis is combined with classification driven by Gaussian mixture models independently in each subspace. The locality-preserving discriminant analysis preserves the potentially multimodal statistical structure of the data, which the Gaussian mixture model classifier learns in the reduced-dimensional subspace. Experimental results demonstrate that the proposed system significantly outperforms traditional classification approaches, even when few training samples are employed.\"",
        "1 is \"Structure-constrained low-rank representation.\", 2 is \"Sparsity preserving projections with applications to face recognition\".",
        "\nGiven above information, for an author who has written the paper with the title \"Hyperspectral and LiDAR Data Fusion: Outcome of the 2013 GRSS Data Fusion Contest\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01116": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Derandomizing Random Walks in Undirected Graphs Using Locally Fair Exploration Strategies':",
        "title: \"Taking advantage of symmetries: Gathering of many asynchronous oblivious robots on a ring\" with abstract: \"One of the recently considered models of robot-based computing makes use of identical, memoryless mobile units placed in nodes of an anonymous graph. The robots operate in Look-Compute-Move cycles; in one cycle, a robot takes a snapshot of the current configuration (Look), takes a decision whether to stay idle or to move to one of the nodes adjacent to its current position (Compute), and in the latter case makes an instantaneous move to this neighbor (Move). Cycles are performed asynchronously for each robot. In such a restricted scenario, we study the influence of symmetries of the robot configuration on the feasibility of certain computational tasks. More precisely, we deal with the problem of gathering all robots at one node of the graph, and propose a solution based on a symmetry-preserving strategy. When the considered graph is an undirected ring and the number of robots is sufficiently large (more than 18), such an approach is proved to solve the problem for all starting situations, as long as gathering is feasible. In this way we also close the open problem of characterizing symmetric situations on the ring which admit a gathering [R. Klasing, E. Markou, A. Pelc: Gathering asynchronous oblivious mobile robots in a ring, Theoret. Comput. Sci. 390 (1) (2008) 27-39]. The proposed symmetry-preserving approach, which is complementary to symmetry-breaking techniques found in related work, appears to be new and may have further applications in robot-based computing.\"",
        "title: \"On Convergence and Threshold Properties of Discrete Lotka-Volterra Population Protocols.\" with abstract: \"In this work we focus on a natural class of population protocols whose dynamics are modeled by the discrete version of LotkaVolterra equations with no linear term. In such protocols, when an agent a of type (species) i interacts with an agent b of type (species) j with a as the initiator, then b's type becomes i with probability P-ij. In such an interaction, we think of a as the predator, b as the prey, and the type of the prey is either converted to that of the predator or stays as is. Such protocols capture the dynamics of some opinion spreading models and generalize the well-known Rock-Paper-Scissors discrete dynamics. We consider the pairwise interactions among agents that are scheduled uniformly at random. We start by considering the convergence time and show that any Lotka-Volterra-type protocol on an n-agent population converges to some absorbing state in time polynomial in n, w.h.p., when any pair of agents is allowed to interact. By contrast, when the interaction graph is a star, there exist protocols of the considered type, such as Rock-PaperScissors, which require exponential time to converge. We then study threshold effects exhibited by Lotka-Volterra-type protocols with 3 and more species under interactions between any pair of agents. We present a simple 4-type protocol in which the probability difference of reaching the two possible absorbing states is strongly amplified by the ratio of the initial populations of the two other types, which are transient, but \"control\" convergence. We then prove that the Rock-Paper-Scissors protocol reaches each of its three possible absorbing states with almost equal probability, starting from any configuration satisfying some sub-linear lower bound on the initial size of each species. That is, Rock-Paper-Scissors is a realization of a \"coin-flip consensus\" in a distributed system. Some of our techniques may be of independent value.\"",
        "title: \"Cooperative mobile guards in grids\" with abstract: \"A grid P is a connected union of vertical and horizontal segments. A mobile guard is a guard which is allowed to move along a grid segment, thus a point x is seen by a mobile guard g if either x is on the same segment as g or x is on a grid segment crossing g. A set of mobile guards is weakly cooperative if at any point on its patrol, every guard can be seen by at least one other guard. In this paper we discuss the classes of polygon-bounded grids and simple grids for which we propose a quadratic time algorithm for solving the problem of finding the minimum weakly cooperative guard set (MinWCMG). We also provide an O(nlogn) time algorithm for the MinWCMG problem in horizontally or vertically unobstructed grids. Next, we investigate complete rectangular grids with obstacles. We show that as long as both dimensions of a grid are larger than the number of obstacles k, k+2 weakly cooperative mobile guards always suffice to cover the grid. Finally, we prove that the MinWCMG problem is NP-hard even for grids in which every segment crosses at most three other segments. Consequently, the minimum k-periscope guard problem for 2D grids is NP-hard as well, and this answers the question posed by Gewali and Ntafos [L.P. Gewali, S. Ntafos, Covering grids and orthogonal polygons with periscope guards, Computational Geometry: Theory and Applications 2 (1993) 309-334].\"",
        "title: \"An Efficient Algorithm for the Longest Tandem Scattered Subsequence Problem\" with abstract: \"The paper deals with the problem of finding a tandem scattered subsequence of maximum length (LTS) for a given character sequence. A sequence is referred to as tandem if it can be split into two identical sequences. An efficient algorithm for the LTS problem is presented and is shown to have O(n(2)) computational complexity and linear memory complexity with respect to the length n of the analysed sequence: A conjecture is, put forward and discussed, stating that the complexity of the given algorithm may not be easily improved. Finally, the potential application of the solution to the LTS problem in approximate tandem substring matching in DNA sequences is discussed.\"",
        "title: \"When Patrolmen Become Corrupted: Monitoring a Graph Using Faulty Mobile Robots.\" with abstract: \"A team of  mobile robots is deployed on a weighted graph whose edge weights represent distances. The robots move perpetually along the domain, represented by all points belonging to the graph edges, without exceeding their maximum speed. The robots need to patrol the graph by regularly visiting all points of the domain. In this paper, we consider a team of robots (patrolmen), at most  of which may be unreliable, i.e., they fail to comply with their patrolling duties. What algorithm should be followed so as to minimize the maximum time between successive visits of every edge point by a reliable patrolman? The corresponding measure of efficiency of patrolling called  has been widely accepted in the robotics literature. We extend it to the case of untrusted patrolmen; we denote by  the maximum time that a point of the domain may remain unvisited by reliable patrolmen. The objective is to find patrolling strategies minimizing . We investigate this problem for various classes of graphs. We design optimal algorithms for line segments, which turn out to be surprisingly different from strategies for related patrolling problems proposed in the literature. We then use these results to study general graphs. For Eulerian graphs , we give an optimal patrolling strategy with idleness , where || is the sum of the lengths of the edges of . Further, we show the hardness of the problem of computing the idle time for three robots, at most one of which is faulty, by reduction from 3-edge-coloring of cubic graphs\u2014a known NP-hard problem. A byproduct of our proof is the investigation of classes of graphs minimizing idle time (with respect to the total length of edges); an example of such a class is known in the literature under the name of Kotzig graphs.\"",
        "1 is \"Computational Complexity of the Distance Constrained Labeling Problem for Trees (Extended Abstract)\", 2 is \"Parallel Processing with the Perfect Shuffle\".",
        "\nGiven above information, for an author who has written the paper with the title \"Derandomizing Random Walks in Undirected Graphs Using Locally Fair Exploration Strategies\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01117": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Building timing predictable embedded systems':",
        "title: \"A high-end real-time digital film processing reconfigurable platform\" with abstract: \"Digital film processing is characterized by a resolution of at least 2 K (2048 \u00d7 1536 pixels per frame at 30 bit/pixel and 24 pictures/s, data rate of 2.2 Gbit/s); higher resolutions of 4 K (8.8 Gbit/s) and even 8 K (35.2Gbit/s) are on their way. Real-time processing at this data rate is beyond the scope of today's standard and DSP processors, and ASICs are not economically viable due to the small market volume. Therefore, an FPGA-based approach was followed in the FlexFilm project. Different applications are supported on a single hardware platform by using different FPGA configurations. The multiboard, multi-FPGA hardware/software architecture, is based on Xilinx Virtex-II Pro FPGAs which contain the reconfigurable image stream processing data path, large SDRAM memories for multiple frame storage, and a PCI-Express communication backbone network. The FPGA-embedded CPU is used for control and less computation intensive tasks. This paper will focus on three key aspects: (a) the used design methodology which combines macro component configuration and macrolevel floorplaning with weak programmability using distributed microcoding, (b) the global communication framework with communication scheduling, and (c) the configurable multistream scheduling SDRAM controller with QoS support by access prioritization and traffic shaping. As an example, a complex noise reduction algorithm including a 2.5-dimension discrete wavelet transformation (DWT) and a full 16 \u00d7 16 motion estimation (ME) at 24 fps, requiring a total of 203 Gops/s net computing performance and a total of 28 Gbit/s DDR-SDRAM frame memory bandwidth, will be shown.\"",
        "title: \"Multiple process execution in cache related preemption delay analysis\" with abstract: \"Cache prediction for preemptive scheduling is an open issue despite its practical importance. First analysis approaches use simplified models for cache behavior or they assume simplified preemption and execution scenarios that seriously impact analysis precision. We present an analysis approach which considers multiple executions of processes and preemption scenarios for static priority periodic scheduling. The results of our experiments show that caches introduce a strong and complex timing dependency between process executions that are not appropriately captured in the simplified models.\"",
        "title: \"Mixed Criticality Systems - A History of Misconceptions?\" with abstract: \"Mixed criticality systems have recently received much attention both in research and in industrial design practice. However, industrial practice and much of the research community seem to follow different objectives and research directions. We will review the status of mixed criticality systems design and compare the open challenges with the current research trends.\"",
        "title: \"Intervals in software execution cost analysis\" with abstract: \"Timing and power consumption of embedded systems are state and input data dependent. Formal analysis of such dependencies leads to intervals rather than single values. These intervals depend on program properties, execution paths and states of processes, as well as on the target architecture. This paper presents an approach to analysis of process behavior using intervals. It improves previous work by exploiting program segments with single paths and by taking the execution context into account. The example of an ATM cell handler demonstrates significant improvements in analysis precision.\"",
        "title: \"Providing accurate event models for the analysis of heterogeneous multiprocessor systems\" with abstract: \"This paper proposes a new method for deriving quantitative event information for compositional multiprocessor performance analysis. This procedure brakes down the complexity into the analysis of individual components (tasks mapped to resources) and the propagation of the timing information with the help of event models. This paper improves previous methods to derive event models in a multiprocessor system by providing tighter bounds and allowing arbitrarily shaped event models. The procedure is based on a a simple yet expressive resource model called the multiple event busy time which can be derived on the basis of classical scheduling theory -- it can therefore be provided for a large domain of scheduling policies. Our experiments show that overestimation by previous methods can be reduced significantly.\"",
        "1 is \"Indicator-Based Selection in Multiobjective Search\", 2 is \"Modeling Fault-tolerant Distributed Systems for Discrete Controller Synthesis\".",
        "\nGiven above information, for an author who has written the paper with the title \"Building timing predictable embedded systems\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01118": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Real-Time Feature Extraction for High Speed Networks':",
        "title: \"Using speculative functional units in high level synthesis\" with abstract: \"Speculative Functional Units (SFUs) enable a new execution paradigm for High Level Synthesis (HLS). SFUs are arithmetic functional units that operate using a predictor for the carry signal, which reduces the critical path delay. The performance of these units is determined by the success in the prediction of the carry value, i.e. the hit rate of the prediction. Hence SFUs reduce critical path at a low cost, but they cannot be used in HLS with the current techniques. In order to use them, it is necessary to include hardware support to recover from mispredictions of the carry signals. In this paper, we present techniques for designing a datapath controller for seamless deployment of SFUs in HLS. We have developed two techniques for this goal. The first approach stops the execution of the entire datapath for each misprediction and resumes execution once the correct value of the carry is known. The second approach decouples the functional unit suffering from the misprediction from the rest of the datapath. Hence, it allows the rest of the SFUs to carry on execution and be at different scheduling states at different times. Experiments show that it is possible to reduce execution time by as much as 38% and by 33% on average.\"",
        "title: \"Power-Driven Design Partitioning\" with abstract: \"In order to enable efficient integration of FPGAs into cost effective and reliable high-performance systems as well potentially into low power mobile systems, their power efficiency needs to be improved. In this paper, we propose a power management scheme for FPGAs centered on a power-driven partitioning technique. Our power-driven partitioner creates clusters within a design such that within individual clusters, power consumption can be improved via voltage scaling. We tested the effectiveness of our approach on a set of LUT-level benchmark netlists. Further we did constrained placement of the clusters into predefined V-dd(high) and V-dd(low) regions for a single FPGA. Average savings in power consumption with our approach is 48% whereas penalty in channel width and wire length due to constrained placement is 23% and 26% respectively.\"",
        "title: \"Peak temperature control and leakage reduction during binding in high level synthesis\" with abstract: \"Temperature is becoming a first rate design criterion in ASICs due to its negative impact on leakage power, reliability, performance, and packaging cost. Incorporating awareness of such lower level physical phenomenon in high level synthesis algorithms will help to achieve better designs. In this work, we developed a temperature aware binding algorithm. Switching power of a module correlates with its operating temperature. The goal of our binding algorithm is to distribute the activity evenly across functional units. This approach avoids steep temperature differences between modules on a chip, hence, the occurrence of hot spots. Starting with a switching optimal binding solution, our algorithm iteratively minimizes the maximum temperature reached by the hottest functional unit. Our algorithm does not change the number of resources used in the original binding. We have used HotSpot, a temperature modeling tool, to simulate temperature of a number ASIC designs. Our binding algorithm reduces temperature reached by the hottest resource by 12.21\u00b0C on average. Reducing the peak temperature has a positive impact on leakage as well. Our binding technique improves leakage power by 11.89%, and overall power by 3.32% on average at 130nm technology node compared to a switching optimal binding\"",
        "title: \"A self-adjusting clock tree architecture to cope with temperature variations\" with abstract: \"Ensuring resilience against environmental variations is becoming one of the great challenges of chip design. In this paper, we propose a self adjusting clock tree architecture, SACTA, to improve chip performance and reliability in the presence of on-chip temperature variations. SACTA performs temperature dependent dynamic clock skew scheduling to prevent timing violations in a pipelined circuit. We present an automatic temperature adjustable skew buffer design, which enables the adaptive feature of SACTA. Furthermore, we propose an efficient and general optimization framework to determine the configuration of these special delay elements. Experimental results show that a pipeline supported by SACTA is able to prevent thermal induced timing violations within a significantly larger range of operating temperatures (enhancing the violation-free range by as much as 45\u00b0C).\"",
        "title: \"Thermal monitoring mechanisms for chip multiprocessors\" with abstract: \"With large-scale integration and increasing power densities, thermal management has become an important tool to maintain performance and reliability in modern process technologies. In the core of dynamic thermal management schemes lies accurate reading of on-die temperatures. Therefore, careful planning and embedding of thermal monitoring mechanisms into high-performance systems becomes crucial. In this paper, we propose three techniques to create sensor infrastructures for monitoring the maximum temperature on a multicore system. Initially, we extend a nonuniform sensor placement methodology proposed in the literature to handle chip multiprocessors (CMPs) and show its limitations. We then analyze a grid-based approach where the sensors are placed on a static grid covering each core and show that the sensor readings can differ from the actual maximum core temperature by as much as 12.6\u00b0C when using 16 sensors per core. Also, as large as 10.6&percnt; of the thermal emergencies are not captured using the same number of sensors. Based on this observation, we first develop an interpolation scheme, which estimates the maximum core temperature through interpolation of the readings collected at the static grid points. We show that the interpolation scheme improves the measurement accuracy and emergency coverage compared to grid-based placement when using the same number of sensors. Second, we present a dynamic scheme where only a subset of the sensor readings is collected to predict the maximum temperature of each core. Our results indicate that, we can reduce the number of active sensors by as much as 50&percnt;, while maintaining similar measurement accuracy and emergency coverage compared to the case where the entire sensor set on the grid is sampled at all times.\"",
        "1 is \"Minimization of dynamic and static power through joint assignment of threshold voltages and sizing optimization\", 2 is \"BeepBeep: a high accuracy acoustic ranging system using COTS mobile devices\".",
        "\nGiven above information, for an author who has written the paper with the title \"Real-Time Feature Extraction for High Speed Networks\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01119": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Generating rules from trained network using fast pruning':",
        "title: \"NeuroLinear: From neural networks to oblique decision rules\" with abstract: \"We present NeuroLinear, a system for extracting oblique decision rules from neural networks that have been trained for classification of patterns. Each condition of an oblique decision rule corresponds to a partition of the attribute space by a hyperplane that is not necessarily axisparallel. Allowing a set of such hyperplanes to form the boundaries of the decision regions leads to a significant reduction in the number of rules generated while maintaining the accuracy rates of the networks. We describe the components of NeuroLinear in detail by way of two examples using artificial datasets. Our experimental results on real-world datasets show that the system is effective in extracting compact and comprehensible rules with high predictive accuracy from neural networks.\"",
        "title: \"Tagging documents using neural networks based on local word features\" with abstract: \"Keywords and key-phrases that concisely represent text documents are integral to many knowledge management and text information retrieval systems, as well as digital libraries in general. Not all text documents, however, are annotated with good keywords; and the quality of these keywords is often dependent on a tedious, sometimes manual, extraction and tagging process. To automatically extract high quality keywords without the need for a semantic analysis of the document, it is shown that artificial neural networks (ANN) can be trained to only consider in-document word features such as word frequency, word distribution in document, use of word in special parts of the document, and use of word formatting features (i.e. bold-faced, italicized, large-font size). Results show that purely local features are adequate in determining whether a word in a document is a keyword or not. Classification performance yields a G mean of a least 0.83, and weighted f-measure of 0.96 for both keywords and non-keywords. Precision for keywords alone, however, is not as high. To understand the basis for classifying keywords, C4.5 is used to extract rules from the ANN. The extracted rules from C4.5, in the form of a decision tree, show the relative importance of the different document features that were extracted.\"",
        "title: \"Understanding consumer heterogeneity: A business intelligence application of neural networks\" with abstract: \"This paper describes a business intelligence application of neural networks in analyzing consumer heterogeneity in the context of eating-out behavior in Taiwan. We apply a neural network rule extraction algorithm which automatically groups the consumers into identifiable segments according to their socio-demographic information. Within each of these segments, the consumers are distinguished between those who eat-out frequently from those who do not based on their psychological traits and eat-out considerations. The data set for this study has been collected through a survey of 800 Taiwanese consumers. Demographic information such as gender, age and income were recorded. In addition, information about their psychological traits and eating-out considerations that might influence the frequency of eating-out were obtained. The results of our data analysis show that the neural network rule extraction algorithm is able to find distinct consumer segments and predict the consumers within each segment with good accuracy.\"",
        "title: \"Incremental Feature Selection\" with abstract: \"Feature selection is a problem of finding relevant features.When the number of features of a dataset is large and its number of patternsis huge, an effective method of feature selection can help in dimensionalityreduction. An incremental probabilistic algorithm is designed and implementedas an alternative to the exhaustive and heuristic approaches. Theoretical analysis is given to support the idea of the probabilistic algorithm in finding an optimal or near-optimal subset of features. Experimental results suggest that (1) the probabilistic algorithm is effective in obtaining optimal/suboptimal feature subsets; (2) its incremental version expedites feature selection further when the number of patterns is largeand can scale up without sacrificing the quality of selected features.\"",
        "title: \"Extracting rules from pruned networks for breast cancer diagnosis.\" with abstract: \"A new algorithm for neural network pruning is presented. Using this algorithm, networks with small number of connections and high accuracy rates for breast cancer diagnosis are obtained. We will then describe how rules can be extracted from a pruned network by considering only a finite number of hidden unit activation values. The accuracy of the extracted rules is as high as the accuracy of the pruned network. For the breast cancer diagnosis problem, the concise rules extracted from the network achieve an accuracy rate of more than 95% on the training data set and on the test data set.\"",
        "1 is \"Adaptive histograms and dissimilarity measure for texture retrieval and classification\", 2 is \"Rule learning by searching on adapted nets\".",
        "\nGiven above information, for an author who has written the paper with the title \"Generating rules from trained network using fast pruning\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01120": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Scale-Invariant Directional Alignment of Surface Parametrizations.':",
        "title: \"Asynchronous integration with phantom meshes\" with abstract: \"Asynchronous variational integration of layered contact models provides a framework for robust collision handling, correct physical behavior, and guaranteed eventual resolution of even the most difficult contact problems. Yet, even for low-contact scenarios, this approach is significantly slower compared to its less robust alternatives---often due to handling of stiff elastic forces in an explicit framework. We propose a method that retains the guarantees, but allows for variational implicit integration of some of the forces, while maintaining asynchronous integration needed for contact handling. Our method uses phantom meshes for calculations with stiff forces, which are then coupled to the original mesh through constraints. We use the augmented discrete Lagrangian of the constrained system to derive a variational integrator with the desired conservation properties.\"",
        "title: \"Worst-case structural analysis\" with abstract: \"Direct digital manufacturing is a set of rapidly evolving technologies that provide easy ways to manufacture highly customized and unique products. The development pipeline for such products is radically different from the conventional manufacturing pipeline: 3D geometric models are designed by users often with little or no manufacturing experience, and sent directly to the printer. Structural analysis on the user side with conventional tools is often unfeasible as it requires specialized training and software. Trial-and-error, the most common approach, is time-consuming and expensive. We present a method that would identify structural problems in objects designed for 3D printing based on geometry and material properties only, without specific assumptions on loads and manual load setup. We solve a constrained optimization problem to determine the \"worst\" load distribution for a shape that will cause high local stress or large deformations. While in its general form this optimization has a prohibitively high computational cost, we demonstrate that an approximate method makes it possible to solve the problem rapidly for a broad range of printed models. We validate our method both computationally and experimentally and demonstrate that it has good predictive power for a number of diverse 3D printed shapes.\"",
        "title: \"Approximate Boolean operations on free-form solids\" with abstract: \"In this paper we describe a method for computing approximate results of boolcan operations (union, intersection, difference) applied to free-form solids bounded by multiresolution subdivision surfaces.We present algorithms for generating a control mesh for a multiresolution surface approximating the result, optimizing the parameterization of the new surface with respect to the original surfaces, and fitting the new surface to the geometry of the original surfaces. Our algorithms aim to minimize the size and optimize the quality of the new control mesh. The original control meshes are modified only in a neighborhood of the intersection.While the main goal is to obtain approximate results, high-accuracy approximations are also possible at additional computational expense, if the topology of the intersection curve is resolved correctly.\"",
        "title: \"4-8 Subdivision\" with abstract: \"In this paper we introduce 4-8 subdivision, a new scheme that generalizes the four-directional box spline of class C^4 to surfaces of arbitrary topological type. The crucial advantage of the proposed scheme is that it uses bisection refinement as an elementary refinement operation, rather than more commonly used face or vertex splits. In the uniform case, bisection refinement results in doubling, rather than quadrupling of the number of faces in a mesh. Adaptive bisection refinement automatically generates conforming variable-resolution meshes in contrast to face and vertex split methods which require a postprocessing step to make an adaptively refined mesh conforming. The fact that the size of faces decreases more gradually with refinement allows one to have greater control over the resolution of a refined mesh. It also makes it possible to achieve higher smoothness while using small stencils (the size of the stencils used by our scheme is similar to Loop subdivision). We show that the subdivision surfaces produced by the 4-8 scheme are C^4 continuous almost everywhere, except at extraordinary vertices where they are is C^1-continuous.\"",
        "title: \"A numerical method for simulating the dynamics of 3D axisymmetric vesicles suspended in viscous flows\" with abstract: \"We extend [Shravan K. Veerapaneni, Denis Gueyffier, Denis Zorin, George Biros, A boundary integral method for simulating the dynamics of inextensible vesicles suspended in a viscous fluid in 2D, Journal of Computational Physics 228(7) (2009) 2334-2353] to the case of three-dimensional axisymmetric vesicles of spherical or toroidal topology immersed in viscous flows. Although the main components of the algorithm are similar in spirit to the 2D case-spectral approximation in space, semi-implicit time-stepping scheme-the main differences are that the bending and viscous force require new analysis, the linearization for the semi-implicit schemes must be rederived, a fully implicit scheme must be used for the toroidal topology to eliminate a CFL-type restriction and a novel numerical scheme for the evaluation of the 3D Stokes single layer potential on an axisymmetric surface is necessary to speed up the calculations. By introducing these novel components, we obtain a time-scheme that experimentally is unconditionally stable, has low cost per time step, and is third-order accurate in time. We present numerical results to analyze the cost and convergence rates of the scheme. To verify the solver, we compare it to a constrained variational approach to compute equilibrium shapes that does not involve interactions with a viscous fluid. To illustrate the applicability of method, we consider a few vesicle-flow interaction problems: the sedimentation of a vesicle, interactions of one and three vesicles with a background Poiseuille flow.\"",
        "1 is \"Preserving form features in interactive mesh deformation\", 2 is \"Multiperspective panoramas for cel animation\".",
        "\nGiven above information, for an author who has written the paper with the title \"Scale-Invariant Directional Alignment of Surface Parametrizations.\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01121": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Mutable elastic models for sculpting structured shapes.':",
        "title: \"The Randomized Sample Tree: A Data Structure for Interactive Walk-Throughs in Externally Stored Virtual Environments\" with abstract: \"We present a new data structure for rendering highly complex virtual environments of arbitrary topology. The special feature of our approach is that it allows an interactive navigation in very large scenes (30 GB/400 million polygons in our benchmark scenes) that cannot be stored in main memory, but only on a local or remote hard disk. Furthermore, it allows interactive rendering of substantially more complex scenes by instantiating objects.For the computation of an approximate image of the scene, a sampling technique is used. In the preprocessing, a so-called sample tree is built whose nodes contain randomly selected polygons from the scene. This tree only uses space that is linear in the number of polygons. In order to produce an image of the scene, the tree is traversed and polygons stored in the visited nodes are rendered. During the interactive walkthrough, parts of the sample tree are loaded from local or remote hard disk.We implemented our algorithm in a prototypical walkthrough system. Analysis and experiments show that the quality of our images is comparable to images computed by the conventional z-buffer algorithm regardless of the scene topology.\"",
        "title: \"Meshless Modeling of Deformable Shapes and their Motion.\" with abstract: \"We present a new framework for interactive shape deformation modeling and key frame interpolation based on a meshless finite element formulation. Starting from a coarse nodal sampling of an object's volume, we formulate rigidity and volume preservation constraints that are enforced to yield realistic shape deformations at interactive frame rates. Additionally, by specifying key frame poses of the deforming shape and optimizing the nodal displacements while targeting smooth interpolated motion, our algorithm extends to a motion planning framework for deformable objects. This allows reconstructing smooth and plausible deformable shape trajectories in the presence of possibly moving obstacles. The presented results illustrate that our framework can handle complex shapes at interactive rates and hence is a valuable tool for animators to realistically and efficiently model and interpolate deforming 3D shapes.\n\n\"",
        "title: \"Biosignal-Based Spoken Communication: A Survey.\" with abstract: \"Speech is a complex process involving a wide range of biosignals, including but not limited to acoustics. These biosignals-stemming from the articulators, the articulator muscle activities, the neural pathways, and the brain itself-can be used to circumvent limitations of conventional speech processing in particular, and to gain insights into the process of speech production in general. Research o...\"",
        "title: \"Improving Speaker-Independent Lipreading With Domain-Adversarial Training\" with abstract: \"We present a Lipreading system, i.e. a speech recognition system using only visual features, which uses domain-adversarial training for speaker independence. Domain-adversarial training is integrated into the optimization of a lipreader based on a stack of feedforward and LSTM (Long Short-Term Memory) recurrent neural networks, yielding an end-to-end trainable system which only requires a very small number of frames of untranscribed target data to substantially improve the recognition accuracy on the target speaker. On pairs of different source and target speakers, we achieve a relative accuracy improvement of around 40% with only 15 to 20 seconds of untranscribed target speech data. On multi-speaker training setups, the accuracy improvements are smaller but still substantial.\"",
        "title: \"A kaleidoscopic approach to surround geometry and reflectance acquisition\" with abstract: \"We describe a system for acquiring reflectance fields of objects without moving parts and without a massively parallel hardware setup. Our system consists of a set of planar mirrors which serve to multiply a single camera and a single projector into a multitude of virtual counterparts. Using this arrangement, we can acquire reflectance fields with an average angular sampling rate of about 120+ view/light pairs per surface point. The mirror system allows for freely programmable illumination with full directional coverage. We employ this setup to realize a 3D acquisition system that employs structured illumination to capture the unknown object geometry, in addition to dense reflectance sampling. On the software side, we combine state-of-the-art 3D reconstruction algorithms with a reflectance sharing technique based on non-negative matrix factorization in order to reconstruct a joint model of geometry and reflectance. We demonstrate for a number of test scenes that the kaleidoscopic approach can acquire complex reflectance properties faithfully. The main limitation is that the multiplexing approach limits the attainable spatial resolution, trading it off for improved directional coverage.\"",
        "1 is \"Large displacement optical flow computation withoutwarping\", 2 is \"Hierarchical disparity estimation with energy-based regularization\".",
        "\nGiven above information, for an author who has written the paper with the title \"Mutable elastic models for sculpting structured shapes.\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01122": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Three Architectures for Volume Rendering':",
        "title: \"Towards secure and privacy sensitive surveillance\" with abstract: \"This paper analyzes the requirements of an ideal vision system. Two major challenges are identified -- security and privacy. Security ensures a reliable and dependable operation where the whole chain is robust against modifications and erasures. This comprises the aspect of authenticity to qualify for legal actions on one hand and to prevent (e.g., man in the middle) attacks from modifying content to burden innocent persons. Privacy is strongly penetrated by today's surveillance systems. It has to be ensured that both the derived video analysis results and the adequately filtered imaging stream (if still required) are only accessible to adequate user groups to address the privacy dilemma. A vision based sensor should be applicable in the same application fields as other non-camera based application specific sensors, e.g., photoelectric sensors used in mens' restroom facilities (urinals/pissoirs) -- with the same confidence and trust. Related work is surveyed and a first concept is introduced to address these demands. It is based on a smart camera approach and various certification and authentification mechanisms to allow application specific sensors to use the power of the visual modality without the traditional drawbacks. Different use cases are discussed from the point of view of the persons surveilled, persons intending to modify and persons utilizing the results. Finally, the \"survmotion\" system is presented where some aspects of the concept are implemented to illustrate the potential applicability in various fields.\"",
        "title: \"Texram: a smart memory for texturing\" with abstract: \"Logic embedded memory is an emerging technology that combines high transfer rates and computing power. Texram implements this technology and a new filtering algorithm to achieve high speed, high quality texture mapping. Integrating arithmetic units and large memory arrays on the same chip and thus exploiting the enormous internal transfer rates provides an elegant solution to the memory access bottleneck of high quality texture mapping. Using this technology, we can not only achieve higher texturing speed at lower system costs, we can also incorporate new functionalities such as detail mapping and footprint assembly to produce higher quality images at real time rendering speeds. Environment and video mapping are also integrated on the Texram, which therefore represents an autonomous and versatile texturing coprocessor. Logic enhanced memories might become the computing paradigm of the future, not just in graphics applications. Technological advances will foster this trend by providing an ever increasing amount of memory capacity and chip space for arithmetic units. As the ultimate solution, we can expect a complete 3D graphics pipeline including all memory systems integrated on a single chip\"",
        "title: \"Magnets in motion\" with abstract: \"We introduce magnetic interaction for rigid body simulation. Our approach is based on an equivalent dipole method and as such it is discrete from the ground up. Our approach is symmetric as we base both field and force computations on dipole interactions. Enriching rigid body simulation with magnetism allows for many new and interesting possibilities in computer animation and special effects. Our method also allows the accurate computation of magnetic fields for arbitrarily shaped objects, which is especially interesting for pedagogy as it allows the user to visually discover properties of magnetism which would otherwise be difficult to grasp. We demonstrate our method on a variety of problems and our results reflect intuitive as well as surprising effects. Our method is fast and can be coupled with any rigid body solver to simulate dozens of magnetic objects at interactive rates.\"",
        "title: \"Cube-4 implementations on the teramac custom computing machine\" with abstract: \"We present two implementations of the Cube-4 volume rendering architecture on the Teramac custom computing machine. Cube-4 uses a sliceparallel ray-casting algorithm that allows for a parallel and pipelined implementation of ray-casting with tri-linear interpolation and surface normal estimation from interpolated samples. Shading, classification and compositing are part of rendering pipeline. With the partitioning schemes introduced in this paper, Cube-4 is capable of rendering large datasets with a limited number of pipelines. The Teramac hardware simulator at the Hewlett-Packard research laboratories, Palo Alto, CA, on which Cube-4 was implemented, belongs to the new class of custom computing machines. Teramac combines the speed of special-purpose hardware with the flexibility of general-purpose computers. With Teramac as a development tool we were able to implement in just five weeks working Cube-4 prototypes, capable of rendering for example datasets of 1283 voxels in 0.65 seconds at 0.96 MHz processing frequency. The performance results from these implementations indicate real-time performance for high-resolution data-sets.\"",
        "title: \"A Fast and Accurate Approach for the Segmentation of the Paranasal Sinus\" with abstract: \"Recently, functional endoscopic sinus surgery is the state of the art in the surgical treatment of endonasal pathology. For a more accurate access planning including 3D measures of the cavity, especially in complex cases of tumor diseases, a segmentation of the paranasal sinus is certainly a useful tool. Unfortunately, this structure is quite complicated and di-cult to segment. In our contribution, we propose a semi-automatic segmentation pipeline that signiflcantly reduces the total processing time and the required interaction.\"",
        "1 is \"The Neuron Navigator: Exploring the information pathway through the neural maze\", 2 is \"Back-to-Front Display of Voxel Based Objects\".",
        "\nGiven above information, for an author who has written the paper with the title \"Three Architectures for Volume Rendering\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01123": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Power consumption in telecommunication networks: overview and reduction strategies.':",
        "title: \"An improved method to determine the antenna factor\" with abstract: \"We present an improved method to determine the antenna factor of three antennas. Instead of using a reflecting ground plane we use absorbers. Destructive interference between the direct beam and the residual reflected beam from the absorbers is avoided by splitting the measured frequency range in bands and changing the distance between the two antennas depending on the frequency band. Furthermore, this method is applicable for both E- and H-field probes. Our method has also the advantage of being low-cost: the method does not need to be performed in an anechoic chamber to obtain high accuracy. To take the residual reflections of the environment into account, we perform a de-embedding procedure. We have developed two de-embedding methods.\"",
        "title: \"Intuitive human-device interaction for video control and feedback\" with abstract: \"Current video services are still controlled in an old-fashioned way using keyboard and mouse for computers and remote control for TV sets. This paper presents more attractive and intuitive interaction methods using commercially available motion sensing input devices. These devices are typically based on a webcam-style add-on peripheral, thereby enabling facial recognition, gesture control, and speech recognition. These technologies are applied in this research to automatically authenticate a user, enable video control (play, pause, seeking), and browsing, selecting, and rating content by hand gestures or voice commands. By monitoring the user's gaze and using emotion recognition techniques, the user's interests and engagement with the content can be estimated. This is interpreted as implicit user feedback for the video content, and establishes an automatic feedback channel, which can be used for content personalization and recommendation. User tests showed an accurate recognition of voice and gestures and confirmed the attractiveness and intuitiveness of these techniques for end-users.\"",
        "title: \"Performance Evaluation of 5G Millimeter-Wave Cellular Access Networks Using a Capacity-Based Network Deployment Tool.\" with abstract: \"The next fifth generation (5G) of wireless communication networks comes with a set of new features to satisfy the demand of data-intensive applications: millimeter-wave frequencies, massive antenna arrays, beamforming, dense cells, and so forth. In this paper, we investigate the use of beamforming techniques through various architectures and evaluate the performance of 5G wireless access networks, using a capacity-based network deployment tool. This tool is proposed and applied to a realistic area in Ghent, Belgium, to simulate realistic 5G networks that respond to the instantaneous bit rate required by the active users. The results show that, with beamforming, 5G networks require almost 15% more base stations and 4 times less power to provide more capacity to the users and the same coverage performances, in comparison with the 4G reference network. Moreover, they are 3 times more energy efficient than the 4G network and the hybrid beamforming architecture appears to be a suitable architecture for beamforming to be considered when designing a 5G cellular network.\"",
        "title: \"A multi-objective approach to indoor wireless heterogeneous networks planning based on biogeography-based optimization\" with abstract: \"In this paper, we present a multi-objective optimization approach for indoor wireless network planning subject to constraints for exposure minimization, coverage maximization and power consumption minimization. We consider heterogeneous networks consisting of WiFi access points (APs) and long term evolution (LTE) femtocells. We propose a design framework based on multi-objective biogeography-based optimization (MOBBO). We apply the MOBBO algorithm to network planning design cases in a real office environment. To validate this approach we compare results with other multi-objective algorithms like the nondominated sorting genetic algorithm-II (NSGA-II) and the generalized differential evolution (GDE3) algorithm. The results of the proposed method indicate the advantages and applicability of the multi-objective approach.\"",
        "title: \"Statistical Analysis of Multipath Clustering in an Indoor Office Environment\" with abstract: \"A parametric directional-based MIMO channel model is presented which takes multipath clustering into account. The directional propagation path parameters include azimuth of arrival (AoA), azimuth of departure (AoD), delay, and power. MIMO measurements are carried out in an indoor office environment using the virtual antenna array method with a vector network analyzer. Propagation paths are extracted using a joint 5D ESPRIT algorithm and are automatically clustered with the K-power-means algorithm. This work focuses on the statistical treatment of the propagation parameters within individual clusters (intracluster statistics) and the change in these parameters from one cluster to another (intercluster statistics). Motivated choices for the statistical distributions of the intracluster and intercluster parameters are made. To validate these choices, the parameters' goodness of fit to the proposed distributions is verified using a number of powerful statistical hypothesis tests. Additionally, parameter correlations are calculated and tested for their significance. Building on the concept of multipath clusters, this paper also provides a new notation of the MIMO channel matrix (named FActorization into a BLock-diagonal Expression or FABLE) which more visibly shows the clustered nature of propagation paths.\"",
        "1 is \"Gephi: An Open Source Software for Exploring and Manipulating Networks\", 2 is \"Throughput Performance Optimization of Super Dense Wireless Networks With the Renewal Access Protocol.\".",
        "\nGiven above information, for an author who has written the paper with the title \"Power consumption in telecommunication networks: overview and reduction strategies.\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01124": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Models for Wireless Data Communications in Indoor Train Environment.':",
        "title: \"Evaluating a recommendation application for online video content: an interdisciplinary study\" with abstract: \"In this paper, we discuss the set-up and results from an interdisciplinary study aimed at evaluating a recommendation application for online video content, called PersonalTV. By involving (possible) users (i.e. a panel of test users), we tried to gather insights that might help to optimize and refine the application. In this respect, implicit and explicit user feedback were complemented. This paper explores the relation between the PersonalTV suggestions (recommended content) and the consumption percentage (objective data) (RQ 1) and between the recommended content and the reported satisfaction (subjective data) (RQ 2) of the test users. We also investigated whether the objective and subjective measures converge (RQ 3) and collected feedback that suggests measures for further improvement and optimization of the application.\"",
        "title: \"Designing Energy-Efficient Wireless Access Networks: LTE and LTE-Advanced\" with abstract: \"As large energy consumers, base stations need energy-efficient wireless access networks. This article compares the design of Long-Term Evolution (LTE) networks to energy-efficient LTE-Advanced networks. LTE-Advanced introduces three new functionalities--carrier aggregation, heterogeneous networks, and extended multiple-input, multiple-output (MIMO) support. The authors develop a power consumption model for LTE and LTE-Advanced macrocell and femtocell base stations, along with an energy efficiency measure. They show that LTE-Advanced's carrier aggregation and MIMO improve networks' energy efficiency up to 400 and 450 percent, respectively.\"",
        "title: \"Offline optimization for user-specific hybrid recommender systems\" with abstract: \"Massive availability of multimedia content has given rise to numerous recommendation algorithms that tackle the associated information overload problem. Because of their growing popularity, selecting the best one is becoming an overload problem in itself. Hybrid algorithms, combining multiple individual algorithms, offer a solution, but often require manual configuration and power only a few individual recommendation algorithms. In this work, we regard the problem of configuring hybrid recommenders as an optimization problem that can be trained in an offline context. Focusing on the switching and weighted hybridization techniques, we compare and evaluate the resulting performance boosts for hybrid configurations of up to 10 individual algorithms. Results showed significant improvement and robustness for the weighted hybridization strategy which seems promising for future self-adapting, user-specific hybrid recommender systems.\"",
        "title: \"A Framework for Dataset Benchmarking and Its Application to a New Movie Rating Dataset.\" with abstract: \"Rating datasets are of paramount importance in recommender systems research. They serve as input for recommendation algorithms, as simulation data, or for evaluation purposes. In the past, public accessible rating datasets were not abundantly available, leaving researchers no choice but to work with old and static datasets like MovieLens and Netflix. More recently, however, emerging trends as social media and smartphones are found to provide rich data sources which can be turned into valuable research datasets. While dataset availability is growing, a structured way for introducing and comparing new datasets is currently still lacking. In this work, we propose a five-step framework to introduce and benchmark new datasets in the recommender systems domain. We illustrate our framework on a new movie rating dataset\u2014called MovieTweetings\u2014collected from Twitter. Following our framework, we detail the origin of the dataset, provide basic descriptive statistics, investigate external validity, report the results of a number of reproducible benchmarks, and conclude by discussing some interesting advantages and appropriate research use cases.\"",
        "title: \"Performance Evaluation of 5G Millimeter-Wave Cellular Access Networks Using a Capacity-Based Network Deployment Tool.\" with abstract: \"The next fifth generation (5G) of wireless communication networks comes with a set of new features to satisfy the demand of data-intensive applications: millimeter-wave frequencies, massive antenna arrays, beamforming, dense cells, and so forth. In this paper, we investigate the use of beamforming techniques through various architectures and evaluate the performance of 5G wireless access networks, using a capacity-based network deployment tool. This tool is proposed and applied to a realistic area in Ghent, Belgium, to simulate realistic 5G networks that respond to the instantaneous bit rate required by the active users. The results show that, with beamforming, 5G networks require almost 15% more base stations and 4 times less power to provide more capacity to the users and the same coverage performances, in comparison with the 4G reference network. Moreover, they are 3 times more energy efficient than the 4G network and the hybrid beamforming architecture appears to be a suitable architecture for beamforming to be considered when designing a 5G cellular network.\"",
        "1 is \"A generic component model for building systems software\", 2 is \"An Ant Colony Optimization Approach for Maximizing the Lifetime of Heterogeneous Wireless Sensor Networks\".",
        "\nGiven above information, for an author who has written the paper with the title \"Models for Wireless Data Communications in Indoor Train Environment.\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01125": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'How Does Correlation Affect the Capacity of MIMO Systems with Rate Constraints?':",
        "title: \"Interleave division multiple-access\" with abstract: \"This paper provides a comprehensive study of interleave-division multiple-access (IDMA) systems. The IDMA receiver principles for different modulation and channel conditions are outlined. A semi-analytical technique is developed based on the density evolution technique to estimate the bit-error-rate (BER) of the system. It provides a fast and relatively accurate method to predict the performance of the IDMA scheme. With simple convolutional/repetition codes, overall throughputs of 3 bits/chip with one receive antenna and 6 bits/chip with two receive antennas are observed for IDMA systems involving as many as about 100 users.\"",
        "title: \"Decentralized Power Control for Random Access with Successive Interference Cancellation\" with abstract: \"This paper is concerned with the decentralized power allocation problem in random access systems. We propose a scheme that is especially suitable for systems requiring high throughput but with difficulty in establishing centralized control, such as cognitive radio environments. Specifically, we assume successive interference cancellation (SIC) at the receiver for multi-packet reception (MPR). We consider a decentralized random power transmission strategy where each user selects its transmitted power level randomly according to a power distribution conditioned on its own channel state. Our focus is on the design of this distribution such that the system packet throughput is maximized under rate and power constraints. We start from a two-user system. A main finding of this paper is that the supports of the optimal power distributions are of discrete nature. This finding greatly simplifies the distribution optimization problem. We also discuss a sub-optimal solution to systems with more than two users. Numerical results demonstrate that the proposed scheme can achieve noticeable performance improvement compared with conventional single-user detection (SUD) based ones and offer a flexible tradeoff between the system throughput and power consumption.\"",
        "title: \"Spatially Coupled Ldpc Coding And Linear Precoding For Mimo Systems\" with abstract: \"In this paper, we present a transmission scheme for a multiple-input multiple-output (MIMO) quasi-static fading channel with imperfect channel state information at the transmitter (CSIT). In this scheme, we develop a precoder structure to exploit the available CSIT and apply spatial coupling for further performance enhancement. We derive an analytical evaluation method based on extrinsic information transfer (EXIT) functions, which provides convenience for our precoder design. Furthermore, we observe an area property indicating that, for a spatially coupled system, the iterative receiver can perform error-free decoding even the original uncoupled system has multiple fixed points in its EXIT chart. This observation implies that spatial coupling is useful to alleviate the uncertainty in CSIT which causes difficulty in designing LDPC code based on the EXIT curve matching technique. Numerical results are presented, showing an excellent performance of the proposed scheme in MIMO fading channels with imperfect CSIT.\"",
        "title: \"Low complexity concatenated two-state TCM schemes with near capacity performance\" with abstract: \"This paper presents a family of low complexity concatenated two-state trellis-coded modulation (CT-TCM) schemes. A joint design strategy of all component codes is established. This leads to the so-called \"asymmetrical and time-varying\" structures. Compared with the existing turbo TCM codes, the proposed CT-TCM schemes have significantly reduced decoding complexity and demonstrate comparable or even better performance.\"",
        "title: \"Evolution Analysis Of Iterative Lmmse-App Detection For Coded Linear System With Cyclic Prefixes\" with abstract: \"This paper is concerned with the iterative detection principles for coded linear systems with cyclic prefixes. We derive a matrix-form low-cost fast Fourier transform (FFT) based iterative LMMSE-APP detector and propose an evolution technique for the performance evaluation of the proposed detector. Numerical results show a good match between simulation and evolution prediction.\"",
        "1 is \"Steepest Descent Algorithms for Optimization Under Unitary Matrix Constraint\", 2 is \"Challenges and recent advances in IR-UWB system design\".",
        "\nGiven above information, for an author who has written the paper with the title \"How Does Correlation Affect the Capacity of MIMO Systems with Rate Constraints?\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01126": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Distance-Based Location Update and Routing in Irregular Cellular Networks':",
        "title: \"Embedding Metric Spaces in the Rectilinear Plane: a Six-Point Criterion\" with abstract: \"We show that a metric space embeds in the rectilinear plane (i.e., is L1- embeddable inR2) if and only if every subspace with five or six points does. A simple construction shows that for higher dimensions k of the host rectilinear space the number c.k/ of points that need to be tested grows at least quadratically with k, thus disproving a conjecture of Seth and Jerome Malitz.\"",
        "title: \"Addressing, Distances and Routing in Triangular Systems with Applications in Cellular and Sensor Networks\" with abstract: \"Triangular systems are the subgraphs of the regular tri- angular grid which are formed by a simple circuit of the grid and the region bounded by this circuit. They are used to model cellular networks where nodes are base stations. In this paper, we propose an addressing scheme for triangu- lar systems by employing their isometric embeddings into the Cartesian product of three trees. This embedding pro- vides a simple representation of any triangular system with only three small integers per vertex, and allows to employ the compact labeling schemes for trees for distance queries and routing. We show that each such system with n ver- tices admits a labeling that assigns O(log2 n) bit labels to vertices of the system such that the distance between any two vertices u and v can be determined in constant time by merely inspecting the labels of u and v, without using any other information about the system. Furthermore, there is a labeling, assigning labels of size O(log n) bits to vertices, which allows, given the label of a source vertex and the la- bel of a destination, to compute in constant time the port number of the edge from the source that heads in the di- rection of the destination. These results are used in solving some problems in cellular networks. Our addressing and distance labeling schemes allow efficient implementation of distance and movement based tracking protocols in cellular networks, by providing information, generally not available to the user, and means for accurate cell distance determi- nation. Our routing and distance labeling schemes provide elegant and efficient routing and connection rerouting pro- tocols for cellular networks.\"",
        "title: \"A Counterexample to Thiagarajan's Conjecture on Regular Event Structures.\" with abstract: \"We provide a counterexample to a conjecture by Thiagarajan (1996 and 2002) that regular prime event structures correspond exactly to those obtained as unfoldings of finite 1-safe Petri nets.  The same counterexample is used to disprove a closely related conjecture by Badouel, Darondeau, and Raoult (1999) that domains of regular event structures with bounded natural-cliques are recognizable by finite trace automata.  Event structures, trace automata, and Petri nets are fundamental models in concurrency theory. There exist nice interpretations of these structures as combinatorial and geometric objects and both conjectures can be reformulated in this framework. Namely, the domains of prime event structures correspond exactly to pointed median graphs; from a geometric point of view, these domains are in bijection with pointed CAT(0) cube complexes.A necessary condition for both conjectures to be true is that domains of respective regular event structures admit a regular nice labeling. To disprove these conjectures, we describe a regular event domain (with bounded natural-cliques) that does not admit a regular nice labeling. Our counterexample is derived from an example by Wise (1996 and 2007) of a nonpositively curved square complex whose universal cover is a CAT(0) square complex containing a particular plane with an aperiodic tiling.\"",
        "title: \"Deciding the Satisfiability of Propositional Formulas in Finitely-Valued Signed Logics\" with abstract: \"Signed logic is a way of expressing the semantics of many-valued connectives and quantifiers in a formalism that is well-suited for automated reasoning. In this paper we consider propositional, finitely-valued formulas in clausal normal form. We show that checking the satisfiability of formulas with three or more literals per clause is eitherNP-complete or trivial, depending on whether the intersection of all signs is empty or not. The satisfiability of bijunctive formulas, i.e., of formulas with at most two literals per clause, is decidable in linear time if the signs form a Helly family, and is NP-complete otherwise. We present a polynomial-time algorithm for deciding whether a given set of signs satisfies the Helly property. Our results unify and extend previous results obtained for particular sets of signs.\"",
        "title: \"Basis graphs of even Delta-matroids\" with abstract: \"A @D-matroid is a collection B of subsets of a finite set I, called bases, not necessarily equicardinal, satisfying the symmetric exchange property: ForA,B@?Bandi@?A@DB, there existsj@?B@DAsuch that(A@D{i,j})@?B. A @D-matroid whose bases all have the same cardinality modulo 2 is called an even @D-matroid. The basis graphG=G(B) of an even @D-matroid B is the graph whose vertices are the bases of B and edges are the pairs A,B of bases differing by a single exchange (i.e., |A@DB|=2). In this note, we present a characterization of basis graphs of even @D-matroids, extending the description of basis graphs of ordinary matroids given by S. Maurer in 1973: TheoremA graphG=(V,E)is a basis graph of an even @D-matroid if and only if it satisfies the following conditions:(a)ifx\"1x\"2x\"3x\"4is a square andb@?V, thend(b,x\"1)+d(b,x\"3)=d(b,x\"2)+d(b,x\"4); (b)each 2-interval of G contains a square and is an induced subgraph of the 4-octahedron; (c)the neighborhoods of vertices induce line graphs, or, equivalently, the neighborhoods of vertices do not contain induced 5- and 6-wheels. (A 2-interval is the subgraph induced by two vertices at distance 2 and all their common neighbors; a square is an induced 4-cycle of G.)\"",
        "1 is \"DIMES: let the internet measure itself\", 2 is \"Finding a smallest augmentation to biconnect a graph\".",
        "\nGiven above information, for an author who has written the paper with the title \"Distance-Based Location Update and Routing in Irregular Cellular Networks\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01127": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Privacy Protection in Video Surveillance Systems Using Scalable Video Coding':",
        "title: \"Secure Hybrid Digital-Analog Wyner-Ziv Coding\" with abstract: \"In this work, the problem of transmitting an i.i.d Gaussian source over an i.i.d Gaussian wiretap channel with an i.i.d Gaussian side information at the intended receiver is considered. The intended receiver is assumed to have a certain minimum SNR and the eavesdropper is assumed to have a strictly lower SNR compared to the intended receiver. The objective is minimizing the distortion of source reconstruction at the intended receiver. In this work, it is shown that the source-channel separation coding scheme is optimum in the sense of achieving the minimum distortion. A hybrid digital-analog Wyner-Ziv coding scheme is then proposed which achieve the minimum distortion. This secure joint source channel coding scheme is based on Wyner-Ziv coding scheme and wiretap channel coding scheme when the analog source is not explicitly quantized. The proposed secure hybrid digital-analog scheme is analyzed under the main channel SNR mismatch. It is proven that the proposed scheme can give a graceful degradation of distortion with SNR under SNR mismatch, i.e., when the actual SNR is larger than the designed SNR.\"",
        "title: \"Spatiotemporal Demosaicking Using Multi-Stage Processing Concepts\" with abstract: \"This paper presents a spatiotemporal demosaicking scheme suitable for digital video cameras. The proposed method uses multi-stage processing concepts to follow varying spatiotemporal characteristics of the captured video. The method also uses both the structural and spectral information during demosaicking in order to produce visually pleasing full-color videos.\"",
        "title: \"Contribution of non-scrambled chroma information in privacy-protected face images to privacy leakage\" with abstract: \"To mitigate privacy concerns, scrambling can be used to conceal face regions present in surveillance video content. Given that lightweight scrambling tools may not protect chroma information in order to limit bit rate overhead in heterogeneous usage environments, this paper investigates how the presence of non-scrambled chroma information in face regions influences the effectiveness of automatic and human face recognition (FR). To that end, we apply three automatic FR techniques to face images that have been privacy-protected by means of a layered scrambling technique developed for Motion JPEG XR, testing the effectiveness of automatic FR and layered scrambling using various experimental conditions. In addition, we investigate whether agreement exists between the judgments of 32 human observers and the output of automatic FR. Our experimental results demonstrate that human observers are not able to successfully recognize face images when simultaneously visualizing scrambled luma and non-scrambled chroma information. However, when an adversary has access to the coded bit stream structure, the presence of non-scrambled chroma information may significantly contribute to privacy leakage. By additionally applying layered scrambling to chroma information, our experimental results show that the amount of privacy leakage can be substantially decreased at the cost of an increase in bit rate overhead, and with the increase in bit rate overhead dependent on the number of scalability layers used.\"",
        "title: \"An Automated Approach for Kidney Segmentation in Three-Dimensional Ultrasound Images.\" with abstract: \"Automated segmentation of kidneys in three-dimensional (3-D) abdominal ultrasound volumes is a task of paramount importance in automated diagnosis of abdominal trauma. However, ultrasound speckle noise, low-contrast boundaries, partial kidney occlusion, and probe misalignment restrict the utility of the solution, especially when it is used in emergency rooms and Focused Assessment with Sonography ...\"",
        "title: \"Selecting Kernel Eigenfaces For Face Recognition With One Training Sample Per Subject\" with abstract: \"It is well-known that supervised learning techniques such as linear discriminant analysis (LDA) often suffer from the so called small sample size problem when apply to solve face recognition problems. This is due to the fact that in most cases, the number of training samples is much smaller than the dimensionality of the sample space. The problem becomes even more severe if only one training sample is available for each subject. In this paper, followed by the well-known unsupervised technique, kernel principal component analysis(KPCA), a novel feature selection scheme is proposed to establish a discriminant feature subspace in which the class separability is maximized. Extensive experiments performed on the FERET database indicate that the proposed scheme significantly boosts the recognition performance of the traditional KPCA solution.\"",
        "1 is \"Visual Understanding via Multi-Feature Shared Learning With Global Consistency.\", 2 is \"Cognitive radar: a way of the future\".",
        "\nGiven above information, for an author who has written the paper with the title \"Privacy Protection in Video Surveillance Systems Using Scalable Video Coding\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01128": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Calculi of Granules Based on Rough Set Theory: Approximate Distributed Synthesis and Granular Semantics for Computing with Words':",
        "title: \"Reasoning based on information changes in information maps\" with abstract: \"We discuss basic concepts for approximate reasoning about information changes. Any rule for reasoning about information changes specifies how changes of information granules from the rule premise influence changes of information granules from the rule conclusion. Changes in information granules can be measured, e.g., using expressions analogous to derivatives. We illustrate our approach by means of information maps and information granules defined in such maps.\"",
        "title: \"Information systems in modeling interactive computations on granules\" with abstract: \"In this paper, we discuss the importance of information systems in modeling interactive computations performed on (complex) granules and we propose a formal approach to interactive computations based on generalized information systems and rough sets which can be combined with other soft computing paradigms such as fuzzy sets or evolutionary computing, but also with machine learning and data mining techniques. Information systems are treated as dynamic granules used for representing the results of the interaction of attributes with the environment. Two kinds of attributes are distinguished, namely, the perception attributes, including sensory attributes, and the action attributes. Sensory attributes are the basic perception attributes, other perception attributes are constructed on the basis of the sensory ones. Actions are activated when their guards, being often complex and vague concepts, are satisfied to a satisfactory degree. The guards can be approximated on the basis of measurements performed by sensory attributes rather than defined exactly. Satisfiability degrees for guards are results of reasoning called the adaptive judgment. The approximations are induced using hierarchical modeling. We show that information systems can be used for modeling more advanced forms of interactions in hierarchical modeling. The role of hierarchical interactions is emphasized in the modeling of interactive computations. Some illustrative examples of interactions used in the ACT-R 6.0 system are reported. ACT-R 6.0 is based on a cognitive architecture and can be treated as an example of a highly interactive complex granule which can be involved in hierarchical interactions. For modeling of interactive computations, we propose much more general information systems than the studied dynamic information systems (see, e.g., Ciucci (2010) [8] and Pa\u0142asi\u0144ski and Pancerz (2010)\u00a0 [32] ). For example, the dynamic information systems are making it possible to consider incremental changes in information systems. However, they do not contain the perception and action attributes necessary for modeling interactive computations, in particular for modeling intrastep interactions.\"",
        "title: \"Information Granule Decomposition\" with abstract: \"Information sources provide us with granules of information that must be transformed, analyzed and built into structures that support problem solving. One of the main goals of information granule calculi is to develop algorithmic methods for construction of complex information granules from elementary ones by means of available operations and inclusion (closeness) measures. These constructed complex granules represent a form of information fusion. Such granules should satisfy some constraints like quality criteria or/and degrees of granule inclusion in (closeness to) a given information granule. Information granule decomposition methods are important components of those methods. We discuss some information granule decomposition methods.\"",
        "title: \"Two Families of Classification Algorithms\" with abstract: \"In the paper, two families of lazy classification algorithms of polynomial time complexity are considered. These algorithms are based on ordinary and inhibitory rules, but the direct generation of rules is not required. Instead of this, the considered algorithms extract efficiently for a new object some information on the set of rules which is next used by a decision-making procedure.\"",
        "title: \"Searching for Relational Patterns in Data\" with abstract: \" . We consider several basic classes of tolerance relations amongobjects. These (global) relations are defined from some predefined similaritymeasures on values of attributes. A tolerance relation in a givenclass of tolerance relations is optimal with respect to a given decisiontable A if it contains only pairs of objects with the same decision andthe number of such pairs contained in the relation is maximal among allrelations from the class. We present a method for (sub-)optimal... \"",
        "1 is \"Rough sets and bayes factor\", 2 is \"Granular Computing: Examples, Intuitions And Modeling\".",
        "\nGiven above information, for an author who has written the paper with the title \"Calculi of Granules Based on Rough Set Theory: Approximate Distributed Synthesis and Granular Semantics for Computing with Words\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01129": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Adaptively Secure Identity-Based Broadcast Encryption With a Constant-Sized Ciphertext':",
        "title: \"On the internal structure of ALPHA-MAC\" with abstract: \"Alpha-MAC is a MAC function which uses the building blocks of AES. This paper studies the internal structure of this new design. First, we provide a method to find second preimages based on the assumption that a key or an intermediate value is known. The proposed searching algorithm exploits the algebraic properties of the underlying block cipher and needs to solve eight groups of linear functions to find a second preimage. Second, we show that our idea can also be used to find internal collisions under the same assumption. We do not make any claims that those findings in any way endanger the security of this MAC function. Our contribution is showing how algebraic properties of AES can be used for analysis of this MAC function.\"",
        "title: \"Counting techniques specifying the existence of submatrices in weighing matrices\" with abstract: \"Two algorithmic techniques for specifying the existence of a k \u00d7 k submatrix with elements 0,\u00b11 in a skew and symmetric conference matrix of order n are described. This specification is achieved using an appropriate computer algebra system.\"",
        "title: \"Authentication via Multi-Service Tickets in the Kuperee Server\" with abstract: \"The subject of this paper is the authentication services as found in the Kuperee server. The authentication protocol is based on the Zheng-Seberry public key cryptosystem, and makes use of the distinct features of the cryptosystem. Although couched in the terminology of Kerberos, the protocol has subtle features, such as the binding together of two entities by a third entity, leading to the need of equal co-operation by the two entities in order to complete the authentication procedure. Another important feature is the use of a multi-service ticket to access multiple services offered by different servers. This removes the need of the Client to consult the Trusted Authority each time it needs a service from a Server. In addition, this allows an increased level of parallelism in which several Servers may be concurrently executing applications on behalf of a single Client. The scheme is also extendible to cover a more global scenario in which several realms exist, each under the care of a trusted authority. Finally, the algorithms that implement the scheme are presented in terms of the underlying cryptosystem. Although the scheme currently employs a public key cryptosystem, future developments of the server may combine private key cryptosystems to enhance performance.\"",
        "title: \"A New and Efficient Fail-stop Signature Scheme\" with abstract: \"The security of ordinary digital signature schemes relies on a computational assumption. Fail-stop signature schemes provide security for a sender against a forger with unlimited computational power by enabling the sender to provide a proof of forgery if it occurs. In this paper we give an efficient fail-stop signature scheme that uses two hard problems, discrete logarithm and factorization, as th...\"",
        "title: \"Some remarks on Hadamard matrices\" with abstract: \"In this note we use combinatorial methods to show that the unique, up to equivalence, 5 \u81335 (1,\u9a74\u9a74\u9a741)-matrix with determinant 48, the unique, up to equivalence, 6 \u81336 (1,\u9a74\u9a74\u9a741)-matrix with determinant 160, and the unique, up to equivalence, 7 \u81337 (1,\u9a74\u9a74\u9a741)-matrix with determinant 576, all cannot be embedded in the Hadamard matrix of order 8. We also review some properties of Sylvester Hadamard matrices, their Smith Normal Forms, and pivot patterns of Hadamard matrices when Gaussian Elimination with complete pivoting is applied on them. The pivot values which appear reconfirm the above non-embedding results.\"",
        "1 is \"Computation of discrete logarithms in prime fields\", 2 is \"New Approaches for Deniable Authentication\".",
        "\nGiven above information, for an author who has written the paper with the title \"Adaptively Secure Identity-Based Broadcast Encryption With a Constant-Sized Ciphertext\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01130": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Deep Co-attention based Comparators For Relative Representation Learning in Person Re-identification.':",
        "title: \"Adversarial Examples for Hamming Space Search.\" with abstract: \"Due to its strong representation learning ability and its facilitation of joint learning for representation and hash codes, deep learning-to-hash has achieved promising results and is becoming increasingly popular for the large-scale approximate nearest neighbor search. However, recent studies highlight the vulnerability of deep image classifiers to adversarial examples; this also introduces profound security concerns for deep retrieval systems. Accordingly, in order to study the robustness of modern deep hashing models to adversarial perturbations, we propose hash adversary generation (HAG), a novel method of crafting adversarial examples for Hamming space search. The main goal of HAG is to generate imperceptibly perturbed examples as queries, whose nearest neighbors from a targeted hashing model are semantically irrelevant to the original queries. Extensive experiments prove that HAG can successfully craft adversarial examples with small perturbations to mislead targeted hashing models. The transferability of these perturbations under a variety of settings is also verified. Moreover, by combining heterogeneous perturbations, we further provide a simple yet effective method of constructing adversarial examples for black-box attacks.\"",
        "title: \"Local Coordinates Alignment (Lca): A Novel Manifold Learning Approach\" with abstract: \"Manifold learning has been demonstrated as an effective way to represent intrinsic geometrical structure of samples. In this paper, a new manifold learning approach, named Local Coordinates Alignment (LCA), is developed based on the alignment technique. LCA first obtains local coordinates as representations of local neighborhood by preserving proximity relations on a patch, which is Euclidean. Then, these extracted local coordinates are aligned to yield the global embeddings. To solve the out of sample problem, linearization of LCA (LLCA) is proposed. In addition, in order to solve the non-Euclidean problem in real world data when building the locality, kernel techniques are utilized to represent similarity of the pairwise points on a local patch. Empirical studies on both synthetic data and face image sets show effectiveness of the developed approaches.\"",
        "title: \"General tensor discriminant analysis and gabor features for gait recognition.\" with abstract: \"The traditional image representations are not suited to conventional classification methods, such as the linear discriminant analysis (LDA), because of the under sample problem (USP): the dimensionality of the feature space is much higher than the number of training samples. Motivated by the successes of the two dimensional LDA (2DLDA) for face recognition, we develop a general tensor discriminant analysis (GTDA) as a preprocessing step for LDA. The benefits of GTDA compared with existing preprocessing methods, e.g., principal component analysis (PCA) and 2DLDA, include 1) the USP is reduced in subsequent classification by, for example, LDA; 2) the discriminative information in the training tensors is preserved; and 3) GTDA provides stable recognition rates because the alternating projection optimization algorithm to obtain a solution of GTDA converges, while that of 2DLDA does not. We use human gait recognition to validate the proposed GTDA. The averaged gait images are utilized for gait representation. Given the popularity of Gabor function based image decompositions for image understanding and object recognition, we develop three different Gabor function based image representations: 1) the GaborD representation is the sum of Gabor filter responses over directions, 2) GaborS is the sum of Gabor filter responses over scales, and 3) GaborSD is the sum of Gabor filter responses over scales and directions. The GaborD, GaborS and GaborSD representations are applied to the problem of recognizing people from their averaged gait images.A large number of experiments were carried out to evaluate the effectiveness (recognition rate) of gait recognition based on first obtaining a Gabor, GaborD, GaborS or GaborSD image representation, then using GDTA to extract features and finally using LDA for classification. The proposed methods achieved good performance for gait recognition based on image sequences from the USF HumanID Database. Experimental comparisons are made with nine state of the art classification methods in gait recognition.\"",
        "title: \"Tracking Using Multilevel Quantizations\" with abstract: \"Most object tracking methods only exploit a single quantization of an image space: pixels, superpixels, or bounding boxes, each of which has advantages and disadvantages. It is highly unlikely that a common optimal quantization level, suitable for tracking all objects in all environments, exists. We therefore propose a hierarchical appearance representation model for tracking, based on a graphical model that exploits shared information across multiple quantization levels. The tracker aims to find the most possible position of the target by jointly classifying the pixels and superpixels and obtaining the best configuration across all levels. The motion of the bounding box is taken into consideration, while Online Random Forests are used to provide pixel-and superpixel-level quantizations and progressively updated on-the-fly. By appropriately considering the multilevel quantizations, our tracker exhibits not only excellent performance in non-rigid object deformation handling, but also its robustness to occlusions. A quantitative evaluation is conducted on two benchmark datasets: a non-rigid object tracking dataset (11 sequences) and the CVPR2013 tracking benchmark (50 sequences). Experimental results show that our tracker overcomes various tracking challenges and is superior to a number of other popular tracking methods.\"",
        "title: \"Learning User'S Perception Using Region-Based Svm For Content-Based Image Retrieval\" with abstract: \"Relevance feedback is often a critical component for content-based image retrieval to capture the user's perception. Previous methods for image retrieval with relevance feedback are image-based. In this paper, we propose a novel region-based retrieval method with relevance feedback using the support vector machine (SVM) algorithm. A new region-based image model is given to represent an image, with region-based low-level features defined. A novel distance measure based on SVM is proposed to achieve better image retrieval performance. Experimental results are given to show that our method performs better than the image-based retrieval scheme with relevance feedback using SVM.\"",
        "1 is \"A Dirichlet Process Mixture Model for Spherical Data.\", 2 is \"Direct Importance Estimation with Model Selection and Its Application to Covariate Shift Adaptation\".",
        "\nGiven above information, for an author who has written the paper with the title \"Deep Co-attention based Comparators For Relative Representation Learning in Person Re-identification.\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01131": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Data Management in the Worldwide Sensor Web':",
        "title: \"Event queries on correlated probabilistic streams\" with abstract: \"A major problem in detecting events in streams of data is that the data can be imprecise (e.g. RFID data). However, current state-ofthe-art event detection systems such as Cayuga [14], SASE [46] or SnoopIB[1], assume the data is precise. Noise in the data can be captured using techniques such as hidden Markov models. Inference on these models creates streams of probabilistic events which cannot be directly queried by existing systems. To address this challenge we propose Lahar1, an event processing system for probabilistic event streams. By exploiting the probabilistic nature of the data, Lahar yields a much higher recall and precision than deterministic techniques operating over only the most probable tuples. By using a novel static analysis and novel algorithms, Lahar processes data orders of magnitude more efficiently than a na\u00efve approach based on sampling. In this paper, we present Lahar's static analysis and core algorithms. We demonstrate the quality and performance of our approach through experiments with our prototype implementation and comparisons with alternate methods.\"",
        "title: \"Cascadia: A System for Specifying, Detecting, and Managing RFID Events\" with abstract: \"Cascadia is a system that provides RFID-based pervasive computing applications with an infrastructure for specifying, extracting and managing meaningful high-level events from raw RFID data. Cascadia provides three important services. First, it allows application developers and even users to specify events using either a declarative query language or an intuitive visual language based on direct manipulation. Second, it provides an API that facilitates the development of applications which rely on RFID-based events. Third, it automatically detects the specified events, forwards them to registered applications and stores them for later use (e.g., for historical queries). We present the design and implementation of Cascadia along with an evaluation that includes both a user study and measurements on traces collected in a building-wide RFID deployment. To demonstrate how Cascadia facilitates application development, we built a simple digital diary application in the form of a calendar that populates itself with RFID-based events. Cascadia copes with ambiguous RFID data and limitations in an RFID deployment by transforming RFID readings into probabilistic events. We show that this approach outperforms deterministic event detection techniques while avoiding the need to specify and train sophisticated models.\"",
        "title: \"Systems aspects of probabilistic data management\" with abstract: \"There has been a wide interest recently in managing probabilistic data [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]. But in order to follow the rich literature on probabilistic databases one is often required to take a detour into probability theory, correlations, conditionals, Monte Carlo simulations, error bounds, topics that have been studied extensively in several areas of Computer Science and Mathematics. Because of that, it is often difficult to get to the algorithmic and systems level aspects of probabilistic data management. In this tutorial, we will distill these aspects from the, often theory-heavy literature on probabilistic databases. We will start by describing a real application at the University of Washington, using the RFID Ecosystem; we will show how probabilities arise naturally, and why we need to cope with them. We will then describe what an implementor needs to know to process SQL queries on probabilistic databases. In the second half of the tutorial, we will discuss more advanced issues, such as event processing over probabilistic streams, and views over probabilistic data.\"",
        "title: \"SnipSuggest: context-aware autocompletion for SQL\" with abstract: \"In this paper, we present SnipSuggest, a system that provides on-the-go, context-aware assistance in the SQL composition process. SnipSuggest aims to help the increasing population of non-expert database users, who need to perform complex analysis on their large-scale datasets, but have difficulty writing SQL queries. As a user types a query, SnipSuggest recommends possible additions to various clauses in the query using relevant snippets collected from a log of past queries. SnipSuggest's current capabilities include suggesting tables, views, and table-valued functions in the FROM clause, columns in the SELECT clause, predicates in the WHERE clause, columns in the GROUP BY clause, aggregates, and some support for sub-queries. SnipSuggest adjusts its recommendations according to the context: as the user writes more of the query, it is able to provide more accurate suggestions. We evaluate SnipSuggest over two query logs: one from an undergraduate database class and another from the Sloan Digital Sky Survey database. We show that SnipSuggest is able to recommend useful snippets with up to 93.7% average precision, at interactive speed. We also show that SnipSuggest outperforms na\u00efve approaches, such as recommending popular snippets.\"",
        "title: \"Efficient iterative processing in the SciDB parallel array engine\" with abstract: \"Many scientific data-intensive applications perform iterative computations on array data. There exist multiple engines specialized for array processing. These engines efficiently support various types of operations, but none includes native support for iterative processing. In this paper, we develop a model for iterative array computations and a series of optimizations. We evaluate the benefits of an optimized, native support for iterative array processing on the SciDB engine and real workloads from the astronomy domain.\"",
        "1 is \"Granularity hierarchies in concurrency control\", 2 is \"Emerging trends in the enterprise data analytics: connecting Hadoop and DB2 warehouse\".",
        "\nGiven above information, for an author who has written the paper with the title \"Data Management in the Worldwide Sensor Web\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01132": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Content modeling using latent permutations':",
        "title: \"INS/Twine: A Scalable Peer-to-Peer Architecture for Intentional Resource Discovery\" with abstract: \"The decreasing cost of computing technology is speeding the deployment of abundant ubiquitouscomputation and communication. With increasingly large and dynamic computing environments comes the challenge of scalable resource discovery, where client applications search for resources (services, devices, etc.) on the network by describing some attributesof what they are looking for. This is normally achieved through directory services (also called resolvers), which store resource information and resolve queries. This paper describes the design, implementation, and evaluation of INS/Twine, an approach to scalable intentional resource discovery, where resolvers collaborate as peers to distribute resource information and to resolve queries. Our system maps resources to resolvers by transforming descriptions into numeric keys in a manner that preserves their expressiveness, facilitates even data distribution and enables efficient query resolution. Additionally, INS/Twine handles resource and resolver dynamism by treating all data as soft-state.\"",
        "title: \"Tie strength in question & answer on social network sites\" with abstract: \"Asking friends, colleagues, or other trusted people to help answer a question or find information is a familiar and tried-and-true concept. Widespread use of online social networks has made social information seeking easier, and has provided researchers with opportunities to better observe this process. In this paper, we relate question answering to tie strength, a metric drawn from sociology describing how close a friendship is. We present a study evaluating the role of tie strength in question answers. We used previous research on tie strength in social media to generate tie strength information between participants and their answering friends, and asked them for feedback about the value of answers across several dimensions. While sociological studies have indicated that weak ties are able to provide better information, our findings are significant in that weak ties do not have this effect, and stronger ties (close friends) provide a subtle increase in information that contributes more to participants' overall knowledge, and is less likely to have been seen before.\"",
        "title: \"The role of context in question answering systems\" with abstract: \"Despite recent advances in natural language question an-swering technology, the problem of designing effective user interfaces has been largely unexplored. We conducted a user study to investigate the problem and discovered that overall, users prefer a paragraph-sized chunk of text over just an exact phrase as the answer to their questions. Fur-thermore, users generally prefer answers embedded in con-text, regardless of the perceived reliability of the source documents. When users research a topic, increasing the amount of text returned to users significantly decreases the number of queries that they pose to the system, suggesting that users utilize supporting text to answer related ques-tions. We believe that these results can serve to guide future developments in question answering user interfaces.\"",
        "title: \"Finding nearest neighbors in growth-restricted metrics\" with abstract: \"Most research on nearest neighbor algorithms in the literature has been focused on the Euclidean case. In many practical search problems however, the underlying metric is non-Euclidean. Nearest neighbor algorithms for general metric spaces are quite weak, which motivates a search for other classes of metric spaces that can be tractably searched.In this paper, we develop an efficient dynamic data structure for nearest neighbor queries in growth-constrained metrics. These metrics satisfy the property that for any point q and number r the ratio between numbers of points in balls of radius 2r and r is bounded by a constant. Spaces of this kind may occur in networking applications, such as the Internet or Peer-to-peer networks, and vector quantization applications, where feature vectors fall into low-dimensional manifolds within high-dimensional vector spaces.\"",
        "title: \"Better random sampling algorithms for flows in undirected graphs\" with abstract: \"No abstract available.\n\n\"",
        "1 is \"Navigational Exploration and Declarative Queries in a Prototype for Visual Information Systems\", 2 is \"Answering Clinical Questions with Knowledge-Based and Statistical Techniques\".",
        "\nGiven above information, for an author who has written the paper with the title \"Content modeling using latent permutations\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01133": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Flexible attribute-based encryption applicable to secure e-healthcare records':",
        "title: \"Rule protection for indirect discrimination prevention in data mining\" with abstract: \"Services in the information society allow automatically and routinely collecting large amounts of data. Those data are often used to train classification rules in view of making automated decisions, like loan granting/denial, insurance premium computation, etc. If the training datasets are biased in what regards sensitive attributes like gender, race, religion, etc., discriminatory decisions may ensue. Direct discrimination occurs when decisions are made based on biased sensitive attributes. Indirect discrimination occurs when decisions are made based on non-sensitive attributes which are strongly correlated with biased sensitive attributes. This paper discusses how to clean training datasets and outsourced datasets in such a way that legitimate classification rules can still be extracted but indirectly discriminating rules cannot.\"",
        "title: \"Secure network bootstrapping: an algorithm for authentic key exchange and digital signatures\" with abstract: \"A new method is presented which enables key exchange without any previous secret agreement; a variant of it yields a signature scheme. This method requires less interactive computation then usual public key protocols, and also provides secrecy and authentication, so that its implementation is very attractive from a cost point of view.\"",
        "title: \"LHS-Based Hybrid Microdata vs Rank Swapping and Microaggregation for Numeric Microdata Protection\" with abstract: \"In previous work by Domingo-Ferrer et al., rank swapping and multivariate microaggregation has been identified as well-performing masking methods for microdata protection. Recently, Dandekar et al. proposed using synthetic microdata, as an option, in place of original data by using Latin hypercube sampling (LHS) technique. The LHS method focuses on mimicking univariate as well as multivariate statistical characteristics of original data. The LHS-based synthetic data does not allow one to one comparison with original data. This prevents estimating the overall information loss by using current measures. In this paper we utilize unique features of LHS method to create hybrid data sets and evaluate their performance relative to rank swapping and multivariate microaggregation using generalized information loss and disclosure risk measures.\"",
        "title: \"A polynomial-time approximation to optimal multivariate microaggregation\" with abstract: \"Microaggregation is a family of methods for statistical disclosure control (SDC) of microdata (records on individuals and/or companies), that is, for masking microdata so that they can be released without disclosing private information on the underlying individuals. Microaggregation techniques are currently being used by many statistical agencies. The principle of microaggregation is to group original database records into small aggregates prior to publication. Each aggregate should contain at least k records to prevent disclosure of individual information, where k is a constant value preset by the data protector. In addition to it being a good masking method, microaggregation has recently been shown useful to achieve k-anonymity. In k-anonymity, the parameter k specifies the maximum acceptable disclosure risk, so that, once a value for k has been selected, the only job left is to maximize data utility: if microaggregation is used to implement k-anonymity, maximizing utility can be achieved by microaggregating optimally, i.e. with minimum within-groups variability loss. Unfortunately, optimal microaggregation can only be computed in polynomial time for univariate data. For multivariate data, it has been shown to be NP-hard. We present in this paper a polynomial-time approximation to microaggregate multivariate numerical data for which bounds to optimal microaggregation can be derived at least for two different optimality criteria: minimum within-groups Euclidean distance and minimum within-groups sum of squares. Beyond the theoretical interest of being the first microaggregation proposal with proven approximation bounds for any k, our method is empirically shown to be comparable to the best available heuristics for multivariate microaggregation.\"",
        "title: \"From t-Closeness to PRAM and Noise Addition Via Information Theory\" with abstract: \"t-Closeness is a privacy model recently defined for data anonymization. A data set is said to satisfy t-closeness if, for each group of records sharing a combination of key attributes, the distance between the distribution of a confidential attribute in the group and the distribution of the attribute in the data is no more than a threshold t. We state here the t-closeness property in terms of information theory and then use the tools of that theory to show that t-closeness can be achieved by the PRAM masking method in the discrete case and by a form of noise addition in the general case.\"",
        "1 is \"Utility-preserving sanitization of semantically correlated terms in textual documents.\", 2 is \"EM analysis of rijndael and ECC on a wireless java-based PDA\".",
        "\nGiven above information, for an author who has written the paper with the title \"Flexible attribute-based encryption applicable to secure e-healthcare records\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01134": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Parameterized specification, configuration and execution of\u00a0data-intensive scientific workflows':",
        "title: \"Role of Sentiment in Message Propagation: Reply vs. Retweet Behavior in Political Communication\" with abstract: \"This paper examines the role of sentiment in information propagation. We make use of political communication in the Twitter space, and relate emotion expressions in a message to the degrees of responses generated by the message. We also compare differences between user reply vs. retweet behavior with respect to sentiment variables. The current results indicate that that degree of emotion expressions in twitter messages can affect the number of replies generated as well as retweet rates. Due to the difference in the nature of endorsement (retweet) vs. responses (replies or conversation), some of the variables present opposite roles in explaining the degree of responses the message receives. We expect these results will help generating a predictive model of message propagation.\"",
        "title: \"PedConnect: an intelligent assistant for teacher social networking\" with abstract: \"Social networking has gained immense traction in many areas, including teaching and learning. Networking sites for teachers aim to facilitate teacher communication and information sharing, but fall short of their potential. In order to support more effective use of online resources and better communication among teachers, we develop a suite of new user modeling and recommendation capabilities within a middle school teacher networking site. We foster collaboration among novice and experienced teachers when they share similar interests, enabling new mentoring relationships, and promote the use of relevant educational resources. We illustrate our approach with an implemented system called PedConnect that analyzes user activities and presents intelligent suggestions for collaboration and resource use.\"",
        "title: \"Scaffolding On-Line Discussions with Past Discussions: An Analysis and Pilot Study of PedaBot\" with abstract: \"PedaBot is a new discussion scaffolding application designed to aid student knowledge acquisition, promote reflection about course topics and encourage student participation in discussions. It dynamically processes student discussions and presents related discussions from a knowledge base of past discussions. This paper describes the system and presents a comparative analysis of the information retrieval techniques used to respond to free-form student discussions, a combination of topic profiling, term frequency-inverse document frequency, and latent semantic analysis. Responses are presented as annotated links that students can follow and rate. We report a pilot study of PedaBot based on student viewings, student ratings, and a small survey. Initial results indicate that there is a high level of student interest in the feature and that its responses are moderately relevant to student discussions.\"",
        "title: \"An integrated environment for knowledge acquisition\" with abstract: \"This paper describes an integrated acquisition interface that includes several techniques previously developed to support users in various ways as they add new knowledge to an intelligent system. As a result of this integration, the individual techniques can take better advantage of the context in which they are invoked and provide stronger guidance to users. We describe the current implementation using examples from a travel planning domain, and demonstrate how users can add complex knowledge to the system.\"",
        "title: \"Constraining Learning with Search Control\" with abstract: \"Many learning systems must confront the problem of run time after learning being greater than run time before learning. This utility problem has been a particular focus of research in explanation-based learning. In past work we have examined an approach to the utility problem that is based on restricting the expressiveness of the rule language so as to guarantee polynomial bounds on the cost of using learned rules. In this article we propose a new approach that limits the cost of learned rules without guaranteeing an a priori bound on the match process or restricting the expressibility of rule conditions. By making the learning mechanism sensitive to the control knowledge utilized during the problem solving that led to the creation of the new rule \u2014 i.e., by incorporating such control knowledge into the explanation - the cost of using the learned rule becomes bounded by the cost of the problem solving from which it was learned.\"",
        "1 is \"Using bayesian priors to combine classifiers for adaptive filtering\", 2 is \"Shared Memory Programming in Metacomputing Environments: The Global Array Approach\".",
        "\nGiven above information, for an author who has written the paper with the title \"Parameterized specification, configuration and execution of\u00a0data-intensive scientific workflows\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01135": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Using Sacks to Organize Registers in VLIW Machines':",
        "title: \"A case for resource-conscious out-of-order processors: towards kilo-instruction in-flight processors\" with abstract: \"Modern out-of-order processors tolerate long-latency memory operations by supporting a large number of in-flight instructions. This is achieved in part through proper sizing of critical resources, such as register files or instruction queues. In light of the increasing gap between processor speed and memory latency, tolerating upcoming latencies in this way would require impractical sizes of such critical resources.To tackle this scalability problem, we make a case for resource-conscious out-of-order processors. We present quantitative evidence that critical resources are increasingly underutilized in these processors. We advocate that better use of such resources should be a priority in future research in processor architectures. In particular, we present some of our research having such observations as a basis to deal with future resource conscious processors.\n\n\"",
        "title: \"Simultaneous multithreaded vector architecture: merging ILP and DLP for high performance.\" with abstract: \"The goal of this paper is to show that instructionlevel parallelism (ILP) and data-level parallelism(DLP) can be merged in a single simultaneous vectormultithreaded architecture to execute regular vectorizablecode at a performance level that can not be achieved using either paradigm on its own.We willshow that the combination of the two techniques yieldsvery high performance at a low cost and alow complexity:We will show that this architecture achievesa sustained performance on numerical regular codesthat is 20 times the performance that can be achievedwith today's superscalar microprocessors.Moreover,we will show that the architecture can tolerate verylarge memory latencies, of up to a 100 cycles, witha relatively small performance degradation.This highperformance is independent of working set size or oflocality considerations, since the DLP paradigm allowsvery efficient exploitation of a high performance flatmemory bandwidth.\"",
        "title: \"An optimized front-end physical register file with banking and writeback filtering\" with abstract: \"Register file design is one of the critical issues facing designers of out\u2013of\u2013order processors. Scaling up its size and number of ports with issue width and instruction window size is difficult in terms of both performance and power consumption. Two types of register file architectures have been proposed in the past: a future logical file and a centralized physical file. The centralized register file does not scale well but allows fast branch mis\u2013prediction recovery. The Future File scales well, but requires reservation stations and has slow mis\u2013prediction recovery. This paper proposes a register file architecture that combines the best features of both approaches. The new register file has the large size of the centralized file and its ability to quickly recover from branch misprediction. It has the advantage of the future file in that it is accessed in the \u201dfront end\u201d allowing about 1/3rd of the source operands that are ready when an instruction enters the window to be read immediately. The remaining operands come from bypass logic / instruction queues and do not require register file access. The new architecture does require reservation stations for operand storage and it investigates two approaches in terms of power\u2013efficiency. Another advantage of the new architecture is that banking is much easier to use in this case as compared to the centralized register file. Banking further improves the scalability of the new architecture. A technique for early release of short\u2013lived registers called writeback filtering is used in combination with banking to further improve the new architecture. The use of a large front\u2013end register file results in significant power savings and a slight IPC degradation (less than 1%). Overall, the resulting energy\u2013delay product is lower than in previous proposals.\"",
        "title: \"Microarchitectural Support for Speculative Register Renaming\" with abstract: \"This paper proposes and evaluates a new microarchitec- ture for out-of-order processors that supports speculative renaming. We call speculative renaming to the speculative omission of physical register allocation along with the speculative early release of physical registers. These renaming policies may cause a register operand not to be kept in the Physical Register File (PRF). Thus, we add a low-ported Auxiliary Register File (XRF) located outside the processor core that keeps the values absent in PRF and supplies them at higher latency. To support the location of register operands being either in PRF or XRF, we use vir- tual registers. We consider omission and release policies directed by hardware prediction. Namely, we will use a sin- gle Last-Use Predictor that directs both speculative omis- sion and release. We call this mechanism SR-LUP (Speculative Renaming based on Last-Use Prediction). Two Last-Use predictor designs of incremental complexity and performance are analyzed. In a 256-ROB, 8-way processor with an 80int+80fp PRF, SR-LUP with an 11-port 256int+256fp XRF, speeds up computations up to 11.5% and 29% for INT and FP SPEC2K benchmarks, respec- tively. For FP benchmarks, if the PRF limits the clock fre- quency, a conventionally managed 128int+128fp PRF can be replaced using SR-LUP by a 64int+64fp PRF backed up with a 10-port 224int+224fp XRF, showing 19% IPS gain.\"",
        "title: \"Effective Usage of Vector Registers in Advanced Vector Architectures\" with abstract: \"This paper presents data confirming the fact that traditional vector architectures can not reduce their vector register length without suffering a severe performance penalty. However, we will show that by combining the vector register length reduction with two different ILP techniques, decoupling and multithreading, the performance penalty can be made very small. We will show that each resulting architecture tolerates very well long memory latencies and also makes a better usage of the available storage space in each vector register. Using decoupling and short vectors, each register can be halved while still providing speedups in the range 1.04--1.49 over a traditional architecture with long registers. Using multithreading, we split a vector register file in two halfs and show that two independent threads running on such machine can yield speedups in the range 1.23--1.29 . The paper also explores configurations with 1/4 and 1/8 the original vector register size aimed at cost-conscious designs, and shows that even at 1/4 the original size, the resulting architectures can outperform a traditional machine. We also present results across a wide range of memory latencies, and show that the combination of short vectors and ILP techniques results in a very good tolerance of slow memory systems.\"",
        "1 is \"Accelerating multicore reuse distance analysis with sampling and parallelization\", 2 is \"Network management by delegation - From research prototypes towards standards.\".",
        "\nGiven above information, for an author who has written the paper with the title \"Using Sacks to Organize Registers in VLIW Machines\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01136": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Solving language equations and disequations with applications to disunification in description logics and monadic set constraints':",
        "title: \"Linear grammars with one-sided contexts and their automaton representation.\" with abstract: \"The paper considers a family of formal grammars that extends linear context-free grammars with an operator for referring to the left context of a substring being defined, as well as with a conjunction operation (as in linear conjunctive grammars). These grammars are proved to be computationally equivalent to an extension of one-way real-time cellular automata with an extra data channel. The main result is the undecidability of the emptiness problem for grammars restricted to a one-symbol alphabet, which is proved by simulating a Turing machine by a cellular automaton with feedback. The same construction proves the Sigma(0)(2)-completeness of the finiteness problem for these grammars.\"",
        "title: \"Communication of two stacks and rewriting\" with abstract: \"Rewriting systems working on words with a center marker are considered. The derivation is done by erasing a prefix or a suffix and then adding a prefix or a suffix. This can be naturally viewed as two stacks communicating with each other according to a fixed protocol. The paper systematically considers different cases of these systems and determines their expressiveness. Several cases are identified where very limited communication surprisingly yields universal computation power\"",
        "title: \"Equations over Sets of Natural Numbers with Addition Only\" with abstract: \"Systems of equations of the form X = Y Z and X = C are considered, in which the unknowns are sets of natural numbers, \"+\" denotes pairwise sum of sets S +T = {m + n | m 2 S, n 2 T}, and C is an ultimately periodic constant. It is shown that such systems are computationally universal, in the sense that for every recursive (r.e., co-r.e.) set SN there exists a system with a unique (least, greatest) solution containing a component T with S = {n| 16n + 13 2 T}. This implies undecidability of basic properties of these equations. All results also apply to language equations over a one-letter alphabet with concatenation and regular constants.\"",
        "title: \"Top-Down Parsing of Conjunctive Languages\" with abstract: \" This paper generalizes the notion of a strong LL(k) context-free grammarfor the case of conjunctive grammars and develops a top-down parsing algorithmfor the resulting language family. \"",
        "title: \"State complexity of operations on input-driven pushdown automata\" with abstract: \"The family of deterministic input-driven pushdown automata (IDPDA; a.k.a. visibly pushdown automata, a.k.a. nested word automata) is known to be closed under reversal, concatenation and Kleene star. As shown by Alur and Madhusudan (\"Visibly pushdown languages\", STOC 2004), the reversal and the Kleene star of an n-state IDPDA can be represented by an IDPDA with 2O(n2) states, while concatenation of an m-state and an n-state IDPDA is represented by an IDPDA with 2O((m+n)2) states. This paper presents more efficient constructions for the reversal and for the Kleene star, which yield 2\u0398(n log n) states, as well as an m2\u0398(n log n)-state construction for the concatenation. These constructions are optimal due to the previously known matching lower bounds.\"",
        "1 is \"On real time one-way cellular array\", 2 is \"Unification of commutative terms\".",
        "\nGiven above information, for an author who has written the paper with the title \"Solving language equations and disequations with applications to disunification in description logics and monadic set constraints\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01137": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Combining linear-time temporal logic with constructiveness and paraconsistency':",
        "title: \"A decidable paraconsistent relevant logic: Gentzen system and Routley-Meyer semantics.\" with abstract: \"In this paper, the positive fragment of the logic RW of contraction-less relevant implication is extended with the addition of a paraconsistent negation connective similar to the strong negation connective in Nelson's paraconsistent four-valued logic N4. This extended relevant logic is called RWP, and it has the property of constructible falsity which is known to be a characteristic property of N4. A Gentzen-type sequent calculus SRWP for RWP is introduced, and the cut-elimination and decidability theorems for SRWP are proved. Two extended Routley-Meyer semantics are introduced for RWP, and the completeness theorems with respect to these semantics are proved. (C) 2016 WILEY-VCH Verlag GmbH & Co. KGaA, Weinheim\"",
        "title: \"Temporal BI: Proof system, semantics and translations.\" with abstract: \"The logic BI of bunched implications is a combination of intuitionistic logic and multiplicative intuitionistic linear logic. In this paper, a temporal extension tBI of BI is introduced and studied. A Gentzen-type sequent calculus GtBI for tBI is introduced, and the cut-elimination and decidability theorems for GtBI are proved using a theorem for syntactically embedding GtBI into a sequent calculus GBI for BI. A semantics for GtBI is introduced extending the Grothendieck topological semantics for BI, and the completeness theorem with respect to this semantics is proved using a theorem for semantically embedding tBI into BI. A semantics for GtBI without additive falsity constant is introduced extending the Kripke resource semantics for BI without additive falsity constant, and the completeness theorem with respect to this semantics is proved in a similar way. Moreover, an intuitionistic temporal linear logic, ITLL, is introduced as a Gentzen-type sequent calculus, and a theorem for embedding GtBI into ITLL is proved using a temporal extension of the Girard translation of intuitionistic logic into intuitionistic linear logic.\"",
        "title: \"Natural Deduction for Connexive Paraconsistent Quantum Logic\" with abstract: \"In this study, a new logic called the connexive paraconsistent quantum logic is introduced as a common denominator of a paraconsistent logic and a quantum logic. A natural deduction system for this logic is introduced, and the weak normalization theorem for this system is shown. A typed lambda calculus for the implication-negation fragment of this logic is developed on the basis of the Curry-Howard correspondence. The strong normalization theorem for this calculus is proved.\"",
        "title: \"Reasoning About Bounded Time Domain An Alternative To Np-Complete Fragments Of Ltl\" with abstract: \"It is known that linear-time temporal logic (LTL) is one of the most useful logics for reasoning about time and for verifying concurrent systems. It is also known that the satisfiability problem for LTL is PSPACE-complete and that finding NP-complete fragments of LTL is an important issue for constructing efficiently executable temporal logics. In this paper, an alternative NP-complete logic called bounded linear-time temporal logic is obtained from LTL by restricting the time domain of temporal operators.\"",
        "title: \"On temporal G\u00f6del-Gentzen translation\" with abstract: \"Temporal logics and their intuitionistic counterparts are of growing importance in Computer Science. These intuitionistic counterparts, called intuitionistic (or constructive) temporal logics, are known to be useful for formalizing functional programming. To show a clear relationship between temporal logics and their intuitionistic counterparts has thus been required. In this paper, a theorem for embedding first-order linear-time temporal logic into its intuitionistic counterpart is proved using Baratella-Masini's temporal extension of the G\u00f6del-Gentzen negative translation of classical logic into intuitionistic logic.\"",
        "1 is \"A logic of authentication\", 2 is \"Evolving competitive car controllers for racing games with neuroevolution\".",
        "\nGiven above information, for an author who has written the paper with the title \"Combining linear-time temporal logic with constructiveness and paraconsistency\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01138": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Anonymous authentication and secure communication protocol for wireless mobile ad hoc networks':",
        "title: \"Efficient convertible Limited Verifier Signatures\" with abstract: \"The notion of limited verifier signature (LVS) was first introduced by Araki et al. in 1999. It is a useful cryptographic primitive to limit the publicly verifiable property of ordinary digital signatures. In a LVS, the signature can be verified by a limited verifier. When necessary, the signer or the limited verifier can provide a proof to convince a designated verifier (eg., a judge) that the signer has indeed generated the signature. However, the judge cannot transfer this proof to convince any other party. Also, the LVS should be converted into an ordinary one for public verification if required. In this paper, we propose an efficient LVS scheme which is more efficient than previous proposed schemes. Based on the intractability of the Computational Diffie-Hellman (CDH) problem, we give the security proofs of the scheme in the random oracle model.\"",
        "title: \"A Sanitizing Signature Scheme with Indexing\" with abstract: \"A sanitizing signature scheme, which is a variant of digital signatures, enables a trusted party named sanitizer to modify parts of signed documents without corresponding to the signer. A lot of sanitizing signature schemes have been proposed since 2001. However these schemes are suitable for table-style documents, but none of them are considered for story-style documents. We consider that the solution to this problem is adding index to the sanitized document in the signing phase. Thus, a verifier can verify the validity of the document and understand the story in the help of the indexes, even though some messages have been masked. In this paper, we propose a new sanitizing signature scheme with indexing that realizes index-based verification and holds its privacy.\"",
        "title: \"Virtual certificates and synthetic certificates: new paradigms for improving public key validation\" with abstract: \"The certificate paradigm is applied recursively to obtain the public keys of a number of Certification Authorities and, accordingly, to obtain the public keys of a number of final entities. Thus, validation of the authorized public key of a party in a network transaction is commonly based on processing the certificate chain descended from a trusted root issuer, involving non-negligible time and cost. Those chains become long in communications between large organizations, which is the typical case of e-commerce and e-government applications. The process of validation of extensive chains introduces performance problems in two aspects: signature verification and revocation checking. That is, the repeated processing of long chains of certificates creates severe efficiency problems. This fact causes that most of the advantages provided by Public Key Infrastructures (PKIs) are not conveniently exploited. In this paper we analyze the scenarios in which large volumes of digitally signed transactions between commercial entities exist. These cases require of interoperation among PKIs. We show that solutions available in those scenarios still involve processing of too long chains of certificates, either at the receiving computer or by an outsourced entity. For this reason, we propose new concepts of virtual certificate and synthetic certificate for faster and less costly processing of certificate chains. In this way, communications in a certificate-based intercommunity can be highly improved. We also show how these types of certificates can be applied in practice.\"",
        "title: \"A trellis-coded chaotic modulation scheme\" with abstract: \"Recently, a chaotic modulation has been proposed and attracts many researchers' attention because of some properties such as noise-like signal, nonlinearity, and pseudo-periodicity. The chaotic modulation is usually treated as a secure transmission scheme rather than an efficient scheme in terms of channel noise. In this paper, to achieve a secure and high-performance transmission, we propose a trellis-based chaotic modulation scheme (called as chaotic-coded modulation: CCM). The coding gain is obtained by the combination of trellis and chaotic convolutions and an adaptive decoding algorithm is used for this coded modulation. Simulation results show that the decoding scheme has a tradeoff between the performance and the decoding complexity.\"",
        "title: \"Identifying Potentially-Impacted Area by Vulnerabilities in Networked Systems Using CVSS\" with abstract: \"CVSS (Common Vulnerability Scoring System) is a framework scoring IT vulnerabilities. CVSS is composed of three metric groups: Base, Temporal, and Environmental. Although, the environmental score which gives risk of vulnerabilities in network environment of each user should be used for prioritizing actions, only base score is currently used. One of the reason for unused of environmental score is hard to score uniquely, because the criterion for determining \u201dTarget Distribution (TD),\u201d which is a parameter indicating impacted proportion, is vague. We propose a method for identifying the potentially-impacted area enabling TD measurement in networked systems in terms of three security objectives: confidentiality, integrity and availability. We also apply the method to some model cases of networked systems, and assess their TD. The results correspond to a popular wisdom that trilayer structure is more secure.\"",
        "1 is \"QoS provisioning in wireless/mobile multimedia networks using an adaptive framework\", 2 is \"Practical multi-candidate election system\".",
        "\nGiven above information, for an author who has written the paper with the title \"Anonymous authentication and secure communication protocol for wireless mobile ad hoc networks\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01139": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'eFraudCom: An E-commerce Fraud Detection System via Competitive Graph Neural Networks':",
        "title: \"A workflow-centric study of organizational knowledge distribution\" with abstract: \"Abstract Organizations,require ,mechanisms ,to  efficiently distribute knowledge such as news releases, seminar announcements, and memos. While the machinery for information storage, manipulation, and retrieval exists, research dealing directly with knowledge,distribution in an organizationa l context is scarce. In this paper, we address this need by first examining,the pros and cons of the conventional,\u201cmailing lists\u201d approach,and,then proposing,new,workflow,mechanisms,that improve,the efficiency and effectiveness of knowledge,distribution. The main,contributions of this study include: (1) a workflow,perspective ,on  organizational  knowledge distribution, (2) workflow analysis of two new knowledge distribution methods,based on dynamic mailing lists and profile matching, respectively, and (3) a new way of matching,knowledge,supply and demand,that extends existing information filtering algorithms. 1.,Introduction Organizational knowledge,is a form,of collective\"",
        "title: \"Impact Of Service-Centric Computing On Business And Education\" with abstract: \"Service-centric computing is one of the new IT paradigms that are transforming the way corporations organize their information resources. However, research and teaching activities in the IS community are lagging behind the recent advances in the corporate world. This paper investigates the impact of service-centric computing on business and education. We first examine the transformative impacts of service-centric computing on business and education in the foreseeable future. Then, we discuss opportunities and challenges in new research directions and instructional innovations with respect to service-centric computing. We believe that this article will serve as a good starting point for our IS colleagues to explore this exciting and emerging area of research and teaching.\"",
        "title: \"An Extensible Workflow Architecture through Web Services\" with abstract: \"ABSTRACT The growing,availability of Web,services provides a new,opportunity for enabling new ways of system integration and interoperability, which are needed to meet the  needs  of  global  e-business.  In  this  study,  we  propose  an  extensible workflow,architecture based on Web,services and object-oriented techniques. The proposed architecture contains three components, the Open Kernel Framework, the Universal Resource Manager, and the Process Definition Publisher. Weshow,how this architecture can help extend workflow,systems with new features dynamically such as new workflow patterns, new process semantics, new workflow resources, and even new process languages. Keywords: adaptive workflow management; object-oriented frameworks; Web services,\"",
        "title: \"Can e-learning replace classroom learning?\" with abstract: \"In an e-learning environment that emphasizes learner-centered activity and system interactivity, remote learners can outperform traditional classroom students.\"",
        "title: \"Data management for multiuser access to digital video libraries\" with abstract: \"Abstract To support heterogeneous application types a video digital library will contain a large num - ber of video objects with various lengths and display requirements Multi - user access to the same video objects is required in order to increase the availability of video information and to make full use of the limited computing and storage resources The access frequency and delay sensitivity of video objects require special methods to guarantee smooth playback of video ob - jects and to minimize average waiting time We propose an integrated approach  to  bu er  and disk  management for  dynamic  loading  and  simultaneous  delivery  of  multiple video  objects  to multiple users The  allocation of  bu er  and disk  resources  in this  study  is  based  on  quality of service  variables such  as average waiting time, display continuity, and viewer enrollment\"",
        "1 is \"Structural Deep Network Embedding\", 2 is \"Future paths for integer programming and links to artificial intelligence\".",
        "\nGiven above information, for an author who has written the paper with the title \"eFraudCom: An E-commerce Fraud Detection System via Competitive Graph Neural Networks\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01140": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'On Homogeneous Semilattices and Their Automorphism Groups.':",
        "title: \"Effectively given information systems and domains\" with abstract: \"Without Abstract\"",
        "title: \"Asynchronous Cellular Automata for Pomsets Without Auto-concurrency\" with abstract: \"This paper extends to pomsets without auto-concurrency the fundamental notion of asynchronous cellular automata (ACA) which was originally introduced for traces by Zielonka. We generalize to pomsets the notion of asynchronous mapping introduced by Zielonka and we show how to construct a deterministic ACA from an asynchronous mapping. Our main result generalizes B\u00fcchi's theorem for a class of pomsets without auto-concurrency which satisfy a natural axiom. This axiom ensures that an asynchronous cellular automaton works on the pomset as a concurrent read owner write machine. More precisely, we prove the equivalence between non deterministic ACA, deterministic ACA and monadic second order logic for this class of pomsets.\"",
        "title: \"A Kleene Theorem for Weighted Tree Automata\" with abstract: \"In this paper we prove Kleene\u2019s result for formal tree series over a commutative semiring A (which is not necessarily complete or continuous or idempotent), i.e., the class of formal tree series over A which are accepted by weighted tree automata, and the class of rational tree series over A are equal. We show the result by direct automata-theoretic constructions and prove their correctness.\"",
        "title: \"Construction of Some Uncountable 2-Arc-Transitive Bipartite Graphs\" with abstract: \"We give various constructions of uncountable arc-transitive bipartite graphs employing techniques from partial orders, starting\n with the cycle-free case, but generalizing to cases where this may be violated.\"",
        "title: \"Regular expressions on average and in the long run\" with abstract: \"Quantitative aspects of systems like consumption of resources, output of goods, or reliability can be modeled by weighted automata. Recently, objectives like the average cost or the longtime peak power consumption of a system have been modeled by weighted automata which are not semiring weighted anymore. Instead, operations like limit superior, limit average, or discounting are used to determine the behavior of these automata. Here, we introduce a new class of weight structures subsuming a range of these models as well as semirings. Our main result shows that such weighted automata and Kleene-type regular expressions are expressively equivalent both for finite and infinite words.\"",
        "1 is \"The algorithmic analysis of hybrid systems\", 2 is \"Categories of embeddings\".",
        "\nGiven above information, for an author who has written the paper with the title \"On Homogeneous Semilattices and Their Automorphism Groups.\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01141": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Dynamic Routing of Restorable QoS Connections in MPLS Networks':",
        "title: \"Viewing YouTube from a metropolitan area: What do users accessing from residential ISPs experience?\" with abstract: \"In this work, we perform a controlled study on the perceived experience of viewing YouTube videos as observed from the end users' point of view through their residential ISPs in a metropolitan area. This study is conducted using our tool Pytomo, which we developed to emulate the end users' experience of viewing YouTube videos. Pytomo crawls and downloads YouTube videos to collect a number of measures, including information about the YouTube servers that are delivering them. This open-source tool was provided to a group of volunteers located in the Kansas City metropolitan area. These volunteers, who use different residential ISPs to access the Internet, were instructed to synchronously run the tool to collect the measurement data. Based on the data collected over specific time windows (separated by three months), we observed that there is a noticeable difference in the quality of experience depending on the residential ISPs. Furthermore, the content distribution policies for YouTube, for different residential ISPs vary and the round trip time is not the primary factor for choosing video servers.\"",
        "title: \"SDNIPS: Enabling Software-Defined Networking based intrusion prevention system in clouds\" with abstract: \"Security has been considered as one of the top concerns in clouds. Intrusion Detection and Prevention Systems (IDPS) have been widely deployed to enhance the cloud security. Using Software-Defined Networking (SDN) approaches to enhance the system security in clouds has been recently presented in [1], [2]. However, none of existing works established a comprehensive IPS solution to reconfigure the cloud networking environment on-the-fly to counter malicious attacks. In this paper, we present an SDN-based IPS solution called SDNIPS that is a full lifecycle solution including detection and prevention in the cloud. We propose a new IDPS architecture based on Snort-based IDS and Open vSwitch (OVS). We also compare the SDN-based IPS solution with the traditional IPS approach from both mechanism analysis and evaluation. Network Reconfiguration (NR) features are designed and implemented based on the POX controller to enhance the prevention flexibility. Finally, evaluations of SDNIPS demonstrate its feasibility and efficiency over traditional approaches.\"",
        "title: \"Impact of Ethernet Multipath Routing on Data Center Network Consolidations\" with abstract: \"With the advent of network virtualization, data center networking is reaching a high level of management complexity. Indeed, interconnection networks in data center networks (DCN) are no longer just based on flat over-provisioned pipes, but are increasingly facing traffic engineering (TE) issues that commonly characterize long-haul provider networks. TE objectives, however, are opposite to energy efficiency (EE) objectives commonly chased by virtual machine (VM) consolidations. Moreover, the specific topologies of DCNs and the systematic use of multipath forwarding make the joint TE and VM consolidation optimization complex. The contribution of this paper is twofold. First, we propose a repeated matching heuristic for the DCN optimization problem with multipath capabilities, which also scales well for large topologies without discarding both TE and EE objectives. Second, we assess the impact of multipath forwarding on TE and EE goals. Extensive simulations show us that multipath forwarding is beneficial only when EE is not the primary goal in network-aware VM consolidations, and that it can be counterproductive when instead the EE is the primary goal of such optimizations.\"",
        "title: \"A Network Optimization Model for Multi-layer IP/MPLS over OTN/DWDM Networks\" with abstract: \"The operational model for large Internet service providers is moving to a multi-layer architecture consisting of IP/MPLS coupled with OTN/DWDM. While there has been significant work on multi-layer networks, the explicit modeling in IP/MPLS over OTN/DWDM has not been addressed before. In this paper, we present a detailed network optimization model for the operational planning of such an environment that considers OTN as a distinct layer with defined restrictions.\"",
        "title: \"Optimal standby virtual routers selection for node failures in a virtual network environment\" with abstract: \"Network virtualization allows flexibility to configure virtual networks in a dynamic manner. In such a setting, to provide resilient services to virtual networks, we consider the situation where the substrate network provider wants to have standby virtual routers ready to serve the virtual networks in the event of a failure. Such a failure can affect one or more virtual routers in multiple virtual networks. The goal of our work is to make the optimal selection of standby virtual routers so that virtual networks can be dynamically reconfigured back to their original topologies after a failure. We present an optimization formulation and a heuristic for this problem. By considering a number of factors, we present numerical studies to show how the optimal selection is affected. The results show that the proposed heuristic's performance was close to the optimization model when there were sufficient standby virtual routers for each virtual network and the substrate nodes have the capability to support multiple standby virtual routers to be in service, concurrently.\"",
        "1 is \"On the feasibility of real-time phone-to-phone 3D localization\", 2 is \"Improving MapReduce performance in heterogeneous environments\".",
        "\nGiven above information, for an author who has written the paper with the title \"Dynamic Routing of Restorable QoS Connections in MPLS Networks\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01142": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Provably good global buffering using an available buffer block plan':",
        "title: \"Distance approximating trees: complexity and algorithms\" with abstract: \"Let \u0394\u2265 1 and \u03b4\u2265 0 be real numbers. A tree T=(V,E\u2032) is a distance (\u0394,\u03b4)\u2013approximating tree of a graph G=(V,E) if dH(u,v)\u2264\u0394 dG(u,v)+\u03b4 and dG(u,v)\u2264\u0394 dH(u,v)+\u03b4 hold for every u,v\u2208 V. The distance (\u0394,\u03b4)-approximating tree problem asks for a given graph G to decide whether G has a distance (\u0394,\u03b4)-approximating tree. In this paper, we consider unweighted graphs and show that the distance (\u0394,0)-approximating tree problem is NP-complete for any \u0394\u2265 5 and the distance (1,1)-approximating tree problem is polynomial time solvable.\"",
        "title: \"Tree Spanners on Chordal Graphs: Complexity, Algorithms, Open Problems\" with abstract: \"A tree t-spanner T in a graph G is a spanning tree of G such that the distance in T between every pair of vertices is at most t times their distance in G. The TREE t-SPANNER problem asks whether a graph admits a tree t-spanner, given t. We substantially strengthen the hardness result of Cai and Corneil [SIAM J. Discrete Math. 8 (1995) 359- 387] by showing that, for any t \u9a74 4, TREE t-SPANNER is NP-complete even on chordal graphs of diameter at most t + 1 (if t is even), respectively, at most t + 2 (if t is odd).Then we point out that every chordal graph of diameter at most t - 1 (respectively, t - 2) admits a tree t-spanner whenever t \u9a74 2 is even (respectively, t \u9a74 3 is odd), and such a tree spanner can be constructed in linear time.The complexity status of TREE 3-SPANNER still remains open for chordal graphs, even on the subclass of undirected path graphs that are strongly chordal as well. For other important subclasses of chordal graphs, such as very strongly chordal graphs (containing all interval graphs), 1-split graphs (containing all split graphs) and chordal graphs of diameter at most 2, we are able to decide Tree 3-Spanner efficiently.\"",
        "title: \"The algorithmic use of hypertree structure and maximum neighbourhood orderings\" with abstract: \"The use of (generalized) tree structure in graphs is one of the main topics in the field of efficient graph algorithms. The well-known partial k -tree (resp. treewidth) approach belongs to this kind of research and bases on a tree structure of constant-size bounded maximal cliques. Without size bound on the cliques this tree structure of maximal cliques characterizes chordal graphs which are known to be important also in connection with relational database schemes where hypergraphs with tree structure (acyclic hypergraphs) and their elimination orderings (perfect elimination orderings for chordal graphs, Graham-reduction for acyclic hypergraphs) are studied. We consider here graphs with a tree structure which is dual (in the sense of hypergraphs) to that one of chordal graphs (therefore we call these graphs dually chordal ). The corresponding vertex elimination orderings of these graphs are the maximum neighbourhood orderings . These orderings were studied recently in several papers and some of the algorithmic consequences of such orderings are given. The aim of this paper is a systematic treatment of the algorithmic use of maximum neighhourhood orderings . These orderings are useful especially for dominating-like problems (including Steiner tree) and distance problems. Many problems efficiently solvable for strongly chordal and doubly chordal graphs remain efficiently solvable for dually chordal graphs too. Our results on dually chordal graphs not only generalize, but also improve and extend the corresponding results on strongly chordal and doubly chordal graphs, since a maximum neighbourhood ordering (if it exists) can be constructed in linear time and we consequently use the underlying structure properties of dually chordal graphs closely connected to hypergraphs. Furthermore, a collection of problems remaining NP -complete on dually chordal graphs is given.\"",
        "title: \"A Linear-Time Algorithm for Finding a Central Vertex of a Chordal Graph\" with abstract: \"In a graph G=(V, E), the eccentricity e(v) of a vertex v is max{d(v, u)\u2236u \u2208 V}. The center of a graph is the set of vertices with minimum eccentricity. A graph G is chordal if every cycle of length at least four has a chord. We present an algorithm which computes in linear time a central\n vertex of a chordal graph. The algorithm uses the metric properties of chordal graphs and Tarjan and Yannakakis linear-time\n test for graph chordality.\n \"",
        "title: \"Clique r-Domination and Clique r-Packing Problems on Dually Chordal Graphs\" with abstract: \"Let $\\cal C$ be a family of cliques of a graph G=(V,E). Suppose that each clique C of $\\cal C$ is associated with an integer r(C)$, where $r(C) \\ge 0$. A vertex v r-dominates a clique C of G if $d(v,x) \\le r(C)$ for all $x \\in C$, where d(v,x) is the standard graph distance. A subset $D \\subseteq V$ is a clique r-dominating set of G if for every clique $C \\in \\cal C$ there is a vertex $u \\in D$ which r-dominates C. A clique r-packing set is a subset $P \\subseteq \\cal C$ such that there are no two distinct cliques $C',C''\\in P$ $r$-dominated by a common vertex of G. The clique r-domination problem is to find a clique r-dominating set with minimum size and the clique r-packing problem is to find a clique r-packing set with maximum size. The formulated problems include many domination and clique-transversal-related problems as special cases. In this paper an efficient algorithm is proposed for solving these problems on dually chordal graphs which are a natural generalization of strongly chordal graphs. The efficient algorithm is mainly based on the tree structure and special vertex elimination orderings of dually chordal graphs. In some important particular cases where the algorithm works in linear time the obtained results generalize and improve known results on strongly chordal graphs.\"",
        "1 is \"Process variation-aware routing in NoC based multicores\", 2 is \"Convex sets in graphs, II. Minimal path convexity\".",
        "\nGiven above information, for an author who has written the paper with the title \"Provably good global buffering using an available buffer block plan\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01143": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'VMCD: A Virtual Multi-Channel Disk I/O Scheduling Method for Virtual Machines':",
        "title: \"A novel adaptive fuzzy variable structure control for a class of nonlinear uncertain systems via backstepping\" with abstract: \"Presents a new design of adaptive fuzzy variable structure control to solve the traditional problem of model reference adaptive control (MRAC) for a class of single-input, single-output minimum-phase uncertain nonlinear systems via backstepping. Instead of taking the tedious coordinate transformation and yielding a \u201chard\u201d high-gain controller, we introduce smooth B-spline-type membership functions into the controller so as to compensate for the uncertainties much \u201csofter\u201d, i.e., in a much smoother and locally weighted manner. To be rigorous, it is shown that the stability of the closed-loop system can be assured and the tracking error can globally approach to an arbitrary preset dead-zone range\"",
        "title: \"A Reciprocal and Extensible Architecture for Multiple-Target Tracking in a Smart Home\" with abstract: \"Every home has its own unique considerations for location-aware applications. This makes a flexible architecture very crucial for efficiently integrating various tracking devices/models for adapting to real human needs. Here, we propose a reciprocal and extensible architecture to flexibly add/remove tracking sensors/models for tracking multiple targets in a smart home. Regarding tracking devices, we employ sensors from two different categories, those with seamless sensors and those with seamful ones. This allows us to take human-centric needs into consideration and to facilitate reciprocal and cooperative interaction among sensors from the two categories. Such reciprocal cooperation aims to increase the accuracy of location estimates and to compensate for the limitations of each sensor or a tracking algorithm, which allows us to track multiple targets simultaneously in a more reliable way. Moreover, the approach demonstrated in this paper can serve as a guideline to help users customize sensor arrangements to fulfill their requirements. Our experimental results, which comprise three tracking scenarios using a load sensory floor as the seamless sensor and RF identifications (RFIDs) as seamful sensors, demonstrate the effectiveness of the proposed architecture.\"",
        "title: \"Pedestrian detection using histograms of Oriented Gradients of granule feature\" with abstract: \"To robustly detect people in a video sequence is hard due to various challenges. One of the most successful discriminative features for finding people goes to the Histograms of Oriented Gradients (HOG). Although the major contour information is encoded in the HOG feature well, the background clutter disturbs the gradient information. Thus, an extension of HOG, called histograms of oriented gradient of granules (HOGG), is proposed. Instead of collecting gradient information at each pixel, the histograms of gradients in small regions are computed. HOGG with different granularity can describe the contour while ignoring the noisy edges. Moreover, the clutter background problem can be solved by encoding extra region information. With the help of the integral image technique, the evaluation of HOGG can be efficient. The final HOG+HOGG classifier obtains 92% detection rate at 10-4 false positive per window in the experiments.\"",
        "title: \"Real-Time Multitarget Visual Tracking With An Active Camera\" with abstract: \"This paper presents a real-time surveillance system to track multiple moving objects by controlling a pan-tilt camera platform. In order to describe the relationship between the targets and camera in this surveillance system, the input/output hidden Markov model (HMM) is applied here in the well-defined spherical camera coordinate. Since the targets are hard to be distinguished with one single camera when they are close to each other, we extend the particle filter for multitarget tracking with depth level estimate to track interacting targets. The targets overlapping each other still can be tracked in the images captured by single camera. Furthermore, an optimal camera action selection strategy is proposed to track multitarget within its limited field of view. The maximization of mutual information for the action design is formalized and implemented by the Monte Carlo method. The overall performance has been validated in the experiments of real-time tracking.\"",
        "title: \"Design and Realization of a Framework for Human\u2013System Interaction in Smart Homes\" with abstract: \"The current smart home is a ubiquitous computing environment consisting of multiple autonomous spaces, and its advantage is that a service interacting with home users can be set with different configurations in space, hardware, software, and quality. As well as being smart technologically speaking, a smart home should also never forget to retain the \u201chome nature\u201d when it is serving its users. In this paper, we first analyze the relationship among services, spaces, and users, and then we propose a framework as well as a corresponding algorithm to model their interaction relationship. Later, we also realize the human-system interaction framework to implement a smart home system and develop \u201cpervasive applications\u201d to demonstrate how to utilize our framework to fulfill the human-centric interaction requirement of a smart home. Finally, our preliminary evaluations show that our proposed work can enhance the performance of the human-system interaction in a smart home environment.\"",
        "1 is \"Energy-Efficient Hardware Data Prefetching\", 2 is \"Model Predictive Direct Torque Control-Part I: Concept, Algorithm, And Analysis\".",
        "\nGiven above information, for an author who has written the paper with the title \"VMCD: A Virtual Multi-Channel Disk I/O Scheduling Method for Virtual Machines\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01144": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Exploiting fine grained parallelism for acceleration of web retrieval':",
        "title: \"Emerging Service Orchestration Discovery and Monitoring.\" with abstract: \"Due to the popularity of web services on the Internet, it is important to have a clear view of their utilization behaviors. Despite asynchronous service invocations and distributed executions can provide better user experience, our views are blurred by out-of-order and fragmented service logs. Researchers have been trying various methods to reveal emerging service orchestration patterns, but nearl...\"",
        "title: \"Scalable Transactions for Web Applications in the Cloud\" with abstract: \"Cloud computing platforms provide scalability and high availability properties for web applications but they sacrifice data consistency at the same time. However, many applications cannot afford any data inconsistency. We present a scalable transaction manager for cloud database services to execute ACID transactions of web applications, even in the presence of server failures. We demonstrate the scalability of our system using a prototype implementation, and show that it scales linearly to at least 40 nodes sustaining a maximum throughput of 7286 transactions per second.\"",
        "title: \"QoS requirement generation and algorithm selection for composite service based on reference vector\" with abstract: \"Under SOA (Service-Oriented Architecture), composite service is formed by aggregating multiple component services together in a given workflow. One key criterion of this research topic is QoS composition. Most work on service composition mainly focuses on the algorithms about how to compose services according to assumed QoS, without considering where the required QoS comes from and the selection of user preferred composition algorithm among those with different computational cost and different selection results. In this paper, we propose to strengthen current service composition mechanism by generation of QoS requirement and its algorithm selection based on the QoS reference vectors which are calculated optimally from the existing individual services' QoS by registry to represent QoS overview about the best QoS, the worst (or most economical) QoS, or the average QoS of all composite services. To implement QoS requirement, which is determined according to QoS overview, this paper introduces two selection algorithms as two kinds of experiment examples, one aiming at the most accurate service selection and the other chasing for trade-off between selection cost and result. Experimental results show our mechanism can help the requester achieve his expected composite service with appropriate QoS requirement and customized selection algorithm.\"",
        "title: \"Complete Decentralized Mechanism Design for Online Machine Scheduling\" with abstract: \"We study an online version of the classical parallel scheduling problem to minimize the total weighted completion time in a total decentralized setting: both the jobs and the machines are selfish entities who try to maximize their own interest. We study the setting from an algorithmic mechanism design and present a polynomial time decentralized scheduling mechanism that induces the rational jobs and machines to report truthfully about their private information, and lures the jobs to choose proper machines such that the resulting schedule is 3.281-competitive.\"",
        "title: \"Multi-lingual cascading text compressors for WWW\" with abstract: \"Global sharing and distribution of information on the Internet result in a great demand for efficient multi-lingual text compression for Web servers and proxy implementations. Current text compressors such as Huffman coding, Lempel-Ziv (LZ) variants, and LZ-Huffman cascading fail to perform efficiently because of the mis-matched character sampling size and the large character set of multilingual languages. Our previous research has shown that a better compression ratio can be obtained by re-adjusting the character sampling rate. We investigate the cascading of LZ variants to Huffman coding for multilingual documents. Two basic approaches, static and dynamic dictionaries, are proposed. Techniques for reducing the dictionary overhead are also suggested. Based on our multi-lingual corpus, our adaptive cascading scheme can perform better than the well-known cascading compressor, gzip, by an average of about 20%\"",
        "1 is \"Soft ARQ for Layered Streaming Media\", 2 is \"Queue response to input correlation functions: continuous spectral analysis\".",
        "\nGiven above information, for an author who has written the paper with the title \"Exploiting fine grained parallelism for acceleration of web retrieval\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01145": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'JobMiner: a real-time system for mining job-related patterns from social media':",
        "title: \"A fine-grained indoor fingerprinting localization based on magnetic field strength and channel state information.\" with abstract: \"With the popularity of wireless networks and smart devices, indoor localization gets developed rapidly. The location-based information services have attracted more and more attentions and the accurate location information has played an important role in the practical application. However, a large position measurement error in the unique indoor environment brings some challenges for accurate indoor localization. In this paper, we propose a hybrid fingerprint localization algorithm by synthetically utilizing Channel State Information (CSI) and magnetic field strength. Firstly, we give an improved Line of Sight (LOS) identification algorithm to narrow down the matching area that localization requires. Then, we combine CSI with magnetic field information to construct a fusion fingerprint database and provide a Multi-Dimensional Scaling k-Nearest Neighbor (MDS-KNN) method to achieve the fingerprint matching. Experiment result reveals that our proposed localization algorithm has better robustness and higher positioning accuracy than traditional fingerprint location methods.\"",
        "title: \"Robust Group Linkage.\" with abstract: \"  We study the problem of group linkage: linking records that refer to entities in the same group. Applications for group linkage include finding businesses in the same chain, finding conference attendees from the same affiliation, finding players from the same team, etc. Group linkage faces challenges not present for traditional record linkage. First, although different members in the same group can share some similar global values of an attribute, they represent different entities so can also have distinct local values for the same or different attributes, requiring a high tolerance for value diversity. Second, groups can be huge (with tens of thousands of records), requiring high scalability even after using good blocking strategies.   We present a two-stage algorithm: the first stage identifies cores containing records that are very likely to belong to the same group, while being robust to possible erroneous values; the second stage collects strong evidence from the cores and leverages it for merging more records into the same group, while being tolerant to differences in local values of an attribute. Experimental results show the high effectiveness and efficiency of our algorithm on various real-world data sets. \"",
        "title: \"Optimal Resource Allocation for Cross-layer Utility Maximization in Ad Hoc Networks.\" with abstract: \"This paper adopt the generalized network utility maximization (GNUM) approach and propose a cross-layer optimized congestion, contention and power control algorithm for ad hoc networks. The goal is to find optimal end-to-end source rates at the transport layer, per-link persistence probabilities at the medium access control layer and transmitting power at the physical layer to maximize the aggregate source utility. Despite the inherent difficulties of non-convexity and non-separability of variables in the original optimization problem, we obtain a decoupled and dual-decomposable convex formulation by applying an appropriate transformation and introducing some new variables. The three decomposed sub-optimization problems are coordinated through the congestion prices. The convergence properties of the three sub-algorithms are also proved. Simulation results further verify the effectiveness and the convergence of our proposed algorithm. \u00a9 2013 ACADEMY PUBLISHER.\"",
        "title: \"On addressing accuracy concerns in privacy preserving association rule mining\" with abstract: \"Randomized Response techniques have been empirically investigated in privacy preserving association rule mining. In this paper, we investigate the accuracy (in terms of bias and variance of estimates) of both support and confidence estimates of association rules derived from the randomized data. We demonstrate that providing confidence on data mining results from randomized data is significant to data miners. We propose the novel idea of using interquantile range to bound those estimates derived from the randomized market basket data. The performance is evaluated using both representative real and synthetic data sets.\"",
        "title: \"Software-Defined Firewall: Enabling Malware Traffic Detection and Programmable Security Control.\" with abstract: \"Network-based malware has posed serious threats to the security of host machines. When malware adopts a private TCP/IP stack for communications, personal and network firewalls may fail to identify the malicious traffic. Current firewall policies do not have a convenient update mechanism, which makes the malicious traffic detection difficult.\n\nIn this paper, we propose Software-Defined Firewall (SDF), a new security design to protect host machines and enable programmable security policy control by abstracting the firewall architecture into control and data planes. The control plane strengthens the easy security control policy update, as in the SDN (Software-Defined Networking) architecture. The difference is that it further collects host information to provide application-level traffic control and improve the malicious traffic detection accuracy. The data plane accommodates all incoming/outgoing network traffic in a network hardware to avoid malware bypassing it. The design of SDF is easy to be implemented and deployed in today's network. We implement a prototype of SDF and evaluate its performance in real-world experiments. Experimental results show that SDF can successfully monitor all network traffic (i.e., no traffic bypassing) and improves the accuracy of malicious traffic identification. Two examples of use cases indicate that SDF provides easier and more flexible solutions to today's host security problems than current firewalls.\n\n\"",
        "1 is \"Asynchronous distributed power and rate control in ad hoc networks: a game-theoretic approach\", 2 is \"A web-based kernel function for measuring the similarity of short text snippets\".",
        "\nGiven above information, for an author who has written the paper with the title \"JobMiner: a real-time system for mining job-related patterns from social media\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01146": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Optimal F-Reliable Protocols for the Do-All Problem on Single-Hop Wireless Networks':",
        "title: \"Randomized parallel algorithms\" with abstract: \" e assigned problem. Clearly the goal is to make &quot;small&quot; the probabilitythat the algorithm solution is incorrect.The other approach consists in giving a suitable probability distributionon the input space and then designing algorithms having an efficientcomplexity in the average-case according to the previously defined inputdistribution.There are at least two motivations in studying parallel algorithms, forcombinatorial optimization problems, which follow one or both of the probabilistic... \"",
        "title: \"Small Pseudo-Random Sets Yield Hard Functions: New Tight Explict Lower Bounds for Branching Programs\" with abstract: \"In several previous works the construction of a computationally hard function with respect to a certain class of algorithms or Boolean circuits has been used to derive small pseudo-random spaces. In this paper, we revert this connection by presenting two new direct relations between the efficient construction of pseudo-random (both two-sided and one-sided) sets for Boolean affine spaces and the explicit construction of Boolean functions having hard branching program complexity. In the case of 1-read branching programs (1-Br.Pr.), we show that the construction of non trivial (i.e. of cardinality 2o(n)) discrepancy sets (i.e. two-sided pseudo-random sets) for Boolean affine spaces of dimension greater than n/2 yield a set of explicit Boolean functions having very hard 1-Br.Pr. size. By combining the best known construction of \u0190-biased sample spaces for linear tests and a simple \"Reduction\" Lemma, we derive the required discrepancy set and obtain a Boolean function in P having 1-Br.Pr. size not smaller than 2n-O(log2 n) and a Boolean function in DTIME(2O(log2 n)) having 1-Br.Pr. size not smaller than 2n-O(log n). The latter bound is optimal and both of them are exponential improvements over the best previously known lower bound that was 2n-3n1=2 [21]. As for non deterministic syntactic k-read branching programs (k-Br.Pr.), we introduce a new method to derive explicit, exponential lower bounds that involves the construction of hitting sets (one-sided pseudo-random sets) for affine spaces of dimension o(n/2). Using an appropriate \"orthogonal\" representation of small Boolean affine spaces, we efficiently construct these hitting sets thus obtaining an explicit Boolean function in P that has k-Br.Pr. size not smaller than 2n1-o(1) for any k = o(log n/log log n. This improves over the previous best known lower bounds given in [8,11, 17] for some range of k.\"",
        "title: \"MANETS: High Mobility Can Make Up for Low Transmission Power.\" with abstract: \"  We consider a Mobile Ad-hoc NETworks (MANET) formed by \"n\" nodes that move independently at random over a finite square region of the plane. Nodes exchange data if they are at distance at most \"r\" within each other, where r>0 is the node transmission radius. The \"flooding time\" is the number of time steps required to broadcast a message from a source node to every node of the network. Flooding time is an important measure of the speed of information spreading in dynamic networks.   We derive a nearly-tight upper bound on the flooding time which is a decreasing function of the maximal \"velocity\" of the nodes. It turns out that, when the node velocity is sufficiently high, even if the node transmission radius \"r\" is far below the \"connectivity threshold\", the flooding time does not asymptotically depend on \"r\". This implies that flooding can be very fast even though every \"snapshot\" (i.e. the static random geometric graph at any fixed time) of the MANET is fully disconnected. Data reach all nodes quickly despite these ones use very low transmission power.   Our result is the first analytical evidence of the fact that high, random node mobility strongly speed-up information spreading and, at the same time, let nodes save energy. \"",
        "title: \"Efficient Construction of Hitting Sets for Systems of Linear Functions\" with abstract: \"Given a positive number   (0,1), a subset H  {0,1}n is a -Hitting Set for a class R of boolean functions with n inputs if, for any function f  R such that Pr (f=1), there exists an element h  H such that f(h)=1. Our paper presents a new deterministic method to efficiently construct -Hitting Set for the class of systems (i.e. logical conjunctions) of boolean linear functions. Systems of boolean linear functions can be considered as the algebraic generalization of boolean combinatorial rectangular functions, the only significative example for which an efficient deterministic construction of Hitting Sets were previously known. In the restricted case of boolean rectangular functions, our method (even though completely different) achieves equivalent results to those obtained in [11]. Our results also gives an upper bound on the minimum cardinality of solution covers for the class of systems of linear equations defined over a finite field. Furthermore, as preliminary result, we show a new upper bound on the circuit complexity of integer monotone functions generalizing previous results obtained in [12].\"",
        "title: \"Maximizing the Number of Broadcast Operations in Random Geometric Ad Hoc Wireless Networks\" with abstract: \"We consider static ad hoc wireless networks whose nodes, equipped with the same initial battery charge, may dynamically change their transmission range. When a node v transmits with range r(v), its battery charge is decreased by \\beta r(v)^2, where \\beta 0 is a fixed constant. The goal is to provide a range assignment schedule that maximizes the number of broadcast operations from a given source (this number is denoted by the length of the schedule). This maximization problem, denoted by Max LifeTime, is known to be NP-hard and the best algorithm yields worst-case approximation ratio \\Theta (\\log n), where n is the number of nodes of the network. We consider random geometric instances formed by selecting n points independently and uniformly at random from a square of side length \\sqrt{n} in the euclidean plane. We present an efficient algorithm that constructs a range assignment schedule having length not smaller than 1/12 of the optimum with high probability. Then we design an efficient distributed version of the above algorithm, where nodes initially know n and their own position only. The resulting schedule guarantees the same approximation ratio achieved by the centralized version, thus, obtaining the first distributed algorithm having provably good performance for this problem.\"",
        "1 is \"Lower bounds for the broadcast problem in mobile radio networks\", 2 is \"Permutation Capacities of Families of Oriented Infinite Paths\".",
        "\nGiven above information, for an author who has written the paper with the title \"Optimal F-Reliable Protocols for the Do-All Problem on Single-Hop Wireless Networks\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01147": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Complexity classes and sparse oracles':",
        "title: \"The parallel complexity of approximating the high degree subgraph problem\" with abstract: \"The HIGH DEGREE SUBGRAPH problem is to find a subgraph H of a graph G such that the minimum degree of H is as large as possible. This problem is known to be P-hard so that parallel approximation algorithms are very important for it. Our first goal is to determine how effectively the approximation algorithm based on a well-known extremal graph result parallelizes. In particular, we show that two natural decision problems associated with this algorithm are P-complete: these results suggest that the parallel implementation of the algorithm itself requires more sophisticated techniques. Successively, we study the HIGH DEGREE SUBGRAPH problem for random graphs with any edge probability function and we provide different parallel approximation algorithms depending on the type of this function. (C) 1998-Elsevier Science B.V. All rights reserved.\"",
        "title: \"Synchronous Context-Free Grammars and Optimal Linear Parsing Strategies.\" with abstract: \"Synchronous Context-Free Grammars (SCFGs), also known as syntax-directed translation schemata 1,2], are unlike context-free grammars in that they do not have a binary normal form. In general, parsing with SCFGs takes space and time polynomial in the length of the input strings, but with the degree of the polynomial depending on the permutations of the SCFG rules. We consider linear parsing strategies, which add one nonterminal at a time. We show that for a given input permutation, the problems of finding the linear parsing strategy with the minimum space and time complexity are both NP-hard.\"",
        "title: \"From theory to practice: NP-completeness for every CS student\" with abstract: \"NP-completeness is one of the most central concepts in computer science, and has been extensively applied in many diverse application areas. Despite this, students have problems grasping the concept and, more specifically, applying it to new problems. Independently, we have identified these problems at our universities in different countries and cultures. In an action research approach we have modified our courses and studied the effects. We here present some promising results. Our approach is mainly based on the idea of making more evident the fact that proving a new NP-completeness result is not at all different from designing a new algorithm. Based on this idea, we used tools typically used to teach algorithms (such as automatic program assessment and algorithm visualization systems), accompanied by other activities mainly devoted to augmenting the motivation to study computational complexity and forcing students to think and adopt a standpoint.\"",
        "title: \"A uniform approach to define complexity classes\" with abstract: \"Complexity classes are usually defined by referring to computation models and by putting suitable restrictions on them. Following this approach, many proofs of results are tightly bound to the characteristics of the computation model and of its restrictions and, therefore, they sometimes hide the essential properties which insure the obtained results. In order to obtain more general results, a uniform family of computation models which encompasses most of the complexity classes of interest is introduced. As a first initial set of results derivable from the proposed approach, we will give a sufficient and necessary condition for proving separations of relativized complexity classes, a characterization of complexity classes with complete languages and a sufficient condition for proving strong separations of relativized complexity classes. Examples of applications of these results to some specific complexity classes are then given. Additional results related to separations by sparse oracles can be found in Bovet (1991).\"",
        "title: \"Enumerating chemical organisations in consistent metabolic networks: complexity and algorithms\" with abstract: \"The structural analysis of metabolic networks aims both at understanding the function and the evolution of metabolism. While it is commonly admitted that metabolism is modular, the identification of metabolic modules remains an open topic. Several definitions of what is a module have been proposed. We focus here on the notion of chemical organisations, i.e. sets of molecules which are closed and self-maintaining. We show that finding a reactive organisation is NP-hard even if the network is flux-consistent and that the hardness comes from blocking cycles. We then propose new algorithms for enumerating chemical organisations that are theoretically more efficient than existing approaches.\"",
        "1 is \"How shall we assess this?\", 2 is \"On-Line Load Balancing in a Hierarchical Server Topology\".",
        "\nGiven above information, for an author who has written the paper with the title \"Complexity classes and sparse oracles\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01148": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Rumor Spreading in Random Evolving Graphs':",
        "title: \"Expansion properties of (secure) wireless networks\" with abstract: \"We show that some topologies arising naturally in the context of wireless networking are low-degree, expander graphs.\"",
        "title: \"Packing cycles in undirected graphs\" with abstract: \"Given an undirected graph G with n nodes and m edges, we address the problem of finding a largest collection of edge-disjoint cycles in G. The problem, dubbed CYCLE PACKING, is very closely related to a few genome rearrangement problems in computational biology. In this paper, we study the complexity and approximability of CYCLE PACKING, about which very little is known although the problem is natural and has practical applications. We show that the problem is APX- hard but can be approximated within a factor of O(logn) by a simple greedy approach. We do not know whether the O(log n) factor is tight, but we give a nontrivial example for which the ratio achieved by greedy is not constant, namely \u03a9(\u221alogn/(loglogn)). We also show that, for \"not too sparse\" graphs, i.e., graphs for which m = \u03a9 (n1+1/t+\u03b4) for some positive integer t and for any fixed \u03b4 0, we can achieve an approximation arbitrarily close to 2t/3 in polynomial time. In particular, for any \u03b5 0, this yields a 4/3 + \u03b5 approximation when m = \u03a9(n3/2+\u03b4), therefore also for dense graphs. Finally, we briefly discuss a natural linear programming relaxation for the problem.\"",
        "title: \"Connectivity properties of secure wireless sensor networks\" with abstract: \"We address the problem of connectivity in Secure Wireless Sensor Networks (SWSN) using random pre-distribution of keys. We propose a geometric random model for SWSNs. Under this new and realistic model, we describe how to design secure and connected networks using a small constant number of keys per sensor. Extensive simulations support the above stated result and demonstrate how connectivity can be guaranteed for a wide interval of practical network sizes and sensor communication ranges.\"",
        "title: \"Cuts and disjoint paths in the valley-free path model of internet BGP routing\" with abstract: \"In the valley-free path model, a path in a given directed graph is valid if it consists of a sequence of forward edges followed by a sequence of backward edges. This model is motivated by BGP routing policies of autonomous systems in the Internet. Robustness considerations lead to the problem of computing a maximum number of disjoint paths between two nodes, and the minimum size of a cut that separates them. We study these problems in the valley-free path model. For the problem of computing a maximum number of edge- or vertex-disjoint valid paths between two given vertices s and t, we give a 2-approximation algorithm and show that no better approximation ratio is possible unless P = NP. For the problem of computing a minimum vertex cut that separates s and t with respect to all valid paths, we give a 2-approximation algorithm and prove that the problem is APX-hard. The corresponding problem for edge cuts is shown to be polynomial-time solvable. We present additional results for acyclic graphs.\"",
        "title: \"Robustness of the internet at the topology and routing level\" with abstract: \"Classical measures of network robustness are the number of disjoint paths between two nodes and the size of a smallest cut separating them. In the Internet, the paths that traffic can take are constrained by the routing policies of the individual autonomous systems (ASs). These policies mainly depend on the economic relationships between ASs, e.g., customer-provider or peer-to-peer. Paths that are consistent with these policies can be modeled as valley-free paths. We give an overview of existing approaches to the inference of AS relationships, and we survey recent results concerning the problem of computing a maximum number of disjoint valley-free paths between two given nodes, and the problem of computing a smallest set of nodes whose removal disconnects two given nodes with respect to all valley-free paths. For both problems, we discuss NP-hardness and inapproximability results, approximation algorithms, and exact algorithms based on branch-and-bound techniques. We also summarize experimental findings that have been obtained with these algorithms in a comparison of different graph models of the AS-level Internet with respect to robustness properties.\"",
        "1 is \"Randomized routing and sorting on fixed-connection networks\", 2 is \"Non-Oblivious Local Search for Grpah and Hyperpraph Coloring Problems\".",
        "\nGiven above information, for an author who has written the paper with the title \"Rumor Spreading in Random Evolving Graphs\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01149": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Dynamic texture as foreground and background':",
        "title: \"Residual of Resonant SVD as Salient Feature\" with abstract: \"Computer vision approaches to saliency are based, among others, on uniqueness [1], local complexity [2], distinctiveness [3,4], spectral variation [5], and irregularity [6]. Saliency can also be viewed as the information in the data relative to a representation or model [7]. When a representation is built, a residual error is often minimised. The residual can be used to obtain saliency maps for solving challenging tasks of image and video processing. We introduce the notion of the resonant SVD and demonstrate that the SVD residual at the resonant spacing is selective to defects in spatially periodic surface textures and events in time-periodic videos. Examples with real-world images and videos are shown and discussed.\"",
        "title: \"Image-guided ToF depth upsampling: a survey.\" with abstract: \"Recently, there has been remarkable growth of interest in the development and applications of time-of-flight (ToF) depth cameras. Despite the permanent improvement of their characteristics, the practical applicability of ToF cameras is still limited by low resolution and quality of depth measurements. This has motivated many researchers to combine ToF cameras with other sensors in order to enhance and upsample depth images. In this paper, we review the approaches that couple ToF depth images with high-resolution optical images. Other classes of upsampling methods are also briefly discussed. Finally, we provide an overview of performance evaluation tests presented in the related studies.\"",
        "title: \"Generating contrast curves for texture regularity analysis\" with abstract: \"Statistical approaches to texture use features that describe the distribution of intensities or local features but ignore their spatial interdependence. Structural approaches concentrate on the spatial interaction of elementary regions, local features, or intensities. In most of the structural methods, it is assumed that the examined texture pattern is more or less regular. Such methods are inappropriate for dealing with the gradual transition from regularity to randomness which is typical for the spectrum of natural textures. In this paper, an attempt is made to bridge the gap between statistical and structural texture analyzers. The contrast curve of a texture pattern is defined as the first moment of a co-occurrence matrix plotted as a function of the intersample distance. The curve is used as a texture descriptor. An algorithm is presented that generates the contrast curves of both regular and random textures in the framework of a simple, uniform model. The model has four parameters that are easy to interpret. The generated curves are fitted to the experimental ones, and certain combinations of the parameters of the best fitting curves are introduced to measure texture regularity.\"",
        "title: \"Dynamic texture recognition using normal flow and texture regularity\" with abstract: \"The processing, description and recognition of dynamic (time-varying) textures are new exciting areas of texture analysis. Many real-world textures are dynamic textures whose retrieval from a video database should be based on both dynamic and static features. In this article, a method for extracting features revealing fundamental properties of dynamic textures is presented. These features are based on the normal flow and on the texture regularity though the sequence. Their discriminative ability is then successfully demonstrated on a full classification process.\"",
        "title: \"Pattern Orientation and Texture Symmetry\" with abstract: \"Human texture perception relies on a few basic high-level features including directionality and symmetry. Today, research on oriented p atterns finds its applications in various areas of applied machine vision. In this study, we present and investigate a new method for assessing pattern anisotropy via texture symmetry. Pattern o rientation is viewed as the direction o f persistent statistical t exture symmetry. The proposed method uses the spatial dependence of an extended spatial gray-level difference feature to yield an interaction symmetry map which reflects the symmetry of both short- and long-range pixel i nteractions. Pattern o rientation can then b e a ssessed via the directions of global symmetry - the characteristic axes of the pattern. Experimental results are shown which support our claim that texture symmetry is deeply related to the perceived orientation. The results are compared to the orientations obtained in a recent study that uses the traditional filtering framework. The properties of the two approaches are compared and discussed. Texture perception by humans is strongly influenced by a few fundamental properties including symmetry and directionality. The perceptional value of pattern symmetry has been confirmed by numerous studies. Rao and Lohse (1993) identify directionality as one of those high level features that account for most of the texture discrimination capability of the human v ision. The oriented p atterns s uch as those originating from flow-like processes are in the focus of texture research. The practical motivations for directionality assessment i nclude, among others, flow research (e.g. Rao and Jain 1992), image database retrieval (Gorkani and Picard 1994 ) and o rientation invariant texture classification (Greenspan et.al. 1994). The widespread, traditional approach to directionality, as originally proposed by Kass and Witkin (1987), involves multiscale orientation-sensitive filtering followed, if necessary, by the c oherence e valuation of local orientation. With different technical variations, this basic framework is shared by most researchers. The traditional analysis reveals the directionality that stems from the dominating orientation of elongated texture elements. This is a perceptually important component of anisotropy. In terms of texture e lements, however, anisotropy may have a t l east three components: that of the placement rules, that of the shape/orientation, and that of the intensity distribution of the e lements. These c omponents may interact producing\"",
        "1 is \"Perception of stress and speaking style for selected elements of the SUSAS database\", 2 is \"Estimating fluid optical flow\".",
        "\nGiven above information, for an author who has written the paper with the title \"Dynamic texture as foreground and background\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01150": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Autonomous driving in urban environments: Boss and the Urban Challenge':",
        "title: \"Opportunities and Obligations for Physical Computing Systems\" with abstract: \"Seamlessly integrating computing with the physical world via sensors and actuators, physical computing systems promise to give society an improved living standard, greater security, and unparalleled convenience and efficiency\"",
        "title: \"Towards a viable autonomous driving research platform\" with abstract: \"We present an autonomous driving research vehicle with minimal appearance modifications that is capable of a wide range of autonomous and intelligent behaviors, including smooth and comfortable trajectory generation and following; lane keeping and lane changing; intersection handling with or without V2I and V2V; and pedestrian, bicyclist, and workzone detection. Safety and reliability features include a fault-tolerant computing system; smooth and intuitive autonomous-manual switching; and the ability to fully disengage and power down the drive-by-wire and computing system upon E-stop. The vehicle has been tested extensively on both a closed test field and public roads.\"",
        "title: \"Towards dependable autonomous driving vehicles: a system-level approach\" with abstract: \"Autonomous driving technologies have been emerging over the past few years, and semi-autonomous driving functionalities have been deployed to vehicles available in the market. Since autonomous driving is realized by the intelligent processing of data from various types of sensors such as LIDAR, radar, camera, etc., the complexity of designing a dependable real-time autonomous driving system is rather high. Although there has been much research on building a reliable real-time system using hardware replication, the resulting systems tend to add significant extra cost due to hardware replication. Therefore, an alternative solution would be helpful in building an autonomous vehicle in a cost-effective way. An autonomous driving system is different from the conventional reliable real-time system because it requires (1) flexible design, (2) adaptive graceful degradation and (3) effective use of different modalities of sensors and actuators. To address these characteristics, we summarize SAFER (System-level Architecture for Failure Evasion in Real-time applications) our previous work on flexible system design. We then present a conceptual framework for autonomous vehicles to provide adaptive graceful degradation and support for using different types of sensors/actuators when a failure happens. We motivate our proposed framework with various scenarios, and we describe how SAFER can be extended to support the proposed conceptual framework.\"",
        "title: \"Critical power slope: understanding the runtime effects of frequency scaling\" with abstract: \"Energy efficiency is becoming an increasingly important feature for both mobile and high-performance server systems. Most processors designed today include power management features that provide processor operating points which can be used in power management algorithms. However, existing power management algorithms implicitly assume that lower performance points are more energy efficient than higher performance points. Our empirical observations indicate that for many systems, this assumption is not valid.We introduce a new concept called critical power slope to explain and capture the power-performance characteristics of systems with power management features. We evaluate three systems - a clock throttled Pentium laptop, a frequency scaled PowerPC platform, and a voltage scaled system to demonstrate the benefits of our approach. Our evaluation is based on empirical measurements of the first two systems, and publicly available data for the third. Using critical power slope, we explain why on the Pentium-based system, it is energy efficient to run only at the highest frequency, while on the PowerPC-based system, it is energy efficient to run at the lowest frequency point. We confirm our results by measuring the behavior of a web serving benchmark. Furthermore, we extend the critical power slope concept to understand the benefits of voltage scaling when combined with frequency scaling. We show that in some cases, it may be energy efficient not to reduce voltage below a certain point.\"",
        "title: \"Portable RK: a portable resource kernel for guaranteed and enforced timing behavior\" with abstract: \"Portable RK is a portable implementation of a resource kernel, a resource-centric approach to build a real-time kernel that provides explicit timely, guaranteed, and enforced access by applications to system resources. Portable RK is designed to work with widely available operating systems with minimal changes. This facilitates experimentation in familiar software environments and helps the faster deployment of research results. Execution in resource kernels is directly based on OS-enforced resource reservation. As a result, an application can request the reservation of a certain amount of a resource, and the kernel can guarantee that the requested amount is available to that application in timely fashion. In this paper, we describe the design and implementation of Portable RK called Linux/RK that resides within the Linux kernel. The evaluation results show that Portable RK in the form of Linux/RK gives direct control over timely resource utilization by applications and that its overhead costs are small enough to be negligible.\"",
        "1 is \"Rhex: A Simple And Highly Mobile Hexapod Robot\", 2 is \"WiseMAC: an ultra low power MAC protocol for the downlink of infrastructure wireless sensor networks\".",
        "\nGiven above information, for an author who has written the paper with the title \"Autonomous driving in urban environments: Boss and the Urban Challenge\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01151": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Multi-level feedback control for Quality of Service Management':",
        "title: \"An object-oriented tool for simulating distributed real-time control systems\" with abstract: \"This paper presents an object-oriented software tool, called RTSIM, aimed at simulating real-time embedded controllers. The tool consists of a collection of C++ libraries permitting a separate specification of the functional behaviour of the controller and of the hardware/software architecture to be used for its deployment. In particular, it is possible to provide an accurate modelling of the concurrent architecture of the control tasks and of the run-time support offered by the operating system for the real-time scheduling of the shared resources (CPU, memory buffers and network links). In this way, it is possible to compare different scheduling solutions by evaluating their simulated performance directly in the domain of the control application. Moreover, the tool can be utilized to tune up design parameters such as the activation frequencies of the tasks. The application of the tool is shown in a meaningful case study.\"",
        "title: \"Hierarchical QoS Management for Time Sensitive Applications\" with abstract: \"Abstract: The use of real-time techniques in new application fields, such as multimedia computing, has extended classical algorithms to more dynamic environments, introducing the problem of controlling and adapting the quality of service provided by an application. In this paper, we investigate the possibility of integrating application-dependent adaptation strategies with reservation techniques for handling multimedia real-time applications. We show how a global adaptive reservation mechanism can be combined with a local application-level adaptation, obtaining a hierarchical management scheme. The need for the two forms of adaptation (application dependent and global) and the effectiveness of the proposed hierarchical scheme are shown by a set of experiments on multimedia applications. All the experiments have been performed by running the real applications on a real-time kernel.\"",
        "title: \"An Experimental Analysis of the Xen and KVM Latencies\" with abstract: \"The recent developments in virtualisation technologies have made feasible the execution of complex and performance critical applications in virtual machines. When such applications are characterised by real-time constraints, the virtual machines must be scheduled predictably over the physical cores. Several works in real-time literature have analysed such a scenario, proposing advance scheduling and design techniques to respect the application constraints. However, most of the previous works focused on scheduling algorithms and theoretical analysis, without considering important implementation details such as the latencies introduced by the virtualisation mechanism. The paper, which can be seen as a complement for such works, investigates the latencies introduced by two of the most widely used hypervisors, Xen and KVM, so that previous theoretical analysis and algorithms can be used in practice.\"",
        "title: \"A Robust Mechanism for Adaptive Scheduling of Multimedia Applications\" with abstract: \"We propose an adaptive scheduling technique to schedule highly dynamic multimedia tasks on a CPU. We use a combination of two techniques: the first one is a feedback mechanism to track the resource requirements of the tasks based on \u201clocal\u201d observations. The second one is a mechanism that operates with a \u201cglobal\u201d visibility, reclaiming unused bandwidth. The combination proves very effective: resource reclaiming increases the robustness of the feedback, while the identification of the correct bandwidth made by the feedback increases the effectiveness of the reclamation. We offer both theoretical results and an extensive experimental validation of the approach.\"",
        "title: \"Efficient and robust probabilistic guarantees for real-time tasks\" with abstract: \"This paper presents a new method for providing probabilistic real-time guarantees to tasks scheduled through resource reservations. Previous work on probabilistic analysis of reservation-based schedulers is extended by improving the efficiency and robustness of the probability computation. Robustness is improved by accounting for a possibly incomplete knowledge of the distribution of the computation times (which is typical in realistic applications). The proposed approach computes a conservative bound for the probability of missing deadlines, based on the knowledge of the probability distributions of the execution times and of the inter-arrival times of the tasks. In this paper, such a bound is computed in realistic situations, comparing it with simulative results and with the exact computation of deadline miss probabilities (without pessimistic bounds). Finally, the impact of the incomplete knowledge of the execution times distribution is evaluated.\"",
        "1 is \"Analysis and Classification of Multi-Criteria Recommender Systems\", 2 is \"An exact stochastic analysis of priority-driven periodic real-time systems and its approximations\".",
        "\nGiven above information, for an author who has written the paper with the title \"Multi-level feedback control for Quality of Service Management\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01152": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Building Multiobjective Resilient Networks':",
        "title: \"Automatic circle detection on digital images with an adaptive bacterial foraging algorithm\" with abstract: \"This article presents an algorithm for the automatic detection of circular shapes from complicated and noisy images without using the conventional Hough transform methods. The proposed algorithm is based on a recently developed swarm intelligence technique, known as the bacterial foraging optimization (BFO). A new objective function has been derived to measure the resemblance of a candidate circle with an actual circle on the edge map of a given image based on the difference of their center locations and radii lengths. Guided by the values of this objective function (smaller means better), a set of encoded candidate circles are evolved using the BFO algorithm so that they can fit to the actual circles on the edge map of the image. The proposed method is able to detect single or multiple circles from a digital image through one shot of optimization. Simulation results over several synthetic as well as natural images with varying range of complexity validate the efficacy of the proposed technique in terms of its final accuracy, speed, and robustness.\"",
        "title: \"Distributed Port-Scan Attack In Cloud Environment\" with abstract: \"Cloud Computing is becoming a promising technology for processing a huge chunk of data. Hence, its security aspect has drawn the attentions of researchers and academician. The security of the cloud environment must be reliable as well as scalable.The cloud environment is vulnerable to many security attacks. Attacks can be launched individually or in tandem. In this article, the overview of port-scan attack and the response of IDS are studied. The experimentation is carried out using virtual-box and SNORT, the open-source IDS.\"",
        "title: \"Two phase semi-supervised clustering using background knowledge\" with abstract: \"Using background knowledge in clustering, called semi-clustering, is one of the actively researched areas in data mining. In this paper, we illustrate how to use background knowledge related to a domain more efficiently. For a given data, the number of classes is investigated by using the must-link constraints before clustering and these must-link data are assigned to the corresponding classes. When the clustering algorithm is applied, we make use of the cannot-link constraints for assignment. The proposed clustering approach improves the result of COP k-means by about 10%.\"",
        "title: \"An LSB Data Hiding Technique Using Prime Numbers\" with abstract: \"In this paper, a novel data hiding technique is proposed, as an improvement over the Fibonacci LSB data-hiding technique proposed by Battisti et al. (2006), First we mathematically model and generalize our approach. Then we propose our novel technique, based on decomposition of a number (pixel-value) in sum of prime numbers. The particular representation generates a different set of (virtual) bit-planes altogether, suitable for embedding purposes. They not only allow one to embed secret message in higher bit-planes but also do it without much distortion, with a much better stego-image quality, and in a reliable and secured manner, guaranteeing efficient retrieval of secret message. A comparative performance study between the classical least significant bit (LSB) method, the Fibonacci LSB data-hiding technique and our proposed schemes has been done. Analysis indicates that image quality of the stego-image hidden by the technique using Fibonacci decomposition improves against that using simple LSB substitution method, while the same using the prime decomposition method improves drastically against that using Fibonacci decomposition technique. Experimental results show that, the stego-image is visually indistinguishable from the original cover-image.\"",
        "title: \"IDEAS: Intrusion Detection based on Emotional Ants for Sensors\" with abstract: \"Due to the wide deployment of sensor networks recently security in sensor networks has become a hot research topic. Popular ways to secure a sensor network are by including cryptographic techniques or by safeguarding sensitive information from unauthorized access/manipulation and by implementing efficient intrusion detection mechanisms. This paper proposes a novel ant colony based intrusion detection mechanism which could also keep track of the intruder trials. The IDEAS technique could work in conjunction with the conventional machine learning based intrusion detection techniques to secure the sensor networks. The algorithm is presented and illustrated by simulating a sensor network.\"",
        "1 is \"Teaching-learning based optimization algorithm based fuzzy-PID controller for automatic generation control of multi-area power system.\", 2 is \"Fast simulation of rare events in queueing and reliability models\".",
        "\nGiven above information, for an author who has written the paper with the title \"Building Multiobjective Resilient Networks\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01153": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Hardware Software Partitioning Problem in Embedded System Design Using Particle Swarm Optimization Algorithm':",
        "title: \"Ensemble of hybrid neural network learning approaches for designing pharmaceutical drugs\" with abstract: \"Designing drugs is a current problem in the pharmaceutical research. By designing a drug we mean to choose some variables of drug formulation (inputs), for obtaining optimal characteristics of drug (outputs). To solve such a problem we propose an ensemble of three learning algorithms namely an evolutionary artificial neural network, Takagi-Sugeno neuro-fuzzy system and an artificial neural network. The ensemble combination is optimized by a particle swarm optimization algorithm. The experimental data were obtained from the Laboratory of Pharmaceutical Techniques of the Faculty of Pharmacy in Cluj-Napoca, Romania. Bootstrap techniques were used to generate more samples of data since the number of experimental data was low due to the costs and time durations of experimentations. Experiment results indicate that the proposed methods are efficient.\"",
        "title: \"Modified Line Search Method for Global Optimization\" with abstract: \"This paper introduces a modified version of the well known global optimization technique named line search method. The modifications refer to the way in which the direction and the steps are determined. The modified line search technique (MLS) is applied for some global optimization problems. Functions having a high number of dimensions are considered (50 in this case). Results obtained by the proposed method on a set of well known benchmarks are compared to the results obtained by the standard line search method, genetic algorithms and differential evolution. Numerical results show the effectiveness of the proposed approach while compared to the other techniques.\"",
        "title: \"Using traceless genetic programming for solving multi-objective optimization problems\" with abstract: \"Traceless genetic programming (TGP) is a genetic programming (GP) variant that is used in cases where the focus is on the output of the program rather than the program itself. The main difference between TGP and other GP techniques is that TGP does not explicitly store the evolved computer programs. Two genetic operators are used in conjunction with TGP: crossover and insertion. In this paper, we will focus on applying TGP to solving multi-objective optimization problems, which are quite unusual in GP. Each TGP individual stores the output of a computer program (tree), representing a point in the search space. Numerical experiments show that TGP is able to solve the considered test problems both rapidly and accurately.\"",
        "title: \"How to solve a multicriterion problem for which pareto dominance relationship cannot be applied? a case study from medicine\" with abstract: \"The most common way to deal with a multiobjective optimization problem is to apply Pareto dominance relationship between solutions. The question is: how can we make a decision for a multiobjective problem if we cannot use the conventional Pareto dominance for ranking solutions? We will exemplify this by considering a multicriterion problem for a medical domain problem. Trigeminal Neuralgia (TN) is a pain that is described as among the most acute known to mankind. TN produces excruciating, lightning strikes of facial pain, typically near the nose, lips, eyes or ears. Essential trigeminal neuralgia has questioned treatment methods. We consider five different treatment methods of the essential trigeminal neuralgia for evaluation under several criteria. We give a multiple criteria procedure using evolutionary algorithms for ranking the treatment methods of the essential trigeminal neuralgia for the set of all evaluation criteria. Results obtained by our approach using a very simple method are the same as the results obtained by applying weighted sum method (which requires lots of domain expert input). The advantage of the new proposed technique is that it does not require any additional information about the problem (like weights for each criteria in the case of weighted sum approach).\"",
        "title: \"Programming Risk Assessment Models for Online Security Evaluation Systems\" with abstract: \"Risk assessment is often done by human experts, becausethere is no exact and mathematical solution to the problem.Usually the human reasoning and perception process cannotbe expressed precisely. This paper propose a geneticprogramming approach for risk assessment. Preliminaryresults indicate that genetic programming methods are robustand suitable for this problem when compared to otherrisk assessment models.\"",
        "1 is \"Scanline Algorithms on a Grid\", 2 is \"Modeling gene expression with differential equations.\".",
        "\nGiven above information, for an author who has written the paper with the title \"Hardware Software Partitioning Problem in Embedded System Design Using Particle Swarm Optimization Algorithm\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01154": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Training artificial neural networks using APPM':",
        "title: \"Whole Genome Comparison on a Network of Workstations\" with abstract: \"Whole genome comparison consists of comparing or aligning genome sequences with a goal of finding similarities between them. Previously we have shown how SIMD Extensions used in Intel processors can be used to efficiently implement the, genome comparing, Smith-Waterman algorithm. Here we present distributed version of that algorithm. We show that on somewhat outdated hardware we can achieve speeds upwards of 8000 MCUPS; one of the fastest implementations of the Smith-Waterman algorithm.\"",
        "title: \"A Novel Scheme for Secured Data Transfer Over Computer Networks\" with abstract: \"This paper presents a novel encryption-less algorithm to enhance security in transmission of data in networks. The algorithm uses an intuitively simple idea of a 'jigsaw puzzle' to break the transformed data into multiple parts where these parts form the pieces of the puzzle. Then these parts are packaged into packets and sent to the receiver. A secure and efficient mechanism is provided to convey the information that is necessary for obtaining the original data at the receiver-end from its parts in the packets, that is, for solving the 'jigsaw puzzle'. The algorithm is designed to provide information-theoretic (that is, unconditional) security by the use of a one-time pad like scheme so that no intermediate or unintended node can obtain the entire data. A parallelizable design has been adopted for the implementation. An authentication code is also used to ensure authenticity of every packet.\"",
        "title: \"A Multi-Factor Security Protocol for Wireless Payment - Secure Web Authentication using Mobile Devices\" with abstract: \"  Previous Web access authentication systems often use either the Web or the Mobile channel individually to confirm the claimed identity of the remote user. This paper proposes a new protocol using multifactor authentication system that is both secure and highly usable. It uses a novel approach based on Transaction Identification Code and SMS to enforce extra security level with the traditional Login/password system. The system provides a highly secure environment that is simple to use and deploy, that does not require any change in infrastructure or protocol of wireless networks. This Protocol for Wireless Payment is extended to provide two way authentications. \"",
        "title: \"Finding numerical solutions of diophantine equations using ant colony optimization\" with abstract: \"The paper attempts to find numerical solutions of Diophantine equations, a challenging problem as there are no general methods to find solutions of such equations. It uses the metaphor of foraging habits of real ants. The ant colony optimization based procedure starts with randomly assigned locations to a fixed number of artificial ants. Depending upon the quality of these positions, ants deposit pheromone at the nodes. A successor node is selected from the topological neighbourhood of each of the nodes based on this stochastic pheromone deposit. If an ant bumps into an already encountered node, the pheromone is updated correspondingly. A suitably defined pheromone evaporation strategy guarantees that premature convergence does not take place. The experimental results, which compares with those of other machine intelligence techniques, validate the effectiveness of the proposed method.\"",
        "title: \"Grid Security and Integration with Minimal Performance Degradation\" with abstract: \"  Computational grids are believed to be the ultimate framework to meet the growing computational needs of the scientific community. Here, the processing power of geographically distributed resources working under different ownerships, having their own access policy, cost structure and the likes, is logically coupled to make them perform as a unified resource. The continuous increase of availability of high-bandwidth communication as well as powerful computers built of low-cost components further enhance chances of computational grids becoming a reality. However, the question of grid security remains one of the important open research issues. Here, we present some novel ideas about how to implement grid security, without appreciable performance degradation in grids. A suitable alternative to the computationally expensive encryption is suggested, which uses a key for message authentication. Methods of secure transfer and exchange of the required key(s) are also discussed. \"",
        "1 is \"Something About All or Nothing (Transforms)\", 2 is \"A Monarch Butterfly Optimization for the Dynamic Vehicle Routing Problem.\".",
        "\nGiven above information, for an author who has written the paper with the title \"Training artificial neural networks using APPM\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01155": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Abnormal events detection based on spatio-temporal co-occurences':",
        "title: \"Evaluation metric of an image understanding result\" with abstract: \"Image processing algorithms include methods that process images from their acquisition to the extraction of useful information for a given application. Among interpretation algorithms, some are designed to detect, localize, and identify one or several objects in an image. The problem addressed is the evaluation of the interpretation results of an image or a video given an associated ground truth. Challenges are multiple, such as the comparison of algorithms, evaluation of an algorithm during its development, or the definition of its optimal settings. We propose a new metric for evaluating the interpretation result of an image. The advantage of the proposed metric is to evaluate a result by taking into account the quality of the localization, recognition, and detection of objects of interest in the image. Several parameters allow us to change the behavior of this metric for a given application. Its behavior has been tested on a large database and showed interesting results. (C) 2015 SPIE and IS&T\"",
        "title: \"Enhancing the Security of Transformation Based Biometric Template Protection Schemes\" with abstract: \"Template protection is a crucial issue in biometrics. Many algorithms have been proposed in the literature among secure computing approaches, crypto-biometric algorithm and feature transformation schemes. The BioHashing algorithm belongs to this last category and has very interesting properties. Among them, we can cite its genericity since it could be applied on any biometric modality, the possible cancelability of the generated BioCode and its efficiency when the secret is not stolen by an impostor. Its main drawback is its weakness face to a combined attack (zero effort with the stolen secret scenario). In this paper, we propose a transformation-based biometric template protection scheme as an improvement of the BioHashing algorithm where the projection matrix is generated by combining the secret and the biometric data. Experimental results on two biometric modalities, namely digital fingerprint and finger knuckle print images, show the benefits of the proposed method face to attacks while keeping a good efficiency.\"",
        "title: \"User Dependent Template Update for Keystroke Dynamics Recognition\" with abstract: \"Regarding the fact that individuals have different interactions with biometric authentication systems, several techniques have been developed in the literature to model different users categories. Doddington Zoo is a concept of categorizing users behaviors into animal groups to reflect their characteristics with respect to biometric systems. This concept was developed for different biometric modalities including keystroke dynamics. The present study extends this biometric classification, by proposing a novel adaptive strategy based on the Doddinghton Zoo, for the recognition of the user's keystroke dynamics. The obtained results demonstrate competitive performances on significant keystroke dynamics datasets.\"",
        "title: \"Evaluation Protocol for Localization Metrics\" with abstract: \"Localization metrics permit to quantify the correctness of object detection in an image interpretation result. This paper deals with the definition of a protocol in order to evaluate the behavior of localization metrics. We first define some properties that metrics should verify and create a synthetic database that enables to verify those properties on different metrics. After presenting the tested localization metrics, the results obtained following the proposed protocol are exposed. Finally, some conclusions and perspectives are given.\"",
        "title: \"An e-payment Architecture Ensuring a High Level of Privacy Protection.\" with abstract: \"Online shopping is becoming more and more interesting for clients because of the ease of use and the large choice of products. As a consequence, 2.3 billion online clients have been identified in 2011. This rapid increase was accompagnied by various frauds, including stolen smart cards or fraudulent repudiation. Several e-payment systems have been proposed to reduce these security threats and the 3D-Secure protocol is becoming a standard for the payment on the Internet. Nevertheless, this protocol has not been studied in-depth, particularly in terms of privacy. This paper proposes a detailed description and an analysis of the 3D-Secure protocol, through a new privacy-orienting model for e-payment architectures. Some improvements of 3D-Secure protocol, concerning the protection of banking information, are also presented. Then, this article presents and analyses a new online payment architecture centered on the privacy of individuals.\"",
        "1 is \"Asymptotic efficiency of two-stage disjunctive testing\", 2 is \"A Reference Architecture for Biometric Template Protection based on Pseudo Identities\".",
        "\nGiven above information, for an author who has written the paper with the title \"Abnormal events detection based on spatio-temporal co-occurences\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01156": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Optimal battery management with ADHDP in smart home environments':",
        "title: \"Improved evoked potential estimation using neural network\" with abstract: \"The possibility of using the multilayer perceptron (MLP) neural network for the processing of the evoked potentials (EPs) is analyzed. In this case, the process can be conceived as deterministic low amplitude signals (damped sine waves), corresponding to the brain's response to stimuli, embedded in strongly colored noise, the EEG background activity. Typical values of the signal-to-noise ratio are less than 0 dB. The network, used as a nonlinear filter, is trained using iteratively as the input signal one of a set of available EP ensembles and as the target signal another ensemble of the same set. Experimental results, both on synthetic and real data, show that the method provides good results with very few EP ensembles. Therefore, it allows a noteworthy reduction of the signal nonstationarity and the patient's annoyance\"",
        "title: \"A Combined Approach for Channel Decorrelation in Stereo Acoustic Echo Cancellation Exploiting Time-Varying Frequency Shifting.\" with abstract: \"Multichannel acoustic echo cancellers are used in teleconferencing systems in order to reduce undesired echoes and to introduce better performance in terms of sound localization in the presence of more than one participant. Already stereophonic systems obtain realistic performance, since listeners have spatial information that helps to identify the speaker position. Unfortunately, the correlation between the two channels prevents a correct identification of the echo paths and signal decorrelation is needed. In this letter, a decorrelation approach is proposed, combining the missing-fundamental theory with frequency shifting. Therefore, the proposed approach is based on the computation of an adaptive parameter related to the fundamental frequency that is also used for controlling the desired frequency shift value. Experiments confirm that high performance in terms of magnitude squared coherence reduction and convergence speed increase can be obtained, also comparing it with other approaches of the state of the art and outperforming previous results.\"",
        "title: \"Calculation of non-mixed second derivatives in multirate systems through signal flow graph techniques\" with abstract: \"This paper proposes a new approach for calculation of derivatives in general multirate systems through a signal flow graph (SFG) technique. The first original aspect consists of the derivation of an adjoint graph without using Lee's theorem. Secondly, such a graph is able to deliver not only the first derivatives but also the full second derivatives of an output of the initial system with respect to the node variables of the starting SFG. Some examples are reported to show the right way of working of the proposed method on derivative calculation in general situations. Hence, the overall algorithm represents a useful tool for determination of Jacobean and Hessian based information in learning systems, as was already done in other related but less general contributions in the literature.\"",
        "title: \"Complex-valued neural networks with adaptive spline activation function for digital-radio-links nonlinear equalization\" with abstract: \"In this paper, a new complex-valued neural network based on adaptive activation functions is proposed. By varying the control points of a pair of Catmull-Rom cubic splines, which are used as an adaptable activation function, this new kind of neural network can be implemented as a very simple structure that is able to improve the generalization capabilities using few training samples. Due to its lo...\"",
        "title: \"Online sequential extreme learning machine in nonstationary environments.\" with abstract: \"System identification in nonstationary environments represents a challenging problem to solve and lots of efforts have been put by the scientific community in the last decades to provide adequate solutions on purpose. Most of them are targeted to work under the system linearity assumption, but also some have been proposed to deal with the nonlinear case study. In particular the authors have recently advanced a neural architecture, namely time-varying neural networks (TV-NN), which has shown remarkable identification properties in the presence of nonlinear and nonstationary conditions. TV-NN training is an issue due to the high number of free parameters and the extreme learning machine (ELM) approach has been successfully used on purpose. ELM is a fast learning algorithm that has recently caught much attention within the neural networks (NNs) research community. Many variants of ELM have been appeared in recent literature, specially for the stationary case study. The reference one for TV-NN training is named ELM-TV and is of batch-learning type. In this contribution an online sequential version of ELM-TV is developed, in response to the need of dealing with applications where sequential arrival or large number of training data occurs. This algorithm generalizes the corresponding counterpart working under stationary conditions. Its performances have been evaluated in some nonstationary and nonlinear system identification tasks and related results show that the advanced technique produces comparable generalization performances to ELM-TV, ensuring at the same time all benefits of an online sequential approach.\"",
        "1 is \"Adaptive Dynamic Programming for a Class of Nonlinear Control Systems with General Separable Performance Index\", 2 is \"Generalization and PAC learning: some new results for the class of generalized single-layer networks\".",
        "\nGiven above information, for an author who has written the paper with the title \"Optimal battery management with ADHDP in smart home environments\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01157": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Bridging the Gap Between Past and Future in RE: A Scenario-Based Approach':",
        "title: \"Hazard Relation Diagrams: a diagrammatic representation to increase validation objectivity of requirements-based hazard mitigations.\" with abstract: \"When developing safety-critical embedded systems, it is necessary to ensure that the system under development poses no harm to human users or external systems during operation. To achieve this, potential hazards are identified and potential mitigations for those hazards are documented in requirements. During requirements validation, the stakeholders assess if the documented hazard-mitigating requirements can avoid the identified hazards. Requirements validation is highly subjective. Among others, validation depends on the stakeholders\u2019 understanding of the involved processes, their familiarity with the system under development, and the information available. In consequence, there is the risk that stakeholders judge the adequacy of hazard-mitigating requirements based on their individual opinions about the hazards, rather than on the documented information about the system\u2019s hazards. To improve the validation of hazard-mitigating requirements, we recently proposed a diagrammatic representation called Hazard Relation Diagrams (Tenbergen B, Weyer T, Pohl K, Supporting the validation of adequacy in requirements-based hazard mitigations. In: Requirements engineering: foundation for software quality. LNCS, vol 9013. Springer, pp 17\u201332, 2015). In this paper, we extend the ontology of Hazard Relation Diagrams, present their notations, and define well-formedness rules. We elaborate on the application of Hazard Relation Diagrams to visualize complex relationships between hazards and mitigations and present an automated approach to generate Hazard Relation Diagrams. Finally, we report on our empirical evaluations about the impact of Hazard Relation Diagrams on review objectivity, effectiveness, efficiency, and reviewer\u2019s subjective confidence.\"",
        "title: \"Verification and testing at run-time for online quality prediction\" with abstract: \"This paper summarizes two techniques for online failure prediction allowing to anticipate the need for adaptation of service-oriented systems: (1) SPADE, employing run-time verification to predict failures of service compositions. (2) PROSA, building on online testing to predict failures of individual services.\n\n\"",
        "title: \"Towards pro-active adaptation with confidence: augmenting service monitoring with online testing\" with abstract: \"Service-based applications need to operate in a highly dynamic and distributed world. As those applications are composed of individual services, they have to react to failures of those services to ensure that the applications maintain their expected functionality and quality. Self-adaptation is one solution to this problem, as it allows applications to autonomously react to failures. Currently, monitoring is typically used to identify failures, thus triggering adaptation. However, monitoring only observes failures after they have occurred, which means that adaptation based on monitoring is reactive. This can lead to shortcomings like user dissatisfaction, increased execution times, and late response to critical events. Pro-active adaptation addresses those shortcomings, because in such a setting, the application detects the need for adaptation and thus can adapt before a failure will occur. However, it is important to avoid unnecessary pro-active adaptations, as they can lead to severe shortcomings, such as increased costs or follow-up failures. This means that when taking pro-active adaptation decisions it is key that there is confidence in the predicted future failures, i.e., pro-active adaptation should only be performed if there is certainty that the failure could in fact occur. To avoid unnecessary adaptations, we introduce an approach based on augmenting service monitoring with online testing to produce failure predictions with confidence. We demonstrate the applicability of our approach using a scenario from the eGovernment domain.\"",
        "title: \"Validating the Functional Design of Embedded Systems against Stakeholder Intentions.\" with abstract: \"In the embedded systems industry, function-centered engineering is commonly applied to address the increasing number and complexity of system functions. During function-centered engineering, the functional design that is created based on the defined requirements for the system is the main artifact that serves as a basis for subsequent development activities. If stakeholder intentions change and modifications become necessary, they are frequently incorporated directly into the functional design without updating the behavioral requirements accordingly. As a consequence, the correctness of the interplay of system functions as defined in the functional design cannot be assessed by checking it against the defined requirements (since they are outdated) but needs to be checked against the current stakeholder intentions. More precisely, the requirements engineer has to validate the functional design against the stakeholder intentions because he is the expert concerning the stakeholder intentions and can communicate with the stakeholders regarding them, if necessary. However, the requirements engineer is typically not familiar with the functional design and its notation on the one hand, and, on the other hand, the overall behavior of the system is spread across various diagrams in the functional design. Therefore, the requirements engineer needs a more abstract and consolidated view of the functional design in order to be able to validate its correctness with regard to the current stakeholder intentions. In this paper, we present an approach which is based on a specific kind of review model that is automatically generated from the functional design and supports the requirements engineer in her task. The approach that is presented in this paper is subject of ongoing research.\"",
        "title: \"Experience-based method evaluation and improvement: A process modelling approach\" with abstract: \" Thispaper therefore advocates an experience-based approach in which methodsand tools can be defined, applied, evaluated, and gradually improved. Weargue that this requires three ingredients:. a process meta model which can deal with many different situations ina flexible, decision-oriented manner;. a process repository that links process and product traces, guidance,and improvement through carefully defined concept mappings;. a tool interoperability concept in which tool behavior adapts ... \"",
        "1 is \"The intention behind web queries\", 2 is \"VxBPEL: Supporting variability for Web services in BPEL\".",
        "\nGiven above information, for an author who has written the paper with the title \"Bridging the Gap Between Past and Future in RE: A Scenario-Based Approach\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01158": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Study of the Reliability of Statistical Timing Analysis for Real-Time Systems':",
        "title: \"Systematic approaches to understanding and evaluating design trade-offs\" with abstract: \"The use of trade-off analysis as part of optimising designs has been an emerging technique for a number of years. However, only recently has much work been done with respect to systematically deriving the understanding of the system problem to be optimised and using this information as part of the design process. As systems have become larger and more complex then a need has arisen for suitable approaches. The system problem consists of design choices, measures for individual values related to quality attributes and weights to balance the relative importance of each individual quality attribute. In this paper, a method is presented for establishing an understanding of a system problem using the goal structuring notation (GSN). The motivation for this work is borne out of experience working on embedded systems in the context of critical systems where the cost of change can be large and the impact of design errors potentially catastrophic. A particular focus is deriving an understanding of the problem so that different solutions can be assessed quantitatively, which allows more definitive choices to be made. A secondary benefit is it also enables design using heuristic search approaches which is another area of our research. The overall approach is demonstrated through a case study which is a task allocation problem.\"",
        "title: \"Bio-inspired Error Detection for Complex Systems\" with abstract: \"In a number of areas, for example, sensor networks and systems of systems, complex networks are being used as part of applications that have to be dependable and safe. A common feature of these networks is they operate in a de-centralised manner and are formed in an ad-hoc manner and are often based on individual nodes that were not originally developed specifically for the situation that they are to be used. In addition, the nodes and their environment will have different behaviours over time, and there will be little knowledge during development of how they will interact. A key challenge is therefore how to understand what behaviour is normal from that which is abnormal so that the abnormal behaviour can be detected, and be prevented from affecting other parts of the system where appropriate recovery can then be performed. In this paper we review the state of the art in bio-inspired approaches, discuss how they can be used for error detection as part of providing a safe dependable sensor network, and then provide and evaluate an efficient and effective approach to error detection.\"",
        "title: \"Adaptive data-driven error detection in swarm robotics with statistical classifiers\" with abstract: \"Swarm robotics is an example of a complex system with interactions among distributed autonomous robots as well with the environment. Within the swarm there is no centralised control, behaviour emerges from interactions between agents within the swarm. Agents within the swarm exhibit time varying behaviour in dynamic environments, and are subject to a variety of possible anomalies. The focus within our work is on specific faults in individual robots that can affect the global performance of the robotic swarm. We argue that classical approaches for achieving tolerance through implicit redundancy is insufficient in some cases and additional measures should be explored. Our contribution is to demonstrate that tolerance through explicit detection with statistical techniques works well and is suitable due to its lightweight computation.\"",
        "title: \"New Directions in Worst-Case Execution Time analysis\" with abstract: \"Most software engineering methods require some form of model populated with appropriate information. Real-time systems are no exception. A significant issue is that the information needed is not always freely available and derived it using manual methods is costly in terms of time and money. Previous work showed how machine learning information derived during software testing can be used to derive loop bounds as part of the Worst-Case Execution Time analysis problem. In this paper we build on this work by investigating the issue of branch prediction.\"",
        "title: \"Scheduling HPC workflows for responsiveness and fairness with networking delays and inaccurate estimates of execution times\" with abstract: \"High-Performance Computing systems (HPCs) have grown in popularity in recent years, especially in the form of Grid and Cloud platforms. These platforms may be subject to periods of overload. In our previous research, we found that the Projected-SLR list scheduling policy provides responsiveness and a starvation-free scheduling guarantee in a realistic HPC scenario. This paper extends the previous work to consider networking delays in the platform model and inaccurate estimates of execution times in the application model. P-SLR is shown to be competitive with the best alternative scheduling policies in the presence of network costs (up to 400% computation time) and where execution time estimate inaccuracies are within generous error bounds (\"",
        "1 is \"Energy-efficient, collision-free medium access control for wireless sensor networks\", 2 is \"BANMAC: An Opportunistic MAC Protocol for Reliable Communications in Body Area Networks\".",
        "\nGiven above information, for an author who has written the paper with the title \"Study of the Reliability of Statistical Timing Analysis for Real-Time Systems\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01159": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'A neuroinspired cognitive behavioral control architecture for visually driven mobile robotics.':",
        "title: \"Designing new commercial motorcycles through a highly reconfigurable virtual reality-based simulator\" with abstract: \"In order to make a simulation more realistic, motion base simulators provide users with inertial cues, exerting linear accelerations and angular rates on the mock-up vehicle, in addition to the more common visual, audio and instrumentation feedbacks. A common choice for the design of such simulators is the use of a Stewart platform parallel manipulator. With respect to fixed base simulators, motion base ones can complete the driving experience felt by the user during a simulation. The present article presents the MORIS simulator, a motorcycle simulator intended to be a tool in the hands of motorbike designers. The system allowes designers to test new prototypes before actually building them, thus cutting down costs of the design phases. The key idea is that it should be easier and cheaper to mathematically model a new prototype and test its maneuverability in different driving conditions than to actually build it. In order to meet the strict specifications required by such a type of tool, the performances of the overall system must be very high. In the following the architecture of the simulator is presented and discussed. This includes the mechanical system, the kinematics and dynamic relations that regulate motion, the actuation system, the control system, the motorcycle dynamic model, and the motion controller unit.\"",
        "title: \"A Method for Modeling and Control Complex Tendon Transmissions in Haptic Interfaces\" with abstract: \"One of the principal guidelines in the design of haptic devices is to provide a suitable mechanical design that can improve control performance and the force-feedback fidelity. Unfortunately these guidelines may conflict with other design objectives (reflected mass, balancing, dexterity) as well as with specifications given by users and applications. For haptic interfaces based on tendon driven actuation it is highly important to achieve an accurate model of friction losses in the transmission system, in order to be able to compensate for them through an active control. In this paper it is reported a method for modeling and control complex tendon transmissions used for driving haptic devices and robots. The presented approach can operate in realtime with very low complexity; it is applicable to all kinds of serial manipulators and provides enough flexibility to allow identification of parameters and modeling of distributed friction phenomena all along the transmission. The approach has been implemented and tested on a 4 DOF exoskeleton system, the PERCRO L-EXOS.\"",
        "title: \"Right-Arm Robotic-Aided-Therapy with the Light-Exoskeleton: A General Overview\" with abstract: \"Rehabilitation robotics applications and their developments have been spreading out as consequences of the actual needs in the human activities of daily living (ADL). Exoskeletons for rehabilitation are one of them, whose intrinsic characteristics are quite useful for applications where repetitive, robustness and accurate performance are a must. As a part of robotic-mediated-rehabilitation programme into the worldwide, the exoskeletons are trying to improve the ADL of disable people through the fusion of several disciplines that lets to expand the capabilities of wearing a powered robotic exoskeletal device for rehabilitation tasks. This fact deserves to present this contribution from a general scope point of view, i.e., the technologies integration and its associated knowledge. So far, the Light-Exoskeleton which is intended for human arm rehabilitation in post-stroke patients is introduced. Preliminary experimental results as well as the involved stages about the system show the capabilities of using a robotic-constrained-rehabilitation for human arm.\"",
        "title: \"Simulating human fingers: a soft finger proxy model and algorithm\" with abstract: \"This paper presents models and algorithms that can be used to simulate contact between one or more fingertips and a virtual object. First, the paper presents various models for rotational friction obtained from in-vivo fingertip models previously proposed in the robotics and biomechanics community. Then the paper describes two sets of experiments that were performed on in-vivo fingertips in order to understand which of the models presented fits best with the real rotational friction properties of the human fingertips. Finally an extension of the god object/proxy algorithm which allows the simulation of soft finger contact, i.e. a point-contact with friction capable of supporting moments (up to a torsional friction limit) about the contact normal, is proposed. The resulting algorithm is computationally efficient, being point-based, while retaining a good level of realism.\"",
        "title: \"Design and validation of a complete haptic system for manipulative tasks.\" with abstract: \"The present work deals with the design, implementation and assessment of a new haptic system specifically conceived for manipulative tasks in virtual environments. Such a system was designed by taking into account specific issues related to fine manipulation, such as multipoint haptics, coherence, transparency and physical representation. The haptic system described herein is integrated with a virtual environment engine for the simulation of multifinger manipulation. A preliminary evaluation of the system was conducted by comparing human performance in the manipulation of virtual objects with respect to real objects, according to the data available in the literature. The experiments confirm how the most relevant relationships among physiological and physical parameters involved in manipulation are also preserved during virtual manipulation. However, an in-depth analysis of the results shows that simulation parameters affect the level of force control during virtual manipulation and the quality of the perceived force feedback.\"",
        "1 is \"Background-subtraction using contour-based fusion of thermal and visible imagery\", 2 is \"Design of spherical parallel mechanisms for application to laparoscopic surgery\".",
        "\nGiven above information, for an author who has written the paper with the title \"A neuroinspired cognitive behavioral control architecture for visually driven mobile robotics.\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01160": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'The 2005 PASCAL visual object classes challenge':",
        "title: \"Deformations, patches, and discriminative models for automatic annotation of medical radiographs\" with abstract: \"In this paper, we describe three different methods for the classification and annotation of medical radiographs. The methods were applied in the medical image annotation tasks of ImageCLEF in 2005, 2006, and 2007. Image annotation can be used to access and find images in a database using textual queries when no textual image description is available. One of the methods is a non-linear model taking into account local image deformations to compare images which are then classified using the nearest neighbour decision rule. The other two methods use local image descriptors for a bag-of-features approach. The bags of local image features are classified using discriminative classifiers. Our methods performed best in the 2005 and 2006 evaluations and second best in 2007.\"",
        "title: \"What is an object?\" with abstract: \"We present a generic objectness measure, quantifying how likely it is for an image window to contain an object of any class. We explicitly train it to distinguish objects with a well-defined boundary in space, such as cows and telephones, from amorphous background elements, such as grass and road. The measure combines in a Bayesian framework several image cues measuring characteristics of objects, such as appearing different from their surroundings and having a closed boundary. This includes an innovative cue measuring the closed boundary characteristic. In experiments on the challenging PASCAL VOC 07 dataset, we show this new cue to outperform a state-of-the-art saliency measure [17], and the combined measure to perform better than any cue alone. Finally, we show how to sample windows from an image according to their objectness distribution and give an algorithm to employ them as location priors for modern class-specific object detectors. In experiments on PASCAL VOC 07 we show this greatly reduces the number of windows evaluated by class-specific object detectors.\"",
        "title: \"Efficient approximations to model-based joint tracking and recognition of continuous sign language\" with abstract: \"We propose several tracking adaptation approaches to re- cover from early tracking errors in sign language recog- nition by optimizing the obtained tracking paths w.r.t. to the hypothesized word sequences of an automatic sign lan- guage recognition system. Hand or head tracking is usu- ally only optimized according to a tracking criterion. As a consequence, methods which depend on accurate detec- tion and tracking of body parts lead to recognition errors in gesture and sign language processing. We analyze an inte- grated tracking and recognition approach addressing these problems and propose approximation approaches over mul- tiple hand hypotheses to ease the time complexity of the in- tegrated approach. Most state-of-the-art systems consider tracking as a preprocessing feature extraction part. Exper- iments on a publicly available benchmark database show that the proposed methods strongly improve the recognition accuracy of the system.\"",
        "title: \"A deep learning approach to machine transliteration\" with abstract: \"In this paper we present a novel transliteration technique which is based on deep belief networks. Common approaches use finite state machines or other methods similar to conventional machine translation. Instead of using conventional NLP techniques, the approach presented here builds on deep belief networks, a technique which was shown to work well for other machine learning problems. We show that deep belief networks have certain properties which are very interesting for transliteration and possibly also for translation and that a combination with conventional techniques leads to an improvement over both components on an Arabic-English transliteration task.\"",
        "title: \"Object classification by fusing SVMs and Gaussian mixtures\" with abstract: \"We present a new technique that employs support vector machines (SVMs) and Gaussian mixture densities (GMDs) to create a generative/discriminative object classification technique using local image features. In the past, several approaches to fuse the advantages of generative and discriminative approaches were presented, often leading to improved robustness and recognition accuracy. Support vector machines are a well known discriminative classification framework but, similar to other discriminative approaches, suffer from a lack of robustness with respect to noise and overfitting. Gaussian mixtures, on the contrary, are a widely used generative technique. We present a method to directly fuse both approaches, effectively allowing to fully exploit the advantages of both. The fusion of SVMs and GMDs is done by representing SVMs in the framework of GMDs without changing the training and without changing the decision boundary. The new classifier is evaluated on the PASCAL VOC 2006 data. Additionally, we perform experiments on the USPS dataset and on four tasks from the UCI machine learning repository to obtain additional insights into the properties of the proposed approach. It is shown that for the relatively rare cases where SVMs have problems, the combined method outperforms both individual ones.\"",
        "1 is \"The Development and Comparison of Robust Methods for Estimating the Fundamental Matrix\", 2 is \"Modelling What Users See When They Look At Images: A Cognitive Viewpoint\".",
        "\nGiven above information, for an author who has written the paper with the title \"The 2005 PASCAL visual object classes challenge\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01161": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'From Images to Shape Models for Object Detection':",
        "title: \"Do semantic parts emerge in Convolutional Neural Networks?\" with abstract: \"Semantic object parts can be useful for several visual recognition tasks. Lately, these tasks have been addressed using Convolutional Neural Networks (CNN), achieving outstanding results. In this work we study whether CNNs learn semantic parts in their internal representation. We investigate the responses of convolutional filters and try to associate their stimuli with semantic parts. We perform two extensive quantitative analyses. First, we use ground-truth part bounding-boxes from the PASCAL-Part dataset to determine how many of those semantic parts emerge in the CNN. We explore this emergence for different layers, network depths, and supervision levels. Second, we collect human judgements in order to study what fraction of all filters systematically fire on any semantic part, even if not annotated in PASCAL-Part. Moreover, we explore several connections between discriminative power and semantics. We find out which are the most discriminative filters for object recognition, and analyze whether they respond to semantic parts or to other image patches. We also investigate the other direction: we determine which semantic parts are the most discriminative and whether they correspond to those parts emerging in the network. This enables to gain an even deeper understanding of the role of semantic parts in the network.\"",
        "title: \"2D Human Pose Estimation in TV Shows\" with abstract: \"The goal of this work is fully automatic 2D human pose estimation in unconstrained TV shows and feature films. Direct pose estimation on this uncontrolled material is often too difficult, especially when knowing nothing about the location, scale, pose, and appearance of the person, or even whether there is a person in the frame or not.We propose an approach that progressively reduces the search space for body parts, to greatly facilitate the task for the pose estimator. Moreover, when video is available, we propose methods for exploiting the temporal continuity of both appearance and pose for improving the estimation based on individual frames.The method is fully automatic and self-initializing, and explains the spatio-temporal volume covered by a person moving in a shot by soft-labeling every pixel as belonging to a particular body part or to the background. We demonstrate upper-body pose estimation by running our system on four episodes of the TV series Buffy the vampire slayer (i.e. three hours of video). Our approach is evaluated quantitatively on several hundred video frames, based on ground-truth annotation of 2D poses. Finally, we present an application to full-body action recognition on the Weizmann dataset.\"",
        "title: \"Automatically Selecting Inference Algorithms For Discrete Energy Minimisation\" with abstract: \"Minimisation of discrete energies defined over factors is an important problem in computer vision, and a vast number of MAP inference algorithms have been proposed. Different inference algorithms per-form better on factor graph models (GMs) from different underlying problem classes, and in general it is difficult to know which algorithm will yield the lowest energy for a given GM. To mitigate this difficulty, survey papers [1-3] advise the practitioner on what algorithms perform well on what classes of models. We take the next step forward, and present a technique to automatically select the best inference algorithm for an input GM. We validate our method experimentally on an extended version of the OpenGM2 benchmark [3], containing a diverse set of vision problems. On average, our method selects an inference algorithm yielding labellings with 96% of variables the same as the best available algorithm.\"",
        "title: \"Region-Based Semantic Segmentation With End-To-End Training\" with abstract: \"We propose a novel method for semantic segmentation, the task of labeling each pixel in an image with a semantic class. Our method combines the advantages of the two main competing paradigms. Methods based on region classification offer proper spatial support for appearance measurements, but typically operate in two separate stages, none of which targets pixel labeling performance at the end of the pipeline. More recent fully convolutional methods are capable of end-to-end training for the final pixel labeling, but resort to fixed patches as spatial support. We show how to modify modern region-based approaches to enable end-to-end training for semantic segmentation. This is achieved via a differentiable region-to-pixel layer and a differentiable free-form Regionof-Interest pooling layer. Our method improves the state-of-the-art in terms of class-average accuracy with 64.0% on SIFT Flow and 49.9% on PASCAL Context, and is particularly accurate at object boundaries.\"",
        "title: \"Explicit Modeling of Human-Object Interactions in Realistic Videos\" with abstract: \"We introduce an approach for learning human actions as interactions between persons and objects in realistic videos. Previous work typically represents actions with low-level features such as image gradients or optical flow. In contrast, we explicitly localize in space and track over time both the object and the person, and represent an action as the trajectory of the object w.r.t. to the person position. Our approach relies on state-of-the-art techniques for human detection [32], object detection [10], and tracking [39]. We show that this results in human and object tracks of sufficient quality to model and localize human-object interactions in realistic videos. Our human-object interaction features capture the relative trajectory of the object w.r.t. the human. Experimental results on the Coffee and Cigarettes dataset [25], the video dataset of [19], and the Rochester Daily Activities dataset [29] show that 1) our explicit human-object model is an informative cue for action recognition; 2) it is complementary to traditional low-level descriptors such as 3D--HOG [23] extracted over human tracks. We show that combining our human-object interaction features with 3D-HOG improves compared to their individual performance as well as over the state of the art [23], [29].\"",
        "1 is \"Learning image similarity from Flickr groups using Stochastic Intersection Kernel MAchines\", 2 is \"Video Action Detection With Relational Dynamic-Poselets\".",
        "\nGiven above information, for an author who has written the paper with the title \"From Images to Shape Models for Object Detection\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01162": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Joint Optimization of Caching Placement and Trajectory for UAV-D2D Networks':",
        "title: \"Sliding mode control with fuzzy compensator of pneumatic muscle actuator\" with abstract: \"In this paper, the Pneumatic Muscle (PM) as actuator is investigated. The PM model is established by using a phenomenological model consisting of a contractile element, a spring element, and a damping element in parallel. Unlike the previous study, we fully consider the different characteristics of PM model in different pressure ranges, especially in low pressure range. In order to test the effectiveness of our PM modelling, the Sliding Mode Control (SMC) is applied. For the purpose of improving the tracking accuracy, self-adaptation and robustness of PM control, Sliding Mode Control with Fuzzy Compensator (FCSMC) is developed to make PM system track a desired trajectory within a guaranteed accuracy even though there are modelling uncertainties, friction, disturbance, and so on. Based on the Lyapunov theory, the stability criterion under FCSMC control strategy for PM system is established. The experimental results demonstrate the validity of PM modelling as well as indicate the effectiveness of the proposed FCSMC method.\"",
        "title: \"Modeling of Cortical Signals Using Optimized Echo State Networks with Leaky Integrator Neurons\" with abstract: \"Echo State Networks (ESNs) is a newly developed recurrent neural network model. It has a special echo state property which entitles it to model nonlinear dynamic systems whose outputs are determined by previous inputs and outputs. The ESN approach has so far been worked out almost exclusively using standard sigmoid networks. Here we will consider ESNs constructed by leaky integrator neurons, which incorporate a time constant and the dynamics can be slowed down. Furthermore, we optimized relevant parameters of the network by Particle Swarm Optimization (PSO) in order to get a higher modeling precision. Here the input signals are spikes distilled from the monkey's motor cortex in an experiment and the outputs are the moving trajectories of the wrist of a monkey in the experiment. The results show that this model can well translate the neuronal firing activities into the desired positions.\"",
        "title: \"Improved Hybrid Fireworks Algorithm-Based Parameter Optimization in High-Order Sliding Mode Control of Hypersonic Vehicles.\" with abstract: \"With respect to the nonlinear hypersonic vehicle (HV) dynamics, achieving a satisfactory tracking control performance under uncertainties is always a challenge. The high-order sliding mode control (HOSMC) method with strong robustness has been applied to HVs. However, there are few methods for determining suitable HOSMC parameters for an efficacious control of HV, given that the uncertainties are randomly distributed. In this study, we introduce a hybrid fireworks algorithm- (FWA-) based parameter optimization into HV control design to satisfy the design requirements with high probability. First, the complex relation between design parameters and the cost function that evaluates the likelihood of system instability and violation of design requirements is modeled via stochastic robustness analysis. Subsequently, we propose an efficient hybrid FWA to solve the complex optimization problem concerning the uncertainties. The efficiency of the proposed hybrid FWA-based optimization method is demonstrated in the search of the optimal HV controller, in which the proposed method exhibits a better performance when compared with other algorithms.\"",
        "title: \"Corridor-Scene Classification for Mobile Robot Using Spiking Neurons\" with abstract: \"The ability of cognition and recognition for complex environment is very important for a real autonomous robot. A Corridor-Scene-Classifier based on spiking neural networks (SNN) for mobile robot is designed to help the mobile robot to locate correctly. In the SNN classifier, the integrate-and-fire model (IAF) spiking neuron model is used and there is lateral inhibiting in the output layer. The Winner-Take-All rule is used to modify the connecting weights between the hidden layer and the outputting layer. The experimental results show that the Corridor-Scene-Classifier is effective and it also has strong robustness.\"",
        "title: \"A New Procedure For Multi-Mode Sequential Flocking With Application To Multiple Non-Holonomic Mobile Robot Motion Control: Mode Description And Integration Principle\" with abstract: \"This is the first of two-part paper that investigates the multi-mode sequential flocking with application to multiple non-holonomic mobile robot motion control. To provide a new procedure to avoid collisions and obstacles in the process of multi-robot's sequential flocking, this paper presents the design of a multi-model flocking control for multiple mobile robots in terms of behaviour-based robotics. Some nature-imitating behaviour modes are integrated into the new sequential flocking strategy, including single-robot potential-based behaviour, singlerobot wall-following behaviour, multi-robot rigid-body bouncing behaviour, multi-robot path tracking behaviour and their fusion state. In this way, the efficient collision and obstacle avoidance in flocking motion can be achieved.\"",
        "1 is \"Convergence of Ethernet PON and IEEE 802.16 broadband access networks and its QoS-aware dynamic bandwidth allocation scheme\", 2 is \"Cross-VM side channels and their use to extract private keys\".",
        "\nGiven above information, for an author who has written the paper with the title \"Joint Optimization of Caching Placement and Trajectory for UAV-D2D Networks\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01163": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Management Requirements of Web Service Compositions':",
        "title: \"An approach for the Management of Service-oriented Architecture (SoA) based Application Systems\" with abstract: \"Flexible business processes are a key success factor for enterprises to succeed in globalised markets. The Service-oriented Architecture (SoA) concept is very well suited to support flexible business processes and application systems because capabilities (in form of services) can be composed in the most efficient way to achieve a high level of agility. However, the management of SoA-founded application systems is often neglected. Thus, we present an approach that enhances the traditional SoA concept by additional management functionality, e.g. monitoring mechanisms and SLA management. As a validation of our concept we introduce WSQoSX, a prototypical implementation based on Web Services.\"",
        "title: \"Combining simulations, models and testbeds: possibilities and pitfalls\" with abstract: \"Recently, the development in the area of decentralized systems and infrastructureless networks opened avenues for novel applications. A prominent example is the support of on-site units in large scale emergency response scenarios. Here, a combination of peer-to-peer systems and mobile ad hoc networks forms a communication substrate for offering enhanced services beyond the borderlines of the 'traditional' infrastructure-based client/server world. More representatives of the decentralized/infrastructureless domain such as service oriented architectures and mesh and sensor networks also already left their starting blocks. Along with these new technologies new questions regarding their operational bounds in terms of scalability, security, and QoS arose, to name just a few. Due to the yet sparse presence of real-world decentralized and infrastructureless systems, new protocols and applications have to be scrutinized by means of simulation and in (small-scale) testbeds. In addition, mathematical models must be developed both to verify the validity of results obtained from simulation and testbed studies and to provide tools for the online adaptation of relevant system parameters in future real-world scenarios. In this talk we highlight upcoming trends in decentralized systems and infrastructureless networks. We present recently finished and ongoing work of German research projects such as SicAri, SoKNOS, and G-Lab that focus (amongst other objectives) on the emergency response application scenario. In this context, we identify open issues still demanding for appropriate models in order to support studies in testbeds and simulation environments and to help paving the way for real-world deployment of the developed prototypes.\"",
        "title: \"LOG4SWS.KOM: Self-Adapting Semantic Web Service Discovery for SAWSDL\" with abstract: \"In recent years, a number of approaches to semantic Web service matchmaking have been proposed. Most of these proposals are based on discrete and thus relatively coarse Degrees of Match (DoMs). However, different basic assumptions regarding the generalization and specialization of semantic concepts in ontologies and their subsequent rating in matchmaking exist. Hence, most matchmakers are only properly suitable if these assumptions are met. In this paper, we present an approach for mapping subsumption reasoning-based DoMs to a continuous scale. Instead of determining the numerical equivalents of the formerly discrete DoMs manually, these values are automatically derived using a linear regression model. This permits not only easy combination with other numerical similarity measures, but also allows to adapt matchmaking to different basic assumptions. These notions are implemented and tested in LOG4SWS.KOM-a matchmaker for SAWSDL that provides very good evaluation results with respect to Information Retrieval metrics such as precision and recall.\"",
        "title: \"FReSET: an evaluation framework for folksonomy-based recommender systems\" with abstract: \"FReSET is a new recommender systems evaluation framework aiming to support research on folksonomy-based recommender systems. It provides interfaces for the implementation of folksonomy-based recommender systems and supports the consistent and reproducible offline evaluations on historical data. Unlike other recommender systems framework projects, the emphasis here is on providing a flexible framework allowing users to implement their own folksonomy-based recommender algorithms and pre-processing filtering methods rather than just providing a collection of collaborative filtering implementations. FReSET includes a graphical interface for result visualization and different cross-validation implementations to complement the basic functionality.\"",
        "title: \"A Proportionally Fair Centralized Scheduler Supporting Spatial Minislot Reuse for IEEE 802.16 Mesh Networks\" with abstract: \"Mesh and relay networks promise to increase the reach, capacity, and throughput of wireless communication networks. As a prominent example, the reservation-based IEEE 802.16 standard (as the basis for Worldwide Interoperability for Microwave Access WiMAX) comes with basic protocol mechanisms for an optional mesh mode as well as a relay mode of operation. This paper proposes a proportionally fair scheduler to fully utilize the potential of wireless mesh by exploiting spatial reuse. The scheduler is discussed within the setting of an IEEE 802.16 network operating with centralized scheduling in the mesh mode. We investigate the entire process of (1) bandwidth reservation, (2) calculation of the schedule and the bandwidth allocation, and (3) dissemination and activation of the schedule using an extension to the standard to allow for slot reuse. A performance analysis shows the feasibility of the proposed scheduling scheme and allows for insights into prospective future research areas in IEEE 802.16 networks.\"",
        "1 is \"OpenFlow: enabling innovation in campus networks\", 2 is \"Eiffel: a language and environment for software engineering\".",
        "\nGiven above information, for an author who has written the paper with the title \"Management Requirements of Web Service Compositions\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01164": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'MESA: Support for Scenario-Based Design of Concurrent Systems':",
        "title: \"A Natural Language-Based Approach For A Semi-Automatic Data Mart Design And Etl Generation\" with abstract: \"Data warehousing projects still face challenges in the various phases of the development life cycle. In particular, the success of the design phase, the focus of this paper, is hindered by the cross-disciplinary competences it requires. This paper presents a natural language-based method for the design of data mart schemas. Compared to existing approaches, our method has three main advantages: first, it facilitates requirements specification through a template covering all concepts of the decision-making process while providing for the acquisition of analytical requirements written in a structured natural language format. Second, it supports requirement validation with respect to a data source used in the ETL process. Third, it provides for a semi-automatic generation of conceptual data mart schemas that are directly mapped onto the data source; this mapping assists the definition of ETL procedures. The performance of the proposed method is illustrated through a software prototype used in an empirical comparison of our method with top-down methods.\"",
        "title: \"QoS based resource allocation and service selection in the Cloud\" with abstract: \"Web service composition builds a new value-added web service using existing Web services. A Web service may have many implementations, all of which have the same functionality, but may have different Quality of Service (QoS) values. Hence, a challenging issue of Web service composition is how to meet QoS and to fulfil cloud customers' expectations and preferences in the inherently dynamic environment of the Cloud. Addressing the QoS based web service selection and resource allocation is the focus of this paper. This challenge is a multi-objective optimization problem. To tackle this complex problem, we propose a new Penalty Genetic Algorithm (PGA) to help a Cloud provider quickly determine a set of services that compose the workflow of the composite Web service. The proposed approach aims to, at the one hand, meet QoS constraints prioritized by the Cloud customer and, at the other hand, respect the resource constraints of the Cloud provider. To the best of our knowledge, this is the first attempt to handle the problem of the optimal selection of Web services while taking into account the resource allocation in order to guarantee the QoS imposed by the Cloud customer and to maximize the profit of the Cloud provider. The experimental results of Penalty Genetic Algorithm show that it outperforms the Integer Programming method when the number of Web services and the number of resources are large.\"",
        "title: \"Optimization algorithms for the disjunctively constrained knapsack problem.\" with abstract: \"This paper deals with the Knapsack Problem with conflicts, also known as the Disjunctively Constrained Knapsack Problem. The conflicts are represented by a graph whose vertices are the items such that adjacent items cannot be packed in the knapsack simultaneously. We consider a classical formulation for the problem, study the polytope associated with this formulation and investigate the facial aspect of its basic constraints. We then present new families of valid inequalities and describe necessary and sufficient conditions for these inequalities to be facet defining. We also devise separation routines for these inequalities. Using these results, we develop a Branch-and-Cut algorithm for the problem. An extensive computational study is also presented.\"",
        "title: \"Design of a Framework for Electronic Commerce Brokers\" with abstract: \"The abundance of specialized electronic brokers (e-brokers) combined with the complexity of their design phase reinforce the need for a reusable design for e-brokers. This paper presents an e-broker framework, a reusable object-oriented design that captures several e-brokers' behaviour. The framework is expressed in F-UML, a design language that visually distinguishes between the parts common in all e-broker applications and the parts adaptable when deriving a specific e-broker from the framework. The paper briefly presents F-UML. It then focuses on the design process. Finally, it highlights the difficulties, advantages and limits of the proposed design method.\"",
        "title: \"Real Time Face Detection Based on Motion and Skin Color Information\" with abstract: \"In this paper we deal with the problem of lowering down the difficulty of face detection in video. Most of the recently developed systems swap detection accuracy for higher speeds, or vice versa. We have proposed a robust approach which makes use of spatial and temporal information in video to reduce time execution and improve precision rate. Our experiments show that our proposed approach proves efficiency without sacrificing real-time performance which makes it well-suited for live video applications.\"",
        "1 is \"Reachability and Recurrence in Extended Finite State Machines: Modular Vector Addition Systems\", 2 is \"CSCE: A Crawler Engine for Cloud Services Discovery on the World Wide Web\".",
        "\nGiven above information, for an author who has written the paper with the title \"MESA: Support for Scenario-Based Design of Concurrent Systems\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01165": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Workshop HCI for medicine and health care (HCI4MED)':",
        "title: \"Enhancing multi-touch table accessibility for wheelchair users\" with abstract: \"Wheelchair users can find accessing digital content on large multi-touch tables particularly difficult and frustrating due to their limited reach. We present work in progress that is exploring the potential of enhancing touch table accessibility through the use of mid-air gesturing technology. An overview of an experimental prototype is provided along with the key findings from an evaluation conducted with fifteen wheelchair users at a public library and heritage centre.\"",
        "title: \"CASAM: collaborative human-machine annotation of multimedia\" with abstract: \"The CASAM multimedia annotation system implements a model of cooperative annotation between a human annotator and automated components. The aim is that they work asynchronously but together. The system focuses upon the areas where automated recognition and reasoning are most effective and the user is able to work in the areas where their unique skills are required. The system's reasoning is influenced by the annotations provided by the user and, similarly, the user can see the system's work and modify and, implicitly, direct it. The CASAM system interacts with the user by providing a window onto the current state of annotation, and by generating requests for information which are important for the final annotation or to constrain its reasoning. The user can modify the annotation, respond to requests and also add their own annotations. The objective is that the human annotator's time is used more effectively and that the result is an annotation that is both of higher quality and produced more quickly. This can be especially important in circumstances where the annotator has a very restricted amount of time in which to annotate the document. In this paper we describe our prototype system. We expand upon the techniques used for automatically analysing the multimedia document, for reasoning over the annotations generated and for the generation of an effective interaction with the end-user. We also present the results of evaluations undertaken with media professionals in order to validate the approach and gain feedback to drive further research.\"",
        "title: \"Digital Behaviour Change Interventions to Break and Form Habits.\" with abstract: \"Digital behaviour change interventions, particularly those using pervasive computing technology, hold great promise in supporting users to change their behaviour. However, most interventions fail to take habitual behaviour into account, limiting their potential impact. This failure is partly driven by a plethora of overlapping behaviour change theories and related strategies that do not consider the role of habits. We critically review the main theories and models used in the research to analyse their application to designing effective habitual behaviour change interventions. We highlight the potential for Dual Process Theory, modern habit theory, and Goal Setting Theory, which together model how users form and break habits, to drive effective digital interventions. We synthesise these theories into an explanatory framework, the Habit Alteration Model, and use it to outline the state of the art. We identify the opportunities and challenges of habit-focused interventions.\n\n\"",
        "title: \"Music organisation using colour synaesthesia\" with abstract: \"The movement of music from physical discs to digital resources managed on a computer has had an effect on the listening habits of users. We explore using the potential of the innate synaesthesia that some people report feeling between colour and mood in a novel interface that enables a user to explore their music collection and create musical playlists in a more relevant way. We show that there is a reasonable degree of consistency between users' associations of colour and music, and show that an indirect descriptor can aid in the recall of music via mood, making playlist generation a simpler and more useful process.\"",
        "title: \"Pattern tool support to guide interface design\" with abstract: \"Design patterns have proved very helpful in encapsulating the knowledge required for solving design related problems, and have found their way into the CHI domain. Many interface patterns can be formalised and expressed via UML models, which provides the opportunity to incorporate such patterns into CASE tools in order to assist user interface designers. This paper presents an implemented tool-based approach for the discovery of an appropriate set of design patterns applicable to a high-level model of the system. The tool accepts a UML model of the system and presents a set of interface design patterns that can be used to create an effective implementation. The tool is aimed at providing designers with guidance as to which successful design approaches are potentially appropriate for a new interactive system, acting as a supportive aid to the design process. The use of high-level modelling approaches allows designers to focus on the interactions and nature of their systems, rather than on the technologically-driven details.\"",
        "1 is \"Ambient persuasive technology needs little cognitive effort: the differential effects of cognitive load on lighting feedback versus factual feedback\", 2 is \"How to Research People's First Impressions of Websites? Eye-Tracking as a Usability Inspection Method and Online Focus Group Research\".",
        "\nGiven above information, for an author who has written the paper with the title \"Workshop HCI for medicine and health care (HCI4MED)\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01166": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Spatially Disjoint Multipath Routing protocol without location information':",
        "title: \"A User-Centric Internet Of Things Platform To Empower Users For Managing Security And Privacy Concerns In The Internet Of Energy\" with abstract: \"Current worldwide energy challenges require the synergy of multidisciplinary actions and activities affecting different stakeholders, such as governments, utility companies, and citizens. In recent years, these needs are being addressed through the integration of the Smart Grid paradigm with the so-called Internet of Things, in order to make energy decisions aware of data from devices that are physically deployed on a smart city. However, to address such concerns, many of the scenarios from this resulting ecosystem, require huge amounts of data in order to make efficient and effective decisions about energy distribution and saving. In this sense, the application of proper security mechanisms to allow the selective disclosure of information to the intended parties is crucial to guarantee that users' privacy is not harmed. In this work, we propose the application of specific attribute-based access control technologies to protect the energy data that are outsourced from different devices. This integration is part of the ongoing work within MiMurcia Smart City project, which is intended to provide an integral platform to build a more sustainable and efficient city.\"",
        "title: \"Heuristic algorithms for minimum bandwith consumption multicast routing in wireless mesh networks\" with abstract: \"We study the problem of computing multicast trees with minimal bandwidth consumption in multi-hop wireless mesh networks. For wired networks, this problem is known as the Steiner tree problem, and it has been widely studied before. We demonstrate in this paper, that for multihop wireless mesh networks, a Steiner tree does not offer the minimal bandwidth consumption, because it neglects the wireless multicat advantage. Thus, we re-formulate the problem in terms of minimizing the numbrer of transmissions, rather than the edge cost of multicast trees. We show that the new problem is also NP-complete and we propose heuristics to compute good approximations for such bandwidth-optimal trees. Our simulation results show that the proposed heuristics offer a lower bandwidth consumption compared with Steiner trees.\"",
        "title: \"Responsive on-line gateway load-balancing for wireless mesh networks\" with abstract: \"We propose an adaptive online load-balancing protocol for multi-gateway Wireless Mesh Networks (WMNs) which, based on the current network conditions, balances load between gateways. Traffic is balanced at the TCP flow level and, as a result, the aggregate throughput, average flow throughput and fairness of flows improves. The proposed scheme (referred to as Gateway Load-Balancing, GWLB) is highly responsive, thanks to fast gateway selection and the fact that current traffic conditions are maintained up-to-date at all times without any overhead. It also effectively takes into account intra-flow and inter-flow interference when switching flows between gateway domains. We have found the performance achievable by routes used after gateway selection to be very close to the performance of optimal routes found by solving a MINLP formulation under the protocol model of interference. Through simulations, we analyze performance and compare with a number of proposed strategies, showing that GWLB outperforms them. In particular, we have observed average flow throughput gains of 128% over the nearest gateway strategy.\"",
        "title: \"Adaptive Multimedia Multi-Party Communication in Ad Hoc Environments\" with abstract: \"Adaptivity is a key issue for enhanced multimedia communication in wireless environments, where the network QoS for a whole session cannot be guaranteed. Within this paper we propose a set of mechanisms to provide adaptive multimedia multi-party communication according to user-de.ned QoS levels over self-organizing ad hoc network extensions connected to IP access networks. We use special extensions to SIP/SDPng based on XML to negotiate alternative application defined QoS levels and adaptation paths and optionallycouple local resource management with network layer QoS. At the network layer, we propose the MMARP (Multicast MANET Routing Protocol) which provides efficient multicast communications within ad hoc network extension to public infrastructure based access networks. MMARP is able to deal with the complexity of supporting traditional IP nodes whilst inter-operating smoothly with fixed IP networks. An adaptive application architecture is demonstrated that uses the E2ENP and allows multimedia applications to reconfigure themselves in real-time to match the specific network conditions in order to preserve theuser-perceived QoS. Thus, an integration of application layer QoS as defined by the user and network layer QoS is achieved.\"",
        "title: \"Energy-Efficient face routing on the virtual spanner\" with abstract: \"Geographic routing protocols are one of the most common routing schemes for sensor networks. These protocols consist of two different modes of operation: greedy routing to forward data to the destination using neighbors which are closer to the destination than current node and face routing to avoid voids in the network. Face routing requires the graph to be planar, which usually means that some crossing links of the original network cannot be considered when routing in face mode. In this paper we introduce a new localized scheme to build a virtual spanner which is planar by construction and is guaranteed to be connected if the underlying network is connected as well. Unlike previous works, by performing face routing over this spanner we can reduce energy consumption in face mode because the elimination of any of the original links in the network is not required. Thus, the most energy-efficient paths can be selected when the protocol enters face mode. The virtual spanner is easy-to-build and uses only local information, making it scalable to large-scale networks. Routing is always performed in real nodes; virtual nodes are used only as routing anchors when the agent is in face mode. In addition, our simulation results show that the proposed scheme outperforms the best energy-efficient geographic routing protocol for different network densities and energy models.\"",
        "1 is \"Localized Operations for Distributed Minimum Energy Multicast Algorithm in Mobile Ad Hoc Networks\", 2 is \"A conceptual framework for fuzzy query processing\u2014A step toward very intelligent database systems\".",
        "\nGiven above information, for an author who has written the paper with the title \"Spatially Disjoint Multipath Routing protocol without location information\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01167": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Decouple implementation of weight decay for recursive least square.':",
        "title: \"A chaos-based joint image compression and encryption scheme using DCT and SHA-1\" with abstract: \"A chaos-based joint image compression and encryption algorithm using discrete cosine transformation (DCT) and Secure Hash Algorithm-1 (SHA-1) is proposed. As SHA-1 is fast and input-sensitive, it is employed to enhance the diffusion effect on image pixels. The DCT coefficients of the whole image are separated into two sequences for mutual interaction. The sequence of low-frequency coefficients, together with the secret keys, generates a message digest to perturb another sequence composed of high-frequency coefficients. The last cipher block of the high-frequency sequence is used as a feedback to control the diffusion and permutation in the low-frequency sequence. Huffman code is chosen as the entropy coding to compress the encrypted chains. Experimental results confirm that our algorithm is efficient and highly sensitive to both the key and the plain-image.\"",
        "title: \"Using Recursive Least Square Learning Method For Principal And Minor Components Analysis\" with abstract: \"In combining principal and minor components analysis, a parallel extraction method based on recursive least square algorithm is suggested to extract the principal components of the input vectors. After the extraction, the error covariance matrix obtained in the learning process is used to perform minor components analysis. The minor components found are then pruned so as to achieve a higher compression ratio. Simulation results show that both the convergent speed and the compression ratio are improved, which in turn indicate that our method effectively combines the extraction of the principal components and the pruning of the minor components.\"",
        "title: \"Novel Stability Conditions For Cellular Neural Networks With Time Delay\" with abstract: \"In this paper, the global asymptotic stability of cellular neural networks with time delay is discussed using some novel Lyapunov functionals. Novel sufficient conditions for this type of stability are derived. They are less restrictive and more practical than those currently used. As a result, the design of cellular neural networks with time delay is refined. Our work can also be generalized to cellular neural networks with time-varying delay, a topic on which little research work has been done. By means of several different Lyapunov functionals, some sufficient conditions related to the global asymptotic stability for cellular neural networks with perturbations of time-varying delays are derived.\"",
        "title: \"Criteria for exponential stability of Cohen-Grossberg neural networks\" with abstract: \"In this paper, the Cohen-Grossberg neural network models without and with time delays are considered. By constructing several novel Lyapunov functionals, some sufficient criteria for the existence of a unique equilibrium and global exponential stability of the network are derived. These results are fairly general and can be easily verified. Besides, the approach of the analysis allows one to consider different types of activation functions, including piecewise linear, sigmoids with bounded activations as well as C1 -smooth sigmoids. In the meantime, our approach does not require any symmetric assumption of the connection matrix. It is believed that these results are significant and useful for the design and applications of the Cohen-Grossberg model.\"",
        "title: \"Handwritten digit recognition using multilayer feedforward neural networks with periodic and monotonic activation functions\" with abstract: \"The problem of handwritten digit recognition is tackled by multi-layer feedforward neural networks with different types of neuronal activation functions. Three types of activation functions are adopted in the network, namely, the traditional sigmoid function, the sinusoidal function and a periodic function that can be considered as a combination of the first two functions. To speed up the learning, as well as to reduce the network size, the Extended Kalman Filter (EKF) algorithm conjunct with a pruning method is used to train the network. Simulation results show that periodic activation functions perform better than monotonic ones in solving multi-cluster classification problems such as handwritten digit recognition.\"",
        "1 is \"Selective positive-negative feedback produces the winner-take-all competition in recurrent neural networks.\", 2 is \"A Closed-Form Robust Chinese Remainder Theorem and Its Performance Analysis\".",
        "\nGiven above information, for an author who has written the paper with the title \"Decouple implementation of weight decay for recursive least square.\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01168": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Supporting the comparison of alternative stories.':",
        "title: \"Designing interactive applications to support novel activities\" with abstract: \"R&D in media-related technologies including multimedia, information retrieval, computer vision, and the semantic web is experimenting on a variety of computational tools that, if sufficiently matured, could support many novel activities that are not practiced today. Interactive technology demonstration systems produced typically at the end of their projects show great potential for taking advantage of technological possibilities. These demo systems or \"demonstrators\" are, even if crude or farfetched, a significant manifestation of the technologists' visions in transforming emerging technologies into novel usage scenarios and applications. In this paper, we reflect on design processes and crucial design decisions made while designing some successful, web-based interactive demonstrators developed by the authors. We identifymethodological issues in applying today's requirementdriven usability engineering method to designing this type of novel applications and solicit a clearer distinction between designing mainstream applications and designing novel applications. More solution-oriented approaches leveraging design thinking are required, and more pragmatic evaluation criteria is needed that assess the role of the system in exploiting the technological possibilities to provoke further brainstorming and discussion. Such an approach will support a more efficient channelling of the technology-to-application transformation which are becoming increasingly crucial in today's context of rich technological possibilities.\"",
        "title: \"Social recommendation and visual analysis on the TV\" with abstract: \"In this paper, we present prototype interactive TV software that incorporates visual content analysis tools and social networking in the home TV. We present the challenges of working with the living room TV environment and outline how we have utilized visual processing and search technologies to address these challenges and create a novel prototype interactive TV system.\"",
        "title: \"InSPeCT: integrated surveillance for port container traffic\" with abstract: \"This paper describes a fully-operational content-indexing and management system, designed for monitoring and profiling freight-based vehicular traffic in a seaport environment. The 'InSPeCT' system captures video footage of passing vehicles and uses tailored OCR to index the footage according to vehicle license plates and freight codes. In addition to real-time functionality such as alerting, the system provides advanced search techniques for the efficient retrieval of records, where each vehicle is profiled according to multi-angled video, context information, and links to external information sources. Currently being piloted at a busy national seaport, the feedback from port officials indicates the system to be extremely useful in supplementing their existing transportation-security structures.\"",
        "title: \"User evaluation of F\u00edschl\u00e1r-News: An automatic broadcast news delivery system\" with abstract: \"Technological developments in content-based analysis of digital video information are undergoing much progress, with ideas for fully automatic systems now being proposed and demonstrated. Yet because we do not yet have robust operational video retrieval systems that can be deployed and used, the usual HCI practise of conducting a usage study and an informed iterative system design is thus not possible. F\u00edschl\u00e1r-News is one of the first automatic, content-based broadcast news analysis and archival systems that process broadcast news video so that users can search, browse, and play it in an easy-to-use manner with a conventional web browser. The system incorporates a number of state-of-the-art research components, some of which are not yet considered mature technology, yet it has been built to be robust enough to be deployed to users who are interested in access to daily news throughout a university campus. In this article we report and discuss a user-evaluation study conducted with 16 users, each of whom utilized the system freely for a one month period. Results from a detailed qualitative analysis are presented, looking at collected questionnaires, incident diaries, and interaction-log data. The findings suggest that our users employed the system in conjunction with their other news update methods, such as watching TV news at home and browsing online news websites at their workplace, their major concerns being up-to-dateness and coverage of the news content. They tried to accommodate the system to fit their established web browsing habits, and they found local news content and the ability to play self-contained news stories on their desktop as major values of the system. Our study also resulted in a detailed wishlist of new features which will help in the further development of both our and others' systems.\"",
        "title: \"Mo M\u00fasaem F\u00edor\u00fail: A Web-Based Search and Information Service for Museum Visitors\" with abstract: \"We describe the prototype of an interactive, web-based, museum artifact search and information service. Mo M\u00fasaem F\u00edor\u00fail clusters and indexes images of museum artifacts taken by visitors to the museum where the images are captured using a passive capture device such as Microsoft's SenseCam [1]. The system also matches clustered artifacts to images of the same artifact from the museums official photo collection and allows the user to view images of the same artifact taken by other visitors to the museum. This matching process potentially allows the system to provide more detailed information about a particular artifact to the user based on their inferred preferences, thereby greatly enhancing the user's overall museum experience. In this work, we introduce the system and describe, in broad terms, it's overall functionality and use. Using different image sets of artificial museum objects, we also describe experiments and results carried out in relation to the artifact matching component of the system.\"",
        "1 is \"Personality preferences in graphical interface design\", 2 is \"TextTearing: opening white space for digital ink annotation\".",
        "\nGiven above information, for an author who has written the paper with the title \"Supporting the comparison of alternative stories.\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01169": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'World Knowledge as Indirect Supervision for Document Clustering.':",
        "title: \"Minimally Supervised Model of Early Language Acquisition.\" with abstract: \"Theories of human language acquisition assume that learning to understand sentences is a partially-supervised task (at best). Instead of using 'gold-standard' feedback, we train a simplified \"Baby\" Semantic Role Labeling system by combining world knowledge and simple grammatical constraints to form a potentially noisy training signal. This combination of knowledge sources is vital for learning; a training signal derived from a single component leads the learner astray. When this largely unsupervised training approach is applied to a corpus of child directed speech, the BabySRL learns shallow structural cues that allow it to mimic striking behaviors found in experiments with children and begin to correctly identify agents in a sentence.\"",
        "title: \"End-Task Oriented Textual Entailment via Deep Explorations of Inter-Sentence Interactions.\" with abstract: \"This work deals with SciTail, a natural entailment challenge derived from a multi-choice question answering problem. The premises and hypotheses in SciTail were generated with no awareness of each other, and did not specifically aim at the entailment task. This makes it more challenging than other entailment data sets and more directly useful to the end-task -- question answering. We propose DEISTE (deep explorations of inter-sentence interactions for textual entailment) for this entailment task. Given word-to-word interactions between the premise-hypothesis pair ($P$, $H$), DEISTE consists of: (i) a parameter-dynamic convolution to make important words in $P$ and $H$ play a dominant role in learnt representations; and (ii) a position-aware attentive convolution to encode the representation and position information of the aligned word pairs. Experiments show that DEISTE gets $approx$5% improvement over prior state of the art and that the pretrained DEISTE on SciTail generalizes well on RTE-5.\"",
        "title: \"Semantic role labeling via integer linear programming inference\" with abstract: \"We present a system for the semantic role labeling task. The system combines a machine learning technique with an inference procedure based on integer linear programming that supports the incorporation of linguistic and structural constraints into the decision process. The system is tested on the data provided in CoNLL-2004 shared task on semantic role labeling and achieves very competitive results.\"",
        "title: \"Sorting out the most confusing English phrasal verbs\" with abstract: \"In this paper, we investigate a full-fledged supervised machine learning framework for identifying English phrasal verbs in a given context. We concentrate on those that we define as the most confusing phrasal verbs, in the sense that they are the most commonly used ones whose occurrence may correspond either to a true phrasal verb or an alignment of a simple verb with a preposition. We construct a benchmark dataset with 1,348 sentences from BNC, annotated via an Internet crowdsourcing platform. This dataset is further split into two groups, more idiomatic group which consists of those that tend to be used as a true phrasal verb and more compositional group which tends to be used either way. We build a discriminative classifier with easily available lexical and syntactic features and test it over the datasets. The classifier overall achieves 79.4% accuracy, 41.1% error deduction compared to the corpus majority baseline 65%. However, it is even more interesting to discover that the classifier learns more from the more compositional examples than those idiomatic ones.\"",
        "title: \"Solving Hard Coreference Problems.\" with abstract: \"Coreference resolution is a key problem in natural language understanding that still escapes reliable solutions. One fundamental difficulty has been that of resolving instances involving pronouns since they often require deep language understanding and use of background knowledge. In this paper we propose an algorithmic solution that involves a new representation for the knowledge required to address hard coreference problems, along with a constrained optimization framework that uses this knowledge in coreference decision making. Our representation, Predicate Schemas, is instantiated with knowledge acquired in an unsupervised way, and is compiled automatically into constraints that impact the coreference decision. We present a general coreference resolution system that significantly improves state-of-the-art performance on hard, Winograd-style, pronoun resolution cases, while still performing at the stateof-the-art level on standard coreference resolution datasets.\"",
        "1 is \"Meta-Graph Based Recommendation Fusion over Heterogeneous Information Networks\", 2 is \"Coreference Resolution with Reconcile\".",
        "\nGiven above information, for an author who has written the paper with the title \"World Knowledge as Indirect Supervision for Document Clustering.\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01170": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'NewsComm: a hand-held interface for interactive access to structured audio':",
        "title: \"Human Atlas: A Tool for Mapping Social Networks.\" with abstract: \"Most social network analyses focus on online social networks. While these networks encode important aspects of our lives they fail to capture many real-world social connections. Most of these connections are, in fact, public and known to the members of the community. Mapping them is a task very suitable for crowdsourcing: it is easily broken down in many simple and independent subtasks. Due to the nature of social networks-presence of highly connected nodes and tightly knit groups-if we allow users to map their immediate connections and the connections between them, we will need few participants to map most connections within a community. To this end, we built the Human Atlas, a web-based tool for mapping social networks. To test it, we partially mapped the social network of the MIT Media Lab. We ran a user study and invited members of the community to use the tool. In 4.6 man-hours, 22 participants mapped 984 connections within the lab, demonstrating the potential of the tool.\n\n\"",
        "title: \"Speaking with your Sidekick: Understanding Situated Speech in Computer Role Playing Games\" with abstract: \"Speech and natural language are natural and convenient ways to interact with artificial characters. Current use of language in games, however, is limited to menu systems and inter- player communication. To achieve smooth linguistic commu- nication with synthetic agents, research should focus on how language connects to the situation in which it occurs. Taking account of the physical scene (where is the speaker located, whatisaroundher, whendoesshespeak?) aswellasthefunc- tional aspects of the situation (why did he choose to speak? What are his likely plans?) can disambiguate the linguistic signal in form and content. We present a game environment to collect time synchronized speech and action streams, to visualize these data and to annotate them at different stages of processing. We further sketch a framework for situated speech understanding on such data, taking into account as- pects of the physical situation as well as the plans players follow. Our results show that this combination of influences achieves remarkable improvements over the individual situ- ation models despite the very noisy and spontaneous nature of the speech involved. This work provides a basis for devel- oping characters that use situated natural spoken language to communicate meaningfully with human players.\"",
        "title: \"An immersive system for browsing and visualizing surveillance video\" with abstract: \"HouseFly is an interactive data browsing and visualization system that synthesizes audio-visual recordings from multiple sensors, as well as the meta-data derived from those recordings, into a unified viewing experience. The system is being applied to study human behavior in both domestic and retail situations grounded in longitudinal video recordings. HouseFly uses an immersive video technique to display multiple streams of high resolution video using a realtime warping procedure that projects the video onto a 3D model of the recorded space. The system interface provides the user with simultaneous control over both playback rate and vantage point, enabling the user to navigate the data spatially and temporally. Beyond applications in video browsing, this system serves as an intuitive platform for visualizing patterns over time in a variety of multi-modal data, including person tracks and speech transcripts.\"",
        "title: \"Augmenting user interfaces with adaptive speech commands\" with abstract: \"We present a system that augments any unmodified Java application with an adaptive speech interface. The augmented system learns to associate spoken words and utterances with interface actions such as button clicks. Speech learning is constantly active and searches for correlations between what the user says and does. Training the interface is seamlessly integrated with using the interface. As the user performs normal actions, she may optionally verbally describe what she is doing. By using a phoneme recognizer, the interface is able to quickly learn new speech commands. Speech commands are chosen by the user and can be recognized robustly due to accurate phonetic modelling of the user's utterances and the small size of the vocabulary learned for a single application. After only a few examples, speech commands can replace mouse clicks. In effect, selected interface functions migrate from keyboard and mouse to speech. We demonstrate the usefulness of this approach by augmenting jfig, a drawing application, where speech commands save the user from the distraction of having to use a tool palette.\"",
        "title: \"Coupling Perception And Simulation: Steps Towards Conversational Robotics\" with abstract: \"Human cognition makes extensive use of visualization and imagination. As a first step towards giving a robot similar abilities, we have built a robotic system that uses a perceptually-coupled physical simulator to produce an internal world model of the robot's environment. Real-time perceptual coupling ensures that the model is constantly kept in synchronization with the physical environment as the robot moves and obtains new sense data. This model allows the robot to be aware of objects no longer in its field of view (a form of \"object permanence\"), as well as to visualize its environment through the eyes of the user by enabling virtual shifts in point of view using synthetic vision operating within the simulator. This architecture provides a basis for our long term goals of developing conversational robots that can ground the meaning of spoken language in terms of sensorimotor representations.\"",
        "1 is \"A New Location Technique For The Active Office\", 2 is \"Capturing, structuring, and representing ubiquitous audio\".",
        "\nGiven above information, for an author who has written the paper with the title \"NewsComm: a hand-held interface for interactive access to structured audio\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01171": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Modelling secure cloud systems based on system requirements':",
        "title: \"A practical application of our MDD approach for modeling secure XML data warehouses\" with abstract: \"Data warehouses are systems that provide useful information to support the decision making process, thus improving organizations' business processes. These systems integrate heterogeneous sources which are not only limited to their internal business data but also include data from the Web, the latter of which have become increasingly more important in the decision making process in recent years. This has motivated the extensive use of XML in the implementation of data warehouses, in a manner which facilitates data and metadata interchange among the heterogeneous data sources from the Web and the data warehouse. However, the business information that data warehouses manage is crucial and highly sensitive, and must be carefully protected. Security is thus a key issue in the design of data warehouses, regardless of the implementation technology used. It is important to note that the data available on the Web requires particular security considerations which have been specifically tailored to these systems in order to permit their particularities to be captured correctly. Unfortunately, although security issues have been considered in the development of traditional data warehouses, current research lacks approaches with which to consider security when the target platform is based on XML technology. In order to deal with this situation, in this paper we propose a methodological approach for the model driven development of secure XML data warehouses. We also specify a set of transformation rules that are able to automatically generate not only the corresponding XML structure of the data warehouse from secure conceptual data warehouse models, but also the security rules specified within the data warehouse XML structure, thus allowing both aspects to be implemented simultaneously. We additionally introduce our secure XML DW development approach, in which the secure conceptual DW data model, the PIM, is transformed into a secure XML DW, as a PSM, by applying a set of transformation rules. Our proposal is validated through the practical application of our model driven development approach for Modeling Secure XML Data Warehouses to a case study, which is based on a central Airport DW. We first describe the transformation rules defined, then use a step by step illustration to show how they will be applied to the secure conceptual model of the case study to obtain the Secure XML Data Warehouse, thus demonstrating the benefits of our proposal, and finally we analyze how to achieve the secure implementation into commercial database management systems, providing details of the secure implementation in Oracle XML DB 11g.\"",
        "title: \"Aplicando un Proceso de Ingenier\u00eda de Requisitos de Seguridad de Dominio para L\u00edneas de Producto Software\" with abstract: \"Security requirements management is especially important in software product lines, given that a weakness in security or a security breach can cause problems throughout all the products of a product line. The main contribution of this work is that of illustrating, by describing part of a real case study, a guided, systematic and intuitive way of dealing with security requirements from the early st...\"",
        "title: \"A systematic review of security requirements engineering\" with abstract: \"One of the most important aspects in the achievement of secure software systems in the software development process is what is known as Security Requirements Engineering. However, very few reviews focus on this theme in a systematic, thorough and unbiased manner, that is, none of them perform a systematic review of security requirements engineering, and there is not, therefore, a sufficiently good context in which to operate. In this paper we carry out a systematic review of the existing literature concerning security requirements engineering in order to summarize the evidence regarding this issue and to provide a framework/background in which to appropriately position new research activities.\"",
        "title: \"General Considerations on Data Warehouse Security\" with abstract: \"There is plenty of information regarding research works on Data Warehouse (DW) management systems aimed at improving several aspects such as data modeling techniques, the physical level of modeled data, transactional processing... However, not too many efforts have been made related to security aspects. At present, there are methodologies to design DW but security is not taken into account. On the other hand, there are techniques to design security but these techniques do not consider DW. Therefore, it is necessary to study the way to design secure DW. This paper puts forward general considerations on security (specifically, confidentiality) when designing D W.\"",
        "title: \"Enterprise security pattern: a new type of security pattern\" with abstract: \"AbstractIn recent years, most organizations have suffered attacks against their information systems. For this reason, organizations should seek support from enterprise security architectures ESAs in order to secure their information assets. Security patterns can help when building complex ESAs, but they have some limitations that reduce their usability. In this paper, we define the metapattern of a new type of security pattern called Enterprise Security Pattern. This new metapattern provides a model-driven environment and combines all elements that must be considered when designing and building ESAs. We present here a precise meta-model and four diagrams to describe the metapattern of the enterprise security patterns. When avoiding a security problem, organizations could use enterprise security patterns to provide their designers with an optimal and proven security guideline and so standardize the design and building of the ESA for that problem. Enterprise security patterns could also facilitate the selection and tailoring of security policies, patterns, mechanisms, and technologies when a designer is building ESAs. To illustrate our ideas, we present an instance of this new type of pattern, showing how it can be used. Copyright \u00a9 2014 John Wiley & Sons, Ltd.\"",
        "1 is \"How to Make Personalized Web Browising Simple, Secure, and Anonymous\", 2 is \"A pattern-driven security process for SOA applications\".",
        "\nGiven above information, for an author who has written the paper with the title \"Modelling secure cloud systems based on system requirements\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01172": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'A systematic review of software process tailoring':",
        "title: \"Security Requirement with a UML 2.0 Profile\" with abstract: \"Business processes are important for companies because they allow us to obtain an advanced marketplace position, and then, these enterprises can optimize and assure the quality of their products and services. Moreover, business processes are important for software developers, because they can capture from them the necessary requirements for software design and creation. At the same time, organizations have been opened and this implies more vulnerability. In spite of all these facts, security is an aspect that has been scarcely dealt with in the business process modeling. In this paper, we summarize our UML 2.0 profile for secure business process modeling through activity diagrams, and we apply this approach to a typical health-care business process.\"",
        "title: \"Using Scrum to guide the execution of software process improvement in small organizations\" with abstract: \"For software process improvement - SPI - there are few small organizations using models that guide the management and deployment of their improvement initiatives. This is largely because a lot of these models do not consider the special characteristics of small businesses, nor the appropriate strategies for deploying an SPI initiative in this type of organization. It should also be noted that the models which direct improvement implementation for small settings do not present an explicit process with which to organize and guide the internal work of the employees involved in the implementation of the improvement opportunities. In this paper we propose a lightweight process, which takes into account appropriate strategies for this type of organization. Our proposal, known as a ''Lightweight process to incorporate improvements'', uses the philosophy of the Scrum agile method, aiming to give detailed guidelines for supporting the management and performance of the incorporation of improvement opportunities within processes and their putting into practice in small companies. We have applied the proposed process in two small companies by means of the case study research method, and from the initial results, we have observed that it is indeed suitable for small businesses.\"",
        "title: \"Does object coupling really affect the understanding and modifying of OCL expressions?\" with abstract: \"Early and precise models started to play an increasingly relevant role since models themselves become the primary focus in recent initiatives of Model-Driven Engineering (such as Model-Driven Development and Model-Driven Architecture). However, a precise model cannot be obtained through the use of Unified Modeling Language (UML), due to the limited expressiveness of diagram-based UML notation. A textual add-on to the UML diagrams is needed, such as the Object Constraint Language (OCL), for reaching complete and consistent models and avoiding underspecification. Aware of the proliferation of measures for UML-based models and the lack of measures to capture the quality aspects of UML/OCL combined models we defined a set of measures for measuring the structural properties of OCL expressions. This paper carefully describes an experiment we have conducted to confirm the conclusions and strengthen the external validity of a previous family of experiments, with the purpose of investigating the relationship between object coupling in OCL expressions and the understandability and modifiability of OCL expressions. Empirical evidence that such a relationship exists is reaffirmed and consolidated.\"",
        "title: \"Towards a Data Quality Model for Web Portals\" with abstract: \"The technological advances and the use of the internet have favoured the appearance of a great diversity of web applications,\n among them Web Portals. Through them, organizations develop their businesses in a really competitive environment. A decisive\n factor for this competitiveness is the assurance of data quality. In the last years, several research works on Web Data Quality\n have been developed. However, there is a lack of specific proposals for web portals data quality. Our aim is to develop a\n data quality model for web portals focused -oin three aspects: data quality expectations of data consumer, the software functionality\n of web portals and the web data quality attributes recompiled from a literature review. In this paper, we will present the\n first version of our model.\n \"",
        "title: \"Evaluating the Ability of Novice Analysts to Understand Requirements Models\" with abstract: \"This paper is aimed at evaluating the ability of novice analysts to understand models specified using a RUP extension for modeling requirements. The evaluation is guided by a theoretical model for IS design methods, the Method Evaluation Model (MEM). In this work, we present the empirical testing of the MEM in the evaluation of a RUP extension for modeling requirements. The testing was conducted through an experiment using 39 novice users. The evaluation\u2019s primary goal was to test the users\u2019 ability to understand requirements models. The results provide a strong indication that our RUP extension is indeed both ease to use and useful and there is an intention to use the method in the future.\"",
        "1 is \"Addressing privacy requirements in system design: the PriS method\", 2 is \"Generating a canonical prefix encoding\".",
        "\nGiven above information, for an author who has written the paper with the title \"A systematic review of software process tailoring\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01173": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'On-Device Next-Item Recommendation with Self-Supervised Knowledge Distillation':",
        "title: \"A local information passing clustering algorithm for tagging systems\" with abstract: \"Under social tagging systems, a typical Web2.0 application, users label digital data sources by using tags which are freely chosen textual descriptions. Tags are used to index, annotate and retrieve resource as an additional metadata of resource. Poor retrieval performance remains a major problem of most social tagging systems resulting from the severe difficulty of ambiguity, redundancy and less semantic nature of tags. Clustering method is a useful tool to increase the ability of information retrieval in the aforementioned systems. In this paper, we propose a novel clustering algorithm named LIPC (Local Information Passing Clustering algorithm). The main steps of LIPC are: (1) we estimate a KNN neighbor directed graph G of tags and calculate the kernel density of each tag in its neighborhood; (2) we generate local information, local coverage and local kernel of each tag; (3) we pass the local information on G by I and O operators until they are converged and tag priory are generated; (4) we use tag priory to find out the clusters of tags. Experimental results on two real world datasets namely MedWorm and MovieLens demonstrate the efficiency and the superiority of the proposed method.\"",
        "title: \"Dual Space Graph Contrastive Learning\" with abstract: \"ABSTRACTUnsupervised graph representation learning has emerged as a powerful tool to address real-world problems and achieves huge success in the graph learning domain. Graph contrastive learning is one of the unsupervised graph representation learning methods, which recently attracts attention from researchers and has achieved state-of-the-art performances on various tasks. The key to the success of graph contrastive learning is to construct proper contrasting pairs to acquire the underlying structural semantics of the graph. However, this key part is not fully explored currently, most of the ways generating contrasting pairs focus on augmenting or perturbating graph structures to obtain different views of the input graph. But such strategies could degrade the performances via adding noise into the graph, which may narrow down the field of the applications of graph contrastive learning. In this paper, we propose a novel graph contrastive learning method, namely Dual Space Graph Contrastive (DSGC) Learning, to conduct graph contrastive learning among views generated in different spaces including the hyperbolic space and the Euclidean space. Since both spaces have their own advantages to represent graph data in the embedding spaces, we hope to utilize graph contrastive learning to bridge the spaces and leverage advantages from both sides. The comparison experiment results show that DSGC achieves competitive or better performances among all the datasets. In addition, we conduct extensive experiments to analyze the impact of different graph encoders on DSGC, giving insights about how to better leverage the advantages of contrastive learning between different spaces.\"",
        "title: \"Point-of-Interest Recommendations via a Supervised Random Walk Algorithm.\" with abstract: \"Recently, location-based social networks (LBSNs) such as Foursquare and Whrrl have emerged as a new application for users to establish personal social networks and review various points of interest (POIs), triggering a new recommendation service aimed at helping users locate more preferred POIs. Although users&#39; check-in activities could be explicitly considered as user ratings, in turn being utili...\"",
        "title: \"Improving automatic source code summarization via deep reinforcement learning.\" with abstract: \"Code summarization provides a high level natural language description of the function performed by code, as it can benefit the software maintenance, code categorization and retrieval. To the best of our knowledge, most state-of-the-art approaches follow an encoder-decoder framework which encodes the code into a hidden space and then decode it into natural language space, suffering from two major drawbacks: a) Their encoders only consider the sequential content of code, ignoring the tree structure which is also critical for the task of code summarization; b) Their decoders are typically trained to predict the next word by maximizing the likelihood of next ground-truth word with previous ground-truth word given. However, it is expected to generate the entire sequence from scratch at test time. This discrepancy can cause an exposure bias issue, making the learnt decoder suboptimal. In this paper, we incorporate an abstract syntax tree structure as well as sequential content of code snippets into a deep reinforcement learning framework (i.e., actor-critic network). The actor network provides the confidence of predicting the next word according to current state. On the other hand, the critic network evaluates the reward value of all possible extensions of the current state and can provide global guidance for explorations. We employ an advantage reward composed of BLEU metric to train both networks. Comprehensive experiments on a real-world dataset show the effectiveness of our proposed model when compared with some state-of-the-art methods.\n\n\"",
        "title: \"An enhanced short text categorization model with deep abundant representation.\" with abstract: \"Short text categorization is a crucial issue to many applications, e.g., Information Retrieval, Question-Answering System, MRI Database Construction and so forth. Many researches focus on data sparsity and ambiguity issues in short text categorization. To tackle these issues, we propose a novel short text categorization strategy based on abundant representation, which utilizes Bi-directional Recurrent Neural Network(Bi-RNN) with Long Short-Term Memory(LSTM) and topic model to catch more contextual and semantic information. Bi-RNN enriches contextual information, and topic model discovers more latent semantic information for abundant text representation of short text. Experimental results demonstrate that the proposed model is comparable to state-of-the-art neural network models and method proposed is effective.\"",
        "1 is \"Who are the crowdworkers?: shifting demographics in mechanical turk\", 2 is \"Social information filtering: algorithms for automating \u201cword of mouth\u201d\".",
        "\nGiven above information, for an author who has written the paper with the title \"On-Device Next-Item Recommendation with Self-Supervised Knowledge Distillation\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01174": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Preemptive Scheduling in Overloaded Systems':",
        "title: \"Two-dimensional online bin packing with rotation\" with abstract: \"In two-dimensional bin packing problems, the input items are rectangles which need to be packed in a non-overlapping manner. The goal is to assign the items into unit squares using an axis-parallel packing. Most previous work on online packing concentrated on items of fixed orientation, which must be assigned such that their bottom side is parallel to the bottom of the bin. In this paper we study the case of rotatable items, which can be rotated by ninety degrees. We give almost tight bounds on the (asymptotic) competitive ratio of bounded space bin packing of rotatable items, and introduce a new unbounded space algorithm. This improves the results of Fujita and Hada.\"",
        "title: \"Class Constrained Bin Covering\" with abstract: \"This paper considers the following geometric optimization problem: Input is a matrix R=(r ij ). Each entry r ij represents a radius of a disc with its center at (i,j) in the plane. ...\"",
        "title: \"Colored Bin Packing: Online Algorithms and Lower Bounds.\" with abstract: \"In the  problem a sequence of items of sizes up to 1 arrives to be packed into bins of unit capacity. Each item has one of at least two colors and an additional constraint is that we cannot pack two items of the same color next to each other in the same bin. The objective is to minimize the number of bins. In the important special case when all items have size zero, we characterize the optimal value to be equal to color discrepancy. As our main result, we give an (asymptotically) 1.5-competitive algorithm which is optimal. In fact, the algorithm always uses at most  bins and we can force any deterministic online algorithm to use at least  bins while the offline optimum is  for any value of . In particular, the absolute competitive ratio of our algorithm is 5\u00a0/\u00a03 and this is optimal. For items of arbitrary size we give a lower bound of 2.5 on the asymptotic competitive ratio of any online algorithm and an absolutely 3.5-competitive algorithm. When the items have sizes of at most 1\u00a0/\u00a0 for a real  the asymptotic competitive ratio of our algorithm is . We also show that classical algorithms ,  and  are not constant competitive, which holds already for three colors and small items. In the case of two colors\u2014the  problem\u2014we give a lower bound of 2 on the asymptotic competitive ratio of any online algorithm when items have arbitrary size. We also prove that all  algorithms have the absolute competitive ratio 3. When the items have sizes of at most 1\u00a0/\u00a0 for a real  we show that the  algorithm is absolutely -competitive.\"",
        "title: \"More on batched bin packing.\" with abstract: \"Bin packing is the problem of partitioning a set of items into subsets of total sizes at most 1 . In batched bin packing, items are presented in k batches, such that the items of a batch are presented as a set, to be packed before the next batch. In the disjunctive model, an algorithm must use separate bins for the different batches. We analyze the asymptotic and absolute approximation ratios for this last model completely, and show tight bounds as a function of k .\"",
        "title: \"Optimal on-line flow time with resource augmentation\" with abstract: \"We study the problem of scheduling n jobs that arrive over time. We consider a non-preemptive setting on a single machine. The goal is to minimize the total flow time. We use extra resource competitive analysis: an optimal off-line algorithm which schedules jobs on a single machine is compared to a more powerful on-line algorithm that has l machines. We design an algorithm of competitive ratio 1+2 min(\u03941/l, n1/l), where \u0394 is the maximum ratio between two job sizes, and provide a lower bound which shows that the algorithm is optimal up to a constant factor for any constant l. The algorithm works for a hard version of the problem where the sizes of the smallest and the largest jobs are not known in advance, only \u0394 and n are known. This gives a trade-off between the resource augmentation and the competitive ratio.We also consider scheduling on parallel identical machines. In this case the optimal off-line algorithm has m machines and the on-line algorithm has lm machines. We give a lower bound for this case. Next, we give lower bounds for algorithms using resource augmentation on the speed. Finally, we consider scheduling with hard deadlines, and scheduling so as to minimize the total completion time.\"",
        "1 is \"Online routing in faulty meshes with sub-linear comparative time and traffic ratio\", 2 is \"Complexity of Scheduling under Precedence Constraints\".",
        "\nGiven above information, for an author who has written the paper with the title \"Preemptive Scheduling in Overloaded Systems\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01175": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Reducing errors in the anomaly-based detection of web-based attacks through the combined analysis of web requests and SQL queries':",
        "title: \"Beehive: large-scale log analysis for detecting suspicious activity in enterprise networks\" with abstract: \"As more and more Internet-based attacks arise, organizations are responding by deploying an assortment of security products that generate situational intelligence in the form of logs. These logs often contain high volumes of interesting and useful information about activities in the network, and are among the first data sources that information security specialists consult when they suspect that an attack has taken place. However, security products often come from a patchwork of vendors, and are inconsistently installed and administered. They generate logs whose formats differ widely and that are often incomplete, mutually contradictory, and very large in volume. Hence, although this collected information is useful, it is often dirty. We present a novel system, Beehive, that attacks the problem of automatically mining and extracting knowledge from the dirty log data produced by a wide variety of security products in a large enterprise. We improve on signature-based approaches to detecting security incidents and instead identify suspicious host behaviors that Beehive reports as potential security incidents. These incidents can then be further analyzed by incident response teams to determine whether a policy violation or attack has occurred. We have evaluated Beehive on the log data collected in a large enterprise, EMC, over a period of two weeks. We compare the incidents identified by Beehive against enterprise Security Operations Center reports, antivirus software alerts, and feedback from enterprise security specialists. We show that Beehive is able to identify malicious events and policy violations which would otherwise go undetected.\"",
        "title: \"Investigating TCP performance issues in satellite networks\" with abstract: \"TCP is the widely used transport protocol across the Internet but it was originally designed for wired networks. In satellite networks, TCP encounters serious problems due to the physical properties of the wireless medium. The high delays in GEO networks and high variability of delay in LEO systems are the most significant factors affecting TCP performance. This paper identifies and illustrates TCP performance issues in satellite links by making a detailed comparison between these two common satellite altitudes. In low altitude satellite constellations, the propagation and switching delays are highly variable because of routing changes and handovers. Previous work on variable delay has focused explicitly on the retransmit timer. This paper makes a flow based analysis of abrupt delay changes to better understand TCP performance in LEO systems. Simulations are performed with the NS 2 satellite extension using the iridium constellation. It is observed that TCP performs better in LEO than in GEO systems because of its lower latency. It is also shown that large receiver buffers and intermediate buffers can alleviate the effect of abrupt delay changes in satellite networks.\"",
        "title: \"SN-SEC: a secure wireless sensor platform with hardware cryptographic primitives\" with abstract: \"Security was not considered when current wireless sensor nodes were designed. As a result, providing high level of security on current WSNs platforms is unattainable, especially against attacks based on key resolving and node compromise. In this paper, we scrutinize the security holes in current WSNs platforms and compare the main approaches to implementing their cryptographic primitives in terms of security, time, and energy efficiency. To secure these holes and provide more efficiency, we propose SN-SEC, a 32-bit RISC secure wireless sensor platform with hardware cryptographic primitives. The choice of cryptographic primitives for SN-SEC is based on their compatibility with the constrained nature of WSNs and their security. SN-SEC is implemented using very high-speed integrated circuit hardware description language. Experimental results using synthesis for Spartan-6 low-power FPGA show that the proposed design has a very reasonable computational time and energy consumption compared to well-known WSN processers.\"",
        "title: \"Testing network-based intrusion detection signatures using mutant exploits\" with abstract: \"Misuse-based intrusion detection systems rely on models of attacks to identify the manifestation of intrusive behavior. Therefore, the ability of these systems to reliably detect attacks is strongly affected by the quality of their models, which are often called \"signatures.\" A perfect model would be able to detect all the instances of an attack without making mistakes, that is, it would produce a 100% detection rate with 0 false alarms. Unfortunately, writing good models (or good signatures) is hard. Attacks that exploit a specific vulnerability may do so in completely different ways, and writing models that take into account all possible variations is very difficult. For this reason, it would be beneficial to have testing tools that are able to evaluate the \"goodness\" of detection signatures. This work describes a technique to test and evaluate misuse detection models in the case of network-based intrusion detection systems. The testing technique is based on a mechanism that generates a large number of variations of an exploit by applying mutant operators to an exploit template. These mutant exploits are then run against a victim host protected by a network-based intrusion detection system. The results of the systems in detecting these variations provide a quantitative basis for the evaluation of the quality of the corresponding detection model.\"",
        "title: \"Securing legacy firefox extensions with SENTINEL\" with abstract: \"A poorly designed web browser extension with a security vulnerability may expose the whole system to an attacker. Therefore, attacks directed at \"benign-but-buggy\" extensions, as well as extensions that have been written with malicious intents pose significant security threats to a system running such components. Recent studies have indeed shown that many Firefox extensions are over-privileged, making them attractive attack targets. Unfortunately, users currently do not have many options when it comes to protecting themselves from extensions that may potentially be malicious. Once installed and executed, the extension needs to be trusted. This paper introduces Sentinel, a policy enforcer for the Firefox browser that gives fine-grained control to the user over the actions of existing JavaScript Firefox extensions. The user is able to define policies (or use predefined ones) and block common attacks such as data exfiltration, remote code execution, saved password theft, and preference modification. Our evaluation of Sentinel shows that our prototype implementation can effectively prevent concrete, real-world Firefox extension attacks without a detrimental impact on users' browsing experience.\"",
        "1 is \"A cryptographic file system for UNIX\", 2 is \"Planning and Integrating Deception into Computer Security Defenses\".",
        "\nGiven above information, for an author who has written the paper with the title \"Reducing errors in the anomaly-based detection of web-based attacks through the combined analysis of web requests and SQL queries\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01176": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Honeybot, your man in the middle for automated social engineering':",
        "title: \"Hypervisor Memory Forensics\" with abstract: \"Memory forensics is the branch of computer forensics that aims at extracting artifacts from memory snapshots taken from a running system. Even though it is a relatively recent field, it is rapidly growing and it is attracting considerable attention from both industrial and academic researchers.In this paper, we present a set of techniques to extend the field of memory forensics toward the analysis of hypervisors and virtual machines. With the increasing adoption of virtualization techniques ( both as part of the cloud and in normal desktop environments), we believe that memory forensics will soon play a very important role in many investigations that involve virtual environments.Our approach, implemented in an open source tool as an extension of the Volatility framework, is designed to detect both the existence and the characteristics of any hypervisor that uses the Intel VT-x technology. It also supports the analysis of nested virtualization and it is able to infer the hierarchy of multiple hypervisors and virtual machines. Finally, by exploiting the techniques presented in this paper, our tool can reconstruct the address space of a virtual machine in order to transparently support any existing Volatility plugin - allowing analysts to reuse their code for the analysis of virtual environments.\"",
        "title: \"Cutting the gordian knot: A look under the hood of ransomware attacks\" with abstract: \"In this paper, we present the results of a long-term study of ransomware attacks that have been observed in the wild between 2006 and 2014. We also provide a holistic view on how ransomware attacks have evolved during this period by analyzing 1,359 samples that belong to 15 different ransomware families. Our results show that, despite a continuous improvement in the encryption, deletion, and communication techniques in the main ransomware families, the number of families with sophisticated destructive capabilities remains quite small. In fact, our analysis reveals that in a large number of samples, the malware simply locks the victim's computer desktop or attempts to encrypt or delete the victim's files using only superficial techniques.\u00ef\u00be\u00bfOur analysis also suggests that stopping advanced ransomware attacks is not as complex as it has been previously reported. For example, we show that by monitoring abnormal file system activity, it is possible to design a practical defense system that could stop a large number of ransomware attacks, even those using sophisticated encryption capabilities. A close examination on the file system activities of multiple ransomware samples suggests that by looking at I/O requests and protecting Master File Table MFT in the NTFS file system, it is possible to detect and prevent a significant number of zero-day ransomware attacks.\"",
        "title: \"Inside the SCAM Jungle: A Closer Look at 419 Scam Email Operations\" with abstract: \"Nigerian scam is a popular form of fraud in which the fraudster tricks the victim into paying a certain amount of money under the promise of a future, largerpayoff.\u807d\u807dUsing a public dataset, in this paper we study how these forms of scam campaigns are organized and evolve over time. In particular, we discuss the role of phone numbers as important identifiers to group messages together and depict the way scammers operate their campaigns. In fact, since the victim has to be able to contact the criminal, both email addresses and phone numbers need to be authentic and they are often unchanged and re-used fora long period of time. We also present in details several examples of Nigerian scam campaigns, some of which last for several years - representing them in a graphical way and discussing their characteristics.\"",
        "title: \"Disclosure: detecting botnet command and control servers through large-scale NetFlow analysis\" with abstract: \"Botnets continue to be a significant problem on the Internet. Accordingly, a great deal of research has focused on methods for detecting and mitigating the effects of botnets. Two of the primary factors preventing the development of effective large-scale, wide-area botnet detection systems are seemingly contradictory. On the one hand, technical and administrative restrictions result in a general unavailability of raw network data that would facilitate botnet detection on a large scale. On the other hand, were this data available, real-time processing at that scale would be a formidable challenge. In contrast to raw network data, NetFlow data is widely available. However, NetFlow data imposes several challenges for performing accurate botnet detection. In this paper, we present Disclosure, a large-scale, wide-area botnet detection system that incorporates a combination of novel techniques to overcome the challenges imposed by the use of NetFlow data. In particular, we identify several groups of features that allow Disclosure to reliably distinguish C&C channels from benign traffic using NetFlow records (i.e., flow sizes, client access patterns, and temporal behavior). To reduce Disclosure's false positive rate, we incorporate a number of external reputation scores into our system's detection procedure. Finally, we provide an extensive evaluation of Disclosure over two large, real-world networks. Our evaluation demonstrates that Disclosure is able to perform real-time detection of botnet C&C channels over datasets on the order of billions of flows per day.\"",
        "title: \"AccessMiner: using system-centric models for malware protection\" with abstract: \"Models based on system calls are a popular and common approach to characterize the run-time behavior of programs. For example, system calls are used by intrusion detection systems to detect software exploits. As another example, policies based on system calls are used to sandbox applications or to enforce access control. Given that malware represents a significant security threat for today's computing infrastructure, it is not surprising that system calls were also proposed to distinguish between benign processes and malicious code. Most proposed malware detectors that use system calls follows program-centric analysis approach. That is, they build models based on specific behaviors of individual applications. Unfortunately, it is not clear how well these models generalize, especially when exposed to a diverse set of previously-unseen, real-world applications that operate on realistic inputs. This is particularly problematic as most previous work has used only a small set of programs to measure their technique's false positive rate. Moreover, these programs were run for a short time, often by the authors themselves. In this paper, we study the diversity of system calls by performing a large-scale collection (compared to previous efforts) of system calls on hosts that run applications for regular users on actual inputs. Our analysis of the data demonstrates that simple malware detectors, such as those based on system call sequences, face significant challenges in such environments. To address the limitations of program-centric approaches, we propose an alternative detection model that characterizes the general interactions between benign programs and the operating system (OS). More precisely, our system-centric approach models the way in which benign programs access OS resources (such as files and registry entries). Our experiments demonstrate that this approach captures well the behavior of benign programs and raises very few (even zero) false positives while being able to detect a significant fraction of today's malware.\"",
        "1 is \"WAPTEC: whitebox analysis of web applications for parameter tampering exploit construction\", 2 is \"Tracking DDoS attacks: insights into the business of disrupting the web\".",
        "\nGiven above information, for an author who has written the paper with the title \"Honeybot, your man in the middle for automated social engineering\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01177": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'A layout-similarity-based approach for detecting phishing pages.':",
        "title: \"FIRE: FInding Rogue nEtworks\" with abstract: \"For many years, online criminals have been able to conduct their illicit activities by masquerading behind disreputable Internet service providers (ISPs). For example, organizations such as the Russian business network (RBN), Atrivo (a.k.a., Intercage), McColo, and most recently, the triple fiber network (3FN) operated with impunity, providing a safe haven for Internet criminals for their own financial gain. What primarily sets these ISPs apart from others is the significant longevity of the malicious activities on their networks and the apparent lack of action taken in response to abuse reports. Interestingly, even though the Internet provides a certain degree of anonymity, such ISPs fear public attention. Once exposed, rogue networks often cease their malicious activities quickly, or are de-peered (disconnected) by their upstream providers. As a result, the Internet criminals are forced to relocate their operations. In this paper, we present FIRE, a novel system to identify and expose organizations and ISPs that demonstrate persistent, malicious behavior. The goal is to isolate the networks that are consistently implicated in malicious activity from those that are victims of compromise. To this end, FIRE actively monitors Botnet communication channels, drive-by-download servers, and phishing Web sites. This data is refined and correlated to quantify the degree of malicious activity for individual organizations. We present our results in real-time via the Web site maliciousnetworks.org. These results can be used to pinpoint and to track the activity of rogue organizations, preventing criminals from establishing strongholds on the Internet. Also, the information can be compiled into a null-routing blacklist to immediately halt traffic from malicious networks.\"",
        "title: \"Pixy: A Static Analysis Tool for Detecting Web Application Vulnerabilities (Short Paper)\" with abstract: \"The number and the importance of Web applications have increased rapidly over the last years. At the same time, the quantity and impact of security vulnerabilities in such applications have grown as well. Since manual code reviews are time-consuming, error-prone and costly, the need for automated solutions has become evident. In this paper, we address the problem of vulnerable Web applications by means of static source code analysis. More precisely, we use flow-sensitive, interprocedural and context-sensitive data flow analysis to discover vulnerable points in a program. In addition, alias and literal analysis are employed to improve the correctness and precision of the results. The presented concepts are targeted at the general class of taint-style vulnerabilities and can be applied to the detection of vulnerability types such as SQL injection, cross-site scripting, or command injection. Pixy, the open source prototype implementation of our concepts, is targeted at detecting cross-site scripting vulnerabilities in PHP scripts. Using our tool, we discovered and reported 15 previously unknown vulnerabilities in three web applications, and reconstructed 36 known vulnerabilities in three other web applications. The observed false positive rate is at around 50% (i.e., one false positive for each vulnerability) and therefore, low enough to permit effective security audits.\"",
        "title: \"A view on current malware behaviors\" with abstract: \"Anubis is a dynamic malware analysis platform that executes submitted binaries in a controlled environment. To perform the analysis, the system monitors the invocation of important Windows API calls and system services, it records the network traffic, and it tracks data flows. For each submission, reports are generated that provide comprehensive reports about the activities of the binary under analysis. Anubis receives malware samples through a public web interface and a number of feeds from security organizations and anti-malware companies. Because the samples are collected from a wide range of users, the collected samples represent a comprehensive and diverse mix of malware found in the wild. In this paper, we aim to shed light on common malware behaviors. To this end, we evaluate the Anubis analysis results for almost one million malware samples, study trends and evolution of malicious behaviors over a period of almost two years, and examine the influence of code polymorphism on malware statistics.\"",
        "title: \"DIWE: a framework for constructing device-independent web applications\" with abstract: \"Recent developments in mobile computing software and hardware have highlighted the importance of device-independent access to Web content. This paper introduces a novel conceptual framework for constructing device-independent Web applications. The Device-Independent Web Engineering (DIWE) framework is composed of an XML-based Web language that is used to separate the layout, content and application logic and to model the Web applications and four run-time processors that provide device-independence support during application execution.\"",
        "title: \"Limits of Static Analysis for Malware Detection\" with abstract: \"Relay nodes are a potential threat to networks since they are used in many malicious situations like stepping stone attacks, botnet communication, peer-to-peer streaming etc. Quick and accurate detection of relay nodes in a network can significantly improve security policy enforcement. There has been significant work done and novel solutions proposed for the problem of identifying relay flows active within a node in the network. However these solutions require quadratic number of comparisons in the number of flows. In this paper, a related problem of identifying relay nodes is investigated where a relay node is defined as a node in the network that has an active relay flow. The problem is formulated as a variance estimation problem and a statistical approach is proposed for the solution. The proposed solution requires linear time and space in the number of flows and therefore can be employed in large scale implementations. It can be used on its own to identify relay nodes or as a first step in a scalable relay flow detection solution that performs known quadratic time analysis techniques for relay flow detection only on nodes that have been detected as relay nodes. Experimental results show that the proposed scheme is able to detect relay nodes even in the presence of intentional inter-packet delays and chaff packets introduced by adversaries in order to defeat timing based detection algorithms.\"",
        "1 is \"A simulation tool for dynamically reconfigurable field programmable gate arrays\", 2 is \"Engineering flexible World Wide Web services\".",
        "\nGiven above information, for an author who has written the paper with the title \"A layout-similarity-based approach for detecting phishing pages.\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01178": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Periodicity Detection And Localization Using Spike Timing From The Aer Ear':",
        "title: \"Micro Stories and Mega Stories\" with abstract: \"Given the status of technology today, it is easy to capture experiential data, such as photos, for events and even organize them. That means that one can now easily store experiential data and use it to tell stories. The authors explore the new Internet technology that has led to two novel forms of storytelling that they call micro stories and mega stories.\"",
        "title: \"Video Rewrite: driving visual speech with audio\" with abstract: \"Video Rewrite uses existing footage to create automatically new video of a person mouthing words that she did not speak in the original footage. This technique is useful in movie dubbing, for example, where the movie sequence can be modified to sync the actors' lip motions to the new soundtrack. Video Rewrite automatically labels the phonemes in the train- ing data and in the new audio track. Video Rewrite reorders the mouth images in the training footage to match the phoneme sequence of the new audio track. When particular phonemes are unavailable in the training footage, Video Rewrite selects the clos- est approximations. The resulting sequence of mouth images is stitched into the background footage. This stitching process auto- matically corrects for differences in head position and orientation between the mouth images and the background footage. Video Rewrite uses computer-vision techniques to track points on the speaker's mouth in the training footage, and morphing tech- niques to combine these mouth gestures into the final video sequence. The new video combines the dynamics of the original actor's articulations with the mannerisms and setting dictated by the background footage. Video Rewrite is the first facial-animation system to automate all the labeling and assembly tasks required to resync existing footage to a new soundtrack.\"",
        "title: \"The information content of demodulated speech\" with abstract: \"In this paper we describe the effect of demodulation on speech signals. We compare two different algorithms for demodulating audio: the classic approach based on the Hilbert transform and a new approach based on solving a convex optimization problem. We show that convex demodulation better separates the speech information between the modulator and the carrier. We demonstrate this advantage by measuring the speech-information content using a speech-recognition experiment. Finally, we explore the effect of subband filtering on the demodulation process and the shift of information from the modulator to the carrier as the subbands become wider.\"",
        "title: \"Pattern Playback in the 90s\" with abstract: \"Deciding the appropriate representation to use for modeling human auditory processing is a critical issue in auditory science. While engi(cid:173) neers have successfully performed many single-speaker tasks with LPC and spectrogram methods, more difficult problems will need a richer representation. This paper describes a powerful auditory representation known as the correlogram and shows how this non-linear representation can be converted back into sound, with no loss of perceptually impor(cid:173) tant information. The correlogram is interesting because it is a neuro(cid:173) physiologically plausible representation of sound. This paper shows improved methods for spectrogram inversion (conventional pattern playback), inversion of a cochlear model, and inversion of the correlo(cid:173) gram representation.\"",
        "title: \"A Study of Multimodal Addressee Detection in Human-Human-Computer Interaction\" with abstract: \"The goal of addressee detection is to answer the question , Are you talking to me? When a dialogue system interacts with multiple users, it is crucial to detect when a user is speaking to the system as opposed to another person. We study this problem in a multimodal scenario, using lexical, acoustic, visual, dialogue state, and beamforming information. Using data from a multiparty dialogue system, we quantify the benefits of using multiple modalities over using a single modality. We also assess the relative importance of the various modalities, as well as of key individual features, in estimating the addressee. We find that energy-based acoustic features are by far the most important, that information from speech recognition and system state is useful as well, and that visual and beamforming features provide little additional benefit. While we find that head pose is affected by whom the speaker is addressing, it yields little nonredundant information due to the system acting as a situational attractor. Our findings would be relevant to multiparty, open-world dialogue systems in which the agent plays an active, conversational role, such as an interactive assistant deployed in a public, open space. For these scenarios , our study suggests that acoustic, lexical, and system-state information is an effective and practical combination of modalities to use for addressee detection. We also consider how our analyses might be affected by the ongoing development of more realistic, natural dialogue systems.\"",
        "1 is \"Quantification Of A Spike-Based Winner-Take-All Vlsi Network\", 2 is \"Clustering by Scale-Space Filtering\".",
        "\nGiven above information, for an author who has written the paper with the title \"Periodicity Detection And Localization Using Spike Timing From The Aer Ear\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01179": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Modifying transients for efficient coding of audio':",
        "title: \"Convergence Of Generalized Linear Coordinate-Descent Message-Passing For Quadratic Optimization\" with abstract: \"We study the generalized linear coordinate-descent (GLiCD) algorithm for the quadratic optimization problem. As an extension of the linear coordinate-descent (LiCD) algorithm, the GLiCD algorithm incorporates feedback from last iteration in generating new messages. We show that if the amount of feedback signal from last iteration is above a threshold and the GLiCD algorithm converges, it computes the optimal solution. Based on the result, we further show that if the feedback signal is large enough, the GLiCD algorithm is guaranteed to converge.\"",
        "title: \"Structured total least squares based internal delay estimation for distributed microphone auto-localization\" with abstract: \"Auto-localization in wireless acoustic sensor networks (WASNs) can be achieved by time-of-arrival (TOA) measurements between sensors and sources. Most existing approaches are centralized, and they require a fusion center to communicate with other nodes. In practice, WASN topologies are time-varying with nodes joining or leaving the network, which poses scalability issues for such algorithms. In particular, for an increasing number of nodes, the total transmission power required to reach the fusion center increases. Therefore, in order to facilitate scalability, we present a structured total least squares (STLS) based internal delay estimation for distributed microphone localization where the internal delay refers to the time taken for a source signal reaching a sensor to that it is registered as received by the capture device. Each node only needs to communicate with its neighbors instead of with a remote host, and they run an STLS algorithm locally to estimate local internal delays and positions (i.e., its own and those of its neighbors), such that the original centralized computation is divided into many subproblems. Experiments demonstrate that the decentralized internal delay estimation converges to the centralized results with increasing signal-to-noise ratio (SNR). More importantly, less computational complexity and transmission power are required to obtain comparable localization accuracy.\"",
        "title: \"Evaluation of Binaural Noise Reduction Methods in Terms of Intelligibility and Perceived Localization\" with abstract: \"In this paper, we perceptually evaluate two recently proposed binaural multi-microphone speech enhancement methods in terms of intelligibility improvement and binaural-cue preservation. We compare these two methods with the well-known binaural minimum variance distortionless response (BMVDR) method. More specifically, we measure the 50% speech reception threshold, and the localization error of all dominant point sources in three different acoustic scenes. The listening tests are divided into a parameter selection phase and a testing phase. The parameter selection phase is used to select the algorithms' parameters based on one acoustic scene. In the testing phase, the two methods are evaluated in two other acoustic scenes in order to examine their robustness. Both methods achieve significantly better intelligiblity compared to the unprocessed scene, and slightly worse intelligibility than the BMVDR method. However, unlike the BMVDR method which severely distorts the binaural cues of all interferers, the new methods achieve localization errors which are not significantly different compared to those of the unprocessed scene.\"",
        "title: \"High rate spherical quantization of sinusoidal parameters\" with abstract: \"Quantization of sinusoidal model parameters is of importance in e.g. low-rate audio coding. In this work we introduce entropy constrained unrestricted spherical quantization, where amplitude, phase and frequency are quantized dependently. We derive a high-rate approximation of the average \u21132-distortion and use this to analytically derive formulas for optimal spherical scalar quantizers. These quantizers minimize the average distortion, while the corresponding quantization indices satisfy an entropy constraint. The quantizers turn out to be flexible and of low complexity, in the sense that they can be determined for varying entropy constraints without any iterative retraining procedures. As a consequence of minimizing the \u21132-norm of the (quantization) error signal, the quantizers depend on both the shape and length of the analysis/synthesis window.\"",
        "title: \"Generalized linear coordinate-descent message-passing for convex optimization\" with abstract: \"In this paper we propose a generalized linear coordinate-descent (GLiCD) algorithm for a class of unconstrained convex optimization problems. The considered objective function can be decomposed into edge-functions and node-functions of a graphical model. The messages of the GLiCD algorithm are in a form of linear functions, as compared to the min-sum algorithm of which the form of messages depends on the objective function. Thus, the implementation of the GLiCD algorithm is much simpler than that of the min-sum algorithm. A theorem is stated according to which the algorithm converges to the optimal solution if the objective function satisfies a diagonal-dominant condition. As an application, the GLiCD algorithm is exploited in solving the averaging problem in sensor networks, where the performance is compared to that of the min-sum algorithm.\"",
        "1 is \"Irrelevant Features and the Subset Selection Problem\", 2 is \"Model-based packet loss concealment for AMR coders\".",
        "\nGiven above information, for an author who has written the paper with the title \"Modifying transients for efficient coding of audio\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01180": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'A Small-Cell Caching System in Mobile Cellular Networks With LoS and NLoS Channels.':",
        "title: \"Sinr Degradation Due To Carrier Frequency Offset In Ofdm Based Amplify-And-Forward Relay Systems\" with abstract: \"In this letter, signal to interference plus noise ratio (SINR) performance is analyzed for orthogonal frequency division multiplexing (OFDM) based amplify-and-forward (AF) relay systems in the presence of carrier frequency offset (CFO) for fading channels. The SINR expression is derived under the one-relay-node scenario, and is further extended to the multiple-relay-node scenario. Analytical results show that the SINR is quite sensitive to CFO and the sensitivity of the SINR to CFO is mainly determined by the gain factor and the different power of the direct link channel and relay link channel.\"",
        "title: \"Energy Efficiency Comparison Between Distributed And Co-Located Mimo Systems\" with abstract: \"Energy efficiency (EE) is becoming more and more important in future wireless communications because of limited battery power in mobile terminals. In this paper, we compare EE of the distributed MIMO (D-MIMO) and co-located MIMO (C-MIMO) in uplink systems. Taking into account both circuit and transmit power, we derive an analytical expression for EE of D-MIMO and C-MIMO systems in a composite Rayleigh-lognormal channel. What is more, an optimization algorithm is proposed to get the optimal EE values while satisfying given spectral efficiency requirement for both D-MIMO and C-MIMO systems. Simulation results show that D-MIMO systems are more energy effective than C-MIMO systems when considering the realistic systems, and the optimal EE can be obtained by the proposed algorithm while satisfying given spectral efficiency requirement. Copyright (c) 2012 John Wiley & Sons, Ltd.\"",
        "title: \"Parallel Weighted Bit-Flipping Decoding\" with abstract: \"A parallel weighted bit-flipping (PWBF) decoding algorithm for low-density parity-check (LDPC) codes is proposed. Compared to the best known serial weighted bit-flipping decoding, the PWBF decoding converges significantly faster but with little performance penalty. For decoding of finite-geometry LDPC codes, we demonstrate through examples that the proposed PWBF decoding converges in about 5 iterations with performance very close to that of the standard belief-propagation decoding.\"",
        "title: \"Design Criteria for Distributed Antenna Systems\" with abstract: \"In this paper, we discuss three different design criteria for a distributed antenna system (DAS). They are maximizing throughput under the constraint of the overall transmit power, minimizing the overall transmit power while guaranteeing the minimum spectral efficiency (SE) requirements, and maximizing energy efficiency (EE) under the constraints of minimum SE requirements and overall transmit power. We use sub-gradient iteration approach to solve the first two optimization problems and exploit fractional programming method to deal with the third one. Based on these design criteria, three power allocation algorithms are developed for the downlink multi-user DAS. Depending on application enviroments, we can use the first and second criteria to achieve the highest throughput and to save the most energy, respectively, while we can balance throughput and energy consumption using the third criterion.\"",
        "title: \"A distributed cooperative MAC for cognitive radio Ad-hoc networks\" with abstract: \"Cognitive radio has been suggested as an efficient method for secondary users to promote the efficient utilization of spectrum. Meanwhile, Cooperative relay allows different users or nodes to share resources and to create collaboration through distributed transmission in a wireless networks. The combination of Cognitive radio with cooperative communication could significantly improve the system performance in cognitive radio ad-hoc networks (CRAHNs). In this paper, we discuss how to use cooperative relay to increase the transmission rate in CRAHNs. We first give a new distributed relay selection algorithm, it considers several aspects including channel gain, channel available probability and spectrum heterogeneity of secondary nodes. A cooperative MAC protocol, Cooper-MAC, is then proposed for CRAHNs which enables secondary users to negotiate channels and relays. Simulation results demonstrate the effectiveness of the Cooper-MAC.\"",
        "1 is \"Trellis-coded modulation with redundant signal sets Part II: State of the art\", 2 is \"Blind MIMO system identification based on cumulant subspace decomposition\".",
        "\nGiven above information, for an author who has written the paper with the title \"A Small-Cell Caching System in Mobile Cellular Networks With LoS and NLoS Channels.\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01181": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Approximation of centroid end-points and switch points for replacing type reduction algorithms':",
        "title: \"Spatial Filtering for EEG-Based Regression Problems in Brain-Computer Interface (BCI).\" with abstract: \"Electroencephalogram (EEG) signals are frequently used in brain-computer interfaces (BC!s), but they are easily contaminated by artifacts and noise, so preprocessing must be done before they are fed into a machine learning algorithm for classification or regression. Spatial filters have been widely used to increase the signal-to-noise ratio of EEG for BC! classification problems, but their applica...\"",
        "title: \"Optimal Arousal Identification and Classification for Affective Computing Using Physiological Signals: Virtual Reality Stroop Task\" with abstract: \"A closed-loop system that offers real-time assessment and manipulation of a user's affective and cognitive states is very useful in developing adaptive environments which respond in a rational and strategic fashion to real-time changes in user affect, cognition, and motivation. The goal is to progress the user from suboptimal cognitive and affective states toward an optimal state that enhances user performance. In order to achieve this, there is need for assessment of both 1) the optimal affective/cognitive state and 2) the observed user state. This paper presents approaches for assessing these two states. Arousal, an important dimension of affect, is focused upon because of its close relation to a user's cognitive performance, as indicated by the Yerkes-Dodson Law. Herein, we make use of a Virtual Reality Stroop Task (VRST) from the Virtual Reality Cognitive Performance Assessment Test (VRCPAT) to identify the optimal arousal level that can serve as the affective/cognitive state goal. Three stimuli presentations (with distinct arousal levels) in the VRST are selected. We demonstrate that when reaction time is used as the performance measure, one of the three stimuli presentations can elicit the optimal level of arousal for most subjects. Further, results suggest that high classification rates can be achieved when a support vector machine is used to classify the psychophysiological responses (skin conductance level, respiration, ECG, and EEG) in these three stimuli presentations into three arousal levels. This research reflects progress toward the implementation of a closed-loop affective computing system.\"",
        "title: \"Pool-Based Sequential Active Learning for Regression.\" with abstract: \"Active learning (AL) is a machine-learning approach for reducing the data labeling effort. Given a pool of unlabeled samples, it tries to select the most useful ones to label so that a model built from them can achieve the best possible performance. This paper focuses on pool-based sequential AL for regression (ALR). We first propose three essential criteria that an ALR approach should consider in selecting the most useful unlabeled samples: informativeness, representativeness, and diversity, and compare four existing ALR approaches against them. We then propose a new ALR approach using passive sampling, which considers both the representativeness and the diversity in both the initialization and subsequent iterations. Remarkably, this approach can also be integrated with other existing ALR approaches in the literature to further improve the performance. Extensive experiments on 11 University of California, Irvine, Carnegie Mellon University StatLib, and University of Florida Media Core data sets from various domains verified the effectiveness of our proposed ALR approaches.\"",
        "title: \"Enhanced Interval Approach for encoding words into interval type-2 fuzzy sets and convergence of the word FOUs\" with abstract: \"The Interval Approach (IA) [4] is a method for synthesizing an interval type-2 fuzzy set (IT2 FS) model for a word from data that are collected from a group of subjects. A key assumption made by the IA is: each person's data interval is random and uniformly distributed. This means, of course, that the IT2 FS model for the word is random. Consequently, one can question whether or not the IT2 FS model for the word converges in a stochastic sense. This paper focuses on this question. As a part of our study, we have had to modify some steps of the IA, the resulting being an Enhanced IA (EIA). The paper shows by means of some simulations, that the IT2 FS word models that are obtained from the EIA are converging in a mean-square sense. This provides substantial credence for using the EIA to obtain T2 FS word models.\"",
        "title: \"Switching EEG Headsets Made Easy: Reducing Offline Calibration Effort Using Active Weighted Adaptation Regularization.\" with abstract: \"Electroencephalography (EEG) headsets are the most commonly used sensing devices for brain-computer interface. In real-world applications, there are advantages to extrapolating data from one user session to another. However, these advantages are limited if the data arise from different hardware systems, which often vary between application spaces. Currently, this creates a need to recalibrate clas...\"",
        "1 is \"Nonlinear tracking control in the presence of state and control constraints: a generalized reference governor\", 2 is \"Using fuzzy labels as background knowledge for linguistic summarization of databases\".",
        "\nGiven above information, for an author who has written the paper with the title \"Approximation of centroid end-points and switch points for replacing type reduction algorithms\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01182": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Similarity-based perceptual reasoning for perceptual computing':",
        "title: \"Introduction to uncertainty bounds and their use in the design of interval type-2 fuzzy logic systems\" with abstract: \"In this paper, we derive inner- and outer-bound sets for the type-reduced set of an interval type-2 fuzzy logic system, based on a new mathematical interpretation of the Karnik-Mendel (2001) iterative procedure. The bound sets can not only provide estimates about the uncertainty contained in the output, but can also be used to design an interval type-2 fuzzy logic system. We demonstrate, by means of a simulation experiment, that the resulting system can operate without type reduction and that it can achieve similar performance to one that uses type reduction. Therefore, our new design method, based on the bound sets, can relieve the computational burden of an interval type-2 fuzzy logic system during its operation.\"",
        "title: \"Comment on \u201cToward General Type-2 Fuzzy Logic Systems Based on zSlices\u201d\" with abstract: \"Wagner and Hagras introduced a novel defuzzification formula in their recent paper and showed that it works very well within the framework of their general type-2 fuzzy logic systems based on zSlices ( $\\alpha$-plane representation). This letter aims to point out the hidden connection between the standard centroid defuzzification formula and Wagner and Hagras\u2019 new defuzzification formula, which leads to the proof of complete equivalence of the two.\"",
        "title: \"Simplified Interval Type-2 Fuzzy Logic Systems\" with abstract: \"Type reduction (TR) followed by defuzzification is commonly used in interval type-2 fuzzy logic systems (IT2 FLSs). Because of the iterative nature of TR, it may be a computational bottleneck for the real-time applications of an IT2 FLS. This has led to many direct approaches to defuzzification that bypass TR, the simplest of which is the Nie\u2013Tan direct defuzzification method (NT method). This paper provides some theoretical analyses of the NT method that answer the question \u201cWhy is the NT method good to use?\u201d This paper also provides a direct relationship between TR followed by defuzzification (using KM algorithms) and the NT method. It also provides an improved NT method. Numerical examples illustrate our theoretical results and suggest that the NT method is a very good way to simplify an interval type-2 fuzzy set.\"",
        "title: \"An Interval Approach To Fuzzistics For Interval Type-2 Fuzzy Sets\" with abstract: \"In this paper, a new and simple approach, called Interval Approach, to type-2 fuzzistics is presented, one that captures the strong points of both the person-MF and interval end-points approaches. It uses interval end-point data that are collected from a group of subjects, assumes a probability distribution for each person's data and maps the mean and standard deviation of that distribution into the parameters of an iteratively specified type-1 person MF. These type-1 person MFs are then aggregated using the union leading to the FOU for a word. Experiments show that this approach is easy to implement and the derived interval type-2 word models match our intuitions, i.e., the FOUs of the small-sounding words are located to the left, the FOUs of the medium-sounding words are located in the middle, and the FOUs of the large-sounding words are located to the right.\"",
        "title: \"On a 50% savings in the computation of the centroid of a symmetrical interval type-2 fuzzy set\" with abstract: \"Computing the centroid of a type-2 fuzzy set (T2 FS) is an important operation for such sets. For an interval T2 FS, the centroid can be computed by using two iterative procedures that were developed by Karnik and Mendel [2]. In this paper, we prove that if the footprint of uncertainty for an interval T2 FS is symmetrical about the primary variable y at y = m, then the centroid is also symmetrical about y = m and its defuzzified value equals m. As a consequence of this, computation of the centroid for such a T2 FS is reduced by 50%, and the importance of obtaining a non-symmetrical interval T2 FS prior to defuzzification is demonstrated.\"",
        "1 is \"Protein network inference from multiple genomic data: a supervised approach.\", 2 is \"A Type-2 Fuzzy Approach to Linguistic Summarization of Data\".",
        "\nGiven above information, for an author who has written the paper with the title \"Similarity-based perceptual reasoning for perceptual computing\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01183": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'A Tabu Search Algorithm for Scheduling Independent Jobs in Computational Grids':",
        "title: \"A layered framework for evaluating on-line collaborative learning interactions\" with abstract: \"Evaluating on-line collaborative learning interactions is a complex task due to the variety of elements and factors that take place and intervene in the way a group of students comes together to collaborate in order to achieve a learning goal. The aim of this paper is to provide a better understanding of group interaction and determine how to best support the collaborative learning process. To that end, we propose a principled framework for the study and analysis of group interaction and group scaffolding which is built by combining different aspects and issues of collaboration, learning and evaluation. In particular, we define learning activity indicators at several levels of description which prompt to the application of a mixed interaction analysis scheme and the use of different data types and specific tools. At an initial layer, the basis of the approach is set by applying a qualitative process for evaluating the individual and group task performance as well as the group functioning and scaffolding. The interaction analysis process is completed by defining and applying two more layers: a social network analysis of the group activity and participation behaviour and a quantitative analysis of group effectiveness as regards task achievement and active interaction involvement. Our work defines a grounded and holistic conceptual model that describes on-line collaborative learning interactions sufficiently and applies it in a real, web-based, complex and long-term collaborative learning situation. An in-depth empirical evaluation of the conceptual model is fully discussed, which demonstrates the usefulness and value of the approach.\"",
        "title: \"Another Look at Ciphertext Updating Algorithms for Cloud Storage.\" with abstract: \"Ciphertext updating is a basic requirement for cloud storage, such as updating the data contents for the data owners, revoking the old data users and sharing the encrypted data contents for new users etc. Recently, several famous ciphertext updating algorithms for cloud storage have been proposed. However, we observe that almost all of them relying the cloud to implement this updating without any auditing mechanism. But we know the cloud can only be semi-trusted, without any auditing mechanism the cloud can implement the updating algorithm with arbitrary departure, such as replacing the original ciphertexts with any new invalid ciphertexts as the updating algorithm's output. By reviewing of these algorithms, we give another look on the security of them. We think the security weakness is essential, and thus new techniques to implement ciphertext updating for cloud storage but with efficient (public or private) auditing mechanism should be urgently developed if these updating algorithms will be adapted in practical applications.\"",
        "title: \"Performance Analysis of WMNs by WMN-GA Simulation System for Exponential Distribution Considering EDCA and DCF\" with abstract: \"In this paper, we evaluate the performance of WMN using our WMN-GA simulation system considering throughput, delay, jitter and fairness index metrics. For simulations, we used ns-3 and Optimized Link State Routing (OLSR). We compare the performance of Distributed Coordination Function (DCF) and Enhanced Distributed Channel Access (EDCA) for exponential distribution of mesh clients by sending multiple Constant Bit Rate (CBR) flows in the network. The simulation results show that for Hybrid WMN, the throughput of both MAC protocols is higher than I/B WMN. The delay and jitter of Hybrid WMN are lower than I/B WMN. The fairness index of I/B WMN is a little bit higher than Hybrid WMN.\"",
        "title: \"Application of GA and Multi-objective Optimization for QoS Routing in Ad-Hoc Networks\" with abstract: \"Much work has been done on routing in Ad-hoc networks, but the proposed routing solutions only deal with the best effort data traffic. Connections with Quality of Service (QoS) requirements, such as voice channels with delay and bandwidth constraints, are not supported. The QoS routing has been receiving increasingly intensive attention, but searching for the shortest path with many metrics is an NP-complete problem. For this reason, approximated solutions and heuristic algorithms should be developed for multi-path constraints QoS routing. Also, the routing methods should be adaptive, flexible, and intelligent. In this paper, we use Genetic Algorithms (GAs) and Multi-objective Optimization for QoS routing in Ad-hoc Networks. In order to reduce the search space of GA, we implemented a Search Space Reduction Algorithm (SSRA). After the reduction of search space the GAMAN search time improves. Our proposed method has the best performance for crossover rate 70% and mutation rate 8%.\"",
        "title: \"Performance Evaluation of WMN-GA System for Dense Networks Considering Different Distributions\" with abstract: \"With the emergence of several new networking paradigms, optimization modeling and resolution turns out to be crucial to achieve optimized performance networks. One such networking paradigm that requires resolution of optimization problems is Wireless Mesh Networks (WMNs). In this paper, we deal with the effects of population size in GA for node placement problem in WMNs. We evaluate the performance of the proposed system for dense networks considering different distributions and considering giant component and number of covered users parameters. The simulation results show that proposed system has better performance in dense networks like hotspots for Wei bull distribution when the population size is big.\"",
        "1 is \"Minimum-energy broadcast in all-wireless networks: NP-completeness and distribution issues\", 2 is \"Adjustable flooding-based discovery with multiple QoSs for cloud services acquisition\".",
        "\nGiven above information, for an author who has written the paper with the title \"A Tabu Search Algorithm for Scheduling Independent Jobs in Computational Grids\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01184": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Modeling and performance analysis of networking and collaborative systems':",
        "title: \"A Multi-fold Time Approach to Address Emotions in Live and Virtualized Collaborative Learning\" with abstract: \"The enrichment of Computer Supported Collaborative (CSCL) Systems with emotion awareness features (detect emotion patterns and respond effectively) opens a window to the future of learner-to-computer interaction. In the current paper we present a system's design that attempts to evaluate the learner's affective state while he/she is taking part into collaborative tasks, in three different time-points: (i) before the task, (ii) in real-time (while collaborating) and (iii) retrospective (after the task). For the emotion assessment process, self-reporting and sentiment analysis are explored. Based on effective production rules that correlate emotions or emotion sequences to affective feedback techniques, the system will provide emotion scaffolds corresponding to user needs and feelings.\"",
        "title: \"A model for providing emotion awareness and feedback using fuzzy logic in online learning.\" with abstract: \"Monitoring users\u2019 emotive states and using that information for providing feedback and scaffolding is crucial. In the learning context, emotions can be used to increase students\u2019 attention as well as to improve memory and reasoning. In this context, tutors should be prepared to create affective learning situations and encourage collaborative knowledge construction as well as identify those students\u2019 feelings which hinder learning process. In this paper, we propose a novel approach to label affective behavior in educational discourse based on fuzzy logic, which enables a human or virtual tutor to capture students\u2019 emotions, make students aware of their own emotions, assess these emotions and provide appropriate affective feedback. To that end, we propose a fuzzy classifier that provides a priori qualitative assessment and fuzzy qualifiers bound to the amounts such as few, regular and many assigned by an affective dictionary to every word. The advantage of the statistical approach is to reduce the classical pollution problem of training and analyzing the scenario using the same dataset. Our approach has been tested in a real online learning environment and proved to have a very positive influence on students\u2019 learning performance.\"",
        "title: \"Enhancing Knowledge Management In Online Collaborative Learning\" with abstract: \"This paper aims to explore two crucial aspects of collaborative work and learning: on the one hand, the importance of enabling collaborative learning applications to capture and structure the information generated by group activity and, on the other hand, to extract the relevant knowledge in order to provide learners and tutors with efficient awareness, feedback and support with regards to group performance and collaboration. To this end, in this paper we first propose a conceptual model for data analysis and management that identifies and classifies the many kinds of indicators that describe collaboration and learning into high-level aspects of collaboration. Then, we provide a computational platform that, at a first step, collects and classifies both the event information generated asynchronously from the users' actions and the labeled dialogues from the synchronous collaboration according to these indicators. This information is then analyzed in next steps to eventually extract and present to participants the relevant knowledge about the collaboration. The ultimate aim of this platform is to efficiently embed information and knowledge into collaborative learning applications. We eventually suggest a generalization of our approach to be used in diverse collaborative learning situations and domains.\"",
        "title: \"Improving e-Assessment in Collaborative and Social Learning Settings\" with abstract: \"Cognitive assessment in collaborative and social learning requires assessment processes that achieve significant effect on collaborative learning and engage learners through accountability and constructive feedback. In order to design a coherent and efficient assessment system for collaborative and social learning it is necessary to design an enriched learning experience that predisposes the feedback and awareness in the group. This research focuses on e-assessment of collaborative learning and extends it with Social Network Analysis (SNA) techniques that are able to analyze and represent social network interaction during the live collaborative sessions. The interaction data extracted from social and collaborative networking must be integrated into a general assessment system to produce an efficient and personalized awareness and feedback about the collaborative activity and the social behavior of the participants. In previous work we provided a conceptual and methodological research approach of e-assessment applications and tools that meet the mentioned requirements and goals. In this paper we provide empirical data and interpretation to validate the approach.\"",
        "title: \"A Prototype of an eLearning Platform in Support for Learning Analytics and Gamification\" with abstract: \"This paper presents the implementation and prototyping of an innovative web-based eLearning platform, featuring Learning Analytics and Gamification called ICT-FLAG. A previous contribution presented the analysis and design of the platform. Following the design, this paper implements the platform and reports on the first experiences of connecting and integrating the platform with a real eLearning tool through an API. The purpose of this connection is to provide the tool with learning analytics and gamification-based services. The research reported in this paper is currently undertaken within the research project \"Enhancing ICT education through Formative assessment, Learning Analytics and Gamification\" (ICT-FLAG) funded by the Spanish Government.\"",
        "1 is \"Design and development of a general purpose collaborative environment\", 2 is \"Max-Min D-Cluster Formation in Wireless Ad Hoc Networks\".",
        "\nGiven above information, for an author who has written the paper with the title \"Modeling and performance analysis of networking and collaborative systems\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01185": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Control Theory Optimization of MECN in Satellite Networks':",
        "title: \"Design of an Omnidirectional Wheelchair for Playing Tennis\" with abstract: \"In our precious work, we presented the design and implementation of a omnidirectional wheelchair. The wheelchair with good performance for the aged and disabled is attracting attention from the society. Also, the wheelchair can provide the user with many benefits, such as maintaining mobility, continuing or broadening community and social activities, conserving strength and energy, and enhancing quality of life. The wheelchair body must be compact enough and should be able to make different movements in order to have many applications. In this paper, we present the design of an omnidirectional wheelchair for playing tennis. Finally, we discuss some implementation and application issues.\"",
        "title: \"A Simulation Study for Allocation of Base Stations in Mobile Communication Networks\" with abstract: \"In actual mobile communication networks, the users' locations, the number of mobile communication users, and their behavior change depending on the time zone. This dependency affects the cell traffic; thus, the allocation of Base Stations (BSs) in the service area is an important problem to be investigated. In this paper, we propose a new model for allocation of BSs in mobile communication networks, taking account of user distribution, user movement, channel occupancy, and handover. On the basis of this model, we developed a simulation system and carried out many simulations to evaluate the system performance. For simulations, we consider the actual location of road network and the population distribution in Yamagata City, Japan. We carried out many simulations for various scenarios and parameters. The performance evaluation shows that our simulation system makes a good BS allocation.\"",
        "title: \"Friedman Test for Analysing WMNs: A Comparison Study for Genetic Algorithms and Simulated Annealing\" with abstract: \"In this paper, we deal with connectivity and coverage problem in Wireless Mesh Networks (WMNs). We used Friedman test to check if we can compare Genetic Algorithm (GA) and Simulated Annealing (SA). We found out that GA and SA have differences in their performance. Then, we used the implemented systems WMN-GA and WMN-SA to evaluate and compare the performance of the systems for different distributions of mesh clients in terms of Size of Giant Component (SGC) and Number of Covered Mesh Clients (NCMC). The simulation results show that for Uniform distribution the WMN-SA performs better than WMN-GA. For Normal distribution, for big radius of communication distance, the WMN-GA has the best performance. For Exponential distribution, the WMN-SA performs better than WMN-GA for all communication distances. For Weibull distribution, the WMN-SA has better performance than WMN-GA.\"",
        "title: \"Experimental Results of a Raspberry Pi Based WMN Testbed for Multiple Flows and Distributed Concurrent Processing.\" with abstract: \"Wireless Mesh Networks (WMNs) are attracting a lot of attention from wireless network researchers, because of their potential use in several fields such as collaborative computing and communications. Considering mobility of the terminals, routing is a key process for operation of WMNs. In this paper, we present the implementation of a testbed for WMNs. We analyze the performance of Optimized Link State Routing (OLSR) protocol and parallel distributed processing in an indoor scenario. For evaluation we considered hop count, delay, jitter and processing time metrics. The experimental results show that the nodes in the testbed were communicating smoothly. The processing time for node 1 is smaller than other nodes.\"",
        "title: \"Experimental Results of a Raspberry Pi Based WMN Testbed for Different OSs in Indoor Environment Considering LoS Scenario.\" with abstract: \"Wireless Mesh Networks (WMNs) are attracting a lot of attention from wireless network researchers, because of their potential use in several fields such as collaborative computing and communications. In this paper, we present the implementation of a testbed for WMNs using Raspbian and OpenWRT OSs. We analyze the performance of Optimized Link State Routing (OLSR) protocol in an indoor scenario. For evaluation we considered throughput, Packet Delivery Ratio (PDR), delay, jitter and hop count metrics. The experimental results show that the testbed mounted in OpenWRT has better results than Raspbian.\"",
        "1 is \"Revealing skype traffic: when randomness plays with you\", 2 is \"Continuous Range Search Query Processing in Mobile Navigation\".",
        "\nGiven above information, for an author who has written the paper with the title \"Control Theory Optimization of MECN in Satellite Networks\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01186": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Configuring Function-based Communication Protocols for Multimedia Applications':",
        "title: \"Configuring Function-based Communication Protocols for Multimedia Applications\" with abstract: \"Next generation communication systems must support di- verse applications operating over high-performance local, metropolitan, and wide area networks. This paper describes a framework that contains a number of resource, language, and tool components for generating customized protocols to support diverse multimedia applications running in high- performance network environments. These components help to simplify the process of generating application-tailored communication protocols by automating many development and configuration steps. A collaborative distance learning application scenario is presented to motivate and demon- strate techniques used to compose function-based protocols that are customized for particular application requirements. In addition, the structure of a protocol resource pool that contains reusable protocol function building-blocks is also examined.\"",
        "title: \"DRE system performance optimization with the SMACK cache efficiency metric.\" with abstract: \"System performance improvements are critical for the resource-limited environment of multiple integrated applications executing inside a single distributed real-time and embedded (ORE) system, such as integrated avionics platform or vehtronics systems. While processor caches can effectively reduce execution time there are several factors, such as cache size, system data sharing, and task execution schedule, which make it hard to quantify, predict, and optimize the cache usage of a DRE system. This article presents SMACK, a novel heuristic for estimating the hardware cache usage of a DRE system, and describes a method of varying the runtime behavior of DRE system software without (1) requiring extensive safety recertification or (2) violating the real-time scheduling deadlines. By using SMACK as a maximization target, we were able to reduce integrated DRE system execution time by an average of 2.4% and a maximum of 4.34%. (C) 2014 Elsevier Inc. All rights reserved.\"",
        "title: \"Panels at MoDELS 2006\" with abstract: \"MoDELS 2006 contained the following two panels that provided an interactive forum to conduct lively discussions on subjects that are highly germane to conference attendees: Panel 1. Is Standardization of Model-Driven Technologies Harming or Helping the Field? The best standards seem to be those that arise from codifying technologies that have been vetted after extensive experience by researchers and practitioners over many years. Good examples of such standards include POSIX, the Internet protocols, Ada, C, C++, and Java. Although model-driven technologies have long been studied by researchers in-the-small, there is little practical experience yet applying model-driven tools in-the-large. Moreover, many of the specifications proposed by various standards groups have not un dergone the same degree of scrutiny and vetting as earlier language and platform tech nologies. As a result, model-driven technology standards are being proposed and adopted with neither a firm formal foundation nor significant practical experience. This panel will explore the extent to which this phenomenon is helping accelerate the adoption of model-driven technologies or hurting the field due to lack of credibility.\"",
        "title: \"Design and performance of an object-oriented framework for high-speed electronic medical imaging\" with abstract: \"This paper describes the design and performance of an object-oriented communication framework being developed by the Health Imaging division of Eastman Kodak and the Electronic Radiology Laboratory at Washington University School of Medicine. The framework is designed to meet the demands of next-generation electronic medical imaging systems, which must transfer extremely large quantities of data efficiently and flexibly in a distributed environment. A novel aspect of this framework is its seamless integration of flexible high-level CORBA distributed object computing middleware with efficient low-level socket network programming mechanisms. In the paper, we outline the design goals and software architecture of our framework, describe how we resolved design challenges, and illustrate the performance of the framework over high-speed ATM networks.\"",
        "title: \"Model-driven auto-scaling of green cloud computing infrastructure\" with abstract: \"Cloud computing can reduce power consumption by using virtualized computational resources to provision an application's computational resources on demand. Auto-scaling is an important cloud computing technique that dynamically allocates computational resources to applications to match their current loads precisely, thereby removing resources that would otherwise remain idle and waste power. This paper presents a model-driven engineering approach to optimizing the configuration, energy consumption, and operating cost of cloud auto-scaling infrastructure to create greener computing environments that reduce emissions resulting from superfluous idle resources. The paper provides four contributions to the study of model-driven configuration of cloud auto-scaling infrastructure by (1) explaining how virtual machine configurations can be captured in feature models, (2) describing how these models can be transformed into constraint satisfaction problems (CSPs) for configuration and energy consumption optimization, (3) showing how optimal auto-scaling configurations can be derived from these CSPs with a constraint solver, and (4) presenting a case study showing the energy consumption/cost reduction produced by this model-driven approach.\"",
        "1 is \"Shibboleth and community authorization services: enabling role-based grid access\", 2 is \"Local atomicity properties: modular concurrency control for abstract data types\".",
        "\nGiven above information, for an author who has written the paper with the title \"Configuring Function-based Communication Protocols for Multimedia Applications\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01187": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Proving Termination Using Recursive Path Orders and SAT Solving':",
        "title: \"Breaking Symmetries in Graph Search with Canonizing Sets\" with abstract: \"There are many complex combinatorial problems which involve searching for an undirected graph satisfying given constraints. Such problems are often highly challenging because of the large number of isomorphic representations of their solutions. This paper introduces effective and compact, complete symmetry breaking constraints for small graph search. Enumerating with these symmetry breaks generates all and only non-isomorphic solutions. For small search problems, with up to 10 vertices, we compute instance independent symmetry breaking constraints. For small search problems with a larger number of vertices we demonstrate the computation of instance dependent constraints which are complete. We illustrate the application of complete symmetry breaking constraints to extend two known sequences from the OEIS related to graph enumeration. We also demonstrate the application of a generalization of our approach to fully-interchangeable matrix search problems.\"",
        "title: \"Bottom-up abstract interpretation of logic programs\" with abstract: \"This paper presents a formal framework for the bottom-up abstract interpretation of logic programs which can be applied to approximate answer substitutions, partial answer substitutions and call patterns for a given program and arbitrary initial goal. The framework is based on a T p -like semantics defined over a Herbrand universe with variables which has previously been shown to determine the answer substitutions for arbitrary initial goals. The first part of the paper reconstructs this semantics to provide a more adequate initial goals. The first part of the paper reconstructs this semantics to provide a more adequate basis for abstract interpretation. A notion of abstract substitution is introduced and shown to determine an abstract semantic function which for a given program can be applied to approximate the answer substitutions for an arbitrary initial goal. The second part of the paper extends the bottom-up approach to provide approximations of both partial answer substitutions and call patterns. This is achieved by applying Magic Sets and other existing techniques to transform a program in such a way that the answer substitutions of the transformed program correspond to the partial answer substitutions and call patterns of the original program. This facilitates the analysis of concurrent logic programs (ignoring synchronization) and provides a collecting semantics which characterizes both success and call patterns.\"",
        "title: \"Proving Termination Using Recursive Path Orders and SAT Solving\" with abstract: \"We introduce a propositional encoding of the recursive path order with status (RPO). RPO is a combination of a multiset path order and a lexicographic path order which considers permutations of the arguments in the lexicographic comparison. Our encoding allows us to apply SAT solvers in order to determine whether a given term rewrite system is RPO-terminating. Furthermore, to apply RPO within the dependency pair framework, we combined our novel encoding for RPO with an existing encoding for argument filters. We implemented our contributions in the termination prover AProVE. Our experiments show that due to our encoding, combining termination provers with SAT solvers improves the performance of RPO-implementations by orders of magnitude.\"",
        "title: \"A simple polynomial groundness analysis for logic programs\" with abstract: \"The domain of positive Boolean functions, Pos, is by now well established for the analysis of the variable dependencies that arise within logic programs. Analyses based on Pos that use binary decision diagrams (BDDs) have been shown to be efficient for a wide range of practical programs. However, independent of the representation, a Pos analysis can never come with any efficiency guarantees because of its potential exponential behaviour. This paper considers groundness analysis based on a simple subdomain of Pos and compares its precision with that of Pos.\"",
        "title: \"Termination analysis of logic programs through combination of type-based norms\" with abstract: \"This article makes two contributions to the work on semantics-based termination analysis for logic programs. The first involves a novel notion of type-based norm where for a given type, a corresponding norm is defined to count in a term the number of subterms of that type. This provides a collection of candidate norms, one for each type defined in the program. The second enables an analyzer to base termination proofs on the combination of several different norms. This is useful when different norms are better suited to justify the termination of different parts of the program. Application of the two contributions together consists in considering the combination of the type-based candidate norms for a given program. This results in a powerful and practical technique. Both contributions have been introduced into a working termination analyzer. Experimentation indicates that they yield state-of-the-art results in a fully automatic analysis tool, improving with respect to methods that do not use both types and combined norms.\"",
        "1 is \"Acceptability with General Orderings\", 2 is \"Sorting Networks: The End Game.\".",
        "\nGiven above information, for an author who has written the paper with the title \"Proving Termination Using Recursive Path Orders and SAT Solving\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01188": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Online Ski Rental for ON/OFF Scheduling of Energy Harvesting Base Stations.':",
        "title: \"Sparse Signal Recovery With Omp Algorithm Using Sensing Measurement Matrix\" with abstract: \"Orthogonal matching pursuit (OMP) algorithm with random measurement matrix (RMM), often selects an incorrect variable due to the induced coherent interference between the columns of RMM. In this paper, we propose a sensing measurement matrix (SMM)-OMP which mitigates the coherent interference and thus improves the successful recovery probability of signal. It is shown that the SMM-OMP selects all the significant variables of the sparse signal before selecting the incorrect ones. We present a mutual incoherent property (MIP) based theoretical analysis to verify that the proposed method has a better performance than RMM-OMP. Various simulation results confirm our proposed method efficiency.\"",
        "title: \"The Application of MIMO to Non-Orthogonal Multiple Access.\" with abstract: \"This paper considers the application of multiple-input multiple-output (MIMO) techniques to nonorthogonal multiple access (NOMA) systems. A new design of precoding and detection matrices for MIMO-NOMA is proposed and its performance is analyzed for the case with a fixed set of power allocation coefficients. To further improve the performance gap between MIMO-NOMA and conventional orthogonal multip...\"",
        "title: \"Harq Throughput Performance Of Multicode Ds-Cdma With Mmse Turbo Equalization\" with abstract: \"Hybrid automatic repeat request (HARQ) is an indispensable technique for packet access. This is employed in high speed downlink packet access (HSDPA) using the orthogonal multicode direct sequence code division multiple access (DS-CDMA). As the data rate increases, the frequency-selectivity of the channel becomes severer and some equalization technique other than rake combining is necessary. The use of MMSE frequency-domain equalization (NIMSE-FDE) can significantly improve the throughput performance. However, the residual inter-chip-inference (ICI) after MMSE-FDE produces the orthogonality distortion among the spreading codes and the throughput performance is far from the theoretical lower bound. In this paper, we study an MMSE turbo equalization for HARQ using multicode DS-CDMA. It is shown by computer simulation that MMSE turbo equalization can significantly improve the throughput performance. An E-s/N-0 reduction of as much as 2.5 similar to 3 dB from the no turbo equalization case is obtained for a throughput range of 2 similar to 2.5 bit/s/Hz.\"",
        "title: \"Single-Carrier Hybrid Arq Using Joint Iterative Tx/Rx Mmse-Fde & Isi Cancellation\" with abstract: \"Recently, we proposed a joint iterative transmit/receive (Tx/Rx) minimum mean square error (MMSE) frequency-domain equalization (FDE) & inter-symbol interference (ISI) cancellation (ISIC) for single-carrier (SC) block transmissions. In this paper, we extend the previously proposed joint iterative Tx/Rx MMSE-FDE & ISIC to the SC hybrid automatic repeat request (HARQ) packet access to exploit the retransmission of the same packet for improving the throughput. We optimize the set of Tx/Rx MMSE-FDE weights by taking into account the packet retransmission/combining as well as ISIC. We show by computer simulation that the proposed scheme significantly improves the packet error rate (PER) and throughput performances in a severe frequency-selective fading channel.\"",
        "title: \"Adaptive window width control for cyclic shifted pilot aided channel estimation suitable for SC-ANC bi-directional relay using joint Tx/Rx MMSE-FDE\" with abstract: \"In analog network coded (ANC) relay with conventional channel estimation, the feedback of the estimated channel state information (CSI) is required, and hence, the bandwidth efficiency decreases. A cyclic-shifted pilot aided channel estimation (CSPACE) is used to simultaneously estimate two equivalent channels which are required for joint transmit/receive frequency-domain equalization (FDE) and own transmitted signal removal and hence, it requires no CSI feedback. In CSPACE, the delay time-domain windowing is used to separate two equivalent channels and suppress the impact of noise. However, in single-carrier (SC) ANC multi-antenna bi-directional relay (SC-ANC-MBDR) with the joint transmit/receive FDE, the equivalent channel is a concatenation of the propagation channel and the transmit FDE, and hence, its impulse response spreads over the entire delay time-domain. Therefore, the optimal delay time-domain window varies according to changing instantaneous received signal-to-noise power ratio (SNR), and as a consequence, the channel estimation accuracy degrades if the window width for the delay time-domain windowing is not adapted to the instantaneous received SNR. In this paper, we propose an adaptive window width control (AWWC) for CSPACE. The proposed AWWC adaptively changes the window size so as to minimize the mean square error (MSE) between the channel estimate and the actual channel. It is confirmed by the computer simulation that CSPACE using our proposed AWWC can always achieve BER performance superior to when using fixed window width.\"",
        "1 is \"Distributed User Association in Energy Harvesting Small Cell Networks: A Probabilistic Model.\", 2 is \"Gossip algorithms: design, analysis and applications\".",
        "\nGiven above information, for an author who has written the paper with the title \"Online Ski Rental for ON/OFF Scheduling of Energy Harvesting Base Stations.\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01189": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Fuzzy Linguistic Summaries in Rule-Based Adaptive Hypermedia Systems':",
        "title: \"Introducing Fuzziness In Object Models And Database Interfaces Through Aspects\" with abstract: \"Imperfection in information can be considered a crosscutting concern that manifests itself in diverse kinds of imprecision, uncertainty, or inconsistency in the data models of a software system. The extension of existing programming and querying interfaces for the different aspects of information imperfection requires a proper modularization of the different concerns of numerical imprecision handling, so that the extensions do not interfere with existing programming practices and do not obscure the original design. Aspect-oriented design (AOD) enables such a form of nonintrusive extensions to be added to existing software libraries, clearly separating fuzziness or other imperfections in data as a differentiated concern that can be considered from the early phases of development. In this article, a general framework for aspect-based extension of data models and fuzzy databases is described, and some design and implementation issues of such AOD-based extensions on OJB database libraries are described as a case study. (c) 2006 Wiley Periodicals, Inc.\"",
        "title: \"Empirical assessment of a collaborative filtering algorithm based on OWA operators\" with abstract: \"Classical collaborative filtering algorithms generate recommendations on the basis of ratings provided by users that express their subjective preference on concrete items. The correlation of ratings is used in such schemes as an implicit measure of common interest between users, that is used to predict ratings, so that these ratings determine recommendations. The common formulae used for the computation of predicted ratings use standard weighted averaging schemes as the fixed aggregation mechanism that determines the result of the prediction. Nonetheless, the surrounding context of these rating systems suggest that an approach considering a degree of group consensus in the aggregation process may better capture the essence of the \u201cword\u2013of\u2013mouth\u201d philosophy of such systems. This paper reports on the empirical evaluation of such an alternative approach in which OWA operators with different properties are tested against a dataset to search for the better empirical adjustment. The resulting algorithm can be considered as a generalization of the original Pearson formula based algorithm that allows for the fitting of the aggregation behavior to concrete databases of ratings. The results show that for the particular context studied, higher orness degrees reduce overall error measures, especially for high ratings, which are more relevant in recommendation settings. The adjustment procedure can be used as a general-purpose method for the empirical fit of the behavior of collaborative filtering systems. \u00a9 2008 Wiley Periodicals, Inc.\"",
        "title: \"Ontologies of Software Artifacts and Activities: Resource Annotation and Application to Learning Technologies\" with abstract: \"The emerging consensus on the boundaries and main el- ements of the Software Engineering (SE) discipline repre- sents an opportunity for the engineering of shared concep- tualizations that may serve both to design automated tools and tasks that help in diverse phases and aspects of the soft- ware process, and also to annotate learning-oriented re- sources. Formal ontologies provide an appropriate logics- based framework for such conceptual models. This paper describes the main ontological commitments that underlie the Onto-SWEBOK project, focusing on how SE artifacts and activities can be represented. The paper also discusses how semantic annotations can be provided for learning re- sources oriented to the initial and continuing education on the discipline, which enables the reuse of such resources in diverse learning designs.\"",
        "title: \"Exploring the Potential for Mapping Schema.org Microdata and the Web of Linked Data.\" with abstract: \"In recent years the exposure of Linked Open Data (LOD) has become widespread, with an increasing number of datasets available and enabling new opportunities for interlinking. In parallel, microdata in several forms has also proliferated mainly as a means to improve the effectiveness of search engines. Concretely, Schema. org consists of a vocabulary for microdata that enriches the information on pages, helping search engines to provide better results. In this paper we explore the potential of mapping Schema. org and the Web of Linked Data. First, mappings between Schema. org terms and terms in Linked Open Vocabularies (LOV) are extracted. Then we use these mappings to obtain an analysis, aimed at gaining insights about the potential impact of this vocabulary in the Web of Linked Data. The results show that is easier to find a mapping between classes than between properties, but the occurrences of these are many more in LOD.\"",
        "title: \"Representing instructional design methods using ontologies and rules\" with abstract: \"Instructional design theories are design theories that offer explicit guidance on how to help people to learn in specific situation. They can be used to guide the design of learning activities and the arrangement of associated resources. These theories are currently expressed in natural language, but they are often given some structure in terms of methods and conditions. Tools supporting theory-based instructional design require formal models of these theories to be expressed in languages with computational semantics, thus allowing their processing. Recent research has resulted in ontologies describing theory-neutral learning activity sequences and resources in accordance with proposed standards like IMS LD, but these are not sufficient for building instructional design aid tools. This paper describes the use of formal ontologies to partially represent instructional design methods in a form that can be used to build such supporting tools. Combining ontologies describing learning activities in a learning design with rules and constraints, it is possible to encode some forms of instructional design. This paper describes such an approach using OWL and SWRL. The approach has been evaluated by building a catalogue of instructional design methods expressed in these languages, obtained from a systematic extraction from a catalogue of instructional design theories. The practical utility of the ontological schema is demonstrated by means of a relevant case study.\"",
        "1 is \"A formal knowledge management ontology: Conduct, activities, resources, andinfluences\", 2 is \"Relief: a scalable actuated shape display\".",
        "\nGiven above information, for an author who has written the paper with the title \"Fuzzy Linguistic Summaries in Rule-Based Adaptive Hypermedia Systems\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01190": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Generating extractive summaries of scientific paradigms':",
        "title: \"Toward Answer-Focused Summarization Using Search Engines\" with abstract: \"People query search engines to find answers to a variety of questions on the Internet. Search cost would have been greatly reduced if search engines could accept natural language questions as queries, and provide summaries that contain the answers to these questions. We introduce the notion of Answer-Focused Summarization, which is to combine summarization and question answering. We develop a set of criteria and performance metrics, to evaluate Answer-Focused Summarization. We demonstrate that the summaries produced by Google, the most popular search engine nowadays, can be largely improved for question answering. We develop a proximity-based summary extraction system, and then utilize question types, i.e. whether the question is a \"person\" or a \"place\" question, to improve the performance. We suggest that there is a great research potential for Answer-Focused Summarization. 2 Chapter 18\"",
        "title: \"Biased LexRank: Passage retrieval using random walks with question-based priors\" with abstract: \"We present Biased LexRank, a method for semi-supervised passage retrieval in the context of question answering. We represent a text as a graph of passages linked based on their pairwise lexical similarity. We use traditional passage retrieval techniques to identify passages that are likely to be relevant to a user's natural language question. We then perform a random walk on the lexical similarity graph in order to recursively retrieve additional passages that are similar to other relevant passages. We present results on several benchmarks that show the applicability of our work to question answering and topic-focused text summarization.\"",
        "title: \"Columbia digital news system an environment for briefing and search over multimedia information\" with abstract: \"In this paper we describe an ongoing research project called the Columbia Digital News System. The goal of this project is to develop a suite of effective interoperable tools with which people can find relevant information (text, images, video, and structured documents) from distributed sources and track it over a period of time. Our initial focus is on the development of a system with which researchers, journalists, and students can keep track of current news events in specific areas.\"",
        "title: \"Summarization evaluation using relative utility\" with abstract: \"We present a series of experiments to demonstrate the validity of Relative Utility (RU) as a measure for evaluating extractive summarizers. RU is applicable in both single-document and multi-document summarization, is extendable to arbitrary compression rates with no extra annotation effort, and takes into account both random system performance and interjudge agreement. Our results using the JHU summary corpus indicate that RU is a reasonable and often superior alternative to several common evaluation metrics.\"",
        "title: \"Comparing semantically related sentences: the case of paraphrase versus subsumption\" with abstract: \"Paraphrases and other semantically related sentences present a challenge to NLP and IR applications such as multi-document summarization and question answering systems. While it is generally agreed that paraphrases contain approximately equivalent ideas, they often differ from one another in subtle, yet non-trivial, ways. In this paper, we examine semantic differences in cases of paraphrase and subsumption, in an effort to understand what makes one sentence significantly more informative than another. Using manually annotated data from the news domain, we concentrate on developing a framework for analyzing and comparing pairs of related sentences.\"",
        "1 is \"Syntactic Simplification and Semantic Enrichment - Trimming Dependency Graphs for Event Extraction.\", 2 is \"Handling Stuctural Divergences and Recovering Dropped Arguments in a Korean/English Machine Translation System\".",
        "\nGiven above information, for an author who has written the paper with the title \"Generating extractive summaries of scientific paradigms\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01191": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Symbolic planning and control of robot motion [Grand Challenges of Robotics]':",
        "title: \"Choosing Poses for Force and Stiffness Control.\" with abstract: \"In humanoids and other redundant robots interacting with the environment, one can often choose between different configurations and control parameters to achieve a given task. A classic tool to describe specifications of the desired force/displacement behavior in such problems is the stiffness ellipsoid, whose geometry is affected by the choice of parameters in both joint control and redundancy re...\"",
        "title: \"Haptic exploration of unknown surfaces with discontinuities\" with abstract: \"This work presents an approach for exploring unknown surfaces with discontinuities using only force/torque information. The motivation is to build an information map of an unknown object or environment by performing a fully-autonomous haptic exploration. Examples of discontinuities considered here are contours with sharp turns (such as wall corners) and abrupt dips (such as cliffs). Compliant motion control using force information has the ability to conform to unknown, smooth surfaces but not to discontinuous surfaces. This paper investigates solutions to address the limitation in compliant motion control over discontinuities while maintaining a desired normal force along the surface. We propose two methods to address the problem: (1) superposition of motion and force control and (2) rotation of axes for force and motion control. The theoretical principles are discussed and experimental results with a KUKA lightweight arm moving in 2D space are presented. Both approaches successfully negotiate objects with sharp 90-degree and 120-degree turns while still maintaining good tracking of the desired force.\"",
        "title: \"Shortest paths with side sensors\" with abstract: \"We present a complete characterization of shortest paths to a goal position for a vehicle with unicycle kinematics and a limited range sensor, constantly keeping a given landmark in sight. Previous work on this subject studied the optimal paths in case of a frontal, symmetrically limited Field-Of-View (FOV). In this paper we provide a generalization to the case of arbitrary FOVs, including the case that the direction of motion is not an axis of symmetry for the FOV, and even that it is not contained in the FOV. The provided solution is of particular relevance to applications using side-scanning, such as e.g. in underwater sonar-based surveying and navigation.\"",
        "title: \"A Real Time Robust Observer For An Agonist-Antagonist Variable Stiffness Actuator.\" with abstract: \"We consider the problem of estimating the timevarying stiffness in real-time of a Variable Stiffness Actuator in an agonistic-antagonistic configuration. The estimation of the stiffness is done in two steps. First, we use operational calculus which provides a relation between the positions/velocities of the motors and the link, the torques of the motors and the stiffness. Second, we combine the obtained relation with a polynomial approximation of the stiffness and a recursive least square algorithm to fit the data. Simulations and experimental results are provided and demonstrate the effectiveness of the proposed approach.\"",
        "title: \"Adaptive synergies for the design and control of the Pisa/IIT SoftHand\" with abstract: \"In this paper we introduce the Pisa/IIT SoftHand, a novel robot hand prototype designed with the purpose of being robust and easy to control as an industrial gripper, while exhibiting high grasping versatility and an aspect similar to that of the human hand. In the paper we briefly review the main theoretical tools used to enable such simplification, i.e. the neuroscience-based notion of soft synergies. A discussion of several possible actuation schemes shows that a straightforward implementation of the soft synergy idea in an effective design is not trivial. The approach proposed in this paper, called adaptive synergy, rests on ideas coming from underactuated hand design. A synthesis method to realize a desired set of soft synergies through the principled design of adaptive synergy is discussed. This approach leads to the design of hands accommodating in principle an arbitrary number of soft synergies, as demonstrated in grasping and manipulation simulations and experiments with a prototype. As a particular instance of application of the synthesis method of adaptive synergies, the Pisa/IIT SoftHand is described in detail. The hand has 19 joints, but only uses 1 actuator to activate its adaptive synergy. Of particular relevance in its design is the very soft and safe, yet powerful and extremely robust structure, obtained through the use of innovative articulations and ligaments replacing conventional joint design. The design and implementation of the prototype hand are shown and its effectiveness demonstrated through grasping experiments, reported also in multimedia extension.\"",
        "1 is \"Essentiality and damage in metabolic networks.\", 2 is \"Navigation using an appearance based topological map\".",
        "\nGiven above information, for an author who has written the paper with the title \"Symbolic planning and control of robot motion [Grand Challenges of Robotics]\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01192": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Almost Boolean functions: the design of Boolean functions by spectral inversion':",
        "title: \"Security evaluation of generalized patchwork algorithm from cryptanalytic viewpoint\" with abstract: \"In this paper we present a cryptanalysis of the generalized patchwork algorithm under the assumption that the attacker possesses only a single copy of the watermarked image. In the scheme, watermark is inserted by modifying randomly chosen DCT values in each block of the original image. Towards the attack we first fit low degree polynomials (which minimize the mean square error) on the data available from each block of the watermarked content. Then we replace the corresponding DCT data of the attacked image by the available data from the polynomials to construct an attacked image. The technique nullifies the modification achieved during watermark embedding. Experimental results show that recovery of the watermark becomes difficult after the attack.\"",
        "title: \"Application of Grover\u2019s algorithm to check non-resiliency of a Boolean function\" with abstract: \"In this paper, we explore quantum algorithms to check the resiliency property of a Boolean function (in particular, when it is non-resilient). First we explain that Deutsch-Jozsa algorithm can be immediately used for this purpose. We further analyse how the quadratic improvement in query complexity can be obtained using Grover's technique. While the worst case quantum query complexity to check the resiliency order is exponential in the number of input variables of the Boolean function, in our strategy one requires polynomially many measurements only. We also describe a subset of n-variable Boolean functions for which the algorithm works in polynomially many steps, i.e., we can achieve an exponential speed-up over best known classical algorithms.\"",
        "title: \"Revisiting Wiener's Attack --- New Weak Keys in RSA\" with abstract: \"In this paper we revisit Wiener's method (IEEE-IT, 1990) of continued fraction (CF) to find new weaknesses in RSA. We consider RSA with N= pq, qpq, public encryption exponent eand private decryption exponent d. Our motivation is to find out when RSA is insecure given dis O(n\u8302\u622e\u9a74), where we are mostly interested in the range 0.3 \u2264 \u8302\u622e\u9a74\u2264 0.5. We use both the upper and lower bounds on \u8302\u622e\u9a74(N) and then try to find out what are the cases when $\\frac{t}{d}$ is a convergent in the CF expression of $\\frac{e}{N - \\frac{3}{\\sqrt{2}} \\sqrt{N} + 1}$. First we show that the RSA keys are weak when d= N\u8302\u622e\u9a74and $\\delta , where 2q\u8302\u622e\u9a74 p= N\u8302\u622e\u9a74and \u8302\u622e\u9a74is a small value based on certain parameters. This presents additional results over the work of de Weger (AAECC 2002). Further we show that, the RSA keys are weak when $d and eis $O(N^{\\frac{3}{2}-2\\delta})$ for $\\delta \\leq \\frac{1}{2}$. Using similar idea we also present new results over the work of Bl\u00f6mer and May (PKC 2004).\"",
        "title: \"Patterson-Wiedemann Type Functions on 21 Variables With Nonlinearity Greater Than Bent Concatenation Bound.\" with abstract: \"Nonlinearity is one of the most challenging combinatorial property in the domain of Boolean function research. Obtaining nonlinearity greater than the bent concatenation bound for odd number of variables continues to be one of the most sought after combinatorial research problems. The pioneering result in this direction has been discovered by Patterson and Wiedemann in 1983 (IEEE-IT), which consid...\"",
        "title: \"Cryptanalysis of RSA with two decryption exponents\" with abstract: \"In this paper, we consider RSA with N=pq, where p,q are of same bit size, i.e., q<p<2q. We study the weaknesses of RSA when multiple encryption and decryption exponents are considered with same RSA modulus N. A decade back, Howgrave-Graham and Seifert (CQRE 1999) studied this problem in detail and presented the bounds on the decryption exponents for which RSA is weak. For the case of two decryption exponents, the bound was N0.357. We have exploited a different lattice based technique to show that RSA is weak beyond this bound. Our analysis provides improved results and it shows that for two exponents, RSA is weak when the RSA decryption exponents are less than N0.416. Moreover, we get further improvement in the bound when some of the most significant bits (MSBs) of the decryption exponents are same (but unknown).\"",
        "1 is \"Homogeneous Bent Functions, Invariants, and Designs\", 2 is \"Training genetic programming on half a million patterns: an example from anomaly detection\".",
        "\nGiven above information, for an author who has written the paper with the title \"Almost Boolean functions: the design of Boolean functions by spectral inversion\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01193": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'On a tool for reasoning with mass distributions':",
        "title: \"Identifying single good clusters in data sets\" with abstract: \"Local patterns in the form of single clusters are of interest in various areas of data mining. However, since the intention of cluster analysis is a global partition of a data set into clusters, it is not suitable to identify single clusters in a large data set where the majority of the data can not be assigned to meaningful clusters. This paper presents a new objective function-based approach to identify a single good cluster in a data set making use of techniques known from prototype-based, noise and fuzzy clustering. The proposed method can either be applied in order to identify single clusters or to carry out a standard cluster analysis by finding clusters step by step and determining the number of clusters automatically in this way.\"",
        "title: \"Erlerner von Fuzzy-Regeln\" with abstract: \"Zusammenfassung.\u00a0\u00a0In diesem Aufsatz wird untersucht, wie man Fuzzy-Systeme auf der Basis von repr\u00e4sentativem Datenmaterial automatisch generieren\n kann. Wir analysieren dazu induktive Lernverfahren, die ihren Ursprung in der Clusteranalyse und den Neuronalen Netzen haben.\n Anhand von zwei konkreten Softwaretools wird gezeigt, da\u00df diese induktiven Methoden eine Erg\u00e4nzung zu den klassischen Verfahren\n der Erstellung von Fuzzy-Systemen bieten.\n \n \"",
        "title: \"SignalNet: visualization of signal network responses by quantitative proteome data\" with abstract: \"Interactome databases summarize our present knowledge of how proteins can interact at the molecular level under variable conditions. Signal networks, in which proteins and their interactions are represented by nodes and edges, constitute an essential part in these interactomes. The subset of nodes and edges, which become involved under certain biological conditions, necessitate the integration of further experimental information. Mass spectrometry used in proteomics can provide such data describing the expression and responses of nodes in signal networks. SignalNet is a program that connects mass spectrometry (MS) data with a protein interaction database to recognize most likely utilized or affected signaling pathways. Regulatory information derived from quantitative MS analyses is used to calculate and visualize which nodes feature altered expression or response levels. Since signals naturally propagate from node to node, SignalNet also emphasizes edges, which are overconnected to several regulated nodes. Both the regulation factor and the robustness of the underlying MS data are statistically evaluated and assigned to the nodes and edges. Thus SignalNet can filter highly complex interactome data to extract information about signal networks coordinating certain biological conditions. Through this filtering ordinarily densely connected interaction networks get purged from irrelevant interactions. By the presentation of a reduced network with respect to the MS data, the actual observed state of the cell can be resolved.\"",
        "title: \"Reducing the number of parameters of a fuzzy system using scaling functions\" with abstract: \"Learning techniques are tailored for fuzzy systems in order to tune them or even for deriving fuzzy rules from data. However, a compromise between accuracy and interpretability has to be found. Flexible fuzzy systems with a large number of parameters and high degrees of freedom tend to function as black boxes. In this paper, we introduce an interpretation of fuzzy systems that enables us to work with a small number of parameters without loosing flexibility or interpretability. In this way, we can provide a learning algorithm that is efficient and yields accuracy as well as interpretability. Our fuzzy system is based on extremely simple fuzzy sets and transformations using interpretable scaling functions of the input variables.\"",
        "title: \"M-Estimator induced Fuzzy Clustering Algorithms.\" with abstract: \"M-estimators can be seen as a special case of robust clustering algorithms. In this paper, we present the reversed direction and show that clustering algorithms can be constructed by using M-estimators. A clever normalization is used to link the values of several M-estimator prototypes together in one clustering algorithm. A variety of M-estimators and several normalization strategies are used in 4 data sets to present their differences and properties. The results are evaluated using 5 different clustering validation indices.\"",
        "1 is \"Classifying brain states and determining the discriminating activation patterns: Support Vector Machine on functional MRI data.\", 2 is \"A Bayesian framework for the analysis of microarray expression data: regularized t -test and statistical inferences of gene changes.\".",
        "\nGiven above information, for an author who has written the paper with the title \"On a tool for reasoning with mass distributions\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01194": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Fuzzy Learning Vector Quantization with Size and Shape Parameters':",
        "title: \"Intelligent data analysis with fuzzy decision trees\" with abstract: \"Intelligent data analysis has gained increasing attention in business and industry environments. Many applications are looking not only for solutions that can automate and de-skill the data analysis process, but also methods that can deal with vague information and deliver comprehensible models. Under this consideration, we present an automatic data analysis platform, in particular, we investigate fuzzy decision trees as a method of intelligent data analysis for classification problems. We present the whole process from fuzzy tree learning, missing value handling to fuzzy rules generation and pruning. To select the test attributes of fuzzy trees we use a generalized Shannon entropy. We discuss the problems connected with this generalization arising from fuzzy logic and propose some amendments. We give a theoretical comparison on the fuzzy rules learned by fuzzy decision trees with some other methods, and compare our classifiers to other well-known classification methods based on experimental results. Moreover, we show a real-world application for the quality control of car surfaces using our approach.\"",
        "title: \"Single Cluster Visualization to Optimize Air Traffic Management\" with abstract: \"In this paper we present an application of single cluster visualization (SCV) a technique to visualize single clusters of high-dimensional data. This method maps a single cluster to the plane trying to preserve the relative distances of feature vectors to the corresponding prototype vector. Thus, fuzzy clustering results representing relative distances in the form of a partition matrix as well as hard clustering partitions can be visualized with this technique. The resulting two-dimensional scatter plot illustrates the compactness of a certain cluster and the need of additional prototypes as well. In this work, we will demonstrate the visualization method on a practical application.\"",
        "title: \"Erlerner von Fuzzy-Regeln\" with abstract: \"Zusammenfassung.\u00a0\u00a0In diesem Aufsatz wird untersucht, wie man Fuzzy-Systeme auf der Basis von repr\u00e4sentativem Datenmaterial automatisch generieren\n kann. Wir analysieren dazu induktive Lernverfahren, die ihren Ursprung in der Clusteranalyse und den Neuronalen Netzen haben.\n Anhand von zwei konkreten Softwaretools wird gezeigt, da\u00df diese induktiven Methoden eine Erg\u00e4nzung zu den klassischen Verfahren\n der Erstellung von Fuzzy-Systemen bieten.\n \n \"",
        "title: \"NEFCLASS-J - A JAVA-Based Soft Computing Tool\" with abstract: \"\n Neuro-fuzzy classification systems offer means to obtain fuzzy classification rules by a learning algorithm. It is usually\n no problem to find a suitable fuzzy classifier by learning from data; however, it can be hard to obtain a classifier that\n can be interpreted conveniently. There is usually a trade-off between accuracy and readability. In this paper we discuss NEFCLASS\n \u2013 our neuro-fuzzy approach for classification problems \u2013 and its most recent JAVA implementation NEFCLASS-J. We show how a\n comprehensible fuzzy classifier can be obtained by a learning process and how automatic strategies for pruning rules and variables\n from a trained classifier can enhance its interpretability.\n \n \"",
        "title: \"Behavioral Clustering for Point Processes.\" with abstract: \"Groups of (parallel) point processes may be analyzed with a variety of different goals. Here we consider the case in which one has a special interest in finding subgroups of processes showing a behavior that differs significantly from the other processes. In particular, we are interested in finding subgroups that exhibit an increased synchrony. Finding such groups of processes poses a difficult problem as its naive solution requires enumerating the power set of all processes involved, which is a costly procedure. In this paper we propose a method that allows us to efficiently filter the process set for candidate subgroups. We pay special attention to the possibilities of temporal imprecision, meaning that the synchrony is not exact, and selective participation, meaning that only a subset of the related processes participates in each synchronous event.\"",
        "1 is \"Induction of decision trees\", 2 is \"Exploratory search interfaces: categorization, clustering and beyond: report on the XSI 2005 workshop at the Human-Computer Interaction Laboratory, University of Maryland\".",
        "\nGiven above information, for an author who has written the paper with the title \"Fuzzy Learning Vector Quantization with Size and Shape Parameters\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01195": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Exploiting the Small-World Effect for Resource Finding in P2P Grids/Clouds':",
        "title: \"ECCS and LIPS: two languages for OSI systems specification and verification\" with abstract: \"An issue of current interest in the Open Systems Interconnection (OSI) field is the choice of a language well suited to specification and verification. For this purpose, two languages based on Milner's communication calculi are proposed, respectively intended for the specification of asynchronous and synchronous OSI systems. A formal verification method, relying upon the algebraic foundations of the two languages, is introduced and illustrated by means of examples based on nontrivial protocols and services.\"",
        "title: \"Implementing Communicating Processes in the Event of Interface Difference\" with abstract: \"We present here an implementation relation which formalises the notion that a system built of communicating processes is an acceptable implementation of another base or specification system in the event that the two systems have different interfaces. Such a treatment has obvious applicability to the software development process, where an implementation component may be expressed at a different level of abstraction to the relevant specification component. We extend the results of our previous work and combine into a single scheme implementation relations previously presented. We also relax the restrictions previously placed upon specification processes. Using this new implementation relation, two basic kinds of results are obtained: realisability and compositionality. The former ensures that an implementation, when plugged into an appropriate environment, should yield a conventional implementation of the specification, and also that the implementation relation is acceptable when used in the event that specification and implementation systems have the same interfaces. The latter requires that a specification composed of several connected systems may be implemented by connecting their respective implementations.\"",
        "title: \"Making Android Apps Data-Leak-Safe by Data Flow Analysis and Code Injection\" with abstract: \"Some support is needed in order to shun the possibility that sensitive data handled by applications are sent to improper destinations. Although apps running on Android OS declare the accessed services, once the user accepts, the application receives complete permissions and may use sensitive data improperly. Some tools have emerged to check data access and flow, however such tools are either based on static analysis or dynamic tracking. The former brings no overhead at run-time, but is less precise, the latter can bring a costly overhead during execution, having to monitor any access to sensitive data and all destinations. Our approach is innovative in that it takes advantage of static analysis and then monitors at run-time only data paths that potentially give sensitive data out. The correspondent tool is tailored to Android environment, tool-chain, libraries, and typical requirements that applications have to satisfy.\"",
        "title: \"A LOTOS Specification of the PROWAY Highway Service\" with abstract: \"The Language for temporal ordering specification (LOTOS) is a formal description technique whose development is under way within ISO, the International Organization for standardization, mainly for application to open systems interconnection (OSI) standards. The paper presents a LOTOS specification of the PROWAY interface for process control applicatioins, defined by IEC, the International Electrotechnical Commision. LOTOS is shown to be tailored for the specification of asynchronous systems. In particular, it proves suitable for the specification both of the services which define an interface and of the protocols which implement it. The paper shows how LOTOS supports formal reasoning aimed at establishing consistency between service and protocol specifications. Two examples of such a verification are developed that are related to the PROWAY interface. Finally, advantages and limitations of this approach are outlined.\"",
        "title: \"Modelling Replicated Processing\" with abstract: \"Without Abstract\"",
        "1 is \"A goal-centric framework for behaviour programming in autonomous robotic systems\", 2 is \"An Agent Based Multilevel architecture for Robotics Vision Systems\".",
        "\nGiven above information, for an author who has written the paper with the title \"Exploiting the Small-World Effect for Resource Finding in P2P Grids/Clouds\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01196": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'A grid-based infrastructure to support multimedia content distribution':",
        "title: \"Modeling multi-agent communication contexts\" with abstract: \"This paper presents a model for agent communication and coordination which takes into account the \"human-like\" behavior of agents. We analyze the aspects of human communication and human social contexts to derive some characteristics which can be mapped onto the agent world. This leads us to provide a flexible way to express agent interactions and interaction dynamics. Based on this analysis, the paper proposes a formalization of interaction contexts by providing a model for agent messages and interaction laws; the latter are expressed by means of logic rules which specify communication constraints. The proposed models are the basis for a communication infrastructure for agents, offering a set of primitives suitable for message exchanging and rule handling.\"",
        "title: \"An economic model for resource management in a Grid-based content distribution network\" with abstract: \"This paper presents an architecture to federate Content Distribution Networks (CDNs) in order to share computational resources, thus building an infrastructure that we call a Content Distribution Grid (CDG). The purpose of a CDG is to use a community policy allowing each CDN to put together a portion of its own resources to meet each other's requirements, and therefore to guarantee a stronger quality of service to users. Since CDNs of a CDG belong to different organisations, the interaction scenario can be considered competitive, that is, organizations are mainly self-interested and are aiming at maximizing the performances of its own system. For this reason, the resource sharing policy proposed here is based on a offer/demand competitive model, in which resources are purchased by paying for them a certain amount of (virtual or real) money. An economic model is thus derived to guide such a sale; here the CDN requesting resources (buyer) and the CDN offering resources (seller) agree on the quantity and the price by means of a utility-based negotiation approach. A multi-agent system is then proposed to realise the software architecture supporting this model of CDG.\"",
        "title: \"Using the Erlang Language for Multi-Agent Systems Implementation\" with abstract: \"Even if Java is widely used for agent development, some agent platforms employ ad-hoc programming languages, so the question that arises is: Is there a language that fits the model of an autonomous software agent better than Java? This paper deals with such an issue by deriving an abstract model for agents and proposing some parameters to evaluate programming languages for agent development. As a result, the paper introduces Erlang, a functional language that presents some interesting characteristics for the engineering of agent-based applications. An Erlang-based platform, called eXAT and developed by the authors, is then presented. Finally, a comparison with a Java-based approach explains why, in the authors\u00fd opinion, this language cannot be considered a good choice for the implementation of agent systems.\"",
        "title: \"The Analysis And Evaluation Of Design Patterns For Distributed Real-Time Java Software\" with abstract: \"The Real-time Specification for Java (RTSJ) introduces a new memory model featuring some programming constraints that impede the \"as-is\" use of many well known design patterns. In this context, this paper describes and evaluates two design patterns, developed by the authors for distributed real-time Java soft ware, that are able to overcome the limitations imposed by RTSJ. The first pattern, RTJ-Leader-Follower, is a RTSJ-compliant version of the well-known Leader/-Follower pattern. The second pattern, called Scoped Tunnels, provides a new communication mechanism for RTSJ threads executing in different and incompatible memory areas, thus making possible the realization of the standard pattern half-sync/half-async for efficient network I/O handling. The paper presents both a qualitative and quantitative evaluation of these patterns, showing, above all, that they are able to provide a safe execution environment.\"",
        "title: \"Locating mobile agents in a wide distributed environment\" with abstract: \"Finding the position of a mobile agent in a wide distributed system still represents an open research issue. The various existing mobile agent platforms implement ad hoc naming and location policies, studied to address the requirements of the design choices made. This paper proposes a naming scheme and a location protocol of general validity for mobile agents able to effectively meet all the typical requirements of mobile agent environments and, thus, easy to integrate into different platforms. The paper identifies the main characteristics which an agent naming scheme and a location protocol of general validity should have, and suggests some properties and parameters to be taken into account to evaluate the effectiveness of naming schemes and location protocols. Then, we propose a \u9a74human readable\u9a74 agent naming scheme based on the distributed environment outlined in MASIF, and a suitable location finding protocol called the Search-By-Path-Chase. Both of them are compared with some of the solutions already provided, using the properties and the parameters suggested. The performances are finally evaluated by means of a set of measurements.\"",
        "1 is \"Task Allocation and Precedence Relations for Distributed Real-Time Systems\", 2 is \"A view of cloud computing\".",
        "\nGiven above information, for an author who has written the paper with the title \"A grid-based infrastructure to support multimedia content distribution\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01197": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'ANEMONE - A Network of Multi-Agent Platforms for Academic Communities':",
        "title: \"Deduct: A Data Dependence Based Concern Tagger For Modularity Analysis\" with abstract: \"Modularity of a software system can be assessed once responsibilities of each method and class have been determined. Generally, developers attribute responsibilities to methods and classes manually. This can be problematic given that it relies on developers judgement and effort. This paper proposes an approach to automatically attribute concern tags to each instructions. The approach is based on taint analysis to determine which code lines are related to each other by data dependence. Moreover, Java APIs provide the tags used to mark code lines. The automatic concern tagging that we bring about is used to find out how responsibilities are spread in the code, and then to suggest refactoring activities in case tangling occurs.\"",
        "title: \"Automatically discovering design patterns and assessing concern separations for applications\" with abstract: \"In this paper we present a tool that assists in the automated analysis of a Java application, aimed at two purposes: (i) identifying class structure and, within this, micro-architectures that conform to known design patterns; (ii) providing visual representations of classes, concerns and their relationships. This affords a more abstract view of the analysed application, letting its structure emerge more clearly and its components be separately understood. As a result, it becomes easier for developers to assess whether well-known desirable characteristics, notably those favouring modularity and concern separation, or rather bad design choices, have been incorporated into the application.The proposed approach can be helpful both within the undertaking of a new development effort, and reverse engineering of an existing application in view of its evolution.\"",
        "title: \"Enhancing Environmental Surveillance Against Organised Crime With Radial Basis Neural Networks\" with abstract: \"A huge amount of data concerning the position of individual is often gathered in surveillance scenarios, to prevent crimes or to collect evidence of unlawful behaviour. Given the abundance of data available, detectives need advanced analysis means in order to set apart the interesting locations. This paper proposes a solution that makes use of radial basis neural networks to find the points of interests, i.e. locations that have been used for meetings, by surveilled people whose paths have been traced. In our solution, newly gathered data will be analysed in order to find the points of interest, and will also be given to our neural network for further training. Our results show that the proposed approach is accurate enough and can improve the unaided search for meeting points between observed individuals.\"",
        "title: \"Superimposing roles for design patterns into application classes by means of aspects\" with abstract: \"Application classes participating as a role in some design patterns often implement both pattern- and application-related concerns. This tangling potentially makes classes more complex, more prone to changes and less reusable. This paper proposes an aspect-oriented solution that makes classes free of pattern-related code, whereas a connecting code concisely maps roles into classes. Developers having to modify the role of a class need only update such a connecting code, thus reducing changes to application code.\"",
        "title: \"SHARK, a Multi-Agent System to Support Document Sharing and Promote Collaboration\" with abstract: \"In this paper, we present a family of novel P2P routing schemes based on Chord [17] (and its variation F-Chord (\u9a74)[2]) that trades off uniformity with efficiency without using any additional overhead. We prove that H-F-Chord (\u9a74)\u00fdsrouting is more ...\"",
        "1 is \"Admissible Heuristics for Optimal Planning\", 2 is \"Game-Theoretic Methods for the Smart Grid: An Overview of Microgrid Systems, Demand-Side Management, and Smart Grid Communications.\".",
        "\nGiven above information, for an author who has written the paper with the title \"ANEMONE - A Network of Multi-Agent Platforms for Academic Communities\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01198": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Visual cues for imminent object contact in realistic virtual environment':",
        "title: \"Detecting Moving Objects Using the Rigidity Constraint\" with abstract: \"A method for visually detecting moving objects from a moving camera using point correspondences in two orthographic views is described. The method applies a simple structure-from-motion analysis and then identifies those points inconsistent with the interpretation of the scene as a single rigid object. It is effective even when the actual motion parameters cannot be recovered. Demonstrations are presented using point correspondences automatically determined from real image sequences.\"",
        "title: \"Geometric reasoning under uncertainty for map-based localization\" with abstract: \"Map-based navigation in outdoor terrain lacking man-made structures or other highly distinctive landmarks can produce severe localization problems. This paper presents an approach to navigation which implements high level geometric reasoning and matching strategies based on those used by skilled human navigators. This approach, which is demonstrated on a real example involving imagery of mountainous terrain obtained with a video camera and USGS map data, is designed to avoid many of the pitfalls occurring when an attempt is made to navigate by modeling the environment mathematically. It exploits feature attributes which cannot be easily expressed quantitatively but are central to the successful human navigation process.\"",
        "title: \"Kinect based 3D object manipulation on a desktop display\" with abstract: \"Gesture-based controllers such as the Microsoft Kinect are low cost devices that allow a user to interact with complex, three-dimensional simulations using an interface argued to be more natural than game controllers, joy sticks, or a mouse and keyboard. This paper presents a controlled experimental evaluation of the use of Microsoft Kinect to support a 3D object manipulation task. Users were asked to match the orientation of objects with a manipulation interface that displayed either a self-avatar hand and arm or a sphere, both corresponding to users' arm gestures and wrist rotation. Our results show that while there was no overall difference in performance between the self-avatar and sphere visual display conditions, there were clear differences in the two visual display conditions as a function of gender and video-game experience.\"",
        "title: \"Feature-Based Reverse Engineering Of Mechanical Parts\" with abstract: \"Reverse engineering of mechanical parts requires extraction of information about an instance of a particular part sufficient to replicate the part using appropriate manufacturing techniques. This is important in a wide variety of situations, since functional CAD models are often unavailable or unusable for parts which must be duplicated or modified. Computer vision techniques applied to three-dimensional (3-D) data acquired using noncontact, 3-D position digitizers have the potential for significantly aiding the process. Serious challenges must be overcome, however, if sufficient accuracy is to be obtained and if models produced from sensed data are to be truly useful for manufacturing operations. This paper describes a prototype of a reverse engineering system which uses manufacturing features as geometric primitives, This approach has two advantages over current practice, The resulting models can be directly imported into feature-based CAD systems without loss of the semantics and topological information inherent in feature-based representations. In addition, the feature-based approach facilitates methods capable of producing highly accurate models, even when the original 3-D sensor data has substantial errors.\"",
        "title: \"The influence of feedback on egocentric distance judgments in real and virtual environments\" with abstract: \"A number of investigators have reported that distance judgments in virtual environments (VEs) are systematically smaller than distance judgments made in comparably-sized real environments. Many variables that may contribute to this difference have been investigated but none of them fully explain the distance compression. One approach to this problem that has implications for both VE applications and the study of perceptual mechanisms is to examine the influence of the feedback available to the user. Most generally, we asked whether feedback within a virtual environment would lead to more accurate estimations of distance. Next, given the prediction that some change in behavior would be observed, we asked whether specific adaptation effects would generalize to other indications of distance. Finally, we asked whether these effects would transfer from the VE to the real world. All distance judgments in the head-mounted display (HMD) became near accurate after three different forms of feedback were given within the HMD. However, not all feedback sessions within the HMD altered real world distance judgments. These results are discussed with respect to the perceptual and cognitive mechanisms that may be involved in the observed adaptation effects as well as the benefits of feedback for VE applications.\"",
        "1 is \"Radiance interpolants for accelerated bounded-error ray tracing\", 2 is \"Automatic extraction of Irregular Network digital terrain models\".",
        "\nGiven above information, for an author who has written the paper with the title \"Visual cues for imminent object contact in realistic virtual environment\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01199": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Analysis and Implementation of the SNOW 3G Generator Used in 4G/LTE Systems.':",
        "title: \"On the Use of Linear Cellular Automata for the Synthesis of Cryptographic Sequences\" with abstract: \"This work shows that a class of cryptographic sequences, the so-called interleaved sequences, can be generated by means of linear hybrid cellular automata. More precisely, linear multiplicative polynomial cellular automata generate all the components of this family of interleaved sequences. As an illustrative example, the linearization procedure of the self-shrinking generator is described. In this way, popular nonlinear sequence generators with cryptographic application are linearized in terms of simple cellular automata.\"",
        "title: \"Strong authentication on smart wireless devices\" with abstract: \"The rapid deployment of wireless technologies has given rise to the current situation where mobile phones and other wireless devices have become essential elements in all types of activities, including in the home. In particular, smartphones and laptops are used for wirelessly sharing photos and documents, playing games, browsing websites, and viewing multimedia, for example. This work describes a proposal for both desktop and mobile applications that use Identity-Based Cryptography (IBC) to protect communications between smart wireless devices in the home. It combines the use of IBC for Wi-Fi and Bluetooth communication, with the promising Near Field Communication (NFC) technology for secure authentication. The proposed scheme involves NFC pairing to establish as public key a piece of information linked to the device, such as a phone number or an IP address. In this way, such information can be then used in an IBC scheme for peer-to-peer communication. Preliminary implementations of prototypes on several mobile platforms have already produced some promising results.\"",
        "title: \"A simple acceptance/rejection criterium for sequence generators in symmetric cryptography\" with abstract: \"A simple method of checking the degree of balancedness in key-stream generators of cryptographic application has been developed. The procedure is based exclusively on the handling of bit-strings by means of logic operations and can be applied to standard generators proposed and published in the open literature (combinational generators, multiple clocking generators, irregularly clocked generators). The requirements of time and memory complexity are negligible. The method here developed is believed to be a first selective criterium for acceptance/rejection of this type of generators with application in symmetric cryptography.\"",
        "title: \"Poster: Revocation in VANETs Based on k-ary Huffman Trees\" with abstract: \"One of the biggest problems of vehicular ad-hoc networks is revocation. The efficient management of such issue has become one of the major paradigms in this area of research. A solution proposed here is based on the use of authenticated data structures like revocation trees to replace the classical and inefficient certificate revocation lists. In particular, the idea of this paper is to propose the use of k-ary hash trees, Huffman coding and a duplex version of the SHA-3 hash function, to optimize insertions and searches in the revocation structure. Thus, the inclusion of a new certificate revoked in the tree, only implies a new iteration of the duplex construction of the hash function, avoiding recalculating the entire hashes and the entire tree. Furthermore, a k-ary Huffman tree is used to insert leaf nodes at different levels so that those revoked nodes that are more queried, are located closer to the root node position, so the revocation proof is smaller for those vehicles that spend more time on the roads. This paper details a method to calculate the optimum value $k$ for the k-ary tree in order to optimize the revocation proof size. Therefore, the proposal described here improves both the insertion of new revoked certificates in the revocation structure and the search of revoked certificates in the revocation structure. This paper is part of a work in progress, so that we plan to implement the scheme in real scenarios to get ideal values of the parameters and comparisons with other schemes.\"",
        "title: \"Software implementation of the SNOW 3G Generator on iOS and Android platforms.\" with abstract: \"The standard for wireless communication of high-speed data in mobile phones and data terminals, called LTE (Long-Term Evolution) and marketed as 4G/LTE, is quickly being adopted worldwide. The security of this type of communication is a crucial factor mainly due to its mobile and wireless nature. This work includes a practical analysis of the SNOW 3G generator used to protect the confidentiality and integrity in LTE communications. In particular, several techniques to perform multiplications and LFSR operations have been studied and implemented on both iOS and Android platforms. The evaluation of those implementations led to some conclusions that could be used to improve the efficiency of future implementations of the standard.\"",
        "1 is \"Nash Equilibria of Packet Forwarding Strategies in Wireless Ad Hoc Networks\", 2 is \"Lightweight RFID authentication with forward and backward security\".",
        "\nGiven above information, for an author who has written the paper with the title \"Analysis and Implementation of the SNOW 3G Generator Used in 4G/LTE Systems.\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01200": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Experiences with an interactive museum tour-guide robot':",
        "title: \"Levesque's axiomatization of only knowing is incomplete\" with abstract: \" We show that the axiomatization given by Levesque for his logic of &quot;only knowing&quot;(Levesque 1990), which he showed to be sound and complete for the unquantified version ofthe logic and conjectured to be complete for the full logic, is in fact incomplete.1 IntroductionLevesque (1990) introduced a first-order modal logic OL with a modal operator for &quot;onlyknowing&quot;, which was taken to be the conjunction of &quot;knowing at least&quot; and &quot;knowing at most&quot;.1He provided a collection of axioms for... \"",
        "title: \"On First-Order Definability and Computability of Progression for Local-Effect Actions and Beyond.\" with abstract: \"In a seminal paper, Lin and Reiter introduced the notion of progression for basic action theories in the situation calculus. Unfortunately, progression is not first-order definable in general. Recently, Vassos, Lakemeyer, and Levesque showed that in case actions have only local effects, progression is first-order representable. However, they could show computability of the first-order representation only for a restricted class. Also, their proofs were quite involved. In this paper, we present a result stronger than theirs that for local-effect actions, progression is always first-order definable and computable. We give a very simple proof for this via the concept of forgetting. We also show first-order definability and computability results for a class of knowledge bases and actions with non-local effects. Moreover, for a certain class of local-effect actions and knowledge bases for representing disjunctive information, we show that progression is not only first-order definable but also efficiently computable.\"",
        "title: \"Steps towards a first-order logic of explicit and implicit belief\" with abstract: \"Modelling the beliefs of an agent who lacks logical omniscience has been a major concern recently. While most of the work has concentrated on propositional logics of belief, this paper primarily addresses issues raised by adding quantifiers to such logics. In particular, we are focusing on quantifying in and the distinction between \"knowing what\" and \"knowing that\". After arguing why a model of limited reasoning should preserve this distinction, we show how this can be accomplished by a semantics based on a restricted form of tautological entailment.\"",
        "title: \"A Computationally Attractive First-Order Logic of Belief\" with abstract: \"Logics of belief play an essential role in formal approaches to knowledge representation (KR). Such logics allow us to formalize the idea that a knowledge base (KB\n ) represents an epistemic state, that is, a set of beliefs. In its simplest form, an epistemic state can be defined as the set of all beliefs that logically follow from believing the sentences that are explicitly stored in the KB\n . Reasoning can be understood as computing whether a belief follows logically from believing the sentences in the KB\n . Since the classical model of belief, possible-world semantics, renders reasoning undecidable, it is important to find weaker models with better computational properties.\"",
        "title: \"A Tractable Knowledge Representation Service with Full Introspection\" with abstract: \"A Knowledge Representation service for a knowledge-based system (or agent) can be viewed as providing, at the very least, two operations that (a) give precise information about what is and is not believed (ASK) and (b) add new facts to the knowledge base when they become available (TELL). An appropriate model of belief for such operations should support the notion that only certain facts are believed, in particular those that have been added to a knowledge base via TELL. For logically omniscient and fully introspective agents, models of this kind lead to intractable ASK and TELL operations. In this paper, we show that tractability can be retained by giving up logical omniscience, but without sacrificing full introspection. This is done within the framework of a propositional logic of belief. In particular, the logic allows us to express that only a sentence (or finite set of them) is believed. We show that the validity of certain classes of sentences involving belief can be decided efficiently. These results are then applied to the specification of efficient TELL and ASK operations.\"",
        "1 is \"Plug and Play with Query Algebras: SECONDO-A Generic DBMS Development Environment\", 2 is \"Learning policies for partially observable environments: scaling up\".",
        "\nGiven above information, for an author who has written the paper with the title \"Experiences with an interactive museum tour-guide robot\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01201": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of '\u201cSofter\u201d optimization and control models via fuzzy linguistic quantifiers':",
        "title: \"Reasoning with doubly uncertain soft constraints\" with abstract: \"We describe the basic ideas of the theory of approximate reasoning and indicate how it provides a framework for representing human sourced soft information. We discuss how to translate linguistic knowledge into formal representations using generalized constraints. We consider the inference process within the theory of approximate reasoning and introduce the entailment principle and describe its centrality to this inference process. Next we introduce the idea of doubly uncertain statements such as John's friend is young. In these statements there exists uncertainty both with respect to value of the age, young, and the object associated with the age, John's friend. We suggest a method for representing these complex statements and investigate the problem of making inferences about specific objects.\"",
        "title: \"Extending the participatory learning paradigm to include source credibility\" with abstract: \"We provide an overview of the participatory learning paradigm (PLP) and discuss the importance of the acceptance function in determining which observations are used for learning. We introduce a formal model that uses this (PLP) We then extend this model in two directions. First, we consider situations in which we have incomplete observations, we only have observations about a subset of the variables of interest. Next we extend this model to allow for the inclusion in the learning process of information about the learning agents belief about the credibility of the source of the learning experience. Here we distinguish between the content of a learning experience and the source of the experience. We provide a means to allow the learning agents belief about the credibility of the source to determine the effect of the content. Furthermore we suggest a method to allow the modification of agents belief about the credibility of the source to also be part of the learning process.\"",
        "title: \"Summarizing data using a similarity based mountain method\" with abstract: \"We consider the problem of summarizing a collection of data values. Here we use a mountain method like approach based on the similarities of the data. Fundamental to our work is the possibility of allowing for multiple summarizing values. We present an algorithm, in the spirit of the mountain method, that uses the similarity between the data points to find focus points which serve as the seed for finding summarizing centers. Central to this algorithm is a process of reducing the energy of the data points which we show can be implemented most generally using a t-norm. We provide an application of the algorithm to the problem of binning data which is used in data mining and the development of histograms. Here we allow the location of the bins to be determined by the data rather then fixed a priori.\"",
        "title: \"Golden Rule and Other Representative Values for Atanassov Type Intuitionistic Membership Grades\" with abstract: \"AbstractOur interest here is on comparing Atanassov type intuitionistic membership grades, this is a problem since they are not completely ordered. We suggest the use of an associated scalar value that we refer to as the representative value. We note the formal correspondence between interval valued membership grades and Atanassov type intuitionistic membership grades. Inspired by this we review some ideas on the formulation of representative values for interval valued membership grades. We then use this formal correspondence to help transfer these ideas to Atanassov type intuitionistic membership grades. We note while the construction of a representative value requires the satisfaction of some properties, these properties do not completely constrain its formulation and as such leave room for the inclusion of some subjective aspects in formulating representative values. Here we look at different representative values. A particularly notable example of representative values investigated here is what we refer to as the Golden Rule representative value.\"",
        "title: \"Approximate reasoning with generalized orthopair fuzzy sets\" with abstract: \"Introduction to Generalized Orthopair Fuzzy Sets.Discussion Knowledge Representation With Generalized Orthopair Fuzzy Sets.Inference Mechanism for Orthopair Approximate Reasoning System.Formulation of the Ideas of Possibility and Certainty Using Orthopair Fuzzy Sets. We introduce the idea of generalized orthopair fuzzy sets, which provide an extension of intuitionistic fuzzy sets. The basic properties of these generalized orthopair fuzzy sets are discussed. We discuss the use of these sets in knowledge representation. We consider the use of these types of orthopair fuzzy sets as a basis for the system of approximate reasoning introduced by Zadeh. This is referred to as OPAR. The basic operations of OPAR are introduced. A reasoning mechanism in OPAR, based on the idea of entailment, is provided. We look at the formulation of the ideas of possibility and certainty using these orthopair fuzzy sets.\"",
        "1 is \"Intuitionistic Fuzzy Geometric Bonferroni Means and Their Application in Multicriteria Decision Making\", 2 is \"Semantic cores for representing documents in IR\".",
        "\nGiven above information, for an author who has written the paper with the title \"\u201cSofter\u201d optimization and control models via fuzzy linguistic quantifiers\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01202": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Mosaicing of Flattened Images from Straight Homogeneous Generalized Cylinders':",
        "title: \"Scalable 3-D Terrain Visualization Through Reversible JPEG2000-Based Blind Data Hiding\" with abstract: \"In this paper a new method is presented for 3-D terrain visualization via reversible JPEG2000-based blind data hiding with special focus on data synchronization and scalability. Online real-time 3-D terrain visualization involves considerable amount of data. The process is essentially the mapping of the aerial photograph, called texture, onto its corresponding digital elevation model (DEM) implying at least two distinct data inputs. The presence of large disparate data necessitates a compression strategy on one hand and the integration of the DEM and texture into one unit on the other. Whilst the compression must accommodate the scalability requirement originated by the diversity of clients, the unification of data ought to be synchronous. For scalability this paper relies on the multiresolution nature of the DWT-based JPEG2000 standard whereas the synchronized unification of DEM with the texture is realized by the application of a perceptually transparent data hiding strategy in the DWT domain. The proposed method is blind in the sense that only a secret key, if any, and the size of the original DEM are needed to extract the data from the texture image. We believe that this is one of the pioneering methods to propose scalable embedding of DEM in the texture image. The method is cost effective, in terms of memory and bandwidths, which is an advantage, especially, in real-time environments when quicker transfer of data is required. The results of a 3-D visualization simulation effected with our method were encouraging and gave a useful insight to the effectiveness of our method in various bandwidth scenarios.\"",
        "title: \"Considering the reconstruction loop for watermarking of intra and inter frames OF H.264/AVC\" with abstract: \"This paper presents design and analysis of watermarking of intra and inter frames in H.264/AVC video codec. Most of video watermarking algorithms take into account only intra for watermark embedding. In this paper, we analyze water- mark embedding in intra as well as in inter and we note that watermark embedding capability of inter is comparable to that of intra. Watermark embedding, in only those non-zero quantized transform coefficients (QTCs) which are above a specific threshold, enables us to detect and extract the wa- termark on the decoding side. There is not significant com- promise on quality and bitrate of the video bitstream because we have taken into account the reconstruction loop during the watermarking step. The proposed scheme does not tar- get robustness. Rather the main contribution of our scheme is higher payload as compared to payloads obtained in previ- ous works.\"",
        "title: \"3D non-rigid pattern recognition based on structural analysis\" with abstract: \"This paper presents a new retrieval approach for 3D non-rigid objects. The proposed method consists in using the Reeb graph representation as local shape descriptor. The generated Reeb graph, based on the heat diffusion properties, is segmented into Reeb charts having a controlled topology. Each Reeb chart is associated with a couple of geometrical signatures, based on the area and angle distortions. The matching procedure is carried out on each pair of Reeb charts, according to the minimum distance between the corresponding signatures. A global similarity measure quantifies the degree of correspondence between all the matched Reeb charts. Experimental results, conducted on SHREC 2011 dataset, have shown that our retrieval approach provides an overall retrieval efficiency gain compared to three state-of-the-art methods.\"",
        "title: \"A Homomorphic Method for Sharing Secret Images\" with abstract: \"In this paper, we present a new method for sharing images between two parties exploiting homomorphic property of public key cryptosystem. With our method, we show that it is possible to multiply two encrypted images, to decrypt the resulted image and after to extract and reconstruct one of the two original images if the second original image is available. Indeed, extraction and reconstruction of original image at the receiving end is done with the help of carrier image. Experimental results and security analysis show the effectiveness of the proposed scheme.\"",
        "title: \"A two-stage traitor tracing scheme for hierarchical fingerprints.\" with abstract: \"The multimedia traitor tracing field involves the embedding of a collusion secure fingerprint in the host signal to retrieve and prevent any multimedia content fraud. Trendy work aims at providing a tracing system which offers a good protection of the digital content and an efficient tracing process. These challenges depend on reducing the length of the embedded fingerprint and the complexity of the accusation process. Furthermore, addressing these issues becomes more and more relevant in media distribution applications involving an important number of users. In this paper, we propose a secure fingerprinting system based on a two-stage tracing strategy which combines two probabilistic tracing codes: Boneh Shaw with replication scheme and Tardos codes. This strategy is applied to a multilevel hierarchical fingerprint which is then embedded using a DCT-based audio watermarking technique. By taking the advantage of grouping users and applying a weight-based tracing process, the proposed fingerprinting system offers to reduce efficiently the computational costs of the tracing time. It has also a good robustness to different types of attacks. We have carried out different tests to evaluate the performance of the system in terms of robustness and imperceptibility. The experimental results show that the proposed fingerprinting system provides a suitable solution to the the infringement copyright problem in multimedia distribution platforms by reducing significantly the users' retrieval space and performing good detection results in a reduced time.\"",
        "1 is \"Eigenfaces vs. Fisherfaces: Recognition Using Class Specific Linear Projection\", 2 is \"Texturing techniques for terrain visualization\".",
        "\nGiven above information, for an author who has written the paper with the title \"Mosaicing of Flattened Images from Straight Homogeneous Generalized Cylinders\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01203": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Packing squares into a square':",
        "title: \"Optimal methods for coordinated enroute web caching for tree networks\" with abstract: \"Web caching is an important technology for improving the scalability of Web services. One of the key problems in coordinated enroute Web caching is to compute the locations for storing copies of an object among the enroute caches so that some specified objectives are achieved. In this article, we address this problem for tree networks, and formulate it as a maximization problem. We consider this problem for both unconstrained and constrained cases. The constrained case includes constraints on the cost gain per node and on the number of object copies to be placed. We present dynamic programming-based solutions to this problem for different cases and theoretically show that the solutions are either optimal or convergent to optimal solutions. We derive efficient algorithms that produce these solutions. Based on our mathematical model, we also present a solution to coordinated enroute Web caching for autonomous systems as a natural extension of the solution for tree networks. We implement our algorithms and evaluate our model on different performance metrics through extensive simulation experiments. The implementation results show that our methods outperform the existing algorithms of either coordinated enroute Web caching for linear topology or object placement (replacement) at individual nodes only.\"",
        "title: \"Linear-Time Haplotype Inference on Pedigrees without Recombinations and Mating Loops\" with abstract: \"In this paper, an optimal linear-time algorithm is presented to solve the haplotype inference problem for pedigree data when there are no recombinations and the pedigree has no mating loops. The approach is based on the use of graphs to capture SNP, Mendelian, and parity constraints of the given pedigree. This representation allows us to capture the constraints as the edges in a graph, rather than as a system of linear equations as in previous approaches. Graph traversals are then used to resolve the parity of these edges, resulting in an optimal running time.\"",
        "title: \"Improved on-line broadcast scheduling with deadlines\" with abstract: \"We study an on-line broadcast scheduling problem in which requests have deadlines, and the objective is to maximize the weighted throughput, i.e., the weighted total length of the satisfied requests. For the case where all requested pages have the same length, we present an online deterministic algorithm named BAR and prove that it is 4.56-competitive. This improves the previous algorithm of (Kim, J.-H., Chwa, K.-Y. in Theor. Comput. Sci. 325(3):479---488, 2004) which is shown to be 5-competitive by (Chan, W.-T., et al. in Lecture Notes in Computer Science, vol. 3106, pp. 210---218, 2004). In the case that pages may have different lengths, we give a ( $\\Delta+ 2\\sqrt{\\Delta}+2$ )-competitive algorithm where \u0394 is the ratio of maximum to minimum page lengths. This improves the (4\u0394+3)-competitive algorithm of (Chan, W.-T., et al. in Lecture Notes in Computer Science, vol. 3106, pp. 210---218, 2004). We also prove an almost matching lower bound of \u9a74(\u0394/log\u9a74\u0394). Furthermore, for small values of \u0394 we give better lower bounds.\"",
        "title: \"Finding motifs from all sequences with and without binding sites\" with abstract: \"Motivation: Finding common patterns, motifs, from a set of promoter regions of coregulated genes is an important problem in molecular biology. Most existing motif-finding algorithms consider a set of sequences bound by the transcription factor as the only input. However, we can get better results by considering sequences that are not bound by the transcription factor as an additional input. Results: First, instead of using the simple hyper-geometric analysis, we propose to calculate the likelihood based on a more precise probabilistic analysis which considers motif length, sequence length and number of binding sites as input parameters for testing whether motif is found. Second, we adopt an heuristic algorithm bases on our analysis to find motifs. For the simulated and real datasets, our algorithm ALSE compares favorably against common motif-finding programs such as SeedSearch and MEME in all cases and performs very well, especially when each input sequence contains more than one binding site. Availability: ALSE is available for download at the homepage http://alse.cs.hku.hk Contact: cmleung2@cs.hku.hk\"",
        "title: \"A clique-based algorithm for constructing feasible timetables\" with abstract: \"Constructing a feasible solution, where the focus is on 'hard' constraints only, is an important part of solving timetabling problems. For the University Course Timetabling Problem, we propose a heuristic algorithm to schedule events to timeslots based on cliques, each representing a set of events that could be scheduled in the same timeslot, which the algorithm constructs. Our algorithm has been tested on a set of well-known instances, and the experimental results show that our algorithm is efficient and can compete with other effective algorithms.\"",
        "1 is \"HITON: A Novel Markov Blanket Algorithm for Optimal Variable Selection\", 2 is \"An Ant Colony algorithm hybridized with insertion heuristics for the Time Dependent Vehicle Routing Problem with Time Windows\".",
        "\nGiven above information, for an author who has written the paper with the title \"Packing squares into a square\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01204": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Case Representation, Acquisition, and Retrieval in SIROCCO':",
        "title: \"Teaching Case-Based Argumentation Concepts Using Dialectic Arguments vs. Didactic Explanations\" with abstract: \"We compared two automated approaches to teaching distinguishing, a fundamental skill of case-based reasoning that involves assessing the relevant differences among cases in a context-sensitive way. The approaches are implemented in two versions of CATO, an ITS designed to teach law students basic skills of case-based legal argument. The original version of CATO employed a didactic explanatory dialogue. The newer version, CATO-Dial, teaches the same skill with a simulated dialectic argument in a courtroom setting. Our hypothesis was that students would learn better by engaging in the simulated argument than by receiving interactive explanation. We showed that students in the dialectic argument simulation group performed significantly better on certain sections of the post-test aimed at assessing transfer of their skills of distinguishing.\"",
        "title: \"Reasoning symbolically about partially matched cases\" with abstract: \"Can an AI program contribute to scientific discovery? An area where this gauntlet has been thrown is that of understanding the mechanisms of chemical carcinogenesis. One approach is to obtain Structure-Activity Relationships (SARs) relating molecular ...\"",
        "title: \"Finding factors: learning to classify case opinions under abstract fact categories\" with abstract: \" Fact CategoriesStefanie Bruninghaus and Kevin D. AshleyUniversity of PittsburghLearning Research and Development Center,Intelligent Systems Program and School of LawPittsburgh, PA 15260steffi+@pitt.edu, ashley+@pitt.eduAbstractThis paper presents preliminary work towards automaticallyassigning to full-text opinion texts the applicable factors, thatis, fact patterns influencing the outcome of a legal claim,to full-text opinion texts, which are used in CATO's modelof case-based... \"",
        "title: \"Using critical questions to disambiguate and formalize statutory provisions\" with abstract: \"This paper outlines a process model for disambiguating legal provisions in order to ease their formalization into logic. It centers around a reformulation of the provision driven by critical questioning and mandatory legal justifications.\"",
        "title: \"Toward AI-enhanced Computer-supported Peer Review in Legal Education.\" with abstract: \"Applying Bayesian data analysis to model a computer-supported peerreview process in a legal class writing exercise yielded pedagogically useful information about student-understanding of problem-specific legal concepts and of more general domain-related legal writing criteria and about the criteria's effectiveness. The approach suggests how AI and Law can impact legal education.\"",
        "1 is \"Group-Oriented Modelling Tools with Heterogeneous Semantics\", 2 is \"Using background knowledge in case-based legal reasoning: a computational model and an intelligent learning environment\".",
        "\nGiven above information, for an author who has written the paper with the title \"Case Representation, Acquisition, and Retrieval in SIROCCO\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01205": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'The Scalable Adapter Design Pattern: Enabling Interoperability Between Educational Software Tools':",
        "title: \"Strategy-Based learning through communication with humans\" with abstract: \"In complex application systems, there are typically not only autonomous components which can be represented by agents, but humans may also play a role. The interaction between agents and humans can be learned to enhance the stability of a system. How can agents adopt strategies of humans to solve conflict situations? In this paper, we present a learning algorithm for agents based on interactions with humans in conflict situations. The learning algorithm consists of four phases: 1) agents detect a conflict situation, 2) a conversation takes place between a human and agents, 3) agents involved in a conflict situation evaluate the strategy applied by the human, and 4) agents which have interacted with humans apply the best rated strategy in a similar conflict situation. We have evaluated this learning algorithm using a Jade/Repast simulation framework. An evaluation study shows two benefits of the learning algorithm. First, through interaction with humans, agents can handle conflict situations, and thus, the system becomes more stable. Second, agents adopt the problem solving strategy which has been applied most frequently by humans.\"",
        "title: \"How Do We Get the Pieces to Talk? An Architecture to Support Interoperability between Educational Tools\" with abstract: \"For many practical learning scenarios, the integrated use of more than one learning tool is educationally beneficial. In these cases, interoperability between learning tools --- getting the pieces to talk --- is a crucial requirement that is often hard to achieve. This paper describes an architecture that aims at the integration of independent learning tools into one collaborative learning scenario.\"",
        "title: \"Engineering Hybrid Learning Communities: The Case Of A Regional Parent Community\" with abstract: \"We present an approach (and a corresponding system design) for supporting regionally bound hybrid learning communities (i.e., communities which combine traditional face-to-face elements with web based media such as online community platforms, e-mail and SMS newsletters). The goal of the example community used to illustrate the approach was to support and motivate (especially hard-to-reach underprivileged) parents in the education of their young children. The article describes the design process used and the challenges faced during the socio-technical system design. An analysis of the community over more than one year indicates that the hybrid approach works better than the two separated \"traditional\" approaches separately. Synergy effects like advertising effects from the offline trainings for the online platform and vice versa occurred and regular newsletters turned out to have a noticeable effect on the community.\"",
        "title: \"Using agents to create learning opportunities in a collaborative learning environment\" with abstract: \"In order to foster situated learning in a virtual community of practice, we developed a multi-user, real-time, 3D car-driving simulation environment. In such a situation-based learning environment, the availability of enough appropriate learning situations is crucial for success. However, we experienced that often a collaborative usage of the system does not result in a large number of these critical situations. This paper introduces the idea of situation creators, intelligent agents who intentionally create specific situations for learners, into our 3D real-time simulation environment. These created situations challenge a learner much more and force him to react in order to master the driving knowledge.\"",
        "title: \"Evaluation of a question generation approach using semantic web for supporting argumentation\" with abstract: \"Discourse and argumentation are effective techniques for education not only in social domains but also in science domains. However, it is difficult for some teachers to stimulate an active discussion between students because several students might not be able to develop their arguments. This paper proposes to use WordNet as a semantic source in order to generate questions that are intended to stimulate students' brainstorming and to help them develop arguments in a discussion session. In a study including 141 questions generated by human experts and 44 questions generated by a computer system, the following research questions have been investigated: Are system-generated questions understandable? Are they relevant to given discussion topics? Would they be useful for supporting students in developing new arguments? Are understandable and relevant system-generated questions predicted to be useful for students in order to develop new arguments? The evaluation showed that system-generated questions could not be distinguished from human-generated questions in the context of two discussion topics while the difference between system-generated and human-generated questions was noticed in the context of one discussion topic. In addition, the evaluation study showed that system-generated questions that are relevant to a discussion topic correlate moderately with questions that are predicted as useful for students in developing new arguments in the context of two discussion topics and understandable system-generated questions are rated as useful in the context of one specific discussion topic.\"",
        "1 is \"Spatial Social Behavior in Second Life\", 2 is \"Social visualization encouraging participation in online communities\".",
        "\nGiven above information, for an author who has written the paper with the title \"The Scalable Adapter Design Pattern: Enabling Interoperability Between Educational Software Tools\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01206": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Bringing Non-programmer Authoring of Intelligent Tutors to MOOCs.':",
        "title: \"Creating a Corpus of Targeted Learning Resources with a Web-Based Open Authoring Tool\" with abstract: \"Personalizing learning to students\u2019 traits and interests requires diverse learning content. Previous studies have demonstrated the value of such materials in learning but a challenge remains in creating a corpus of content large enough to meet students\u2019 varied interests and abilities. We present and evaluate a prototype web-based tool for open authoring of learning materials. We conducted a study (an open web experiment) to evaluate whether specific student profiles presented in the tool\u2019s interface increase the diversity of the contributions, and whether authors tailor their contributions to the features in the profiles. We report on the quality of materials produced, authors\u2019 facility in rating them, effects of author traits, and impact of the tailoring feature. Participants were professional teachers (math and non-math) and amateurs. Participants were randomly assigned to the tailoring tool or a simplified version without the tailoring feature. We find that while there are differences by teaching status, all three groups make contributions of worth. The tailoring feature leads contributors to tailor materials with greater potential to engage students. The experiment suggests that an open access web-based tool is a feasible technology for developing a large corpus of materials for personalized learning.\"",
        "title: \"Toward Meta-cognitive Tutoring: A Model of Help Seeking with a Cognitive Tutor\" with abstract: \"The research reported in this paper focuses on the hypothesis that an intelligent tutoring system that provides guidance with respect to students' meta-cognitive abilities can help them to become better learners. Our strategy is to extend a Cognitive Tutor (Anderson, Corbett, Koedinger, & Pelletier, 1995) so that it not only helps students acquire domain-specific skills, but also develop better general help-seeking strategies. In developing the Help Tutor, we used the same Cognitive Tutor technology at the meta-cognitive level that has been proven to be very effective at the cognitive level. A key challenge is to develop a model of how students should use a Cognitive Tutor's help facilities. We created a preliminary model, implemented by 57 production rules that capture both effective and ineffective help-seeking behavior. As a first test of the model's efficacy, we used it off-line to evaluate students' help-seeking behavior in an existing data set of student-tutor interactions. We then refined the model based on the results of this analysis. Finally, we conducted a pilot study with the Help Tutor involving four students. During one session, we saw a statistically significant reduction in students' meta-cognitive error rate, as determined by the Help Tutor's model. These preliminary results inspire confidence as we gear up for a larger-scale controlled experiment to evaluate whether tutoring on help seeking has a positive effect on students' learning outcomes.\"",
        "title: \"Intercultural negotiation with virtual humans: the effect of social goals on gameplay and learning\" with abstract: \"One innovative use of digital games is to facilitate learning skills with social components by simulating human behavior with virtual humans. We investigate learners' social goals to understand how they help learners learn intercultural skills from virtual humans in BiLAT, a virtual world that teaches cross-cultural negotiation. We hypothesize that students learn more when they approach the simulation as a social interaction rather than taking a trial-and-error approach perhaps characteristic of video gaming. In a randomized controlled experiment with 59 participants, we found that participants improved cross-cultural negotiation skills through game play. Our hypothesis that participants given an explicit social goal would learn more than those given task-related goals was not confirmed. We did, however, find a positive relation between students' self-reported social goals, regardless of condition, and their learning results. This relation was confirmed through analysis of log data. Although it is still an open question how best to promote students' approaching a simulation with a social orientation, the results underline the importance of such goals.\"",
        "title: \"Graph Grammars: An ITS Technology for Diagram Representations\" with abstract: \"For many educational applications such as learning tools for argumentation, structured diagrams are a suitable form of external representation. However, student-created graphs pose some problems to ITS designers, especially in defined domains. This paper demonstrates a graph-grammar-based approach for ITS construction in domains that benefit from diagram representations. For these, graph grammars offer some helpful affordances. They make it easy to express possible manipulations by which a student might create a diagram, they facilitate the definition of structurally complex and pedagogically interesting constellations of graph elements to which an ITS should respond with feedback messages, and they offer a general parsing mechanism that allows the analysis of student-created diagrams by recognizing these constellations in graphs.\"",
        "title: \"More Accurate Student Modeling through Contextual Estimation of Slip and Guess Probabilities in Bayesian Knowledge Tracing\" with abstract: \"Modeling students' knowledge is a fundamental part of intelligent tutoring systems. One of the most popular methods for estimating students' knowledge is Corbett and Anderson's [6] Bayesian Knowledge Tracing model. The model uses four parameters per skill, fit using student performance data, to relate performance to learning. Beck [1] showed that existing methods for determining these parameters are prone to the Identifiability Problem:the same performance data can be fit equally well by different parameters, with different implications on system behavior. Beck offered a solution based on Dirichlet Priors [1], but, we show this solution is vulnerable to a different problem, Model Degeneracy, where parameter values violate the model's conceptual meaning (such as a student being more likely to get a correct answer if he/she does not know a skill than if he/she does).We offer a new method for instantiating Bayesian Knowledge Tracing, using machine learning to make contextual estimations of the probability that a student has guessed or slipped. This method is no more prone to problems with Identifiability than Beck's solution, has less Model Degeneracy than competing approaches, and fits student performance data better than prior methods. Thus, it allows for more accurate and reliable student modeling in ITSs that use knowledge tracing.\"",
        "1 is \"Hints: is it better to give or wait to be asked?\", 2 is \"About the relationship between ROC curves and Cohen's kappa\".",
        "\nGiven above information, for an author who has written the paper with the title \"Bringing Non-programmer Authoring of Intelligent Tutors to MOOCs.\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01207": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Forty years of the European Journal of Operational Research: A bibliometric overview.':",
        "title: \"Think co(mpletely)positive ! Matrix properties, examples and a clustered bibliography on copositive optimization\" with abstract: \"Copositive optimization is a quickly expanding scientific research domain with wide-spread applications ranging from global nonconvex problems in engineering to NP-hard combinatorial optimization. It falls into the category of conic programming (optimizing a linear functional over a convex cone subject to linear constraints), namely the cone $${\\mathcal{C}}$$ of all completely positive symmetric n \u8133 n matrices (which can be factorized into $${FF^\\top}$$ , where F is a rectangular matrix with no negative entry), and its dual cone $${\\mathcal{C}^*}$$ , which coincides with the cone of all copositive matrices (those which generate a quadratic form taking no negative value over the positive orthant). We provide structural algebraic properties of these cones, and numerous (counter-)examples which demonstrate that many relations familiar from semidefinite optimization may fail in the copositive context, illustrating the transition from polynomial-time to NP-hard worst-case behaviour. In course of this development we also present a systematic construction principle for non-attainability phenomena, which apparently has not been noted before in an explicit way. Last but not least, also seemingly for the first time, a somehow systematic clustering of the vast and scattered literature is attempted in this paper.\"",
        "title: \"Regularity versus Degeneracy in Dynamics, Games, and Optimization: A Unified Approach to Different Aspects.\" with abstract: \"In this paper, links are established between optimality conditions for quadratic optimization problems, qualitative properties in the nonlinear selection replicator dynamics, and central solution concepts of evolutionary game theory, with particular emphasis on several regularity conditions that are desirable in any of the three fields mentioned above: as strictness conditions for locally optimal solutions, as hyperbolicity conditions for fixed points, and as quasi-strictness conditions for game equilibria.\"",
        "title: \"Copositivity detection by difference-of-convex decomposition and \u03c9-subdivision.\" with abstract: \"We present three new copositivity tests based upon difference-of-convex (d.c.) decompositions, and combine them to a branch-and-bound algorithm of \u03c9-subdivision type. The tests employ LP or convex QP techniques, but also can be used heuristically using appropriate test points. We also discuss the selection of efficient d.c. decompositions and propose some preprocessing ideas based on the spectral d.c. decomposition. We report on first numerical experience with this procedure which are very promising.\"",
        "title: \"Ellipsoidal Approach to Box-Constrained Quadratic Problems\" with abstract: \"We present a new heuristic for the global solution of box constrained quadratic problems, based on the classical results which hold for the minimization of quadratic problems with ellipsoidal constraints. The approach is tested on several problems randomly generated and on graph instances from the DIMACS challenge, medium size instances of the Maximum Clique Problem. The numerical results seem to suggest some effectiveness of the proposed approach.\"",
        "title: \"Copositive optimization - Recent developments and applications.\" with abstract: \"Due to its versatility, copositive optimization receives increasing interest in the Operational Research community, and is a rapidly expanding and fertile field of research. It is a special case of conic optimization, which consists of minimizing a linear function over a cone subject to linear constraints. The diversity of copositive formulations in different domains of optimization is impressive, since problem classes both in the continuous and discrete world, as well as both deterministic and stochastic models are covered. Copositivity appears in local and global optimality conditions for quadratic optimization, but can also yield tighter bounds for NP-hard combinatorial optimization problems. Here some of the recent success stories are told, along with principles, algorithms and applications. (C) 2011 Published by Elsevier B.V.\"",
        "1 is \"Consideration of Partial User Preferences in Evolutionary Multiobjective Optimization\", 2 is \"Fully copositive matrices\".",
        "\nGiven above information, for an author who has written the paper with the title \"Forty years of the European Journal of Operational Research: A bibliometric overview.\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01208": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Detecting conversational groups in images and sequences: A robust game-theoretic approach.':",
        "title: \"Randomized Prediction Games for Adversarial Machine Learning.\" with abstract: \"In spam and malware detection, attackers exploit randomization to obfuscate malicious data and increase their chances of evading detection at test time, e.g., malware code is typically obfuscated using random strings or byte sequences to hide known exploits. Interestingly, randomization has also been proposed to improve security of learning algorithms against evasion attacks, as it results in hidi...\"",
        "title: \"Dominant-Set Clustering Using Multiple Affinity Matrices\" with abstract: \"Pairwise (or graph-based) clustering algorithms typically assume the existence of a single affinity matrix, which describes the similarity between the objects to be clustered. In many practical applications, however, several similarity relations might be envisaged and the problem arises as to how properly select or combine them. In this paper we offer a solution to this problem for the case of dominant sets, a well-known formalization of the notion of a cluster, which generalizes the notion of maximal clique to edge-weighted graphs and has intriguing connections to evolutionary game theory. Specifically, it has been shown that dominant sets can be bijectively related to Evolutionary Stable Strategies (ESS) - a classic notion of equilibrium in the evolutionary game theory field - of a so-called \"clustering game\". The clustering game is a non-cooperative game between two-players, where the objects to cluster form the set of strategies, while the affinity matrix provides the players' payoffs. The proposed approach generalizes dominant sets to multiple affinities by extending the clustering game, which has a single payoff, to a multi-payoff game. Accordingly, dominant sets in the multi-affinity setting become equivalent to ESSs of a corresponding multi-payoff clustering game, which can be found by means of so-called Biased Replicator Dynamics. Experiments conducted over standard benchmark datasets consistently show that the proposed combination scheme allows one to substantially improve the performance of dominant-set clustering over its single-affinity counterpart.\"",
        "title: \"Graph transduction as a noncooperative game\" with abstract: \"Graph transduction is a popular class of semisupervised learning techniques that aims to estimate a classification function defined over a graph of labeled and unlabeled data points. The general idea is to propagate the provided label information to unlabeled nodes in a consistent way. In contrast to the traditional view, in which the process of label propagation is defined as a graph Laplacian regularization, this article proposes a radically different perspective, based on game-theoretic notions. Within the proposed framework, the transduction problem is formulated in terms of a noncooperative multiplayer game whereby equilibria correspond to consistent labelings of the data. An attractive feature of this formulation is that it is inherently a multiclass approach and imposes no constraint whatsoever on the structure of the pairwise similarity matrix, being able to naturally deal with asymmetric and negative similarities alike. Experiments on a number of real-world problems demonstrate that the proposed approach performs well compared with state-of-the-art algorithms, and it can deal effectively with various types of similarity relations.\"",
        "title: \"Structured Labels in Random Forests for Semantic Labelling and Object Detection\" with abstract: \"Ensembles of randomized decision trees, known as Random Forests, have become a valuable machine learning tool for addressing many computer vision problems. Despite their popularity, few works have tried to exploit contextual and structural information in random forests in order to improve their performance. In this paper, we propose a simple and effective way to integrate contextual information in random forests, which is typically reflected in the structured output space of complex problems like semantic image labelling. Our paper has several contributions: We show how random forests can be augmented with structured label information and be used to deliver structured low-level predictions. The learning task is carried out by employing a novel split function evaluation criterion that exploits the joint distribution observed in the structured label space. This allows the forest to learn typical label transitions between object classes and avoid locally implausible label configurations. We provide two approaches for integrating the structured output predictions obtained at a local level from the forest into a concise, global, semantic labelling. We integrate our new ideas also in the Hough-forest framework with the view of exploiting contextual information at the classification level to improve the performance on the task of object detection. Finally, we provide experimental evidence for the effectiveness of our approach on different tasks: Semantic image labelling on the challenging MSRCv2 and CamVid databases, reconstruction of occluded handwritten Chinese characters on the Kaist database and pedestrian detection on the TU Darmstadt databases. \"",
        "title: \"Image Segmentation by Dominant Sets\" with abstract: \"We develop a framework for the image segmentation problem based on a new graph-theoretic formulation of clustering. The approach is motivated by the analogies between the intuitive concept of a cluster and that of a dominant set of vertices, a novel notion that generalizes that of a maximal complete subgraph to edge-weighted graphs. We also establish a correspondence between dominant sets and the extrema of a quadratic form over the standard simplex, thereby allowing us the use of continuous optimization techniques such as replicator dynamics from evolutionary game theory. Such systems are attractive as can easily be implemented in a parallel network of locally interacting computational units, and offer the advantage of biological plausibility. We present experimental results on real-world images which show the effectiveness of the approach.\"",
        "1 is \"Spectral clustering with inconsistent advice\", 2 is \"Fast illumination-invariant background subtraction using two views: error analysis, sensor placement and applications\".",
        "\nGiven above information, for an author who has written the paper with the title \"Detecting conversational groups in images and sequences: A robust game-theoretic approach.\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01209": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'A Tool to Schedule Parallel Applications on Multiprocessors: The NANOS CPU MANAGER':",
        "title: \"Fast Soliton Automata\" with abstract: \"Solitons, are special moving waves with the remarcable characteristic that when colliding they come out of the collision without loosing their initial properties. Soliton Automata (SA) are a particular class of Cellular Automata which support solitons.\"",
        "title: \"Scheduling User-Level Threads on Distributed Shared-Memory Multiprocessors\" with abstract: \"In this paper we present Dynamic Bisectioning or DBS, a simple but powerful comprehensive scheduling policy for user-level threads, which unifies the exploitation of (multidimensional) loop and nested functional (or task) parallelism. Unlike other schemes that have been proposed and used thus far, DBS is not constrained to scheduling DAGs or singly nested parallel loops. Rather, our policy encompasses the most general type of parallel program model that allows arbitrary mix of nested loops and nested DAGs (directed acyclic task-graphs) or any combination of the above. DBS employs a simple but powerful two-level dynamic policy which is adaptive and sensitive to the type and amount of parallelism at hand. On one extreme DBS approximates static scheduling, hence facilitating locality of data, while at the other extreme it resorts to dynamic thread migration in order to balance uneven loads. Even the latter is done in a controlled way so as to minimize network latency.\"",
        "title: \"Web services for digital rights management and copyright protection in digital media\" with abstract: \"The paper focuses on the implementation of an advanced digital rights management (DRM) system which supported by web services offers copyright protection and management of digital media. The main components of the DRM system are a digital image library, which offers specialized services for storing and searching and a copyright protection and digital rights management subsystem for the digitized media based on innovative watermarking and web technologies.\"",
        "title: \"Tying memory management to parallel programming models\" with abstract: \"Stand-alone threading libraries lack sophisticated memory management techniques. In this paper, we present a methodology that allows threading libraries that implement non-preemptive parallel programming models to reduce their memory requirements, based on the properties of those models. We applied the methodology to NthLib, which is an implementation of the Nano-Threads programming model, and evaluated it on an Intel based multiprocessor system with HyperThreading and on the SMTSIM simulator. Our results indicate that not only memory requirements drop drastically, but that execution time also improves, compared to the original implementation. This allows more fine-grained, but also larger numbers of parallel tasks to be created.\"",
        "title: \"Semantic Web enabled digital repositories\" with abstract: \"Digital repositories and digital libraries are today among the most common tools for managing and disseminating digital object collections of cultural, educational, and other kinds of content over the Web. However, it is often the case that descriptive information about these assets, known as metadata, are usually semi-structured from a semantics point of view; implicit knowledge about this content may exist that cannot always be represented in metadata implementations and thus is not always discoverable. To this end, in this article we propose a method and a practical implementation that could allow traditional metadata-intensive repositories to benefit from Semantic Web ideas and techniques. In particular, we show how, starting with a semi-structured knowledge model (like the one offered by DSpace), we can end up with inference-based knowledge discovery, retrieval, and navigation among the repository contents. Our methodology and results are applied on the University of Patras institutional repository. The resulting prototype is also available as a plug-in, although it can fit, in principle, any other kind of digital repository.\"",
        "1 is \"Romberg integration using systolic arrays\", 2 is \"Onipss: A Proposal For Programming Heterogeneous Multi-Core Architectures\".",
        "\nGiven above information, for an author who has written the paper with the title \"A Tool to Schedule Parallel Applications on Multiprocessors: The NANOS CPU MANAGER\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01210": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'On the Longest Common Rigid Subsequence Problem':",
        "title: \"Alignment between Two RNA Structures\" with abstract: \"The primary structure of a ribonucleic acid (RNA) molecule can be represented as a sequence of nucleotides (bases) over the fourletter alphabet {A,C,G,U}. The RNA secondary and tertiary structures can be represented as a set of nested base pairs and a set of crossing base pairs, respectively. These base pairs form bonds between A - U, C - G, and G - U. This paper considers alignment with affine gap penalty between two RNA molecule structures. In general this problem is Max SNP-hard for tertiary structures. We present an algorithm for the case where aligned base pairs are non-crossing. Experimental results show that this algorithm can be used for practical application of RNA structure alignment.\"",
        "title: \"Computing similarity between RNA structures\" with abstract: \"The primary structure of a ribonucleic acid (RNA) molecule is a sequence of nucleotides (bases) over the four-letter alphabet {A,C,G,U}. The secondary or tertiary structure of an RNA is a set of base-pairs (nucleotide pairs) which forms bonds between AU and CG. For secondary structures, these bonds have been traditionally assumed to be one to one and non-crossing. This paper considers a notion of similarity between two RNA molecule structures taking into account the primary, the secondary and the tertiary structures. We show that, for tertiary structures, it is Max SNP-hard for both minimization and maximization versions. We show a stronger result for the maximization version where it cannot be approximated within ratio 2logn in polynomial time, unless NPDTIME[2polylogn]. We then present an algorithm that can be used for practical application. Our algorithm will produce an optimal solution for the case where at least one of the RNA involved is of a secondary structure. We also show an approximation algorithm.\"",
        "title: \"An approach for N-linked glycan identification from MS/MS spectra by target-decoy strategy.\" with abstract: \"Glycan structure determination serves as an essential step for the thorough investigation of the structure and function of protein. Currently, appropriate sample preparation followed by tandem mass spectrometry has emerged as the dominant technique for the characterization of glycans and glycopeptides. Although extensive efforts have been made to the development of computational approaches for the automated interpretation of glycopeptide spectra, the previously appeared methods lack a reasonable quality control strategy for the statistical validation of reported results. In this manuscript, we introduced a novel method that constructed a decoy glycan database based on the glycan structures in the target database, and searched the experimental spectra against both the target and decoy databases to find the best matched glycans. Specifically, a two-layer scoring scheme for calculating a normalized matching score is applied in the search procedure which enables the unbiased ranking of the matched glycans. Experimental analysis showed that our proposed method can report more structures with high confidence compared with previous approaches.\"",
        "title: \"Identifying approximately common substructures in trees based on a restricted edit distance\" with abstract: \"In this paper we present a dynamic programming algorithm for identifying the largest approximately common substructures (LACSs) of two ordered labeled trees. We consider a substructure of a tree T to be a connected subgraph of T. Given two trees T-1, T-2 and an integer d, the LACS problem is to find a substructure U-1 of T-1 and a substructure U-2 of T-2 such that U-1 is within distance d of U-2 and where there does not exist any other substructure V-1 of T-1 and V-2 of T-2 such that V-1 and V-2 satisfy the distance restriction and the sum of the sizes of V-1 and V-2 is greater than the sum of the sizes of U-1 and U-2. The distance measure considered in the paper is a restricted edit distance originated from Selkow. The LACS problem is motivated by the studies of program and document comparison. The proposed algorithm solves the problem in time O(d(2) x \\T-1\\ x \\T-2\\), which is as fast as the best known algorithm for calculating Selkow's distance of two trees when the distance allowed in the common substructures is a constant independent of the input trees. (C) 1999 Elsevier Science Inc. All rights reserved.\"",
        "title: \"A better tree-structured vector quantizer\" with abstract: \"A new vector quantizer permits logarithmic-time encoding and yet performs better than the locally optimal quantizers generated by the LBG algorithm. The success is credited to an elaborated tree-structured optimization process in the codebook design\"",
        "1 is \"Preprocessing an undirected planar network to enable fast approximate distance queries\", 2 is \"Ordered Types in the AQUA Data Model\".",
        "\nGiven above information, for an author who has written the paper with the title \"On the Longest Common Rigid Subsequence Problem\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01211": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Advances In Data Modeling Research':",
        "title: \"Understanding the Competitive Landscape of News Providers on Social Media.\" with abstract: \"Social media has emerged as a mechanism for online news propagation. This in turn has changed the competitive landscape of news providers, a landscape that was previously partitioned based on the traditional channels of news dispersion. The channels of news distribution refer to - television, newspaper, magazine, radio, news agency and online only. In this paper, we examine similarities and differences in news propagation patterns on social media based on the primary channel of a news provider. We collected news article propagation activity data from Twitter for 32 news providers over a three-week period and analyzed their propagation networks. Our analysis shows that the structural properties of the propagation networks are statistically different based on the type of primary channel. Our study has useful implications for understanding the competition between news providers in an online environment.\n\n\"",
        "title: \"On augmenting database design-support environments to capture the geo-spatio-temporal data semantics\" with abstract: \"A database design-support environment supports a data analyst in eliciting, articulating, specifying and validating data-related requirements. Extant design-support environments--based on conventional conceptual models--do not adequately support applications that need to organize data based on time (e.g., accounting, portfolio management, personnel management) and/or space (e.g., facility management, transportation, logistics). For geo-spatio-temporal applications, it is left to database designers to discover, design and implement--on an ad-hoc basis--the temporal and geospatial concepts that they need to represent the miniworld. To elicit the geo-spatio-temporal data semantics, we characterize guiding principles for augmenting the conventional conceptual database design approach, present our annotation-based approach, and illustrate how our proposed approach can be instantiated via a proof-of-concept prototype. Via a proof-of-concept database design-support environment, we exemplify our annotation-based approach, and show how segregating \"what\" from \"when/where\" via annotations satisfies ontologic- and cognition-based requirements, dovetails with existing database design methodologies, results in upward-compatible conceptual as well as XML schemas, and provides a straightforward mechanism to extend extant design-support environments.\"",
        "title: \"Modeling Spatial and Temporal Set-Based Constraints During Conceptual Database Design\" with abstract: \"From a database perspective, business constraints provide an accurate picture of the real world being modeled and help enforce data integrity. Typically, rules are gathered during requirements analysis and embedded in code during the implementation phase. We propose that the rules be explicitly modeled during conceptual design, and develop a framework for understanding and classifying spatiotemporal set-based (cardinality) constraints and an associated syntax. The constraint semantics are formally specified using first-order logic. Modeling rules in conceptual design ensures they are visible to designers and users and not buried in application code. The rules can then be semiautomatically translated into logical design triggers yielding productivity gains. Following the principles of design science research, we evaluate the framework's expressiveness and utility with a case study.\"",
        "title: \"Using a knowledge learning framework to predict errors in database design\" with abstract: \"Conceptual data modeling is a critical but difficult part of database development. Little research has attempted to find the underlying causes of the cognitive challenges or errors made during this stage. This paper describes a Modeling Expertise Framework (MEF) that uses modeler expertise to predict errors based on the revised Bloom's taxonomy (RBT). The utility of RBT is in providing a classification of cognitive processes that can be applied to knowledge activities such as conceptual modeling. We employ the MEF to map conceptual modeling tasks to different levels of cognitive complexity and classify current modeler expertise levels. An experimental exercise confirms our predictions of errors. Our work provides an understanding into why novices can handle entity classes and identifying binary relationships with some ease, but find other components like ternary relationships difficult. We discuss implications for data modeling training at a novice and intermediate level, which can be extended to other areas of Information Systems education and training.\"",
        "title: \"IAIS: A Methodology to Enable Inter-Agency Information Sharing In eGovernment\" with abstract: \"Recently, there has been increased interest in information sharing among government agencies, with a view toward improving security, reducing costs and offering better quality service to users of government services. In this work, the authors complement earlier work by proposing a comprehensive methodology called IAIS (Inter Agency Information Sharing) that uses XML to facilitate the definition of information that needs to be shared, the storage of such information, the access to this information and finally the maintenance of shared information. The authors compare IAIS with two alternate methodologies to share information among agencies, and analyze the pros and cons of each. They also show how IAIS leverages the recently proposed XML (extensible markup language) standard to allow for inclusion of various groups' viewpoints when determining what information should be shared and how it should be structured.\"",
        "1 is \"tagging, communities, vocabulary, evolution\", 2 is \"Two case studies of open source software development: Apache and Mozilla\".",
        "\nGiven above information, for an author who has written the paper with the title \"Advances In Data Modeling Research\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01212": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Fifth Workshop on Software Engineering for Adaptive and Self-Managing Systems (SEAMS 2010)':",
        "title: \"Uncertainty reduction in self-adaptive systems.\" with abstract: \"Self-adaptive systems depend on models of themselves and their environment to decide whether and how to adapt, but these models are often affected by uncertainty. While current adaptation decision approaches are able to model and reason about this uncertainty, they do not consider ways to reduce it. This presents an opportunity for improving decision-making in self-adaptive systems, because reducing uncertainty results in a better characterization of the current and future states of the system and the environment (at some cost), which in turn supports making better adaptation decisions. We propose uncertainty reduction as the natural next step in uncertainty management in the field of self-adaptive systems. This requires both an approach to decide when to reduce uncertainty, and a catalog of tactics to reduce different kinds of uncertainty. We present an example of such a decision, examples of uncertainty reduction tactics, and describe how uncertainty reduction requires changes to the different activities in the typical self-adaptation loop.\n\n\"",
        "title: \"Bridging the Gap between Systems Design\" with abstract: \"A challenging problem for software engineering practitioners is moving from high-level system architectures produced by system engineers to deployable software produced by software engineers. In this paper we describe our experience working with NASA engineers to develop an approach and toolset for automating the generation of space systems software from architectural specifications. Our experience shows that it is possible to leverage the space systems domain, formal architectural specifications, and component technology to provide retargetable code generators for this class of software\"",
        "title: \"Improving architecture-based self-adaptation using preemption\" with abstract: \"One common approach to self-adaptive systems is to incorporate a control layer that monitors a system, supervisorily detects problems, and applies adaptation strategies to fix problems or improve system behavior. While such approaches have been found to be quite effective, they are typically limited to carrying out a single adaptation at a time, delaying other adaptations until the current one finishes. This in turn leads to a problem in which a time-critical adaptation may have to wait for an existing long-running adaptation to complete, thereby missing a window of opportunity for that adaptation. In this paper we improve on existing practice through an approach in which adaptations can be preempted to allow for other time-critical adaptations to be scheduled. Scheduling is based on an algorithm that maximizes time-related utility for a set of concurrently executing adaptations.\"",
        "title: \"What Ails End-User Composition: A Cross-Domain Qualitative Study.\" with abstract: \"Across many domains, end-users need to compose computational elements into novel configurations to perform their day-to-day tasks. End-user composition is a common programming activity performed by such end-users to accomplish this composition task. While there have been many studies on end-user programming, we still need a better understanding of activities involved in end-user composition and environments to support them. In this paper we report a qualitative study of four popular composition environments belonging to diverse application domains, including: Taverna workflow environment for life sciences, Loni Pipeline for brain imaging, SimMan3G for medical simulations and Kepler for scientific simulations. We interview end-users of these environments to explore their experiences while performing common compositions tasks. We use \u201cContent Analysis\u201d technique to analyze these interviews to explore what are the barriers to end-user composition in these domains. Furthermore, our findings show that there are some unique differences in the requirements of naive end-users vs. expert programmers. We believe that not only are these findings useful to improve the quality of end-user composition environments, but they can also help towards development of better end-user composition frameworks.\"",
        "title: \"Style-based reuse for software architectures\" with abstract: \"Although numerous mechanisms for promoting software reuse have been proposed and implemented over the years, most have focused on the reuse of implementation code. There is much conjecture and some empirical evidence, however, that the most effective forms of reuse are generally found at more abstract levels of software design. We discuss software reuse at the architectural level of design. Specifically, we argue that the concept of \"architectural style\" is useful for supporting the classification, storage, and retrieval of reusable architectural design elements. We briefly describe the Aesop system's Software Shelf (D. Garlan et al., 1994), a tool that assists designers in selecting appropriate design elements and patterns based on stylistic information and design constraints.\"",
        "1 is \"Design patterns for object-oriented software development (tutorial)\", 2 is \"Building intelligent web applications using lightweight wrappers\".",
        "\nGiven above information, for an author who has written the paper with the title \"Fifth Workshop on Software Engineering for Adaptive and Self-Managing Systems (SEAMS 2010)\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01213": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Optimization of Wrappers and Mediators for Web Accessible Data Sources (WebSources)':",
        "title: \"BIPASS: BioInformatics Pipeline Alternative Splicing Services.\" with abstract: \"BioInformatics Pipeline Alternative Splicing Services (BIPASS) offer support to scientists interested in gathering information related to alternative splicing (AS) events. The service BIPAS-SpliceDB provides access to AS information that has been extracted a priori from various public databases and stored in a data warehouse. In contrast, the BIPAS-Align&Splice service allows scientists to submit their own sequences and genome to compute AS analysis results. BIPAS services offer various user-friendly ways to navigate through the results. AS results are organized at different conceptual levels (clusters and sequences), and are displayed in graphs or summarized in tables that can be down-loaded in XML or text format. The two BIPAS services SpliceDB and Align&Splice are available online at http://bip.umiacs.umd.edu:8080/.\"",
        "title: \"Wrapper Generation for Web Accessible Data Sources\" with abstract: \"There is an increase in the number of data sources that can be queried across the WWW. Such sources typically support HTML forms-based interfaces and search engines query collections of suitably indexed data. The data is displayed via a browser. One drawback is that there is no standard programming interface suitable for applications to submit queries. Second, the output (answer to a query) is not well structured. Structured objects have to be extracted from the HTML documents which contain irrelevant data and which may be volatile. Third, domain knowledge about the data source is also embedded in HTML documents and must be extracted. To solve these problems, we present technology to define and (automatically) generate wrappers for Web accessible sources. Our contributions are as follows: (1)Defining a wrapper interface to specify the capability of Web accessible data sources. (2) Developing a wrapper generation toolkit of graphical interfaces and specification languages to specify the capability of sources and the functionality of the wrapper. (3) Developing the technology to automatically generate a wrapper appropriate to the Web accessible source, from the specifications.\"",
        "title: \"Interoperable query processing with multiple heterogeneous knowledge servers\" with abstract: \"This paper describes a technique for infor- mation mediation when multiple heterogeneous knowl- edge and data servers are to be accessed during query processing. One problem is building an intelligent in- terface between each knowledge server (KS) and its processor (KP); and the second is to provide interoper- ability among multiple K P/KS so that a query may be answered using information from multiple sources. We present example scenarios which highlight these prob- lems and then outline query mapping and transforma- tion techniques that are applicable. The techniques for solving the interoperability problems involve rep- resentations in some canonical form. This includes a canonical represent ation (CR) corresponding to each KP/KS pair and a merged CR (MCR) to represent the mapping among the C%. The MCR and CRa include relevant information obtained from a source query, and heterogeneous mapping (bet-map) information, for all possible mappings among the multiple servers. The knowledge in the canonical form must be represented so that it can be easily accessed during query transfor- mation. We use an example of translating queries from an object schema to a relational schema to illustrate typical knowledge that must be represented in some canonical form. We use a high level logical language, F-logic, to represent the heterogeneous mapping (het- map) and query transformation information as a set of declarative rules, in the canonical form.\"",
        "title: \"Measuring Relatedness Between Scientific Entities in Annotation Datasets\" with abstract: \"Linked Open Data has made available a diversity of scientific collections where scientists have annotated entities in the datasets with controlled vocabulary terms (CV terms) from ontologies. These semantic annotations encode scientific knowledge which is captured in annotation datasets. One can mine these datasets to discover relationships and patterns between entities. Determining the relatedness (or similarity) between entities becomes a building block for graph pattern mining, e.g., identifying drug-drug relationships could depend on the similarity of the diseases (conditions) that are associated with each drug. Diverse similarity metrics have been proposed in the literature, e.g., i) string-similarity metrics; ii) path-similarity metrics; iii) topological-similarity metrics; all measure relatedness in a given taxonomy or ontology. In this paper, we consider a novel annotation similarity metric AnnSim that measures the relatedness between two entities in terms of the similarity of their annotations. We model AnnSim as a 1-to-1 maximal weighted bipartite match, and we exploit properties of existing solvers to provide an efficient solution. We empirically study the effectiveness of AnnSim on real-world datasets of genes and their GO annotations, clinical trials, and a human disease benchmark. Our results suggest that AnnSim can provide a deeper understanding of the relatedness of concepts and can provide an explanation of potential novel patterns.\"",
        "title: \"Logic-based query optimization for object databases\" with abstract: \"We present a technique for transferring query optimization techniques, developed for relational databases, into object databases. We demonstrate this technique for ODMG database schemas defined in ODL and object queries expressed in OQL. The object schema is represented using a logical representation (Datalog). Semantic knowledge about the object data model, e.g., class hierarchy information, relationship between objects, etc., as well as semantic knowledge about a particular schema and application domain are expressed as integrity constraints. An OQL object query is represented as a logic query and query optimization is performed in the Datalog representation. We obtain equivalent (optimized) logic queries, and subsequently obtain equivalent (optimized) OQL queries for each equivalent logic query. We present one optimization technique for semantic query optimization (SQO) based on the residue technique of U. Charavarthy et al. (1990; 1986; 1988). We show that our technique generalizes previous research on SQO for object databases. We handle a large class of OQL queries, including queries with constructors and methods. We demonstrate how SQO can be used to eliminate queries which contain contradictions and simplify queries, e.g., by eliminating joins, or by reducing the access scope for evaluating a query to some specific subclass(es). We also demonstrate how the definition of a method or integrity constraints describing the method, can be used in optimizing a query with a method\"",
        "1 is \"Heuristic and randomized optimization for the join ordering problem\", 2 is \"Compilation of query-rewriting problems into tractable fragments of propositional logic\".",
        "\nGiven above information, for an author who has written the paper with the title \"Optimization of Wrappers and Mediators for Web Accessible Data Sources (WebSources)\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01214": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Impact of Information on the Complexity of Asynchronous Radio Broadcasting':",
        "title: \"Pairwise Compatibility Graphs: A Survey.\" with abstract: \"A graph G = (V, E) is a pairwise compatibility graph (PCG) if there exists an edge-weighted tree T and two nonnegative real numbers d(min) and d(max) such that each leaf u of T is a node of V and there is an edge (u, v) is an element of E if and only if d(min) <= d(T) (u, v) <= d(max), where d(T) (u, v) is the sum of weights of the edges on the unique path from u to v in T. In this article, we survey the state of the art concerning this class of graphs and some of its subclasses.\"",
        "title: \"Maximizing the Number of Broadcast Operations in Random Geometric Ad Hoc Wireless Networks\" with abstract: \"We consider static ad hoc wireless networks whose nodes, equipped with the same initial battery charge, may dynamically change their transmission range. When a node v transmits with range r(v), its battery charge is decreased by \\beta r(v)^2, where \\beta 0 is a fixed constant. The goal is to provide a range assignment schedule that maximizes the number of broadcast operations from a given source (this number is denoted by the length of the schedule). This maximization problem, denoted by Max LifeTime, is known to be NP-hard and the best algorithm yields worst-case approximation ratio \\Theta (\\log n), where n is the number of nodes of the network. We consider random geometric instances formed by selecting n points independently and uniformly at random from a square of side length \\sqrt{n} in the euclidean plane. We present an efficient algorithm that constructs a range assignment schedule having length not smaller than 1/12 of the optimum with high probability. Then we design an efficient distributed version of the above algorithm, where nodes initially know n and their own position only. The resulting schedule guarantees the same approximation ratio achieved by the centralized version, thus, obtaining the first distributed algorithm having provably good performance for this problem.\"",
        "title: \"Efficient algorithms for checking the equivalence of multistage interconnection networks\" with abstract: \"In this paper we study the topological equivalence problem of multistage interconnection networks (MINs). We prove a new characterization of topologically equivalent MINs by means of a novel approach. Applying this characterization to log N stage MINs we completely describe the equivalence class which the Reverse Baseline belongs to. Most important, we apply the characterization to (2 log N - 1) stage MINs obtained as concatenation of two log N stage Reverse Baseline equivalent MINs: in this way, we deduce an O(N log N) time algorithm testing the equivalence of two such MINs. This result substantially improves the time complexity of the previously known algorithms (O(N4 log N)). Finally, we determine the number of different equivalence classes of (2 log N - 1) stage MINs and we characterize each of them.\"",
        "title: \"L(h,1,1)-Labeling of outerplanar graphs\" with abstract: \"An L(h,1,1)-labeling of a graph is an assignment of labels from the set of integers {0, \u22ef, \u03bb} to the vertices of the graph such that adjacent vertices are assigned integers of at least distance h \u22651 apart and all vertices of distance three or less must be assigned different labels. The aim of the L(h,1,1)-labeling problem is to minimize \u03bb, denoted by \u03bbh,1,1 and called span of the L(h,1,1)-labeling As outerplanar graphs have bounded treewidth, the L(1,1,1)-labeling problem on outerplanar graphs can be exactly solved in O(n3), but the multiplicative factor depends on the maximum degree \u0394 and is too big to be of practical use. In this paper we give a linear time approximation algorithm for computing the more general L(h,1,1)-labeling for outerplanar graphs that is within additive constants of the optimum values\"",
        "title: \"Labeling trees with a condition at distance two\" with abstract: \"An L(h,k)-labeling of a graph G is an integer labeling of vertices of G, such that adjacent vertices have labels which differ by at least h, and vertices at distance two have labels which differ by at least k. The span of an L(h,k)-labeling is the difference between the largest and the smallest label. We investigate L(h,k)-labelings of trees of maximum degree \u0394, seeking those with small span. Given \u0394, h and k, span \u03bb is optimal for the class of trees of maximum degree \u0394, if \u03bb is the smallest integer such that every tree of maximum degree \u0394 has an L(h,k)-labeling with span at most \u03bb. For all parameters \u0394,h,k, such that h<k, we construct L(h,k)-labelings with optimal span. We also establish optimal span of L(h,k)-labelings for stars of arbitrary degree and all values of h and k.\"",
        "1 is \"Lie patterns in search procedures\", 2 is \"Frequency Channel Assignment on Planar Networks\".",
        "\nGiven above information, for an author who has written the paper with the title \"Impact of Information on the Complexity of Asynchronous Radio Broadcasting\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01215": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Hybrid neural network models for bankruptcy predictions':",
        "title: \"Hybrid neural network models for bankruptcy predictions\" with abstract: \"The objective of this paper is to develop the hybrid neural network models for bankruptcy prediction. The proposed hybrid neural network models are (1) a MDA-assisted neural network, (2) an ID3-assisted neural network, and (3) a SOFM(self organizing feature map)-assisted neural network. Both the MDA-assisted neural network and the 11)3-assisted neural network are the neural network models operating with the input variables selected by the MDA method and 1133 respectively. The SOFM-assisted neural network combines a backpropagation model (supervised learning) with a SOFM model (unsupervised learning). The performance of the hybrid neural network model is evaluated using MDA and ID3 as a benchmark. Empirical results using Korean bankruptcy data show that hybrid neural network models are very promising neural network models for bankruptcy prediction in terms of predictive accuracy and adaptability.\"",
        "title: \"The priority factor model for customer relationship management system success\" with abstract: \"As the market competition becomes keen, constructing a customer relationship management system is coming to the front for winning over new customers, developing service and products for customer satisfaction and retaining existing customers. However, decisions for CRM implementation have been hampered by inconsistency between information technology and marketing strategies, and the lack of conceptual bases necessary to develop the success measures. Using a structural equation analysis, this study explores the CRM system success model that consists of CRM initiatives: process fit, customer information quality, and system support; intrinsic success: efficiency and customer satisfaction; and extrinsic success: profitability. These constructs underlie much of the existing literature on information system success and customer satisfaction perspectives. We found the empirical support for CRM implementation decision-making from 253 respondents of 14 companies which have implemented the CRM system. These findings should be of great interest to both researchers and practitioners.\"",
        "title: \"The cluster-indexing method for case-based reasoning using self-organizing maps and learning vector quantization for bond rating cases\" with abstract: \"This paper presents a hybrid data mining model for the prediction of corporate bond rating. This model uses a new case-indexing method of case-based reasoning (CBR), which utilizes the cluster information of financial data in order to improve classification accuracy. This method uses not only case-specific knowledge of past problems like conventional CBR, but also uses additional knowledge derived from the clusters of cases. The cluster-indexing method assumes that there are some distinct subgroups (clusters) in each rated group. Competitive artificial neural networks are used to generate the centroid values of clusters because these techniques produce better adaptive clusters than statistical clustering algorithms. The experiments using corporate bond rating cases show that the cluster-indexing CBR is superior to conventional CBR and inductive learning-indexing CBR\u2014a rival case indexing method.\"",
        "title: \"The discovery of experts' decision rules from qualitative bankruptcy data using genetic algorithms\" with abstract: \"Numerous studies on bankruptcy prediction have widely applied data mining techniques to finding out the useful knowledge automatically from financial databases, while few studies have proposed qualitative data mining approaches capable of eliciting and representing experts' problem-solving knowledge from experts' qualitative decisions. In an actual risk assessment process, the discovery of bankruptcy prediction knowledge from experts is still regarded as an important task because experts' predictions depend on their subjectivity. This paper proposes a genetic algorithm-based data mining method for discovering bankruptcy decision rules from experts' qualitative decisions. The results of the experiment show that the genetic algorithm generates the rules which have the higher accuracy and larger coverage than inductive learning methods and neural networks. They also indicate that considerable agreement is achieved between the GA method and experts' problem-solving knowledge. This means that the proposed method is a suitable tool for eliciting and representing experts' decision rules and thus it provides effective decision supports for solving bankruptcy prediction problems.\"",
        "title: \"The Different Effects Of Online Consumer Reviews On Consumers' Purchase Intentions Depending On Trust In Online Shopping Malls An Advertising Perspective\" with abstract: \"Purpose - With the increasing influence of online consumer reviews (OCRs) on a consumer's decision making, online sellers have begun to embed the OCRs in their advertisements (OEAs). This study has the following two research objectives: first, to investigate the effects of two types of (OCRs vs OEAs) on consumers' purchase intention from an informational influence perspective; second, to investigate the effects of OCRs from a credibility perspective.Design/methodology/approach - The data for this study are obtained from a two-way factorial experimental research design. The factors included are the type of OCRs and the trust level of online shopping malls. In addition, PLS test is used to understand the underlying effects of trust in online shopping malls, credibility of OCRs/OEAs, and consumers' purchase intentions.Findings - The results show that OCRs are more influenced by trust in online shopping malls than OEAs. The greater the perceived credibility of OCRs among potential consumers, the higher is the purchase intention. When the trust in online shopping malls is high, consumers' purchase intentions influenced by OCRs are more favorable than those influenced by OEAs.Originality/value - This study is an initial consumer endorsement research that uses OCRs to extend the trust transfer theory and extends the interpersonal online trust perspective. For practitioners, this study is useful in determining which type of OCRs is useful for marketing, depending on the trust in online shopping malls. Moreover, the results of this study could aid in the development of an e-commerce strategy using OCRs.\"",
        "1 is \"Managing the knowledge paradox in product development\", 2 is \"Rethinking the Concept of User Involvement.\".",
        "\nGiven above information, for an author who has written the paper with the title \"Hybrid neural network models for bankruptcy predictions\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01216": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'The wireless control network: Monitoring for malicious behavior':",
        "title: \"Distributed Diagnosis Using Predetermined Synchronization Strategies In The Presence Of Communication Constraints\" with abstract: \"We consider distributed fault diagnosis in a discrete event system modeled as a nondeterministic finite automaton that is observed at multiple observation sites through distinct natural projection maps. The majority of previous work in this setting has focused on local observers (diagnosers) that are separately implemented at each observation site, i.e., these diagnosers do not attempt at any point to refine their diagnostic information by exchanging information among themselves, and typically rely exclusively on communicating their decision (fault or no fault) to a coordinator that is responsible for making the ultimate diagnosis decision (e.g., co-diagnosability). In our previous work, we extended these techniques to allow local observers/diagnosers to communicate their state estimates along with diagnostic information (normal or fault condition) to a coordinator that fuses local diagnostic information and subsequently provides this refined information to the different observation sites (to aid them in taking immediate or future diagnostic decisions). In this work, we extend these synchronization strategies to cases where a coordinator is not present and there may exist communication constraints between the exchange of information among local sites (i.e., each local diagnoser may only exchange information with a subset of the local diagnosers). We verify diagnosability in this setting via the construction of an appropriate composition of local diagnosers, which is able to capture the refinement of information under the given synchronization strategy and communication constraints.\"",
        "title: \"Verification of initial-state opacity in security applications of discrete event systems\" with abstract: \"In this paper, we formulate and analyze methodologies for verifying the notion of initial-state opacity in discrete event systems that are modeled as non-deterministic finite automata with partial observation on their transitions. A system is initial-state opaque if the membership of its true initial state to a set of secret states remains opaque (i.e., uncertain) to an intruder who observes system activity through some projection map. Initial-state opacity can be used to characterize security requirements in a variety of applications, including tracking problems in sensor networks. In order to model and analyze the intruder capabilities regarding initial-state opacity, we first address the initial-state estimation problem in a non-deterministic finite automaton via the construction of an initial-state estimator. We analyze the properties and complexity of the initial-state estimator, and show how the complexity of the verification method can be greatly reduced in the special case when the set of secret states is invariant. We also establish that the verification of initial-state opacity is a PSPACE-complete problem.\"",
        "title: \"Finite-Time Distributed Consensus In Graphs With Time-Invariant Topologies\" with abstract: \"We present a method for achieving consensus in distributed systems in a finite number of time-steps. Our scheme involves a linear iteration where, at each time-step, each node updates its value to be a weighted average of its own previous value and those of its neighbors. If D denotes the degree of the minimal polynomial of the weight matrix associated with the linear iteration, we show that each node can immediately calculate the consensus value as a linear combination of its own past values over at most D time-steps. We also show that each node can determine the coefficients for this linear combination in a decentralized manner. The proposed scheme has the potential to significantly reduce the time and communication required to reach consensus in distributed systems.\"",
        "title: \"Maximum Likelihood Failure Diagnosis in Finite State Machines Under Unreliable Observations\" with abstract: \"In this paper, we develop a probabilistic methodology for failure diagnosis in finite state machines based on a sequence of unreliable observations. Given prior knowledge of the input probability distribution but without actual knowledge of the applied input sequence, the core problem we consider is to choose from a pool of known, deterministic finite state machines (FSMs) the one that most likely matches the given sequence of observations. The problem becomes challenging because of sensor failures which may corrupt the observed sequence by inserting, deleting, and transposing symbols with certain probabilities (that are assumed known). We propose an efficient recursive algorithm for obtaining the most likely underlying FSM, given the possibly erroneous observed sequence. The proposed algorithm essentially allows us to perform online maximum likelihood failure diagnosis and is applicable to more general settings where one is required to choose the most likely underlying hidden Markov model (HMM) based on a sequence of observations that may get corrupted with known probabilities. The algorithm generalizes existing recursive algorithms for likelihood calculation in HMMs by allowing loops in the associated trellis diagram. We illustrate the proposed methodology using an example of diagnosis in the context of communication protocols.\"",
        "title: \"Sensor Selection for Structural Observability in Discrete Event Systems Modeled by Petri Nets\" with abstract: \"This paper studies optimal sensor selection in discrete event systems modeled by partially observed Petri nets. The goal is to place a minimum number of sensors while maintaining structural observability, i.e., the ability to uniquely determine the system state at any given time step based on sensor information up to that time step, knowledge of the system model, and an arbitrary but known initial state. The problem is important because the majority of existing control schemes for Petri nets rely on complete knowledge of the system state at any given time step. To simplify the problem, we consider two subproblems: the optimal place sensor selection (OPSS) problem and the optimal transition sensor selection (OTSS) problem. The OPSS problem is shown to be computationally hard by establishing that the corresponding decision problem is NP -complete. For this reason, we first reduce the problem to the linear integer programming problem, which can be solved optimally using existing linear integer programming solvers (at least for small problem instances), and then propose two heuristic algorithms to approximate its solution with polynomial complexity. Simulations suggest that the two proposed heuristics run faster and can find reasonably good solutions when compared to optimal methods that are based on linear integer programming solvers. Unlike the OPSS problem, the OTSS problem is solvable with polynomial complexity.\"",
        "1 is \"A chaotic asynchronous algorithm for computing the fixed point of a nonnegative matrix of unit spectral radius\", 2 is \"Adaptive Newton Method for Empirical Risk Minimization to Statistical Accuracy.\".",
        "\nGiven above information, for an author who has written the paper with the title \"The wireless control network: Monitoring for malicious behavior\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01217": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Joint estimation of DOA and speech based on EM beamforming':",
        "title: \"On Improving Dynamic State Space Approaches to Articulatory Inversion With MAP-Based Parameter Estimation\" with abstract: \"This paper presents a complete framework for articulatory inversion based on jump Markov linear systems (JMLS). In the model, the acoustic measurements and the position of each articulator are considered as observable measurement and continuous-valued hidden state of the system, respectively, and discrete regimes of the system are represented by the use of a discrete-valued hidden modal state. Articulatory inversion based on JMLS involves learning the model parameter set of the system and making inference about the state (position of each articulator) of the system using acoustic measurements. Iterative learning algorithms based on maximum-likelihood (ML) and maximum a posteriori (MAP) criteria are proposed to learn the model parameter set of the JMLS. It is shown that the learning procedure of the JMLS is a generalized version of hidden Markov model (HMM) training when both acoustic and articulatory data are given. In this paper, it is shown that the MAP-based learning algorithm improves modeling performance of the system and gives significantly better results compared to ML. The inference stage of the proposed algorithm is based on an interacting multiple models (IMM) approach, and done online (filtering), and/or offline (smoothing). Formulas are provided for IMM-based JMLS smoothing. It is shown that smoothing significantly improves the performance of articulatory inversion compared to filtering. Several experiments are conducted with the MOCHA database to show the performance of the proposed method. Comparison of the performance of the proposed method with the ones given in the literature shows that the proposed method improves the performance of state space approaches, making state space approaches comparable to the best published results.\"",
        "title: \"Detecting interaction links in a collaborating group using manually annotated data.\" with abstract: \"Identification of network linkages through direct observation of human interaction has long been a staple of network analysis. It is, however, time consuming and labor intensive when undertaken by human observers. This paper describes the development and validation of a two-stage methodology for automating the identification of network links from direct observation of groups in which members are free to move around a space. The initial manual annotation stage utilizes a web-based interface to support manual coding of physical location, posture, and gaze direction of group members from snapshots taken from video recordings of groups. The second stage uses the manually annotated data as input for machine learning to automate the inference of links among group members. The manual codings were treated as observed variables and the theory of turn taking in conversation was used to model temporal dependencies among interaction links, forming a Dynamic Bayesian Network (DBN). The DBN was modeled using the Bayes Net Toolkit and parameters were learned using Expectation Maximization (EM) algorithm. The Viterbi algorithm was adapted to perform the inference in DBN. The result is a time series of linkages for arbitrarily long segments that utilizes statistical distributions to estimate linkages. The validity of the method was assessed through comparing the accuracy of automatically detected links to manually identified links. Results show adequate validity and suggest routes for improvement of the method. (C) 2012 Elsevier B.V. All rights reserved.\"",
        "title: \"Sensitive Talking Heads [Applications Corner]\" with abstract: \"Spoken language user interfaces can dramatically speed up computer use. Unfortunately, if the speech user interface interferes too often, the user turns it off. Users are unforgiving: a technology that impairs productivity just once may never get a second chance. To give the user interface a fighting chance, why not endow it with a certain amount of emotional sensitivity? Users respond better to a...\"",
        "title: \"Positive-Unlabeled Learning in Streaming Networks\" with abstract: \"Data of many problems in real-world systems such as link prediction and one-class recommendation share common characteristics. First, data are in the form of positive unlabeled (PU) measurements (e.g. Twitter \\\"following\\\", Facebook \\\"like\\\", etc.) that do not provide negative information, which can be naturally represented as networks. Second, in the era of big data, such data are generated temporally-ordered, continuously and rapidly, which determines its streaming nature. These common characteristics allow us to unify many problems into a novel framework -- PU learning in streaming networks. In this paper, a principled probabilistic approach SPU is proposed to leverage the characteristics of the streaming PU inputs. In particular, SPU captures temporal dynamics and provides real-time adaptations and predictions by identifying the potential negative signals concealed in unlabeled data. Our empirical results on various real-world datasets demonstrate the effectiveness of the proposed framework over other state-of-the-art methods in both link prediction and recommendation.\"",
        "title: \"Online and offline computational reduction techniques using backward filtering in CELP speech coders\" with abstract: \"The authors review the backward filtering algorithm and give a compact proof of its validity using matrix notation. They review the relation between backward filtering and offline perceptual weighting in sparse codebook CELP and show how a combination online/offline parallel weighting algorithm can be used to reduce the search complexity of an overlapped sparse codebook by 30% to 50%\"",
        "1 is \"Multiband Modulation Energy Tracking for Noisy Speech Detection\", 2 is \"Supervised and semi-supervised suppression of background music in monaural speech recordings\".",
        "\nGiven above information, for an author who has written the paper with the title \"Joint estimation of DOA and speech based on EM beamforming\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01218": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Learning families of closed sets in matroids':",
        "title: \"Privacy Preserving Using Dummy Data For Set Operations In Itemset Mining Implemented With Zdds\" with abstract: \"We present a privacy preserving method based on inserting dummy data into original data on the data structure called Zero-suppressed BDDs (ZDDs). Our task is distributed itemset mining, which is frequent itemset mining from horizontally partitioned databases stored in distributed places called sites. We focus on the fundamental case in which there are two sites and each site has a database managed by its owner. By dividing the process of distributed itemset mining into the set union and the set intersection, we show how to make the operations secure in the sense of undistinguishability of data, which is our criterion for privacy preserving based on the already proposed criterion, p-indistinguishability. Our method conceals the original data in each operation by inserting dummy data, where ZDDs, BDD-based directed acyclic graphs, are adopted to represent sets of itemsets compactly and to implement the set operations in constructing the distributed itemset mining process. As far as we know, this is the first technique which gives a concrete representation of sets of itemsets and an implementation of set operations for privacy preserving in distributed itemset mining. Our experiments show that the proposed method provides undistinguishability of dummy data. Furthermore, we compare our method with Secure Multiparty Computation (SMC), which is one of the well-known techniques of secure computation.\"",
        "title: \"Learning Bounded Unions of Noetherian Closed Set Systems Via Characteristic Sets\" with abstract: \"In this paper, we study a learning procedure from positive data for bounded unions of certain class of languages. Our key tools are the notion of characteristic sets and hypergraphs. We generate hypergraphs from given positive data and exploit them in order to find characteristic sets.\"",
        "title: \"Inferability of closed set systems from positive data\" with abstract: \"In this paper, we generalize previous results showing connections between inductive inference from positive data and algebraic structures by using tools from universal algebra. In particular, we investigate the inferability from positive data of language classes defined by closure operators. We show that some important properties of language classes used in inductive inference correspond closely to commonly used properties of closed set systems. We also investigate the inferability of algebraic closed set systems, and show that these types of systems are inferable from positive data if and only if they contain no infinite ascending chain of closed sets. This generalizes previous results concerning the inferability of various algebraic classes such as the class of ideals of a ring. We also show the relationship with algebraic closed set systems and approximate identifiability as introduced by Kobayashi and Yokomori [11]. We propose that closure operators offer a unifying framework for various approaches to inductive inference from positive data.\"",
        "title: \"Mind change complexity of inferring unbounded unions of restricted pattern languages from positive data\" with abstract: \"This paper shows that the mind change complexity of inferring from positive data the class of unbounded unions of languages of regular patterns with constant segment length bound is of the form @w^@w^^^@a+@b, assuming that the patterns are defined over a finite alphabet containing at least two elements. Here @a and @b are natural numbers, and we give tight bounds on their values based on the length of the constant segments and the size of the alphabet of the pattern languages. This is, to the authors' knowledge, the first time a natural class of languages has been shown to be inferable with mind change complexity above @w^@w. The proof uses the notion of closure operators on a class of languages, and also uses the order type of well-partial-orderings to obtain a mind change bound. The inference algorithm presented can be easily applied to a wide range of classes of languages. Finally, we show an interesting connection between proof theory and mind change complexity.\"",
        "title: \"A Fast and Flexible Clustering Algorithm Using Binary Discretization\" with abstract: \"We present in this paper a new clustering algorithm for multivariate data. This algorithm, called BOOL (Binary coding Oriented clustering), can detect arbitrarily shaped clusters and is noise tolerant. BOOL handles data using a two-step procedure: data points are first discretized and represented as binary words, clusters are then iteratively constructed by agglomerating smaller clusters using this representation. This latter step is carried out with linear complexity by sorting such binary representations, which results in dramatic speedups when compared with other techniques. Experiments show that BOOL is faster than K-means, and about two to three orders of magnitude faster than two state-of-the-art algorithms that can detect non-convex clusters of arbitrary shapes. We also show that BOOL's results are robust to changes in parameters, whereas most algorithms for arbitrarily shaped clusters are known to be overly sensitive to such changes. The key to the robustness of BOOL is the hierarchical structure of clusters that is introduced automatically by increasing the accuracy of the discretization.\"",
        "1 is \"Parameter learning of logic programs for symbolic-statistical modeling\", 2 is \"Learning and Consistency\".",
        "\nGiven above information, for an author who has written the paper with the title \"Learning families of closed sets in matroids\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01219": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Verification of CERT Secure Coding Rules: Case Studies':",
        "title: \"Game theoretic models for detecting network intrusions\" with abstract: \"In this paper, we study using game theory the problem of detecting intrusions in wired infrastructure networks. Detection is accomplished by sampling a subset of the transmitted packets over selected network links or router interfaces. Given a total sampling budget, our framework aims at developing a network packet sampling strategy to effectively reduce the success chances of an intruder. We consider two different scenarios: (1) A well informed intruder divides his attack over multiple packets in order to increase his chances of successfully intruding a target domain. (2) Different cooperating intruders distribute the attack among themselves each send their attack fragments to the target node. Each of the packets containing a fragment of the attack is transmitted through a different path using multi-path routing, where each path is selected with a different probability. Knowing that, if these packets are independently analyzed then the intrusion will not be detected, i.e., a series of packets form an intrusion. To the best of our knowledge, there has not been any work done for the case where the attack is split over multiple packets or distributed over cooperative intruders using game theory. Non-cooperative game theory is used to formally express the problem, where the two players are: (1) the smart intruder or the cooperative intruders (depends on which scenario we are solving) and (2) the Intrusion Detection System (IDS). Our game theoretic framework will guide the intruder or the intruders to know their attack strategy and the IDS to have an optimal sampling strategy in order to detect the malicious packets.\"",
        "title: \"An Efficient and Truthful Leader IDS Election Mechanism for MANET\" with abstract: \"In this paper, we consider the problem of increasing the effectiveness of an Intrusion Detection System (IDS) for a cluster of nodes in ad hoc networks. To solve such a prob- lem, a head cluster is elected by the nodes to handle the de- tection service. Current solution elects a leader randomly without considering the energy level of nodes. Such solu- tion is vulnerable to selfish nodes that do not provide IDS service to others while at the same time benefiting from oth- ers' services. From our experiments, selfish nodes reduce the effectiveness of an IDS since less packets are inspected over time. Here, we are modeling a distributed, truthful, and efficient mechanism for electing a leader IDS that han- dles the detection process in a cluster. Our solution is able to balance the energy among all the nodes and increase the overall lifetime of an IDS in a cluster. In our model, incen- tives are given in the form of reputation to encourage the nodes to cooperate in the leader election process. The rep- utation is used to track the cooperative behavior of nodes where miss-behaving nodes are punished by withholding the cluster's services. Reputations are calculated based on the truth-telling mechanism design known as Vickrey, Clarke, and Groves (VCG). Our analysis prove that truth-telling is the dominant strategy for all the nodes and therefore effi- ciency is guaranteed. Finally, simulation results show that our mechanism improves the performance of an IDS in an- alyzing packets and punishes misbehaving nodes.\"",
        "title: \"Privacy-preserving traffic padding in web-based applications\" with abstract: \"While web-based applications are gaining popularity, they also pose new security challenges. In particular, Chen et al. recently revealed that many popular Web applications actually leak out highly sensitive data from encrypted traffic due to side-channel attacks using packet sizes and timing [1]. They further demonstrated that existing solutions usually incur a high overhead while still not guaranteeing privacy protection. In this paper, we observe a striking similarity between this issue and another well studied problem, privacy-preserving data publishing (PPDP). Based on such a similarity, we propose a formal model for privacy-preserving traffic padding (PPTP) that encompasses privacy requirement, padding cost, and padding methods.\"",
        "title: \"On the verification and validation of UML structural and behavioral diagrams\" with abstract: \"The primary intent of this paper is to present an innovative verification and validation paradigm for both software and systems engineering design models. We consider here two mainstream languages that are UML and SysML. The proposed paradigm relies mainly on formal methods providing a better assessment when used as complementary to ubiquitous techniques such as simulation. Moreover, we advocate the use of software engineering and program analysis techniques in order to augment the results of the aforementioned formal methods. To validate the proposed approach, we designed and implemented an integrated and automated environment capable of assessing software and systems engineering design models. We illustrate our methodology when applied to the assessment of class and package diagrams and the verication of work ow systems modeled using activity diagrams.\"",
        "title: \"Mining writeprints from anonymous e-mails for forensic investigation\" with abstract: \"Many criminals exploit the convenience of anonymity in the cyber world to conduct illegal activities. E-mail is the most commonly used medium for such activities. Extracting knowledge and information from e-mail text has become an important step for cybercrime investigation and evidence collection. Yet, it is one of the most challenging and time-consuming tasks due to special characteristics of e-mail dataset. In this paper, we focus on the problem of mining the writing styles from a collection of e-mails written by multiple anonymous authors. The general idea is to first cluster the anonymous e-mail by the stylometric features and then extract the writeprint, i.e., the unique writing style, from each cluster. We emphasize that the presented problem together with our proposed solution is different from the traditional problem of authorship identification, which assumes training data is available for building a classifier. Our proposed method is particularly useful in the initial stage of investigation, in which the investigator usually have very little information of the case and the true authors of suspicious e-mail collection. Experiments on a real-life dataset suggest that clustering by writing style is a promising approach for grouping e-mails written by the same author.\"",
        "1 is \"Local Filtering: Improving the Performance of Approximate Queries on String Collections\", 2 is \"IRM Enforcement of Java Stack Inspection\".",
        "\nGiven above information, for an author who has written the paper with the title \"Verification of CERT Secure Coding Rules: Case Studies\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01220": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Fast maintenance of semantic integrity assertions using redundant aggregate data':",
        "title: \"The failure and recovery problem for replicated databases\" with abstract: \"A replicated database is a distributed database in which some data items are stored redundantly at multiple sites. The main goal is to improve system reliability. By storing critical data at multiple sites, the system can operate even though some sites have failed. However, few distributed database systems support replicated data, because it is difficult to manage as sites fail and recover. A replicated data algorithm has two parts. One is a discipline for reading and writing data item copies. The other is a concurrency control algorithm for synchronizing those operations. The read-write discipline ensures that if one transaction writes logical data item \u00d7, and another transaction reads or writes x, there is some physical manifestation of that logical conflict. The concurrency control algorithm synchronizes physical conflicts; it knows nothing about logical conflicts. In a correct replicated data algorithm, the physical manifestation of conflicts must be strong enough so that synchronizing physical conflicts is sufficient for correctness. This paper presents a theory for proving the correctness of algorithms that manage replicated data. The theory is an extension of serializability theory. We apply it to three replicated data algorithms: Gifford's \u201cquorum consensus\u201d algorithm, Eager and Sevcik's \u201cmissing writes\u201d algorithm, and Computer Corporation of America's \u201cavailable copies\u201d algorithm.\"",
        "title: \"The power of inequality semijoins\" with abstract: \"Semijoin is a relational operator used in many relational query processing algorithms. Semijoins can be used to \u201creduce\u201d the database by delimitting portions of the database that contain data relevant to a given query. For some queries, there exist sequences of semijoins that delimit the exact portions of the database needed to answer the query. Such sequences are called full reducers.\"",
        "title: \"Model Management Engine for Data Integration with Reverse-Engineering Support\" with abstract: \"Model management is a high-level programming language designed to efficiently manipulate schemas and mappings. It is comprised of robust operators that combined in short programs can solve complex metadata-oriented problems in a compact way. For instance, countless enterprise data integration scenarios can be easily expressed in this high-level language thus saving hundreds of development man-hours. Here we present the first model management engine that has reverse-engineering support for data integration, which is one of the most pressing metadata-oriented problems. It merges two schemas based on the mappings between them and allows user to correct the result keeping all the mappings in sync automatically. For user it is much more convenient than determining which mappings to correct in order to get desired result. In addition, the engine supports restructuring merging which is important when the sources are structured differently and cannot be mapped directly. While making schema merging fully automatic is not yet possible, our work simplifies and automates this process to make it practical in complex data integration scenarios.\"",
        "title: \"A multi-level architecture for relational data base systems\" with abstract: \"Most of the literature on implementation of relations has been directed toward user features, with little attention paid to an overall conceptual view of underlying structures. Performance oriented considerations have been treated only for isolated problems. Toward a solution to these problems we describe a multi-level architecture for relational data base systems. This architecture distinguishes clearly between user oriented features, access path structures, data structures and file organization. It also allows efficiency problems to be isolated within levels and solved independently of each other, without impacting the logical structure of the user's virtual machine. Specific problems considered here in the context of this architecture include the mapping of relations into files, the implementation of fast access paths, and some file level optimizations that are particularly useful in relational systems.\"",
        "title: \"Merging models based on given correspondences\" with abstract: \"A model is a formal description of a complex application artifact, such as a database schema, an application interface, a UML model, an ontology, or a message format. The problem of merging such models lies at the core of many meta data applications, such as view integration, mediated schema creation for data integration, and ontology merging. This paper examines the problem of merging two models given correspondences between them. It presents requirements for conducting a merge and a specific algorithm that subsumes previous work.\"",
        "1 is \"Nondeterminism and the correctness of parallel programs\", 2 is \"Property Preserving Simulations\".",
        "\nGiven above information, for an author who has written the paper with the title \"Fast maintenance of semantic integrity assertions using redundant aggregate data\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01221": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'To catch a fake: Curbing deceptive Yelp ratings and venues':",
        "title: \"ActiVis: Visual Exploration of Industry-Scale Deep Neural Network Models.\" with abstract: \"While deep learning models have achieved state-of-the-art accuracies for many prediction tasks, understanding these models remains a challenge. Despite the recent interest in developing visual tools to help users interpret deep learning models, the complexity and wide variety of models deployed in industry, and the large-scale datasets that they used, pose unique design challenges that are inadequ...\"",
        "title: \"Mining large graphs: Algorithms, inference, and discoveries\" with abstract: \"How do we find patterns and anomalies, on graphs with billions of nodes and edges, which do not fit in memory? How to use parallelism for such terabyte-scale graphs? In this work, we focus on inference, which often corresponds, intuitively, to \u201cguilt by association\u201d scenarios. For example, if a person is a drug-abuser, probably its friends are so, too; if a node in a social network is of male gender, his dates are probably females. We show how to do inference on such huge graphs through our proposed HADOOP Line graph Fixed Point (HA-LFP), an efficient parallel algorithm for sparse billion-scale graphs, using the HADOOP platform. Our contributions include (a) the design of HA-LFP, observing that it corresponds to a fixed point on a line graph induced from the original graph; (b) scalability analysis, showing that our algorithm scales up well with the number of edges, as well as with the number of machines; and (c) experimental results on two private, as well as two of the largest publicly available graphs - the Web Graphs from Yahoo! (6.6 billion edges and 0.24 Tera bytes), and the Twitter graph (3.7 billion edges and 0.13 Tera bytes). We evaluated our algorithm using M45, one of the top 50 fastest supercomputers in the world, and we report patterns and anomalies discovered by our algorithm, which would be invisible otherwise.\"",
        "title: \"TimeStitch: Interactive multi-focus cohort discovery and comparison\" with abstract: \"Whereas event-based timelines for healthcare enable users to visualize the chronology of events surrounding events of interest, they are often not designed to aid the discovery, construction, or comparison of associated cohorts. We present TimeStitch, a system that helps health researchers discover and understand events that may cause abstinent smokers to lapse. TimeStitch extracts common sequences of events performed by abstinent smokers from large amounts of mobile health sensor data, and offers a suite of interactive and visualization techniques to enable cohort discovery, construction, and comparison, using extracted sequences as interactive elements. We are extending TimeStitch to support more complex health conditions with high mortality risk, such as reducing hospital readmission in congestive heart failure.\"",
        "title: \"Inside insider trading: patterns & discoveries from a large scale exploratory analysis\" with abstract: \"How do company insiders trade? Do their trading behaviors differ based on their roles (e.g., CEO vs. CFO)? Do those behaviors change over time (e.g., impacted by the 2008 market crash)? Can we identify insiders who have similar trading behaviors? And what does that tell us? This work presents the first academic, large-scale exploratory study of insider filings and related data, based on the complete Form 4 fillings from the U.S. Securities and Exchange Commission (SEC). We analyzed 12 million transactions by 370 thousand insiders spanning 1986 to 2012, the largest reported in academia. We explore the temporal and network-centric aspects of the trading behaviors of insiders, and make surprising and counter-intuitive discoveries. We study how the trading behaviors of insiders differ based on their roles in their companies, the transaction types, the company sectors, and their relationships with other insiders. Our work raises exciting research questions and opens up many opportunities for future studies. Most importantly, we believe our work could form the basis of novel tools for financial regulators and policymakers to detect illegal insider trading, help them understand the dynamics of the trades and enable them to adapt their detection strategies towards these dynamics.\"",
        "title: \"GLO-STIX: Graph-Level Operations for Specifying Techniques and Interactive eXploration.\" with abstract: \"The field of graph visualization has produced a wealth of visualization techniques for accomplishing a variety of analysis tasks. Therefore analysts often rely on a suite of different techniques, and visual graph analysis application builders strive to provide this breadth of techniques. To provide a holistic model for specifying network visualization techniques (as opposed to considering each tec...\"",
        "1 is \"Analyzing (social media) networks with NodeXL\", 2 is \"The shy mayor: private badges in geosocial networks\".",
        "\nGiven above information, for an author who has written the paper with the title \"To catch a fake: Curbing deceptive Yelp ratings and venues\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01222": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'FMDistance: A fast and effective distance function for motion capture data':",
        "title: \"Fast Discovery of Group Lag Correlations in Streams\" with abstract: \"The study of data streams has received considerable attention in various communities (theory, databases, data mining, networking), due to several important applications, such as network analysis, sensor monitoring, financial data analysis, and moving object tracking. Our goal in this article is to monitor multiple numerical streams and determine which pairs are correlated with lags, as well as the value of each such lag. Lag correlations and anticorrelations are frequent and very interesting in practice. For example, a decrease in interest rates typically precedes an increase in house sales by a few months; higher amounts of fluoride in drinking water may lead to fewer dental cavities some years later. Other lag settings include network analysis, sensor monitoring, financial data analysis, and tracking of moving objects. Such data streams are often correlated or anticorrelated, but with unknown lag. We propose BRAID, a method of detecting lag correlations among data streams. BRAID can handle data streams of semi-infinite length incrementally, quickly, and with small resource consumption. However, BRAID requires space and time quadratic on a number of streams k. We also propose ThinBRAID, which is even faster than BRAID, requiring O(k) space and time per time tick. Our theoretical analysis shows that BRAID/ThinBRAID can estimate lag correlations with little or, often, with no error. Our experiments on real and realistic data show that BRAID and ThinBRAID detect the correct lag perfectly most of the time (the largest relative error was about 1&percnt;), while they are significantly faster (up to 40,000 times) than the na\u00efve implementation.\"",
        "title: \"GCap: Graph-based Automatic Image Captioning\" with abstract: \"Learning the semantics of image retrieval using both text and visual information is a challenging research issue in content-based image retrieval systems. In this paper, we present a statistical natural language processing model for image retrieval, ...\"",
        "title: \"Multimedia Queries by Example and Relevance Feedback\" with abstract: \"We describe the FALCON system for handling multimedia queries with relevance feedback. FALCON distinguishes itself in its ability to handle even disjunctive queries on metric spaces. Our experiments show that it performs well on both real and synthetic data in terms of precision/recall, speed of conver- gence, individual query speed, and scalability. Moreover, it can easily take advantage of off-the-shelf spatial- and metric- access methods.\"",
        "title: \"Self-spacial join selectivity estimation using fractal concepts\" with abstract: \"The problem of selectivity estimation for queries of nontraditional databases is still an open issue. In this article, we examine the problem of selectivity estimation for some types of spatial queries in databases containing real data. We have shown earlier [Faloutsos and Kamel 1994] that real point sets typically have a nonuniform distribution, violating consistently the uniformity and independence assumptions. Moreover, we demonstrated that the theory of fractals can help to describe real point sets. In this article we show how the concept of fractal dimension, i.e., (noninteger) dimension, can lead to the solution for the selectivity estimation problem in spatial databases. Among the infinite family of fractal dimensions, we consider here the Hausdorff fractal dimension D0 and the \u201cCorrelation\u201d fractal dimension D2. Specifically, we show that (a) the average number of neighbors for a given point set follows a power law, with D2 as exponent, and (b) the average number of nonempty range queries follows a power law with E \u2212 D0 as exponent (E is the dimension of the embedding space). We present the formulas to estimate the selectivity for \u201cbiased\u201d range queries, for self-spatial joins, and for the average number of nonempty range queries. The result of some experiments on real and synthetic point sets are shown. Our formulas achieve very low relative errors, typically about 10%, versus 40%\u2013100% of the formulas that are based on the uniformity and independence assumptions.\"",
        "title: \"Spectral analysis of a blogosphere\" with abstract: \"A blogosphere is a representative example of online social networks. In this paper, we address spectral analysis of a blogosphere. We model a real-world blogosphere as a matrix and a tensor, and then analyze it by using the SVD and PARAFAC decomposition. According to the results, the SVD successfully identified communities, each of which focuses on a specific topic, and also found hub blogs and authoritative posts within each community. The PARAFAC decomposition also succeeded in extracting more communities of finer granules than the SVD. Also, the PARAFAC decomposition could identify the dominant keywords in addition to the hub blogs and authoritative posts honored in each community.\"",
        "1 is \"Dynamic terrain traversal skills using reinforcement learning\", 2 is \"Latent aspect rating analysis on review text data: a rating regression approach\".",
        "\nGiven above information, for an author who has written the paper with the title \"FMDistance: A fast and effective distance function for motion capture data\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01223": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Bridging gaps between developers and testers in globally-distributed software development':",
        "title: \"Composing Integrated Systems Using GUI-Based Applications And Web Services\" with abstract: \"Integrated systems are composed of components that exchange information (i.e., interoperate (5)). These compo- nents include Graphical User Interface (GUI) APplications (GAPs) and web services. It is difficult to make GAPs interoperate, especially if they are closed and monolithic. Unlike GAPs, web services are applications that are designed to interoperate over the Internet. We propose a novel generic approach for creating integrated systems by composing GAPs with each other and web services efficiently and non-invasively. This approach combines a nonstan- dard use of accessibility technologies for accessing and controlling GAPs in a uniform way with a visualization mechanism that enables nonprogrammers to compose integrated systems by performing point-and-click, drag-and-drop operations on GAPs and web services. We built a tool based on our approach, and using this tool we created an integrated application that controls two closed and monolithic commercial GAPs and third-party web services. Our evaluation suggests that our approach is effective, and it can be used to create nontrivial integrated systems by composing GAPs with each other and web services.\"",
        "title: \"REST: A tool for reducing effort in script-based testing.\" with abstract: \"ABSTRACT Since manual,black-box testing of GUI-based APplications (GAPs) is tedious and laborious, test engineers create test scripts to automate,the testing process. These test scripts interact with GAPs by performing actions on their GUI ob- jects. An extra effort that test engineers put in writing test scripts is paid off when,these scripts are run repeat- edly. Unfortunately, releasing new versions of GAPs breaks their corresponding test scripts thereby obliterating benefits of test automation. We propose a tool called Reducing Effort in Script-based Testing (REST) for guiding test personnel through changes in test scripts so that they can use these modified scripts to test new versions of their respective GAPs. During demon- stration of REST we will show how,this tool enables test personnel to maintain and evolve test scripts with a high degree of automation and precision. Categories and Subject Descriptors\"",
        "title: \"Automatically generating commit messages from diffs using neural machine translation.\" with abstract: \" Commit messages are a valuable resource in comprehension of software evolution, since they provide a record of changes such as feature additions and bug repairs. Unfortunately, programmers often neglect to write good commit messages. Different techniques have been proposed to help programmers by automatically writing these messages. These techniques are effective at describing what changed, but are often verbose and lack context for understanding the rationale behind a change. In contrast, humans write messages that are short and summarize the high level rationale. In this paper, we adapt Neural Machine Translation (NMT) to automatically ``translate'' diffs into commit messages. We trained an NMT algorithm using a corpus of diffs and human-written commit messages from the top 1k Github projects. We designed a filter to help ensure that we only trained the algorithm on higher-quality commit messages. Our evaluation uncovered a pattern in which the messages we generate tend to be either very high or very low quality. Therefore, we created a quality-assurance filter to detect cases in which we are unable to produce good messages, and return a warning instead. \n\n\"",
        "title: \"Sanitizing and Minimizing Databases for Software Application Test Outsourcing\" with abstract: \"Testing software applications that use nontrivial databases is increasingly outsourced to test centers in order to achieve lower cost and higher quality. Not only do different data privacy laws prevent organizations from sharing this data with test centers because databases contain sensitive information, but also this situation is aggravated by big data - it is time consuming and difficult to anonymize, distribute, and test with large databases. Deleting data randomly often leads to significantly worsened test coverages and fewer uncovered faults, thereby reducing the quality of software applications. We propose a novel approach for Protecting and mInimizing databases for Software TestIng taSks (PISTIS) that both sanitizes and minimizes a database that comes along with an application. PISTIS uses a weight-based data clustering algorithm that partitions data in the database using information obtained using program analysis that describes how this data is used by the application. For each cluster, a centroid object is computed that represents different persons or entities in the cluster, and we use associative rule mining to compute and use constraints to ensure that the centroid objects are representative of the general population of the data in the cluster. Doing so also sanitizes information, since these centroid objects replace the original data to make it difficult for attackers to infer sensitive information. Thus, we reduce a large database to a few centroid objects and we show in our experiments with two applications that test coverage stays within a close range to its original level.\"",
        "title: \"Recommending source code examples via API call usages and documentation\" with abstract: \"Online source code repositories contain software projects that already implement certain requirements that developers must fulfill. Programmers can reuse code from these existing projects if they can find relevant code without significant effort. We propose a new method to recommend source code examples to developers by querying against Application Programming Interface (API) calls and their documentations that are fused with structural information about the code. We conducted an empirical evaluation that suggests that our approach is lightweight and accurate.\"",
        "1 is \"Lazy Diagnosis of In-Production Concurrency Bugs.\", 2 is \"Avoiding deadlock avoidance\".",
        "\nGiven above information, for an author who has written the paper with the title \"Bridging gaps between developers and testers in globally-distributed software development\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01224": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Modeling class cohesion as mixtures of latent topics':",
        "title: \"Integrating conceptual and logical couplings for change impact analysis in software.\" with abstract: \"The paper presents an approach that combines conceptual and evolutionary techniques to support change impact analysis in source code. Conceptual couplings capture the extent to which domain concepts and software artifacts are related to each other. This information is derived using Information Retrieval based analysis of textual software artifacts that are found in a single version of software (e.g., comments and identifiers in a single snapshot of source code). Evolutionary couplings capture the extent to which software artifacts were co-changed. This information is derived from analyzing patterns, relationships, and relevant information of source code changes mined from multiple versions in software repositories. The premise is that such combined methods provide improvements to the accuracy of impact sets compared to the two individual approaches. A rigorous empirical assessment on the changes of the open source systems Apache httpd, ArgoUML, iBatis, KOffice, and jEdit is also reported. The impact sets are evaluated at the file and method levels of granularity for all the software systems considered in the empirical evaluation. The results show that a combination of conceptual and evolutionary techniques, across several cut-off points and periods of history, provides statistically significant improvements in accuracy over either of the two techniques used independently. Improvements in F-measure values of up to 14% (from 3% to 17%) over the conceptual technique in ArgoUML at the method granularity, and up to 21% over the evolutionary technique in iBatis (from 9% to 30%) at the file granularity were reported.\"",
        "title: \"License usage and changes: a large-scale study of Java projects on GitHub\" with abstract: \"Software licenses determine, from a legal point of view, under which conditions software can be integrated, used, and above all, redistributed. Licenses evolve over time to meet the needs of development communities and to cope with emerging legal issues and new development paradigms. Such evolution of licenses is likely to be accompanied by changes in the way how software uses such licenses, resulting in some licenses being adopted while others are abandoned. This paper reports a large empirical study aimed at quantitatively and qualitatively investigating when and why developer change software licenses. Specifically, we first identify licenses' changes in 1,731,828 commits, representing the entire history of 16,221 Java projects hosted on GitHub. Then, to understand the rationale of license changes, we perform a qualitative analysis---following a grounded theory approach---of commit notes and issue tracker discussions concerning licensing topics and, whenever possible, try to build traceability links between discussions and changes. Our results point out a lack of traceability of when and why licensing changes are made. This can be a major concern, because a change in the license of a system can negatively impact those that reuse it.\n\n\"",
        "title: \"Domain matters: bringing further evidence of the relationships among anti-patterns, application domains, and quality-related metrics in Java mobile apps.\" with abstract: \"Some previous work began studying the relationship between application domains and quality, in particular through the prevalence of code and design smells (e.g., anti-patterns). Indeed, it is generally believed that the presence of these smells degrades quality but also that their prevalence varies across domains. Though anecdotal experiences and empirical evidence gathered from developers and researchers support this belief, there is still a need to further deepen our understanding of the relationship between application domains and quality. Consequently, we present a large-scale study that investigated the systematic relationships between the presence of smells and quality-related metrics computed over the bytecode of 1,343 Java Mobile Edition applications in 13 different application domains. Although, we did not find evidence of a correlation between smells and quality- related metrics, we found (1) that larger differences exist between metric values of classes exhibiting smells and classes without smells and (2) that some smells are commonly present in all the domains while others are most prevalent in certain domains\"",
        "title: \"Optimizing energy consumption of GUIs in Android apps: a multi-objective approach\" with abstract: \"The wide diffusion of mobile devices has motivated research towards optimizing energy consumption of software systems\u2014 including apps\u2014targeting such devices. Besides efforts aimed at dealing with various kinds of energy bugs, the adoption of Organic Light-Emitting Diode (OLED) screens has motivated research towards reducing energy consumption by choosing an appropriate color palette. Whilst past research in this area aimed at optimizing energy while keeping an acceptable level of contrast, this paper proposes an approach, named GEMMA (Gui Energy Multi-objective optiMization for Android apps), for generating color palettes using a multi- objective optimization technique, which produces color solutions optimizing energy consumption and contrast while using consistent colors with respect to the original color palette. An empirical evaluation that we performed on 25 Android apps demonstrates not only significant improvements in terms of the three different objectives, but also confirmed that in most cases users still perceived the choices of colors as attractive. Finally, for several apps we interviewed the original developers, who in some cases expressed the intent to adopt the proposed choice of color palette, whereas in other cases pointed out directions for future improvements\"",
        "title: \"Automatically detecting integrity violations in database-centric applications.\" with abstract: \"Database-centric applications (DCAs) are widely used by many companies and organizations to perform various control and analytical tasks using large databases. Real-world databases are described by complex schemas that oftentimes contain hundreds of tables consisting of thousands of attributes. However, when software engineers develop DCAs, they may write code that can inadvertently violate the integrity of these databases. Alternatively, business analysts and database administrators can also make errors that lead to integrity violations (semantic bugs). To detect these violations, stakeholders must create assertions that check the validity of the data in the rows of the database tables. Unfortunately, creating assertions is a manual, laborious and error-prone task. Thus, a fundamental problem of testing DCAs is how to find such semantic bugs automatically. We propose a novel solution, namely DACITE, that enables stakeholders to automatically obtain constraints that semantically relate database attributes and code statements using a combination of static analysis of the source code and associative rule mining of the databases. We rely on SAT-solvers to validate if a solution to the combined constraints exists and issue warnings on possible semantic bugs to stakeholders. We evaluated our approach on eight open-source DCAs and our results suggest that semantic bugs can be found automatically with high precision. The results of the study with developers show that warnings produced by DACITE are useful and enable them to find semantic bugs faster.\"",
        "1 is \"Utilizing Supporting Evidence to Improve Dynamic Requirements Traceability\", 2 is \"Efficient load balancing and data remapping for adaptive grid calculations\".",
        "\nGiven above information, for an author who has written the paper with the title \"Modeling class cohesion as mixtures of latent topics\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01225": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Data association for topic intensity tracking':",
        "title: \"Tradeoffs for Space, Time, Data and Risk in Unsupervised Learning.\" with abstract: \"Faced with massive data, is it possible to trade off (statistical) risk, and (computational) space and time? This challenge lies at the heart of large-scale machine learning. Using k-means clustering as a prototypical unsupervised learning problem, we show how we can strategically summarize the data (control space) in order to trade off risk and time when data is generated by a probabilistic model. Our summarization is based on coreset constructions from computational geometry. We also develop an algorithm, TRAM, to navigate the space/time/data/risk tradeoff in practice. In particular, we show that for a fixed risk (or data size), as the data size increases (resp. risk increases) the running time of TRAM decreases. Our extensive experiments on real data sets demonstrate the existence and practical utility of such tradeoffs, not only for k-means but also for Gaussian Mixture Models.\"",
        "title: \"Learning Implicit Generative Models Using Differentiable Graph Tests.\" with abstract: \"Recently, there has been a growing interest in the problem of learning rich implicit models - those from which we can sample, but can not evaluate their density. These models apply some parametric function, such as a deep network, to a base measure, and are learned end-to-end using stochastic optimization. One strategy of devising a loss function is through the statistics of two sample tests - if we can fool a statistical test, the learned distribution should be a good model of the true data. However, not all tests can easily fit into this framework, as they might not be differentiable with respect to the data points, and hence with respect to the parameters of the implicit model. Motivated by this problem, in this paper we show how two such classical tests, the Friedman-Rafsky and k-nearest neighbour tests, can be effectively smoothed using ideas from undirected graphical models - the matrix tree theorem and cardinality potentials. Moreover, as we show experimentally, smoothing can significantly increase the power of the test, which might of of independent interest. Finally, we apply our method to learn implicit models.\"",
        "title: \"ODIN: ODE-Informed Regression for Parameter and State Inference in Time-Continuous Dynamical Systems.\" with abstract: \"Parameter inference in ordinary differential equations is an important problem in many applied sciences and in engineering, especially in a data-scarce setting. In this work, we introduce a novel generative modeling approach based on constrained Gaussian processes and use it to create a computationally and data efficient algorithm for state and parameter inference. In an extensive set of experiments, our approach outperforms its competitors both in terms of accuracy and computational cost for parameter inference. It also shows promising results for the much more challenging problem of model selection.\"",
        "title: \"A utility-theoretic approach to privacy in online services\" with abstract: \"Online offerings such as web search, news portals, and e-commerce applications face the challenge of providing high-quality service to a large, heterogeneous user base. Recent efforts have highlighted the potential to improve performance by introducing methods to personalize services based on special knowledge about users and their context. For example, a user's demographics, location, and past search and browsing may be useful in enhancing the results offered in response to web search queries. However, reasonable concerns about privacy by both users, providers, and government agencies acting on behalf of citizens, may limit access by services to such information. We introduce and explore an economics of privacy in personalization, where people can opt to share personal information, in a standing or on-demand manner, in return for expected enhancements in the quality of an online service. We focus on the example of web search and formulate realistic objective functions for search efficacy and privacy. We demonstrate how we can find a provably near-optimal optimization of the utility-privacy tradeoff in an efficient manner. We evaluate our methodology on data drawn from a log of the search activity of volunteer participants. We separately assess users preferences about privacy and utility via a large-scale survey, aimed at eliciting preferences about peoples willingness to trade the sharing of personal data in returns for gains in search efficiency. We show that a significant level of personalization can be achieved using a relatively small amount of information about users.\"",
        "title: \"Learning User Preferences to Incentivize Exploration in the Sharing Economy\" with abstract: \"We study platforms in the sharing economy and discuss the need for incentivizing users to explore options that otherwise would not be chosen. For instance, rental platforms such as Airbnb typically rely on customer reviews to provide users with relevant information about different options. Yet, often a large fraction of options does not have any reviews available. Such options are frequently neglected as viable choices, and in turn are unlikely to be evaluated, creating a vicious cycle. Platforms can engage users to deviate from their preferred choice by offering monetary incentives for choosing a different option instead. To efficiently learn the optimal incentives to offer, we consider structural information in user preferences and introduce a novel algorithm - Coordinated Online Learning (CoOL) - for learning with structural information modeled as convex constraints. We provide formal guarantees on the performance of our algorithm and test the viability of our approach in a user study with data of apartments on Airbnb. Our findings suggest that our approach is well-suited to learn appropriate incentives and increase exploration on the investigated platform.\"",
        "1 is \"Towards multiple identity detection in social networks\", 2 is \"Reinforcement learning in robotics: A survey\".",
        "\nGiven above information, for an author who has written the paper with the title \"Data association for topic intensity tracking\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01226": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Fourier Theoretic Probabilistic Inference over Permutations':",
        "title: \"Distributed planning in hierarchical factored MDPs\" with abstract: \"We present a principled and efficient planning algorithm for collaborative multiagent dynamical systems. All computation, during both the planning and the execution phases, is distributed among the agents; each agent only needs to model and plan for a small part of the system. Each of these local subsystems is small, but once they are combined they can represent an exponentially larger problem. The subsystems are connected through a subsystem hierarchy. Coordination and communication between the agents is not imposed, but derived directly from the structure of this hierarchy. A globally consistent plan is achieved by a message passing algorithm, where messages correspond to natural local reward functions and are computed by local linear programs; another message passing algorithm allows us to execute the resulting policy. When two portions of the hierarchy share the same structure, our algorithm can reuse plans and messages to speed up computation.\"",
        "title: \"Resource-Aware Wireless Sensor-Actuator Networks\" with abstract: \"Innovations in wireless sensor networks (WSNs) have dramatically expanded the applicability of control technol- ogy in day-to-day life, by enabling the cost-effective deployment of large scale sensor-actuator systems. In this paper, we discuss the issues and challenges involved in deploying control-oriented applications over unreliable, resource-constrainedWSNs, and describe the design of our planned Sensor Control System (SCS) that can enable the rapid development and deployment of such applications.\"",
        "title: \"Robust sensor placements at informative and communication-efficient locations\" with abstract: \"When monitoring spatial phenomena with wireless sensor networks, selecting the best sensor placements is a fundamental task. Not only should the sensors be informative, but they should also be able to communicate efficiently. In this article, we present a data-driven approach that addresses the three central aspects of this problem: measuring the predictive quality of a set of sensor locations (regardless of whether sensors were ever placed at these locations), predicting the communication cost involved with these placements, and designing an algorithm with provable quality guarantees that optimizes the NP-hard trade-off. Specifically, we use data from a pilot deployment to build nonparametric probabilistic models called Gaussian Processes (GPs) both for the spatial phenomena of interest and for the spatial variability of link qualities, which allows us to estimate predictive power and communication cost of unsensed locations. Surprisingly, uncertainty in the representation of link qualities plays an important role in estimating communication costs. Using these models, we present a novel, polynomial-time, data-driven algorithm, PSPIEL, which selects Sensor Placements at Informative and communication-Efficient Locations. Our approach exploits two important properties of this problem: submodularity, formalizing the intuition that adding a node to a small deployment can help more than adding a node to a large deployment; and locality, under which nodes that are far from each other provide almost independent information. Exploiting these properties, we prove strong approximation guarantees for our PSPIEL approach. In addition, we show how our placements can be made robust against changes in the environment, and how PSPIEL can be used to plan informative paths for information gathering using mobile robots. We also provide extensive experimental validation of this practical approach on several real-world placement problems, and built a complete system implementation on 46 Tmote Sky motes, demonstrating significant advantages over existing methods.\"",
        "title: \"Near-optimal sensor placements in Gaussian processes\" with abstract: \"When monitoring spatial phenomena, which are often modeled as Gaussian Processes (GPs), choosing sensor locations is a fundamental task. A common strategy is to place sensors at the points of highest entropy (variance) in the GP model. We propose a mutual information criteria, and show that it produces better placements. Furthermore, we prove that finding the configuration that maximizes mutual information is NP-complete. To address this issue, we describe a polynomial-time approximation that is within (1 -- 1/e) of the optimum by exploiting the submodularity of our criterion. This algorithm is extended to handle local structure in the GP, yielding significant speedups. We demonstrate the advantages of our approach on two real-world data sets.\"",
        "title: \"Simultaneous Optimization of Sensor Placements and Balanced Schedules.\" with abstract: \"We consider the problem of monitoring spatial phenomena, such as road speeds on a highway, using wireless sensors with limited battery life. A central question is to decide where to locate these sensors to best predict the phenomenon at the unsensed locations. However, given the power constraints, we also need to determine when to activate these sensors in order to maximize the performance while satisfying lifetime requirements. Traditionally, these two problems of sensor placement and scheduling have been considered separately; one first decides where to place the sensors, and then when to activate them.\"",
        "1 is \"Symmetry-Aware Nonrigid Matching of Incomplete 3D Surfaces\", 2 is \"Tracing the event evolution of terror attacks from on-line news\".",
        "\nGiven above information, for an author who has written the paper with the title \"Fourier Theoretic Probabilistic Inference over Permutations\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01227": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'End-to-End Routing for Dual-Radio Sensor Networks':",
        "title: \"A wireless biomedical handheld instrument for evidence-based detection of pressure ulcers\" with abstract: \"Pressure Ulcer (PU) incidence leads to considerable risk, in particular for the frail elderly, and a large national healthcare treatment cost. Evidence-based methods for assuring the health and safety of patients are urgently needed. Recent clinical trials have demonstrated that sub-epidermal moisture (SEM) present in tissue may be measured by interrogation of tissue dielectric properties and are associated with the presence of erythema and development of early stage PU conditions. A novel wireless handheld device has been developed and will be demonstrated that introduces a series of advances including automated measurement, automated measurement method assurance including application of proper measurement applied pressure, and wireless energy recharge capability. This advances previous successful prototype development to now include a complete point-of-care usage product. This device, termed the SEM Scanner, was successfully verified in trials with 30 subjects and is currently deployed in large clinical trials in nursing homes in Los Angeles. This manuscript and the planned demonstration describe a set of significant technology advances over previous technology based on both new Wireless Health hardware and software system solutions. In addition to the demonstration to be provided of a novel end-to-end system including data acquisition, data archiving, reporting, and compliance verification, data from trials will also be presented.\"",
        "title: \"Environmental Samplingwith Multiscale Sensing\" with abstract: \"Environment reconstruction through sampling is a difficult task and usually requires a large amount of resources. In this paper, a sampling technique is presented that approaches exhaustive sampling performance with only sparse samples. The goal is achieved by combining information from sensors of different types and resolutions. Image processing techniques are employed to extract global information. This information is passed on to the local sensors to optimize the number and locations of low-level sampling points. The sampled values are then applied back to the image to reconstruct the whole field. The technique is tested in the lab setup and shown to achieve a better result than traditional sampling methods\"",
        "title: \"Efficient informative sensing using multiple robots\" with abstract: \"The need for efficient monitoring of spatio-temporal dynamics in large environmental applications, such as the water quality monitoring in rivers and lakes, motivates the use of robotic sensors in order to achieve sufficient spatial coverage. Typically, these robots have bounded resources, such as limited battery or limited amounts of time to obtain measurements. Thus, careful coordination of their paths is required in order to maximize the amount of information collected, while respecting the resource constraints. In this paper, we present an efficient approach for near-optimally solving the NP-hard optimization problem of planning such informative paths. In particular, we first develop eSIP (efficient Single-robot Informative Path planning), an approximation algorithm for optimizing the path of a single robot. Hereby, we use a Gaussian Process to model the underlying phenomenon, and use the mutual information between the visited locations and remainder of the space to quantify the amount of information collected. We prove that the mutual information collected using paths obtained by using eSIP is close to the information obtained by an optimal solution. We then provide a general technique, sequential allocation, which can be used to extend any single robot planning algorithm, such as eSIP, for the multi-robot problem. This procedure approximately generalizes any guarantees for the single-robot problem to the multi-robot case. We extensively evaluate the effectiveness of our approach on several experiments performed infield for two important environmental sensing applications, lake and river monitoring, and simulation experiments performed using several real world sensor network data sets.\"",
        "title: \"The SmartCane system: an assistive device for geriatrics\" with abstract: \"Falls are currently a leading cause of death from injury in the elderly. The usage of the conventional assistive cane devices is critical in reducing the risk of falls and is relied upon by over 4 million patients in the U.S.. While canes provide physical support as well as supplementary sensing feedback to patients, at the same time, these conventional aids also exhibit serious adverse effects that contribute to falls. The falls due to the improper usage of the canes are particularly acute in the elderly and disabled where reduced cognitive capacity accompanied by the burden of managing cane motion leads to increased risk. This paper describes the development of the SmartCane assistive system that encompasses broad engineering challenges that will impact general development of individualized, robust assistive and prosthetic devices. The SmartCane system combines advances in signal processing, embedded computing, and wireless networking technology to provide capabilities for remote monitoring, local signal processing, and real-time feedback on the cane usage. This system aims to reduce risks of injuries and falls by enabling training and guidance of patients in proper usage of assistive devices.\"",
        "title: \"Networked infomechanical systems: a mobile embedded networked sensor platform\" with abstract: \"Networked Infomechanical Systems (NIMS) introduces a new actuation capability for embedded networked sensing. By exploiting a constrained actuation method based on rapidly deployable infrastructure, NIMS suspends a network of wireless mobile and fixed sensor nodes in three-dimensional space. This permits run-time adaptation with variable sensing location, perspective, and even sensor type. Discoveries in NIMS environmental investigations have raised requirements for 1) new embedded platforms integrating many diverse sensors with actuators, and 2) advances for in-network sensor data processing. This is addressed with a new and generally applicable processor-preprocessor architecture described in this paper. Also this paper describes the successful integration of R, a powerful statistical computing environment, into the embedded NIMS node platform.\"",
        "1 is \"Defining high-speed protocols: five challenges and an example that survives the challenges\", 2 is \"Taming the underlying challenges of reliable multihop routing in sensor networks\".",
        "\nGiven above information, for an author who has written the paper with the title \"End-to-End Routing for Dual-Radio Sensor Networks\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01228": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'The Semantics Of Similarity In Geographic Information Retrieval':",
        "title: \"Towards sustainable mobility behavior: research challenges for location-aware information and communication technology.\" with abstract: \"Private transport accounts for a large amount of total CO emissions, thus significantly contributing to global warming. Tools that actively support people in engaging in a more sustainable life-style without restricting their mobility are urgently needed. How can location-aware information and communication technology (ICT) enable novel interactive and participatory approaches that help people in becoming more sustainable? In this survey paper, we discuss the different aspects of this challenge from a technological and cognitive engineering perspective, based on an overview of the main information processes that may influence mobility behavior. We review the state-of-the-art of research with respect to various ways of influencing mobility behavior (e.g., through providing real-time, user-specific, and location-based feedback) and suggest a corresponding research agenda. We conclude that future research has to focus on reflecting individual goals in providing personal feedback and recommendations that take into account different motivational stages. In addition, a long-term and large-scale empirical evaluation of such tools is necessary.\"",
        "title: \"A case for space: physical and virtual location requirements in the CouchSurfing social network\" with abstract: \"This paper describes a Location Based Social Network (LBSN) built upon activities that combine virtual and physical location. While many modern social networks are based in the virtual world and strengthen pre-existing connections, the CouchSurfing social network is built upon creating new face-to-face connections between members across the world. The network has connected travelers to cost-free lodging for over 5 years with over 1 million current members. Now it provides a large user database where each user is tagged with a location. This is useful for spatial data mining and knowledge discovery as recommendations about locations are left in user reviews of one another. These are drawn upon to find interesting locations and discover new places, people and activities. Techniques from the field of time geography are used with LBSN information about individual member location to show how spatiotemporal constraints combine the virtual and physical worlds. Additionally, mobile devices afford flexible utility for the LBSN and applications are presented that take advantage of this.\"",
        "title: \"Towards healthier urban mobility\" with abstract: \"As a consequence of the increased dissemination of wireless and location-aware mobile devices, self-monitoring and crowd-sensing has become increasingly popular in recent years. In parallel, discussions about sustainability, air pollution and greenhouse gas emissions have also been intensified. We propose combining ideas of self-monitoring, crowd-sensing and persuasion towards a real-time city atlas which induces urban dwellers to integrate higher levels of physical activities into their daily mobility needs and to guide their mobility behavior towards a higher degree of sustainability as well as a lower exposure to polluted air.\"",
        "title: \"Measuring similarity of mobile phone user trajectories\u2013 a Spatio-temporal Edit Distance method\" with abstract: \"The rapid development of information and communication technologies ICTs has provided rich data sources for analyzing, modeling, and interpreting human mobility patterns. This paper contributes to this research area by developing the Spatio-temporal Edit Distance measure, an extended algorithm to determine the similarity between user trajectories based on call detailed records CDRs. We improve the traditional Edit Distance algorithm by incorporating both spatial and temporal information into the cost functions. The extended algorithm can preserve both space and time information from string-formatted CDR data. The novel method is applied to a large data set from Northeast China in order to test its effectiveness. Three types of analyses are presented for scenarios with and without the effect of time: 1 Edit Distance with spatial information; 2 Edit Distance with time as a factor in the cost function; and 3 Edit Distance with time as a constraint in partitioning trajectories. The outcomes of this research contribute to both methodological and empirical perspectives. The extended algorithm performs well for measuring low-resolution tracking information in CDRs, as well as facilitating the interpretation of user mobility patterns in the age of instant access.\"",
        "title: \"Personalized Multi-Criteria Decision Strategies in Location-Based Decision Support\" with abstract: \"Location-based services (LBS) assist people in decision-making during the performance of tasks in space and time. Current LBS support spatial and attribute queries, such as finding the nearest Italian restaurant from the current location of the user, but they are limited in their capacity to evaluate decision alternatives and to consider individual decision-makers' user preferences. We suggest that LBS should provide personalized spatial decision support to their users. In a prototype implementation, we demonstrate how user preferences can be translated into parameters of a multi- criteria evaluation method. In particular, the Ordered Weighted Averaging (OWA) operator allows users to specify a personal decision strategy. A traveler scenario investigating the influence of different types of users and different decision strategies on the outcome of the analysis serves as a case study.\"",
        "1 is \"When owl: sameAs isn't the same: an analysis of identity in linked data\", 2 is \"The Role of Cognitive Science in Knowledge Engineering\".",
        "\nGiven above information, for an author who has written the paper with the title \"The Semantics Of Similarity In Geographic Information Retrieval\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01229": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Feasibility study on iPhone accelerometer for gait detection':",
        "title: \"Discovering Rules For Fault Management\" with abstract: \"Ar the heart of the Internet revolution is global telecommunication systems. These systems, initially designed for voice traffic, provide the vast backbone bandwidth capabilities necessary for Internet traffic. They have built-in redundancy and complexity to ensure robustness and quality of service. To facilitate this, this requires complex fault identification and management systems. Fault identification and management is generally handled by reducing the amount of alarm events (symptoms) presented to the operating engineer through monitoring, filtering and masking. The ultimate goal is to determine and present the actual underlying fault. While en-route to automated fault identification it is useful to derive rules and techniques to attempt to present less symptoms with greater diagnostic assistance. With these objectives in mind computer-assisted human discovery and human-assisted computer discovery techniques are discussed.\"",
        "title: \"An Investigation into the Viability of a Mobile Ultrasonic Array as a Sensor Substitute in an Autonomic Intelligent Environment\" with abstract: \"In this paper an approach to ensuring fault tolerance in intelligent environments for the elderly through the provision of mobile sensor substitution in the event of the detection of anomalous static sensor behavior, is presented. Specifically this paper focuses on the monitoring of an external door in an intelligent care home environment. A mobile robot equipped with an array of ultrasonic sensors is dispatched to monitor the door state and report a change in state to a central server. For each door state there are consistent changes in the sensor readings identified in the course of the experiments carried out within this work. The use of ultrasonic sensors provides a viable substitution option that can assist a central system in deciding whether a care assistant or maintenance engineer is required to resolve the anomalous static sensor behavior.\"",
        "title: \"Autonomic Computing \" Panacea or Poppycock?\" with abstract: \"Autonomic Computing arose out of a need for a means to cope with rapidly growing complexity of integrating, managing, and operating computer-based systems as well as a need to reduce the total cost of ownership of today\u00fds systems. Autonomic Computing (AC) as a discipline was proposed by IBM in 2001, with the vision to develop self-managing systems [1]. As the name implies, the influence for the new paradigm is the human body's autonomic system, which regulates vital bodily functions such as the control of heart rate, the body\u00fds temperature and blood flow \u9a74 all without conscious effort. The vision is to create selfware through self-* properties. The initial set of properties, in terms of objectives, were self-configuring, self-healing, selfoptimizing and self-protecting, along with attributes of self-awareness, self-monitoring and self-adjusting. This self-* list has grown: self-anticipating, self-critical, selfdefining, self-destructing, self-diagnosis, self-governing, self-organized, self-reflecting, and self-simulation, for instance [2][3].\"",
        "title: \"Dynamic sensor data segmentation for real-time knowledge-driven activity recognition.\" with abstract: \"Approaches and algorithms for activity recognition have recently made substantial progress due to advancements in pervasive and mobile computing, smart environments and ambient assisted living. Nevertheless, it is still difficult to achieve real-time continuous activity recognition as sensor data segmentation remains a challenge. This paper presents a novel approach to real-time sensor data segmentation for continuous activity recognition. Central to the approach is a dynamic segmentation model, based on the notion of varied time windows, which can shrink and expand the segmentation window size by using temporal information of sensor data and activities as well as the state of activity recognition. The paper first analyzes the characteristics of activities of daily living from which the segmentation model that is applicable to a wide range of activity recognition scenarios is motivated and developed. It then describes the working mechanism and relevant algorithms of the model in the context of knowledge-driven activity recognition based on ontologies. The presented approach has been implemented in a prototype system and evaluated in a number of experiments. Results have shown average recognition accuracy above 83% in all experiments for real time activity recognition, which proves the approach and the underlying model.\"",
        "title: \"Handling Uncertainty in a Medical Study of Dietary Intake During Pregnancy\" with abstract: \"This paper is concerned with handling uncertainty as part of the analysis of data from a medical study. The study is investigating connections between the birth weight of babies and the dietary intake of their mothers. Bayesian belief networks were used in the analysis. Their perceived benefits include (i) an ability to represent the evidence emerging from the evolving study, dealing effectively with the inherent uncertainty involved; (ii) providing a way of representing evidence graphically to facilitate analysis and communication with clinicians; (iii) helping in the exploration of the data to reveal undiscovered knowledge; and (iv) providing a means of developing an expert system application.\"",
        "1 is \"Practical Reasoning for Very Expressive Description Logics\", 2 is \"Reconstructing nonlinear dynamic models of gene regulation using stochastic sampling.\".",
        "\nGiven above information, for an author who has written the paper with the title \"Feasibility study on iPhone accelerometer for gait detection\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01230": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Orthogonally-Constrained Extraction of Independent Non-Gaussian Component from Non-Gaussian Background Without ICA.':",
        "title: \"Tensor Networks for Latent Variable Analysis. Part I: Algorithms for Tensor Train Decomposition.\" with abstract: \"Decompositions of tensors into factor matrices, which interact through a core tensor, have found numerous applications in signal processing and machine learning. A more general tensor model which represents data as an ordered network of sub-tensors of order-2 or order-3 has, so far, not been widely considered in these fields, although this so-called tensor network decomposition has been long studied in quantum physics and scientific computing. In this study, we present novel algorithms and applications of tensor network decompositions, with a particular focus on the tensor train decomposition and its variants. The novel algorithms developed for the tensor train decomposition update, in an alternating way, one or several core tensors at each iteration, and exhibit enhanced mathematical tractability and scalability to exceedingly large-scale data tensors. The proposed algorithms are tested in classic paradigms of blind source separation from a single mixture, denoising, and feature extraction, and achieve superior performance over the widely used truncated algorithms for tensor train decomposition.\"",
        "title: \"On Fast Computation of Gradients for CANDECOMP/PARAFAC Algorithms\" with abstract: \"  Product between mode-$n$ unfolding $\\bY_{(n)}$ of an $N$-D tensor $\\tY$ and Khatri-Rao products of $(N-1)$ factor matrices $\\bA^{(m)}$, $m = 1,..., n-1, n+1, ..., N$ exists in algorithms for CANDECOMP/PARAFAC (CP). If $\\tY$ is an error tensor of a tensor approximation, this product is the gradient of a cost function with respect to factors, and has the largest workload in most CP algorithms. In this paper, a fast method to compute this product is proposed. Experimental verification shows that the fast CP gradient can accelerate the CP_ALS algorithm 2 times and 8 times faster for factorizations of 3-D and 4-D tensors, and the speed-up ratios can be 20-30 times for higher dimensional tensors. \"",
        "title: \"CANONICAL POLYADIC TENSOR DECOMPOSITION WITH LOW-RANK FACTOR MATRICES\" with abstract: \"This paper proposes a constrained canonical polyadic (CP) tensor decomposition method with low-rank factor matrices. In this way, we allow the CP decomposition with high rank while keeping the number of the model parameters small. First, we propose an algorithm to decompose the tensors into factor matrices of given ranks. Second, we propose an algorithm which can determine the ranks of the factor matrices automatically, such that the fitting error is bounded by a user-selected constant. The algorithms are verified on the decomposition of a tensor of the MNIST hand-written image dataset.\"",
        "title: \"Partitioned Alternating Least Squares Technique for Canonical Polyadic Tensor Decomposition.\" with abstract: \"Canonical polyadic decomposition (CPD), also known as parallel factor analysis, is a representation of a given tensor as a sum of rank-one components. Traditional method for accomplishing CPD is the alternating least squares (ALS) algorithm. Convergence of ALS is known to be slow, especially when some factor matrices of the tensor contain nearly collinear columns. We propose a novel variant of thi...\"",
        "title: \"Rank Splitting for CANDECOMP/PARAFAC\" with abstract: \"CANDECOMP/PARAFAC CP approximates multiway data by a sum of rank-1 tensors. Our recent study has presented a method to rank-1 tensor deflation, i.e. sequential extraction of rank-1 tensor components. In this paper, we extend the method to block deflation problem. When at least two factor matrices have full column rank, one can extract two rank-1 tensors simultaneously, and rank of the data tensor is reduced by 2. For decomposition of order-3 tensors of size $$R \\\\times R \\\\times R$$R\u00d7R\u00d7R and rank-R, the block deflation has a complexity of $${\\\\mathcal {O}}R^3$$OR3 per iteration which is lower than the cost $${\\\\mathcal {O}}R^4$$OR4 of the ALS algorithm for the overall CP decomposition.\"",
        "1 is \"Inference of Room Geometry From Acoustic Impulse Responses\", 2 is \"Multiple Fundamental Frequency Estimation and Polyphony Inference of Polyphonic Music Signals\".",
        "\nGiven above information, for an author who has written the paper with the title \"Orthogonally-Constrained Extraction of Independent Non-Gaussian Component from Non-Gaussian Background Without ICA.\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01231": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Extending social networks with implicit human-human interaction':",
        "title: \"MuiCSer: A Process Framework for Multi-disciplinary User-Centred Software Engineering Processes\" with abstract: \"In this paper we introduce MuiCSer, a conceptual process framework for Multi-disciplinary User-centred Software Engineering (UCSE) processes. UCSE processes strive for the combination of basic principles and practices from software engineering and user-centred design approaches in order to increase the overall user experience with the resulting product. The MuiCSer framework aims to provide a common understanding of important components and associated activities of UCSE processes. As such, the conceptual framework acts as a frame of reference for future research regarding various aspects and concepts related to this kind of processes, including models, development artefacts and tools. We present the MuiCSer process framework and illustrate its instantiation in customized processes for the (re)design of a system. The conceptual framework has been helpful to investigate the role of members of a multi-disciplinary team when realizing artefacts in a model-based approach. In particular process coverage of existing artefact transformation tools has been studied.\"",
        "title: \"Jelly: a multi-device design environment for managing consistency across devices\" with abstract: \"When creating applications that should be available on multiple computing platforms, designers have to cope with different design tools and user interface toolkits. Incompatibilities between these design tools and toolkits make it hard to keep multi-device user interfaces consistent. This paper presents Jelly, a flexible design environment that can target a broad set of computing devices and toolkits. Jelly enables designers to copy parts of a user interface from one device to another and to maintain the different user interfaces in concert using linked editing. Our approach lowers the burden of designing multi-device user interfaces by eliminating the need to switch between different design tools and by providing tool support for keeping the user interfaces consistent across different platforms and toolkits.\"",
        "title: \"Exploring the design space for situated glyphs to support dynamic work environments\" with abstract: \"This note offers a reflection on the design space for a situated glyph - a single, adaptive and multivariate graphical unit that provides in-situ task information in demanding work environments. Rather than presenting a concrete solution, our objective is to map out the broad design space to foster further exploration. The analysis of this design space in the context of dynamic work environments covers i) information affinity - the type of information can be presented with situated glyphs, ii) representation density - the medium and fidelity of information presentation, iii) spatial distribution - distribution granularity and placement alternatives for situated glyphs, and finally iv) temporal distribution - the timing of information provision through glyphs. Our analysis has uncovered new problem spaces that are still unexplored and could motivate further work in the field.\"",
        "title: \"Untangling Design Meetings: Artefacts as Input and Output of Design Activities.\" with abstract: \"Design meetings with multidisciplinary stakeholders are instrumental for design projects. However, design teams face the challenges of synthetizing large amounts of information, often in a limited time, and with minimal common ground. We investigate these challenges through in-the-wild observations of six design meetings in three different projects, with professional design teams that follow a user-centered design methodology. We found that all the observed design meetings had a similar structure consisting of particular phases, in which design activities were organized around artefacts. These artefacts were used as input to disseminate and gather feedback of previous design outcomes, or as output to collect and process a variety of perspectives. From these findings, we synthetize practical guidelines to optimize artefact-based interactions during design meetings.\"",
        "title: \"Runtime transformations for modal independent user interface migration\" with abstract: \"The usage of computing systems has evolved dramatically over the last years. Starting from a low level procedural usage, in which a process for executing one or several tasks is carried out, computers now tend to be used in a problem oriented way. Future computer usage will be more centered around particular services, and will not be focused on platforms or applications. These services should be i...\"",
        "1 is \"Importance of Communication Influences on a Highly Collaborative Task\", 2 is \"A Comparative Study of Data Dissemination Models for VANETs\".",
        "\nGiven above information, for an author who has written the paper with the title \"Extending social networks with implicit human-human interaction\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01232": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Towards an Extensible Context Ontology for Ambient Intelligence':",
        "title: \"Contextual ConcurTaskTrees: Integrating dynamic contexts in task based design\" with abstract: \"Much research has been done on how context can begathered and how the task of harvesting the context informationcan be made as easy as possible for applicationdevelopers. However, less research has been done on howto design a system that adapts to the context while beingactive. This paper presents Contextual ConcurTaskTrees, anotation that combines the specification of dynamic contextand tasks. The effect of the integration of the new notationon a current task-based approach is demonstrated and discussed.Special attention is paid to how the context is integratedand which parts of the context can be specified usingthe presented notation.\"",
        "title: \"A user study for comparing the programming efficiency of modifying executable multimodal interaction descriptions: a domain-specific language versus equivalent event-callback code.\" with abstract: \"The present paper describes an empirical user study intended to compare the programming efficiency of our proposed domain-specific language versus a mainstream event language when it comes to modify multimodal interactions. By concerted use of observations, interviews, and standardized questionnaires, we managed to measure the completion rates, completion time, code testing effort, and perceived difficulty of the programming tasks along with the perceived usability and perceived learnability of the tool supporting our proposed language. Based on this experience, we propose some guidelines for designing comparative user studies of programming languages. The paper also discusses the considerations we took into account when designing a multimodal interaction description language that intends to be well regarded by its users.\"",
        "title: \"Graphical Toolkits for Rapid Prototyping of Multimodal Systems: A Survey.\" with abstract: \"The creation of prototypes and their iterative adaptation are important stages in the time-consuming development process of a multimodal system. This reality has brought about the appearance of many specialized toolkits intended for facilitating the prototyping of multimodal systems. Using these toolkits, some functionalities of the intended prototype can be specified by means of a visual language...\"",
        "title: \"Cassis: A Modeling Language For Customizable User Interface Designs\" with abstract: \"Current user interface modeling languages usually focus on modeling a single user interface and have a fixed set of user interface components; adding another user interface component requires an extension of the language.In this paper we present CASSIS, a concise language that supports creation of user interface components using models instead of language extensions. It also allows the specification of design-time and runtime user interface variations. The support for variations has been used to generate constraints for custom user interface components, to specify design patterns and design decisions. CASSIS has been used in several projects including a multi-disciplinary applied research project.\"",
        "title: \"CAP3 for interaction design pattern diagrams?\" with abstract: \"Following the success of the \"Gang of four\" design patterns for software engineering, the human-computer interaction community has used design patterns to document interaction design knowledge. PLML is a structured format to describe these patterns in XML. At least one actively used pattern library for interaction design adopted it to structure its knowledge. One feature that is defined in PLML and that is also an important part of the \"Gang of four\" patterns for software engineering is the diagram. In this paper, we propose to use CAP3, a graphical abstract user interface modeling language, to specify the diagram part of interaction design patterns. We show some examples of its usage and discuss benefits and drawbacks.\"",
        "1 is \"Sitemaps, storyboards, and specifications: a sketch of Web site design practice\", 2 is \"A theory of secure control flow\".",
        "\nGiven above information, for an author who has written the paper with the title \"Towards an Extensible Context Ontology for Ambient Intelligence\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01233": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Examining Social Dynamics for Countering Botnet Attacks':",
        "title: \"Secure sharing of electronic health records in clouds\" with abstract: \"In modern healthcare environments, healthcare providers are more willing to shift their electronic medical record systems to clouds. Instead of building and maintaining dedicated data centers, this paradigm enables to achieve lower operational cost and better interoperability with other healthcare providers. However, the adoption of cloud computing in healthcare systems may also raise many security challenges associated with authentication, identity management, access control, trust management, and so on. In this paper, we focus on access control issues in electronic medical record systems in clouds. We propose a systematic access control mechanism to support selective sharing of composite electronic health records (EHRs) aggregated from various healthcare providers in clouds. Our approach ensures that privacy concerns are accommodated for processing access requests to patients' healthcare information.We also demonstrate the feasibility and efficiency of our approach by implementing a proof-of-concept prototype along with evaluation results.\"",
        "title: \"Dynamic Audit Services for Outsourced Storages in Clouds\" with abstract: \"In this paper, we propose a dynamic audit service for verifying the integrity of an untrusted and outsourced storage. Our audit service is constructed based on the techniques, fragment structure, random sampling, and index-hash table, supporting provable updates to outsourced data and timely anomaly detection. In addition, we propose a method based on probabilistic query and periodic verification for improving the performance of audit services. Our experimental results not only validate the effectiveness of our approaches, but also show our audit system verifies the integrity with lower computation overhead and requiring less extra storage for audit metadata.\"",
        "title: \"Towards HIPAA-compliant healthcare systems\" with abstract: \"In healthcare domain, there is a gap between healthcare systems and government regulations such as the Health Insurance Portability and Accountability Act (HIPAA). The violations of HIPAA not only may cause the disclosure of patients' sensitive information, but also can bring about tremendous economic loss and reputation damage to healthcare providers. Taking effective measures to address this gap has become a critical requirement for all healthcare entities. However, the complexity of HIPAA regulations makes it difficult to achieve this requirement. In this paper, we propose a framework to bridge such a critical gap between healthcare systems and HIPAA regulations. Our framework supports compliance-oriented analysis to determine whether a health- care system is complied with HIPAA regulations. We also describe our evaluation results to demonstrate the feasibility and effectiveness of our approach.\"",
        "title: \"Efficient audit service outsourcing for data integrity in clouds\" with abstract: \"Cloud-based outsourced storage relieves the client's burden for storage management and maintenance by providing a comparably low-cost, scalable, location-independent platform. However, the fact that clients no longer have physical possession of data indicates that they are facing a potentially formidable risk for missing or corrupted data. To avoid the security risks, audit services are critical to ensure the integrity and availability of outsourced data and to achieve digital forensics and credibility on cloud computing. Provable data possession (PDP), which is a cryptographic technique for verifying the integrity of data without retrieving it at an untrusted server, can be used to realize audit services. In this paper, profiting from the interactive zero-knowledge proof system, we address the construction of an interactive PDP protocol to prevent the fraudulence of prover (soundness property) and the leakage of verified data (zero-knowledge property). We prove that our construction holds these properties based on the computation Diffie-Hellman assumption and the rewindable black-box knowledge extractor. We also propose an efficient mechanism with respect to probabilistic queries and periodic verification to reduce the audit costs per verification and implement abnormal detection timely. In addition, we present an efficient method for selecting an optimal parameter value to minimize computational overheads of cloud audit services. Our experimental results demonstrate the effectiveness of our approach.\"",
        "title: \"Towards Role-Based Authorization for OSGi Service Environments\" with abstract: \"Service composition accelerates rapid application development, service reuse, and complex service consummation, thus, quantitative characteristics such as service execution throughput should be evaluated to measure the system performance. Based on Continuous ...\"",
        "1 is \"A methodology for empirical analysis of permission-based security models and its application to android\", 2 is \"Visualization of automated trust negotiation\".",
        "\nGiven above information, for an author who has written the paper with the title \"Examining Social Dynamics for Countering Botnet Attacks\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01234": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Security and Privacy Challenges in Cloud Computing Environments':",
        "title: \"Towards scalable authentication in health services\" with abstract: \"Over the last two decades, many attempts have been made to computerize the management of patient records using advanced computing and networking facilities across healthcare providers such as hospitals, clinics, and clearing agencies. In addition to this transition from a disparate and paper-based infrastructure to a consolidated and digital medium-based one, we have been confronted with privacy and security requirements since the advent of the Health Insurance Portability and Accountability Act (HIPPA). The problem we seek to address in this paper is to provide authentication of individual identity in the context of accessing critical information in Web-based e-health systems including secure transmission of data across the Internet. These problems have technical solutions that are well known, but the solutions in general are strongly biased toward a single individual interacting with a single application. In this paper, we propose a scalable token-based authentication architecture and demonstrate how we can implement this architecture using commercial-off-the-set technologies. Our approach focuses on vendor-neutral specifications. The proof-of-concept prototype has been implemented so that the pilot testing may be conducted at various sites.\"",
        "title: \"Assured resource sharing in Grid environments.\" with abstract: \"In Grid-based collaborations, a number of data sharing services in Grid are established to provide a unified platform for dynamic discovery, access and replication of distributed data. Controlling access to Grid data in these services requires the ability to dynamically make authorisation decisions based on the data owners' policies and users' credentials across administrative domains. In this paper, we present a flexible policy-driven authorisation system, called RamarsAuthZ, for secure data sharing services in Grid systems. RamarsAuthZ adopts a flexible role-based approach with trust-aware feature to advocate originator control, delegation and dissemination control. A case study based on Globus data replication service (DRS) is presented to provide effective access control both at the service level and at the data level. Our system is flexible and interoperable with multiple Grid services with little reliance on static policy and attribute management.\"",
        "title: \"A role administration system in role-based authorization infrastructures: design and implementation\" with abstract: \"In this paper we describe a system whose purpose is to help establish a valid set of roles and role hierarchies with assigned users and associated permissions. We have designed and implemented the system, called RA system, which enables role administrators to build and configure various components of a role-based access control (RBAC) model, thereby making it possible to lay a foundation for role-based authorization infrastructures. Three methodological constituents for our purpose are introduced, together with the design and implementation issues. The system has a role-centric view for easily managing constrained roles as well as assigned users and permissions. An LDAP-accessible directory service was used for a role database. We show that the system can be seamlessly integrated with an existing privilege-based authorization infrastructure. We finally discuss our plans for future development of the system.\"",
        "title: \"Access Control Management for SCADA Systems\" with abstract: \"The information technology revolution has transformed all aspects of our society including critical infrastructures and led a significant shift from their old and disparate business models based on proprietary and legacy environments to more open and consolidated ones. Supervisory Control and Data Acquisition (SCADA) systems have been widely used not only for industrial processes but also for some experimental facilities. Due to the nature of open environments, managing SCADA systems should meet various security requirements since system administrators need to deal with a large number of entities and functions involved in critical infrastructures. In this paper, we identify necessary access control requirements in SCADA systems and articulate access control policies for the simulated SCADA systems. We also attempt to analyze and realize those requirements and policies in the context of role-based access control that is suitable for simplifying administrative tasks in large scale enterprises.\"",
        "title: \"Beyond User-to-User Access Control for Online Social Networks\" with abstract: \"With the development of Web 2.0 technologies, online social networks are able to provide open platforms to enable the seamless sharing of profile data to enable public developers to interface and extend the social network services as applications (or APIs). At the same time, these open interfaces pose serious privacy concerns as third party applications are usually given full read access to the user profiles. Current related research has focused on mainly user-to-user interactions in social networks, and seems to ignore the third party applications. In this paper, we present an access control framework to manage the third party to user interactions. Our framework is based on enabling the user to specify the data attributes to be shared with the application and at the same time be able to specify the degree of specificity of the shared attributes. We model applications as finite state machines, and use the required user profile attributes as conditions governing the application execution. We formulate the minimal attribute generalization problem and we propose a solution that maps the problem to the shortest path problem to find the minimum set of attribute generalization required to access the application services.\"",
        "1 is \"On the power of multi-prover interactive protocols\", 2 is \"Towards Improved Partner Selection Using Recommendations and Trust\".",
        "\nGiven above information, for an author who has written the paper with the title \"Security and Privacy Challenges in Cloud Computing Environments\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01235": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'TruncApp: A truncation-based approximate divider for energy efficient DSP applications.':",
        "title: \"Power-Aware Deployment and Control of Forced-Convection and Thermoelectric Coolers\" with abstract: \"Advances in the thermoelectric cooling technology have made it one of the promising solutions for spot cooling in VLSI circuits. Thermoelectric coolers (TECs) generate heat during their operation. This heat plus the heat generated in the circuit should be transferred to the ambient environment in order to avoid high die temperatures. This paper describes a hybrid cooling solution in which TECs are augmented with forced-convection coolers (fans). Precisely, an optimization framework called OFTEC is presented which finds the optimum TEC driving current and the fan speed to minimize the overall power consumption of the cooling system while maintaining safe die temperatures. Simulation results on a set of eight benchmarks show the benefits of the proposed approach. In particular, a baseline system without TECs but with a fan could meet the thermal constraint for only three of the benchmarks whereas the OFTEC solution satisfied thermal constraints for all benchmarks. In addition, OFTEC resulted in 5.4% less average power consumption for the aforesaid three benchmarks while lowering the maximum die temperature by an average of 3.7\u00b0C.\"",
        "title: \"Improving the Efficiency of Power Management Techniques by Using Bayesian Classification\" with abstract: \"This paper presents a supervised learning based dynamic power management (DPM) framework for a multicore processor, where a power manager (PM) learns to predict the system performance state from some readily available input features (such as the state of service queue occupancy and the task arrival rate) and then uses this predicted state to look up the optimal power management action from a pre-computed policy lookup table. The motivation for utilizing supervised learning in the form of a Bayesian classifier is to reduce overhead of the PM which has to recurrently determine and issue voltage-frequency setting commands to each processor core in the system. Experimental results reveal that the proposed Bayesian classification based DPM technique ensures system-wide energy savings under rapidly and widely varying workloads.\"",
        "title: \"Efficient Peak Shaving in a Data Center by Joint Optimization of Task Assignment and Energy Storage Management\" with abstract: \"The usage of energy storage devices in data centers has been widely studied for the purpose of peak shaving in the context of dynamic utility pricing. To effectively achieve peak shaving in a data center with energy storage capability, a joint optimization framework is proposed to solve the task assignment problem and the energy storage management problem. The power hierarchy of a data center is modeled as a tree with battery arrays connected to each node, which is a generalization of common energy storage deployment topologies including the centralized structure and the rack/server-level distributed structure. A multi-dimensional resource request model is adopted based on released cluster traces in order to capture the complex dynamics of the workload of a data center. In addition, the rate capacity effect and aging effect are considered when modeling the batteries. A progressive refinement approach is used to solve the optimization problem. Experimental results show that the proposed algorithm consistently outperforms some baseline algorithms.\"",
        "title: \"Gated clock routing for low-power microprocessor design\" with abstract: \"This paper presents a zero-skew gated clock routing technique for VLSI circuits. Gated clock trees include masking gates at the internal nodes of the clock tree, which are selectively turned on and off by the gate control signals during the active and idle times of the circuit modules to reduce the switched capacitance of the clock tree. We construct a clock-tree topology based on the locations and the activation frequencies of the modules, while the locations of the internal nodes of the clock tree (and, hence, the masking gates) are determined using a dynamic programming approach followed by a gate reduction heuristic. This work assumes that the gates are turned on/off by a centralized controller. Therefore, the additional power and routing area incurred by the controller and the gate control signal routing are examined. Various tradeoffs between power and area for different design options and module activities are discussed and detailed experimental results are presented. Finally, good design practices for implementing the gated clocks are suggested\"",
        "title: \"An empirical study of crosstalk in VDSM technologies\" with abstract: \"We perform a detailed study of various crosstalk scenarios in VDSM technologies by using a distributed model of the crosstalk site and make a number of key observations about the crosstalk effects in VLSI circuits. As example of these observations, we report that the combination of one crosstalk event at some site and another crosstalk event at a different site in the transitive fan-out of the first site may cause a slowdown or speedup of the circuit by an amount that can significantly exceed the sum of crosstalk effects caused by each site in isolation. As another example, we report that the common assumption that zero skew between the input transitions of aggressor and victim lines causes the worst case crosstalk effect is not always valid, and therefore, optimization or test based on such an assumption may be invalid. We also demonstrate the non-monotone behavior of the crosstalk effect with respect to the skew between the input transition of aggressor and victim lines. This work provides a first step toward the development of a new framework for timing analysis and test development in the presence of crosstalk events.\"",
        "1 is \"Reliability- and process variation-aware placement for FPGAs\", 2 is \"Challenges and Solutions for Late- and Post-Silicon Design\".",
        "\nGiven above information, for an author who has written the paper with the title \"TruncApp: A truncation-based approximate divider for energy efficient DSP applications.\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01236": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Attention And Localization Based On A Deep Convolutional Recurrent Model For Weakly Supervised Audio Tagging':",
        "title: \"Clustering Expressive Timing with Regressed Polynomial Coefficients Demonstrated by a Model Selection Test.\" with abstract: \"Though many past works have tried to cluster expressive timing within a phrase, there have been few attempts to cluster features of expressive timing with constant dimensions regardless of phrase lengths. For example, used as a way to represent expressive timing, tempo curves can be regressed by a polynomial function such that the number of regressed polynomial coefficients remains constant with a given order regardless of phrase lengths. In this paper, clustering the regressed polynomial coefficients is proposed for expressive timing analysis. A model selection test is presented to compare Gaussian Mixture Models (GMMs) fitting regressed polynomial coefficients and fitting expressive timing directly. As there are no expected results of clustering expressive timing, the proposed method is demonstrated by how well the expressive timing are approximated by the centroids of GMMs. The results show that GMMs fitting the regressed polynomial coefficients outperform GMMs fitting expressive timing directly. This conclusion suggests that it is possible to use regressed polynomial coefficients to represent expressive timing within a phrase and cluster expressive timing within phrases of different lengths.\"",
        "title: \"INK-SVD: Learning incoherent dictionaries for sparse representations\" with abstract: \"This work considers the problem of learning an incoherent dictionary that is both adapted to a set of training data and incoherent so that existing sparse approximation algorithms can recover the sparsest representation. A new decorrelation method is presented that computes a fixed coherence dictionary close to a given dictionary. That step iterates pairwise decorrelations of atoms in the dictionary. Dictionary learning is then performed by adding this decorrelation method as an intermediate step in the K-SVD learning algorithm. The proposed algorithm INK-SVD is tested on musical data and compared to another existing decorrelation method. INK-SVD can compute a dictionary that approximates the training data as well as K-SVD while decreasing the coherence from 0.6 to 0.2.\"",
        "title: \"SMALLbox - an evaluation framework for sparse representations and dictionary learning algorithms\" with abstract: \"SMALLbox is a new foundational framework for processing signals, using adaptive sparse structured representations. The main aim of SMALLbox is to become a test ground for exploration of new provably good methods to obtain inherently data-driven sparse models, able to cope with large-scale and complicated data. The toolbox provides an easy way to evaluate these methods against state-of-the art alternatives in a variety of standard signal processing problems. This is achieved trough a unifying interface that enables a seamless connection between the three types of modules: problems, dictionary learning algorithms and sparse solvers. In addition, it provides interoperability between existing state-of-the-art toolboxes. As an open source MATLAB toolbox, it can be also seen as a tool for reproducible research in the sparse representations research community.\"",
        "title: \"Deep Remix: Remixing Musical Mixtures Using a Convolutional Deep Neural Network\" with abstract: \"  Audio source separation is a difficult machine learning problem and performance is measured by comparing extracted signals with the component source signals. However, if separation is motivated by the ultimate goal of re-mixing then complete separation is not necessary and hence separation difficulty and separation quality are dependent on the nature of the re-mix. Here, we use a convolutional deep neural network (DNN), trained to estimate 'ideal' binary masks for separating voice from music, to perform re-mixing of the vocal balance by operating directly on the individual magnitude components of the musical mixture spectrogram. Our results demonstrate that small changes in vocal gain may be applied with very little distortion to the ultimate re-mix. Our method may be useful for re-mixing existing mixes. \"",
        "title: \"On Polar Polytopes and the Recovery of Sparse Representations\" with abstract: \"Suppose we have a signal y which we wish to represent using a linear combination of a number of basis atoms ai,y=Sigmaixiai=Ax. The problem of finding the minimum l0 norm representation for y is a hard problem. The basis pursuit (BP) approach proposes to find the minimum l1 norm representation instead, which corresponds to a linear program (LP) that can be solved using modern LP techniques, and several recent authors have given conditions for the BP (minimum l1 norm) and sparse (minimum l0 norm) representations to be identical. In this paper, we explore this sparse representation problem using the geometry of convex polytopes, as recently introduced into the field by Donoho. By considering the dual LP we find that the so-called polar polytope P* of the centrally symmetric polytope P whose vertices are the atom pairs plusmnai is particularly helpful in providing us with geometrical insight into optimality conditions given by Fuchs and Tropp for non-unit-norm atom sets. In exploring this geometry, we are able to tighten some of these earlier results, showing for example that a condition due to Fuchs is both necessary and sufficient for l1-unique-optimality, and there are cases where orthogonal matching pursuit (OMP) can eventually find all l1-unique-optimal solutions with m nonzeros even if the exact recover condition (ERC) fails for m.\"",
        "1 is \"FAST BAYESIAN NMF ALGORITHMS ENFORCING HARMONICITY AND TEMPORAL CONTINUITY IN POLYPHONIC MUSIC TRANSCRIPTION\", 2 is \"Impactless Sagittal Gait Of A Biped Robot During The Single Support Phase\".",
        "\nGiven above information, for an author who has written the paper with the title \"Attention And Localization Based On A Deep Convolutional Recurrent Model For Weakly Supervised Audio Tagging\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01237": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Building detection from mobile imagery using informative SIFT descriptors':",
        "title: \"Learning to Focus Attention on Discriminative Regions for Object Detection\" with abstract: \"A major task of visual attention is to focus processing on regions of interest to enable rapid and robust object search. Instead of integrating generic feature extraction into object specific interpretation we strictly pursue a top-down approach. Early features are tuned to selectively respond to task related visual features, i.e., locally discriminative information that is useful in object recognition. In this work we determine discriminative regions from the information content in the local appearance patterns. A rapid mapping from appearances to discriminative regions is estimated using decision trees. The focus of attention on discriminative patterns enables then efficient detection of a searched object, but also the definition of sparse object representations to respond only to task relevant information. In the experiments, the performance in object recognition from single imagettes dramatically increased considering only discriminative patterns. Evaluation of complete image analysis under various degrees of partial occlusion and image noise resulted in highly robust recognition even in the presence of severe occlusion and noise effects. Finally, preliminary results on attentive object detection in cluttered environments demonstrated successful indexing to relevant locations.\"",
        "title: \"Visual recovery of saliency maps from human attention in 3D environments\" with abstract: \"The estimation of human attention has recently been addressed in the context of human robot interaction. Today, joint work spaces already exist and challenge cooperating systems to jointly focus on common objects, scenes and work niches. With the advent of Google glasses and increasingly affordable wearable eye-tracking, monitoring of human attention will soon become ubiquitous. The presented work describes for the first time a method for the estimation of human fixations in 3D environments that does not require any artificial landmarks in the field of view and enables attention mapping in 3D models. It enables full 3D recovery of the human view frustum and the gaze pointer in a previously acquired 3D model of the environment in real time. The study on the precision of this method reports a mean projection error \u22481.1 cm and a mean angle error \u22480.6\u00b0 within the chosen 3D model - the precision does not go below the one of the technical instrument (\u22481\u00b0) This innovative methodology will open new opportunities for joint attention studies as well as for bringing new potential into automated processing for human factors technologies.\"",
        "title: \"Robust Localization Using Context In Onmidirectional Imaging\" with abstract: \"This work presents the concept to recover and utilize the visual context in panoramic images. Omnidirectinal imaging has become recently an efficient basis for robot navigation. The proposed Bayesian reasoning over local image appearances enables to reject false hypotheses which do not fit the structural constraints in corresponding feature tranjectories. The methodology is proved with real image data from an office robot to dramatically increase the localization performance in the presence of severe occlusion effects, particularly in noisy environments, and to recover rotational information on the fly.\"",
        "title: \"Exploring the possibilities of body motion data for human computer interaction research\" with abstract: \"The ability to move is an important characteristic of the human condition and an important aspect for interactive settings. The role of body movement however was not addressed with priority in human computer interaction until now. In this paper we explore the possibilities and issues for usability and user experience research utilizing body motion data. We provide an overview of relevant related work and report the setup and initial results of two studies utilizing body motion capture. We discuss the experiences made in using motion capture approaches for human computer interaction research and provide an outlook on future directions of research.\"",
        "title: \"Directed cultural probes: detecting barriers in the usage of public transportation\" with abstract: \"In this paper we describe the application of a variation of cultural probing for identifying barriers in the use of public transportation for target groups with visual, cognitive or language-related handicaps. To be able to better focus on the targeted aspect - the barriers - we applied modifications to the traditional cultural probing approach: Users were encouraged to generate data related to the targeted aspect. We found that this approach can produce focused results that can be analysed fast and can help to overcome obstacles related to limitations in verbal skills or expressiveness of the user.\"",
        "1 is \"Finding equitable convex partitions of points in a polygon efficiently\", 2 is \"Interaction techniques for musical performance with tabletop tangible interfaces\".",
        "\nGiven above information, for an author who has written the paper with the title \"Building detection from mobile imagery using informative SIFT descriptors\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01238": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Handling non-functional requirements in Model-Driven Development: An ongoing industrial survey':",
        "title: \"Agile quality requirements engineering challenges: first results from a case study\" with abstract: \"ABSTRACTAgile software development methods have become increasingly popular in the last years. Despite their popularity, they have been criticized for focusing on delivering functional requirements and neglecting the quality requirements. Several studies have reported this shortcoming. However, there is little known about the challenges organizations currently face when dealing with quality requirements. Based on a qualitative exploratory case study, this research investigated real life large-scale distributed Agile projects to understand the challenges Agile teams face regarding quality requirements. Eighteen semi-structured open-ended in-depth interviews were conducted with Agile practitioners representing six different organizations in the Netherlands. Based on the analysis of the collected data, we have identified nine challenges Agile practitioners face when engineering quality requirements in large-scale distributed Agile projects that could harm the implementation of the quality requirements and result in neglecting them.\"",
        "title: \"Uncertain Context Factors In Erp Project Estimation Are An Asset: Insights From A Semi-Replication Case Study In A Financial Services Firm\" with abstract: \"This paper reports on the findings of a case study in a company in the financial services sector in which we replicated the use of a previously published approach to systematically balance the contextual uncertainties in the estimation of Enterprise Resource Planning (ERP) projects. The approach is based on using three techniques, a parametric model, namely COCOMO II, a portfolio management model, and Monte Carlo simulations. We investigated (i) whether the adjustment of uncertain cost drivers in the COCOMO II model increases the chance of project success in a portfolio of ERP projects, (ii) which cost drivers of the COCOMO II model can be adjusted in a way that maximized the chance of portfolio success under time constraints, and (iii) which cost drivers of the COCOMO II model can be adjusted in a way that maximized the chance of portfolio success under effort constraints. We found that 11 COCOMO II cost drivers can be changed so that the change impacts the project outcomes under both time and effort constraints. This result is different from the result in the first case study in which 13 such factors were found.\"",
        "title: \"Understanding Functional Reuse of ERP Requirements in the Telecommunication Sector: An Empirical Study\" with abstract: \"This paper is an empirical study on the application of Function Points (FP) and a FP-based reuse measurement model in Enterprise Resource Planning (ERP) projects in three organizations in the telecommunication sector. The findings of the study are used to compare the requirements reuse for one particular package, namely SAP. We found (1) that reuse is possible up to 80% at best, and (2) that, while for some modules, the organizations achieved the same levels of reuse, for other modules, the organizations' levels of reuse varied widely. We conclude with some implications of our findings for both practitioners and researchers.\"",
        "title: \"Developing an Inter-Enterprise Alignment Maturity Model: Research Challenges and Solutions\" with abstract: \"Abstract Business-IT alignment is pervasive today, as organizations strive to achieve competitive advantage. Like in other areas, e.g., software development, maintenance and IT services, there are maturity models to assess such alignment. Those models, however, do not specifically address the aspects needed for achieving alignment between business and IT in inter-enterprise settings. In this paper, we present the challenges we face in the development of an inter-enterprise alignment maturity model, as well as the current solutions to counter these problems. Contents\"",
        "title: \"Value Creation by Agile Projects: Methodology or Mystery?\" with abstract: \"Business value is a key concept in agile software development approaches. This paper presents results of a systematic review of literature on how business value is created by agile projects. We found that with very few exceptions, most published studies take the concept of business value for granted and do not state what it means in general as well as in the specific study context. We could find no study which clearly indicates how exactly individual agile practices or groups of those create value and keep accumulating it over time. The key implication for research is that we have an incentive to pursue the study of value creation in agile project by deploying empirical research methods.\"",
        "1 is \"Research challenges of autonomic computing\", 2 is \"The role of outcome feedback in improving the uncertainty assessment of software development effort estimates\".",
        "\nGiven above information, for an author who has written the paper with the title \"Handling non-functional requirements in Model-Driven Development: An ongoing industrial survey\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01239": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Formal Analysis of the Shlaer-Mellor Method: Towards a Toolkit of Formal and Informal Requirements Specification Techniques':",
        "title: \"A Coordination Complexity Model to Support Requirements Engineering for Cross-organizational ERP\" with abstract: \"Cross-organizational information systems projects, such as ERP, imply an expensive requirements engineering (RE) cycle. Little is known yet about how to carry it out with more predictable alignment results and chances for success. We propose an approach that allows incremental, systematic improvement of crossorganizational RE. It builds on organizational network research,coordination theory, ERP misalignments, and existing RE improvement standards.\"",
        "title: \"06351 Abstracts Collection -- Methods for Modelling Software Systems (MMOSS)\" with abstract: \"From 27.08.06 to 01.09.06, the Dagstuhl Seminar 06351 ``Methods for Modelling Software Systems (MMOSS)\\u0027\\u0027 was held in the International Conference and Research Center (IBFI), Schloss Dagstuhl.\\r\\nDuring the seminar, several participants presented their current\\r\\nresearch, and ongoing work and open problems were discussed. Abstracts of\\r\\nthe presentations given during the seminar as well as abstracts of\\r\\nseminar results and ideas are put together in this paper. The first section\\r\\ndescribes the seminar topics and goals in general.\\r\\nLinks to extended abstracts or full papers are provided, if available.\"",
        "title: \"Requirements engineering conferences: Wither industry tracks?\" with abstract: \"This position paper argues that industry tracks have no place in any research conference. Instead, a research conference should always have room for industrial case studies, evaluated according to criteria for empirical research. Such case studies would not be acceptable at a practitioners' industrial conference, just as papers presented at such conferences would not be acceptable at research conferences. It follows as corollary that if researchers want to become familiar with problems and solutions of RE practice, they should visit industrial conferences.\"",
        "title: \"Developing a Domain-Specific Cross-Organizational RE Method\" with abstract: \"Cross-organizational requirements engineering (XRE) is the activity in which several business actors perform a joint problem-solving process in which a cooperative, cross-organizational business solution is designed. In XRE, partially conflicting concerns and views must be reconciled to create a shared vision of the goals and structure of a cooperative process. In this paper we report on the development of an XRE method, BusMod, by means of action research. Each iteration of our action research cycle consisted of a series of consultancy projects in which our method is used, followed by a reflection to draw lessons learned to improve the method. Although BusMod has been developed in the domain of electricity power generation, we hypothesize that it can be generalized to any domain of cooperating actors, and that in any such domain value engineering must be part of requirements engineering.\"",
        "title: \"A Fixed-point Characterization of a Deontic Logic of Regular Action\" with abstract: \"We define a deontic logic of regular action as a characterization within a modal \u03bc-calculus of action. First a semantics of deontic notions for regular action is given in terms of conditions on modal action structures. Then modal \u03bc-calculus formulas characterizing these conditions are constructed by closely following the structure of deterministic finite automatons for regular action.\"",
        "1 is \"Measuring similarity between collection of values\", 2 is \"Service components for managing the life-cycle of service compositions\".",
        "\nGiven above information, for an author who has written the paper with the title \"Formal Analysis of the Shlaer-Mellor Method: Towards a Toolkit of Formal and Informal Requirements Specification Techniques\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01240": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'High-efficiency protection solution for off-chip memory in embedded systems':",
        "title: \"Area-Optimized Technology Mapping for Hybrid FPGAs\" with abstract: \"As integration levels in FPGA devices have increased over the past decade, the structure of programmable logic resources has become more diversified. Recently, Altera Corporation has introduced a new family of LUT-based FPGAs that have been augmented with user-configurable programmable logic array blocks (PLAs). In this paper a novel FPGA technology mapping approach is described that automatically partitions user designs into netlist subgraphs appropriately-sized for implementation on both types of available user resources. The subgraphs are subsequently mapped to assigned target resources. It is shown that fast estimation of post-minimization product term counts plays an especially important role in the mapping of designs to PLAs.\"",
        "title: \"Reconfigurable Data Planes for Scalable Network Virtualization\" with abstract: \"Network virtualization presents a powerful approach to share physical network infrastructure among multiple virtual networks. Recent advances in network virtualization advocate the use of field-programmable gate arrays (FPGAs) as flexible high performance alternatives to conventional host virtualization techniques. However, the limited on-chip logic and memory resources in FPGAs severely restrict the scalability of the virtualization platform and necessitate the implementation of efficient forwarding structures in hardware. The research described in this manuscript explores the implementation of a scalable heterogeneous network virtualization platform that integrates virtual data planes implemented in FPGAs with software data planes created using host virtualization techniques. The system exploits data plane heterogeneity to cater to the dynamic service requirements of virtual networks by migrating networks between software and hardware data planes. We demonstrate data plane migration as an effective technique to limit the impact of traffic on unmodified data planes during FPGA reconfiguration. Our system implements forwarding tables in a shared fashion using inexpensive off-chip memories and supports both Internet Protocol (IP) and non-IP-based data planes. Experimental results show that FPGA-based data planes can offer two orders of magnitude better throughput than their software counterparts, and FPGA reconfiguration can facilitate data plane customization within 12 seconds. An integrated system that supports up to 15 virtual networks has been validated on the NetFPGA platform.\"",
        "title: \"A monitor interconnect and support subsystem for multicore processors\" with abstract: \"In many current SoCs, the architectural interface to on-chip monitors is ad hoc and inefficient. In this paper, a new architectural approach which advocates the use of a separate low-overhead subsystem for monitors is described. A key aspect of this approach is an on-chip interconnect specifically designed for monitor data with different priority levels. The efficiency of our monitor interconnect is assessed for a multicore system using both an interconnect and a system-level simulator. Collected monitor information is used by a dedicated processor to control the frequency and voltage of individual multicore processors. Experimental results show that the new low-overhead subsystem facilitates employment of thermal and delay-aware dynamic voltage and frequency scaling.\"",
        "title: \"Application Specific Customization and Scalability of Soft Multiprocessors\" with abstract: \"Although soft microprocessors are widely used in FPGAs, limited work has been performed regarding how to automatically and efficiently generate soft multiprocessors. In this paper, an automated parallel compilation environment for multiple soft processors which incorporates parallel compilation and inter-processor communication structures is described. A total of eight previously-developed parallel processing benchmarks have been automatically mapped to a varying number of synthesized soft microprocessors in commercial FPGAs. The new automated infrastructure allows for an evaluation of area, performance, and power tradeoffs for a range of architectural choices. Experiments show that our soft-multiprocessor systems consisting of up to 16 processors can offer up to 5\u00c3\u0097 improvement in application performance against their uniprocessor counterparts.\"",
        "title: \"Multicore soft error rate stabilization using adaptive dual modular redundancy\" with abstract: \"The use of dynamic voltage and frequency scaling (DVFS) in contemporary multicores provides significant protection from unpredictable thermal events. A side effect of DVFS can be an increased processor exposure to soft errors. To address this issue, a flexible fault prevention mechanism has been developed to selectively enable a small amount of per-core dual modular redundancy (DMR) in response to increased vulnerability, as measured by the processor architectural vulnerability factor (AVF). Our new algorithm for DMR deployment aims to provide a stable effective soft error rate (SER) by using DMR in response to DVFS caused by thermal events. The algorithm is implemented in real-time on the multicore using a dedicated monitor network-on-chip and controller which evaluates thermal information and multicore performance statistics. Experiments with a multicore simulator using standard benchmarks show an average 6% improvement in overall power consumption and a stable SER by using selective DMR versus continuous DMR deployment.\"",
        "1 is \"Using the Spring Physical Model to Extend a Cooperative Caching Protocol for Many-Core Processors\", 2 is \"Diagnosis of interconnect faults in cluster-based FPGA architectures\".",
        "\nGiven above information, for an author who has written the paper with the title \"High-efficiency protection solution for off-chip memory in embedded systems\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01241": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Adjust weight vectors in MOEA/D for bi-objective optimization problems with discontinuous Pareto fronts.':",
        "title: \"Combining particle swarm optimization and neural network for diagnosis of unexplained syncope\" with abstract: \"Given the relative limitations of BP and GA based leaning algorithms, Particle Swarm Optimization (PSO) is proposed to train Artificial Neural Networks (ANN) for the diagnosis of unexplained syncope. Compared with BP and GA based training techniques, PSO based learning method improves the diagnosis accuracy and speeds up the convergence process. Experimental results show that PSO is a robust training algorithm and should be extended to other real-world pattern classification applications.\"",
        "title: \"Effective metaheuristics for scheduling a hybrid flowshop with sequence-dependent setup times.\" with abstract: \"This paper proposes a total of nine algorithms to minimize the makespan for the hybrid flowshop scheduling problem with sequence-dependent setup times. The first six algorithms are trajectory-based metaheuristics, including three variants of iterated local search and three variants of iterated greedy. The remaining three algorithms are population-based metaheuristics, namely, the improved fruit fly optimization, the improved migrating birds optimization, and the discrete artificial bee colony optimization. We present some advanced and effective technologies, including three mixed neighborhood structures, an enhanced perturbation method, and an enhanced destruction and construction procedure for the trajectory-based metaheuristics. We propose a path-relinking-based cooperative search, a diversity control scheme, and a diversified initialization approach for the improved fruit fly optimization. We calibrate the parameters and operators for the proposed algorithms by means of a design of experiments approach. To evaluate the proposed algorithms, we present several adaptations of other recent well-known meta-heuristics for the problem and conduct a comprehensive set of computational and statistical experiments to demonstrate the effectiveness of the presented algorithms. Among them, the discrete artificial bee colony optimization is the best-performing algorithm and it is able to improve 126 out of the 240 best known solutions for the benchmarks in the literature.\"",
        "title: \"A differential evolution algorithm with intersect mutation operator\" with abstract: \"This paper proposes a novel differential evolution (DE) algorithm with intersect mutation operation called intersect mutation differential evolution (IMDE) algorithm. Instead of focusing on setting proper parameters, in IMDE algorithm, all individuals are divided into the better part and the worse part according to their fitness. And then, the novel mutation and crossover operations have been developed to generate the new individuals. Finally, a set of famous benchmark functions have been used to test and evaluate the performance of the proposed IMDE. The experimental results show that the proposed algorithm is better than, or at least comparable to the self-adaptive DE (JDE), which is proven to be better than the standard DE algorithm. In further study, the IMDE algorithm has also been compared with several improved Particle Swarm Optimization (PSO) algorithms, Artificial Bee Colony (ABC) algorithm and Bee Swarm Optimization (BSO) algorithm. And the IMDE algorithm outperforms these algorithms.\"",
        "title: \"Mathematical modeling and evolutionary algorithm-based approach for integrated process planning and scheduling\" with abstract: \"Traditionally, process planning and scheduling were performed sequentially, where scheduling was implemented after process plans had been generated. Considering their complementarity, it is necessary to integrate these two functions more tightly to improve the performance of a manufacturing system greatly. In this paper, a mathematical model of integrated process planning and scheduling has been formulated. And, an evolutionary algorithm-based approach has been developed to facilitate the integration and optimization of these two functions. To improve the optimized performance of the approach, efficient genetic representation and operator schemes have been developed. To verify the feasibility and performance of the proposed approach, experimental studies have been conducted and comparisons have been made between this approach and some previous works. The experimental results show that the integrated process planning and scheduling is necessary and the proposed approach has achieved significant improvement.\"",
        "title: \"A new methodology for multi-objective multidisciplinary design optimization problems based on game theory\" with abstract: \"We propose a new method based on gene expression programming and Nash equilibrium.The solutions are better than some solutions of other methods.The new method can obtain a better Nash solution with more accurate approximation. The design of engineering systems often involves multiple disciplines and competing objectives, which requires coordination, information exchange and share amongst the disciplines. However, in practical design environments, designers have to make decisions in isolation due to organization barriers, time schedules and geographical constraints. This paper will propose a new approach for the multi-objective multidisciplinary design optimization (MDO) problems in non-cooperative environments based on gene expression programming (GEP) and Nash equilibrium in the game theory. In this approach, the GEP method is used as a surrogate to construct the approximate rational reaction sets (RRSs) in the Nash model. The effectiveness of the proposed method is demonstrated by the design of a thin-walled pressure vessel and the hull form parameter design of a small waterplane area twin hull (SWATH) ship. The results show that this approach can fully explore and provide the explicit functional relationship between the strategy of an isolated player and the control variables of the other players, thus able to obtain a better Nash equilibrium solution.\"",
        "1 is \"NeuroLinear: From neural networks to oblique decision rules\", 2 is \"Iterated local search heuristics for the Vehicle Routing Problem with Cross-Docking\".",
        "\nGiven above information, for an author who has written the paper with the title \"Adjust weight vectors in MOEA/D for bi-objective optimization problems with discontinuous Pareto fronts.\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01242": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Clustering Goes Big: CLUBS-P, an Algorithm for Unsupervised Clustering Around Centroids Tailored For Big Data Applications':",
        "title: \"RFID-data compression for supporting aggregate queries\" with abstract: \"RFID-based systems for object tracking and supply chain management have been emerging since the RFID technology proved effective in monitoring movements of objects. The monitoring activity typically results in huge numbers of readings, thus making the problem of efficiently retrieving aggregate information from the collected data a challenging issue. In fact, tackling this problem is of crucial importance, as fast answers to aggregate queries are often mandatory to support the decision making process. In this regard, a compression technique for RFID data is proposed, and used as the core of a system supporting the efficient estimation of aggregate queries. Specifically, this technique aims at constructing a lossy synopsis of the data over which aggregate queries can be estimated, without accessing the original data. Owing to the lossy nature of the compression, query estimates are approximate, and are returned along with intervals that are guaranteed to contain the exact query answers. The effectiveness of the proposed approach has been experimentally validated, showing a remarkable trade-off between the efficiency and the accuracy of the query estimation.\"",
        "title: \"A Framework for Adaptive Mail Classification\" with abstract: \"We introduce a technique based on data mining algorithms for classifying incoming messages, as a basis for an overall architecture for maintenance and management of e-mail messages. We exploit clustering techniques for grouping structured and unstructured information extracted from e-mail messages in an unsupervised way, and exploit the resulting algorithm in the process of folder creation (and maintenance) and e-mail redirection. Some initial experimental results show the effectiveness of the technique, both from an efficiency and a quality-of-results viewpoint.\"",
        "title: \"Web wrapper induction: a brief survey\" with abstract: \"Nowadays several companies use the information available on the Web for a number of purposes. However, since most of this information is only available as HTML documents, several techniques that allow information from the Web to be automatically extracted have recently been defined. In this paper we review the main techniques and tools for extracting information available on the Web, devising a taxonomy of existing systems. In particular we emphasize the advantages and drawbacks of the techniques analyzed from a user point of view.\"",
        "title: \"Detecting Structural Similarities between XML Documents\" with abstract: \"In this paper we propose a technique for detecting the similarity in the structure of XML documents. The technique is based on the idea of representing the structure of an XML document as a time series in which each occurrence of a tag corresponds to a given impulse. By analyzing the frequencies of the corresponding Fourier transform, we can hence state the degree of similarity between documents. The eciency and eectiveness of this approach are compelling when compared with traditional ones.\"",
        "title: \"Efficiently interpreting traces of low level events in business process logs.\" with abstract: \"\u2022Interpret the traces of low-level events in multi-process logs as high-level activity instances.\u2022Many-to-many mapping between events and activities requires dealing with uncertainty.\u2022Capability of dealing with complex activities (generating multiple events).\"",
        "1 is \"Dynamic itemset counting and implication rules for market basket data\", 2 is \"Efficient Structural Joins on Indexed XML Documents\".",
        "\nGiven above information, for an author who has written the paper with the title \"Clustering Goes Big: CLUBS-P, an Algorithm for Unsupervised Clustering Around Centroids Tailored For Big Data Applications\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01243": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Improving phone duration modelling using support vector regression fusion':",
        "title: \"Phone duration modeling: overview of techniques and performance optimization via feature selection in the context of emotional speech\" with abstract: \"Accurate modeling of prosody is prerequisite for the production of synthetic speech of high quality. Phone duration, as one\n of the key prosodic parameters, plays an important role for the generation of emotional synthetic speech with natural sounding.\n In the present work we offer an overview of various phone duration modeling techniques, and consequently evaluate ten models,\n based on decision trees, linear regression, lazy-learning algorithms and meta-learning algorithms, which over the past decades\n have been successfully used in various modeling tasks. Furthermore, we study the opportunity for performance optimization\n by applying two feature selection techniques, the RReliefF and the Correlation-based Feature Selection, on a large set of\n numerical and nominal linguistic features extracted from text, such as: phonetic, phonologic and morphosyntactic ones, which\n have been reported successful on the phone and syllable duration modeling task. We investigate the practical usefulness of\n these phone duration modeling techniques on a Modern Greek emotional speech database, which consists of five categories of\n emotional speech: anger, fear, joy, neutral, sadness. The experimental results demonstrated that feature selection significantly\n improves the accuracy of phone duration prediction regardless of the type of machine learning algorithm used for phone duration\n modeling. Specifically, in four out of the five categories of emotional speech, feature selection contributed to the improvement\n of the phone duration modeling, when compared to the case without feature selection. The M5p trees based phone duration model was observed to achieve the best phone duration prediction accuracy in terms of RMSE and\n MAE.\"",
        "title: \"Recognition of greek phonemes using support vector machines\" with abstract: \"In the present work we study the applicability of Support Vector Machines (SVMs) on the phoneme recognition task. Specifically, the Least Squares version of the algorithm (LS-SVM) is employed in recognition of the Greek phonemes in the framework of telephone-driven voice-enabled information service. The N-best candidate phonemes are identified and consequently feed to the speech and language recognition components. In a comparative evaluation of various classification methods, the SVM-based phoneme recognizer demonstrated a superior performance. Recognition rate of 74.2% was achieved from the N-best list, for N=5, prior to applying the language model.\"",
        "title: \"The Effect of Emotional Speech on a Smart-Home Application\" with abstract: \"The present work studies the effect of emotional speech on a smart-home application. Specifically, we evaluate the recognition performance of the automatic speech recognition component of a smart-home dialogue system for various categories of emotional speech. The experimental results reveal that word recognition rate for emotional speech varies significantly across different emotion categories.\"",
        "title: \"Online Seizure Detection From Eeg And Ecg Signals For Monitoring Of Epileptic Patients\" with abstract: \"In this article, we investigate the performance of a seizure detection module for online monitoring of epileptic patients. The module is using as input data streams from electroencephalographic and electrocardiographic recordings. The architecture of the module consists of time and frequency domain feature extraction followed by classification. Four classification algorithms were evaluated on three epileptic subjects. The best performance was achieved by the support vector machine algorithm, with more than 90% for two of the subjects and slightly lower than 90% for the third subject.\"",
        "title: \"Support vector regression fusion scheme in phone duration modeling\" with abstract: \"A fusion scheme of phone duration models (PDMs) is presented in this work. Specifically, a support vector regression (SVR)-fusion model is fed with the predictions of a group of independent PDMs operating in parallel. The American-English KED TIMIT and the Greek WCL-1 databases are used for evaluating the PDMs and the fusion scheme. The fusion scheme contributes to the accuracy improvement over the best individual model, achieving a relative reduction of the mean absolute error (MAE) and the root mean square error (RMSE), by 1.9% and 2.0% on KED TLVHT, and 2.6% and 1.8% respectively on WCL-1. Moreover, for evaluating the impact the accuracy improvement will have on synthetic speech, perceptual evaluation test was performed. This test showed that the accuracy improvement achieved by the SVR-fusion would contribute to the improvement of the naturalness of synthetic speech.\"",
        "1 is \"Efficient Speaker Recognition Using Approximated Cross Entropy (ACE)\", 2 is \"Maximum-Likelihood Linear-Regression For Speaker Adaptation Of Continuous Density Hidden Markov-Models\".",
        "\nGiven above information, for an author who has written the paper with the title \"Improving phone duration modelling using support vector regression fusion\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01244": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'INSPIRE: Evaluation of a Smart-Home System for Infotainment Management and Device Control':",
        "title: \"Phone duration modeling: overview of techniques and performance optimization via feature selection in the context of emotional speech\" with abstract: \"Accurate modeling of prosody is prerequisite for the production of synthetic speech of high quality. Phone duration, as one\n of the key prosodic parameters, plays an important role for the generation of emotional synthetic speech with natural sounding.\n In the present work we offer an overview of various phone duration modeling techniques, and consequently evaluate ten models,\n based on decision trees, linear regression, lazy-learning algorithms and meta-learning algorithms, which over the past decades\n have been successfully used in various modeling tasks. Furthermore, we study the opportunity for performance optimization\n by applying two feature selection techniques, the RReliefF and the Correlation-based Feature Selection, on a large set of\n numerical and nominal linguistic features extracted from text, such as: phonetic, phonologic and morphosyntactic ones, which\n have been reported successful on the phone and syllable duration modeling task. We investigate the practical usefulness of\n these phone duration modeling techniques on a Modern Greek emotional speech database, which consists of five categories of\n emotional speech: anger, fear, joy, neutral, sadness. The experimental results demonstrated that feature selection significantly\n improves the accuracy of phone duration prediction regardless of the type of machine learning algorithm used for phone duration\n modeling. Specifically, in four out of the five categories of emotional speech, feature selection contributed to the improvement\n of the phone duration modeling, when compared to the case without feature selection. The M5p trees based phone duration model was observed to achieve the best phone duration prediction accuracy in terms of RMSE and\n MAE.\"",
        "title: \"Support vector regression fusion scheme in phone duration modeling\" with abstract: \"A fusion scheme of phone duration models (PDMs) is presented in this work. Specifically, a support vector regression (SVR)-fusion model is fed with the predictions of a group of independent PDMs operating in parallel. The American-English KED TIMIT and the Greek WCL-1 databases are used for evaluating the PDMs and the fusion scheme. The fusion scheme contributes to the accuracy improvement over the best individual model, achieving a relative reduction of the mean absolute error (MAE) and the root mean square error (RMSE), by 1.9% and 2.0% on KED TLVHT, and 2.6% and 1.8% respectively on WCL-1. Moreover, for evaluating the impact the accuracy improvement will have on synthetic speech, perceptual evaluation test was performed. This test showed that the accuracy improvement achieved by the SVR-fusion would contribute to the improvement of the naturalness of synthetic speech.\"",
        "title: \"Dynamic selection of a speech enhancement method for robust speech recognition in moving motorcycle environment\" with abstract: \"We present a speech pre-processing scheme (SPPS) for ro bust speech recognition in the moving motorcycle environment. The SPPS is dynamically adapted during the run-time operation of the speech front-end, depending on short-time characteristics of the acoustic environment. In detail, the fast varying acoustic environment is modeled by GMM clusters based on which a selection function determines the speech enhancement method to be applied. The correspondence between input audio and speech enhancement method is learned during the training of the selection function. The SPPS was found to outperform the best performing speech enhancement method by approximately 3.3% in terms of word recognition rate (WRR).\"",
        "title: \"Affect Recognition in Real Life Scenarios\" with abstract: \"\n Affect awareness is important for improving human-computer interaction, but also facilitates the detection of atypical behaviours,\n danger, or crisis situations in surveillance and in human behaviour monitoring applications. The present work aims at the\n detection and recognition of specific affective states, such as panic, anger, happiness in close to real-world conditions.\n The affect recognition scheme investigated here relies on an utterance-level audio parameterization technique and a robust\n pattern recognition scheme based on the Gaussian Mixture Models with Universal Background Modelling (GMM-UBM) paradigm. We\n evaluate the applicability of the suggested architecture on the PROMETHEUS database, implemented in a number of indoor and\n outdoor conditions. The experimental results demonstrate the potential of the suggested architecture on the challenging task\n of affect recognition in real world conditions. However, further enhancement of the affect recognition performance would be\n needed before any deployment of practical applications.\n \n \"",
        "title: \"Automatic acoustic identification of crickets and cicadas\" with abstract: \"The general problem addressed in this work is automatic identification of insects using only the acoustic modality. In particular, we discuss the characteristics of the acoustic profiles of two target groups of insects: crickets and cicadas. Subsequently, we employ advanced machine learning techniques to categorize them on the levels of specific insect, family, subfamily, genus, and species. To deal with the sparse spectral representation of some species, we adopt a score-level fusion of classifiers with non-parametric (probabilistic neural network) and parametric (Gaussian mixture models) estimation of the probability density function. We apply this approach to a large and well documented catalogue of cricket and cicada recordings, and we report identification accuracy that exceeds 99% on the levels of singing insect and family, and 90% on the level of a species out of 220 species.\"",
        "1 is \"Toward adaptive conversational interfaces: Modeling speech convergence with animated personas\", 2 is \"Learning vector quantization for the probabilistic neural network\".",
        "\nGiven above information, for an author who has written the paper with the title \"INSPIRE: Evaluation of a Smart-Home System for Infotainment Management and Device Control\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01245": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'INSPIRE: Evaluation of a Smart-Home System for Infotainment Management and Device Control':",
        "title: \"Dealing with Imbalanced Data using Bayesian Techniques\" with abstract: \"For the present work, we deal with the significant problem of high imbalance in data in binary or multi-class classification problems. We study two different linguistic applications. The former determines whether a syntactic construction (environment) co-occurs with a verb in a natural text corpus consists a subcategorization frame of the verb or not. The latter is called Name Entity Recognition (NER) and it concerns determining whether a noun belongs to a specific Name Entity class. Regarding the subcategorization domain, each environment is encoded as a vector of heterogeneous attributes, where a very high imbalance between positive and negative examples is observed (an imbalance ratio of approximately 1:80). In the NER application, the imbalance between a name entity class and the negative class is even greater (1:120). In order to confront the plethora of negative instances, we suggest a search tactic during training phase that employs Tomek links for reducing unnecessary negative examples from the training set. Regarding the classification mechanism, we argue that Bayesian networks are well suited and we propose a novel network structure which efficiently handles heterogeneous attributes without discretization and is more classification-oriented. Comparing the experimental results with those of other known machine learning algorithms, our methodology performs significantly better in detecting examples of the rare class.\"",
        "title: \"MeteoBayes: Effective Plan Recognition in a Weather Dialogue System\" with abstract: \"When you've called a voice portal for any kind of information retrieval, chances are that an automated system guided the entire interaction. Perhaps it correctly identified your goal, but only after too many questions. MeteoBayes, a meteorological information dialogue system, lets users employ natural language to clarify their goals. MeteoBayes uses a Bayesian networks-based inference engine to establish user intentions by consulting its past dialogue repository. Because unknown terms are likely to come up, MeteoBayes's unknown term disambiguation module learns word similarities from texts to avoid unnecessary system inquiries, thus speeding up the understanding process.\"",
        "title: \"The MoveOn Motorcycle Speech Corpus\" with abstract: \"A speech and noise corpus dealing with the extreme conditions of the motorcycle environment is developed within the MoveOn project. Speech utterances in British English are recorded and processed approaching the issue of command and control and template driven dialog systems on the motorcycle. The major part of the corpus comprises noisy speech and environmental noise recorded on a motorcycle, but several clean speech recordings in a silent environment are also available. The corpus development focuses on distortion free recordings and accurate descriptions of both recorded speech and noise. Not only speech segments are annotated but also annotation of environmental noise is performed. The corpus is a small-sized speech corpus with about 12 hours of clean and noisy speech utterances and about 30 hours of segments with environmental noise without speech. This paper addresses the motivation and development of the speech corpus and finally presents some statistics and results of the database creation.\"",
        "title: \"Evaluating Intonational Features For Emotion Recognition From Speech\" with abstract: \"This paper presents and discusses the problem of emotion recognition from speech signals with the utilization of features bearing intonational information. In particular parameters extracted from Fujisaki's model of intonation are presented and evaluated. Machine learning models were build with the utilization of C4.5 decision tree inducer, instance based learner and Bayesian learning. The datascts utilized for the purpose of training machine learning models were extracted from two emotional databases of acted speech. Experimental results showed the effectiveness of Fujisaki's model attributes since they enhanced the recognition process for most of the emotion categories and learning approaches helping to the segregation of emotion categories.\"",
        "title: \"Text-Independent Speaker Verification for Real Fast-Varying Noisy Environments\" with abstract: \"Investigating Speaker Verification in real-world noisy environments, a novel feature extraction process suitable for suppression of time-varying noise is compared with a fine-tuned spectral subtraction method. The proposed feature extraction process is based on approximating the clean speech and the noise spectral magnitude with a mixture of Gaussian probability density functions (pdfs) by using the Expectation-Maximization algorithm (EM). Subsequently, the Bayesian inference framework is applied to the degraded spectral coefficients, and by employing Minimum Mean Square Error Estimation (MMSE), a closed form solution for the spectral magnitude estimation task is derived. The estimated spectral magnitude finally is incorporated into the Mel-Frequency Cepstral Coefficients (MFCCs) front-end of a baseline text-independent speaker verification system, based on Probabilistic Neural Networks, which participated successfully in the 2002 NIST (National Institute of Standards and Technology of USA) Speaker Recognition Evaluation. A comparative study of the proposed technique for real-world noise types demonstrates a significant performance gain compared to the baseline speech features and to the spectral subtraction enhancement method. Improvements of the absolute speaker verification performance with more than 27% for 0 dB signal-to-noise ratio (SNR), compared to the MFCCs, and with more than 13% for -5 dB SNR, compared to the spectral subtraction version, were obtained in the case of a passing-by aircraft scenario.\"",
        "1 is \"Comparative study of Automatic Phone Segmentation methods for TTS\", 2 is \"Automated UI evaluation based on a cognitive architecture and UsiXML.\".",
        "\nGiven above information, for an author who has written the paper with the title \"INSPIRE: Evaluation of a Smart-Home System for Infotainment Management and Device Control\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01246": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Marine environment monitoring using Wireless Sensor Networks: A systematic review':",
        "title: \"Agent-based collaborative product design engineering: an industrial case study\" with abstract: \"Globalization and rapid evolving of Internet and Web-based technologies have revolutionized the product development process. Engineering a product is a complex process involving the integration of distributed resources, such as human beings, engineering tools, and a large variety of product-related information systems. Software agents have been playing an increasingly important role in this area to reduce the need for large, complex, and centralized systems. This paper presents the results of an industrial case study in the development of a collaborative e-Engineering environment for mechanical product design engineering by applying intelligent software agents, Internet/Web, workflow, and database technologies. A software prototype system has been implemented on a FIPA-compliant agent platform, with a wheel-axle assembly being used as a test case for system validation.\"",
        "title: \"A methodology for developing clinical collaborative communication systems.\" with abstract: \"As healthcare organizations are struggling to enhance healthcare qualities and efficiencies, they focus on secure and efficient sharing of clinical information among healthcare providers including doctors, nurses, medical laboratories, and informal caregivers, throughout the entire healthcare life cycle. Any failure or error during this clinical collaborative communication can have negative impact on the efficiencies of the whole healthcare delivery system. This paper suggests a methodological approach to address the needs in this area and proposes a methodology for developing clinical collaborative communication (C systems. Since C demands a variety of collaborative communications among multiple healthcare stakeholders, the proposed approach focuses on the effective system design and analysis with the objective to enhance collaboration and communication in healthcare organizations.\"",
        "title: \"Agent Based Dynamic Information Gathering And Organization For Distributed Product Development\" with abstract: \"The concept of Web-based Visualized Product Structure Tree (VPST) has been proposed to realize a platform-independent, visualized and integrated product information sharing. This paper extends this concept by developing all agent based dynamic information gathering and organization framework to support updating the VPST dynamically. The objective is to enable different users to perform real time collaboration over the Web. The proposed framework is composed of three modules: Information Gathering, Information Organization Server and Information Sharing Server. Each of them is composed of a number of agents. The paper presents a detailed design and implementation of the proposed framework and its three modules.\"",
        "title: \"An weighted ontology-based semantic similarity algorithm for web service\" with abstract: \"A critical step in the process of reusing existing WSDL-specified services for building web-based applications is the discovery of potentially relevant services. However, the category-based service discovery, such as UDDI, is clearly insufficient. Semantic web services, augmenting web service descriptions using semantic web technology, were introduced to facilitate the publication, discovery, and execution of web services at the semantic level. Semantic matchmaker enhances the capability of UDDI service registries in the semantic web services architecture by applying some matching algorithms between advertisements and requests described in OWL-S to recognize various degrees of matching for web services. Based on semantic web service framework, semantic matchmaker and probabilistic matching approach, this paper proposes a weighted ontology-based semantic similarity algorithm for web service to support a more automated and veracious service discovery and rank process in the semantic web service framework.\"",
        "title: \"A Java 3d-enabled cyber workspace\" with abstract: \"Along with the browser paradigm, Java has fundamentally changed the work environment, helping produce compelling applications for collaborating over the Internet.\"",
        "1 is \"Transmitter Identification Signal Analyzer for Single Frequency Network\", 2 is \"Detecting and resolving conflicts among cooperating human and machine-based design agents\".",
        "\nGiven above information, for an author who has written the paper with the title \"Marine environment monitoring using Wireless Sensor Networks: A systematic review\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01247": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Tools for Inventing Organizations: Toward a Handbook of Organizational Processes':",
        "title: \"Core-Periphery Communication and the Success of Free/Libre Open Source Software Projects.\" with abstract: \"We examine the relationship between communications by core and peripheral members and Free/Libre Open Source Software project success. The study uses data from 74 projects in the Apache Software Foundation Incubator. We conceptualize project success in terms of success building a community, as assessed by graduation from the Incubator. We compare successful and unsuccessful projects on volume of communication and on use of inclusive pronouns as an indication of efforts to create intimacy among team members. An innovation of the paper is that use of inclusive pronouns is measured using natural language processing techniques. We also compare the volume and content of communication produced by core (committer) and peripheral members and by those peripheral members who are later elected to be core members. We find that volume of communication is related to project success but use of inclusive pronouns does not distinguish successful projects. Core members exhibit more contribution and use of inclusive pronouns than peripheral members.\"",
        "title: \"Motivation and Data Quality in a Citizen Science Game: A Design Science Evaluation\" with abstract: \"Citizen science is a form of social computation where members of the public are recruited to contribute to scientific investigations. Citizen-science projects often use web-based systems to support collaborative scientific activities. However, finding ways to attract participants and ensure the accuracy of the data they produce are key issues in making such systems successful. In this paper we describe the design and preliminary evaluation of a simple game that addresses these two concerns for the task of species identification.\"",
        "title: \"Lessons From Volunteering And Free/Libre Open Source Software Development For The Future Of Work\" with abstract: \"In this paper, we review research on voluntary organizations to identify key features of and problems in volunteer work and organizations. We then use the example of free/libre open source software (FLOSS) development teams to examine how those features and problems apply in this situation and how they might be affected by the use of information and communications technologies (ICT). We suggest that understanding volunteer organizations can illuminate the changing nature of all knowledge work, paid as well as unpaid.\"",
        "title: \"Cognitive science and organizational design: a case study of computer conferencing\" with abstract: \"Many researchers have investigated and speculated about the link between information technology and organizational structure with very mixed results. This paper suggests that part of the reason for these mixed results is the coarseness of previous analyses of both technology and structure. The paper describes a new and much more detailed perspective for investigating this link. Using concepts of object-oriented programming from artificial intelligence, the information processing that occurs in organizations is characterized in terms of the kinds of messages people exchange and the ways they process those messages. The utility of this approach is demonstrated through the analysis of a case in which a reduction in levels of management is coupled with the introduction of a computer conferencing system. The detailed model developed for this case helps explain both macro-level data about thd changes in the organizational structure, and micro-level data about individuals' use of the system.\"",
        "title: \"The interdisciplinary study of coordination\" with abstract: \"This survey characterizes an emerging research area, sometimes called coordination theory, that focuses on the interdisciplinary study of coordination. Research in this area uses and extends ideas about coordination from disciplines such as computer science, organization theory, operations research, economics, linguistics, and psychology.A key insight of the framework presented here is that coordination can be seen as the process of managing dependencies among activities. Further progress, therefore, should be possible by characterizing different kinds of dependencies and identifying the coordination processes that can be used to manage them. A variety of processes are analyzed from this perspective, and commonalities across disciplines are identified. Processes analyzed include those for managing shared resources, producer/consumer relationships, simultaneity constraints, and task/subtask dependencies.Section 3 summarizes ways of applying a coordination perspective in three different domains:(1) understanding the effects of information technology on human organizations and markets, (2) designing cooperative work tools, and (3) designing distributed and parallel computer systems. In the final section, elements of a research agenda in this new area are briefly outlined.\"",
        "1 is \"INTERNET-BASED INTERMEDIARIES - THE CASE OF THE REAL ESTATE MARKET\", 2 is \"A formal basis for architectural connection\".",
        "\nGiven above information, for an author who has written the paper with the title \"Tools for Inventing Organizations: Toward a Handbook of Organizational Processes\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01248": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Aspects of Rumor Spreading on a Microblog Network':",
        "title: \"Interplay between Social Influence and Network Centrality: A Comparative Study on Shapley Centrality and Single-Node-Influence Centrality.\" with abstract: \"We study network centrality based on dynamic influence propagation models in social networks. To illustrate our integrated mathematical-algorithmic approach for understanding the fundamental interplay between dynamic influence processes and static network structures, we focus on two basic centrality measures: (a) Single Node Influence (SNI) centrality, which measures each node's significance by its influence spread; and (b) Shapley Centrality, which uses the Shapley value of the influence spread function --- formulated based on a fundamental cooperative-game-theoretical concept --- to measure the significance of nodes. We present a comprehensive comparative study of these two centrality measures. Mathematically, we present axiomatic characterizations, which precisely capture the essence of these two centrality measures and their fundamental differences. Algorithmically, we provide scalable algorithms for approximating them for a large family of social-influence instances. Empirically, we demonstrate their similarity and differences in a number of real-world social networks, as well as the efficiency of our scalable algorithms. Our results shed light on their applicability: SNI centrality is suitable for assessing individual influence in isolation while Shapley centrality assesses individuals' performance in group influence settings.\n\n\"",
        "title: \"An Issue in the Martingale Analysis of the Influence Maximization Algorithm IMM.\" with abstract: \"This paper explains a subtle issue in the martingale analysis of the IMM algorithm, a state-of-the-art influence maximization algorithm. Two workarounds are proposed to fix the issue, both requiring minor changes on the algorithm and incurring a slight penalty on the running time of the algorithm.\"",
        "title: \"Influence Maximization in Social Networks When Negative Opinions May Emerge and Propagate.\" with abstract: \"Influence maximization, defined by Kempe, Kleinberg, and Tardos (2003), is the problem of finding a small set of seed nodes in a social network that maximizes the spread of influence under certain influence cascade models. In this paper, we propose an extension to the independent cascade model that incorporates the emergence and propagation of negative opinions. The new model has an explicit parameter called quality factor to model the natural behavior of people turning negative to a product due to product defects. Our model incorporates negativity bias (negative opinions usually dominate over positive opinions) commonly acknowledged in the social psychology literature. The model maintains some nice properties such as submodularity, which allows a greedy approximation algorithm for maximizing positive influence within a ratio of 1 - 1/e. We define a quality sensitivity ratio (qs-ratio) of influence graphs and show a tight bound of \u03b8(\u221an/k) on the qs-ratio, where n is the number of nodes in the network and k is the number of seeds selected, which indicates that seed selection is sensitive to the quality factor for general graphs. We design an efficient algorithm to compute influence in tree structures, which is nontrivial due to the negativity bias in the model. We use this algorithm as the core to build a heuristic algorithm for influence maximization for general graphs. Through simulations, we show that our heuristic algorithm has matching influence with a standard greedy approximation algorithm while being orders of magnitude faster. Copyright \u00a9 SIAM.\"",
        "title: \"A compact routing scheme and approximate distance oracle for power-law graphs\" with abstract: \"Compact routing addresses the tradeoff between table sizes and stretch, which is the worst-case ratio between the length of the path a packet is routed through by the scheme and the length of an actual shortest path from source to destination. We adapt the compact routing scheme by Thorup and Zwick [2001] to optimize it for power-law graphs. We analyze our adapted routing scheme based on the theory of unweighted random power-law graphs with fixed expected degree sequence by Aiello et al. [2000]. Our result is the first analytical bound coupled to the parameter of the power-law graph model for a compact routing scheme. Let n denote the number of nodes in the network. We provide a labeled routing scheme that, after a stretch--5 handshaking step (similar to DNS lookup in TCP/IP), routes messages along stretch--3 paths. We prove that, instead of routing tables with \u00d5(n1/2) bits (\u00d5 suppresses factors logarithmic in n) as in the general scheme by Thorup and Zwick, expected sizes of O(n\u03b3 log n) bits are sufficient, and that all the routing tables can be constructed at once in expected time O(n1+\u03b3 log n), with \u03b3 = \u03c4-22/\u03c4-3 + &epsiv;, where \u03c4\u2208(2,3) is the power-law exponent and &epsiv; 0 (which implies &epsiv; < \u03b3 < 1/3 + &epsiv;). Both bounds also hold with probability at least 1-1/n (independent of &epsiv;). The routing scheme is a labeled scheme, requiring a stretch--5 handshaking step. The scheme uses addresses and message headers with O(log n log log n) bits, with probability at least 1-o(1). We further demonstrate the effectiveness of our scheme by simulations on real-world graphs as well as synthetic power-law graphs. With the same techniques as for the compact routing scheme, we also adapt the approximate distance oracle by Thorup and Zwick [2001, 2005] for stretch-3 and we obtain a new upper bound of expected \u00d5(n1+\u03b3) for space and preprocessing for random power-law graphs. Our distance oracle is the first one optimized for power-law graphs. Furthermore, we provide a linear-space data structure that can answer 5--approximate distance queries in time at most \u00d5(n1/4+&epsiv;) (similar to \u03b3, the exponent actually depends on \u03c4 and lies between &epsiv; and 1/4 + &epsiv;).\"",
        "title: \"Stochastic Online Learning With Probabilistic Graph Feedback\" with abstract: \"We consider a problem of stochastic online learning with general probabilistic graph feedback, where each directed edge in the feedback graph has probability p(ij). Two cases are covered. (a) The one-step case, where after playing arm i the learner observes a sample reward feedback of arm j with independent probability p(ij). (b) The cascade case where after playing arm i the learner observes feedback of all arms j in a probabilistic cascade starting from i - for each (i, j) with probability p(ij), if arm i is played or observed, then a reward sample of arm j would be observed with independent probability p(ij). Previous works mainly focus on deterministic graphs which corresponds to one-step case with p(ij) is an element of {0, 1), an adversarial sequence of graphs with certain topology guarantees, or a specific type of random graphs. We analyze the asymptotic lower bounds and design algorithms in both cases. The regret upper bounds of the algorithms match the lower bounds with high probability.\"",
        "1 is \"Monotone equilibria in Bayesian games of strategic complementarities\", 2 is \"Tighter Bounds for Facility Games\".",
        "\nGiven above information, for an author who has written the paper with the title \"Aspects of Rumor Spreading on a Microblog Network\", which reference is related? Just choose 1 or 2! No explanation."
    ],
    "01249": [
        "Here are the documents ranked by relevance, with titles and keywords, from most relevant to least relevant, for the topic of 'Min-cost multicast networks in Euclidean space':",
        "title: \"Online Vertex Cover and Matching: Beating the Greedy Algorithm\" with abstract: \"  In this paper, we explicitly study the online vertex cover problem, which is a natural generalization of the well-studied ski-rental problem. In the online vertex cover problem, we are required to maintain a monotone vertex cover in a graph whose vertices arrive online. When a vertex arrives, all its incident edges to previously arrived vertices are revealed to the algorithm. For bipartite graphs with the left vertices offline (i.e. all of the left vertices arrive first before any right vertex), there are algorithms achieving the optimal competitive ratio of $\\frac{1}{1-1/e}\\approx 1.582$.   Our first result is a new optimal water-filling algorithm for this case. One major ingredient of our result is a new charging-based analysis, which can be generalized to attack the online fractional vertex cover problem in general graphs. The main contribution of this paper is a 1.901-competitive algorithm for this problem. When the underlying graph is bipartite, our fractional solution can be rounded to an integral solution. In other words, we can obtain a vertex cover with expected size at most 1.901 of the optimal vertex cover in bipartite graphs.   The next major result is a primal-dual analysis of our algorithm for the online fractional vertex cover problem in general graphs, which implies the dual result of a 0.526-competitive algorithm for online fractional matching in general graphs. Notice that both problems admit a well-known 2-competitive greedy algorithm. Our result in this paper is the first successful attempt to beat the greedy algorithm for these two problems.   On the hardness side, we show that no randomized online algorithm can achieve a competitive ratio better than 1.753 and 0.625 for the online fractional vertex cover problem and the online fractional matching problem respectively, even for bipartite graphs. \"",
        "title: \"Envy-free pricing with general supply constraints\" with abstract: \"The envy-free pricing problem can be stated as finding a pricing and allocation scheme in which each consumer is allocated a set of items that maximize her utility under the pricing. The goal is to maximize seller revenue.We study the problem with general supply constraints which are given as an independence system1 defined over the items. The constraints, for example, can be a number of linear constraints or matroids. This captures the situation where items do not pre-exist, but are produced in reflection of consumer valuation of the items under the limit of resources. This paper focuses on the case of unit-demand consumers. In the setting, there are n consumers and m items; each item may be produced in multiple copies. Each consumer i \u2208 [n] has a valuation vij on item j in the set Si in which she is interested. She must be allocated (if any) an item which gives the maximum (non-negative) utility. Suppose we are given an a-approximation oracle for finding the maximum weight independent set for the given independence system (or a slightly stronger oracle); for a large number of natural and interesting supply constraints, constant approximations are available. We obtain the following results. - O(\u03b1 log n)-approximation for the general case. - O(\u03b1k)-approximation when each consumer is interested in at most k distinct types of items. - O(\u03b1f)-approximation when each item is interesting to at most f consumers. Note that the final two results were previously unknown even without the independence system constraint.\"",
        "title: \"On Maximizing Tree Bandwidth for Topology-Aware Peer-to-Peer Streaming\" with abstract: \"In recent years, there has been an increasing interest in peer-to-peer (P2P) multimedia streaming. In this paper, we consider constructing a high-bandwidth overlay tree for streaming services. We observe that underlay information such as link connectivity and link bandwidth is important in tree construction, because two seemingly disjoint overlay paths may share common links on the underlay. We hence study how to construct a high-bandwidth overlay tree given the underlay topology. We formulate the problem as building a Maximum Bandwidth Multicast Tree (MBMT) or a Minimum Stress Multicast Tree (MSMT), depending on whether link bandwidth is available or not. We prove that both problems are NP-hard and are not ap-proximable within a factor of (2/3 + epsiv), for any epsiv > 0, unless P = NP. We then present approximation algorithms to address them and analyze the algorithm performance. Furthermore, we discuss some practical issues (e.g., group dynamics, resilience and scalability) in system implementation. We evaluate our algorithms on Internet-like topologies. The results show that our algorithms can achieve high tree bandwidth and low link stress with low penalty in end-to-end delay. Measurement study based on Plan-etLab further confirms this. Our study shows that the knowledge of underlay is important for constructing efficient overlay trees.\"",
        "title: \"Constant-Work-Space algorithm for a shortest path in a simple polygon\" with abstract: \"We present two space-efficient algorithms. First, we show how to report a simple path between two arbitrary nodes in a given tree. Using a technique called \u201ccomputing instead of storing\u201d, we can design a naive quadratic-time algorithm for the problem using only constant work space, i.e., O(logn) bits in total for the work space, where n is the number of nodes in the tree. Then, another technique \u201ccontrolled recursion\u201d improves the time bound to O(n1+\u03b5) for any positive constant \u03b5. Second, we describe how to compute a shortest path between two points in a simple n-gon. Although the shortest path problem in general graphs is NL-complete, this constrained problem can be solved in quadratic time using only constant work space.\"",
        "title: \"Shadow configuration as a network management primitive\" with abstract: \"Configurations for today's IP networks are becoming increasingly complex. As a result, configuration management is becoming a major cost factor for network providers and configuration errors are becoming a major cause of network disruptions. In this paper, we present and evaluate the novel idea of shadow configurations. Shadow configurations allow configuration evaluation before deployment and thus can reduce potential network disruptions. We demonstrate using real implementation that shadow configurations can be implemented with low overhead.\"",
        "1 is \"Integrating concept ontology and multitask learning to achieve more effective classifier training for multilevel image annotation.\", 2 is \"A Survey Of Visual Sensor Networks\".",
        "\nGiven above information, for an author who has written the paper with the title \"Min-cost multicast networks in Euclidean space\", which reference is related? Just choose 1 or 2! No explanation."
    ]
}