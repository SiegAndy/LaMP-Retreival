[
    {
        "id": "000",
        "input": "For an author who has written the paper with the title \"Algorithms for routing around a rectangle\", which reference is related? Just answer with [1] or [2] without explanation. [1]: \"Reliable broadcasting in product networks with Byzantine faults\" [2]: \"Auctions with heterogeneous items and budget limits\"",
        "profile": [
            {
                "title": "Convex Drawings of Internally Triconnected Plane Graphs on O(n2) Grids",
                "abstract": "In a convex grid drawing of a plane graph, every edge is drawn as a straight-line segment without any edge-intersection, every vertex is located at a grid point, and every facial cycle is drawn as a convex polygon. A plane graph G has a convex drawing if and only if G is internally triconnected. It has been known that an internally triconnected plane graph G of n vertices has a convex grid drawing on a grid of O(n 3) area if the triconnected component decomposition tree of G has at most four leaves. In this paper, we improve the area bound O(n 3) to O(n 2), which is optimal up to a constant factor. More precisely, we show that G has a convex grid drawing on a 2n\u81334n grid. We also present an algorithm to find such a drawing in linear time.",
                "id": "0000"
            },
            {
                "title": "Best Security Index for Digital Fingerprinting",
                "abstract": "Digital watermarking used for fingerprinting may receive a collusion attack; two or more users collude, compare their data, find a part of embedded watermarks, and make an unauthorized copy by masking their identities. In this paper, assuming that at most c users collude, we give a characterization of the fingerprinting codes that have the best security index in a sense of \"(c,p/q)-secureness\" proposed by Orihara et al. The characterization is expressed in terms of intersecting families of sets. Using a block design, we also show that a distributor of data can only find asymptotically a set of c users including at least one culprit, no matter how good fingerprinting code is used.",
                "id": "0001"
            },
            {
                "title": "No-bend orthogonal drawings of series-parallel graphs",
                "abstract": "In a no-bend orthogonal drawing of a plane graph, each vertex is drawn as a point and each edge is drawn as a single horizontal or vertical line segment. A planar graph is said to have a no-bend orthogonal drawing if at least one of its plane embeddings has a no-bend orthogonal drawing. Every series-parallel graph is planar. In this paper we give a linear-time algorithm to examine whether a series-parallel graph G of the maximum degree three has a no-bend orthogonal drawing and to find one if G has.",
                "id": "0002"
            },
            {
                "title": "Approximation Algorithms for Bandwidth Consecutive Multicolorings - (Extended Abstract).",
                "abstract": "Let G be a graph in which each vertex v has a positive integer weight b(v) and each edge (v, w) has a nonnegative integer weight b(v, w). A bandwidth consecutive multicoloring, simply called a b-coloring of G, assigns each vertex v a specified number b(v) of consecutive positive integers as colors of v so that, for each edge (v, w), all integers assigned to vertex v differ from all integers assigned to vertex w by more than b(v, w). The maximum integer assigned to vertices is called the span of the coloring. The b-coloring problem asks to find a b-coloring of a given graph G with the minimum span. In the paper, we present four efficient approximation algorithms for the problem, which have theoretical performance guarantees for the computation time, the span of a found b-coloring and the approximation ratio. We also obtain several upper bounds on the minimum span, expressed in terms of the maximum b-degrees, one of which is an extension of Brooks' theorem on an ordinary coloring.",
                "id": "0003"
            },
            {
                "title": "Decompositions to degree-constrained subgraphs are simply reducible to edge-colorings",
                "abstract": "The degree-constrained subgraphs decomposition problem, such as an f -coloring, an f -factorization, and a [ g , f ]-factorization, is to decompose a given graph G =( V , E ) to edge-disjoint subgraphs degree-constrained by integer-valued functions f and g on V . In this paper we show that the problem can be simply reduced to the edge-coloring problem in polynomial-time. That is, for any positive integer k , we give a polynomial-time transformation of G to a new graph such that G can be decomposed to at most k degree-constrained subgraphs if and only if the new graph can be edge-colored with k colors.",
                "id": "0004"
            },
            {
                "title": "Generalized Vertex-Rankings of Partial k-trees",
                "abstract": "A c-vertex-ranking of a graph G for a positive integer c is a labeling of the vertices of G with integers such that, for any label i, deletion of all vertices with labels > i leaves connected components, each having at most c vertices with label i. We present a polynomial-time algorithm to find a c-vertex-ranking of a partial k-tree using the minimum number of ranks for any bounded integers c and k.",
                "id": "0005"
            },
            {
                "title": "A Linear Algorithm for Finding Total Colorings of Partial k-Trees",
                "abstract": "A total coloring of a graph G is a coloring of all elements of G, i.e. vertices and edges, in such a way that no two adjacent or incident elements receive the same color. The total coloring problem is to find a total coloring of a given graph with the minimum number of colors. Many combinatorial problems can be efficiently solved for partial k-trees, i.e., graphs with bounded tree-width. However, no efficient algorithm has been known for the total coloring problem on partial k-trees although a polynomial-time algorithm of very high order has been known. In this paper, we give a linear-time algorithm for the total coloring problem on partial k-trees with bounded.",
                "id": "0006"
            },
            {
                "title": "Finding Independent Spanning Trees in Partial k-Trees",
                "abstract": "Spanning trees rooted at a vertex r of a graph G are independent if, for each vertex v in G, all the paths connecting v and r in the trees are pairwise internally disjoint. In this paper we give a linear-time algorithm to find the maximum number of independent spanning trees rooted at any given vertex r in partial k-trees G, that is, graphs G with tree-width bounded by a constant k.",
                "id": "0007"
            },
            {
                "title": "Partitioning Trees With Supply, Demand And Edge-Capacity",
                "abstract": "Let T be a given tree. Each vertex of T is either a supply vertex or a demand vertex, and is assigned a positive number, called the supply or demand. Each demand vertex v must be supplied an amount of \"power,\" equal to the demand of a, from exactly one supply vertex through edges in T. Each edge is assigned a positive number called the capacity. One wishes to partition T into subtrees by deleting edges from T so that each subtree contains exactly one supply vertex whose supply is no less than the sum of all demands in the subtree and the power flow through each edge is no more than capacity of the edge. The \"partition problem\" is a decision problem to ask whether T has such a partition. The \"maximum partition problem\" is an optimization version of the partition problem. In this paper, we give three algorithms for the problems. First is a linear-time algorithm for the partition problem. Second is a pseudo-polynomial-time algorithm for the maximum partition problem. Third is a fully polynomial-time approximation scheme (FPTAS) for the maximum partition problem.",
                "id": "0008"
            },
            {
                "title": "Partitioning Trees of Supply and Demand",
                "abstract": "Assume that a tree T has a number n\n s of \u201csupply vertices\u201d and all the other vertices are \u201cdemand vertices.\u201d Each supply vertex is assigned a positive number called\n a supply, while each demand vertex is assigned a positive number called a demand. One wish to partition T into exactly n\n s subtrees by deleting edges from T so that each subtree contains exactly one supply vertex whose supply is no less than the sum of demands of all demand vertices\n in the subtree. The \u201cpartition problem\u201d is a decision problem to ask whether T has such a partition. The \u201cmaximum partition problem\u201d is an optimization version of the partition problem. In this paper,\n we give three algorithms for the problems. First is a linear-time algorithm for the partition problem. Second is a pseudo-polynomial-time\n algorithm for the maximum partition problem. Third is a fully polynomial-time approximation scheme (FPTAS) for the maximum\n partition problem.\n ",
                "id": "0009"
            },
            {
                "title": "A better than \u201cbest possible\u201d algorithm to edge color multigraphs",
                "abstract": "By a result of Holyer, unless P = NP, there does not exist a polynomial-time approximation algorithm to edge color a multigraph that always uses fewer than 43 \u03c7\u2032 colors, where \u03c7\u2032 is the optimal number of colors. This makes it appear that finding provably good edge colorings is extremely difficult. However, in this paper we present an algorithm to find an edge coloring of a multigraph that never uses more than \u230a98\u03c7\u2032 + 34\u230b colors. In addition, if \u03c7\u2032 \u2265 \u230a98\u0394 + 34\u230b then the algorith optimally colors the graph in polynomial time. Furthermore, this algorithm never uses more than (43)\u03c7\u2032 colors and runs in O(|E|(|V| + \u0394)) time, where E is the set of edges, and V is the set of vertices.",
                "id": "00010"
            },
            {
                "title": "Necessary and Sufficient Numbers of Cards for Sharing Secret Keys on Hierarchical Groups",
                "abstract": "Suppose that there are players in two hierarchical groups and a computationally unlimited eavesdropper. Using a random deal of cards, a player in the higher group wishes to send a one-bit message information-theoretically securely either to all the players in her group or to all the players in the two groups. This can be done by the socalled 2-level key set protocol. In this paper we give a necessary and sufficient condition for the 2-level key set protocol to succeed.",
                "id": "00011"
            },
            {
                "title": "Finding Steiner forests in planar graphs",
                "abstract": "No abstract available.\n\n",
                "id": "00012"
            },
            {
                "title": "Edge-Coloring and f-Coloring for Various Classes of Graphs",
                "abstract": "In an ordinary edge-coloring of a graph each color appears at each vertex at most once. An f-coloring is a generalized edge-coloring in which each color appears at each vertex v at most f(v) times where f(v) is a positive integer assigned to v. This paper gives ecient sequential and parallel algorithms to nd ordinary edge-colorings and f-colorings for various classes of graphs such as bipartite graphs, planar graphs, and graphs having xed degeneracy, tree-width, genus, arboricity, unicyclic index or thickness.",
                "id": "00013"
            },
            {
                "title": "A Revised Transformation Protocol for Unconditionally Secure Secret Key Exchange",
                "abstract": "The transformation protocol can allow two players to share an unconditionally secure secret key using a random deal of cards. A\u00a0sufficient condition on the number of cards for the transformation protocol to succeed was known. However, it has been an open problem to obtain a necessary and sufficient condition. This paper improves the transformation protocol and gives a necessary and sufficient condition for the resulting protocol to succeed.",
                "id": "00014"
            },
            {
                "title": "Grid Drawings of 4-Connected Plane Graphs",
                "abstract": "A grid drawing of a plane graph G is a drawing of G on the plane so that all vertices of G are put on plane grid points and all edges are drawn as straight line segments between their endpoints without any edge-intersection. In this paper we give a very simple algorithm to find a grid drawing of any given 4-connected plane graph G with four or more vertices on the outer face. The algorithm takes time O(n) and yields a drawing in a rectangular grid of width \\\\lceil n/2 \\\\rceil - 1 and height \\\\lfloor n/2\\\\rfloor if G has n vertices. The algorithm is best possible in the sense that there are an infinite number of 4-connected plane graphs, any grid drawings of which need rectangular grids of width \\\\lceil n/2 \\\\rceil - 1 and height \\\\lfloor n/2\\\\rfloor .",
                "id": "00015"
            },
            {
                "title": "Algorithms for Finding f-Colorings of Partial k-Trees",
                "abstract": "In an ordinary edge-coloring of a graph G=(V, E) each color appears at each vertex v  V at most once. An f-coloring is a generalized edge-coloring in which each color appears at each vertex v  V at most f(v) times, where f(v) is a positive integer assigned to v. This paper gives a linear-time sequential algorithm and an optimal parallel algorithm which find an f-coloring of a givenpartial k-tree with the minimum number of colors.",
                "id": "00016"
            },
            {
                "title": "Lower bounds on the cardinality of the maximum matchings of planar graphs",
                "abstract": "Lower bounds on the cardinality of the maximum matchings of planar graphs, with a constraint on the minimum degree, are established in terms of a linear polynomial of the number of vertices. The bounds depend upon the minimum degree and the connectivity of graphs. Some examples are given which show that all the lower bounds are best possible in the sense that neither the coefficients nor the constant terms can be improved.",
                "id": "00017"
            },
            {
                "title": "Optimal parallel algorithm for edge-coloring partial k-trees with bounded degrees",
                "abstract": "Many combinatorial problems can be efficiently solved for partial k-trees (graphs of treewidth bounded by k). The edge-coloring problem is one of the well-known combinatorial problems for which no NC algorithms have been obtained for partial k-trees. This paper gives an optimal and first NC parallel algorithm to find an edge-coloring of any given partial k-tree using a minimum number of colors if k and the maximum degree \u0394 are bounded",
                "id": "00018"
            },
            {
                "title": "Orthogonal Drawings of Plane Graphs Without Bends",
                "abstract": "In an orthogonal drawing of a plane graph each vertex is drawn as a point and each edge is drawn as a sequence of vertical and horizontal line segments. A bend is a point at which the drawing of an edge changes its direction. Every plane graph of the maximum degree at most four has an orthogonal drawing, but may need bends. A simple necessary and sufficient condition has not been known for a plane graph to have an orthogonal drawing without bends. In this paper we obtain a necessary and sufficient condition for a plane graph G of the maximum degree three to have an orthogonal drawing without bends. We also give a linear-time algorithm to find such a drawing of G if it exists.",
                "id": "00019"
            },
            {
                "title": "List total colorings of series-parallel graphs",
                "abstract": "A total coloring of a graph G is a coloring of all elements of G, i.e. vertices and edges, in such a way that no two adjacent or incident elements receive the same color. Let L(x) be a set of colors assigned to each element x of G. Then a list total coloring of G is a total coloring such that each element x receives a color contained in L(x). The list total coloring problem asks whether G has a list total coloring. In this paper, we first show that the list total coloring problem is NP-complete even for series-parallel graphs. We then give a sufficient condition for a series-parallel graph to have a list total coloring, that is, we prove a theorem that any series-parallel graph G has a list total coloring if |L(v)| \u2265 min{5, \u0394 + 1} for each vertex v and |L(e)| \u2265 max{5, d(v) + 1, d(w)+1} for each edge e = vw, where \u0394 is the maximum degree of G and d(/v) and d(w) are the degrees of the ends v and w of e, respectively. The theorem implies that any series-parallel graph G has a total coloring with \u0394 + 1 colors if \u0394 \u2265 4. We finally present a linear-time algorithm to find a list total coloring of a given series-parallel graph G if G satisfies the sufficient condition.",
                "id": "00020"
            },
            {
                "title": "Partitioning a Multi-Weighted Graph to Connected Subgraphs of Almost Uniform Size",
                "abstract": "Assume that each vertex of a graph G is assigned a constant number q of nonnegative integer weights, and that q pairs of nonnegative integers li and ui, 1 \u2264 i \u2264 q, are given. One wishes to partition G into connected components by deleting edges from G so that the total i-th weights of all vertices in each component is at least li and at most ui for each index i, 1 \u2264 i \u2264 q. The problem of finding such a \"uniform\" partition is NP-hard for series-parallel graphs, and is strongly NP-hard for general graphs even for q = 1. In this paper we show that the problem and many variants can be solved in pseudo-polynomial time for series-parallel graphs and partial k-trees, that is, graphs with bounded tree-width.",
                "id": "00021"
            },
            {
                "title": "An Approximation Algorithm For The Hamiltonian Walk Problem On Maximal Planar Graphs",
                "abstract": "A hamiltonian walk of a graph is a shortest closed walk that passes through every vertex at least once, and the length is the total number of traversed edges. The hamiltonian walk problem in which one would like to find a hamiltonian walk of a given graph is NP-complete. The problem is a generalized hamiltonian cycle problem and is a special case of the traveling salesman problem. Employing the techniques of divide-and-conquer and augmentation, we present an approximation algorithm for the problem on maximal planar graphs. The algorithm finds, in O( p 2 ) time, a closed spanning walk of a given arbitrary maximal planar graph, and the length of the obtained walk is at most 3 2 ( p \u2212 3) if the graph has p (\u2265 9) vertices. Hence the worst-case bound is 3 2 .",
                "id": "00022"
            },
            {
                "title": "A Linear Algorithm for Bend-Optimal Orthogonal Drawings of Triconnected Cubic Plane Graphs",
                "abstract": "An orthogonal drawing of a plane graph G is a drawing of G in which each edge is drawn as a sequence of alternate horizontal and vertical line segments. In this paper we give a linear-time algorithm to nd an or- thogonal drawing of a given 3-connected cubic plane graph with the min- imum number of bends. The best previously known algorithm takes time O(n7=4 p logn) for any plane graph with n vertices.",
                "id": "00023"
            },
            {
                "title": "Size-energy tradeoffs for unate circuits computing symmetric Boolean functions",
                "abstract": "A unate gate is a logical gate computing a unate Boolean function, which is monotone in each variable. Examples of unate gates are AND gates, OR gates, NOT gates, threshold gates, etc. A unate circuit C is a combinatorial logic circuit consisting of unate gates. Let f be a symmetric Boolean function of n variables, such as the Parity function, MOD function, and Majority function. Let m\"0 and m\"1 be the maximum numbers of consecutive 0's and consecutive 1's in the value vector of f, respectively, and let l=min{m\"0,m\"1} and m=max{m\"0,m\"1}. Let C be a unate circuit computing f. Let s be the size of the circuit C, that is, C consists of s unate gates. Let e be the energy of C, that is, e is the maximum number of gates outputting ''1'' over all inputs to C. In this paper, we show that there is a tradeoff between the size s and the energy e of C. More precisely, we show that (n+1-l)/m@?s^e. We also present lower bounds on the size s of C represented in terms of n, l and m. Our tradeoff immediately implies that logn@?elogs for every unate circuit C computing the Parity function of n variables.",
                "id": "00024"
            },
            {
                "title": "Minimizing AND-EXOR Expressions for Multiple-Valued Two-Input Logic Functions",
                "abstract": "A minimum ESOP (Exclusive-OR Sum-of-Products) form of a logic function f is an AND-EXOR 2-level expression of f having the minimum number of product terms. In the paper we deal with multiple-valued 2-input logic functions f , and give an algorithm to find a minimum ESOP form of a given function f in polynomial time.",
                "id": "00025"
            },
            {
                "title": "Partitioning a weighted graph to connected subgraphs of almost uniform size",
                "abstract": "Assume that each vertex of a graph G is assigned a nonnegative integer weight and that l and u are nonnegative integers. One wish to partition G into connected components by deleting edges from G so that the total weight of each component is at least l and at most u. Such an \u201calmost uniform\u201d partition is called an $(l, u) \\mbox{-}$partition. We deal with three problems to find an $(l, u) \\mbox{-}$partition of a given graph. The minimum partition problem is to find an $(l, u) \\mbox{-}$partition with the minimum number of components. The maximum partition problem is defined similarly. The p-partition problem is to find an $(l, u) \\mbox{-}$partition with a fixed number p of components. All these problems are NP-complete or NP-hard even for series-parallel graphs. In this paper we show that both the minimum partition problem and the maximum partition problem can be solved in time O(u4n) and the p-partition problem can be solved in time O(p2u4n) for any series-parallel graph of n vertices. The algorithms can be easily extended for partial k-trees, that is, graphs with bounded tree-width.",
                "id": "00026"
            },
            {
                "title": "An NC parallel algorithm for edge-coloring series-parallel multigraphs",
                "abstract": "Many combinatorial problems can be efficiently solved in parallel for series parallel multigraphs. The edge-coloring problem is one of a few combinatorial problems for which no NC parallel algorith...",
                "id": "00027"
            },
            {
                "title": "A linear algorithm for five-coloring a planar graph",
                "abstract": "A simple linear algorithm is presented for coloring planar graphs with at most five colors. The algorithm employs a recursive reduction of a graph involving the deletion of a vertex of degree 6 or less possibly together with the identification of its several neighbors.",
                "id": "00028"
            },
            {
                "title": "Efficient approximation algorithms for bandwidth consecutive multicolorings of graphs",
                "abstract": "Let G be a graph in which each vertex v has a positive integer weight b ( v ) and each edge ( v , w ) has a nonnegative integer weight b ( v , w ) . A bandwidth consecutive multicoloring, simply called a b-coloring of G, assigns each vertex v a specified number b ( v ) of consecutive positive integers as colors of v so that, for each edge ( v , w ) , all integers assigned to vertex v differ from all integers assigned to vertex w by more than b ( v , w ) . The maximum integer assigned to vertices is called the span of the coloring. The b-coloring problem asks to find a b-coloring of a given graph G with the minimum span. In the paper, we present four efficient approximation algorithms for the problem, which have theoretical performance guarantees for the computation time, the span of a found b-coloring and the approximation ratio. We also obtain several upper bounds on the minimum span, expressed in terms of the maximum b-degrees, one of which is an extension of Brooks' theorem on an ordinary coloring.",
                "id": "00029"
            },
            {
                "title": "A Linear-Time Algorithm to Find Four Independent Spanning Trees in Four-Connected Planar Graphs",
                "abstract": "\n Given a graph G, a designated vertex r and a natural number k, we wish to find k independent spanning trees of G rooted at r, that is, k spanning trees such that, for any vertex v, the k paths connecting r and v in the k trees are internally disjoint in G. In this paper we give a linear-time algorithm to find four independent spanning trees in a 4-connected planar graph rooted\n at any vertex.\n \n ",
                "id": "00030"
            },
            {
                "title": "Convex grid drawings of plane graphs with rectangular contours",
                "abstract": "In a convex drawing of a plane graph, all edges are drawn as straight-line segments without any edge-intersection and all facial cycles are drawn as convex polygons. In a convex grid drawing, all vertices are put on grid points. A plane graph G has a convex drawing if and only if G is internally triconnected, and an internally triconnected plane graph G has a convex grid drawing on an n \u00d7n grid if G is triconnected or the triconnected component decomposition tree T(G) of G has two or three leaves, where n is the number of vertices in G. In this paper, we show that an internally triconnected plane graph G has a convex grid drawing on a 2n \u00d7n2 grid if T(G) has exactly four leaves. We also present an algorithm to find such a drawing in linear time. Our convex grid drawing has a rectangular contour, while most of the known algorithms produce grid drawings having triangular contours.",
                "id": "00031"
            },
            {
                "title": "Rectangular drawings of planar graphs",
                "abstract": "A plane graph is a planar graph with a fixed embedding in the plane. In a rectangular drawing of a plane graph, each vertex is drawn as a point, each edge is drawn as a horizontal or vertical line segment, and each face is drawn as a rectangle. A planar graph is said to have a rectangular drawing if at least one of its plane embeddings has a rectangular drawing. In this paper we give a linear-time algorithm to examine whether a planar graph G of maximum degree three has a rectangular drawing or not, and to find a rectangular drawing of G if it exists.",
                "id": "00032"
            },
            {
                "title": "Bandwidth consecutive multicolorings of graphs.",
                "abstract": "Let G be a simple graph in which each vertex v has a positive integer weight b(v) and each edge (v, w) has a nonnegative integer weight b(v, w). A bandwidth consecutive multicoloring of G assigns each vertex v a specified number b(v) of consecutive positive integers so that, for each edge (v, w), all integers assigned to vertex v differ from all integers assigned to vertex w by more than b(v, w). The maximum integer assigned to a vertex is called the span of the coloring. In the paper, we first investigate fundamental properties of such a coloring. We then obtain a pseudo polynomial-time exact algorithm and a fully polynomial-time approximation scheme for the problem of finding such a coloring of a given series-parallel graph with the minimum span. We finally extend the results to the case where a given graph G is a partial k-tree, that is, G has a bounded tree-width.",
                "id": "00033"
            },
            {
                "title": "Characterization of optimal key set protocols",
                "abstract": "Using a random deal of cards to players and a computatio-nally unlimited eavesdropper, all players wish to share a common\n one-bit secret key which is information-theoretically secure from the eavesdrop-per. This can be done by the so-called key\n set protocol. In this paper we give a necessary and sufficient condition for a key set protocol to be \u201coptimal,\u201d that is,\n to succeed always in sharing a one-bit secret key.\n ",
                "id": "00034"
            },
            {
                "title": "Generalized Edge-Ranking of Trees (Extended Abstract)",
                "abstract": "In this paper we newly define a generalized edge-ranking of a graph G as follows: for a positive integer c, a c-edge-ranking of G is a labeling (ranking) of the edges of G with integers such that, for any label i, deletion of all edges with labels u003e i leaves connected components, each having at most c edges with label i. The problem of finding an optimal c-edge-ranking of G, that is, a c-edge-ranking using the minimum number of ranks, has applications in scheduling the manufacture of complex multi-part products; it is equivalent to finding a c-edge-separator tree of G having the minimum height. We present an algorithm to find an optimal c-edge-ranking of a given tree T for any positive integer c in time O(n2 log \u03b4), where n is the number of vertices in T and \u03b4 is the maximum vertex-degree of T. Our algorithm is faster than the best algorithm known for the case c=1.",
                "id": "00035"
            },
            {
                "title": "A Linear-Time Routing Algorithm for Convex Grids",
                "abstract": "In this paper, we consider the channel routing problem involving two-terminal nets on rectilinear grids. An efficient algorithm is described which necessarily finds a routing in a given grid whenever it exists. The algorithm is not a heuristic but an exact one, and works for a rather large class of grids, called convex grids, including the grids of rectangular, T-, L-, or X-shape boundaries. Both the running time and required space are linear in the number of vertices of a grid.",
                "id": "00036"
            },
            {
                "title": "Finding a Shortest Pair of Paths on the Plane with Obstacles and Crossing Areas",
                "abstract": "Given axis-parallel rectangular obstacles and crossing areas together with two pairs of terminals on the plane, our algorithm finds a shortest pair of rectilinear paths which connect the pairs of terminals and neither pass through any obstacle nor cross each other except in the crossing areas. The algorithm takes O(n log n) time and O(n) space, where n is the total number of obstacles and crossing areas.",
                "id": "00037"
            },
            {
                "title": "Algorithms for Finding Noncrossing Steiner Forests in Plane Graphs",
                "abstract": "Let G = (V, E) be a plane graph with nonnegative edge lengths, and let N be a family of k vertex sets N1, N2, . . ., Nk \u2286 V, called nets. Then a noncrossing Steiner forest for N in G is a set T of k trees T1; T2; . . .,Tk in G such that each tree Ti \u2208 T connects all vertices in Ni, any two trees in T do not cross each other, and the sum of edge lengths of all trees is minimum. In this paper we give an algorithm to find a noncrossing Steiner forest in a plane graph G for the case where all vertices in nets lie on two of the face boundaries of G. The algorithm takes time O(n log n) if G has n vertices.",
                "id": "00038"
            },
            {
                "title": "Linear-time computability of combinatorial problems on series-parallel graphs",
                "abstract": "A series-parallel graph can be constructed from a certain graph by recurslvely applying \"series\" and \"parallel\" connections The class of such graphs, which Is a well-known model of series-parallel electrical networks, is a subclass of planar graphs It is shown in a umfied manner that there exist hnear- ume algorithms for many combinatorial problems ff an input graph is restricted to the class of series-parallel graphs. These include 0) the decision problem with respect to a property characterized by a finite number of forbidden graphs, (u) the mlmmum edge (vertex) deletion problem with respect to the same property as above, and (Ul) the generalized matching problem Consequently, the following problems, among others, prove to be hnear-tlme computable for the class of series-parallel graphs. (I) the minimum vertex cover problem, (2) the maximum outerplanar (reduced) subgraph problem, (3) the minimum feedback vertex set problem, (4) the maximum (induced) hne-subgraph problem, (5) the maximum matching problem, and (6) the maximum disjoint triangle problem.",
                "id": "00039"
            },
            {
                "title": "Finding optimal edge-rankings of trees",
                "abstract": "No abstract available.\n\n",
                "id": "00040"
            },
            {
                "title": "Minimum Cost Partitions of Trees with Supply and Demand",
                "abstract": "\n Let T be a given tree. Each vertex of T is either a supply vertex or a demand vertex, and is assigned a positive integer, called the supply or the demand. Every\n demand vertex v of T must be supplied an amount of \u201cpower,\u201d equal to the demand of v, from exactly one supply vertex through edges in T. Each edge e of T has a direction, and is assigned a positive integer which represents the cost required to delete e from T or reverse the direction of e. Then one wishes to obtain subtrees of T by deleting edges and reversing the directions of edges so that (a) each subtree contains exactly one supply vertex whose\n supply is no less than the sum of all demands in the subtree and (b) each subtree is rooted at the supply vertex in a sense\n that every edge is directed away from the root. We wish to minimize the total cost to obtain such rooted subtrees from T. In the paper, we first show that this minimization problem is NP-hard, and then give a pseudo-polynomial-time algorithm\n to solve the problem. We finally give a fully polynomial-time approximation scheme (FPTAS) for the problem.\n \n ",
                "id": "00041"
            },
            {
                "title": "A linear-time algorithm for four-partitioning four-connected planar graphs",
                "abstract": "Given a graph G=(V, E), four distinct vertices u\n1,u2,u3,u4 \n\n\n\n\n\n_boxclose^4 n_i = V\\sum\\nolimits_{i = 1}^4 {n_i = |V|}\n, we wish to find a partition V\n1, V2, V3, V4 of the vertex set V such that u\ni  Vi, \u00a6Vi\u00a6=ni and V\ni induces a connected subgraph of G for each i, 1  i  4. In this paper we give a simple linear-time algorithm to find such a partition if G is a 4-connected planar graph and u\n1, u2, u3, u4 are located on the same face of a plane embedding of G. Our algorithm is based on a 4-canonical decomposition of G, which is a generalization of an st-numbering and a canonical 4-ordering known in the area of graph drawings.",
                "id": "00042"
            },
            {
                "title": "Generalized Edge-Colorings Of Weighted Graphs",
                "abstract": "Let G be a graph with a positive integer weight omega(v) for each vertex v. One wishes to assign each edge e of G a positive integer f(e) as a color so that omega(v) <= vertical bar f(e) - f(e')vertical bar for any vertex v and any two edges e and e' incident to v. Such an assignment f is called an omega-edge-coloring of G, and the maximum integer assigned to edges is called the span of f. The omega-chromatic index of G is the minimum span over all omega-edge-colorings of G. In the paper, we present various upper and lower bounds on the omega-chromatic index, and obtain three efficient algorithms to find an omega-edge-coloring of a given graph. One of them finds an omega-edge-coloring with span smaller than twice the omega-chromatic index.",
                "id": "00043"
            },
            {
                "title": "Multicolorings of Series-Parallel Graphs",
                "abstract": "Let G be a graph,\n and let each vertex v of G have a positive integer weight\n \u03c9(v).\n A multicoloring of G is to assign each vertex v \n a set of \u03c9(v) colors\n so that\n any pair of adjacent vertices receive disjoint sets of colors.\n This paper presents an algorithm \n to find \n a multicoloring of a given series-parallel graph G \n with the minimum number of colors \n in \n time O(n\n W),\n where n is the number of vertices \n and W is the maximum weight of vertices in G.",
                "id": "00044"
            },
            {
                "title": "Size and Energy of Threshold Circuits Computing Mod Functions",
                "abstract": "Let C be a threshold logic circuit computing a Boolean function MOD$_m: \\{ 0, 1\\}^n \\rightarrow \\{0, 1\\}$, where n \u9a74 1 and m \u9a74 2. Then C outputs \"0\" if the number of \"1\"s in an input x \u9a74 {0, 1} n to C is a multiple of m and, otherwise, C outputs \"1.\" The function MOD2 is the so-called PARITY function, and MOD n + 1 is the OR function. Let s be the size of the circuit C, that is, C consists of s threshold gates, and let e be the energy complexity of C, that is, at most e gates in C output \"1\" for any input x \u9a74 { 0, 1} n . In the paper, we prove that a very simple inequality n/(m \u9a74 1) \u2264 s e holds for every circuit C computing MOD m . The inequality implies that there is a tradeoff between the size s and energy complexity e of threshold circuits computing MOD m , and yields a lower bound e = \u9a74((logn \u9a74 logm)/loglogn) on e if s = O(polylog(n)). We actually obtain a general result on the so-called generalized mod function, from which the result on the ordinary mod function MOD m immediately follows. Our results on threshold circuits can be extended to a more general class of circuits, called unate circuits.",
                "id": "00045"
            },
            {
                "title": "Partitioning a Weighted Tree into Subtrees with\u00a0Weights in a Given Range",
                "abstract": "Assume that each vertex of a graph G is assigned a nonnegative integer weight and that l and u are given integers such that 0\u2264l\u2264u. One wishes to partition G into connected components by deleting edges from G so that the total weight of each component is at least l and at most u. Such a partition is called an (l,u)-partition. We deal with three problems to find an (l,u)-partition of a given graph: the minimum partition problem is to find an (l,u)-partition with the minimum number of components; the maximum partition problem is defined analogously; and the p-partition problem is to find an (l,u)-partition with a given number p of components. All these problems are NP-hard even for series-parallel graphs, but are solvable in linear time for paths. In this paper, we present the first polynomial-time algorithm to solve the three problems for arbitrary trees.",
                "id": "00046"
            },
            {
                "title": "The edge-disjoint path problem is NP-complete for series-parallel graphs",
                "abstract": "Many combinatorial problems are NP-complete for general graphs. However, when restricted to series\u2013parallel graphs or partial k -trees, many of these problems can be solved in polynomial time, mostly in linear time. On the other hand, very few problems are known to be NP-complete for series\u2013parallel graphs or partial k -trees. These include the subgraph isomorphism problem and the bandwidth problem. However, these problems are NP-complete even for trees. In this paper, we show that the edge-disjoint paths problem is NP-complete for series\u2013parallel graphs and for partial 2-trees although the problem is trivial for trees and can be solved for outerplanar graphs in polynomial time.",
                "id": "00047"
            },
            {
                "title": "A 1-tough nonhamiltonian maximal planar graph",
                "abstract": "We construct a maxima' planar graph which is 1-tough but nonhamiltonian. The graph is an answer to Chvatal's question on the existence of such a graph.",
                "id": "00048"
            },
            {
                "title": "Shortest Noncrossing Paths in Plane Graphs",
                "abstract": "Let G be an undirected plane graph with nonnegative edge length, and letk terminal pairs lie on two specified face boundaries. This paper presents an algorithm for findingk \u201cnoncrossing paths\u201d inG, each connecting a terminal pair, and whose total length is minimum. Noncrossing paths may share common vertices or edges but do not cross each other in the plane. The algorithm runs in timeO(n logn) wheren is the number of vertices inG andk is an arbitrary integer.",
                "id": "00049"
            },
            {
                "title": "Absolutely Secure Message Transmission using a Key Sharing Graph.",
                "abstract": "Assume that there are players and an eavesdropper Eve of unlimited computational power and that several pairs of players have shared secret keys beforehand. A key sharing graph is one in which each vertex corresponds to a player, and each edge corresponds to a secret key shared by the two players corresponding to the ends of the edge. Given a key sharing graph, a player wishes to send a message to another player so that the eavesdropper Eve and any other player can get no information on the message. In this paper, we give a necessary and sufficient condition on a key sharing graph for the existence of a protocol for such a purpose.",
                "id": "00050"
            },
            {
                "title": "Small grid drawings of planar graphs with balanced bipartition",
                "abstract": "In a grid drawing of a planar graph, every vertex is located at a grid point, and every edge is drawn as a straight-line segment without any edge-intersection. It has been known that every planar graph G of n vertices has a grid drawing on an (n\u22122)\u00d7(n\u22122) integer grid and such a drawing can be found in linear time. In this paper we show that if a planar graph G has a balanced bipartition then G has a grid drawing with small grid area. More precisely, if a separation pair bipartitions G into two edge-disjoint subgraphs G1 and G2, then G has a grid drawing on a W\u00d7H grid such that both the width W and height H are smaller than the larger number of vertices in G1 and in G2. In particular, we show that every series-parallel graph G has a grid drawing on a (2n/3)\u00d7(2n/3) grid and such a drawing can be found in linear time.",
                "id": "00051"
            },
            {
                "title": "Parametric power supply networks",
                "abstract": "Suppose that each vertex of a graph $$G$$ G is either a supply vertex or a demand vertex and is assigned a supply or a demand. All demands and supplies are nonnegative constant numbers in a steady network, while they are functions of a variable $$\\\\lambda $$ \u00bf in a parametric network. Each demand vertex can receive \\\"power\\\" from exactly one supply vertex through edges in $$G$$ G . One thus wishes to partition $$G$$ G to connected components by deleting edges from $$G$$ G so that each component has exactly one supply vertex whose supply is at least the sum of demands in the component. The \\\"partition problem\\\" asks whether $$G$$ G has such a partition. If $$G$$ G has no such partition, one wishes to find the maximum number $$r^*$$ r \u00bf , $$0\\\\le r^* <1$$ 0 \u2264 r \u00bf < 1 , such that $$G$$ G has such a partition when every demand is reduced to $$r^*$$ r \u00bf times the original demand. The \\\"maximum supply rate problem\\\" asks to compute $$r^*$$ r \u00bf . In this paper, we deal with a network in which $$G$$ G is a tree, and first give a polynomial-time algorithm for the maximum supply rate problem for a steady tree network, and then give an algorithm for the partition problem on a parametric tree network, which takes pseudo-polynomial time if all the supplies and demands are piecewise linear functions of $$\\\\lambda $$ \u00bf .",
                "id": "00052"
            },
            {
                "title": "Partitioning a Weighted Tree to Subtrees of Almost Uniform Size",
                "abstract": "Assume that each vertex of a graph G is assigned a nonnegative integer weight and that l and u are integers such that 0 \u2264 l \u2264 u. One wishes to partition G into connected components by deleting edges from G so that the total weight of each component is at least l and at most u. Such an \"almost uniform\" partition is called an (l, u)-partition. We deal with three problems to find an (l, u)-partition of a given graph: the minimum partition problem is to find an (l, u)-partition with the minimum number of components; the maximum partition problem is defined analogously; and the p-partition problem is to find an (l, u)-partition with a given number p of components. All these problems are NP-hard even for series-parallel graphs, but are solvable for paths in linear time and for trees in polynomial time. In this paper, we give polynomial-time algorithms to solve the three problems for trees, which are much simpler and faster than the known algorithms.",
                "id": "00053"
            },
            {
                "title": "Rectangular drawings of plane graphs without designated corners",
                "abstract": "This paper addresses the problem of finding rectangular drawings of plane graphs, in which each vertex is drawn as a point, each edge is drawn as a horizontal or a vertical line segment, and the contour of each face is drawn as a rectangle. A graph is a 2\u20133 plane graph if it is a plane graph and each vertex has degree 3 except the vertices on the outer face which have degree 2 or 3. A necessary and sufficient condition for the existence of a rectangular drawing has been known only for the case where exactly four vertices of degree 2 on the outer face are designated as corners in a 2\u20133 plane graph G . In this paper we establish a necessary and sufficient condition for the existence of a rectangular drawing of G for the general case in which no vertices are designated as corners. We also give a linear-time algorithm to find a rectangular drawing of G if it exists.",
                "id": "00054"
            },
            {
                "title": "Scheduling File Transfers under Port and Channel Constraints",
                "abstract": "The file transfer scheduling problem was introduced and studied by Coffman, Garey, Johnson and LaPaugh. The problem is to schedule transfers of a large collection of files between various nodes of a network under port constraint so as to minimize overall finishing time. This paper extends their model to include communication channel constraint in addition to port constraint. We formulate the problem with both port and channel constraints as a new type of edge-coloring of multigraphs, called an fg-edge-coloring, and give an efficient approximation algorithm with absolute worst-case ratio 3/2.",
                "id": "00055"
            },
            {
                "title": "Energy and depth of threshold circuits",
                "abstract": "In this paper we show that there is a close relationship between the energy complexity and the depth of threshold circuits computing any Boolean function although they have completely different physical meanings. Suppose that a Boolean function f can be computed by a threshold circuit C of energy complexity e and hence at most e threshold gates in C output ''1'' for any input to C. We prove that the function f can also be computed by a threshold circuit C^' of the depth 2e+1 and hence the parallel computation time of C^' is 2e+1. If the size of C is s, that is, there are s threshold gates in C, then the size s^' of C^' is s^'=2es+1. Thus, if the size s of C is polynomial in the number n of input variables, then the size s^' of C^' is polynomial in n, too.",
                "id": "00056"
            },
            {
                "title": "Eulerian Secret Key Exchange",
                "abstract": "Designing a protocol to exchange a secret key is one of the most fundamental subjects in cryptography. Using a random deal of cards, pairs of card players (agents) can share information-theoretically secure keys that are secret from an eavesdropper. In this paper we first introduce the notion of an Eulerian secret key exchange, in which the pairs of players sharing secret keys form an Eulerian circuit passing through all players. Along the Eulerian circuit any designated player can send a message to the rest of players and the message can be finally returned to the sender. Checking whether the returned message is the same as the original one, the sender can know whether the message circulation has been completed without any false alteration. We then give three efficient protocols to realize such an Eulerian secret key exchange. Each of the three protocols is optimal in a sense. The first protocol requires the minimum number of cards under a natural assumption that the same number of cards are dealt to each player. The second requires the minimum number of cards dealt to all players when one does not make the assumption. The third forms the shortest Eulerian circuit, and hence the time required to send the message to all players and acknowledge the secure receipt is minimum in this case.",
                "id": "00057"
            },
            {
                "title": "Multicommodity flows in planar undirected graphs and shortest paths",
                "abstract": "This paper deals with the multicommodity flow problems for two classes of planar undirected graphs. The first class C12 consists of graphs in which each source-sink pair is located on one of two specified face boundaries. The second class C01 consists of graphs in which some of the source-sink pairs are located on a specified face boundary and all the other pairs share a common sink located on the boundary. We show that the multicommodity flow problem for a graph in C12 (resp. C01) can be reduced to the shortest path problem for an undirected (resp. a directed) graph obtained from the dual of the original undirected graph.",
                "id": "00058"
            },
            {
                "title": "Algorithms for generalized vertex-rankings of partial k-trees",
                "abstract": "A c-vertex-ranking of a graph G for a positive integer c is a labeling of the vertices of G with integers such that, for any label i, deletion of all vertices with labels > i leaves connected components, each having at most c vertices with label i. A c-vertex-ranking is optimal if the number of labels used is as small as possible. We present sequential and parallel algorithms to find an optimal c-vertex-ranking of a partial k-tree, that is, a graph of treewidth bounded by a fixed integer fi. The sequential algorithm takes polynomial-time for any positive integer c. The parallel algorithm takes O(log n) parallel time using a polynomial number of processors on the common CRCW PRAM, where n is the number of vertices in G. (C) 2000 Elsevier Science B.V. All rights reserved.",
                "id": "00059"
            },
            {
                "title": "Improvements of HITS Algorithms for Spam Links",
                "abstract": "The HITS algorithm proposed by Kleinberg is one of the representative methods of scoring Web pages by using hyperlinks. In the days when the algorithm was proposed, most of the pages given high score by the algorithm were really related to a given topic, and hence the algorithm could be used to find related pages. However, the algorithm and the variants including Bharat's improved HITS, abbreviated to BHITS, proposed by Bharat and Henzinger cannot be used to find related pages any more on today's Web, due to an increase of spam links. In this paper, we first propose three methods to find \u201clinkfarms,\u201d that is, sets of spam links forming a densely connected subgraph of a Web graph. We then present an algorithm, called a trust-score algorithm, to give high scores to pages which are not spam pages with a high probability. Combining the three methods and the trust-score algorithm with BHITS, we obtain several variants of the HITS algorithm. We ascertain by experiments that one of them, named TaN+BHITS using the trust-score algorithm and the method of finding linkfarms by employing name servers, is most suitable for finding related pages on today's Web. Our algorithms take time and memory no more than those required by the original HITS algorithm, and can be executed on a PC with a small amount of main memory.",
                "id": "00060"
            },
            {
                "title": "Finding a Noncrossing Steiner Forest in Plane Graphs Under a 2-Face Condition",
                "abstract": "Let G = (V,E) be a plane graph with nonnegative edge weights, and let \n<img src=\"/fulltext-image.asp?format=htmlnonpaginated&src=VW01081001H67X67_html\\10878_2004_Article_333492_TeX2GIFIE1.gif\" border=\"0\" alt=\"\n$$\\mathcal{N}$$\n\" /> be a family of k vertex sets \n<img src=\"/fulltext-image.asp?format=htmlnonpaginated&src=VW01081001H67X67_html\\10878_2004_Article_333492_TeX2GIFIE2.gif\" border=\"0\" alt=\"\n$$N_1 ,N_2 ,...,N_k \\subseteq V$$\n\" />, called nets. Then a noncrossing Steiner forest for \n<img src=\"/fulltext-image.asp?format=htmlnonpaginated&src=VW01081001H67X67_html\\10878_2004_Article_333492_TeX2GIFIE3.gif\" border=\"0\" alt=\"\n$$\\mathcal{N}$$\n\" /> in G is a set \n<img src=\"/fulltext-image.asp?format=htmlnonpaginated&src=VW01081001H67X67_html\\10878_2004_Article_333492_TeX2GIFIE4.gif\" border=\"0\" alt=\"\n$$\\mathcal{T}$$\n\" /> of k trees \n<img src=\"/fulltext-image.asp?format=htmlnonpaginated&src=VW01081001H67X67_html\\10878_2004_Article_333492_TeX2GIFIE5.gif\" border=\"0\" alt=\"\n$$T_1 ,T_2 ,...,T_k$$\n\" /> in G such that each tree \n<img src=\"/fulltext-image.asp?format=htmlnonpaginated&src=VW01081001H67X67_html\\10878_2004_Article_333492_TeX2GIFIE6.gif\" border=\"0\" alt=\"\n$$T_i \\in \\mathcal{T}$$\n\" /> connects all vertices, called terminals, in net Ni, any two trees in \n<img src=\"/fulltext-image.asp?format=htmlnonpaginated&src=VW01081001H67X67_html\\10878_2004_Article_333492_TeX2GIFIE7.gif\" border=\"0\" alt=\"\n$$\\mathcal{T}$$\n\" /> do not cross each other, and the sum of edge weights of all trees is minimum. In this paper we give an algorithm to find a noncrossing Steiner forest in a plane graph G for the case where all terminals in nets lie on any two of the face boundaries of G. The algorithm takes time \n<img src=\"/fulltext-image.asp?format=htmlnonpaginated&src=VW01081001H67X67_html\\10878_2004_Article_333492_TeX2GIFIE8.gif\" border=\"0\" alt=\"\n$$O\\left( {n\\log n} \\right)$$\n\" /> if G has n vertices and each net contains a bounded number of terminals.",
                "id": "00061"
            },
            {
                "title": "A complete characterization of a family of key exchange protocols",
                "abstract": ". \u00a0\u00a0Using a random deal of cards to players and a computationally unlimited eavesdropper, all players wish to share a one-bit\n secret key which is information-theoretically secure from the eavesdropper. This can be done by a protocol to make several\n pairs of players share one-bit secret keys so that all these pairs form a tree over players. In this paper we obtain a necessary\n and sufficient condition on the number of cards for the existence of such a protocol.",
                "id": "00062"
            },
            {
                "title": "Variable-Priority Queue and Doughnut Routing",
                "abstract": "This paper proposes a new data structure called a variable-priority queue. The queue supports, in addition to the ordinary queue operations, an operation MIN to find an item of minimum key and three operations to change keys of items. Any sequence of these m operations can be processed in O(m) time. Furthermore, as its application, this paper presents two efficient algorithms for network problems. The first finds multicommodity flows in cycles in linear time. The second, using the first, finds edge-disjoint paths connecting terminal pairs in a doughnut-shaped grid. The grid is bounded by two nested rectangles, and terminals are specified on the two rectangular boundaries outside the four corners. If there are k terminal pairs and all the terminals are ordered in clockwise order around rectangles, then the algorithm decides in O(k) time whether there are edge-disjoint paths connecting terminals in the grid, and actually finds edge-disjoint paths in O(k log k) time.",
                "id": "00063"
            },
            {
                "title": "A Linear Algorithm for Edge-Coloring Partial k-Trees",
                "abstract": "Many combinatorial problems can be efficiently solved for partial k-trees. The edge-coloring problem is one of a few combinatorial problems for which no linear-time algorithm has been obtained for partial k-trees. The best known algorithm solves the problem for partial k-trees G in time \nO( nD</font\n>22( k + 1 )   )O\\left( {n\\Delta ^{2^{2\\left( {k + 1} \\right)} } } \\right)\n where n is the number of vertices and  is the maximum degree of G. This paper gives a linear algorithm which optimally edge-colors a given partial k-tree for fixed k.",
                "id": "00064"
            },
            {
                "title": "The Edge-Disjoint Paths Problem is NP-Complete for Partial k-Trees",
                "abstract": "Many combinatorial problems are NP-complete for general graphs, but are not NP-complete for partial k-trees (graphs of treewidth bounded by a constant k) and can be efficiently solved in polynomial time or mostly in linear time for partial k-trees. On the other hand, very few problems are known to be NP-complete for partial k-trees with bounded k. These include the subgraph isomorphism problem and the bandwidth problem. However, all these problems are NP-complete even for ordinary trees or forests. In this paper we show that the edge-disjoint paths problem is NP-complete for partial k-trees with some bounded k, say k = 3, although the problem is trivially solvable for trees.",
                "id": "00065"
            },
            {
                "title": "Partitioning a graph of bounded tree-width to connected subgraphs of almost uniform size",
                "abstract": "Assume that each vertex of a graph G is assigned a nonnegative integer weight and that l and u are nonnegative integers. One wishes to partition G into connected components by deleting edges from G so that the total weight of each component is at least l and at most u. Such an \u201calmost uniform\u201d partition is called an (l,u)-partition. We deal with three problems to find an (l,u)-partition of a given graph; the minimum partition problem is to find an (l,u)-partition with the minimum number of components; the maximum partition problem is defined analogously; and the p-partition problem is to find an (l,u)-partition with a fixed number p of components. All these problems are NP-complete or NP-hard, respectively, even for series-parallel graphs. In this paper we show that both the minimum partition problem and the maximum partition problem can be solved in time O(u4n) and the p-partition problem can be solved in time O(p2u4n) for any series-parallel graph with n vertices. The algorithms can be extended for partial k-trees, that is, graphs with bounded tree-width.",
                "id": "00066"
            },
            {
                "title": "Extended Rectangular Drawings of Plane Graphs with Designated Corners",
                "abstract": "In a rectangular drawing of a plane graph, each edge is drawn as a horizontal or vertical line segment, and all faces including the outer face are drawn as rectangles. In this paper, we introduce an \"extended rectangular drawing\" in which all inner faces are drawn as rectangles but the outer face is drawn as a rectilinear polygon with designated corners, and give a necessary and sufficient condition for a plane graph to have an extended rectangular drawing.",
                "id": "00067"
            },
            {
                "title": "Approximability of partitioning graphs with supply and demand",
                "abstract": "Suppose that each vertex of a graph G is either a supply vertex or a demand vertex and is assigned a positive real number, called the supply or the demand. Each demand vertex can receive ''power'' from at most one supply vertex through edges in G. One thus wishes to partition G into connected components by deleting edges from G so that each component C either has no supply vertex or has exactly one supply vertex whose supply is at least the sum of demands in C, and wishes to maximize the fulfillment, that is, the sum of demands in all components with supply vertices. This maximization problem is known to be NP-hard even for trees having exactly one supply vertex and strongly NP-hard for general graphs. In this paper, we focus on the approximability of the problem. We first show that the problem is MAXSNP-hard and hence there is no polynomial-time approximation scheme (PTAS) for general graphs unless P=NP. We then present a fully polynomial-time approximation scheme (FPTAS) for series-parallel graphs having exactly one supply vertex.",
                "id": "00068"
            },
            {
                "title": "Partitioning graphs of supply and demand",
                "abstract": "Assume that each vertex of a graph G is either a supply vertex or a demand vertex and is assigned a positive integer, called a supply or a demand. Each demand vertex can receive \"power\" from at most one supply vertex. One thus wishes to partition G into connected components by deleting edges from G so that each component C has exactly one supply vertex whose supply is no less than the sum of demands of all demand vertices in C.I fG has no such partition, one wishes to partition G into connected components so that each component C either has no supply vertex or has exactly one supply vertex whose supply is no less than the sum of demands in C, and wishes to maximize the sum of demands in all components with supply vertices. We deal with such a maximization problem, which is NP-hard even for trees and strong NP-hard for general graphs. In this paper, we give a pseudo-polynomial-time algorithm to solve the problem for series- parallel graphs. The algorithm can be easily extended for partial k-trees, that is, graphs with bounded tree-width.",
                "id": "00069"
            },
            {
                "title": "Small grid drawings of planar graphs with balanced partition",
                "abstract": "In a grid drawing of a planar graph, every vertex is located at a grid point, and every edge is drawn as a straight-line segment without any edge-intersection. It is known that every planar graph G of n vertices has a grid drawing on an (n\u9a742)\u8133(n\u9a742) or (4n/3)\u8133(2n/3) integer grid. In this paper we show that if a planar graph G has a balanced partition then G has a grid drawing with small grid area. More precisely, if a separation pair bipartitions G into two edge-disjoint subgraphs G 1 and G 2, then G has a max\u9a74{n 1,n 2}\u8133max\u9a74{n 1,n 2} grid drawing, where n 1 and n 2 are the numbers of vertices in G 1 and G 2, respectively. In particular, we show that every series-parallel graph G has a (2n/3)\u8133(2n/3) grid drawing and a grid drawing with area smaller than 0.3941n 2 (2 n 2).",
                "id": "00070"
            },
            {
                "title": "Combinatorial problems on series-parallel graphs",
                "abstract": "We show, in a unified manner, that there exist linear time algorithms for many combinatorial problems defined on the class of series-parallel graphs. These include (i) the decision problem, and (ii) the minimum edge (vertex) deletion problem both with respect to a property characterized by a finite number of forbidden graphs, and (iii) the generalized matching problem.",
                "id": "00071"
            },
            {
                "title": "Drawing Plane Graphs",
                "abstract": "\n Automatic aesthetic drawing of plane graphs has recently created intense interest due to its broad applications, and as a\n consequence, a number of drawing methods, such as the straight line drawing, convex drawing, orthogonal drawing, rectangular\n drawing and box-rectangular drawing, have come out [8,9,3,4,5,6,7, 10,11,14,16,23,29,33]. In this talk we survey the recent\n results on these drawings of plane graphs.\n \n ",
                "id": "00072"
            },
            {
                "title": "Generalized vertex-rankings of trees",
                "abstract": "We newly define a generalized vertex-ranking of a graph G as follows: for a positive integer c, a c-vertex-ranking of G is a labeling (ranking) of the vertices of G with integers such that: for any label i, every connected component of the graph obtained from G by deleting the vertices with label > i has at most c vertices with label i. Clearly an ordinary vertex-ranking is a 1-vertex-ranking and vice-versa. We present an algorithm to find a c-vertex-ranking of a given tree T using the minimum number of ranks in time O(cn) where n is the number of vertices in T.",
                "id": "00073"
            },
            {
                "title": "Octagonal drawings of plane graphs with prescribed face areas",
                "abstract": "An orthogonal drawing of a plane graph is called an octagonal drawing if each inner face is drawn as a rectilinear polygon of at most eight corners and the contour of the outer face is drawn as a rectangle. A slicing graph is obtained from a rectangle by repeatedly slicing it vertically and horizontally. A slicing graph is called a good slicing graph if either the upper subrectangle or the lower one obtained by any horizontal slice will never be vertically sliced. In this paper we show that any good slicing graph has an octagonal drawing with prescribed face areas, in which the area of each inner face is equal to a prescribed value. Such a drawing has practical applications in VLSI floorplanning. We also give a linear-time algorithm to find such a drawing. We furthermore present a sufficient condition for a plane graph to be a good slicing graph, and give a linear-time algorithm to find a tree-structure of slicing paths for a graph satisfying the condition.",
                "id": "00074"
            },
            {
                "title": "Rectangular grid drawings of plane graphs",
                "abstract": "The rectangular grid drawing of a plane graph G is a drawing of G such that each vertex is located on a grid point, each edge is drawn as a horizontal or vertical line segment, and the contour of each face is drawn as a rectangle. In this paper we give a simple linear-time algorithm to find a rectangular grid drawing of G if it exists. We also give an upper bound \n\n\n\n\n\n\nW + H \\leqslant \\fracn2W + H \\leqslant \\frac{n}{2}\non the sum of required width W and height H and a bound \n\n\n\n\n\n\nW + H \\leqslant \\fracn2 16W + H \\leqslant \\frac{{n^2 }}{{16}}\non the area of a rectangular grid drawing of G, where n is the number of vertices in G. These bounds are best possible, and hold for any compact rectangular grid drawing.",
                "id": "00075"
            },
            {
                "title": "Canonical Decomposition, Realizer, Schnyder Labeling And Orderly Spanning Trees Of Plane Graphs",
                "abstract": "A canonical decomposition, a realizer, a Schnyder labeling and an orderly spanning tree of a plane graph play an important role in straight-line grid drawings, convex grid drawings, floor-plannings, graph encoding, etc. It is known that the triconnectivity is a sufficient condition for their existence, but no necessary and sufficient condition has been known. In this paper, we present a necessary and sufficient condition for their existence, and show that a canonical decomposition, a realizer, a Schnyder labeling, an orderly spanning tree, and an outer triangular convex grid drawing are notions equivalent with each other. We also show that they can be found in linear time whenever a plane graph satisfies the condition.",
                "id": "00076"
            },
            {
                "title": "A linear algorithm for compact box-drawings of trees",
                "abstract": "In a box-drawing of a rooted tree, each node is drawn by a rectangular box of prescribed size, no two boxes overlap each other, all boxes corresponding to siblings of the tree have the same x-coordinate at their left sides, and a parent node is drawn at a given distance apart from its first child. A box drawing of a tree is compact if it attains the minimum possible rectangular area enclosing the drawing. We give a linear-time algorithm for finding a compact box-drawing of a tree. A known algorithm does not always find a compact box-drawing and takes time O(n(2)) if a tree has n nodes. (C) 2003 Wiley Periodicals, Inc.",
                "id": "00077"
            },
            {
                "title": "Convex Grid Drawings of Plane Graphs with Rectangular Contours",
                "abstract": "In a convex drawing of a plane graph, all edges are drawn as straight- line segments without any edge-intersection and all facial cycles are drawn as convex polygons. In a convex grid drawing, all vertices are put on grid points. A plane graph G has a convex drawing if and only if G is internally triconnected, and an internally triconnected plane graph G has a convex grid drawing on an (n 1)\u00d7(n 1) grid if either G is triconnected or the triconnected component decomposition tree T(G) of G has two or three leaves, where n is the number of vertices in G. In this paper, we show that an internally triconnected plane graph G has a convex grid drawing on a 2n \u00d7 n2 grid if T(G) has exactly four leaves. We also present an algorithm to find such a drawing in linear time. Our convex grid drawing has a rectangular contour, while most of the known algorithms produce grid drawings having triangular contours.",
                "id": "00078"
            },
            {
                "title": "An NC Parallel Algorithm for Generalized Vertex-Rankings of Partial k-Trees",
                "abstract": "A c-vertex-ranking of a graph G for a positive integer c is a labeling of the vertices of G with integers such that, for any label i, deletion of all vertices with labels i leaves connected components, each having at most c vertices with label i. We present a parallel algorithm to find a c-vertex-ranking of a partial k-tree using the minimum number of ranks. This is the first parallel algorithm for c-vertex-ranking of a partial k-tree G, and takes O(\\log n) time using a polynomial number of processors on the common CRCW PRAM for any positive integer c and any fixed integer k, where n is the number of vertices in G.",
                "id": "00079"
            },
            {
                "title": "Sufficient Condition and Algorithm for List Total Colorings of Series-Parallel Graphs",
                "abstract": "A total coloring of a graph G is a coloring of all elements of G, i.e. vertices and edges, such that no two adjacent or incident elements receive the same color. Let L(x) be a set of colors assigned to each element x of G. Then a list total coloring of G is a total coloring such that each element x receives a color contained in L(x). The list total coloring problem asks whether G has a list total coloring for given L. In this paper, we give a sufficient condition for a series-parallel graph to have a list total coloring, and we present a linear-time algorithm to find a list total coloring of a given series-parallel graph G if G and L satisfy the sufficient condition.",
                "id": "00080"
            },
            {
                "title": "Finding Edge-Disjoint Paths in Partial k-Trees (Extended Abstract)",
                "abstract": "For a given graph G and p pairs (s\ni, ti), 1ip, of vertices in G, the edge-disjoint paths problem is to find p pairwise edge-disjoint paths P\ni, 1ip, connecting s\ni and t\ni. Many combinatorial problems can be efficiently solved for partial k-trees (graphs of treewidth bounded by a fixed integer k), but it has not been known whether the edge-disjoint paths problem can be solved in polynomial time for partial k-trees unless p=O(1). This paper gives two algorithms for the edge-disjoint paths problem on partial k-trees. The first one solves the problem for any partial k-tree G and runs in polynomial time if p=O(log n) and in linear time if p=O(1), where n is the number of vertices in G. The second one solves the problem under some restriction on the location of terminal pairs even if p  log n.",
                "id": "00081"
            },
            {
                "title": "Drawing Plane Graphs Nicely",
                "abstract": "This paper presents two efficient algorithms for drawing plane graphs nicely. Both draw all edges of a graph as straight line segments without crossing lines. The first draws a plane graph \u201cconvex\u201d if possible, that is, in a way that every inner face and the complement of the outer face are convex polygons. The second, using the first, produces a pleasing drawing of a given plane graph that satisfies the following property as far as possible: the complements of 3-connected components, together with inner faces and the complement of the outer face, are convex polygons. The running time and storage space of both algorithms are linear in the number of vertices of the graph.",
                "id": "00082"
            },
            {
                "title": "An Efficient Algorithm for Edge-Ranking Trees",
                "abstract": "An edge-ranking of an undirected graph G is a labeling of the edges of G with integers such that all paths between two edges with the same label i contain an edge with label j>i. The problem of finding an edge-ranking of G using a minimum number of ranks has applications in scheduling the manufacture of complex multi-part products; it is equivalent\n to finding the minimum height edge separator tree. Deogun and Peng and independently de la Torre et al. have given polynomialtime algorithms which find an edge-ranking of trees T using a minimum number of ranks in time O(n\n 3) and O(n\n 3 log n) respectively, where n is the number of nodes in T. This paper presents a more efficient and simple algorithm, which finds an edge-ranking of trees using a minimum number of\n ranks in O(n\n 2) time.\n ",
                "id": "00083"
            },
            {
                "title": "On the maximum matchings of regular multigraphs",
                "abstract": "The lower bounds on the cardinality of the maximum matchings of regular multigraphs are established in terms of the number of vertices, the degree of vertices and the edge-connectivity of a multigraph. The bounds are attained by infinitely many multigraphs, so are best possible.",
                "id": "00084"
            },
            {
                "title": "Bend-Minimum Orthogonal Drawings of Plane 3-Graphs",
                "abstract": "In an orthogonal drawing of a plane graph, any edge is drawn as a sequence of line segments, each having either a horizontal or a vertical direction. A bend is a point where an edge changes its direction. A drawing is called a bend-minimum orthogonal drawing if the number of bends is minimum among all orthogonal drawings. This paper presents a linear-time algorithm to find a bend-minimum orthogonal drawing of any given plane 3-graph, that is, a plane graph of maximum degree three.",
                "id": "00085"
            },
            {
                "title": "Algorithms for Finding Non-Crossing Paths with Minimum Total Length in Plane Graphs",
                "abstract": "Let G be an undirected plane graph with non-negative edge length, and let k terminal pairs lie on two specified face boundaries. This paper presents an algorithm for finding k non-crossing paths in G, each connecting a terminal pair, whose total length is minimum. Here non-crossing paths may share common vertices or edges but do not cross each other in the plane. The algorithm runs in time O(n log n) where n is the number of vertices in G.",
                "id": "00086"
            },
            {
                "title": "Box-Rectangular Drawings of Plane Graphs",
                "abstract": "In this paper we introduce a new drawing style of a plane graph G, called a \"box-rectangular drawing.\" It is defined to be a drawing of G on an integer grid such that every vertex is drawn as a rectangle, called a box, each edge is drawn as either a horizontal line segment or a vertical line segment, and the contour of each face is drawn as a rectangle. We establish a necessary and sufficient condition for the existence of a box-rectangular drawing of G. We also give a simple linear-time algorithm to find a box-rectangular drawing of G if it exists.",
                "id": "00087"
            },
            {
                "title": "A linear 5-coloring algorithm of planar graphs",
                "abstract": "A simple linear algorithm is presented for coloring planar graphs with at most five colors. The algorithm employs a recursive reduction of a graph involving the deletion of a vertex of degree 6 or less possibly together with the identification of its several neighbors.",
                "id": "00088"
            },
            {
                "title": "Algorithms for the Multicolorings of Partial k-Trees",
                "abstract": "Let each vertex v of a graph G have a positive integer weight \u9a74(v). Then a multicoloring of G is to assign each vertex v a set of \u9a74(v) colors so that any pair of adjacent vertices receive disjoint sets of colors. A partial k-tree is a graph with tree-width bounded by a fixed constant k. This paper presents an algorithm which finds a multicoloring of any given partial k-tree G with the minimum number of colors. The computation time of the algorithm is bounded by a polynomial in the number of vertices and the maximum weight of vertices in G.",
                "id": "00089"
            },
            {
                "title": "A linear algorithm for edge-coloring series-parallel multigraphs",
                "abstract": "Many combinatorial problems can be efficiently solved for series\u2013parallel multigraphs. However, the edge-coloring problem of finding the minimum number of colors required for edge-coloring given graphs is one of a few well-known combinatorial problems for which no efficient algorithms have been obtained for series\u2013parallel multigraphs. This paper gives a linear algorithm for the problem on series\u2013parallel multigraphs.",
                "id": "00090"
            },
            {
                "title": "Orthogonal Drawings of Plane Graphs without Bends",
                "abstract": "In an orthogonal drawing of a plane graph G each vertex is drawn as a point and each edge is drawn as a sequence of vertical and horizontal line segments. A point at\n which the drawing of an edge changes its direction is called a bend. Every plane graph of the maximum degree at most four\n has an orthogonal drawing, but may need bends. A simple necessary and sufficient condition has not been known for a plane\n graph to have an orthogonal drawing without bends. In this paper we obtain a necessary and sufficient condition for a plane\n graph G of the maximum degree three to have an orthogonal drawing without bends. We also give a linear-time algorithm to find such\n a drawing of G if it exists.\n ",
                "id": "00091"
            },
            {
                "title": "Algorithms for finding distance-edge-colorings of graphs",
                "abstract": "For a bounded integer \u2113, we wish to color all edges of a graph G so that any two edges within distance \u2113 have different colors. Such a coloring is called a distance-edge-coloring or an \u2113-edge-coloring of G. The distance-edge-coloring problem is to compute the minimum number of colors required for a distance-edge-coloring of a given graph G. A partial k-tree is a graph with tree-width bounded by a fixed constant k. We first present a polynomial-time exact algorithm to solve the problem for partial k-trees, and then give a polynomial-time 2-approximation algorithm for planar graphs.",
                "id": "00092"
            },
            {
                "title": "Cost Total Colorings Of Trees",
                "abstract": "A total coloring of a graph G is to color all vertices and edges of G so that no two adjacent or incident elements receive the same color. Let C be a set of colors, and let omega be a cost function which assigns to each color c in C a real number omega(c) as a cost of c. A total coloring f of G is called an optimal total coloring if the sum of costs omega(f(x)) of colors f(x) assigned to all vertices and edges x is as small as possible. In this paper, we give an algorithm to find an optimal total coloring of any tree T in time O(nDelta(3)) where n is the number of vertices in T and Delta is the maximum degree of T.",
                "id": "00093"
            },
            {
                "title": "Parallel algorithms for finding Steiner forests in planar graphs",
                "abstract": "Given an unweighted planar graph G together with nets of terminals, our problem is to find a Steiner forest, i.e., vertex-disjoint trees, each of which interconnects all the terminals of a net. This paper presents four parallel algorithms for the Steiner forest problem and a related one. The first algorithm solves the problem for the case all the terminals are located on the outer boundary of G in O(log2\nn) time using O(n\n3/log n) processors on a CREW PRAM, where n is the number of vertices in G. The second algorithm solves the problem for the case all terminals of each net lie on one of a fixed number of face boundaries in poly-log time using a polynomial number of processors. The third solves the problem for the case all terminals lie on two face boundaries. The fourth finds a maximum number of internally disjoint paths between two specified vertices in planar graphs. Both the third and fourth run either in O(log2\nn) time using O(n\n6/log n) processors or in (log3\nn) time using O(n\n3/log n) processors.",
                "id": "00094"
            },
            {
                "title": "Algorithm for the Cost Edge-Coloring of Trees",
                "abstract": "Let C be a set of colors, and let \u03c9 be a cost function which assigns a real number \u03c9(c) to each color c in C. An edge-coloring of a graph G is to color all the edges of G so that any two adjacent edges are colored with different colors. In this paper we give an efficient algorithm to find an\n optimal edge-coloring of a given tree T, that is, an edgecoloring f of T such that the sum of costs \u03c9(f(e)) of colors f(e) assigned to all edges e is minimum among all edge-colorings of T. The algorithm takes time O(n\u03942) if n is the number of vertices and \u0394 is the maximum degree of T.\n ",
                "id": "00095"
            },
            {
                "title": "Relationship Between The Genus And The Cardinality Of The Maximum Matchings Of A Graph",
                "abstract": "Lower bounds on the cardinality of the maximum matchings of graphs are established in terms of a linear polynomial of p , p (1) , p (2) and \u03b3 whose coefficients are functions of \u03ba, where p is the number of the vertices of a graph, p ( i ) the number of the vertices of degree i ( i = 1,2), \u03b3 the genus and \u03ba the connectivity.",
                "id": "00096"
            },
            {
                "title": "Algorithms For Drawing Plane Graphs",
                "abstract": "Graph drawing addresses the problem of constructing geometric representation of information and finds applications in almost every branch of science and technology. Efficient algorithms are essential for automatic drawings of graphs, and hence a lot of research has been carried out in the last decade by many researchers over the world to develop efficient algorithms for drawing graphs. In this paper we survey the recent algorithmic results on various drawings of plane graphs: straight line drawing, convex drawing, orthogonal drawing, rectangular drawing and box-rectangular drawing.",
                "id": "00097"
            },
            {
                "title": "The Hamiltonian cycle problem is linear-time solvable for 4-connected planar graphs",
                "abstract": "An algorithm is presented for finding a Hamiltonian cycle in 4-connected planar graphs. The algorithm uses linear time and storage space, while the previously best one given by Gouyou-Beauchamps uses O(n3) time and space, where n is the number of vertices in a graph.",
                "id": "00098"
            },
            {
                "title": "Labeling Points With Rectangles Of Various Shapes",
                "abstract": "We deal with a map-labeling problem, named LOFL (Left-part Ordered Flexible Labeling), to label a set of points in a plane in the presence of polygonal obstacles. The label for each point is selected from a set of rectangles with various shapes satisfying the left-part ordered property, and is placed near to the point after scaled by a scaling factor a which is common to all points. In this paper, we give an optimal O((n + m) log(n + m)) algorithm to decide the feasibility of LOFL for a fixed scaling factor sigma, and an O((n + m) log(2) (n + m)) time algorithm to find the largest feasible scaling factor sigma, where n is the number of points and m is the total number of edges of the polygonal obstacles.",
                "id": "00099"
            },
            {
                "title": "Spanning Distribution Forests of Graphs - (Extended Abstract).",
                "abstract": "Assume that a graph G has l sources, each assigned a non-negative integer called a supply, that all the vertices other than the sources are sinks, each assigned a non-negative integer called a demand, and that each edge of G is assigned a non-negative integer, called a capacity. Then one wishes to find a spanning forest F of G such that F consists of l trees, each tree T in F contains a source w, and the flow through each edge of T does not exceed the edge-capacity when a flow of an amount equal to a demand is sent from w to each sink in T along the path in T. Such a forest F is called a spanning distribution forest of G. In the paper, we first present a pseudo-polynomial time algorithm to find a spanning distribution forest of a given series-parallel graph, and then extend the algorithm for graphs with bounded tree-width.",
                "id": "000100"
            },
            {
                "title": "A linear algorithm for embedding planar graphs using PQ-trees",
                "abstract": "This paper presents a simple linear algorithm for embedding (or drawing) a planar graph in the plane. The algorithm is based on the \u201cvertex-addition\u201d algorithm of Lempel, Even, and Cederbaum (\u201cTheory of Graphs,\u201d Intl. Sympos. Rome, July 1966, pp. 215\u2013232, Gordon & Breach, New York, 1967) for the planarity testing, and is a modification of Booth and Lueker's ( J. Comput. System Sci. 13 (1976), 335\u2013379) implementation of the testing algorithm using a PQ -tree. Compared with the known embedding algorithm of Hopcroft and Tarjan ( J. Assoc. Comput. Mach. 21 , No. 4 (1974), 549\u2013568), this algorithm is conceptually simple and easy to understand or implement. Moreover this embedding algorithm can find all the embeddings of a planar graph.",
                "id": "000101"
            },
            {
                "title": "Dealing necessary and sufficient numbers of cards for sharing a one-bit secret key",
                "abstract": "Using a random deal of cards to players and a computationally unlimited eavesdropper, all players wish to share a one-bit secret key which is information-theoretically secure from the eavesdropper. This can be done by a protocol to make several pairs of players share one-bit secret keys so that all these pairs form a spanning tree over players. In this paper we obtain a necessary and sufficient condition on the number of cards for the existence of such a protocol. Our condition immediately yields an efficient linear-time algorithm to determine whether there exists a protocol to achieve such a secret key sharing.",
                "id": "000102"
            },
            {
                "title": "Improved edge-coloring algorithms for planar graphs",
                "abstract": " We consider the problem of edge-coloring planar graphs. It isknown that a planar graph G with maximum degree \\Delta  8 can becolored with \\Delta colors. We present two algorithms which find sucha coloring when \\Delta  9. The first one is a sequential O(n log n) timealgorithm. The other one is a parallel EREW PRAM algorithm whichworks in time O(log3n) and uses O(n) processors.Department of Mathematics and Computer Science, University of California, Riverside,CA 92521. On leave from... ",
                "id": "000103"
            },
            {
                "title": "Orthogonal Drawings of Series-Parallel Graphs with Minimum Bends",
                "abstract": "In an orthogonal drawing of a planar graph $G$, each vertex is drawn as a point, each edge is drawn as a sequence of alternate horizontal and vertical line segments, and any two edges do not cross except at their common end. A bend is a point where an edge changes its direction. A drawing of $G$ is called an optimal orthogonal drawing if the number of bends is minimum among all orthogonal drawings of $G$. In this paper we give an algorithm to find an optimal orthogonal drawing of any given series-parallel graph of the maximum degree at most three. Our algorithm takes linear time, while the previously known best algorithm takes cubic time. Furthermore, our algorithm is much simpler than the previous one. We also obtain a best possible upper bound on the number of bends in an optimal drawing.",
                "id": "000104"
            },
            {
                "title": "Efficient Algorithms for Weighted Colorings of Series-Parallel Graphs",
                "abstract": "Let G be a weighted graph such that each vertex v has a positive integer weight \u03c9(v). A weighted coloring of G is to assign a set of \u03c9(v) colors to each vertex v so that any two adjacent vertices receive disjoint sets of colors. This paper gives an efficient algorithm to find the minimum number of colors required for a weighted coloring of a given series-parallel graph G in time O(n\u03c9max), where n is the number of vertices and \u03c9max is the maximum vertex-weight of G.",
                "id": "000105"
            },
            {
                "title": "A Polynomial-Time Algorithm for Finding Total Colorings of Partial k-Trees",
                "abstract": "\n A total coloring of a graph G is a coloring of all elements of G, i.e. vertices and edges, in such a way that no two adjacent or incident elements receive the same color. Many combinatorial\n problems can be efficiently solved for partial k-trees (graphs of treewidth bounded by a constant k). However, no polynomial-time algorithm has been known for the problem of finding a total coloring of a given partial k-tree with the minimum number of colors. This paper gives such a first polynomial-time algorithm.\n \n ",
                "id": "000106"
            },
            {
                "title": "k-connectivity and decomposition of graphs into forests",
                "abstract": "We show that, for every k -(edge) connected graph G , there exists a sequence T 1 , T 2 ,..., T k of spanning trees with the property that T 1 \u2323 T 2 \u2323 \\h. \u2323 T j is j -(edge) connected for every j = 1,..., k . Nagamochi and Ibaraki have recently presented a linear time decomposition procedure by which such a sequence of trees can be constructed. We discuss some properties of this procedure and its relation to the arboricity of a graph.",
                "id": "000107"
            },
            {
                "title": "Energy complexity and depth of threshold circuits",
                "abstract": "In the paper we show that there is a close relationship between the energy complexity and the depth of threshold circuits computing any Boolean function although they have completely different physical meanings. Suppose that a Boolean function f can be computed by a threshold circuit C of energy complexity e and hence at most e threshold gates in C output \"1\" for any input to C. We then prove that the function f can be computed also by a threshold circuit C\u2032 of depth 2e+1 and hence the parallel computation time of C\u2032 is 2e+1. If the size of C is s, that is, there are s threshold gates in C, then the size s\u2032 of C\u2032 is s\u2032 = 2es+1. Thus, if the size s of C is polynomial in the number n of input variables, then the size s\u2032 of C\u2032 is polynomial in n, too.",
                "id": "000108"
            },
            {
                "title": "Mining Communities on the Web Using a Max-Flow and a Site-Oriented Framework",
                "abstract": "There are several methods for mining communities on the Web using hyperlinks. One of the well-known ones is a max-flow based method proposed by Flake et al. The method adopts a page-oriented framework, that is, it uses a page on the Web as a unit of information, like other methods including HITS and trawling. Recently, Asano et al. built a site-oriented framework which uses a site as a unit of information, and they experimentally showed that trawling on the site-oriented frame- work often outputs significantly better communities than trawling on the page-oriented framework. However, it has not been known whether the site-oriented framework is effective in mining communities through the max-flow based method. In this paper, we first point out several problems of the max-flow based method, mainly owing to the page-oriented frame- work, and then propose solutions to the problems by utilizing several advantages of the site-oriented framework. Computational experiments reveal that our max-flow based method on the site-oriented framework is significantly effective in mining communities, related to the topics of given pages, in comparison with the original max-flow based method on the page-oriented framework.",
                "id": "000109"
            },
            {
                "title": "Inner rectangular drawings of plane graphs",
                "abstract": "A drawing of a plane graph is called an inner rectangular drawing if every edge is drawn as a horizontal or vertical line segment so that every inner face is a rectangle In this paper we show that a plane graph G has an inner rectangular drawing D if and only if a new bipartite graph constructed from G has a perfect matching We also show that D can be found in time O(n1.5/log n) if G has n vertices and a sketch of the outer face is prescribed, that is, all the convex outer vertices and concave ones are prescribed.",
                "id": "000110"
            },
            {
                "title": "Simple Reduction of f-Colorings to Edge-Colorings",
                "abstract": "In an edge-coloring of a graph G = (V, E) each color appears around each vertex at most once. An f-coloring is a generalization of an edge-coloring in which each color appears around each vertex v at most f(v) times where f is a function assigning a natural number f(v)  N to each vertex v  V. In this paper we first give a simple reduction of the f-coloring problem to the ordinary edge-coloring problem, that is, we show that, given a graph G = (V, E) and a function f: V  N, one can directly construct in polynomial-time a new simple graph whose edge-coloring using a minimum number of colors immediately induces an f-coloring of G using a minimum number of colors. As by-products, we give a necessary and sufficient condition for a graph to have an f-factorization, and show that the edge-coloring problem for multigraphs can be easily reduced to edge-coloring problems for simple graphs.",
                "id": "000111"
            },
            {
                "title": "Algorithms for Multicommodity Flows in Planar Graphs",
                "abstract": "This paper gives efficient algorithms for the muiticommodity flow problem for two classes C12 and C01 of planar undirected graphs. Every graph in C12 has two face boundaries B1 and B2 such that each of the source-sink pairs lies on B1 or B2. On the other hand, every graph inC\n 01 has a face boundaryB\n 1 such that some of the source-sink pairs lie onB\n 1 and all the other pairs share a common sink lying onB\n 1. The algorithms run inO(kn +nT(n)) time if a graph hasn vertices andk source-sink pairs andT(n) is the time required for finding the single-source shortest paths in a planar graph ofn vertices.",
                "id": "000112"
            },
            {
                "title": "Total Colorings Of Degenerate Graphs",
                "abstract": "A total coloring of a graph G is a coloring of all elements of G, i.e. vertices and edges, such that no two adjacent or incident elements receive the same color. A graph G is s-degenerate for a positive integer s if G can be reduced to a trivial graph by successive removal of vertices with degree \u2264s. We prove that an s-degenerate graph G has a total coloring with \u0394+1 colors if the maximum degree \u0394 of G is sufficiently large, say \u0394\u22654s+3. Our proof yields an efficient algorithm to find such a total coloring. We also give a lineartime algorithm to find a total coloring of a graph G with the minimum number of colors if G is a partial k-tree, that is, the tree-width of G is bounded by a fixed integer k.",
                "id": "000113"
            },
            {
                "title": "Convex Grid Drawings Of Four-Connected Plane Graphs",
                "abstract": "A convex grid drawing of a plane graph G is a drawing of G on the plane such that all vertices of G are put on grid points, all edges are drawn as straight-line segments without any edge-intersection, and every face boundary is a convex polygon. In this paper we give a linear-time algorithm for finding a convex grid drawing of every 4-connected plane graph G with four or more vertices on the outer face. The size of the drawing satisfies W + H <= n - 1, where n is the number of vertices of G, W is the width and H is the height of the grid drawing. Thus the area W center dot H is at most [(n - 1)/2] - [(n - 1)/2]. Our bounds on the sizes are optimal in a sense that there exist an infinite number of 4-connected plane graphs whose convex drawings need grids such that W + H = n - I and W center dot H = [(n - 1)/2] - [(n - 1)/2].",
                "id": "000114"
            },
            {
                "title": "Spanning Distribution Trees Of Graphs",
                "abstract": "Let G be a graph with a single source w, assigned a positive integer called the supply. Every vertex other than w is a sink, assigned a nonnegative integer called the demand. Every edge is assigned a positive integer called the capacity. Then a spanning tree T of G is called a spanning distribution tree if the capacity constraint holds when, for every sink v, an amount of flow, equal to the demand of v, is sent from w to v along the path in T between them. The spanning distribution tree problem asks whether a given graph has a spanning distribution tree or not. In the paper, we first observe that the problem is NP-complete even for series-parallel graphs, and then give a pseudo-polynomial time algorithm to solve the problem for a given series-parallel graph G. The computation time is bounded by a polynomial in n and D, where n is the number of vertices in G and D is the sum of all demands in G.",
                "id": "000115"
            },
            {
                "title": "Algorithms for routing around a rectangle",
                "abstract": "Simple efficient algorithms are given for three routing problems around a rectangle. The algorithms find routing in two or three layers for two-terminal nets specified on the sides of a rectangle. All algorithms run in linear time. One of the three routing problems is the minimum area routing previously considered by LaPaugh and Gonzalez and Lee. The algorithms they developed run in time O( n 3 ) and O( n ) respectively. Our simple linear time algorithm is based on a theorem of Okamura and Seymour and on a data structure developed by Suzuki, Ishiguro and Nishizeki.",
                "id": "000116"
            }
        ]
    },
    {
        "id": "001",
        "input": "For an author who has written the paper with the title \"Autonomous and Interactive Improvement of Binocular Visual Depth Estimation through Sensorimotor Interaction\", which reference is related? Just answer with [1] or [2] without explanation. [1]: \"Neural networks with dynamic synapses.\" [2]: \"Uncertainty Measure for Selective Sampling Based on Class Probability Output Networks.\"",
        "profile": [
            {
                "title": "A local maximum intensity projection tracing of vasculature in knife-edge scanning microscope volume data",
                "abstract": "A local maximum intensity projection (MIP) approach to the extraction of a 3D vascular network, acquired by the Knife-Edge Scanning Microscope (KESM), is presented. We build a local volume for local MIP processing at each tracing step in order to reduce the dimension of input data from 3D to 2D, which leads to a 65.22% reduction of computation time compared to 3D tracing method. The proposed method makes use of existing 2D tracing methods, extending them into a 3D tracing method. Our experimental results show that our approach can rapidly and accurately extract the medial axis of vascular data acquired by the KESM.",
                "id": "0010"
            },
            {
                "title": "Fast Cell Detection In High-Throughput Imagery Using Gpu-Accelerated Machine Learning",
                "abstract": "High-throughput microscopy allows fast imaging of large tissue samples, producing an unprecedented amount of subcellular information. The size and complexity of these data sets often out-scale current reconstruction algorithms. Overcoming this computational bottleneck requires extensive parallel processing and scalable algorithms. As high-throughput imaging techniques move into main stream research, processing must also be inexpensive and easily available.In this paper, we describe a method for cell soma detection in Knife-Edge Scanning Microscopy (KESM) using machine learning. The proposed method requires very little training data and can be mapped to consumer graphics hardware, allowing us to perform real-time cell detection at a rate that exceeds the data rate of KESM.",
                "id": "0011"
            },
            {
                "title": "Facilitating neural dynamics for delay compensation and prediction in evolutionary neural networks",
                "abstract": "Delay in the nervous system is a serious issue for an organism that needs to act in real time. For example, during the time a signal travels from a peripheral sensor to the central nervous system, a moving object in the environment can cover a significant distance which can lead to critical errors in the effect of the corresponding motor output. This paper proposes that facilitating synapses which show a dynamic sensitivity to the changing input may play an important role in compensating for neural delays, through extrapolation. The idea was tested in a modified 2D pole-balancing problem which included sensory delays. Within this domain, we tested the behavior of recurrent neural networks with facilitatory neural dynamics trained via neuroevolution. Analysis of the performance and the evolved network parameters showed that, under various forms of delay, networks utilizing extrapolatory dynamics are at a significant competitive advantage compared to networks without such dynamics. In sum, facilitatory (or extrapolatory) dynamics can be used to compensate for delay at a single-neuron level, thus allowing a developing nervous system to stay in touch with the present environmental state.",
                "id": "0012"
            },
            {
                "title": "Probabilistic Combination of Multiple Evidence",
                "abstract": "In pattern recognition systems, data fusion is an important issue and evidence theory is one such method that has been successful. Many researchers have proposed different rules for evidence theory, and recently, a variety of averaging rules emerged that are better than others. In these methods, the key issue becomes how to give the weights to the multiple contributing factors, in order to calculate the average. To get better weights for the multiple bodies of evidence, we propose the use of structural information of the evidence. The bodies of evidence lie on a certain informational structure which can be described by a probability distribution and the probability of each evidence can serve as a weight for the evidence. Our experimental results show that our method outperforms other previous methods.",
                "id": "0013"
            },
            {
                "title": "Explaining Low-Level Brightness-Contrast Illusions Using Disinhibition",
                "abstract": "Conventional Difference of Gaussian (DOG) filter is usually used to model the early stage of visual processing. However, convolution operation used with DOG does not explicitly account for the effects of disinhibition. Because of this, complex brightness-contrast (B-C) illusions such as the White's effect cannot be explained using DOG filters. We discovered that a model based on lateral disinhibition in biological retinas allows us to explain subtle B-C illusions. Further, we show that a feedforward filter can be derived to achieve this operation in a single pass. The results suggest that contextual effects can be processed through recurrent disinhibition. Such a context sensitive structure might be employed to improve network robustness of visual capturing or displaying systems. Another potential application of this algorithm could be automatic detection and correction of perceived incoherences where accurate perception of intensity level is critical.",
                "id": "0014"
            },
            {
                "title": "Kernel Oriented Discriminant Analysis For Speaker-Independent Phoneme Spaces",
                "abstract": "Speaker independent feature extraction is a critical problem in speech recognition. Oriented principal component analysis (OPCA) is a potential solution that can find a subspace robust against noise of the data set. The objective of this paper is to find a speaker-independent subspace by generalizing OPCA in two steps: First, we find a nonlinear subspace with the help of a kernel trick, which we refer to as kernel OPCA. Second, we generalize OPCA to problems with more than two phonemes, which leads to oriented discriminant analysis (ODA). In addition, we equip ODA with the kernel trick again, which we refer to as kernel ODA. The models are tested on the CMU ARCTIC speech database. Our results indicate that our proposed kernel methods can outperform linear OPCA and linear ODA at finding a speaker-independent phoneme space.",
                "id": "0015"
            },
            {
                "title": "Motion-based autonomous grounding: inferring external world properties from encoded internal sensory states alone",
                "abstract": "How can we build artificial agents that can autonomously explore and understand their environments? An immediate requirement for such an agent is to learn how its own sensory state corresponds to the external world properties: It needs to learn the semantics of its internal state (i.e., grounding). In principle, we as programmers can provide the agents with the required semantics, but this will compromise the autonomy of the agent. To overcome this problem, we may fall back on natural agents and see how they acquire meaning of their own sensory states, their neural firing patterns. We can learn a lot about what certain neural spikes mean by carefully controlling the input stimulus while observing how the neurons fire. However, neurons embedded in the brain do not have direct access to the outside stimuli, so such a stimulus-to-spike association may not be learnable at all. How then can the brain solve this problem? (We know it does.) We propose that motor interaction with the environment is necessary to overcome this conundrum. Further, we provide a simple yet powerful criterion, sensory invariance, for learning the meaning of sensory states. The basic idea is that a particular form of action sequence that maintains invariance of a sensory state will express the key property of the environmental stimulus that gave rise to the sensory state. Our experiments with a sensorimotor agent trained on natural images show that sensory invariance can indeed serve as a powerful objective for semantic grounding.",
                "id": "0016"
            },
            {
                "title": "Scalable, incremental learning with MapReduce parallelization for cell detection in high-resolution 3D microscopy data",
                "abstract": "Accurate estimation of neuronal count and distribution is central to the understanding of the organization and layout of cortical maps in the brain, and changes in the cell population induced by brain disorders. High-throughput 3D microscopy techniques such as Knife-Edge Scanning Microscopy (KESM) are enabling whole-brain survey of neuronal distributions. Data from such techniques pose serious challenges to quantitative analysis due to the massive, growing, and sparsely labeled nature of the data. In this paper, we present a scalable, incremental learning algorithm for cell body detection that can address these issues. Our algorithm is computationally efficient (linear mapping, non-iterative) and does not require retraining (unlike gradient-based approaches) or retention of old raw data (unlike instance-based learning). We tested our algorithm on our rat brain Nissl data set, showing superior performance compared to an artificial neural network-based benchmark, and also demonstrated robust performance in a scenario where the data set is rapidly growing in size. Our algorithm is also highly parallelizable due to its incremental nature, and we demonstrated this empirically using a MapReduce-based implementation of the algorithm. We expect our scalable, incremental learning approach to be widely applicable to medical imaging domains where there is a constant flux of new data.",
                "id": "0017"
            },
            {
                "title": "Predictive internal neural dynamics for delay compensation",
                "abstract": "Neural transmission delay may cause serious problems unless a compensation mechanism exists in the neural system. We showed previously that facilitating neural dynamics is a key mechanism to compensate for delay in the neural transmission. We also investigated a role of internal neural dynamics in an evolutionary context: Internal neural dynamics that are predictable was found to give an evolutionary advantage. Here we suggest that predictable internal neural dynamics modulated by neural facilitation helps neural networks in coping with longer delay in neural signal transmission while giving the agents an evolutionary edge. The results are profound since they show a road to prediction that is an important prerequisite to cognitive abilities such as goal-directed behavior in intelligent systems. We expect our results to help better understand the role of predictive internal neural dynamics in both natural and artificial agents.",
                "id": "0018"
            },
            {
                "title": "Predictable internal brain dynamics in EEG and its relation to conscious states.",
                "abstract": "Consciousness is a complex and multi-faceted phenomenon defying scientific explanation. Part of the reason why this is the case is due to its subjective nature. In our previous computational experiments, to avoid such a subjective trap, we took a strategy to investigate objective necessary conditions of consciousness. Our basic hypothesis was that predictive internal dynamics serves as such a condition. This is in line with theories of consciousness that treat retention (memory), protention (anticipation), and primary impression as the tripartite temporal structure of consciousness. To test our hypothesis, we analyzedpublicly available sleep and awake electroencephalogram (EEG) data. Our results show that EEG signals from awake or rapid eye movement (REM) sleep states have more predictable dynamics compared to those from slow-wave sleep (SVVS). Since awakeness and REM sleep are associated with conscious states and SVVS with unconscious or less consciousness states, these results support our hypothesis. The results suggest an intricate relationship among prediction, consciousness, and time, with potential applications to time perception and neurorobotics.",
                "id": "0019"
            },
            {
                "title": "Dynamical pathway analysis.",
                "abstract": "Although a great deal is known about one gene or protein and its functions under different environmental conditions, little information is available about the complex behaviour of biological networks subject to different environmental perturbations. Observing differential expressions of one or more genes between normal and abnormal cells has been a mainstream method of discovering pertinent genes in diseases and therefore valuable drug targets. However, to date, no such method exists for elucidating and quantifying the differential dynamical behaviour of genetic regulatory networks, which can have greater impact on phenotypes than individual genes.We propose to redress the deficiency by formulating the functional study of biological networks as a control problem of dynamical systems. We developed mathematical methods to study the stability, the controllability, and the steady-state behaviour, as well as the transient responses of biological networks under different environmental perturbations. We applied our framework to three real-world datasets: the SOS DNA repair network in E. coli under different dosages of radiation, the GSH redox cycle in mice lung exposed to either poisonous air or normal air, and the MAPK pathway in mammalian cell lines exposed to three types of HIV type I Vpr, a wild type and two mutant types; and we found that the three genetic networks exhibited fundamentally different dynamical properties in normal and abnormal cells.Difference in stability, relative stability, degrees of controllability, and transient responses between normal and abnormal cells means considerable difference in dynamical behaviours and different functioning of cells. Therefore differential dynamical properties can be a valuable tool in biomedical research.",
                "id": "00110"
            },
            {
                "title": "Autonomous Learning Of The Semantics Of Internal Sensory States Based On Motor Exploration",
                "abstract": "What is available to developmental programs in autonomous mental development, and what should be learned at the very early stages of mental development? Our observation is that sensory and motor primitives are the most basic components present at the beginning, and what developmental agents need to learn from these resources is what their internal sensory states stand for. In this paper, we investigate the question in the context of a simple biologically motivated visuomotor agent. We observe and acknowledge, as many other researchers do, that action plays a key role in providing content to the sensory state. We propose a simple, yet powerful learning criterion, that of invariance, where invariance simply means that the internal state does not change over time. We show that after reinforcement learning based on the invariance criterion, the property of action sequence based on an internal sensory state accurately reflects the property of the stimulus that triggered that internal state. That way, the meaning of the internal sensory state can be firmly grounded on the property of that particular action sequence. We expect the framing of the problem and the proposed solution presented in this paper to help shed new light on autonomous understanding in developmental agents such as humanoid robots.",
                "id": "00111"
            },
            {
                "title": "Effects Of Varying The Delay Distribution In Random, Scale-Free, And Small-World Networks",
                "abstract": "Graph-theory-based approaches have been used with great success when analyzing abstract properties of natural and artificial networks. However these approaches have not factored in delay, which plays an important role in real-world networks. In this paper we (1) developed a simple yet powerful method to include delay in graph-based analysis of networks, and (2) evaluated how different classes of networks (random, scale-free, and small-world) behave under different forms of delay (peaked, unimodal, or uniform delay distribution). We compared results from synthetically generated networks using two different sets of algorithms for network construction. In the first approach (naive), we generated directed graphs following the literal definition of the three types of networks. In the second approach (modified conventional), we adapted methods by Erdos-Renyi (random), Barabasi (scale-free), and Watts-Strogatz (small-world). With these networks, we investigated the effect of adding and varying the delay distribution. As a measure of robustness to added delay, we calculated the ratio between the sum of shortest path length between every node. Our main findings show that different types of network show different levels of robustness, but the shape of the delay distribution has more influence on the overall result, where uniformly randomly distributed delay showed the most robust result. Other network parameters such as neighborhood size in small-world networks were also found to play a key role in delay tolerance. These results are expected to extend our understanding of the relationship between network structure and delay.",
                "id": "00112"
            },
            {
                "title": "The role of temporal parameters in a thalamocortical model of analogy.",
                "abstract": "How multiple specialized cortical areas in the brain interact with each other to give rise to an integrated behavior is a largely unanswered question. This paper proposes that such an integration can be understood under the framework of analogy and that part of the thalamus and the thalamic reticular nucleus (TRN) may be playing a key role in this respect. The proposed thalamocortical model of analogy heavily depends on a diverse set of temporal parameters including axonal delay and membrane time constant, each of which is critical for the proper functioning of the model. The model requires a specific set of conditions derived from the need of the model to process analogies. Computational results with a network of integrate and fire (IF) neurons suggest that these conditions are indeed necessary, and furthermore, data found in the experimental literature also support these conditions. The model suggests that there is a very good reason for each temporal parameter in the thalamocortical network having a particular value, and that to understand the integrated behavior of the brain, we need to study these parameters simultaneously, not separately.",
                "id": "00113"
            },
            {
                "title": "Relative Advantage Of Touch Over Vision In The Exploration Of Texture",
                "abstract": "Texture segmentation is an effortless process in scene analysis, yet its mechanisms have not been sufficiently understood. A common assumption in most current approaches is that texture segmentation is a vision problem. However, considering that texture is basically a surface property, this assumption can at times be misleading. One interesting possibility is that texture may be more intimately related with touch than with vision. Recent neurophysiological findings showed that receptive fields for touch resemble that of vision, albeit with some subtle differences. To leverage on this, we tested how such distinct properties in tactile receptive fields can affect texture segmentation performance, as compared to that of visual receptive fields. Our main results suggest that touch has an advantage over vision in texture processing. We expect our findings to shed new light on the role of tactile perception of texture and its interaction with vision, and help develop more powerful, biologically inspired texture segmentation algorithms.",
                "id": "00114"
            },
            {
                "title": "Emergence Of Tool Use In An Articulated Limb Controlled By Evolved Neural Circuits",
                "abstract": "Tool use requires high levels of cognitive function and is only observed in higher mammals and some avian species such as corvids. In this paper, we will investigate how the capability to use tools can spontaneously emerge in a simulated evolution of a two degree-of-freedom articulated limb. The controller for the limb was evolved as neural circuits that can gradually take on arbitrary topologies (NeuroEvolution of Augmenting Topologies, or NEAT). First, we show how very broad fitness criteria such as distance to the target, number of steps to reach the target, and tool pick-up frequency are enough to give rise to tool using behavior. Second, we analyze the evolved neural circuits to find properties that enable tool use. We expect our results to help us understand the origin of tool use and the kind of neural circuits that enabled such a powerful trait.",
                "id": "00115"
            },
            {
                "title": "Self-Organization of Tactile Receptive Fields: Exploring Their Textural Origin and Their Representational Properties",
                "abstract": "In our earlier work, we found that feature space induced by tactile receptive fields (TRFs) are better than that by visual receptive fields (VRFs) in texture boundary detection tasks. This suggests that TRFs could be intimately associated with texture-like input. In this paper, we investigate how TRFs can develop in a cortical learning context. Our main hypothesis is that TRFs can be self-organized using the same cortical development mechanism found in the visual cortex, simply by exposing it to texture-like inputs (as opposed to natural-scene-like inputs). To test our hypothesis, we used the LISSOM model of visual cortical development. Our main results show that texture-like inputs lead to the self-organization of TRFs while natural-scene-like inputs lead to VRFs. These results suggest that TRFs can better represent texture than VRFs. We further analyzed the effectiveness of TRFs in representing texture, using kernel Fisher discriminant (KFD) and the results, along with texture classification performance, confirm that this is indeed the case. We expect these results to help us better understand the nature of texture, as a fundamentally tactile property.",
                "id": "00116"
            },
            {
                "title": "A neural model of the scintillating grid illusion: disinhibition and self-inhibition in early vision.",
                "abstract": "A stationary display of white discs positioned on intersecting gray bars on a dark background gives rise to a striking scintillating effect--the scintillating grid illusion. The spatial and temporal properties of the illusion are well known, but a neuronal-level explanation of the mechanism has not been fully investigated. Motivated by the neurophysiology of the Limulus retina, we propose disinhibition and self-inhibition as possible neural mechanisms that may give rise to the illusion. In this letter, a spatiotemporal model of the early visual pathway is derived that explicitly accounts for these two mechanisms. The model successfully predicted the change of strength in the illusion under various stimulus conditions, indicating that low-level mechanisms may well explain the scintillating effect in the illusion.",
                "id": "00117"
            },
            {
                "title": "Analogical cascade: a theory on the role of the thalamo-cortical loop in brain function.",
                "abstract": "The cortico-thalamic connections and the nucleus reticularis thalami (nRt) have gained much attention lately because of their integrative and modulatory functions. This particular architecture has been thought to perform analysis and synthesis of memories, work as active blackboards or a global workspace, or gate attention and give rise to consciousness. In this paper, I show that this circuitry can be implementing a general analogical functionality. The concrete connection between analogical function and its exact neural basis established in this paper can help us better understand the brain function. (C) 2003 Elsevier Science B.V. All rights reserved.",
                "id": "00118"
            },
            {
                "title": "Electron microscopy image segmentation with graph cuts utilizing estimated symmetric three-dimensional shape prior",
                "abstract": "Understanding neural connectivity and structures in the brain requires detailed three-dimensional (3D) anatomical models, and such an understanding is essential to the study of the nervous system. However, the reconstruction of 3D models from a large set of dense nanoscale microscopy images is very challenging, due to the imperfections in staining and noise in the imaging process. To overcome this challenge, we present a 3D segmentation approach that allows segmenting densely packed neuronal structures. The proposed algorithm consists of two main parts. First, different from other methods which derive the shape prior in an offline phase, the shape prior of the objects is estimated directly by extracting medial surfaces from the data set. Second, the 3D image segmentation problem is posed as Maximum A Posteriori (MAP) estimation of Markov Random Field (MRF). First, the MAPMRF formulation minimizes the Gibbs energy function, and then we use graph cuts to obtain the optimal solution to the energy function. The energy function consists of the estimated shape prior, the flux of the image gradients, and the gray-scale intensity. Experiments were conducted on synthetic data and nanoscale image sequences from the Serial Block Face Scanning Electron Microscopy (SBFSEM). The results show that the proposed approach provides a promising solution to EM reconstruction. We expect the reconstructed geometries to help us better analyze and understand the structure of various kinds of neurons.",
                "id": "00119"
            },
            {
                "title": "Emergence of Memory in Reactive Agents Equipped With Environmental Markers",
                "abstract": "In the neuronal circuits of natural and artificial agents, memory is usually implemented with recurrent connections, since recurrence allows past agent state to affect the present, on-going behavior. Here, an interesting question arises in the context of evolution: how reactive agents could have evolved into cognitive ones with internalized memory? Our idea is that reactive agents with simple feedforward circuits could have achieved behavior comparable to internal memory if they can drop and detect external markers (e.g., pheromones or excretions) in the environment. We tested this idea in two tasks (ball-catching and food-foraging task) where agents needed memory to be successful. We evolved feedforward neural network controllers with a dropper and a detector, and compared their performance with recurrent neural network controllers. The results show that feedforward controllers with external material interaction show adequate performance compared to recurrent controllers in both tasks. This means that even memoryless feedforward networks can evolve behavior that can solve tasks requiring memory, when material interaction is allowed. These results are expected to help us better understand the possible evolutionary route from reactive to cognitive agents.",
                "id": "00120"
            },
            {
                "title": "Fast and accurate retinal vasculature tracing and kernel-Isomap-based feature selection",
                "abstract": "The blood vessels in the retina have a characteristic radiating pattern, while there exists a significant variation dependent on the individual and/or medical condition. Extracting the geometric properties of these blood vessels have several important applications, such as biometrics (for identification) and medical diagnosis. In this paper, we will focus on biometric applications. For this, we propose a fast and accurate algorithm for tracing the blood vessels, and compare several candidate summary features based on the tracing results. Existing tracing algorithms based on a detailed analysis of the image can be too slow to quickly process a large volume of retinal images in real time (e.g., at a security check point). In order to select good features that can be extracted from the traces, we used kernel Isomap to test the distance between different retinal images as projected onto their respective feature spaces. We tested the following feature set: (1) angle among branches, (2) the number of fiber based on distance, (3) distance between branches, and (4) inner product among branches. Our results indicate that features 3 and 4 are prime candidates for use in fast, realtime biometric tasks. We expect our method to lead to fast and accurate biometric systems based on retinal images.",
                "id": "00121"
            },
            {
                "title": "Neural model of disinhibitory interactions in the modified Poggendorff illusion.",
                "abstract": "Visual illusions can be strengthened or weakened with the addition of extra visual elements. For example, in the Poggendorff illusion, with an additional bar added, the illusory skew in the perceived angle can be enlarged or reduced. In this paper, we show that a nontrivial interaction between lateral inhibitory processes in the early visual system (i.e., disinhibition) can explain such an enhancement or degradation of the illusory effect. The computational model we derived successfully predicted the perceived angle in the Poggendorff illusion task that was modified to include an extra thick bar. The concept of disinhibition employed in the model is general enough that we expect it can be further extended to account for other classes of geometric illusions.",
                "id": "00122"
            },
            {
                "title": "How Compact?: Assessing Compactness of Representations through Layer-Wise Pruning.",
                "abstract": "Various forms of representations may arise in the many layers embedded in deep neural networks (DNNs). Of these, where can we find the most compact representation? We propose to use a pruning framework to answer this question: How compact can each layer be compressed, without losing performance? Most of the existing DNN compression methods do not consider the relative compressibility of the individual layers. They uniformly apply a single target sparsity to all layers or adapt layer sparsity using heuristics and additional training. We propose a principled method that automatically determines the sparsity of individual layers derived from the importance of each layer. To do this, we consider a metric to measure the importance of each layer based on the layer-wise capacity. Given the trained model and the total target sparsity, we first evaluate the importance of each layer from the model. From the evaluated importance, we compute the layer-wise sparsity of each layer. The proposed method can be applied to any DNN architecture and can be combined with any pruning method that takes the total target sparsity as a parameter. To validate the proposed method, we carried out an image classification task with two types of DNN architectures on two benchmark datasets and used three pruning methods for compression. In case of VGG-16 model with weight pruning on the ImageNet dataset, we achieved up to 75% (17.5% on average) better top-5 accuracy than the baseline under the same total target sparsity. Furthermore, we analyzed where the maximum compression can occur in the network. This kind of analysis can help us identify the most compact representation within a deep neural network.",
                "id": "00123"
            },
            {
                "title": "Tracing and analysis of the whole mouse brain vasculature with systematic cleaning to remove and consolidate erroneous images",
                "abstract": "Whole mouse brain microvascular images at submicrometer scale can be obtained by Knife-Edge Scanning Microscopy (KESM). However, due to the large size of the image dataset and the noise from the serial sectioning process of the KESM, whole mouse brain vascular reconstruction and analysis with submicrometer resolution have not been achieved yet, while several previous studies demonstrated manually selected small noise-free portion of the KESM dataset. In addition to the KESM dataset, there have been studies for vessel reconstruction and analysis of the whole mouse brain at lower resolution or of partial brain regions at submicrometer resolution. However, to the best of our knowledge, there has been no study for vessel reconstruction and analysis of the whole mouse brain at submicrometer resolution. In this paper, we propose a framework for the 3D reconstruction and analysis of the whole KESM mouse brain vasculature dataset with rich vasculature information extracted at submicrometer resolution. The framework consists of two methods. The propose methods provide the systematic cleaning to remove and consolidate erroneous images automatically, which enables the full tracing and analysis of the whole KESM mouse brain dataset with richer vasculature information.",
                "id": "00124"
            },
            {
                "title": "Evolution of recollection and prediction in neural networks",
                "abstract": "A large number of neural network models are based on a feedforward topology (perceptrons, backpropagation networks, radial basis functions, support vector machines, etc.), thus lacking dynamics. In such networks, the order of input presentation is meaningless (i.e., it does not affect the behavior) since the behavior is largely reactive. That is, such neural networks can only operate in the present, having no access to the past or the future. However, biological neural networks are mostly constructed with a recurrent topology, and recurrent (artificial) neural network models are able to exhibit rich temporal dynamics, thus time becomes an essential factor in their operation. In this paper, we will investigate the emergence of recollection and prediction in evolving neural networks. First, we will show how reactive, feedforward networks can evolve a memory-like function (recollection) through utilizing external markers dropped and detected in the environment. Second, we will investigate how recurrent networks with more predictable internal state trajectory can emerge as an eventual winner in evolutionary struggle when competing networks with less predictable trajectory show the same level of behavioral performance. We expect our results to help us better understand the evolutionary origin of recollection and prediction in neuronal networks, and better appreciate the role of time in brain function.",
                "id": "00125"
            },
            {
                "title": "A Self-Organizing Neural Network Model Of The Primary Visual Cortex",
                "abstract": "Based on recent experimental results on the connectivity and plasticity of the visual cortex, a new theory of the visual cortex has started to emerge. The visual cortex appears to be a continuously-adapting structure in a dynamic equilibrium with both the external and intrinsic input. This equilibrium is maintained by cooperative and competitive interactions within the cortex, mediated by lateral connections. With the advent of parallel supercomputers in the past few years, it has become viable to test theories about these mechanisms computationally. This paper describes a computational model of the primary visual cortex called RF-SLISSOM that shows how the observed receptive fields, columnar organization, and lateral connectivity can arise through input-driven Hebbian self-organization, and how such plasticity can account for partial recovery following retinal and cortical lesions. The self-organized network forms a redundancy-reduced sparse coding of the input, which allows it to process massive amounts of information efficiently. The self-organized model can then be used to model various low-level visual phenomena, including tilt aftereffects and segmentation and binding.",
                "id": "00126"
            },
            {
                "title": "Tactile or visual?: Stimulus characteristics determine receptive field type in a self-organizing map model of cortical development",
                "abstract": "Tactile receptive fields (RFs) are similar to visual receptive fields, while there is a subtle difference. Our previous work showed that tactile RFs have advantage in texture boundary detection tasks compared to visual RFs. Our working hypothesis was that tactile RFs are better in texture tasks since texture is basically a surface property, more intimately linked with touch than with vision. From an information processing point of view, touch and vision are very similar (i.e., two-dimensional sensory surface). Then, the question is what drives the two types of RFs to become different? In this paper, we investigated the possibility that tactile RF and visual RF emerge based on an identical cortical learning process, where the only difference is in the input type, natural-scene-like vs. texture-like. We trained a self-organizing map model of the cortex (the LISSOM model) on two different kinds of input, (1) natural scene and (2) texture, and compared the resulting RFs. The main result is that RFs trained on natural scenes have RFs resembling visual RFs, while those trained on texture resemble tactile RFs. These results suggest that the type of input most commonly stimulating the sensory modality (natural scene for vision and texture for touch), and not the intrinsic organization of the sensors or the developmental process in the cortex, determine the RF property. We expect these results to shed new light on the differences and similarities between touch and vision.",
                "id": "00127"
            },
            {
                "title": "Cell tracking and segmentation in electron microscopy images using graph cuts",
                "abstract": "Understanding neural connectivity and structures in the brain requires detailed 3D anatomical models, and such an understanding is essential to the study of the nervous system. However, the reconstruction of 3D models from a large set of dense nanoscale medical images is very challenging, due to the imperfections in staining and noise in the imaging process. Manual segmentation in 2D followed by tracking the 2D contours through cross-sections to build 3D structures can be a solution, but it is impractical. In this paper, we propose an automated tracking and segmentation framework to extract 2D contours and to trace them through the z direction. The segmentation is posed as an energy minimization problem and solved via graph cuts. The energy function to be minimized contains a regional term and a boundary term. The regional term is defined over the flux of the gradient vector fields and the distance function. Our main idea is that the distance function should carry the information of the segmentation from the previous image based on the assumption that successive images have a similar segmentation. The boundary term is defined over the gray-scale intensity of the image. Experiments were conducted on nanoscale image sequences from the Serial Block Face Scanning Electron Microscope (SBF-SEM). The results show that our method can successfully track and segment densely packed cells in EM image stacks.",
                "id": "00128"
            },
            {
                "title": "Learning Alpha-Integration With Partially-Labeled Data",
                "abstract": "Sensory data integration is an important task in human brain for multimodal processing as well as in machine learning for multisensor processing. alpha-integration was proposed by Amari as a principled way of blending multiple positive measures (e.g., stochastic models in the form of probability distributions), providing an optimal integration in the sense of minimizing the alpha-divergence. It also encompasses existing integration methods as its special case, e. g., weighted average and exponential mixture. In alpha-integration, the value of a determines the characteristics of the integration and the weight vector w assigns the degree of importance to each measure. In most of the existing work, however, a and w are given in advance rather than learned. In this paper we present two algorithms, for learning a and w from data when only a few integrated target values are available. Numerical experiments on synthetic as well as real-world data confirm the proposed method's effectiveness.",
                "id": "00129"
            },
            {
                "title": "An interactive editing framework for electron microscopy image segmentation",
                "abstract": "There are various automated segmentation algorithms for medical images. However, 100% accuracy may be hard to achieve because medical images usually have low contrast and high noise content. These segmentation errors may require manual correction. In this paper, we present an interactive editing framework that allows the user to quickly correct segmentation errors produced by automated segmentation algorithms. The framework includes two editing methods: (1) editing through multiple choice and (2) interactive editing through graph cuts. The first method provides a set of alternative segmentations generated from a confidence map that carries valuable information from multiple cues, such as the probability of a boundary, an intervening contour cue, and a soft segmentation by a random walker. The user can then choose the most acceptable one from those segmentation alternatives. The second method introduces an interactive editing tool where the user can interactively connect or disconnect presegmented regions. The editing task is posed as an energy minimization problem: We incorporate a set of constraints into the energy function and obtain an optimal solution via graph cuts. The results show that the proposed editing framework provides a promising solution for the efficient correction of incorrect segmentation results.",
                "id": "00130"
            },
            {
                "title": "Parameter learning for alpha integration.",
                "abstract": "In pattern recognition, data integration is an important issue, and when properly done, it can lead to improved performance. Also, data integration can be used to help model and understand multimodal processing in the brain. Amari proposed \u03b1-integration as a principled way of blending multiple positive measures (e.g., stochastic models in the form of probability distributions), enabling an optimal integration in the sense of minimizing the \u03b1-divergence. It also encompasses existing integration methods as its special case, for example, a weighted average and an exponential mixture. The parameter \u03b1 determines integration characteristics, and the weight vector w assigns the degree of importance to each measure. In most work, however, \u03b1 and w are given in advance rather than learned. In this letter, we present a parameter learning algorithm for learning \u03b1 and \u03c9 from data when multiple integrated target values are available. Numerical experiments on synthetic as well as real-world data demonstrate the effectiveness of the proposed method.",
                "id": "00131"
            },
            {
                "title": "Alpha-integration of multiple evidence",
                "abstract": "In pattern recognition, data integration is a processing method to combine multiple sources so that the combined result can be more accurate than a single source. Evidence theory is one of the methods that have been successfully applied to the data integration task. Since Dempster-Shafer theory as the first evidence theory can be against our intuitive reasoning with some data sets, many researchers have proposed different rules for evidence theory. Among all these rules, the averaging rule is known to be better than others. On the other hand, a-integration was proposed by Amari as a principled way of blending multiple positive measures. It is a generalized averaging algorithm including arithmetic, geometric and harmonic means as its special case. In this paper, we generalize evidence theory with \u03b1-integration. Our experimental results show how our proposed methods work.",
                "id": "00132"
            },
            {
                "title": "Construction of Anatomically Correct Models of Mouse Brain Networks",
                "abstract": "The Mouse Brain Web (MBW), a web-organized database, provides for the construction of anatomically correct models of mouse brain networks. Each web page in this database provides the position, orientation, morphology, and putative synapses for each biologically observed neuron. The MBW has been designed to support (1) mapping of the spatial distribution and morphology of neurons by type; (2) wiring of the network\u2013synaptic assembly; (3) projection of neuron morphology and synapses to geometric multi-compartmental models; (4) search for motifs and basic circuits in the brain networks using customized web-crawlers; and (5) the mapping of anatomically correct networks to physiologically correct network simulations.",
                "id": "00133"
            },
            {
                "title": "Autonomous and Interactive Improvement of Binocular Visual Depth Estimation through Sensorimotor Interaction",
                "abstract": "We investigate how a humanoid robot with a randomly initialized binocular vision system can learn to improve judgments about egocentric distances using limited action and interaction that might be available to human infants. First, we show how distance estimation can be improved autonomously. We consider our approach to be autonomous because the robot learns to accurately estimate distance without a human teacher providing the distances to training targets. We find that actions that, in principle, do not alter the robot's distance to the target are a powerful tool for exposing estimation errors. These errors can be used to train a distance estimator. Furthermore, the simple action used (i.e., neck rotation) does not require high level cognitive processing or fine motor skill. Next, we investigate how interaction with humans can further improve visual distance estimates. We find that human interaction can improve distance estimates for far targets outside of the robot's peripersonal space. This is accomplished by extending our autonomous approach above to integrate additional information provided by a human. Together these experiments suggest that both action and interaction are important tools for improving perceptual estimates.",
                "id": "00134"
            },
            {
                "title": "Ground truth estimation by maximizing topological agreements in electron microscopy data",
                "abstract": "Manual editing can correct segmentation errors produced by automated segmentation algorithms, but it also introduces a practical challenge: the combination of multiple users' annotations of an image to obtain an estimation of the true, unknown labeling. Current estimation methods are not suited for electron microscopy (EM) images because they typically do not take into account topological correctness of a segmentation that can be critical in EM analysis. This paper presents a ground truth estimation method for EM images. Taking a collection of alternative segmentations, the algorithm seeks an estimated segmentation that is topologically equivalent and geometrically similar to the true, unknown segmentation. To this end, utilizing warping error as the evaluation metric, which measures topological disagreements between segmentations, the algorithm iteratively modifies the topology of an estimated segmentation to minimize the topological disagreements between this estimated segmentation and the given segmentations. Our experimental results obtained using EM images with densely packed cells demonstrate that the proposed method is superior to majority voting and STAPLE commonly used for combining multiple segmentation results.",
                "id": "00135"
            },
            {
                "title": "Context-sensitive intra-class clustering",
                "abstract": "This paper describes a new semi-supervised learning algorithm for intra-class clustering (ICC). ICC partitions each class into sub-classes in order to minimize overlap across clusters from different classes. This is achieved by allowing partitioning of a certain class to be assisted by data points from other classes in a context-dependent fashion. The result is that overlap across sub-classes (both within- and across class) is greatly reduced. ICC is particularly useful when combined with algorithms that assume that each class has a unimodal Gaussian distribution (e.g., Linear Discriminant Analysis (LDA), quadratic classifiers), an assumption that is not always true in many real-world situations. ICC can help partition non-Gaussian, multimodal distributions to overcome such a problem. In this sense, ICC works as a preprocessor. Experiments with our ICC algorithm on synthetic data sets and real-world data sets indicated that it can significantly improve the performance of LDA and quadratic classifiers. We expect our approach to be applicable to a broader class of pattern recognition problems where class-conditional densities are significantly non-Gaussian or multi-modal.",
                "id": "00136"
            },
            {
                "title": "Facilitating neural dynamics for delay compensation: a road to predictive neural dynamics?",
                "abstract": "Goal-directed behavior is a hallmark of cognition. An important prerequisite to goal-directed behavior is that of prediction. In order to establish a goal and devise a plan, one needs to see into the future and predict possible future events. Our earlier work has suggested that compensation mechanisms for neuronal transmission delay may have led to a preliminary form of prediction. In that work, facilitating neuronal dynamics was found to be effective in overcoming delay (the Facilitating Activation Network model, or FAN). The extrapolative property of the delay compensation mechanism can be considered as prediction for incoming signals (predicting the present based on the past). The previous FAN model turns out to have a limitation especially when longer delay needs to be compensated, which requires higher facilitation rates than FAN\u2019s normal range. We derived an improved facilitating dynamics at the neuronal level to overcome this limitation. In this paper, we tested our proposed approach in controllers for 2D pole balancing, where the new approach was shown to perform better than the previous FAN model. Next, we investigated the differential utilization of facilitating dynamics in sensory vs. motor neurons and found that motor neurons utilize the facilitating dynamics more than the sensory neurons. These findings are expected to help us better understand the role of facilitating dynamics in delay compensation, and its potential development into prediction, a necessary condition for goal-directed behavior.",
                "id": "00137"
            },
            {
                "title": "Manifold integration with Markov random walks",
                "abstract": "Most manifold learning methods consider only one similarity matrix to induce a low-dimensional manifold embedded in data space. In practice, however, we often use multiple sensors at a time so that each sensory information yields different similarity matrix derived from the same objects. In such a case, manifold integration is a desirable task, combining these similarity matrices into a compromise matrix that faithfully reflects multiple sensory information. A small number of methods exists for manifold integration, including a method based on reproducing kernel Krein space (RKKS) or DISTATIS, where the former is restricted to the case of only two manifolds and the latter considers a linear combination of normalized similarity matrices as a compromise matrix. In this paper we present a new manifold integration method, Markov random walk on multiple manifolds (RAMS), which integrates transition probabilities defined on each manifold to compute a compromise matrix. Numerical experiments confirm that RAMS finds more informative manifolds with a desirable projection property.",
                "id": "00138"
            },
            {
                "title": "Autonomous Acquisition of the Meaning of Sensory States Through Sensory-Invariance Driven Action",
                "abstract": "How can artificial or natural agents autonomously gain understanding of its own internal (sensory) state? This is an important question not just for physically embodied agents but also for software agents in the information technology environment. In this paper, we investigate this issue in the context of a simple biologically motivated sensorimotor agent. We observe and acknowledge, as many other researchers do, that action plays a key role in providing meaning to the sensory state. However, our approach differs from the others: We propose a new learning criterion, that of on-going maintenance of sensory invariance. We show that action sequence resulting from reinforcement learning of this criterion accurately portrays the property of the input that triggered a certain sensory state. This way, the meaning of a sensory state can be firmly grounded on the choreographed action which maintains invariance in the internal state.",
                "id": "00139"
            },
            {
                "title": "Extrapolative Delay Compensation Through Facilitating Synapses and Its Relation to the Flash-Lag Effect",
                "abstract": "Neural conduction delay is a serious issue for organisms that need to act in real time. Various forms of flash-lag effect (FLE) suggest that the nervous system may perform extrapolation to compensate for delay. For example, in motion FLE, the position of a moving object is perceived to be ahead of a brief flash when they are actually colocalized. However, the precise mechanism for extrapolation at a single-neuron level has not been fully investigated. Our hypothesis is that facilitating synapses, with their dynamic sensitivity to the rate of change in the input, can serve as a neural basis for extrapolation. To test this hypothesis, we constructed and tested models of facilitating dynamics. First, we derived a spiking neuron model of facilitating dynamics at a single-neuron level, and tested it in the luminance FLE domain. Second, the spiking neuron model was extended to include multiple neurons and spike-timing-dependent plasticity (STDP), and was tested with orientation FLE. The results showed a strong relationship between delay compensation, FLE, and facilitating synapses/STDP. The results are expected to shed new light on real time and predictive processing in the brain, at the single neuron level.",
                "id": "00140"
            },
            {
                "title": "Stereo pseudo 3D rendering for web-based display of scientific volumetric data",
                "abstract": "Advancement in high-throughput microscopy technology such as the Knife-Edge Scanning Microscopy (KESM) is enabling the production of massive amounts of high-resolution volumetric data of biological microstructures. To fully utilize these data, they should be efficiently distributed to the scientific research community (e.g., through the Internet) and should be easily annotated and analyzed. Given the volumetric nature of the data, visualizing them in 3D is important. Since we cannot assume that every end user has high-end hardware, an approach that has minimal hardware requirement will be necessary. There are several prominent applications that facilitate the viewing of large collections of images over the web. Google Maps and Google Maps-like interfaces such as Brainmaps.org allow users to pan and zoom 2D images efficiently. However, they do not yet support the rendering of volumetric data in their standard web interface. Thus, we propose a new method of rendering volumetric data over the web that directly uses the raw image stack, without any computation on the data at all. The human visual system has the capability of viewing stereo images in 2D and turn that into a 3D perception. To generate stereo images, we will need to create the effects of depth and binocular disparity using 2D images. By using simple HTML and JavaScript that are computationally cheap, we can accomplish both tasks dynamically in a standard web browser, by overlaying the images with intervening semi-opaque layers. We expect the approach presented in this paper to be applicable to a broader domain, including geology and meteorology.",
                "id": "00141"
            },
            {
                "title": "Structural systems identification of genetic regulatory networks.",
                "abstract": "Reverse engineering of genetic regulatory networks from experimental data is the first step toward the modeling of genetic networks. Linear state-space models, also known as linear dynamical models, have been applied to model genetic networks from gene expression time series data, but existing works have not taken into account available structural information. Without structural constraints, estimated models may contradict biological knowledge and estimation methods may over-fit.In this report, we extended expectation-maximization (EM) algorithms to incorporate prior network structure and to estimate genetic regulatory networks that can track and predict gene expression profiles. We applied our method to synthetic data and to SOS data and showed that our method significantly outperforms the regular EM without structural constraints.The Matlab code is available upon request and the SOS data can be downloaded from http://www.weizmann.ac.il/mcb/UriAlon/Papers/SOSData/, courtesy of Uri Alon. Zak's data is available from his website, http://www.che.udel.edu/systems/people/zak.",
                "id": "00142"
            },
            {
                "title": "Modeling cortical maps with Topographica",
                "abstract": "The biological function of cortical neurons can often be understood only in the context of large, highly interconnected networks. These networks typically form two-dimensional topographic maps, such as the retinotopic maps in the visual system. Computational simulations of these areas have led to valuable insights about how cortical topography develops and functions, but further progress is difficult because appropriate simulation tools are not available. This paper introduces the freely available Topographica map-level simulator, currently under development at the University of Texas at Austin. Topographica is designed to make large-scale, detailed models practical. The goal is to allow neuroscientists and computational scientists to understand how topographic maps and their connections organize and operate. This understanding will be crucial for integrating experimental observations into a comprehensive theory of cortical function. (C) 2004 Elsevier B.V. All rights reserved.",
                "id": "00143"
            },
            {
                "title": "Automated Cropping And Artifact Removal For Knife-Edge Scanning Microscopy",
                "abstract": "Knife Edge Scanning Microscopy (KESM) is a high-throughput imaging technique used to obtain large-scale anatomical information (approximate to 1cm(3)) at sub-micrometer resolution. Data acquisition has been fully automated, however significant post-processing and reconstruction must be done manually. KESM is unique in that illumination and tissue sectioning are performed using a diamond knife. Therefore many of the physical forces applied to the knife (e. g., vibration, slip, and light refraction) manifest as image artifacts and must be removed in post-processing. In this paper, we propose a fully automated framework to extract valid data from imaged sections and remove lighting artifacts, allowing reconstruction of the volumetric structures in multiple terabyte-scale data sets.",
                "id": "00144"
            },
            {
                "title": "Enhanced Facilitatory Neuronal Dynamics for Delay Compensation",
                "abstract": "Our earlier work has suggested that neuronal transmission delay may cause serious problems unless a compensation mechanism exists. In that work, facilitating neuronal dynamics was found to be effective in battling delay (the facilitating activation network model, or FAN). A systematic analysis showed that the previous FAN model has a subtle problem especially when high facilitation rates are used. We derived an improved facilitating dynamics at the neuronal level to overcome this limitation. In this paper, we tested our proposed approach in 2D pole balancing controllers, where it was shown to perform better than the previous FAN model. We also systematically tested the correlation between delay duration on the one hand and facilitation rate that effectively overcome the increasing delay on the other hand. Finally, we investigated the differential utilization of facilitating dynamics in sensory vs. motor neurons and found that motor neurons utilize the facilitating dynamics more than the sensory neurons. These findings are expected to help us better understand the role of facilitation in natural and artificial agents.",
                "id": "00145"
            },
            {
                "title": "Delay Compensation Through Facilitating Synapses and STDP: A Neural Basis for Orientation Flash-Lag Effect",
                "abstract": "In orientation flash-lag effect (FLE), a continuously rotating bar in the center is perceived to be misaligned toward the direction of rotation when compared to a briefly flashed pair of flanking bars that are actually aligned. The implication of this simple visual illusion is quite profound: The effect may be due to motion extrapolation, undoing the effects of neural conduction delay. Previously, we showed that facilitating synapses may be a neural basis of such a delay compensation mechanism in other forms of FLE such as luminance FLE. However, the approach based on a single neuron cannot be applied to orientation FLE since firing rate in a single neuron cannot represent the full range of orientations. Here, we extend our model to multiple neurons, and show that facilitating synapses, together with adaptation through Spike-Timing-Dependent Plasticity (STDP), can serve as a neural basis for delay compensation giving rise to orientation FLE.",
                "id": "00146"
            },
            {
                "title": "Salience in orientation-filter response measured as suspicious coincidence in natural images",
                "abstract": "Visual cortex neurons have receptive fields resembling oriented bandpass filters, and their response distributions on natural images are non-Gaussian. Inspired by this, we previously showed that comparing the response distribution to normal distribution with the same variance gives a good thresholding criterion for detecting salient levels of edginess in images. However, (1) the results were based on comparison with human data, thus, an objective, quantitative performance measure was not taken. Furthermore, (2) why a normal distribution would serve as a good baseline was not investigated in full. In this paper, we first conduct a quantitative analysis of the normal-distribution baseline, using artificial images that closely mimic the statistics of natural images. Since in these artificial images, we can control and obtain the exact saliency information, the performance of the thresholding algorithm can be measured objectively. We then interpret the issue of the normal distribution being an effective baseline for thresholding, under the general concept of suspicious coincidence proposed by Barlow. It turns out that salience defined our way can be understood as a deviation from the unsuspicious baseline. Our results show that the response distribution on white-noise images (where there is no structure, thus zero salience and nothing suspicious) has a near-Gaussian distribution. We then show that the response threshold directly calculated from the response distribution to white-noise images closely matches that of humans, providing further support for the analysis. In sum, our results and analysis show an intimate relationship among subjective perceptual measure of salience, objective measures of salience using normal distributions as a baseline, and the theory of suspicious coincidence.",
                "id": "00147"
            },
            {
                "title": "Emergence of Memory-like Behavior in Reactive Agents Using External Markers",
                "abstract": "Early primitive animals with simple feed-forward neuronal circuits were limited to reactive behavior. Through evolution, they were gradually equipped with memory and became able to utilize information from the past. Such memory is usually implemented with recurrent connections and certain behavioral changes are thought to precede the reconstitution of the neuronal circuit's topology. If so, what could have been the behavior to drive such a rewiring? Our hypothesis is that the secretion and detection of chemical markers in the environment could be a precursor of internal memory. We will show how memory-like behavior can be expressed in memoryless reactive agents by taking advantage of the external chemical markers. Our results show that given chemical marker use, reactive agents are able to develop intelligent strategies in solving a biologically plausible food foraging task requiring spatial memory. We also found interesting analogy between the evaporation of the chemical markers and the recency effect in memory and how it affects the foraging strategy. These results are expected to help us better understand the possible evolutionary route from reactive to cognitive agents.",
                "id": "00148"
            },
            {
                "title": "Segmentation of Textures Defined on Flat vs. Layered Surfaces using Neural Networks: Comparison of 2D vs. 3D Representations.",
                "abstract": "Texture boundary detection (or segmentation) is an important capability in human vision. Usually, texture segmentation is viewed as a 2D problem, as the definition of the problem itself assumes a 2D substrate. However, an interesting hypothesis emerges when we ask a question regarding the nature of textures: What are textures, and why did the ability to discriminate texture evolve or develop? A possible answer to this question is that textures naturally define physically distinct (i.e., occluded) surfaces. Hence, we can hypothesize that 2D texture segmentation may be an outgrowth of the ability to discriminate surfaces in 3D. In this paper, we conducted computational experiments with artificial neural networks to investigate the relative difficulty of learning to segment textures defined on flat 2D surfaces vs. those in 3D configurations where the boundaries are defined by occluding surfaces and their change over time due to the observer's motion. It turns out that learning is faster and more accurate in 3D, very much in line with our expectation. Furthermore, our results showed that the neural network's learned ability to segment texture in 3D transfers well into 2D texture segmentation, bolstering our initial hypothesis, and providing insights on the possible developmental origin of 2D texture segmentation function in human vision.",
                "id": "00149"
            }
        ]
    },
    {
        "id": "002",
        "input": "For an author who has written the paper with the title \"Temperature-aware floorplanning of microarchitecture blocks with IPC-power dependence modeling and transient analysis\", which reference is related? Just answer with [1] or [2] without explanation. [1]: \"A survey of power estimation techniques in VLSI circuits\" [2]: \"Terrestrial cosmic ray intensities\"",
        "profile": [
            {
                "title": "Stochastic Functions Using Sequential Logic",
                "abstract": "Stochastic computing is a novel approach to real arithmetic, offering better error tolerance and lower hardware costs over the conventional implementations. Stochastic modules are digital systems that process random bit streams representing real values in the unit interval. Stochastic modules based on finite state machines (FSMs) have been shown to realize complicated arithmetic functions much more efficiently than combinational stochastic modules. However, a general approach to synthesize FSMs for realizing arbitrary functions has been elusive. We describe a systematic procedure to design FSMs that implement arbitrary real-valued functions in the unit interval using the Taylor series approximation.",
                "id": "0020"
            },
            {
                "title": "Large Block CLOCK (LB-CLOCK): A write caching algorithm for solid state disks",
                "abstract": "Solid State Disks (SSDs) using NAND flash memory are increasingly being adopted in the high-end servers of data- centers to improve performance of the I/O-intensive applications. Compared to the traditional enterprise class hard disks, SSDs provide faster read performance, lower cooling cost, and higher power efficiency. However, write performance of a flash based SSD can be up to an order of magnitude slower than its read performance. Furthermore, frequent write operations degrade the lifetime of flash memory. A nonvolatile cache can greatly help to solve these problems. Although a RAM cache is relative high in cost, it has successfully eliminated the performance gap between fast CPU and slow magnetic disk. Similarly, a nonvolatile cache in an SSD can alleviate the disparity between the flash memory's read and write performance. A small write cache that reduces the number of flash block erase operations, can lead to substantial performance gain for write-intensive applications and can extend the overall lifetime of flash based SSDs. This paper presents a novel write caching algorithm, the Large Block CLOCK (LB-CLOCK) algorithm, which considers 'recency' and 'block space utilization' metrics to make cache management decisions. LB-CLOCK dynamically varies the priority between these two metrics to adapt to changes in workload characteristics. Our simulation based experimental results show that LB-CLOCK outperforms the best known existing flash caching algorithms for a wide range of workloads.",
                "id": "0021"
            },
            {
                "title": "High-Level Information - An Approach for Integrating Front-End and Back-End Compilers",
                "abstract": "We propose a new universal High-Level Information (HLI) format to effectively integrate front-end and back-end compilers by passing front-end information to the back-end compiler. Importing this information into an existing back- end leverages the state-of-the-art analysis and transforma- tion capabilities of existing front-end compilers to allow the back-end greater optimization potential than it has when re- lying on only locally-extracted information. A version of the HLI has been implemented in the SUIF parallelizing com- piler and the GCC back-end compiler. Experimental results with the SPEC benchmarks show that HLI can provide GCC with substantially more accurate data dependence informa- tion than it can obtain on its own. Our results show that the number of dependence edges in GCC can be reduced by an average of 48% for the integer benchmark programs and an average of 54% for the floating-point benchmark pro- grams studied, which provides greater flexibility to GCC's code scheduling pass. Even with the scheduling optimiza- tion limited to basic blocks, the use of HLI produces mod- erate speedups compared to using only GCC's dependence tests when the optimized programs are executed on MIPS R4600 and R10000 processors.",
                "id": "0022"
            },
            {
                "title": "Integrating Parallelizing Compilation Technology and Processor Architecture for Cost-Effective Concurrent multithreading",
                "abstract": "As the number of transistors on a single chip continues to grow, it is important to think beyond the traditional approaches of compiler optimization for deeper pipelines and wider instruction issue units to improve performance. This singlethreaded execution model limits these approaches to exploiting only the relatively small amount of instruction-level parallelism available in application programs. While integrating an entire multiprocessor onto a single chip is feasible, this architecture is limited to exploiting only relatively coarse-grained parallelism. We propose a concurrent multithreaded architecture, called the superthreaded architecture, as an alternative. As a hybrid of a wide-issue superscalar processor and a multiprocessor-on-a-chip, this new concurrent multithreading architecture can leverage the best of existing and future parallel hardware and compilation technologies. By combining compiler-directed thread-level speculation for control and data dependences with run-time checking of data dependences, the superthreaded architecture can exploit the multiple granularities of parallelism available in general-purpose application programs to reduce the execution time of a single program.",
                "id": "0023"
            },
            {
                "title": "Improving Energy and Performance with Spintronics Caches in Multicore Systems.",
                "abstract": "Spintronic memory (STT-MRAM) is an attractive alternative technology to CMOS since it offers higher density and virtually no leakage current. Spintronic memory continues to require higher write energy, however, presenting a challenge to memory hierarchy design when energy consumption is a concern. Various techniques for reducing write energy have been studied in the past for a single processor, typically focusing on the last-level caches while keeping the first level caches in CMOS to avoid the write latency. In this work, use of STT-MRAM for the first level caches of a multicore processor is motivated by showing that the impact on throughput due to increased write latency is offset in many cases by increased cache size due to higher density. The Parsec benchmark suite is run on a modern multicore platform simulator, comparing performance and energy consumption of the spintronic cache system to a CMOS design. A small, fully-associative level-0 cache is then introduced (on the order of 8-64 cache lines), and shown to effectively hide the STT-MRAM write latency. Performance degradation due to write latency is restored or slightly improved, while cache energy consumption is reduced by 30-50% for 12 of the 13 benchmarks.",
                "id": "0024"
            },
            {
                "title": "Increasing Instruction-Level Parallelism with Instruction Precomputation (Research Note)",
                "abstract": "Value reuse improves a processor's performance by dynamically caching the results of previous instructions and reusing those results to bypass the execution of future instructions that have the same opcode and input operands. However, continually replacing the least recently used entries could eventually fill the value reuse table with instructions that are not frequently executed. Furthermore, the complex hardware that replaces entries and updates the table may necessitate an increase in the clock period. We propose instruction precomputation to address these issues by profiling programs to determine the opcodes and input operands that have the highest frequencies of execution. These instructions then are loaded into the precomputation table before the program executes. During program execution, the precomputation table is used in the same way as the value reuse table is, with the exception that the precomputation table does not dynamically replace any entries. For a 2K-entry pre-computation table implemented on a 4-way issue machine, this approach produced an average speedup of 11.0%. By comparison, a 2K-entry value reuse table produced an average speedup of 6.7%. Instruction precomputation outperforms value reuse, especially for smaller tables, with the same number of table entries while using less area and having a lower access time.",
                "id": "0025"
            },
            {
                "title": "Cross-layer speculative architecture for end systems and gateways in computer networks with lossy links",
                "abstract": "The throughput degradation of Transport Control Protocol (TCP)/Internet Protocol (IP) networks over lossy links due to the coexistence of congestion losses and link corruption losses is very similar to the degradation of processor performance (i.e., cycle per instruction) due to control hazards in computer design. First, two types of loss events in networks with lossy links are analogous to two possibilities of a branching result in computers (taken vs. not taken). Secondly, both problems result in performance degradations in their applications, i.e., penalties (in clock cycles) in a processor, and throughput degradation (in bits per second) in a TCP/IP network. This has motivated us to apply speculative techniques (i.e., speculating on the outcome of branch predictions), used to overcome control dependencies in a processor, for throughput improvements when lossy links are involved in TCP/IP connections. The objective of this paper is to propose a cross-layer network architecture to improve the network throughput over lossy links. The system consists of protocol-level speculation based algorithms at transport layer, and protocol enhancements at middleware and network layers that provide control and performance parameters to transport layer functions. Simulation results show that, compared with prior research, our proposed system is effective in improving network throughput over lossy links, capable of handling incorrect speculations, fair for other competing flows, backward compatible with legacy networks, and relatively easy to implement.",
                "id": "0026"
            },
            {
                "title": "Self-tuning Speculation for Maintaining the Consistency of Client-Cached Data",
                "abstract": "This paper presents a new protocol, Self-tuning ActiveData-aware Cache Consistency (SADCC), which employsparallel communication and self-tuning speculation toimprove the performance of data-shipping database systems.Using parallel communication with simultaneous client-serverand client-client communication, SADCC reduces thenetwork latency for detecting data conflicts by 50%, whileincreasing message volume overhead by only about 4.8%. Bybeing aware of the global states of cached data, clients self-tunebetween optimistic and pessimistic consistency control.The abort rate is reduced by statistically quantifying thespeculation cost. We compare SADCC with the leadingcache consistency algorithms, Active Data-aware CacheConsistency (ADCC) and Asynchronous Avoidance-basedCache Consistency (AACC), in a page server DBMSarchitecture with page-level consistency. The experimentsshow that, in a non-contention environment, both SADCCand ADCC display a slight reduction (an average of 2.3%)in performance compared to AACC with a high-speednetwork environment. With high contention, however,SADCC has an average of 14% higher throughput thanAACC and 6% higher throughput than ADCC.",
                "id": "0027"
            },
            {
                "title": "Polysynchronous stochastic circuits",
                "abstract": "Clock distribution networks (CDNs) are costly in high-performance ASICs. This paper proposes a new approach: splitting clock domains at a very fine level, down to the level of a handful of gates. Each domain is synchronized with an inexpensive clock signal, generated locally. This is possible by adopting the paradigm of stochastic computation, where signal values are encoded as random bit streams. The design method is illustrated with the synthesis of circuits for applications in signal and image processing.",
                "id": "0028"
            },
            {
                "title": "The Applicability of Adaptive Control Theory to QoS Design: Limitations and Solutions",
                "abstract": "Due to the increasing complexity, the behavior of large-scale distributed systems becomes difficult to predict. The ability of on-line identification and autotuning of adaptive control systems has made the adaptive control theoretical design an attractive approach for quality of service (QoS) guarantee. However, there is an inherent constraint in adaptive control systems, i.e. a conflict between asymptotically good control and asymptotically good parameter estimates. This paper addresses these limitations via sensitivity analysis. The simulation study demonstrates that the adaptive control theoretical design depends on the excitation signal, environment uncertainty, and a priori knowledge on the system. In addition, this paper proposes an adaptive dual control framework for mitigating these constraints in QoS design. By incorporating the existing uncertainty of the on-line prediction into the control strategy, the dual adaptive control framework optimizes the tradeoff between the control goal and the uncertainty.",
                "id": "0029"
            },
            {
                "title": "Exploiting the Prefetching Effect Provided by Executing Mispredicted Load Instructions",
                "abstract": "As the degree of instruction-level parallelism in superscalar architectures increases, the gap between processor and memory performance continues to grow requiring more aggressive techniques to increase the performance of the memory system. We propose a new technique, which is based on the wrong-path execution of loads far beyond instruction fetch-limiting conditional branches, to exploit more instruction-level parallelism by reducing the impact of memory delays. We examine the effects of the execution of loads down the wrong branch path on the performance of an aggressive issue processor. We find that, by continuing to execute the loads issued in the mispredicted path, even after the branch is resolved, we can actually reduce the cache misses observed on the correctly executed path. This wrong-path execution of loads can result in a speedup of up to 5% due to an indirect prefetching effect that brings data or instruction blocks into the cache for instructions subsequently issued on the correctly predicted path. However, it also can increase the amount of memory traffic and can pollute the cache. We propose the Wrong Path Cache (WPC) to eliminate the cache pollution caused by the execution of loads down mispredicted branch paths. For the configurations tested, fetching the results of wrong path loads into a fully associative 8-entry WPC can result in a 12% to 39% reduction in L1 data cache misses and in a speedup of up to 37%, with an average speedup of 9%, over the baseline processor.",
                "id": "00210"
            },
            {
                "title": "Dynamic scheduling strategies for shared-memory multiprocessors",
                "abstract": "Efficiently scheduling parallel tasks on to the processors of a shared-memory multiprocessor is critical to achieving high performance. Given perfect information at compile-time, a static scheduling strategy can produce an assignment of tasks to processors that ideally balances the load among the processors while minimizing the run-time scheduling overhead and the average memory referencing delay. Since perfect information is seldom available, however, dynamic scheduling strategies distribute the task assignment function to the processors by having idle processors allocate work to themselves from a shared queue. While this approach can improve the load balancing compared to static scheduling, the time required to access the shared work queue adds directly to the overall execution time. To overlap the time required to dynamically schedule tasks with the execution of the tasks, we examine a class of self-adjusting dynamic scheduling (SADS) algorithms that centralizes the assignment of tasks to processors. These algorithms dedicate a single processor of the multiprocessor to perform a novel on-line branch-and-bound technique that dynamically computes partial schedules based on the loads of the other processors and the memory locality (affinity) of the tasks and the processors. Our simulation results show that this centralized scheduling outperforms self-scheduling algorithms even when using only a small number of processors.",
                "id": "00211"
            },
            {
                "title": "SCRAP: A Statistical Approach for Creating a Database Query Workload Based on Performance Bottlenecks",
                "abstract": "With the tremendous growth in stored data, the role of database systems has become more significant than ever before. Standard query workloads, such as the TPC-C and TPC-H benchmark suites, are used to evaluate and tune the functionality and performance of database systems. Running and configuring benchmarks is a time consuming task. It requires substantial statistical expertise due to the enormous data size and large number of queries in the workload. Subsetting can be used to reduce the number of queries in a workload. An existing workload subsetting technique selected queries based on similarities of the ranks of the queries for low-level characteristics, such as cache miss rates, or based on the execution time required in different computer systems. However, many low-level characteristics are correlated, produce similar behaviors. Also, raw execution time as a metric is too diffuse to capture important performance bottlenecks. Our goal is to select a subset of queries that can reproduce the same bottlenecks in the system as the original workload. In this paper, we propose a statistical approach for creating a database query workload based on performance bottlenecks (SCRAP). Our methodology takes a query workload and a set of system configuration parameters as inputs, and selects a subset of the queries from the workload based on the similarity of performance bottlenecks. Experimental results using the TPC-H benchmark and the PostgreSQL database system, show that the reduced workload and the original workload produce similar performance bottlenecks, and the subset accurately estimates the total execution time.",
                "id": "00212"
            },
            {
                "title": "Trends in Shared Memory Multiprocessing",
                "abstract": "Shared memory multiprocessing is recognized by industry as a key technology for domains such as decision support systems and multimedia processing. Like uniprocessors, shared memory multiprocessors are often built from high-performance microprocessors, so there is a clear transition path from uniprocessor to multiprocessor program implementations. The challenge lies in making this transition as smooth as possible, both in performance and the programming required to achieve it. The first step is to examine the current use of shared memory multiprocessing and arrive at intelligent projections of future use. The second step is to begin filling gaps in programming models and architectures for shared memory multiprocessing. The third step-possibly taken concurrently with the second-is to make the development of parallel software more feasible. Perhaps the greatest challenge is to develop new techniques in the face of a moving hardware target. The community must somehow improve the software and keep pace with constant increases in integration level, on-chip parallelism, and memory hierarchy complexity.",
                "id": "00213"
            },
            {
                "title": "Utilizing Heterogeneous Networks in Distributed Parallel Computing Systems",
                "abstract": "Heterogeneity is becoming quite common in distributed parallel computing systems, both in processor architectures and in communication networks. Different types of networks have different performance characteristics, while different types of messages may have different communication requirements. In this work, we analyze two techniques for exploiting these heterogeneous characteristics and requirements to reduce the communication overhead of parallel application programs executed on distributed computing systems. The performance based path selection (PBPS) technique selects the best (lowest latency) network among several for each individual message, while the second technique aggregates multiple networks into a single virtual network. We present a general approach for applying and evaluating these techniques to a distributed computing system with multiple interprocessor communication networks. We also generate performance curves for a cluster of IBM workstations interconnected with Ethernet, ATM, and Fibre Channel networks. As we show with several of the NAS benchmarks, these curves can be used to estimate the potential improvement in communication performance that can be obtained with these techniques, given some simple communication characteristics of an application program.",
                "id": "00214"
            },
            {
                "title": "Characterization of Communication Patterns in Message-Passing Parallel Scientific Application Programs",
                "abstract": "This paper examines the communication patterns of parallel scientific programs, including some of the NAS benchmarks and the\n Miami Isopycnic Coordinate Ocean Model (MICOM), that use explicit message-passing. Communication locality, including communication event locality, message destination locality, and message size locality, is proposed and studied\n in addition to the widely accepted metrics of message size, destination, and generation distributions. We find that the locality\n metrics are relatively insensitive to system and problem size variations making them robust metrics for characterizing the\n communication patterns of parallel applications. We observe that the communication patterns of the benchmark programs are\n consistent with those of the actual application. The results of this study will be useful for understanding parallel applications'\n communication behavior and for designing more realistic synthetic benchmarks.\n ",
                "id": "00215"
            },
            {
                "title": "Dynamic code region (DCR) based program phase tracking and prediction for dynamic optimizations",
                "abstract": "Detecting and predicting a program's execution phases are crucial to dynamic optimizations and dynamically adaptable systems. This paper shows that a phase can be associated with dynamic code regions embedded in loops and procedures which are primary targets of compiler optimizations. This paper proposes a new phase tracking hardware, especially for dynamic optimizations, that effectively identifies and accurately predicts program phases by exploiting program control flow information. Our proposed phase tracking hardware uses a simple stack and a phase signature table to track the change of phase signature between dynamic code regions. Several design parameters of our proposed phase tracking hardware are evaluated on 10 SPEC CPU2000 benchmarks. Our proposed phase tracking hardware effectively identifies a phase at a given granularity. It correctly predicts the next program phase for 84.9% of times with a comparable small performance variance within the same phase. A longer phase length and higher phase prediction accuracy together with a reasonably small performance variance are essential to build more efficient dynamic profiling and optimization systems.",
                "id": "00216"
            },
            {
                "title": "Loop-Level Process Control: An Effective Processor Allocation Policy for Multiprogrammed Shared-Memory Multiprocessors",
                "abstract": "Processor time-sharing is the most common way to increase the overall system utilization for shared-memory multiprocessor systems. However, the performance of individual applications might be sacrificed due to the high overhead of context switching, due to the processing power wasted by busy-waiting synchronization and locking operations, and due to poor cache memory utilization. In this paper, we propose a simple and effective processor allocation scheme, called Loop-Level Process Control (LLPC), for multiprogrammed multiprocessors. At the beginning of each parallel section of each application program, LLPC uses the current system load to determine an upper limit on the number of processes the application can create for that parallel section. Preliminary simulation results using the Perfect Club Fortran benchmarks show that this loop-level process control scheme can produce a high system utilization while maintaining high performance for the individual applications. Another advantage of this strategy is that it is transparent to the programmer and does not require any modifications to the operating system. Consequently, the application can remain portable and compatible.",
                "id": "00217"
            },
            {
                "title": "Shared-memory multiprocessing: Current state and future directions",
                "abstract": "Progress in shared-memory multiprocessing research over the last several decades has led to its industrial recognition as a key technology for a variety of performance-demanding application domains. In this chapter, we summarize the current state of this technology including system architectures, programming interfaces, and compiler and tool technology offered to the application writer. We the identify important issues for future research in relation to technology and application trends. We particularly focus on research directions in machine architectures, programming interfaces, and parallelization methodologies.",
                "id": "00218"
            },
            {
                "title": "Using Stochastic Computing to Reduce the Hardware Requirements for a Restricted Boltzmann Machine Classifier.",
                "abstract": "Artificial neural networks are powerful computational systems with interconnected neurons. Generally, these networks have a very large number of computation nodes which forces the designer to use software-based implementations. However, the software based implementations are offline and not suitable for portable or real-time applications. Experiments show that compared with the software based implementations, FPGA-based systems can greatly speed up the computation time, making them suitable for real-time situations and portable applications. However, the FPGA implementation of neural networks with a large number of nodes is still a challenging task.\n\nIn this paper, we exploit stochastic bit streams in the Restricted Boltzmann Machine (RBM) to implement the classification of the RBM handwritten digit recognition application completely on an FPGA. We use finite state machine-based (FSM) stochastic circuits to implement the required sigmoid function and use the novel stochastic computing approach to perform all large matrix multiplications. Experimental results show that the proposed stochastic architecture has much more potential for tolerating faults while requiring much less hardware compared to the currently un-implementable deterministic binary approach when the RBM consists of a large number of neurons. Exploiting the features of stochastic circuits, our implementation achieves much better performance than a software-based approach.\n\n",
                "id": "00219"
            },
            {
                "title": "Towards Theoretical Cost Limit of Stochastic Number Generators for Stochastic Computing",
                "abstract": "Stochastic number generator (SNG) is one important component of stochastic computing (SC). An SNG usually consists of a random number source (RNS) and a probability conversion circuit (PCC). The SNGs occupy a large portion of the total area and power of a stochastic circuit. Thus, it is critical to lower the area and power of the SNGs. The existing methods only focused on simplifying the RNSs inside the SNGs, such as sharing the RNSs and using emerging devices. However, how to reduce the area and power of PCCs is never studied. In this work, we explore this problem and propose a solution that can effectively reduce the area and power of PCCs. We also study the theoretical limit on the cost of SNG and find that our proposed design approaches the limit. The experimental results show that our design can gain up to 2\u00d7 improvement in power-delay product over the traditional SNGs.",
                "id": "00220"
            },
            {
                "title": "Implementing A Dynamic Processor Allocation Policy For Multiprogrammed Parallel Applications In The Solaris (Tm) Operating System",
                "abstract": "Parallel applications typically do not perform well in a multiprogrammed environment that uses time-sharing to allocate processor resources to the applications' parallel threads. Co-scheduling related parallel threads, or statically partitioning the system, often can reduce the applications' execution times, but at the expense of reducing the overall system utilization. To address this problem, there has been increasing interest in dynamically allocating processors to applications based on their resource demands and the dynamically varying system load. The Loop-Level Process Control (LLPC) policy (Yue K, Lilja D. Efficient execution of parallel applications in multiprogrammed multiprocessor systems. 10th International Parallel Processing Symposium, 1996; 448-456) dynamically adjusts the number of threads an application is allowed to execute based on the application's available parallelism and the overall system load. This study demonstrates the feasibility of incorporating the LLPC strategy into an existing commercial operating system and parallelizing compiler and provides further evidence of the performance improvement that is possible using this dynamic allocation strategy. In this implementation, applications are automatically parallelized and enhanced with the appropriate LLPC hooks so that each application interacts with the modified version of the Solaris operating system. The parallelism of the applications are then dynamically adjusted automatically when they are executed in a multiprogrammed environment so that all applications obtain a fair share of the total processing resources. Copyright (C) 2001 John Wiley & Sons, Ltd.",
                "id": "00221"
            },
            {
                "title": "Efficient and Fast Approximate Consensus with Epidemic Failure Detection at Extreme Scale",
                "abstract": "This paper proposes a memory efficient failure detection and consensus algorithm, for fail-stop type process failures, based on epidemic protocols. It is suitable for extreme scale systems with reliable networks (no message loss) and high failure frequency. Communication time dominates the execution time at scale. The redundant failure detections and non-uniform information dissemination speed of epidemic algorithms make approximate epidemic-based consensus detection a useful way to trade communication overhead for accuracy. An approximate technique to the consensus detection is also proposed in this paper for faster consensus detection. Results show that the algorithm detects consensus correctly on failed processes with logarithmic scalability. The algorithm is tolerant to process failures both before and during the execution and the number of failures (occurring both before and during execution) have virtually no effect on the consensus detection time at scale. Comparison with similar deterministic consensus detection technique shows that the algorithm detects consensus at the same time with high probability. Further, benefits of the proposed approximate technique increase as system size increases. Compared to the non-approximate version, for a system size of 218 processes, the communication saved is 34% with accuracy loss of the order of 10^-4 in consensus detection.",
                "id": "00222"
            },
            {
                "title": "Temperature-aware floorplanning of microarchitecture blocks with IPC-power dependence modeling and transient analysis",
                "abstract": "Operating temperatures have become an important concern in high performance microprocessors. Floorplanning or block-level placement offers excellent potential for thermal optimization through better heat spreading between the blocks, but these optimizations can also impact the throughput of a microarchitecture, measured in terms of the number of instructions per cycle (IPC). In nanometer technologies, global buses can have multicycle delays that depend on the positions of the blocks, and it is important for a floorplanner to be microarchitecturally-aware to be sure that thermal and IPC considerations are appropriately balanced. This paper proposes a methodology for thermally-aware microarchitecture floorplanning. The approach models the interactions between the IPC and the temperature distribution, and incorporates both factors in the floorplanning cost function. Our approach uses transient modeling and optimizes both the peak and the average temperatures, and employs a design of experiments (DOE) based strategy, which effectively captures the huge exponential search space with a small number of cycle-accurate simulations. A comparison with a technique based on previous work indicates that the proposed approach results in good reductions both in the average and the peak temperatures for a range of SPEC benchmarks.",
                "id": "00223"
            },
            {
                "title": "Exploiting the parallelism available in loops",
                "abstract": "Because a loop's body often executes many times, loops provide a rich opportunity for exploiting parallelism. This article explains scheduling techniques and compares results on different architectures. Since parallel architectures differ in synchronization overhead, instruction scheduling constraints, memory latencies, and implementation details, determining the best approach for exploiting parallelism can be difficult. To indicate their performance potential, this article surveys several architectures and compilation techniques using a common notation and consistent terminology. First we develop the critical dependence ratio to determine a loop's maximum possible parallelism, given infinite hardware. Then we look at specific architectures and techniques. Loops can provide a large portion of the parallelism available in an application program, since the iterations of a loop may be executed many times. To exploit this parallelism, however, one must look beyond a single basic block or a single iteration for independent operations. The choice of technique depends on the underlying architecture of the parallel machine and the characteristics of each individual loop.<>",
                "id": "00224"
            },
            {
                "title": "Challenges in Computer Architecture Evaluation",
                "abstract": "Reasoning about today's tremendously complex computer systems is difficult and developing them is expensive. Detailed software simulations are thus essential for evaluating computer architecture ideas. Industry uses simulation extensively during processor and system design as the easiest and least expensive way to explore design options.Unfortunately, constructing accurate models of modern computer systems is becoming harder and more time-consuming, while the effort required to develop high-fidelity simulation tools typically yields few academic rewards. Without funding and promising prospects for academic recognition, research and development in these areas will likely languish.",
                "id": "00225"
            },
            {
                "title": "Exploring subsets of standard cell libraries to exploit natural fault masking capabilities for reliable logic",
                "abstract": "Deep submicron technology is expected to be plagued by many reliability issues including soft errors in logic. To address this, we demonstrate how exploiting the natural fault masking characteristics of logical functions can be achieved by exploring the design space for selecting subsets of cells from within a cell library prior to synthesis. Subset selection alone is shown to improve the reliability of combinational logic circuits by more than 35%. We compare how subset libraries effect the trade-offs between reliability, area, power, and performance. Further, we show that added benefits of reduced cell library size can benefit the design.",
                "id": "00226"
            },
            {
                "title": "Statistically translating low-level error probabilities to increase the accuracy and efficiency of reliability simulations in hardware description languages",
                "abstract": "Radiation induced single-event upsets are becoming an increasing issue for designers due to the impact on overall design reliability. This paper presents a method for translating probabilistic information from lower levels in the design hierarchy into efficient, fast, and useful tools at higher levels. This method allows designers to incorporate soft error reliability analysis into the verification process at greatly reduced simulation expense with high accuracy. We also include metrics to estimate the error in the method. Results from both abstracted verification simulations and Verilog soft error simulations are presented. Our method bridges a gap between low level reliability measurements and high level reliability simulations.",
                "id": "00227"
            },
            {
                "title": "An adaptive dual control framework for QoS design",
                "abstract": "The widespread deployment of the advanced computer technology in business and industries has demanded the high standard on quality of service (QoS). For example, many Internet applications, i.e. online trading, e-commerce, and real-time databases, etc., execute in an unpredictable general-purpose environment but require performance guarantees. Failure to meet performance specifications may result in losing business or liability violations. As systems become distributed and complex, it has become a challenge for QoS design. The ability of on-line identification and auto-tuning of adaptive control systems has made the adaptive control theoretical design an attractive approach for QoS design. However, there is an inherent constraint in adaptive control systems, i.e. a conflict between asymptotically good control and asymptotically good on-line identification. This paper first identifies and analyzes the limitations of adaptive control for network QoS by extensive simulation studies. Secondly, as an approach to mitigate the limitations, we propose an adaptive dual control framework. By incorporating the existing uncertainty of on-line prediction into the control strategy and accelerating the parameter estimation process, the adaptive dual control framework optimizes the tradeoff between the control goal and the uncertainty, and demonstrates robust and cautious behavior. The experimental study shows that the adaptive dual control framework mitigate the limitations of the conventional adaptive control framework. Compared with the conventional adaptive control framework under the medium uncertainty, the adaptive dual control framework reduces the deviation from the desired hit-rate ratio from 40% to 13%.",
                "id": "00228"
            },
            {
                "title": "A multiprocessor architecture combining fine-grained and coarse-grained parallelism strategies",
                "abstract": "A wide variety of computer architectures have been proposed that attempt to exploit parallelism at different granularities. For example, pipelined processors and multiple instruction issue processors exploit the fine-grained parallelism available at the machine instruction level, while shared memory multiprocessors exploit the coarse-grained parallelism available at the loop level. Using a register-transfer level simulation methodology, this paper examines the performance of a multiprocessor architecture that combines both coarse-grained and fine-grained parallelism strategies to minimize the execution time of a single application program. These simulations indicate that the best system performance is obtained by using a mix of fine-grained and coarse-grained parallelism in which any number of processors can be used, but each processor should be pipelined to a degree of 2 to 4, or each should be capable of issuing from 2 to 4 instructions per cycle. These results suggest that current high-performance microprocessors, which typically can have 2 to 4 instructions simultaneously executing, may provide excellent components with which to construct a multiprocessor system.",
                "id": "00229"
            },
            {
                "title": "So Many States, So Little Time: Verifying Memory Coherence in the Cray X1",
                "abstract": "This paper investigates a complexity-effective technique for verifying a highly distributed directory-based cache coherence protocol. We develop a novel approach called \"witness strings\" that combines both formal and informal verification methods to expose design errors within the cache coherence protocol and its Verilog implementation. In this approach a formal execution trace is extracted during model checking of the architectural model and re-encoded to provide the input stimulus for a logic simulation of the corresponding Verilog implementation. This approach brings confidence to system architects that the logic implementation of the coherence protocol conforms to the architectural model. The feasibility of this approach is demonstrated by using it to verify the cache coherence protocol of the Cray X1. Using this approach we uncovered three architectural protocol errors and exposed several implementation errors by replaying the witness strings on the Verilog implementation.",
                "id": "00230"
            },
            {
                "title": "Speed versus Accuracy Trade-Offs in Microarchitectural Simulations",
                "abstract": "Due to the long simulation time of the reference input set, computer architects often use reduced time simulation techniques to shorten the simulation time. However, what has not yet been thoroughly evaluated is the accuracy of these techniques relative to the reference input set and with respect to each other. To rectify this deficiency, this paper uses three methods to characterize reduced input set, truncated execution, and sampling-based simulation techniques while also examining their speed vs. accuracy trade-off and configuration dependence. Our results show that the three sampling-based techniques, SimPoint, SMARTS, and random sampling, have the best accuracy, the best speed vs. accuracy trade-off, and the least configuration dependence. On the other hand, the reduced input set and truncated execution simulation techniques had generally poor accuracy, were not significantly faster than the sampling-based techniques, and were severely configuration dependent. The final contribution of this paper is a decision tree which can help architects choose the most appropriate technique for their simulations.",
                "id": "00231"
            },
            {
                "title": "Performing bitwise logic operations in cache using spintronics-based magnetic tunnel junctions",
                "abstract": "Recent exciting developments in the emerging field of spintronics have enabled rapid advances in spintronic devices such as magnetic tunnel junctions (MTJs). While MTJs are being primarily used as the basic devices in non-volatile memory, they have also been shown to accomplish primitive logic functions. However, the spintronic nature of the logic operation makes it challenging to accomplish tasks such as cascading of logic gates, operations on multiple outputs, and various combinations of these. In order to enable these primary functions, we propose the idea of adding interconnections between MTJ devices in a spintronic memory array. With this added connectivity, we show that the memory array gains the ability to perform bitwise logic operations on the data stored within it without intermediate computing circuits. We demonstrate that the basic logic operations of NAND, AND, OR and XOR operations can be performed in the memory array using the memory devices themselves. We describe the algorithms for performing the logic operations in memory and introduce the notion of a 'footprint' to compare the complexities of the operations in terms of their memory usage, data requirements and time steps. Taking into account the minimum interconnection requirements of these logic functions, we propose the design of a general spintronic logic-in-cache block and demonstrate the ADD function with it.",
                "id": "00232"
            },
            {
                "title": "Comparing processor allocation strategies in multiprogrammed shared-memory multiprocessors",
                "abstract": "Small-scale shared-memory multiprocessors are commonly used in a workgroup environment where multiple applications, both parallel and sequential, are executed concurrently while sharing the processors and other system resources. To utilize the processors efficiently, an effective allocation strategy is required. In this paper, we use performance data obtained from an SGI multiprocessor to evaluate several processor allocation strategies when running two parallel programs simultaneously. We examine gang scheduling (coscheduling), static space-sharing (space partitioning), and a dynamic allocation scheme called loop-level process control (LLPC) with three different dynamic allocation heuristics. We use regression analysis to quantify the measured data and thereby explore the relationship between the degree of parallelism of the application, specific system parameters (such as the size of the system), the processor allocation strategy, and the resulting performance. This study shows that dynamically partitioning the system using LLPC or similar heuristics provides better performance for applications with a high degree of parallelism than either gang scheduling or static space-sharing.",
                "id": "00233"
            },
            {
                "title": "Performance-based path determination for interprocessor communication in distributed computing systems",
                "abstract": "The different types of messages used by a parallel application program executing in a distributed computing system can each have unique characteristics so that no single communication network can produce the lowest latency for all messages. For instance, short control messages may be sent with the lowest overhead on one type of network, such as Ethernet, while bulk data transfers may be better suited to a different type of network, such as Fibre Channel or HIPPI. This work investigates how to exploit multiple heterogeneous communication networks that interconnect the same set of processing nodes using a set of techniques we call performance-based path determination (PBPD). The performance-based path selection (PBPS) technique selects the best (lowest latency) network among several for each individual message to reduce the communication overhead of parallel programs. The performance-based path aggregation (PBPA) technique, on the other hand, aggregates multiple networks into a single virtual network to increase the available bandwidth. We test the PBPD techniques on a cluster of SGI multiprocessors interconnected with Ethernet, Fibre Channel, and HiPPI networks using a custom communication library built on top of the TCP/IP protocol layers. We find that PBPS can reduce communication overhead in applications compared to using either network alone, while aggregating networks into a single virtual network can reduce communication latency for bandwidth-limited applications. The performance of the PBPD techniques depends on the mix of message sizes in the application program and the relative overheads of the networks, as demonstrated in our analytical models",
                "id": "00234"
            },
            {
                "title": "Partitioning Tasks Between A Pair Of Interconnected Heterogeneous Processors - A Case-Study",
                "abstract": "With the variety of computer architectures available today, it is often difficult to determine which particular type of architecture will provide the best performance on a given application program, In fact, one type of architecture may be well suited to executing one section of a program while another architecture may be better suited to executing another section of the same program, One potentially promising approach for exploiting the best features of different computer architectures is to partition an application program to simultaneously execute on two or more types of machines interconnected with a high-speed communication network, A fundamental difficulty with this heterogeneous computing, however, is the problem of determining how to partition the application program across the interconnected machines, The goal of this paper is to show how a programmer or a compiler can use a model of a heterogeneous system to determine the machine on which each subtask should be executed. This technique is illustrated with a simple model that relates the relative performance of two heterogeneous machines to the communication time required to transfer partial results across their interconnection network, Experiments with a Connection Machine CM-200 demonstrate how to apply this model to partition two different application programs across the sequential front-end processor and the parallel back-end array.",
                "id": "00235"
            },
            {
                "title": "MEMESTAR: A Simulation Framework for Reliability Evaluation over Multiple Environments",
                "abstract": "The paper presents a methodology for the simulation of soft errors targeting future nano-technological devices. This approach efficiently scales the failure rate of individual devices according to cell area and considers the effect of multiple faults within a circuit. Furthermore this methodology measures circuit operation over a range of environments and consequently provides a means of targeting designs to the expected operating environment rather than worst case. The authors demonstrate the effect area has on circuit reliability and fault tolerance",
                "id": "00236"
            },
            {
                "title": "Education at a distance: a report from the front",
                "abstract": " A course on computer systems performanceanalysis has been adapted for several different distance educationdelivery options, including an interactive televisionsystem, face-to-face presentation at a satellite campus, anddelivery over the Internet to independent study students.Of the 114 students who have enrolled in this graduate-levelcourse in the past three years, half have been nontraditionalstudents who never set foot on campus. While the studentswho enroll through the interactive... ",
                "id": "00237"
            },
            {
                "title": "A balanced approach to high-level verification: performance trade-offs in verifying large-scale multiprocessors",
                "abstract": "A single node of a modern scalable multiprocessor consists of several ASICs comprising tens of millions of gates. This level of integration and complexity imposes an enormous onus on the verification process. A variety of tools, ranging from discrete-event logic simulation to formal model checking, can be used to attack this problem. Unfortunately, conventional simulation techniques, with their primitive interface to the hardware (i.e. test vectors), are inadequate tools for reasoning about the correctness of complex architectural features, such as cache coherence protocols and memory consistency models. Similarly, model checkers offer very limited utility on such large designs. We have previously proposed [1] a novel verification framework, called Raven that addresses many of these challenges. In this paper, we examine the performance implications of verifying systems at higher levels of abstraction. A detailed performance analysis is conducted to compare this higher-level approach against an equivalent Verilog test bench. We establish lower and upper bounds on the performance of the Raven environment executing on a single-processor, on a set of distributed processors, and on a shared-memory multiprocessor.",
                "id": "00238"
            },
            {
                "title": "The Potential of Compile-Time Analysis to Adapt the Cache Coherence Enforcement Strategy to the Data Sharing Characteristics",
                "abstract": "Cache coherence schemes that dynamically adapt to memory referencing patterns have been proposed to improve coherence enforcement in shared-memory multiprocessors. By using only run-time information, however, these existing schemes are incapable of looking ahead in the memory referencing stream. We present a combined hardware-software strategy that uses the predictive capability of the compiler to select updating or invalidating for each write reference. To determine the potential performance improvement that can be achieved with this optimization, three different levels of compiler capabilities are examined. Simulations using memory traces show that with an ideal compiler, this optimization can potentially reduce the miss ratio by 0.4% to 15% compared to an invalidating-only scheme, while reducing the generated network traffic by 13% to 94 % compared to an updating-only scheme. In addition, this optimization can potentially reduce the miss ratio by up to 13%, while reducing the generated network traffic by up to 92%, compared to a dynamic adaptive scheme. Furthermore, performance can be potentially improved even with a compiler capable of performing only imprecise array subscript analysis and no interprocedural analysis.Index Terms\u9a74Compiler optimization, update, invalidate, directory, cache coherence, shared-memory, multiprocessor.",
                "id": "00239"
            },
            {
                "title": "Using stochastic computing to implement digital image processing algorithms",
                "abstract": "As device scaling continues to nanoscale dimensions, circuit reliability will continue to become an ever greater problem. Stochastic computing, which performs computing with random bits (stochastic bits streams), can be used to enable reliable computation using those unreliable devices. However, one of the major issues of stochastic computing is that applications implemented with this technique are limited by the available computational elements. In this paper, first we will introduce and prove a stochastic absolute value function. Second, we will demonstrate a mathematical analysis of a stochastic tanh function, which is a key component used in a stochastic comparator. Third, we will present a quantitative analysis of a one-parameter linear gain function, and propose a new two-parameter version. The validity of the present stochastic computational elements is demonstrated through four basic digital image processing algorithms: edge detection, frame difference based image segmentation, median filter based noise reduction, and image contrast stretching. Our experimental results show that stochastic implementations tolerate more noise and consume less hardware than their conventional counterparts.",
                "id": "00240"
            },
            {
                "title": "Techniques for obtaining high performance in Java programs",
                "abstract": "This survey describes research directions in techniques to improve the performance of programs written in the Java programming language. The standard technique for Java execution is interpretation, which provides for extensive portability of programs. A Java interpreter dynamically executes Java bytecodes, which comprise the instruction set of the Java Virtual Machine (JVM). Execution time performance of Java programs can be improved through compilation, possibly at the expense of portability. Various types of Java compilers have been proposed, including Just-In-Time (JIT) compilers that compile bytecode into native processor instructions on the fly; direct compilers that directly translate the Java source code into the target processor's native language; and bytecode-to-source translators that generate either native code or an intermediate language, such as C, from the bytecodes. Additional techniques, including bytecode optimization, dynamic compilation, and executing Java programs in parallel, attempt to improve Java run-time performance while maintaining Java's portability. Another alternative for executing Java programs is a Java processor that implements the JVM directly in hardware. In this survey, we discuss the basis features, and the advantages and disadvantages, of the various Java execution techniques. We also discuss the various Java benchmarks that are being used by the Java community for performance evaluation of the different techniques. Finally, we conclude with a comparison of the performance of the alternative Java execution techniques based on reported results.",
                "id": "00241"
            },
            {
                "title": "Guiding circuit level fault-tolerance design with statistical methods",
                "abstract": "In the last decade, the focus of fault-tolerance methods has tended towards circuit level modifications, such as transistor resizing, and away from expensive system level redundancy approaches. We present the results from a screening experiment to identify significant parameters in circuit level soft error simulations to guide such approaches to fault-tolerance. This approach allows us to assess which parameters will have the most significance for reducing soft error rates and the impact that process variation will have on the accuracy of soft error rate estimates. We identify supply voltage and transistor type as being the most significant parameters affecting soft errors in logic cells across several technology scales. Additionally, we provide a ranking of more than a dozen parameters, across four technology scales, based on the significance of their impact on soft error rates.",
                "id": "00242"
            },
            {
                "title": "Reducing the Branch Penalty in Pipelined Processors",
                "abstract": "A probabilistic model is developed to quantify the performance effects of the branch penalty in a typical pipeline. The branch penalty is analyzed as a function of the relative number of branch instructions executed and the probability that a branch is taken. The resulting model shows the fraction of maximum performance achievable under the given conditions. Techniques to reduce the branch penalty include static and dynamic branch prediction, the branch target buffer, the delayed branch, branch bypassing and multiple prefetching, branch folding, resolution of branch decision early in the pipeline, using multiple independent instruction streams in a shared pipeline, and the prepare-to-branch instruction.",
                "id": "00243"
            },
            {
                "title": "Improving Computer Architecture Simulation Methodology by Adding Statistical Rigor",
                "abstract": "Due to cost, time, and flexibility constraints, computer architects use simulators to explore the design space when developing new processors and to evaluate the performance of potential enhancements. However, despite this dependence on simulators, statistically rigorous simulation methodologies are typically not used in computer architecture research. A formal methodology can provide a sound basis for drawing conclusions gathered from simulation results by adding statistical rigor and, consequently, can increase the architect's confidence in the simulation results. This paper demonstrates the application of a rigorous statistical technique to the setup and analysis phases of the simulation process. Specifically, we apply a Plackett and Burman design to: 1) identify key processor parameters, 2) classify benchmarks based on how they affect the processor, and 3) analyze the effect of processor enhancements. Our results showed that, out of the 41 user-configurable parameters in SimpleScalar, only 10 had a significant effect on the execution time. Of those 10, the number of reorder buffer entries and the L2 cache latency were the two most significant ones, by far. Our results also showed that Instruction Precomputation\u9a74a value reuse-like microarchitectural technique\u9a74 primarily improves the processor's performance by relieving integer ALU contention.",
                "id": "00244"
            },
            {
                "title": "A programmable and scalable technique to design spintronic logic circuits based on magnetic tunnel junctions",
                "abstract": "Exciting developments are taking place in the field of spintronics, particularly with the advances in the fabrication and characterization of devices such as Magnetic Tunnel Junctions (MTJ). The distinction of spintronic devices from conventional electronic devices makes it challenging to design efficient, scalable and low power logic circuits with MTJs. We propose a programmable and scalable technique to design MTJ-based logic circuits that are capable of implementing any 2-input logic truth table. We present the energy-delay trade-offs of this design with respect to circuit parameters. We also demonstrate that this circuit can be scaled to a 6-input logic function without incurring an increase in the energy consumption.",
                "id": "00245"
            },
            {
                "title": "An evaluation of a compiler optimization for improving the performance of a coherence directory",
                "abstract": "Both hardware-controlled and compiler-directed mechanisms have been proposed for maintaining cache coherence in large-scale shared-memory multiprocessors, but both of these approaches have significant limitations. We examine the potential performance improvement of a new software-hardware controlled cache coherence mechanism. This approach augments the run-time information available to a directory-based coherence mechanism with compile-time analysis that statically identifies write references that cannot cause coherence problems and writes that should be written through to memory. These references are marked as not needing to send invalidation messages to thereby reduce the network traffic produced by the directory while maintaining cache consistency. For those memory references that are ambiguous, due to conditional branches, or due to the need for complex data flow analysis, for instance, the compiler conservatively marks the references and relies on the hardware directory to ensure coherence. Trace-driven simulations are used to emulate the compile-time analysis on memory traces and to estimate potential performance improvement that could be expected from a compiler performing this optimization on the Perfect Club benchmark programs. By reducing the number of invalidations, this optimized directory scheme is capable of reducing the processor-memory network traffic by up to 54 percent compared to an unoptimized directory mechanism. In addition, the overall miss ratio can be reduced up to 42 percent due to a corresponding reduction in the number of write misses.",
                "id": "00246"
            },
            {
                "title": "Improving Data Cache Performance via Address Correlation: An Upper Bound Study",
                "abstract": "Address correlation is a technique that links the addresses that reference the same data values. Using a detailed source-code level analysis, a recent study [1] revealed that different addresses containing the same data can often be correlated at run-time to eliminate on-chip data cache misses. In this paper, we study the upper-bound performance of an Address Correlation System (ACS), and discuss specific optimizations for a realistic hardware implementation. An ACS can effectively eliminate most of the L1 data cache misses by supplying the data from a correlated address already found in the cache to thereby improve the performance of the processor. For 10 of the SPEC CPU2000 benchmarks, 57 to 99% of all L1 data cache load misses can be eliminated, which produces an increase of 0 to 243% in the overall performance of a superscalar processor. We also show that an ACS with 1-2 correlations for a value can usually provide comparable performance results to that of the upper bound. Furthermore, a considerable number of correlations can be found within the same set in the L1 data cache, which suggests that a low-cost ACS implementation is possible.",
                "id": "00247"
            },
            {
                "title": "Design of a spintronic arithmetic and logic unit using magnetic tunnel junctions",
                "abstract": "Conventional electronics technology uses an electron's charge to store information and a current of electrons to transfer information. Spintronics technology, in contrast, uses an electron's 'spin' in addition to its charge to transfer and store information. Magnetic Tunnel Junctions (MTJ) are spintronic devices that exhibit two distinct resistance states due to the tunneling magnetoresistance (TMR) effect. Their properties can provide significant advantages over conventional electronics in the design of computer systems. We characterize some of the challenges in using spintronic technology in large systems, and describe a novel design technique called 'union with neutralization' to combine individual component designs into multi-functional units. We use this technique to present the design of an arithmetic and logical unit (ALU) using 20 MTJs and two CMOS-based sense amplifiers. The spintronics-based ALU has the potential to offer considerable area, timing, and power advantages over a conventional CMOS-based ALU.",
                "id": "00248"
            },
            {
                "title": "Address Correlation: Exceeding the Limits of Locality",
                "abstract": "We investigate a program phenomena, Address Correlation, which links addresses that reference the same data. This work shows that different addresses containing the same data can often be correlated at run-time to eliminate a load miss or a partial hit. For ten of the SPEC CPU2000 benchmarks, 57 to 99% of all L1 data cache load misses, and 4 to 85% of all partial hits, can be supplied from a correlated address already found in the cache. Our source code-level analysis shows that semantically equivalent information, duplicated references, and frequent values are the major causes of address correlations. We also show that, on average, 68% of the potential correlated addresses that could supply data on a miss of an address containing the same value can be correlated at run time. These correlated addresses correspond to an average of 62% of all misses in the benchmark programs tested.",
                "id": "00249"
            },
            {
                "title": "Accelerating the performance of stochastic encoding-based computations by sharing bits in consecutive bit streams",
                "abstract": "Stochastic encoding represents a value using the probability of ones in a random bit stream. Computation based on this encoding has good fault-tolerance and low hardware cost. However, one of its major issues is long processing time. We have to use a long enough bit stream to represent a value to guarantee that random fluctuations introduce only small errors to final computation results. For example, for most digital image processing algorithms, we need a 512-bit stream to represent an 8-bit pixel value stochastically to guarantee that the final computation error is less than 5%. To solve this issue, this paper proposes to share bits between adjacent bit streams to represent adjacent deterministic values. For example, in image processing applications, the bit stream which represents the current pixel value can share parts of the bits in the bit stream which represents the previous pixel value. We use an image contrast stretching algorithm to evaluate this method. Our experimental results show that the proposed methods can improve the performance by 90%.",
                "id": "00250"
            },
            {
                "title": "A Superassociative Tagged Cache Coherence Directory",
                "abstract": "Dynamically tagged directories are memory-efficient mechanisms for maintaining cache coherence in shared- memory multiprocessors. These directories use special- purpose caches of pointers that are subject to two types of overflow: 1) pointer overflow, which limits the maximum sharing of a memory block, and 2) set overflow, which forces the premature invalidation of cached blocks. We propose a superassociative tagged directory that can preserve some of the cached copies of a memory block when a set overflows by allowing multiple address tags in the same set to contain the same address value. Verilog descriptions are used to estimate its implementation cost and timing delay, and a multiprocessor cache simulator is used to evaluate its performance.",
                "id": "00251"
            },
            {
                "title": "A divide-and-conquer approach for solving singular value decomposition on a heterogeneous system",
                "abstract": "Singular value decomposition (SVD) is a fundamental linear operation that has been used for many applications, such as pattern recognition and statistical information processing. In order to accelerate this time-consuming operation, this paper presents a new divide-and-conquer approach for solving SVD on a heterogeneous CPU-GPU system. We carefully design our algorithm to match the mathematical requirements of SVD to the unique characteristics of a heterogeneous computing platform. This includes a high-performanc solution to the secular equation with good numerical stability, overlapping the CPU and the GPU tasks, and leveraging the GPU bandwidth in a heterogeneous system. The experimental results show that our algorithm has better performance than MKL's divide-and-conquer routine [18] with four cores (eight hardware threads) when the size of the input matrix is larger than 3000. Furthermore, it is up to 33 times faster than LAPACK's divide-and-conquer routine [17], 3 times faster than MKL's divide-and-conquer routine with four cores, and 7 times faster than CULA on the same device, when the size of the matrix grows up to 14,000. Our algorithm is also much faster than previous SVD approaches on GPUs.",
                "id": "00252"
            },
            {
                "title": "Analysis of Statistical Sampling in Microarchitecture Simulation: Metric, Methodology and Program Characterization",
                "abstract": "Statistical sampling, especially stratified random sampling, is a promising technique for estimating the performance of the benchmark program without executing the complete program on microarchitecture simulators or real machines. The accuracy of the performance estimate and the simulation cost depend on the three parameters, namely the interval size, the sample size, and the number of phases (or strata). Optimum values for these three parameters depends on the performance behavior of the program and the microarchitecture configuration being evaluated. In this paper, we quantify the effect of these three parameters and their interactions on the accuracy of the performance estimate and simulation cost. We use the Confidence Interval of estimated Mean (CIM), a metric derived from statistical sampling theory, to measure the accuracy of the performance estimate; we also discuss why CIM is an appropriate metric for this analysis. We use the total number of instructions simulated and the total number of samples measured as cost parameters. Finally, we characterize 21 SPEC CPU2000 benchmarks based on our analysis.",
                "id": "00253"
            },
            {
                "title": "On Memory System Design for Stochastic Computing.",
                "abstract": "Growing uncertainty in design parameters (and therefore, in design functionality) renders stochastic computing particularly promising, which represents and processes data as quantized probabilities. However, due to the difference in data representation, integrating conventional memory (designed and optimized for non-stochastic computing) in stochastic computing systems inevitably incurs a signific...",
                "id": "00254"
            },
            {
                "title": "A network status predictor to support dynamic scheduling in network-based computing systems",
                "abstract": "The management of networks has often been ignored in network-based computing systems due to the difficulty of estimating application programs' network latency and bandwidth requirements, and the difficulty of predicting the system network load. To help address this deficiency and thereby support dynamic network resource scheduling, we propose the network status predictor (NSP). This tool is a general and extensible network load monitor that introduces lower and upper latency prediction bounds. We evaluate its ability to dynamically predict TCP/IP end-to-end latency, with varying network loads using a cluster of SGI multiprocessors interconnected with a fibre channel network. Our results show that a combination of numerical predictors can be dynamically selected based on the network's recent state to produce better predictions than when using a single predictor alone",
                "id": "00255"
            },
            {
                "title": "Computation on Stochastic Bit Streams Digital Image Processing Case Studies",
                "abstract": "Maintaining the reliability of integrated circuits as transistor sizes continue to shrink to nanoscale dimensions is a significant looming challenge for the industry. Computation on stochastic bit streams, which could replace conventional deterministic computation based on a binary radix, allows similar computation to be performed more reliably and often with less hardware area. Prior work discussed a variety of specific stochastic computational elements (SCEs) for applications such as artificial neural networks and control systems. Recently, very promising new SCEs have been developed based on finite-state machines (FSMs). In this paper, we introduce new SCEs based on FSMs for the task of digital image processing. We present five digital image processing algorithms as case studies of practical applications of the technique. We compare the error tolerance, hardware area, and latency of stochastic implementations to those of conventional deterministic implementations using binary radix encoding. We also provide a rigorous analysis of a particular function, namely the stochastic linear gain function, which had only been validated experimentally in prior work.",
                "id": "00256"
            },
            {
                "title": "The exigency of benchmark and compiler drift: designing tomorrow's processors with yesterday's tools",
                "abstract": "Due to the amount of time required to design a new processor, one set of benchmark programs may be used during the design phase while another may be the standard when the design is finally delivered. Using one benchmark suite to design a processor while using a different, presumably more current, suite to evaluate its ultimate performance may lead to sub-optimal design decisions if there are large differences between the characteristics of the two suites and their respective compilers. We call this changes across time \"drift\". To evaluate the impact of using yesterday's benchmark and compiler technology to design tomorrow's processors, we compare common benchmarks from the SPEC 95 and SPEC 2000 benchmark suites. Our results yield three key conclusions. First, we show that the amount of drift, for common programs in successive SPEC benchmark suites, is significant. In SPEC 2000, the main memory access time is a far more significant performance bottleneck than in SPEC 95, while less significant SPEC 2000 performance bottlenecks include the L2 cache latency, the L1 I-cache size, and the number of reorder buffer entries. Second, using two different statistical techniques, we show that compiler drift is not as significant as benchmark drift. Third, we show that benchmark and compiler drift can have a significant impact on the final design decisions. Specifically, we use a one-parameter-at-a-time optimization algorithm to design two different year-2000 processors, one optimized for SPEC 95 and the other optimized for SPEC 2000, using the energy-delay product (EDP) as the optimization criterion. The results show that using SPEC 95 to design a year-2000 processor results in an 18.5% larger EDP and a 20.8% higher CPI than using the SPEC 2000 benchmarks to design the corresponding processor. Finally, we make a few recommendations to help computer architects minimize the effects of benchmark and compiler drift.",
                "id": "00257"
            },
            {
                "title": "The synthesis of complex arithmetic computation on stochastic bit streams using sequential logic",
                "abstract": "The paradigm of logical computation on stochastic bit streams has several key advantages compared to deterministic computation based on binary radix, including error-tolerance and low hardware area cost. Prior research has shown that sequential logic operating on stochastic bit streams can compute non-polynomial functions, such as the tanh function, with less energy than conventional implementations. However, the functions that can be computed in this way are quite limited. For example, high order polynomials and non-polynomial functions cannot be computed using prior approaches. This paper proposes a new finite-state machine (FSM) topology for complex arithmetic computation on stochastic bit streams. It describes a general methodology for synthesizing such FSMs. Experimental results show that these FSM-based implementations are more tolerant of soft errors and less costly in terms of the area-time product that conventional implementations.",
                "id": "00258"
            },
            {
                "title": "A Compiler-Assisted Scheme for Adaptive Cache Coherence Enforcement",
                "abstract": "Cache coherence mechanisms in shared-memory multiprocessors typically use either updating or invalidating to prevent access to stale data, but neither enforcement strategy is the best choice for all programs. We present a compile-time optimization that uses the look-ahead capability of the compiler to select updating, invalidating, or neither for each write reference in a program to thereby produce the best overall memory performance. We implement this optimization in the Parafrase-2 compiler for memory references to scalar variables and use trace-driven simulations to compare the performance of this compiler-assisted adaptive coherence enforcement to hardware-only mechanisms. We find that this compiler optimization can produce miss ratios comparable to those produced by an updating-only mechanism while frequently reducing the total network traffic to below that produced by any of the hardware-only mechanisms.",
                "id": "00259"
            },
            {
                "title": "Cache coherence in large-scale shared-memory multiprocessors: issues and comparisons",
                "abstract": "Private data caches have not been as effective in reducing the average memory delay in multiprocessors as in uniprocessors due to data spreading among the processors, and due to the cache coherence problem. A wide variety of mechanisms have been proposed for maintaining cache coherence in large-scale shared memory multiprocessors making it difficult to compare their performance and implementation implications. To help the computer architect understand some of the trade-offs involved, this paper surveys current cache coherence mechanisms, and identifies several issues critical to their design. These design issues include: 1) the coherence detection strategy, through which possibly incoherent memory accesses are detected either statically at compile-time, or dynamically at run-time; 2) the coherence enforcement strategy, such as updating or invalidating, that is used to ensure that stale cache entries are never referenced by a processor; 3) how the precision of block sharing information can be changed to trade-off the implementation cost and the performance of the coherence mechanism; and 4) how the cache block size affects the performance of the memory system. Trace-driven simulations are used to compare the performance and implementation impacts of these different issues. In addition, hybrid strategies are presented that can enhance the performance of the multiprocessor memory system by combining several different coherence mechanisms into a single system.",
                "id": "00260"
            },
            {
                "title": "The NanoBox project: exploring fabrics of self-correcting logic blocks for high defect rate molecular device technologies",
                "abstract": "Trends indicate that emerging process technologies, including molecular computing, will experience an increase in the number of noise induced errors and device defects. In this paper, we in- troduce the NanoBox, a logic lookup table with fault tolerance coding applied to the lookup table bit string. In this way, we contain and self-correct errors within the lookup table, thereby presenting a robust logic block to higher levels of logic design. We explore five different NanoBox coding techniques. We also ex- amine the cost of implementing two different circuit blocks using a homogenous fabric of NanoBox logic elements: 1) a floating point control unit from the IBM Power4 microprocessor and 2) a four-instruction ALU. In this initial investigation, our results are not meant to draw definitive conclusions about any specific NanoBox implementation, but rather to spur discussion and ex- plore the feasibility of fine-grained error correction techniques in molecular computing systems.",
                "id": "00261"
            },
            {
                "title": "An efficient implementation of numerical integration using logical computation on stochastic bit streams",
                "abstract": "Numerical integration is a widely used approach for computing an approximate result of a definite integral. Conventional digital implementations of numerical integration using binary radix encoding are costly in terms of hardware and have long computational delay. This work proposes a novel method for performing numerical integration based on the paradigm of logical computation on stochastic bit streams. In this paradigm, ordinary digital circuits are employed but they operate on stochastic bit streams instead of deterministic values; the signal value is encoded by the probability of obtaining a one versus a zero in the streams. With this type of computation, complex arithmetic operations can be implemented with very simple circuitry. However, typically, such stochastic implementations have long computational delay, since long bit streams are required to encode precise values. This paper proposes a stochastic design for numerical integration characterized by both small area and short delay - so, in contrast to previous applications, a win on both metrics. The design is based on mathematical analysis that demonstrates that the summation of a large number of terms in the numerical integration could lead to a significant delay reduction. An architecture is proposed for this task. Experiments confirm that the stochastic implementation has smaller area and shorter delay than conventional implementations.",
                "id": "00262"
            },
            {
                "title": "Logical Computation on Stochastic Bit Streams with Linear Finite-State Machines",
                "abstract": "Most digital systems operate on a positional representation of data, such as binary radix. An alternative is to operate on random bit streams where the signal value is encoded by the probability of obtaining a one versus a zero. This representation is much less compact than binary radix. However, complex operations can be performed with very simple logic. Furthermore, since the representation is uniform, with all bits weighted equally, it is highly tolerant of soft errors (i.e., bit flips). Both combinational and sequential constructs have been proposed for operating on stochastic bit streams. Prior work has shown that combinational logic can implement multiplication and scaled addition effectively while linear finite-state machines (FSMs) can implement complex functions such as exponentiation and tanh effectively. Prior work on stochastic computation has largely been validated empirically.This paper provides a rigorous mathematical treatment of stochastic implementation of complex functions such as exponentiation and tanh implemented using linear FSMs. It presents two new functions, an absolute value function and exponentiation based on an absolute value, motivated by specific applications. Experimental results show that the linear FSM-based constructs for these functions have smaller area-delay products than the corresponding deterministic constructs. They also are much more tolerant of soft errors.",
                "id": "00263"
            },
            {
                "title": "The Effect of using State-Based Priority Information in a Shared-Memory Multiprocessor Cache Replacement Policy",
                "abstract": " The cache replacement policy is one of the factorsthat determines the effectiveness of cache memories.In this paper, we study the impact of incorporating thecache block coherence state information in the Random replacement policy in a shared--memory multiprocessor.We assign replacement priority to each cacheblock within a set based on its state. To reduce theprobability of replacing a recently accessed block andto adapt to the program's access patterns, we also associatewith each set... ",
                "id": "00264"
            },
            {
                "title": "Write buffer design for cache-coherent shared-memory multiprocessors",
                "abstract": "We evaluate the performance impact of two different write-buffer configurations (one word per buffer entry and one block per buffer entry) and two different write policies (write-through and write-back), when using the partial block invalidation coherence mechanism in a shared-memory multiprocessor. Using an execution-driven simulator, we find that the one word per entry buffer configuration with a write-back policy is preferred for small write-buffer sizes when both buffers have an equal number of data words, and when they have equal hardware cost. Furthermore, when partial block invalidation is supported, we find that a write-through policy is preferred over a write-back policy due to its simpler cache hit detection mechanism, its elimination of write-back transactions, and its competitive-performance when the write-buffer is relatively large.",
                "id": "00265"
            },
            {
                "title": "Coarse-grained speculative execution in shared-memory multiprocessors",
                "abstract": " This thesis presents a new parallelization model, called coarse-grained thread pipelining, for exploitingcoarse-grained parallelism from general-purpose application programs in shared-memorymultiprocessor systems. This parallelization model, which is based on the fine-grained threadpipelining model proposed for the superthreaded architecture [7], allows concurrent execution ofloop iterations in a pipelined fashion with run-time data dependence checking and control speculation.The... ",
                "id": "00266"
            },
            {
                "title": "Deferred updates for flash-based storage",
                "abstract": "The NAND flash memory based storage has faster read, higher power savings, and lower cooling cost compared to the conventional rotating magnetic disk drive. However, in case of flash memory, read and write operations are not symmetric. Write operations are much slower than read operations. Moreover, frequent update operations reduce the lifetime of the flash memory. Due to the faster read performance, flash-based storage is particularly attractive for the read-intensive database workloads, while it can produce poor performance when used for the update-intensive database workloads. This paper aims to improve write performance and lifetime of flash-based storage for the update-intensive workloads. In particular, we propose a new hierarchical approach named as deferred update methodology. Instead of directly updating the data records, first we buffer the changes due to update operations as logs in two intermediate in-flash layers. Next, we apply multiple update logs in bulk to the data records. Experimental results show that our proposed methodology significantly improves update processing overhead and longevity of the flash-based storages.",
                "id": "00267"
            },
            {
                "title": "Layered View Of Qos Issues In Ip-Based Mobile Wireless Networks",
                "abstract": "With the convergence of wireless communication and IP-based networking technologies, future IP-based wireless networks are expected to support real-time multimedia. IP services over wireless networks (e.g. wireless access to Internet) enhance the mobility and flexibility of traditional IP network users. Wireless networks extend the current IP service infrastructure to a mix of transmission media, bandwidth, costs, coverage, and service agreements, requiring enhancements to the IP protocol layers in wireless networks. Furthermore, QoS provisioning is required at various layers of the IP protocol stack to guarantee different types of service requests, giving rise to issues related to cross-layer design methodology. This paper reviews issues and prevailing solutions to performance enhancements and QoS provisioning for IP services over mobile wireless networks from a layered view. Copyright (c) 2006 John Wiley & Sons, Ltd.",
                "id": "00268"
            },
            {
                "title": "Sampling-based garbage collection metadata management scheme for flash-based storage",
                "abstract": "Existing garbage collection algorithms for the flash-based storage use score-based heuristics to select victim blocks for reclaiming free space and wear leveling. The score for a block is estimated using metadata information such as age, block utilization, and erase count. To quickly find a victim block, these algorithms maintain a priority queue in the SRAM of the storage controller. This priority queue takes O(K) space, where K stands for flash storage capacity in total number of blocks. As the flash capacity scales to larger size, K also scales to larger value. However, due to higher price per byte, SRAM will not scale proportionately. In this case, due to SRAM scarcity, it will be challenging to implement a larger priority queue in the limited SRAM of a large-capacity flash storage. In addition to space issue, with any update in the metadata information, the priority queue needs to be continuously updated, which takes O(lg(K)) operations. This computation overhead also increases with the increase of flash capacity. In this paper, we have taken a novel approach to solve the garbage collection metadata management problem of a large-capacity flash storage. We propose a sampling-based approach to approximate existing garbage collection algorithms in the limited SRAM space. Since these algorithms are heuristic-based, our sampling-based algorithm will perform as good as unsampled (original) algorithm, if we choose good samples to make garbage collection decisions. We propose a very simple policy to choose samples. Our experimental results show that small number of samples are good enough to emulate existing garbage collection algorithms.",
                "id": "00269"
            },
            {
                "title": "An Overview of Time-Based Computing with Stochastic Constructs.",
                "abstract": "Computing on time-based data is a recent evolution of research in stochastic computing. As with stochastic computing, complex functions can be computed with remarkably low area cost. Unlike stochastic computing, the latency and energy efficiency are very favorable compared to computations on conventional binary radix. In this article, the authors review and evaluate the design and implementation o...",
                "id": "00270"
            },
            {
                "title": "Exploiting the Impact of Database System Configuration Parameters: A Design of Experiments Approach",
                "abstract": "Tuning database system configuration parameters to proper values acc ording to the expected query workload plays a very important role in determining DBMS performance. H owever, the number of configuration parameters in a DBMS is very large. Furthermore, typicalquery workloads have a large number of constituent queries, which makes tuning very time and effort in tensive. To reduce tuning time and effort, database administrators rely on their experience and some ru les of thumb to select a set of important configuration parameters for tuning. Nonetheless, as a statistica lly rigorous methodology is not used, time and effort may be wasted by tuning those parameters which may have no or marginal effects on the DBMS performance for the given query workload. Database a dministrators also use compressed query workloads to reduce tuning time. If not carefully sele cted, the compressed query workload may fail to include a query which may reveal important performa nce bottleneck parameters. In this article, we provide a systematic approach to help the database adminis trators in tuning activities. We achieve our goals through two phases. First, we estimate the effects of th e configuration parameters for each workload query. The effects are estimated through a design of e xperiments-basedPLACKETT & BURMAN design methodology where the number of experiments required is linearly proportional to the number of input parameters. Second, we exploit the estimated effects to:1) rank DBMS configuration parameters for a given query workload based on their impact on the DBMS performance, and 2) select a compressed query workload that preserves the fidelity of the originalworkload. Experimental results using PostgreSQL and TPC-H query workload show that our methodolog ies are working correctly.",
                "id": "00271"
            },
            {
                "title": "A Comparative Analysis Of Parallel Programming Language Complexity And Performance",
                "abstract": "Several parallel programming languages, libraries and environments have been developed to ease the task of writing programs for multiprocessors. Proponents of each approach often point out various language features that are designed to provide the programmer with a simple programming interface. However, virtually no data exist that quantitatively evaluate the relative ease of use of different parallel programming languages. The paper borrows techniques from the software engineering field to quantify the complexity of three predominant programming models: shared-memory, message-passing and high-performance Fortran, It is concluded that traditional software complexity metrics are effective indicators of the relative complexity of parallel programming languages. The impact of complexity on run-time performance is also discussed in the context of message-passing vs. HPF on an IBM SP2. (C) 1998 John Wiley & Sons, Ltd.",
                "id": "00272"
            },
            {
                "title": "Communicating Quality of Service Requirements to an Object-Based Storage Device",
                "abstract": "Obtaining consistent bandwidth with predictable latency from disk-based storage systems has proven difficult due to the storage system's inability to understand Quality of Service (QoS) requirements. In this paper, we present a feasibility study of QoS with the Object-based Storage Device (OSD) specification. We look at OSD's ability to provide QoS guarantees for consistent bandwidth with predictable latency. Included in this paper is a description of QoS requirements of a sample application and how these requirements are translated into parameters that are then communicated to, and interpreted by, the OSD. Implementation problems lead to the failure of a hard real-time QoS model, but this failure is not due to the OSD protocol. The paper concludes with a description of how well the Revision 9 OSD standard (OSDR9) is able to accommodate QoS. We provide suggestions for improving the OSD specification and its ability to communicate QoS requirements.",
                "id": "00273"
            },
            {
                "title": "MinneSPEC: A New SPEC Benchmark Workload for Simulation-Based Computer Architecture Research",
                "abstract": "Computer architects must determine how tomost effectively use finite computational resources whenrunning simulations to evaluate new architectural ideas.To facilitate efficient simulations with a range of benchmarkprograms, rn have developed the MinneSPEC inputset for the SPEC CPU 2000 benchmark suite. Thisnew workload allows computer architects to obtain simulationresults in a reasonable time using existing sirnulators.While the MinneSPEC workload is derived from thestandard SPEC CPU 2000 warklcad, it is a valid benchmarksuite in and of itself for simulation-based research.MinneSPEC also may be used to run Iarge numbers ofsimulations to find \"sweet spots\" in the evaluation parameterspace. This small number of promising designpoints subsequently may be investigated in more detailwith the full SPEC reference workload. In the processof developing the MinneSPEC datasets, we quantify itsdifferences in terms of function-level execution patterns,instruction mixes, and memory behaviors compared tothe SPEC programs when executed with the reference inputs.We find that for some programs, the MinneSPECprofiles match the SPEC reference dataset program behaviorvery closely. For other programs, however, theMinneSPEC inputs produce significantly different programbehavior. The MinneSPEC workload has been recognizedby SPEC and is distributed with Version 1.2 andhigher of the SPEC CPU 2000 benchmark suite.",
                "id": "00274"
            },
            {
                "title": "Complexity and Performance in Parallel Programming Languages",
                "abstract": "Several parallel programming languages, libraries and environments have been developed to ease the task of writing programs for multiprocessors. Proponents of each approach often point out various language features that are designed to provide the programmer with a simple programming interface. However, virtually no data exists that quantitatively evaluates the relative ease of use of different parallel programming languages. The following paper borrows techniques from the software engineering field to quantify the complexity of three predominate programming models: shared memory, message passing and High-Performance Fortran. It is concluded that traditional software complexity metrics are effective indicators of the relative complexity of parallel programming languages. The impact of complexity on run-time performance is also discussed in the context of message-passing versus HPF on an IBM SP2.",
                "id": "00275"
            },
            {
                "title": "Compiler-Directed Classification of Value Locality Behavior",
                "abstract": "Abstract: Value prediction has been suggested as a way to increase the instruction-level parallelism available in a superscalar processor. One of the potential difficulties in cost-effectively predicting values for a given instruction, however, is selecting the proper type of predictor. We propose a compiler-directed classification scheme that statically partitions instructions in a program into several groups, each of which is associated with a specific value predictability pattern. This value predictability pattern is encoded into the instructions to identify the type of value predictor that will be best suited for each instruction at run-time. Both an idealized profile-based compiler implementation and an implementation based on the GCC compiler are studied. We use execution-driven simulation and SPEC95 and SPEC2000 benchmarks to study the performance of this approach. This work also demonstrates the connection between value locality and source-level program structures thereby leading to a deeper understanding of the causes of this behavior.",
                "id": "00276"
            },
            {
                "title": "Improving nanoelectronic designs using a statistical approach to identify key parameters in circuit level SEU simulations",
                "abstract": "One of the key challenges in nanoelectronics design is the decreasing reliability due to radiation induced single-event upsets. Without detailed device level simulations or physical experimentation, circuit level models can generate misleading reliability information. We present the results from a screening experiment to identify significant parameters in circuit level SEU simulations. We show that cell supply voltage, sizing parameters, and transient waveform descriptions have an important impact on design and should therefore be considered with care in circuit level designs. Larger variations in parameters can lead to soft error rate estimates that vary by more than 4 orders of magnitude, even small variations can lead to 15x variation in soft error rate estimation for a design. We present our methodology for screening and a ranking based on significance of several parameters involved in soft error simulation at the SPICE level.",
                "id": "00277"
            },
            {
                "title": "Accelerating geoscience and engineering system simulations on graphics hardware",
                "abstract": "Many complex natural systems studied in the geosciences are characterized by simple local-scale interactions that result in complex emergent behavior. Simulations of these systems, often implemented in parallel using standard central processing unit (CPU) clusters, may be better suited to parallel processing environments with large numbers of simple processors. Such an environment is found in graphics processing units (GPUs) on graphics cards. This paper discusses GPU implementations of three example applications from computational fluid dynamics, seismic wave propagation, and rock magnetism. These candidate applications involve important numerical modeling techniques, widely employed in physical system simulations, that are themselves examples of distinct computing classes identified as fundamental to scientific and engineering computing. The presented numerical methods (and respective computing classes they belong to) are: (1) a lattice-Boltzmann code for geofluid dynamics (structured grid class); (2) a spectral-finite-element code for seismic wave propagation simulations (sparse linear algebra class); and (3) a least-squares minimization code for interpreting magnetic force microscopy data (dense linear algebra class). Significant performance increases (between 10x and 30x in most cases) are seen in all three applications, demonstrating the power of GPU implementations for these types of simulations and, more generally, their associated computing classes.",
                "id": "00278"
            },
            {
                "title": "Impact of spintronic memory on multicore cache hierarchy design.",
                "abstract": "Spintronic memory [spin-transfer torque-magnetic random access memory (STT-MRAM)] is an attractive alternative technology to CMOS since it offers higher density and virtually no leakage current. Spintronic memory continues to require higher write energy, however, presenting a challenge to memory hierarchy design when energy consumption is a concern. This study motivates the use of STT-MRAM for the...",
                "id": "00279"
            },
            {
                "title": "Characterizing and Comparing Prevailing Simulation Techniques",
                "abstract": "Due to the simulation time of the reference input set, architects often use alternative simulation techniques. Although these alternatives reduce the simulation time, what has not been evaluated is their accuracy relative to the reference input set, and with respect to each other. To rectify this deficiency, this paper uses three methods to characterize the reduced input set, truncated execution, and sampling simulation techniques while also examining their speed versus accuracy trade-off and configuration dependence. Finally, to illustrate the effect that a technique could have on the apparent speedup results, we quantify the speedups obtained with two processor enhancements. The results show that: 1) The accuracy of the truncated execution techniques was poor for all three characterization methods and for both enhancements, 2) The characteristics of the reduced input sets are not reference-like, and 3) SimPoint and SMARTS, the two sampling techniques, are extremely accurate and have the best speed versus accuracy trade-offs. Finally, this paper presents a decision tree which can help architects choose the most appropriate technique for their simulations.",
                "id": "00280"
            },
            {
                "title": "SARD: A statistical approach for ranking database tuning parameters",
                "abstract": "The ability of a database management system (DBMS) to detect problems or problem trends (which have not yet manifested as problems) and either take corrective actions, and/or provide advice or an automated notification to the database administrator (DBA) ...",
                "id": "00281"
            },
            {
                "title": "Polysynchronous Clocking: Exploiting the Skew Tolerance of Stochastic Circuits.",
                "abstract": "In the paradigm of stochastic computing, arithmetic functions are computed on randomized bit streams. The method naturally and effectively tolerates very high clock skew. Exploiting this advantage, this paper introduces polysynchronous clocking, a design strategy in which clock domains are split at a very fine level. Each domain is synchronized by an inexpensive local clock. Alternatively, the ske...",
                "id": "00282"
            },
            {
                "title": "The Recursive NanoBox Processor Grid: A Reliable System Architecture for Unreliable Nanotechnology Devices",
                "abstract": "Advanced molecular nanotechnology devices are expectedto have exceedingly high transient fault rates and largenumbers of inherent device defects compared to conventionalCMOS devices. We introduce the Recursive NanoBoxProcessor Grid as an application specific, fault-tolerant,parallel computing system designed for fabrication with unreliablenanotechnology devices. In this initial study weconstruct VHDL models of the NanoBox Processor cellALU and evaluate the effectiveness of our recursive faultmasking approach in the presence of random transient errors.Our analysis shows that the ALU can calculate correctly100 percent of the time with raw FIT (failures in time)rates as high as 10{23}. We achieve this error correction withan area overhead on the order of 9x, which is quite reasonablegiven the high integration densities expected with nanodevices.",
                "id": "00283"
            },
            {
                "title": "CIM: A Reliable Metric for Evaluating Program Phase Classifications",
                "abstract": "We propose the use of the Confidence Interval of estimated Mean (CIM), a metric based on statistical sampling theory, to evaluate the quality of a given phase classification and for comparing different phase classification schemes. Previous research on phase classification used the Weighted Average of Coefficient of Variation (CoVwa) to estimate phase classification quality. We found that the phase quality indicated by CoVwa could be inconsistent across different phase classifications. We explain the reasons behind this inconsistency and demonstrate the inconsistency using data from several SPEC CPU2000 benchmark programs. We show that the Confidence Interval of estimated Mean (CIM) correctly estimates the quality of phase classification with a meaningful statistical interpretation.",
                "id": "00284"
            },
            {
                "title": "Performance Analysis and Prediction of Processor Scheduling Strategies in Multiprogrammed Shared-Memory Multiprocessors",
                "abstract": " Small-scale shared-memory multiprocessors are commonly used in a workgroup environmentwhere multiple applications, both parallel and sequential, are executed concurrentlywhile sharing the processors and other system resources. To utilize the processors efficiently,an effective scheduling strategy is required. In this paper, we use performance data obtainedfrom an SGI multiprocessor to evaluate several processor scheduling strategies. We examinegang scheduling (coscheduling), static space... ",
                "id": "00285"
            },
            {
                "title": "Coarse-grained thread pipelining: a speculative parallel execution model for shared-memory multiprocessors",
                "abstract": "This paper presents a new parallelization model, called coarse-grained thread pipelining, for exploiting speculative coarse-grained parallelism from general-purpose application programs in shared-memory multiprocessor systems. This parallelization model, which is based on the fine-grained thread pipelining model proposed for the superthreaded architecture, allows concurrent execution of loop iterations in a pipelined fashion with runtime data-dependence checking and control speculation. The speculative execution combined with the runtime dependence checking allows the parallelization of a variety of program constructs that cannot be parallelized with existing runtime parallelization algorithms. The pipelined execution of loop iterations in this new technique results in lower parallelization overhead than in other existing techniques. We evaluated the performance of this new model using some real applications and a synthetic benchmark. These experiments show that programs with a sufficiently large grain size compared to the parallelization overhead obtain significant speedup using this model. The results from the synthetic benchmark provide a means for estimating the performance that can be obtained from application programs that will be parallelized with this model. The library routines developed for this thread pipelining model are also useful for evaluating the correctness of the codes generated by the superthreaded compiler and in debugging and verifying the simulator for the superthreaded processor.",
                "id": "00286"
            },
            {
                "title": "Efficient Use of Dynamically tagged Directories Through Compiler Analysis",
                "abstract": "Dynamiically tagged directories have been recently proposed as a memory-efficient mechanism for maintaining cache coherence in large-scale shared-memory multiprocessors. In order to efficiently use these directories, the number of pointer operations must be minimized and pointers should be allocated as late as possible. If pointers are allocated too early, frequent pointer overflow will occur, which in turn may cause cache thrashing.",
                "id": "00287"
            },
            {
                "title": "Romano: autonomous storage management using performance prediction in multi-tenant datacenters",
                "abstract": "Workload consolidation is a key technique in reducing costs in virtualized datacenters. When considering storage consolidation, a key problem is the unpredictable performance behavior of consolidated workloads on a given storage system. In practice, this often forces system administrators to grossly overprovision storage to meet application demands. In this paper, we show that existing modeling techniques are inaccurate and ineffective in the face of heterogenous devices. We introduce Romano, a storage performance management system designed to optimize truly heterogeneous virtualized datacenters. At its core, Romano constructs and adapts approximate workload-specific performance models of storage devices automatically, along with prediction intervals. It then applies these models to allow highly efficient IO load balancing. End-to-end experiments demonstrate that Romano reduces prediction error by 80% on average compared with existing techniques. The result is improved load balancing with lowered variance by 82% and reduced average and maximum latency observed across the storage systems by 52% and 78%, respectively.",
                "id": "00288"
            },
            {
                "title": "MMV: a metamodeling based microprocessor validation environment",
                "abstract": "With increasing levels of integration of multiple processing cores and new features to support software functionality, recent generations of microprocessors face difficult validation challenges. The systematic validation approach starts with defining the correct behaviors of the hardware and software components and their interactions. This requires new modeling paradigms that support multiple levels of abstraction. Mutual consistency of models at adjacent levels of abstraction is crucial for manual refinement of models from the full chip level to production register transfer level, which is likely to remain the dominant design methodology of complex microprocessors in the near future. In this paper, we present microprocessor modeling and validation environment (MMV), a validation environment based on metamodeling, that can be used to create models at various abstraction levels and to generate most of the important validation collaterals, viz., simulators, checkers, coverage, and test generation tools. We illustrate the functionalities in MMV by modeling a 32-bit reduced instruction set computer processor at the system, instruction set architecture, and microarchitecture levels. We show by examples how consistency across levels is enforced during modeling and also how to generate constraints for automatic test generation.",
                "id": "00289"
            },
            {
                "title": "Dynamic task scheduling using online optimization",
                "abstract": "Algorithms for scheduling independent tasks on to the processors of a multiprocessor system must trade-off processor load balance, memory locality, and scheduling overhead. Most existing algorithms, however, do not adequately balance these conflicting factors. This paper introduces the self-adjusting dynamic scheduling (SADS) class of algorithms that use a unified cost model to explicitly account for these factors at runtime. A dedicated processor performs scheduling in phases by maintaining a tree of partial schedules and incrementally assigning tasks to the least-cost schedule. A scheduling phase terminates whenever any processor becomes idle, at which time partial schedules are distributed to the processors. An extension of the basic SADS algorithm, called DBSADS, controls the scheduling overhead by giving higher priority to partial schedules with more task-to-processor assignments. These algorithms are compared to two distributed scheduling algorithms within a database application on an Intel Paragon distributed memory multiprocessor system.",
                "id": "00290"
            },
            {
                "title": "An effective processor allocation strategy for multiprogrammed shared-memory multiprocessors",
                "abstract": "Existing techniques for sharing the processing resources in multiprogrammed shared-memory multiprocessors, such as time-sharing, space-sharing, and gang-scheduling, typically sacrifice the performance of individual parallel applications to improve overall system utilization. We present a new processor allocation technique called Loop-Level Process Control (LLPC) that dynamically adjusts the number of processors an application is allowed to use for the execution of each parallel section of code, based on the current system load. This approach exploits the maximum parallelism possible for each application without overloading the system. We implement our scheme on a Silicon Graphics Challenge multiprocessor system and evaluate its performance using applications from the Perfect Club benchmark suite and synthetic benchmarks. Our approach shows significant improvements over traditional time-sharing and gang-scheduling. It has performance comparable to, or slightly better than, static space-sharing, but our strategy is more robust since, unlike static space-sharing, it does not require a priori knowledge of the applications' parallelism characteristics.",
                "id": "00291"
            },
            {
                "title": "History index of correct computation for fault-tolerant nano-computing",
                "abstract": "Future nanoscale devices are expected to be more fragile and sensitive to external influences than conventional CMOS-based devices. Researchers predict that it will no longer be possible to test a device and then throw it away if it is found to be defective, as every circuit is expected to have multiple hard and soft defects. Fundamentally new fault-tolerant architectures are required to produce reliable systems that will survive with manufacturing defects and transient faults. This paper introduces the History Index of Correct Computation (HICC) as a run-time reconfiguration technique for fault-tolerant nano-computing. This approach identifies reliable blocks on-the-fly by monitoring the correctness of their outputs and forwarding only good results, ignoring the results from unreliable blocks. Simulation results show that history-based TMR modules offer a better response to fault tolerance at the module level than do conventional fault-tolerant approaches when the faults are nonuniformly distributed among redundant units. A correct computation rate of 99% is achieved despite a 13% average injected fault rate, when one of the redundant units and the decision unit are fault-free as well as when both have a low injected fault rate of 0.1%. A correct computation rate of 89% is achieved when faults are nonuniformly distributed at an average fault rate of 11% and fault rate in the decision unit is 0.5%. The robustness of the history-based mechanism is shown to be better than both majority voting and a Hamming detection and correction code.",
                "id": "00292"
            },
            {
                "title": "Data prefetch mechanisms",
                "abstract": "The expanding gap between microprocessor and DRAM performance has necessitated the use of increasingly aggressive techniques designed to reduce or hide the latency of main memory access. Although large cache hierarchies have proven to be effective in reducing this latency for the most frequently used data, it is still not uncommon for many programs to spend more than half their run times stalled on memory requests. Data prefetching has been proposed as a technique for hiding the access latency of data referencing patterns that defeat caching strategies. Rather than waiting for a cache miss to initiate a memory fetch, data prefetching anticipates such misses and issues a fetch to the memory system in advance of the actual memory reference. To be effective, prefetching must be implemented in such a way that prefetches are timely, useful, and introduce little overhead. Secondary effects such as cache pollution and increased memory bandwidth requirements must also be taken into consideration. Despite these obstacles, prefetching has the potential to significantly improve overall program execution time by overlapping computation with memory accesses. Prefetching strategies are diverse, and no single strategy has yet been proposed that provides optimal performance. The following survey examines several alternative approaches, and discusses the design tradeoffs involved when implementing a data prefetch strategy.",
                "id": "00293"
            },
            {
                "title": "An FPGA implementation of a Restricted Boltzmann Machine classifier using stochastic bit streams",
                "abstract": "Artificial neural networks (ANNs) usually require a very large number of computation nodes and can be implemented either in software or directly in hardware, such as FPGAs. Software-based approaches are offline and not suitable for real-time applications, but they support a large number of nodes. FPGA-based implementations, in contrast, can greatly speedup the computation time. However, resource limitations in an FPGA restrict the maximum number of computation nodes in hardware-based approaches. This work exploits stochastic bit streams to implement the Restricted Boltzmann Machine (RBM) handwritten digit recognition application completely on an FPGA. Exploiting this approach saves a large number of hardware resources making the FPGA-based implementation of large ANNs feasible.",
                "id": "00294"
            },
            {
                "title": "Enhancing the Memory Performance of Embedded Systems with the Flexible Sequential and Random Access Memory",
                "abstract": "The on-chip memory performance of embedded systems directly affects the system designers' decision about how to allocate expensive silicon area. We investigate a novel memory architecture, flexible sequential and random access memory (FSRAM), for embedded systems. To realize sequential accesses, small \"links\" are added to each row in the RAM array to point to the next row to be prefetched. The potential cache pollution is ameliorated by a small sequential access buffer (SAB). To evaluate the architecture-level performance of FSRAM, we run the Mediabench benchmark programs [1] on a modified version of the Simplescalar simulator [2]. Our results show that the FSRAM improves the performance of a baseline processor with a 16KB data cache up to 55%, with an average of 9%. We also designed RTL and SPICE models of the FSRAM [3], which show that the FSRAM significantly improves memory access time, while reducing power consumption, with negligible area overhead.",
                "id": "00295"
            },
            {
                "title": "Fault tolerance for nanotechnology devices at the bit and module levels with history index of correct computation.",
                "abstract": "Future nano-scale devices are expected to shrink to ever smaller dimensions, to operate at low voltages and high frequencies, to be more sensitive to environmental influences and to be characterised by high dynamic fault rates and defect densities. Fundamentally new fault-tolerant architectures are required in order to produce reliable systems that will operate correctly. Simple replication of mic...",
                "id": "00296"
            },
            {
                "title": "BloomFlash: Bloom Filter on Flash-Based Storage",
                "abstract": "The bloom filter is a probabilistic data structure that provides a compact representation of a set of elements. To keep false positive probabilities low, the size of the bloom filter must be dimensioned a priori to be linear in the maximum number of keys inserted, with the linearity constant ranging typically from one to few bytes. A bloom filter is most commonly used as an in memory data structure, hence its size is limited by the availability of RAM space on the machine. As datasets have grown over time to Internet scale, so have the RAM space requirements of bloom filters. If sufficient RAM space is not available, we advocate that flash memory may serve as a suitable medium for storing bloom filters, since it is about one-tenth the cost of RAM per GB while still providing access times orders of magnitude faster than hard disk. We present BLOOMFLASH, a bloom filter designed for flash memory based storage, that provides a new dimension of trade off with bloom filter access times to reduce RAM space usage (and hence system cost). The simple design of a single flat bloom filter on flash suffers from many performance bottlenecks, including in-place bit updates that are inefficient on flash and multiple reads and random writes spread out across many flash pages for a single lookup or insert operation. To mitigate these performance bottlenecks, BLOOMFLASH leverages two key design innovations: (i) buffering bit updates in RAM and applying them in bulk to flash that helps to reduce random writes to flash, and (ii) a hierarchical bloom filter design consisting of component bloom filters, stored one per flash page, that helps to localize reads and writes on flash. We use two real-world data traces taken from representative bloom filter applications to drive and evaluate our design. BLOOMFLASH achieves bloom filter access times in the range of few tens of microseconds, thus allowing up to order of tens of thousands operations per sec.",
                "id": "00297"
            },
            {
                "title": "A Distributed Hardware Mechanism for Process Synchronization on Shared-Bus Multiprocessors",
                "abstract": "Several techniques have been used to reduce the performance impact of process synchronization in fine-grained multiprocessor systems. These existing techniques tend to have long synchronization times or high shared-bus use, or they require complex and expensive hardware. A new technique is presented that uses distributed hardware locking queues to reduce both contention and latency to the minimum values that can be obtained using a shared-bus. This technique is shown to require at most two shared-bus transactions, with one transaction being typical. The latency for process continuation after obtaining a lock is reduced to near zero. Barrier synchronization using this distributed mechanism requires only one shared-bus transaction per processor involved in the barrier. This new technique is scalable and applicable to both new architectures and to existing systems, and is less complex than other hardware solutions.",
                "id": "00298"
            },
            {
                "title": "The superthreaded processor architecture",
                "abstract": "The common single-threaded execution model limits processors to exploiting only the relatively small amount of instruction-level parallelism that is available in application programs. The superthreaded processor, on the other hand, is a concurrent multithreaded architecture (CMA) that can exploit the multiple granularities of parallelism that are available in general-purpose application programs. Unlike other CMAs that rely primarily on hardware for run-time dependence detection and speculation, the superthreaded processor combines compiler-directed thread-level speculation of control and data dependences with run-time data dependence verification hardware. This hybrid of a superscalar processor and a multiprocessor-on-a-chip can utilize many of the existing compiler techniques used in traditional parallelizing compilers developed for multiprocessors. Additional unique compiler techniques, such as the conversion of data speculation into control speculation, are also introduced to generate the superthreaded code and to enhance the parallelism between threads. A detailed execution-driven simulator is used to evaluate the performance potential of this new architecture. It is found that a superthreaded processor can achieve good performance on complex application programs through this close coupling of compile-time and run-time information",
                "id": "00299"
            },
            {
                "title": "Combining hardware and software cache coherence strategies",
                "abstract": "Efficiently maintaining cache coherence is a major problem in large-scale shared memory mul- tiprocessors. Hardware directory schemes have very high memory requirements, while software-directed schemes must rely on imprecise compile-time memory disambiguation. Recently proposed dynamic directory schemes allocate pointers to blocks only as they are referenced, which significantly reduces their memory requirements, but they still allocate pointers to blocks that do not need them. We show how compiler mark- ing can further reduce the directory size by allocating pointers only when necessary. Using trace-driven simulations, we find that the performance of this new approach is comparable to other coherence schemes, but with significantly lower memory requirements.",
                "id": "002100"
            },
            {
                "title": "Independent Component Analysis and Evolutionary Algorithms for Building Representative Benchmark Subsets",
                "abstract": "This work addresses the problem of building representative subsets of benchmarks from an original large set of benchmarks, using statistical analysis techniques. The subsets should be developed in this way to include only the necessary information for evaluating the performance of a computer system or application. The development of representative workloads is not a trivial procedure, since incorrectly selecting benchmarks the representative subset can produce erroneous results. A number of statistical analysis techniques have been developed for identifying representative workloads. The goal of these approaches is to reduce the dimensionality of the original set of benchmarks prior to identifying similar benchmarks. In this work we propose a combination of Independent Component Analysis (ICA) and Evolutionary Algorithm (EA) as a more efficient way for reducing the computational complexity of the problem and the redundant information of the original set of benchmarks. Experimental results validate that the proposed technique generates more representative workloads than prior techniques.",
                "id": "002101"
            },
            {
                "title": "Using ECN Marks to Improve TCP Performance over Lossy Links",
                "abstract": "TCP was designed for wireline networks, where loss events are mostly caused by network congestion. The congestion control mechanism of current TCP uses loss events as the indicator of congestion, and reduces its congestion window size. However, when a lossy link is involved in a TCP connection, non-congestion random losses should also be considered. The congestion window size should not be decreased if a loss event is caused by link corruptions. To improve TCP performance over lossy links, in this paper, we first present that zero congestion loss could be achieved by appropriately setting the ECN marking threshold in the RED buffer. Secondly, we propose a new TCP algorithm, called Differentiation Capable TCP (Diff-C-TCP). Diff-C-TCP makes an assumption that packet losses are caused by link corruptions, and uses ECN (Explicit Congestion Notification) to determine any loss that may occasionally happen due to network congestion. We have shown that Diff-C-TCP performs very well in the presence of a lossy link.",
                "id": "002102"
            },
            {
                "title": "Comparing the performance of stochastic simulation on GPUs and OpenMP",
                "abstract": "Since stochastic computing performs operations using streams of bits that represent probability values instead of deterministic values, it can tolerate a large number of failures in a noisy system. However, the simulation of a stochastic implementation is extremely time-consuming. In this paper, we investigate two approaches to speed up the stochastic simulation: a GPU-based simulation and an OpenMP-based simulation. To compare these two approaches, we start with several basic stochastic computing elements SCEs and then use the stochastic implementation of a frame difference-based image segmentation algorithm as case study to conduct extensive experiments. Measured results show that the GPU-based simulation with 448 processing elements can achieve up to 119x performance speedup compared to the single-threaded CPU simulation and 17x performance speedup over the OpenMP-based simulation with eight processor cores. In addition, we present several performance optimisations for the GPU-based simulation which significantly benefit the performance of stochastic simulation.",
                "id": "002103"
            },
            {
                "title": "Tier-Code: An XOR-Based RAID-6 Code with Improved Write and Degraded-Mode Read Performance",
                "abstract": "The RAID-6 configuration is more tolerant of disk failures than other RAID levels because of its ability to tolerate two disk failures. However, previous RAID-6 codes suffer from two major overheads - the time of encoding or decoding processes plus the need to access multiple blocks when updating parities or recovering failed blocks. For example, the PS and Reed-Solomon codes do not have optimal computation complexity, while P-code, X-code and RDP-code must access multiple blocks to update parities during write operations. This work proposes a new XOR- based RAID-6 code, called Tier-code, which not only achieves the optimal parity computation complexity, but also increases the write and degraded-mode read performance compared to previous codes. It uses two tiers of coding, one at the block level and the other at the chunk level. Experimental results of software testing, simulation and ASIC synthesis for this new hierarchical code demonstrate that Tier-code can outperform the previous RAID-6 codes in both write performance and degraded-mode read performance while maintaining the optimal computation complexity in both hardware and software implementations.",
                "id": "002104"
            },
            {
                "title": "Dynamically Adapting To System Load And Program Behavior In Multiprogrammed Multiprocessor Systems",
                "abstract": "Parallel execution of application programs on a multiprocessor system may lead to performance degradation if the workload of a parallel region is not large enough to amortize the overheads associated with the parallel execution. Furthermore, if too many processes are running on the system in a multiprogrammed environment, the performance of the parallel application may degrade due to resource contention. This work proposes a comprehensive dynamic processor allocation scheme that takes both program behavior and system load into consideration when dynamically allocating processors. This mechanism was implemented on the Solaris operating system to dynamically control the execution of parallel C and Java application programs. Performance results show the effectiveness of this scheme in dynamically adapting to the current execution environment and program behavior, and that it outperforms a conventional time-shared system. Copyright (C) 2002 John Wiley Sons, Ltd.",
                "id": "002105"
            },
            {
                "title": "Circulating shared-registers for multiprocessor systems",
                "abstract": "The techniques for fine-grained data sharing are generally available only on specialized architectures, usually involving a shared-bus. The CIRculating Common-Update Sharing (CIRCUS) mechanism has low latency user-level contention-free access to a set of shared circulating data registers. The local access latency is near zero for both read and write operations. These operations can be mapped into more complex operations, such as arithmetic, logical, or data reduction operations such as minimum or sum to be performed by the circulating register hardware (CRH) on the circulating copy of a register. The CRH can also be used to perform atomic operations, such as fetch&add or swap. For a two-dimensional hierarchy of N processing elements (PEs), the write-latency (until the circulating register is updated with a new value) and the update-latency (when all CRH modules can see the updated value) have an optimum cluster size proportional to (N \u010b I/D)1/2, where I is the intercluster time and D is the inter-PE time, including the time between and through one node. The latencies, when optimally clustered, are proportional to (N \u010b I \u010b D)1/2. Sub-microsecond write-latency is expected for up to 15,255 PEs or 660 workstations. For higher levels of hierarchy, the expected write-latency is shown to be proportional to the sum of the latencies of all loop hierarchies. CIRCUS is applicable to a wide variety of system architectures and topologies.",
                "id": "002106"
            },
            {
                "title": "Self-Adjusting Scheduling: An On-Line Optimization Technique for Locality Management and Load Balancing",
                "abstract": "Techniques for scheduling parallel tasks on to the processors of a multiprocessor architecture must tradeoff three interrelated factors: 1) scheduling and synchronization costs, 2) load balancing, and 3) memory locality. Current scheduling techniques typically consider only one or two of these three factors at a time. We propose a novel Self- Adjusting Scheduling (SAS) algorithm that addresses all three factors simultaneously. This algorithm dedicates a single processor to execute an on-line branch-and-bound algorithm to search for partial schedules concurrent with the execution of tasks previously assigned to the remaining processors. This overlapped scheduling and execution, along with self-adjustment of duration of partial scheduling periods reduces scheduling and synchronization costs significantly. To satisfy the load-balancing and locality management, SAS introduces a unified cost model that accounts for both of these factors simultaneously. We compare the simulated performance of SAS with the Affinity Scheduling algorithm (AFS). The results of our experiments demonstrate that the potential loss of performance caused by dedicating a processor to scheduling is outweighed by the higher performance produced by SAS's dynamically adjusted schedules, even in systems with a small number of processors. SAS is a general on-line optimization technique that can be applied to a variety of dynamic scheduling problems.",
                "id": "002107"
            },
            {
                "title": "A Comprehensive Dynamic Processor Allocation Scheme for Multiprogrammed Multiprocessor Systems",
                "abstract": "Parallel execution of application programs on a multiprocessor system may lead to performance degradation if the workload of a parallel region is not large enough to amortize the overheads associated with the parallel execution. Furthermore, if too many processes are running on the system in a multiprogrammed environment, the performance of the parallel application may degrade due to resource contention. We propose a comprehensive dynamic processor allocation scheme that takes both program behavior and system load into consideration when dynamically allocating processors. We implemented this mechanism in the Java run-time system on Solaris to dynamically control the execution of parallel Java application programs. Performance results show the effectiveness of this scheme in dynamically adapting to the current execution environment and that it outperforms a conventional time-shared system.",
                "id": "002108"
            },
            {
                "title": "High performance solid state storage under Linux",
                "abstract": "Solid state drives (SSDs) allow single-drive performance that is far greater than disks can produce. Their low latency and potential for parallel operations mean that they are able to read and write data at speeds that strain operating system I/O interfaces. Additionally, their performance characteristics expose gaps in existing benchmarking methodologies. We discuss the impact on Linux system design of a prototype PCI Express SSD that operates at least an order of magnitude faster than most drives available today. We develop benchmarking strategies and focus on several areas where current Linux systems need improvement, and suggest methods of taking full advantage of such high-performance solid state storage. We demonstrate that an SSD can perform with high throughput, high operation rates, and low latency under the most difficult conditions. This suggests that high-performance SSDs can dramatically improve parallel I/O performance for future high performance computing (HPC) systems.",
                "id": "002109"
            },
            {
                "title": "Simulation of computer architectures: simulators, benchmarks, methodologies, and recommendations",
                "abstract": "Simulators have become an integral part of the computer architecture research and design process. Since they have the advantages of cost, time, and flexibility, architects use them to guide design space exploration and to quantify the efficacy of an enhancement. However, long simulation times and poor accuracy limit their effectiveness. To reduce the simulation time, architects have proposed several techniques that increase the simulation speed or throughput. To increase the accuracy, architects try to minimize the amount of error in their simulators and have proposed adding statistical rigor to their simulation methodology. Since a wide range of approaches exist and since many of them overlap, this paper describes, classifies, and compares them to aid the computer architect in selecting the most appropriate one.",
                "id": "002110"
            },
            {
                "title": "Dynamic Scheduling Techniques For Heterogeneous Computing Systems",
                "abstract": "There has been a recent increase of interest in heterogeneous computing systems, due partly to the fact that a single parallel architecture may not be adequate for exploiting all of a program's available parallelism. In some eases, heterogeneous systems have been shown to produce higher performance for lower cost than a single large machine. However, there has been only limited work on developing techniques and frameworks for partitioning and scheduling applications across the components of a heterogeneous system. In this paper we propose a general model for describing and evaluating heterogeneous systems that considers the degree of uniformity in the processing elements and the communication channels as a measure of the heterogeneity in the system. We also propose a class of dynamic scheduling algorithms for a heterogeneous computing system interconnected with an arbitrary communication network. These algorithms execute a novel optimization technique to dynamically compute schedules based on the potentially non-uniform computation and communication costs on the processors of a heterogeneous system. A unique aspect of these algorithms is that they easily adapt to different task granularities, to dynamically varying processor and system loads, and to systems with varying degrees of heterogeneity. Our simulations are designed to facilitate the evaluation of different scheduling algorithms under varying degrees of heterogeneity. The results show improved performance for our algorithms compared to the performance resulting from existing scheduling techniques.",
                "id": "002111"
            },
            {
                "title": "JavaSpMT: A Speculative Thread Pipelining Parallelization Model for Java Programs",
                "abstract": "This paper presents a new approach to improve performance of Java programs by extending the superthreaded speculative execution model to exploit coarse-grained parallelism on a shared-memory multiprocessor system. The parallelization model, called Java Speculative MultiThreading (JavaSpMT), combines control speculation with run-time dependence checking to parallelize a wide variety of loop constructs, including do-while loops, that cannot be parallelized using standard parallelization techniques. JavaSpMT is implemented using the standard Java multithreading mechanism and the parallelization is expressed using a Java source-to-source transformation. Thus, the transformed programs are still portable to any shared-memory multiprocessor system with a Java Virtual Machine implementation that supports native threads.",
                "id": "002112"
            },
            {
                "title": "Efficient Execution of Parallel Applications in Multiprogrammed Multiprocessor Systems",
                "abstract": "Existing techniques for sharing the processing resources in multiprogrammed shared-memory multiprocessors, such as time-sharing, space-sharing, and gang-scheduling, typically sacrifice the performance of individual parallel applications to improve overall system utilization. We present a new processor allocation technique that dynamically adjusts the number of processors an application is allowed to use for the execution of each parallel section of code based on the current system load. This approach exploits the maximum parallelism possible for each application without overloading the system. We implement our scheme on a Silicon Graphics Challenge multiprocessor system and evaluate its performance using applications from the Perfect Club benchmark suite and synthetic benchmarks. Our approach shows significant improvements over traditional time-sharing and gang scheduling. It has performance comparable to, or slightly better than, static space-sharing, but our strategy is more robust since, unlike static space-sharing, it does not require a priori knowledge of the applications' parallelism characteristics.",
                "id": "002113"
            },
            {
                "title": "Balancing Reuse Opportunities and Performance Gains with Subblock Value Reuse",
                "abstract": "The fact that instructions in programs often produce repetitive results has motivated researchers to explore various techniques, such as value prediction and value reuse, to exploit this behavior. Value prediction improves the available Instruction-Level Parallelism (ILP) in superscalar processors by allowing dependent instructions to be executed speculatively after predicting the values of their input operands. Value reuse, on the other hand, tries to eliminate redundant computation by storing the previously produced results of instructions and skipping the execution of redundant instructions. Previous value reuse mechanisms use a single instruction or a naturally formed instruction group, such as a basic block, a trace, or a function, as the reuse unit. These naturally-formed instruction groups are readily identifiable by the hardware at runtime without compiler assistance. However, the performance potential of a value reuse mechanism depends on its reuse detection time, the number of reuse opportunities, and the amount of work saved by skipping each reuse unit. Since larger instruction groups typically have fewer reuse opportunities than smaller groups, but they provide greater benefit for each reuse-detection process, it is very important to find the balance point that provides the largest overall performance gain. In this paper, we propose a new mechanism called subblock reuse. Subblocks are created by slicing basic blocks either dynamically or with compiler guidance. The dynamic approaches use the number of instructions, numbers of inputs and outputs, or the presence of store instructions to determine the subblock boundaries. The compiler-assisted approach slices basic blocks using data-flow considerations to balance the reuse granularity and the number of reuse opportunities. The results show that subblocks, which can produce up to 36 percent speedup if reused properly, are better candidates for reuse units than basic blocks. Although subblock reuse with compiler assistance has a substantial and consistent potential to improve the performance of superscalar processors, this scheme is not always the best performer. Subblocks restricted to two consecutive instructions demonstrate surprisingly good performance potential as well.",
                "id": "002114"
            },
            {
                "title": "Extending value reuse to basic blocks with compiler support",
                "abstract": "Speculative execution and instruction reuse are two important strategies that have been investigated for improving processor performance. Value prediction at the instruction level has been introduced to allow even more aggressive speculation and reuse than previous techniques. This study suggests that using compiler support to extend value reuse to a coarser granularity than a single instruction, such as a basic block, may have substantial performance benefits. We investigate the input and output values of basic blocks and find that these values can be quite regular and predictable. For the SPEC benchmark programs evaluated, 90 percent of the basic blocks have fewer than four register inputs, five live register outputs, four memory inputs, and two memory outputs. About 16 to 41 percent of all the basic blocks are simply repeating earlier calculations when the programs are compiled with the -O2 optimization level in the GCC compiler. Compiler optimizations, such as loop-unrolling and function inlining, affect the sizes of basic blocks, but have no significant or consistent impact on their value locality, nor the resulting performance. Based on these results, we evaluate the potential benefit of basic block reuse using a novel mechanism called the block history buffer. This mechanism records input and live output values of basic blocks to provide value reuse at the basic block level. Simulation results show that using a reasonably sized block history buffer to provide basic block reuse in a 4-way issue superscalar processor can improve execution time for the tested SPEC programs by 1 to 14 percent, with an overall average of 9 percent when using reasonable hardware assumptions",
                "id": "002115"
            },
            {
                "title": "Evaluating Benchmark Subsetting Approaches",
                "abstract": "To reduce the simulation time to a tractable amount or due to compilation (or other related) problems, computer architects often simulate only a subset of the benchmarks in a benchmark suite. However, if the architect chooses a subset of benchmarks that is not representative, the subsequent simulation results will, at best, be misleading or, at worst, yield incorrect conclusions. To address this problem, computer architects have recently proposed several statistically-based approaches to subset a benchmark suite. While some of these approaches are well-grounded statistically, what has not yet been thoroughly evaluated is the: 1) Absolute accuracy, 2) Relative accuracy across a range of processor and memory subsystem enhancements, and 3) Representativeness and coverage of each approach for a range of subset sizes. Specifically, this paper evaluates statistically-based subsetting approaches based on principal components analysis (PCA) and the Plackett and Burman (P&B) design, in addition to prevailing approaches such as integer vs. floating-point, core vs. memory-bound, by language, and at random. Our results show that the two statistically-based approaches, PCA and P&B, have the best absolute and relative accuracy for CRI and energy-delay product (EDP), produce subsets that are the most representative, and choose benchmark and input set pairs that are most well-distributed across the benchmark space. To achieve a 5% absolute CPI and EDP error, across a wide range of configurations, PCA and P&B typically need about 17 benchmark and input set pairs, while the other five approaches often choose more than 30 benchmark and input set pairs.",
                "id": "002116"
            },
            {
                "title": "An active data-aware cache consistency protocol for highly-scalable data-shipping DBMS architectures",
                "abstract": "In a data-shipping database system, data items are retrieved from the server machines, cached and processed at the client machines, and then shipped back to the server. Current cache consistency approaches typically rely on a centralized server or servers to enforce the necessary concurrency control actions. This centralized server imposes a limitation on the scalability and performance of these systems. This paper presents a new consistency protocol, Active Data-aware Cache Consistency (ADCC), that allows clients to be aware of the global state of their cached data via a two-tier directory. Using parallel communication with simultaneous client-server and client-client messages, ADCC reduces the network latency for detecting data conflicts by 50%, while increasing message overhead by about 8% only. In addition, ADCC improves scalability by partially offloading the concurrency control function from the server to the clients. An optimization, Lazy Update, is introduced to reduce the message overhead for maintaining client directory consistency. We implement ADCC in a page server DBMS architecture and compare it with the leading cache consistency algorithm, Callback Locking (CBL), which is the most widely implemented algorithm in commercial DBMSs. Our performance study shows that ADCC has a similar or lower abort rate, higher throughput, and better scalability for important workloads and system configurations. Both the simulation results and the analytic study indicate that the message overhead is low and that ADCC produces better behavior compared to the traditional server-based communication under high contention workloads.",
                "id": "002117"
            },
            {
                "title": "Power and Area Efficient Sorting Networks Using Unary Processing",
                "abstract": "Sorting is a common task in a wide range of applications from signal and image processing to switching systems. For applications that require high performance, sorting is often performed in hardware. Hardware cost and power consumption are the dominant concerns. The usual approach is to wire up a network of compare-and-swap units in a configuration called a Batcher (or Bitonic) network. This paper proposes a novel area-and power-efficient approach to sorting networks based on \"unary processing.\" Data is encoded as serial bit-streams, with values represented by the fraction of 1's in a stream of 0's and 1's. (This is an evolution of prior work on stochastic logic. Unlike stochastic logic, the unary approach is deterministic and completely accurate.) Synthesis results of complete sorting networks show up to 87% area and power saving compared to the conventional binary implementations. However, the latency increases. To mitigate the increased latency, the paper uses a novel time-encoding of data. The approach is validated with implementation of an important application of sorting: median filtering. The result is a low-cost, energy-efficient implementation of median filtering with only a slight accuracy loss.",
                "id": "002118"
            },
            {
                "title": "A hardware implementation of a radial basis function neural network using stochastic logic",
                "abstract": "Hardware implementations of artificial neural networks typically require significant amounts of hardware resources. This paper proposes a novel radial basis function artificial neural network using stochastic computing elements, which greatly reduces the required hardware. The Gaussian function used for the radial basis function is implemented with a two-dimensional finite state machine. The norm between the input data and the center point is optimized using simple logic gates. Results from two pattern recognition case studies, the standard Iris flower and the MICR font benchmarks, show that the difference of the average mean squared error between the proposed stochastic network and the corresponding traditional deterministic network is only 1.3% when the stochastic stream length is 10kbits. The accuracy of the recognition rate varies depending on the stream length, which gives the designer tremendous flexibility to tradeoff speed, power, and accuracy. From the FPGA implementation results, the hardware resource requirement of the proposed stochastic hidden neuron is only a few percent of the hardware requirement of the corresponding deterministic hidden neuron. The proposed stochastic network can be expanded to larger scale networks for complex tasks with simple hardware architectures.",
                "id": "002119"
            },
            {
                "title": "The synthesis of combinational logic to generate probabilities",
                "abstract": "As CMOS devices are scaled down into the nanometer regime, concerns about reliability are mounting. Instead of viewing nano-scale characteristics as an impediment, technologies such as PCMOS exploit them as a source of randomness. The technology generates random numbers that are used in probabilistic algorithms. With the PCMOS approach, different voltage levels are used to generate different probability values. If many different probability values are required, this approach becomes prohibitively expensive. In this work, we demonstrate a novel technique for synthesizing logic that generates new probabilities from a given set of probabilities. Three different scenarios are considered in terms of whether the given probabilities can be duplicated and whether there is freedom to choose them. In the case that the given probabilities cannot be duplicated and are predetermined, we provide a solution that is FPGA-mappable. In the case that the given probabilities cannot be duplicated but can be freely chosen, we provide an optimal choice. In the case that the given probabilities can be duplicated and can be freely chosen, we demonstrate how to generate arbitrary decimal probabilities from small sets -- a single probability or a pair of probabilities -- through combinational logic.",
                "id": "002120"
            },
            {
                "title": "PASS: A Hybrid Storage System for Performance-Synchronization Tradeoffs Using SSDs",
                "abstract": "Recent advances in flash memory show great potential to replace traditional hard drives (HDDs) with flash-based solid state drives (SSDs) from personal computing to distributed systems. However, it is still a long way to go before completely using SSDs for enterprise data storage. Considering the cost, performance, and reliability of SSDs, a practical solution is to combine both SSDs and HDDs together. This paper proposes a hybrid storage system named PASS (Performance-dAta Synchronization - hybrid storage System) to tradeoff between I/O performance and data discrepancy between SSDs and HDDs. PASS includes a high-performance SSD and a traditional HDD to store mirrored data for reliability. All of the I/O requests are redirected to the primary SSD first and then the updated data blocks are copied to the backup HDD asynchronously. In order to hide the latency of copying operations, we use an I/O window to coalesce write requests and maintain an ordered I/O queue to shorten the HDD seek and rotation times. Depending on the charateristics of different I/O workloads, we develop an adaptive policy to dynamically balance the foreground I/O processing and background mirroring. We implement a prototype system of PASS by developing a Linux device driver and conduct experiments on the IoMeter, PostMark, and TPCC benchmarks. Our results show that PASS can achieve up to 12 times the performance of a RAID1 storage system for the IoMeter and PostMark workloads while tolerating less than 2% data discrepancy between the primary SSD and the backup HDD. More interestingly, while PASS does not produce any performance benefit for the TPC-C benchmark, it does allow the system to scale to larger sizes than when using an HDD-based RAID system alone.",
                "id": "002121"
            },
            {
                "title": "Archer: A Community Distributed Computing Infrastructure for Computer Architecture Research and Education.",
                "abstract": "This paper introduces Archer, a community-based computing infrastructure supporting computer architecture research and education. The Archer system builds on virtualization techniques to provide a collaborative environment that facilitates sharing of computational resources and data among users. It integrates batch scheduling middleware to deliver high-throughput computing services aggregated from resources distributed across wide-area networks and owned by different participating entities in a seamless manner. The paper discusses the motivations that have led to the design of Archer, describes its core middleware components, and presents an analysis of the functionality and performance of the first wide-area deployment of Archer running a representative computer architecture simulation workload.",
                "id": "002122"
            },
            {
                "title": "Deterministic methods for stochastic computing using low-discrepancy sequences",
                "abstract": "Recently, deterministic approaches to stochastic computing (SC) have been proposed. These compute with the same constructs as stochastic computing but operate on deterministic bit streams. These approaches reduce the area, greatly reduce the latency (by an exponential factor), and produce completely accurate results. However, these methods do not scale well. Also, they lack the property of progressive precision enjoyed by SC. As a result, these deterministic approaches are not competitive for applications where some degree of inaccuracy can be tolerated. In this work we introduce two fast-converging, scalable deterministic approaches to SC based on low-discrepancy sequences. The results are completely accurate when running the operations for the required number of cycles. However, the computation can be truncated early if some inaccuracy is acceptable. Experimental results show that the proposed approaches significantly improve both the processing time and area-delay product compared to prior approaches.\n\n",
                "id": "002123"
            },
            {
                "title": "Exploring sub-block value reuse for superscalar processors",
                "abstract": "The performance potential of a value reuse mechanism depends on its reuse detection time, the number of reuse opportunities, and the amount of work saved by skipping each reuse unit. Since larger instruction groups typically have fewer reuse opportunities than smaller groups, but also provide greater benefit for each reuse-detection process, it is very important to find the balance point that provides the largest overall performance gain. We propose a new mechanism called sub-block reuse to balance the reuse granularity and the number of reuse opportunities. Our simulation results show that sub-block reuse with compiler assistance has a substantial and consistent potential to improve the performance of superscalar processors, with speedups ranging from 10% to 22%",
                "id": "002124"
            },
            {
                "title": "When Caches Aren't Enough: Data Prefetching Techniques",
                "abstract": "For the past few years, CPU performance has outpaced that of dynamic RAM, the primary component of main memory. Developers have had to use increasingly aggressive techniques to reduce or hide delays in accessing main memory. Even so, it is still not uncommon for scientific programs to spend more than half their runtimes stalled on memory requests. This poor performance is partially a result of the policies used to fetch data from main memory: Processors typically request data only when it is needed and then only if it is not first found in the cache. In contrast, data prefetching calls data into the cache before the processor needs it. Ideally, prefetching completes just in time for the processor to access the needed data. Prefetching can nearly double the performance of some scientific applications running on commercial systems. But to achieve this performance, it is critical that the most suitable prefetching technique is used. This article reviews three popular prefetching techniques and examines in which situations they are best used.",
                "id": "002125"
            },
            {
                "title": "Accelerating Lattice Boltzmann Fluid Flow Simulations Using Graphics Processors",
                "abstract": "Lattice Boltzmann Methods (LBM) are used for the computational simulation of Newtonian fluid dynamics. LBM-based simulations are readily parallelizable; they have been implemented on general-purpose processors, field-programmable gate arrays (FPGAs), and graphics processing units (GPUs). Of the three methods, the GPU implementations achieved the highest simulation performance per chip. With memory bandwidth of up to 141 GB/s and a theoretical maximum floating point performance of over 600 GFLOPS, CUDA-ready GPUs from NVIDIA provide an attractive platform for a wide range of scientific simulations, including LBM. This paper improves upon prior single-precision GPU LBM results for the D3Q19 model by increasing GPU multiprocessor occupancy, resulting in an increase in maximum performance by 20%, and by introducing a space-efficient storage method which reduces GPU RAM requirements by 50% at a slight detriment to performance. Both GPU implementations are over 28 times faster than a single-precision quad-core CPU version utilizing OpenMP.",
                "id": "002126"
            },
            {
                "title": "Scaling Analytical Models for Soft Error Rate Estimation Under a Multiple-Fault Environment",
                "abstract": "With continuing increase in soft error rates, its foreseeable that multiple faults will eventually need to be considered when modeling circuit sensitivity and evaluating faulttolerance techniques. Previous work that considers multiple faults assumes the faults are permanent. These assumptions aren't directly valid for soft errors. In this work, we evaluate two currently available models for analyzing circuit sensitivities but subject them to multiple transient fault environments. The first model targets the application of triple modular redundancy (TMR) to a non-branching (no fan-out) circuit with permanent faults. We demonstrate this model's inability to adequately predict sensitivity when circuits with branching are considered and subjected to transient faults. We motivate the need for a model that captures logical masking. This is provided by modifying a soft error rate (SER) estimation algorithm to handle multiple faults and gate input interdependence. We conclude that the simple non-branching model can predict a trade-off threshold for when TMR can benefit a circuit. However accurately predicting the magnitude of reliability changes requires the inclusion of more complicated branching effects and logical masking.",
                "id": "002127"
            }
        ]
    }
]