{
    "002": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Temperature-aware floorplanning of microarchitecture blocks with IPC-power dependence modeling and transient analysis':",
        "Document: \"Scaling Analytical Models for Soft Error Rate Estimation Under a Multiple-Fault Environment. With continuing increase in soft error rates, its foreseeable that multiple faults will eventually need to be considered when modeling circuit sensitivity and evaluating faulttolerance techniques. Previous work that considers multiple faults assumes the faults are permanent. These assumptions aren't directly valid for soft errors. In this work, we evaluate two currently available models for analyzing circuit sensitivities but subject them to multiple transient fault environments. The first model targets the application of triple modular redundancy (TMR) to a non-branching (no fan-out) circuit with permanent faults. We demonstrate this model's inability to adequately predict sensitivity when circuits with branching are considered and subjected to transient faults. We motivate the need for a model that captures logical masking. This is provided by modifying a soft error rate (SER) estimation algorithm to handle multiple faults and gate input interdependence. We conclude that the simple non-branching model can predict a trade-off threshold for when TMR can benefit a circuit. However accurately predicting the magnitude of reliability changes requires the inclusion of more complicated branching effects and logical masking.\"",
        "Document: \"Accelerating Lattice Boltzmann Fluid Flow Simulations Using Graphics Processors. Lattice Boltzmann Methods (LBM) are used for the computational simulation of Newtonian fluid dynamics. LBM-based simulations are readily parallelizable; they have been implemented on general-purpose processors, field-programmable gate arrays (FPGAs), and graphics processing units (GPUs). Of the three methods, the GPU implementations achieved the highest simulation performance per chip. With memory bandwidth of up to 141 GB/s and a theoretical maximum floating point performance of over 600 GFLOPS, CUDA-ready GPUs from NVIDIA provide an attractive platform for a wide range of scientific simulations, including LBM. This paper improves upon prior single-precision GPU LBM results for the D3Q19 model by increasing GPU multiprocessor occupancy, resulting in an increase in maximum performance by 20%, and by introducing a space-efficient storage method which reduces GPU RAM requirements by 50% at a slight detriment to performance. Both GPU implementations are over 28 times faster than a single-precision quad-core CPU version utilizing OpenMP.\"",
        "Document: \"Comparing processor allocation strategies in multiprogrammed shared-memory multiprocessors. Small-scale shared-memory multiprocessors are commonly used in a workgroup environment where multiple applications, both parallel and sequential, are executed concurrently while sharing the processors and other system resources. To utilize the processors efficiently, an effective allocation strategy is required. In this paper, we use performance data obtained from an SGI multiprocessor to evaluate several processor allocation strategies when running two parallel programs simultaneously. We examine gang scheduling (coscheduling), static space-sharing (space partitioning), and a dynamic allocation scheme called loop-level process control (LLPC) with three different dynamic allocation heuristics. We use regression analysis to quantify the measured data and thereby explore the relationship between the degree of parallelism of the application, specific system parameters (such as the size of the system), the processor allocation strategy, and the resulting performance. This study shows that dynamically partitioning the system using LLPC or similar heuristics provides better performance for applications with a high degree of parallelism than either gang scheduling or static space-sharing.\"",
        "Document: \"Performance-based path determination for interprocessor communication in distributed computing systems. The different types of messages used by a parallel application program executing in a distributed computing system can each have unique characteristics so that no single communication network can produce the lowest latency for all messages. For instance, short control messages may be sent with the lowest overhead on one type of network, such as Ethernet, while bulk data transfers may be better suited to a different type of network, such as Fibre Channel or HIPPI. This work investigates how to exploit multiple heterogeneous communication networks that interconnect the same set of processing nodes using a set of techniques we call performance-based path determination (PBPD). The performance-based path selection (PBPS) technique selects the best (lowest latency) network among several for each individual message to reduce the communication overhead of parallel programs. The performance-based path aggregation (PBPA) technique, on the other hand, aggregates multiple networks into a single virtual network to increase the available bandwidth. We test the PBPD techniques on a cluster of SGI multiprocessors interconnected with Ethernet, Fibre Channel, and HiPPI networks using a custom communication library built on top of the TCP/IP protocol layers. We find that PBPS can reduce communication overhead in applications compared to using either network alone, while aggregating networks into a single virtual network can reduce communication latency for bandwidth-limited applications. The performance of the PBPD techniques depends on the mix of message sizes in the application program and the relative overheads of the networks, as demonstrated in our analytical models\"",
        "Document: \"Partitioning Tasks Between A Pair Of Interconnected Heterogeneous Processors - A Case-Study. With the variety of computer architectures available today, it is often difficult to determine which particular type of architecture will provide the best performance on a given application program, In fact, one type of architecture may be well suited to executing one section of a program while another architecture may be better suited to executing another section of the same program, One potentially promising approach for exploiting the best features of different computer architectures is to partition an application program to simultaneously execute on two or more types of machines interconnected with a high-speed communication network, A fundamental difficulty with this heterogeneous computing, however, is the problem of determining how to partition the application program across the interconnected machines, The goal of this paper is to show how a programmer or a compiler can use a model of a heterogeneous system to determine the machine on which each subtask should be executed. This technique is illustrated with a simple model that relates the relative performance of two heterogeneous machines to the communication time required to transfer partial results across their interconnection network, Experiments with a Connection Machine CM-200 demonstrate how to apply this model to partition two different application programs across the sequential front-end processor and the parallel back-end array.\"",
        "1 is \"A survey of power estimation techniques in VLSI circuits\", 2 is \"Terrestrial cosmic ray intensities\"",
        "Given above information, for an author who has written the paper with the title \"Temperature-aware floorplanning of microarchitecture blocks with IPC-power dependence modeling and transient analysis\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "0067": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A web-based p systems simulator and its parallelization':",
        "Document: \"Earth Observation Data Processing In Distributed Systems. Earth observation systems have a continuous growth in the user and internal requirements that can be handled nowadays only using distributed systems. These requirements are shortly reviewed in this paper. Huge data-sets management and processing are of special interest, as well as the particularities of the Earth observation data. On the technological side, the focus is put on service-oriented architectures that are facilitating the linkage of data or resources and processing. As proof of concept of current distributed system capabilities, the technological solutions used to build a training platform for Earth observation data processing are exposed and discussed in details.\"",
        "Document: \"PVMaple: A Distributed Approach to Cooperative Work of Maple Processes. We study the issue of interconnecting computer algebra system Maple and the message passing environment PVM. A prototype system, namely PVMaple, is presented. The system allows to create concurrent tasks and have them executed by Maple kernels running on different machines of a network.\"",
        "Document: \"Remote Control for Graphic Applications. The MathLang project aims at computerizing mathematical texts according to various degrees of formalisations, and without any prior commitment to a particular logical framework (e.g., having to choose either set theory or category theory or type theory, ...\"",
        "Document: \"Experiences in building an event-driven and deployable platform as a service. Conceived to expose remote infrastructures and services to application developers, most of the current Platform-as-a-services are based on proprietary technologies. While several initiatives for building open-source platforms were started only recently, the developers' positive feedback is already reflected in substantial contributions. A such new platform, deployable in private and public Clouds, is presented shortly and details are provided for its event-driven approach and support for web applications.\"",
        "Document: \"Multi-cloud resource management: cloud service interfacing. Cloud service abstractions are currently used to hide the underlying complexity given by existing technologies and services, in hope of facilitating the enacting of Cloud Federations and Marketplaces. In particular, resource management systems dealing with multiple Cloud providers need to expose an uniform interface for various services and to build wrappers for the Cloud service APIs. In this paper we discuss the solution adopted by a recent developed open-source and vendor agnostic platform-as-a-service for Multi-Cloud application deployment. The middleware includes a multi-agent system for automatic Cloud resource management. With a modular design, the solution provides a flexible approach to encompass new Cloud service offers as well as new resource types. This paper focuses on the modules which enable resource abstraction and automatized management.\"",
        "1 is \"CloudCmp: comparing public cloud providers\", 2 is \"Correct metric semantics for a language inspired by DNA computing\"",
        "Given above information, for an author who has written the paper with the title \"A web-based p systems simulator and its parallelization\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "0071": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Optimal dimension-exchange token distribution on complete binary trees':",
        "Document: \"Static Members and Cycles in Java Software. The static modifier is a convenient way to make class members \"global\" in object-oriented software systems. Given this, we wondered if static members significantly contribute to the long dependency cycles among the classes that we observed in a previous empirical study of Java software. In this paper, we examine 81 open source Java applications. We find empirical evidence that classes that declare a non-private static field or method that is accessed from within another class are likely to be involved in dependency cycles.\"",
        "Document: \"A Java Reuse Repository for Eclipse using LSI. Software Reuse is a concept that is frequently mentioned as a way to improve software developers' productivity. However, there are a number of issues that need to be addressed in order for software reuse to be adopted by developers. One of those issues is providing enough reusable artifacts. The Java Standard API has been quite successful in this, with the latest version having over 3000 classes available. However this raises the issue of finding the right artifact to reuse. With the Java API, this means trawling through the JavaDoc webpages, which has the risk of not being able to find the right artifact, even though it is in the API. In this paper, we explore the use of latent semantic indexing as a means to index the Java API JavaDoc pages. Specifically, we describe Prophecy, an Eclipse plug-in that presents the Java API as a software repository.\"",
        "Document: \"Delegation diagrams: visual support for the development of object-oriented designs. Developers have long used pictures to aid design activities and there has been a lot of interest in standard notations for design. We have developed delegation diagrams, a graphical notation that provides visual support for developing object-oriented designs and that makes the relationship between the requirements and the design explicit. We describe both the notation and tool support, and evaluate delegation diagrams using the cognitive dimensions of notations framework.\"",
        "Document: \"Neurophysiological Impact of Software Design Processes on Software Developers. Software development often leads to failed implementations resulting from several factors related to individual reactions to software design. Some design metrics give software developers guidelines and heuristics for use in software design. Furthermore, many metrics have been created to measure outcomes in terms of \u201ccode quality.\u201d However, these guidelines and metrics have been shown only to have a weak relationship and are poorly implemented. This study takes a new approach using tools from cognitive neuroscience to examine the cognitive load and arousal level placed on software engineers while working with different software designs. Specifically, we use electroencephalography (EEG) and skin conductance (SCR) to examine cognitive and emotional reactions to software structure. We propose to examine whether modular design affects levels of cognitive load and arousal. Our findings open the door for future research that combines software engineering and cognitive neuroscience. The potential implications of this study extend beyond optimal ways to structure software to leading the software engineering field to study individual cognition and arousal as a central component in successful software development. This opens a wide array of potential studies in the software engineering field.\"",
        "Document: \"Barriers to modularity: an empirical study to assess the potential for modularisation of java programs. To deal with the challenges when building large and complex systems modularisation techniques such as component-based software engineering and aspect-oriented programming have been developed. In the Java space these include dependency injection frameworks and dynamic component models such as OSGi. The question arises as to how easy it will be to transform existing systems to take advantage of these new techniques. Anecdotal evidence from industry suggests that the presence of certain patterns presents barriers to refactoring of monolithic systems into a modular architecture. In this paper, we present such a set of patterns and analyse a large set of open-source systems for occurrences of these patterns. We use a novel, scalable static analyser that we have developed for this purpose. The key findings of this paper are that almost all programs investigated have a significant number of these patterns, implying that modularising will be therefore difficult and expensive.\"",
        "1 is \"Clusters-Based Relevance Feedback for CBIR: A Combination of Query Movement and Query Expansion\", 2 is \"The personalization privacy paradox: an empirical evaluation of information transparency and the willingness to be profiled online for personalization\"",
        "Given above information, for an author who has written the paper with the title \"Optimal dimension-exchange token distribution on complete binary trees\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00122": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Verification of Multiagent Systems via Unbounded Model Checking':",
        "Document: \"A logic for knowledge, correctness, and real time. We present TCTLKD, a logic for knowledge, correctness and real time. TCTLKD is interpreted on real time deontic interpreted systems, and extension to continuous time of deontic interpreted systems. We exemplify the use of TCTLKD by discussing a variant of the \u201crailroad crossing system\u201d.\"",
        "Document: \"Combining fault injection and model checking to verify fault tolerance in multi-agent systems. The ability to guarantee that a system will continue to operate correctly under degraded conditions is key to the success of adopting multi-agent systems (MAS) as a paradigm for designing complex agent based fault tolerant systems. In order to provide such a guarantee, practically usable tools and techniques for verifying fault tolerant MAS architectures are urgently required. In this paper we address this requirement by combining automatic fault injection with model checking to verify fault tolerance in MAS. We present a generic method to mutate a model of a correctly behaving system into a faulty one, and show how the mutated model can be used to reason about fault tolerance, which includes recovery from faults. The usefulness of the proposed method is demonstrated by injecting automatically a fault into a sender-receiver protocol, and verifying temporal and epistemic specifications of the protocol's fault tolerance using the MCMAS model checker.\"",
        "Document: \"Distributed BDD-based BMC for the verification of multi-agent systems. We present a method of distributed model checking of multiagent systems specified by a branching-time temporal-epistemic logic. We introduce a serial algorithm, central to the distributed approach, for combining binary decision diagrams with bounded model checking. The algorithm is based on a notion of \"seed states\" to allow for state-space partitioning. Exploring individual partitions displays benefits arising from the verification of partial state-spaces. When verifying both an industrial model and a scalable benchmark scenario the serial bounded technique was found to be effective. Results for the distributed technique demonstrate that it out-performs the sequential approach for falsifiable formulae. Experimental data indicates that increasing the number of hosts improves verification efficiency.\"",
        "Document: \"Automatic verification of knowledge and time with NuSMV. We show that the problem ofmodel checking multi-dimensional modal logics can be reduced to the problem of model checking ARCTL, an extension of the temporal logic CTL with action labels and operators to reason about actions. In particular, we introduce a methodology for model checking a temporal-epistemic logic by building upon an extension of the model checker NuSMV that enables the verification of ARCTL. We briefly present the implementation and report experimental results for the verification of a typical security protocol involving temporal-epistemic properties: the protocol of the dining cryptographers.\"",
        "Document: \"Towards verifying contract regulated service composition. We report on a novel approach to (semi-)automatically compile and verify contract-regulated service compositions implemented as multi-agent systems. We model web service behaviours and the contracts governing them as WSBPEL specification. We use the formalism of temporal-epistemic logic, suitably extended to deal with compliance/violations of contracts, to specify properties of service compositions. We compile the WSBPEL behaviours into a specialised system description language ISPL, to be used with the model checker MCMAS to verify the behaviours automatically. We illustrate these concepts using a motivating example whose state space is approximately 106 and discuss experimental results.\"",
        "1 is \"Gossip-based peer sampling\", 2 is \"Symbolic model checking of hierarchical UML state machines\"",
        "Given above information, for an author who has written the paper with the title \"Verification of Multiagent Systems via Unbounded Model Checking\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00153": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Using language tests and emotional expressions to determine the learnability of artificial languages':",
        "Document: \"On the Use of Prosody for On-line Evaluation of Spoken Dialogue Systems. This paper focuses on the users' signaling of information st atus in human-machine interactions, and in particular looks at the role prosody may play in this respect. Using a corpus of interactions with two Dutch spoken dialogue systems, prosodic correlates of users' discon- firmations were investigated. In this corpus, disconfirmati ons may serve as a signal to 'go on' in one context and as a signa l to 'go back' in another. With the data obtained from this corpus an acoustic and a perception experiment have been carried out. The acoustic analysis shows that the difference in signaling function is reflected in the distribution of the various types of disconfirmations as well as in different prosodic variables (pause, duration, intonation contour a nd pitch range). The perception experiment revealed that subjects are very good at classifying disconfirmations as positive or negative signals (without context), which strongly suggests that the ac oustic features have communicative relevance. The implications of these results for human-machine interactions are discussed.\"",
        "Document: \"Clustering and matching headlines for automatic paraphrase acquisition. For developing a data-driven text rewriting algorithm for paraphrasing, it is essential to have a monolingual corpus of aligned paraphrased sentences. News article headlines are a rich source of paraphrases; they tend to describe the same event in various different ways, and can easily be obtained from the web. We compare two methods of aligning headlines to construct such an aligned corpus of paraphrases, one based on clustering, and the other on pairwise similarity-based matching. We show that the latter performs best on the task of aligning paraphrastic headlines.\"",
        "Document: \"Reconstructing dialogue history. This paper deals with a perceptual analysis of accent struct ure in Dutch to see to what extent listeners are able to reconstruct in- formation from the prior discourse context on the basis of pr os- odic properties of the current utterance. Using data collec ted in an earlier dialogue game experiment, subjects were asked to per- form a perceptual task in which they had to try and reconstruct what the previous utterance was on the basis of input utterances with different accent patterns. Our results reveal that lis teners are able to correctly guess the prior context for a significant num- ber of cases, but that performance depends on the type of inton- ation contour of the input utterance.\"",
        "Document: \"GRAPH: the costs of redundancy in referring expressions. We describe a graph-based generation system that participated in the TUNA attribute selection and realisation task of the REG 2008 Challenge. Using a stochastic cost function (with certain properties for free), and trying attributes from cheapest to more expensive, the system achieves overall .76 DICE and .54 MASI scores for attribute selection on the development set. For realisation, it turns out that in some cases higher attribute selection accuracy leads to larger differences between system-generated and human descriptions.\"",
        "Document: \"Taking turns in flying with a virtual wingman. In this study we investigate miscommunications in interactions between human pilots and a virtual wingman, represented by our virtual agent Ashley. We made an inventory of the type of problems that occur in such interactions using recordings of Ashley in flight briefings with pilots and designed a perception experiment to find evidence of human pilots providing cues on the occurrence of miscommunications. In this experiment, stimuli taken from the recordings are rated by naive participants on successfulness. Results show the largest part of miscommunications concern floor management. Participants are able to correctly assess the success of interactions, thus indicating cues for such judgment are present, though successful interactions are better recognized. Moreover, we see stimulus modality (audio, visual or combined) does not influence the ability of participants to judge the success of the interactions. From these results, we present recommendations for further developing virtual wingmen.\"",
        "1 is \"Consistency of personality in interactive characters: verbal cues, non-verbal cues, and user characteristics\", 2 is \"Syntax-based alignment of multiple translations: extracting paraphrases and generating new sentences\"",
        "Given above information, for an author who has written the paper with the title \"Using language tests and emotional expressions to determine the learnability of artificial languages\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00177": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Perfectly Balanced Allocation':",
        "Document: \"Zero-Sum Game Techniques for Approximate Nash Equilibria. We apply existing, and develop new, zero-sum game techniques for designing polynomial-time algorithms to compute additive approximate Nash equilibria in bimatrix games. In particular, we give a polynomial-time algorithm that given an arbitrary bimatrix game as an input, outputs either an additive 1/3-Nash equilibrium or an additive 1/2-well-supported Nash equilibrium; and we give a polynomial-time algorithm that given a bimatrix game in which both payoff matrices are symmetric as an input, computes an additive 1/2-well-supported Nash equilibrium. The former result is unusual: the obvious weakness is that the algorithm does not guarantee which of the two kinds of approximate equilibria it will output, but on the other hand each of the two approximation guarantees it gives are better than the best unconditional bounds known to be computable in polynomial time: $0.3393$ for Nash equilibria and 0.6528 for well-supported Nash equilibria. In the latter case, we motivate the interest in computing additive approximate Nash equilibria efficiently for bimatrix games with symmetric payoff matrices by proving that computing Nash equilibria in bimatrix games is PPAD-complete even if both of the payoff matrices are symmetric.\"",
        "Document: \"Very fast approximation of the matrix chain product problem. This paper considers the matrix chain product problem. This problem can be solved inO(nlogn) sequential time, while the best known parallel NC algorithm runs inO(log2n) time usingn6/log6nprocessors and inO(log3n) time withO(n2) time\u2013processor product. This paper presents a very fast parallel algorithm for approximately solving the matrix chain product problem and for the problem for finding a near-optimal triangulation of a convex polygon. It runs inO(logn) time on a CREW PRAM and inO(loglogn) time on a COMMON CRCW PRAM. If the dimensions of matrices are integers drawn from a domain [k,\u2026,k+s], we can speed up our algorithm to run inO(logloglog(n+s)) time on a COMMON CRCW PRAM. In all cases the total time\u2013processor product is linear. The algorithm produces solutions for the above problems that are at most[formula](\u22480.1547) times the optimal solutions.\"",
        "Document: \"Fast message dissemination in random geometric ad-hoc radio networks. We study the complexity of distributed protocols for the classical information dissemination problem of distributed gossiping. We consider the model of random geometric networks, one of the main models used to study properties of sensor and ad-hoc networks, where n points are randomly placed in a unit square and two points are connected by an edge/link if they are at at most a certain fixed distance r from each other. To study communication in the network, we consider the ad-hoc radio networks model of communication. We examine various scenarios depending on the local knowledge of each node in the networks, and show that in many settings distributed gossiping in asymptotically optimal time O(D) is possible, where D is the diameter of the network and thus a trivial lower bound for any communication.\"",
        "Document: \"Planar Graphs: Random Walks and Bipartiteness Testing. We initiate the study of the testability of properties in\\emph{arbitrary planar graphs}. We prove that \\emph{bipartiteness}can be tested in constant time. The previous bound for this class of graphs was $\\tilde{O}(\\sqrt{n})$, and the constant-time testability was only known for planar graphs with \\emph{bounded degree}. Previously used transformations of unbounded-degree sparse graphs into bounded-degree sparse graphs cannot be used to reduce the problem to the testability of bounded-degree planar graphs. Our approach extends to arbitrary minor-free graphs. Our algorithm is based on random walks. The challenge here is to analyze random walks for a class of graphs that has good separators, i.e., bad expansion. Standard techniques that use a fast convergence to a uniform distribution do not work in this case. Roughly speaking, our analysis technique self-reduces the problem of finding an odd-length cycle in a autograph $G$ induced by a collection of cycles to another multigraph $G'$ induced by a set of shorter odd-length cycles, in such a way that when a random walks finds a cycle in $G'$ with probability $p &gt, 0$, then it does so with probability $\\lambda(p)0$ in $G$. This reduction is applied until the cycles collapse to self-loops that can be easily detected.\"",
        "Document: \"Finding cycles and trees in sublinear time. We present sublinear-time (randomized) algorithms for finding simple cycles of length at least k3 and tree-minors in bounded-degree graphs. The complexity of these algorithms is related to the distance of the graph from being C-k-minor free (resp., free from having the corresponding tree-minor). In particular, if the graph is (1)-far from being cycle-free (i.e., a constant fraction of the edges must be deleted to make the graph cycle-free), then the algorithm finds a cycle of polylogarithmic length in time O</mml:mover>(<mml:msqrt>N</mml:msqrt>), where N denotes the number of vertices. This time complexity is optimal up to polylogarithmic factors. The foregoing results are the outcome of our study of the complexity of one-sided error property testing algorithms in the bounded-degree graphs model. For example, we show that cycle-freeness of N-vertex graphs can be tested with one-sided error within time complexity O</mml:mover>(poly(1/E)<mml:msqrt>N</mml:msqrt>), where denotes the proximity parameter. This matches the known (<mml:msqrt>N</mml:msqrt>) query lower bound for one-sided error cycle-freeness testing, and contrasts with the fact that any minor-free property admits a two-sided error tester of query complexity that only depends on . We show that the same upper bound holds for testing whether the input graph has a simple cycle of length at least k, for any k3. On the other hand, for any fixed tree T, we show that T-minor freeness has a one-sided error tester of query complexity that only depends on the proximity parameter . Our algorithm for finding cycles in bounded-degree graphs extends to general graphs, where distances are measured with respect to the actual number of edges. Such an extension is not possible with respect to finding tree-minors in o<mml:mo stretchy=\"false\">(<mml:msqrt>N</mml:msqrt><mml:mo stretchy=\"false\">) complexity. (c) 2012 Wiley Periodicals, Inc. Random Struct. Alg., 45, 139-184, 2014\"",
        "1 is \"A partial k-arboretum of graphs with bounded treewidth\", 2 is \"Simple On-Line Algorithms for the Maximum Disjoint Paths Problem\"",
        "Given above information, for an author who has written the paper with the title \"Perfectly Balanced Allocation\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00220": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'An algorithmic framework for convex mixed integer nonlinear programs':",
        "Document: \"Multi-scale optimization for process systems engineering. Efficient nonlinear programming (NLP) algorithms and modeling platforms have led to powerful process optimization strategies. Nevertheless, these algorithms are challenged by recent evolution and deployment of multi-scale models (such as molecular dynamics and complex fluid flow) that apply over broad time and length scales. Integrated optimization of these models requires accurate and efficient reduced models (RMs). This study develops a rigorous multi-scale optimization framework that substitutes RMs for complex original detailed models (ODMs) and guarantees convergence to the original optimization problem. Based on trust region concepts this framework leads to three related NLP algorithms for RM-based optimization. The first follows the classical gradient-based trust-region method, the second avoids gradient calculations from the ODM, and the third avoids frequent recourse to ODM evaluations, using the concept of e-exact RMs. We illustrate these algorithms with small examples and discuss RM-based optimization case studies that demonstrate their performance and effectiveness. (C), 2013 Elsevier Ltd. All rights reserved.\"",
        "Document: \"Retrospective on optimization. In this paper, we provide a general classification of mathematical optimization problems, followed by a matrix of applications that shows the areas in which these problems have been typically applied in process systems engineering. We then provide a review of solution methods of the major types of optimization problems for continuous and discrete variable optimization, particularly nonlinear and mixed-integer nonlinear programming (MINLP). We also review their extensions to dynamic optimization and optimization under uncertainty. While these areas are still subject to significant research efforts, the emphasis in this paper is on major developments that have taken place over the last 25 years.\"",
        "Document: \"An algorithmic framework for convex mixed integer nonlinear programs. This paper is motivated by the fact that mixed integer nonlinear programming is an important and difficult area for which there is a need for developing new methods and software for solving large-scale problems. Moreover, both fundamental building blocks, namely mixed integer linear programming and nonlinear programming, have seen considerable and steady progress in recent years. Wishing to exploit expertise in these areas as well as on previous work in mixed integer nonlinear programming, this work represents the first step in an ongoing and ambitious project within an open-source environment. COIN-OR is our chosen environment for the development of the optimization software. A class of hybrid algorithms, of which branch-and-bound and polyhedral outer approximation are the two extreme cases, are proposed and implemented. Computational results that demonstrate the effectiveness of this framework are reported. Both the library of mixed integer nonlinear problems that exhibit convex continuous relaxations, on which the experiments are carried out, and a version of the software used are publicly available.\"",
        "Document: \"Optimization of Supply Chain Systems with Price Elasticity of Demand. A centralized multiechelon, multiproduct supply chain network is presented in a multiperiod setting with products that show varying demand against price. An important consideration in such complex supply chains is to maintain system performance at high levels for varying demands that may be sensitive to product price. To examine the price-centric behavior of the customers, the concept of price elasticity of demand is addressed. The proposed approach includes many realistic features of typical supply chain systems such as production planning and scheduling, inventory management, transportation delay, transportation cost, and transportation limits. In addition, the proposed system can be extended to meet unsatisfied demand in future periods by backordering. Effects of the elasticity in price demand in production and inventory decisions are also examined. The supply chain model is formulated as a convex mixed-integer nonlinear programming problem. Reformulations are presented to make the problem tractable. The differential equations are reformulated as difference equations, and unbounded derivatives in the nonlinear objective function are handled with an approximation, with guaranteed bounds on the loss of optimality. The approach is illustrated on a multiechelon, multiproduct supply chain network.\"",
        "Document: \"Advanced optimization strategies for integrated dynamic process operations. \u2022Dynamic optimization strategies based on simultaneous collocation are reviewed.\u2022Three real-world applications are described for complex, exothermic reactive systems.\u2022Multi-stage formulations are described for grade transition of LLDPE processes.\u2022Integrated dynamic optimization/scheduling policies described for polyol processes.\"",
        "1 is \"Massively Parallel Linear Stability Analysis with P_ARPACK for 3D Fluid Flow Modeled with MPSalsa\", 2 is \"Application-Oriented Mixed Integer Non-Linear Programming\"",
        "Given above information, for an author who has written the paper with the title \"An algorithmic framework for convex mixed integer nonlinear programs\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00287": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Near-optimal Bayesian localization via incoherence and sparsity':",
        "Document: \"A Combined Hardware/Software Optimization Framework for Signal Representation and Recognition. This paper describes a signal recognition system that is jointly optimized from mathematical representation, algorithm dand final implementation. The goal is to exploit signal properties to jointly optimize a computation, beginning with first principles (mathematical representation) and completed with implementation. We use a BestBasis algorithm to search a large collection of orthogonal transforms derived from the Walsh-Hadamard transform to find a series of transforms which best discriminate among signal classes.The implementation exploits the structure of these matrices to compress the matrix representation, and in the process of multiplying the signal by the transform, reuse the results of prior computation and parallelize the implementation in hardware. Through this joint optimization, this dynamic, data-driven system is able to yield much more highly optimized results than if the optimizations were performed statically and in isolation. We provide results taken from applying this system to real input signals of spoken digits, and perform the initial analyses to demonstrate the properties of the transform matrices lead to optimized solutions.\"",
        "Document: \"The very small world of the well-connected. Online networks occupy an increasingly larger position in how we acquire information, how we communicate with one another, and how we disseminate information. Frequently, small sets of vertices dominate various graph and statistical properties of these networks and, because of this, they are relevant for structural analysis and efficient algorithms and engineering. For the web overall, and specifically for social linking in blogs and instant messaging, we provide a principled, rigorous study of the properties, the construction, and the utilization of subsets of special vertices in large online networks. We show that graph synopses defined by the importance of vertices provide small, relatively accurate portraits, independent of the importance measure, of the larger underlying graphs and of the important vertices. Furthermore, they can be computed relatively efficiently.\n\n\"",
        "Document: \"Sparse Approximation Via Iterative Thresholding. The well-known shrinkage technique is still relevant for con- temporary signal processing problems over redundant dictio- naries. We present theoretical and empirical analyses for two iterative algorithms for sparse approximation that use shrink- age. The GENERAL IT algorithm amounts to a Landweber iteration with nonlinear shrinkage at each iteration step. The BLOCK IT algorithm arises in morphological components anal- ysis. A sufficient condition for which General IT exactly re- covers a sparse signal is presented, in which the cumulative coherence function naturally arises. This analysis extends previous results concerning the Orthogonal Matching Pursuit (OMP) and Basis Pursuit (BP) algorithms to IT algorithms. We provide a sufficient condition for which guarantees that GENERAL IT recovers exactly sparse signals. This suf- ficient condition matches the sufficient geometric conditions for the Orthogonal Matching Pursuit (OMP) and Basis Pur- suit (BP) algorithms. We also provide analysis of the fixed points of the BLOCK IT algorithm. In the following section we make rigorous the concepts that arise in sparse approxi- mation problems and define two iterative thresholding algo- rithms. We then provide the theoretical analysis of these al- gorithms, followed by a discussion of the main result of the article: a sufficient condition guaranteeing the recovery of ex- actly sparse signals. The paper concludes with a study of the empirical performance of each algorithm.\"",
        "Document: \"Algorithms for simultaneous sparse approximation: part I: Greedy pursuit. A simultaneous sparse approximation problem requests a good approximation of several input signals at once using different linear combinations of the same elementary signals. At the same time, the problem balances the error in approximation against the total number of elementary signals that participate. These elementary signals typically model coherent structures in the input signals, and they are chosen from a large, linearly dependent collection.The first part of this paper proposes a greedy pursuit algorithm, called simultaneous orthogonal matching pursuit (S-OMP), for simultaneous sparse approximation. Then it presents some numerical experiments that demonstrate how a sparse model for the input signals can be identified more reliably given several input signals. Afterward, the paper proves that the S-OMP algorithm can compute provably good solutions to several simultaneous sparse approximation problems.The second part of the paper develops another algorithmic approach called convex relaxation, and it provides theoretical results on the performance of convex relaxation for simultaneous sparse approximation.\"",
        "Document: \"Fast, Small-Space Algorithms for Approximate Histogram Maintenance. A vector of length is defined implicitly, via a stream of updates of the form \"add 5 to .\" We give a sketching algorithm, that constructs a small sketch from the stream of updates, and a reconstruction algorithm, that produces a -bucket piecewise-constant representation (histogram) for from the sketch, such that , where the error is either (absolute) or (root-mean-square) error. The time to process a single upda te, time to reconstruct the histogram, and size of the sketch are each bounded by . Our result is obtained in two steps. First we obtain what we call a robust histogram approximation for , a histogram such that adding a small number of buckets does not help improve the representation quality significantly. From the robust histogram, we cull a histogram of desired accruacy and buckets in the second step. This technique also provides similar results for Haar wavelet representations , under error. Our results have applications in summarizing data distributions fast and succinctly even in distributed settings.\"",
        "1 is \"Noiseless Data Compression with Low-Density Parity-Check Codes.\", 2 is \"Information-Theoretic Lower Bounds on the Oracle Complexity of Stochastic Convex Optimization\"",
        "Given above information, for an author who has written the paper with the title \"Near-optimal Bayesian localization via incoherence and sparsity\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00294": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Using dynamic time warping for online temporal fusion in multisensor systems':",
        "Document: \"An application of \"agent-oriented\" techniques to symbolic matching and object recognition. This paper describes a feasibility study of \"multi-agent oriented\" techniques on a 2-D and 3-D object recognition system. The main aim of the project is to develop an inspection supporting tool that understands objects in both 2-D and 3-D in a unified system. 2-D and 3-D worlds are mapped to each other via agent-like entities each of which holds a conceptualized representation, allowing for a robust inference ability. Agents, each of which has symbolic representation of a part of an object, are hierarchically organized to represent a complete representation of an object. In this paper, object recognition is carried out with two matching methods: (1) the matching between an object model (agent's knowledge) and observed data, and (2) a constraint propagation-like method to achieve overall consistency among agents. The first is carried out with a symbolic Hopfield-type neural network and the second via a hierarchical Winner-Takes-All algorithm.\"",
        "Document: \"Using dynamic time warping for online temporal fusion in multisensor systems. Sensor fusion is concerned with gaining information from multiple sensors by fusing across raw data, features or decisions. Traditionally these fusion processes only concern fusion at specific points in time. However recently, there is a growing interest in inferring the behavioural aspects of environments or objects that are monitored by multisensor systems, rather than just their states at specific points in time. In order to infer environmental behaviours, it may be necessary to fuse data acquired from (i) geographically distributed sensors at specific points of time and (ii) specific sensors over a period of time. Fusing multisensor data over a period of time (also known as Temporal fusion) is a challenging task, since the data to be fused consists of complex sequences that are multi-dimensional, multimodal, interacting, and time-varying in nature. Additionally, performing temporal fusion efficiently in real-time is another challenge due to the large amounts of data to be fused. To address this issue, we propose a robust and efficient framework that uses dynamic time warping (DTW) as the core recognizer to perform online temporal fusion on either the raw data or the features. We evaluate the performance of the online temporal fusion system on two real world datasets: (1) accelerometer data acquired from performing two hand gestures, and (2) a benchmark dataset acquired from carrying a mobile device and performing the predefined user scenarios. Performance results of the DTW-based system are compared with those of a Hidden Markov Model (HMM) based system. The experimental results from both datasets demonstrate that the proposed system outperforms HMM based systems, and has the capability to perform online temporal fusion efficiently and accurately in real-time.\"",
        "Document: \"3D shape matching and inspection using geometric features and relational learning. In this paper we consider the problem of matching 3D sensed data with models and inspection for defects where the correspondence between models and data needs to be solved in robust and efficient ways. We explore the use of machine learning (in particular, relational learning) as an efficient method for solving correspondence (and so, pose estimation) as well as automatically generating rules for acceptable shape variations from training data. As an additional but necessary issue, we also consider the use of view-independent covariance methods for the extraction of surface features used to determine shape signatures which correspond to curvature-like surface attributes. Such features are utilized in the relational learning model.\"",
        "Document: \"Telederm: Enhancing Dermatological Diagnosis for Rural and Remote Communities. This paper describes techniques used in a Web-based decision support system to enhance dermatological diagnosis for rural and remote communities. We proposed a technique to improve the query representation and shorten the length of the query process, and a knowledge validation technique to help the domain expert supervise and maintain a consistent knowledge base. Experiments were conducted to test the effectiveness of each technique. Results indicated that the new querying technique improves a decision tree query approach. In addition, the results showed that supervised knowledge updating can improve the identification and handling of ambiguous cases to produce 100% correct classification.\"",
        "Document: \"Filter techniques for complex spatio-chromatic image processing. We describe a color image transform which is capable of encoding spatio-chromatic image features. The method relies on the development of physical interpretations of complex image Fourier transforms allowing the direct extraction of spatio-chromatic structures from images. The process is analysed using an elliptical representation in order to determine filter characteristics and responses. Both a spatial and chromatic filter are developed and used on test images with results showing the ease with which this technique can be utilised\"",
        "1 is \"Modelling and Using Sensed Context Information in the Design of Interactive Applications\", 2 is \"3D tracking of human locomotion: a tracking as recognition approach\"",
        "Given above information, for an author who has written the paper with the title \"Using dynamic time warping for online temporal fusion in multisensor systems\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00343": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Iterative Combinatorial Auctions: Theory and Practice':",
        "Document: \"Localizing Search in Reinforcement Learning. Reinforcement learning (RL) can be impractical for many high dimensional problems because of the com- putational cost of doing stochastic search in large state spaces. We propose a new RL method, Boundary Lo- calized Reinforcement Learning (BLRL), which maps RL into a mode switching problem where an agent de- terministically chooses an action based on its state, and limits stochastic search to small areas around mode boundaries, drastically reducing computational cost. BLRL starts with an initial set of parameterized bound- aries that partition the state space into distinct control modes. Reinforcement reward is used to update the boundary parameters using the policy gradient formu- lation of Sutton et al. (2000). We demonstrate that stochastic search can be limited to regions near mode boundaries, thus greatly reducing search, while still guaranteeing convergence to a locally optimal deter- ministic mode switching policy. Further, we give con- ditions under which the policy gradient can be arbitrar- ily well approximated without the use of any stochastic search. These theoretical results are supported experi- mentally via simulation.\"",
        "Document: \"Two Reasons to Make Aggregated Probability Forecasts More Extreme. <P>When aggregating the probability estimates of many individuals to form a consensus probability estimate of an uncertain future event, it is common to combine them using a simple weighted average. Such aggregated probabilities correspond more closely to the real world if they are transformed by pushing them closer to 0 or 1. We explain the need for such transformations in terms of two distorting factors: The first factor is the compression of the probability scale at the two ends, so that random error tends to push the average probability toward 0.5. This effect does not occur for the median forecast, or, arguably, for the mean of the log odds of individual forecasts. The second factor\u0097which affects mean, median, and mean of log odds\u0097is the result of forecasters taking into account their individual ignorance of the total body of information available. Individual confidence in the direction of a probability judgment (high/low) thus fails to take into account the wisdom of crowds that results from combining different evidence available to different judges. We show that the same transformation function can approximately eliminate both distorting effects with different parameters for the mean and the median. And we show how, in principle, use of the median can help distinguish the two effects.</P>\"",
        "Document: \"Active learning for vision-based robot grasping. Reliable vision-based grasping has proved elusive outside of controlled environments. One approach towards building more flexible and domain-independent robot grasping systems is to employ learning to adapt the robot's perceptual and motor system to the task. However, one pitfall in robot perceptual and motor learning is that the cost of gathering the learning set may be unacceptably high. Active learning algorithms address this shortcoming by intelligently selecting actions so as to decrease the number of examples necessary to achieve good performance and also avoid separate training and execution phases, leading to higher autonomy. We describe the IE-ID3 algorithm, which extends the Interval Estimation (IE) active learning approach from discrete to real-valued learning domains by combining IE with a classification tree learning algorithm (ID-3). We present a robot system which rapidly learns to select the grasp approach directions using IE-ID3 given simplified superquadric shape approximations of objects. Initial results on a small set of objects show that a robot with a laser scanner system can rapidly learn to pick up new objects, and simulation studies show the superiority of the active learning approach for a simulated grasping task using larger sets of objects. Extensions of the approach and future areas of research incorporating more sophisticated perceptual and action representation are discussed\"",
        "Document: \"An auction-based method for decentralized train scheduling. We present a computational study of an auction-based method for decentralized train scheduling. The method is well suited to the natural information and control structure of mod- ern railroads. We assume separate network territories, with an autonomous dispatch agent responsible for the ow of trains over each territory. Each train is represented by a self-interested agent that bids for the right to travel across the network from its source to destination, submitting bids to multiple dispatch agents along its route as necessary. The bidding language allows trains to bid for the right to enter and exit territories at particular times, and also to represent indifference over a range of times. Computational results on a simple network with straight-forward best-response bid- ding strategies demonstrate that the auction computes near- optimal system-wide schedules. In addition, the method appears to have useful scaling properties, both with the number of trains and with the number of dispatchers, and generates less extremal solutions than those obtained using traditional centralized optimization techniques.\"",
        "Document: \"Streaming Feature Selection using IIC. In Streaming Feature Selection (SFS), new fea- tures are sequentially considered for addition to a predictive model. When the space of poten- tial features is large, SFS offers many advantages over methods in which all features are assumed to be known in advance. Features can be generated dynamically, focusing the search for new features on promising subspaces, and overfitting can be controlled by dynamically adjusting the thresh- old for adding features to the model. We present a new, adaptive complexity penalty, the Informa- tion Investing Criterion (IIC), which uses an ef- ficient coding of features added, and not added, to the model to dynamically adjust the threshold on the entropy reduction required for adding a new feature. Streaming Feature Selection with IIC gives strong guarantees against overfitting. In contrast, standard penalty methods such as BIC or RIC always drastically over- or under-fit in the limit of infinite numbers of non-predictive fea- tures. Empirical results show that SFS is compet- itive with much more compute-intensive feature selection methods.\"",
        "1 is \"Tagging English text with a probabilistic model\", 2 is \"Methods for boosting revenue in combinatorial auctions\"",
        "Given above information, for an author who has written the paper with the title \"Iterative Combinatorial Auctions: Theory and Practice\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00416": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Designing an API at an appropriate abstraction level for programming social robot applications.':",
        "Document: \"Getting Started with Sketch Tools. Diagrams are an important, if not pivotal, part in both education and design. In an effort to understand abstract concepts, students play an active role in their education by specifying visual and abstract concepts in hand-sketched diagrams. While students are understanding abstract concepts through hand-drawn diagrams, designers are creating those abstract concepts. Just as the act of hand-drawing a diagram (as opposed to using a mouse-and-palette CAD tool) better engages the student in the learning process, the act of hand-drawing a diagram also improves the design process by freeing the designer of constraints that may otherwise impede creativity and innovation.\"",
        "Document: \"INTERACTING with sketched interface designs: an evaluation study. Digital hand-drawn sketches provide a new and unique way of interacting with a prototype user interface design while it is still rendered as a sketch. The successful use of prototypes and scenarios for exploring design ideas is well documented. Hand-sketched designs have also been found preferable to formal diagrams during early design. The study reported here shows that interacting with digital sketches adds an exciting new dimension to the interface design process, we found that people do more revisions and more accurate revisions with digital sketches.\"",
        "Document: \"Real-time anaesthesia diagnosis display system with multi-modal alarms. Fatal errors during anaesthesia administration are usually preventable human mistakes. It is difficult for anaesthetists to keep monitoring every physiological change and to detect clinically critical events during anaesthesia. Intelligent patient monitoring systems to assist anaesthetists are under investigation. These systems require a distinctive and unique way of conveying alerts and diagnostic information to the anaesthetist in busy and noisy operating theatres. We present here a functional prototype of a multi-modal (audio & visual) alarm system, MMAS.\"",
        "Document: \"A Pen And Paper Metaphor For Orchid Modeling. The creation of 3D computer models is an essential task for many applications in science, engineering and arts and is frequently performed by untrained users. In many cases speed and simplicity of the modeling process is more important than matching the geometry of the modeled object exactly. Sketch-based modeling has been suggested as an important tool for such applications.In this paper we extend the pen and paper metaphor with a paper sculpting metaphor which is applied to sketched shapes. Using these techniques we present an efficient and effective tool for orchid modeling. We discuss the inherent properties of orchid flowers and use them to develop constraints for representing the complex surface shapes of orchids with simple 2D sketches. Surface details are added using noise functions. Additional surface modifications are possible using the paper sculpting metaphor. By computing inherent bending axis from the skeleton of a sketched 2D shape the user is able to warp leaf-like structures like if they were cut from a piece of paper. The intuitive object manipulation of our tool means that an otherwise complex model can be created by an inexperienced, non-artistic user in a short period of time.\"",
        "Document: \"Bubbleworld Builder 3d Modeling Using Two-Touch And Sketch Interaction. Commercial 3D modeling applications tend to be difficult and time consuming to use due to complex interfaces and functionality. In this paper we present a simple and intuitive interface for modeling \"blobby\" 3D objects using touch input. Objects are defined by sketching and modifying contours of cross-sectional slices. Two-touch interactions are used to zoom, rotate and slice the object. The resulting application allows rapid creation of 3D models and looks promising for medical. imaging applications. A drawback is that intuitiveness depends on a user's mental abilities such as 3D vision and the ability to develop a mental model and compare it with visual data.\"",
        "1 is \"The interactive dance club: avoiding chaos in a multi participant environment\", 2 is \"User acceptance of information technology: system characteristics, user perceptions and behavioral impacts\"",
        "Given above information, for an author who has written the paper with the title \"Designing an API at an appropriate abstraction level for programming social robot applications.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00514": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Look No Deeper: Recognizing Places from Opposing Viewpoints under Varying Scene Appearance using Single-View Depth Estimation.':",
        "Document: \"Unified Loop Closing and Recovery for Real Time Monocular SLAM. We present a unified method for recovering from tracking fail ure and closing loops in real time monocular simultaneous localisation and mapping. Within a graph-based map representation, we show that recovery and loop closing both reduce to the creation of a graph edge. We describe and implement a bag-of-words appearance model for ranking potential loop closures, and a robust method for using both structure and image appearance to confirm likely matches. The resulting system closes loops and recovers from failures while mapping thousands of landmarks, all in real time.\"",
        "Document: \"Vision-Based Cooperative Pose Estimation for Localization in Multi-Robot Systems Equipped with RGB-D Cameras. We present a new vision based cooperative pose estimation scheme for systems of mobile robots equipped with RGB-D cameras. We first model a multi-robot system as an edge-weighted graph. Then, based on this model, and by using the real-time color and depth data, the robots with shared field-of-views estimate their relative poses in pairwise. The system does not need the existence of a single common view shared by all robots, and it works in 3D scenes without any specific calibration pattern or landmark. The proposed scheme distributes working loads evenly in the system, hence it is scalable and the computing power of the participating robots is efficiently used. The performance and robustness were analyzed both on synthetic and experimental data in different environments over a range of system configurations with varying number of robots and poses.\"",
        "Document: \"Real-Time Tracking of Complex Structures for Visual Servoing. This paper presents a visualserv oing system which incorporates a novelthree-dimensionalmo del-based tracking system. This tracking system extends constrained active contour tracking techniques into three dimensions, placing them within a Lie algebraic framework. This is combined with modern graphicalrendering technology to create a system which can track complex three dimensional structures in real time at video frame rate (25 Hz) on a standard workstation without special hardware. The system is based on an internal CAD model of the object to be tracked which is rendered using binary space partition trees to perform hidden line removal. The visible features are identified on-line at each frame and are tracked in the video feed. Analytical and statistical edge saliency are then used as a means of increasing the robustness of the tracking system.\"",
        "Document: \"Robust feature matching in 2.3\u00b5s. In this paper we present a robust feature matching scheme in which features can be matched in 2.3\u00b5s. For a typical task involving 150 features per image, this re- sults in a processing time of 500\u00b5s for feature extraction and matching. In order to achieve very fast matching we use simple features based on histograms of pixel intensities and an indexing scheme based on their joint distribution. The features are stored with a novel bit mask representation which requires only 44 bytes of memory per feature and al- lows computation of a dissimilarity score in 20ns. A train- ing phase gives the patch-based features invariance to small viewpoint variations. Larger viewpoint variations are han- dled by training entirely independent sets of features from different viewpoints. A complete system is presented where a database of around 13,000 features is used to robustly localise a single planar target in just over a millisecond, including all steps from feature detection to model fitting. The resulting system shows comparable robustness to SIFT (8) and Ferns (14) while using a tiny fraction of the processing time, and in the latter case a fraction of the memory as well.\"",
        "Document: \"Multiview image compression and transmission techniques in Wireless Multimedia Sensor Networks: A survey. We present a survey of recent research works on multiview image compression and transmission techniques developed for Wireless Multimedia Sensor Networks (WMSNs). We classify them into two categories with respect to the coding methods adopted: (i) in-network processing with joint coding schemes, and (ii) distributed source coding schemes. The survey also includes a comprehensive evaluation of the limitations of each approach. Based on the review results, we discuss future research directions, and identify the ways of more efficient transmission of the spatially correlated and redundant data in WMSNs.\"",
        "1 is \"Robust Place Recognition With Stereo Sequences.\", 2 is \"The metaDESK: models and prototypes for tangible user interfaces\"",
        "Given above information, for an author who has written the paper with the title \"Look No Deeper: Recognizing Places from Opposing Viewpoints under Varying Scene Appearance using Single-View Depth Estimation.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00548": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Location-based services: A database perspective':",
        "Document: \"Frequent route based continuous moving object location- and density prediction on road networks. Emerging trends in urban mobility have accelerated the need for effective traffic prediction and management systems. The present paper proposes a novel approach to using continuously streaming moving object trajectories for traffic prediction and management. The approach continuously performs three functions for streams of moving object positions in road networks: 1) management of current evolving trajectories, 2) incremental mining of closed frequent routes, and 3) prediction of near-future locations and densities based on 1) and 2). The approach is empirically evaluated on a large real-world data set of moving object trajectories, originating from a fleet of taxis, illustrating that detailed closed frequent routes can be efficiently discovered and used for prediction.\"",
        "Document: \"Converting XML DTDs to UML diagrams for conceptual data integration. Extensible Markup Language (XML) is fast becoming the new standard for data representation and exchange on the World Wide Web, e.g., in B2B e-commerce. Modern enterprises need to combine data from many sources in order to answer important business questions, creating a need for integration of web-based XML data. Previous web-based data integration efforts have focused almost exclusively on the logical level of data models, creating a need for techniques that focus on the conceptual level in order to communicate the structure and properties of the available data to users at a higher level of abstraction. The most widely used conceptual model at the moment is the Unified Modeling Language (UML).This paper presents algorithms for automatically constructing UML diagrams from XML DTDs, enabling fast and easy graphical browsing of XML data sources on the web. The algorithms capture important semantic properties of the XML data such as precise cardinalities and aggregation (containment) relationships between the data elements. As a motivating application, it is shown how the generated diagrams can be used for the conceptual design of data warehouses based on web data, and an integration architecture is presented. The choice of data warehouses and On-Line Analytical Processing as the motivating application is another distinguishing feature of the presented approach.\"",
        "Document: \"Optimizing Aggregate SPARQL Queries Using Materialized RDF Views. During recent years, more and more data has been published as native RDF datasets. In this setup, both the size of the datasets and the need to process aggregate queries represent challenges for standard SPARQL query processing techniques. To overcome these limitations, materialized views can be created and used as a source of precomputed partial results during query processing. However, materialized view techniques as proposed for relational databases do not support RDF specifics, such as incompleteness and the need to support implicit (derived) information. To overcome these challenges, this paper proposes MARVEL (MAterialized Rdf Views with Entailment and incompLetness). The approach consists of a view selection algorithm based on an associated RDF-specific cost model, a view definition syntax, and an algorithm for rewriting SPARQL queries using materialized RDF views. The experimental evaluation shows that MARVEL can improve query response time by more than an order of magnitude while effectively handling RDF specifics.\"",
        "Document: \"Specification-Based Data Reduction in Dimensional Data Warehouses. Abstract Many data warehouses contain massive amounts of data and grow rapidly Examples include ware - houses with retail sales data capturing customer behavior and warehouses with click - stream data cap - turing user behavior on web sites The sheer size of these warehouses makes them increasingly hard to manage and query efficiently As time passes, old, detailed data in the warehouses tend to become less interesting However, higher - level summaries of the data, taking up far less space, continue to be of interest Thus, it is possible to perform data reduction in dimensional data warehouses by aggregating data to higher levels in the dimensions This paper presents an effective technique for data reduction that handles the gradual change of the data from new, detailed data to older, summarized data The technique enables huge storage gains while ensuring the retention of essential data The data reduction is based on formal specifications of when data should be aggregated to higher levels Care is taken to ensure that the irreversible data reductions are without semantic problems It is defined how queries over the resulting data with varying levels of detail are handled, and a strategy for implementing the technique using standard data warehouse technology is described\"",
        "Document: \"IMaxer: A Unified System for Evaluating Influence Maximization in Location-based Social Networks. Due to the popularity of social networks with geo-tagged activities, so-called location-based social networks (LBSN), a number of methods have been proposed for influence maximization for applications such as word-of-mouth marketing (WOMM), and out-of-home marketing (OOH). It is thus important to analyze and compare these different approaches. In this demonstration, we present a unified system IMaxer that both provides a complete pipeline of state-of-the-art and novel models and algorithms for influence maximization (IM) as well as allows to evaluate and compare IM techniques for a particular scenario. IMaxer allows to select and transform the required data from raw LBSN datasets. It further provides a unified model that utilizes interactions of nodes in an LBSN, i.e., users and locations, for capturing diverse types of information propagations. On the basis of these interactions, influential nodes can be found and their potential influence can be simulated and visualized using Google Maps and graph visualization APIs. Thus, IMaxer allows users to compare and pick the most suitable IM method in terms of effectiveness and cost.\n\n\"",
        "1 is \"Data Stream Warehousing In Tidalrace.\", 2 is \"Routing directions: keeping it fast and simple\"",
        "Given above information, for an author who has written the paper with the title \"Location-based services: A database perspective\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00569": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Fashion IQ: A New Dataset Towards Retrieving Images by Natural Language Feedback':",
        "Document: \"Real-time View-based Face Alignment using Active Wavelet Networks. The Active Wavelet Network (AWN) [9] approach was recently proposedfor automatic face alignment, showing advantages over ActiveAppearance Models (AAM), such as more robustness against partialocclusions and illumination changes. In this paper, we (1) extendthe AWN method to a view-based approach, (2) verify the robustnessof our algorithm with respect to unseen views in a large datasetand (3)show that using only nine wavelets, our method yieldssimilar performance to state-of-the-art face alignment systems,with a significant enhancement in terms of speed. Afteroptimization, our system requires only 3ms per iteration on a1.6GHz Pentium IV. We show applications in face alignment forrecognition and real-time facial feature tracking underlarge posevariations.\"",
        "Document: \"Non-photorealistic camera: depth edge detection and stylized rendering using multi-flash imaging. We present a non-photorealistic rendering approach to capture and convey shape features of real-world scenes. We use a camera with multiple flashes that are strategically positioned to cast shadows along depth discontinuities in the scene. The projective-geometric relationship of the camera-flash setup is then exploited to detect depth discontinuities and distinguish them from intensity edges due to material discontinuities.We introduce depiction methods that utilize the detected edge features to generate stylized static and animated images. We can highlight the detected features, suppress unnecessary details or combine features from multiple images. The resulting images more clearly convey the 3D structure of the imaged scenes.We take a very different approach to capturing geometric features of a scene than traditional approaches that require reconstructing a 3D model. This results in a method that is both surprisingly simple and computationally efficient. The entire hardware/software setup can conceivably be packaged into a self-contained device no larger than existing digital cameras.\"",
        "Document: \"Edge guided single depth image super resolution. Recently, consumer depth cameras have gained significant popularity due to their affordable cost. However, the limited resolution and quality of the depth map generated by these cameras are still problems for several applications. In this paper, we propose a novel framework for single depth image super resolution guided by a high resolution edge map constructed from the edges in the low resolution depth image via a Markov Random Field (MRF) optimization. With the guidance of the high resolution edge map, the high resolution depth image is up-sampled via a joint bilateral filter. The edge guidance not only helps avoid artifacts introduced by direct texture prediction, but also reduces the jagged artifacts and preserves the sharp edges. Experimental results demonstrate the effectiveness of our proposed algorithm compared to previously reported methods.\"",
        "Document: \"An Integrated System for Moving Object Classification in Surveillance Videos. Moving object classification in far-field video is a key component of smart surveillance systems. In this paper, we propose a reliable system for person-vehicle classification which works well in challenging real-word conditions, including the presence of shadows, low resolution imagery, perspective distortions, arbitrary camera viewpoints, and groups of people. Our system runsin real-time (30 Hz) on conventional machines and has low memory consumption. We achieved accurate results by relying on powerful discriminative features, including a novel measure of object deformation based on differences of histograms of oriented gradients. We also provide an interactive user interface, enabling users to specify regions of interest for each class and correct for perspective distortions by specifying different sizes indifferent positions of the camera view. Finally, we use anautomatic adaptation process to continuously update the parameters of the system so that its performance increases for a particular environment. Experimental results demonstrate the effectiveness of our system in standard dataset and a variety of video clips captured with our surveillance cameras.\"",
        "Document: \"Designing Category-Level Attributes for Discriminative Visual Recognition. Attribute-based representation has shown great promises for visual recognition due to its intuitive interpretation and cross-category generalization property. However, human efforts are usually involved in the attribute designing process, making the representation costly to obtain. In this paper, we propose a novel formulation to automatically design discriminative \"category-level attributes\", which can be efficiently encoded by a compact category-attribute matrix. The formulation allows us to achieve intuitive and critical design criteria (category-separability, learn ability) in a principled way. The designed attributes can be used for tasks of cross-category knowledge transfer, achieving superior performance over well-known attribute dataset Animals with Attributes (AwA) and a large-scale ILSVRC2010 dataset (1.2M images). This approach also leads to state-of-the-art performance on the zero-shot learning task on AwA.\"",
        "1 is \"An integrated system for content-based video retrieval and browsing\", 2 is \"Very Deep Convolutional Networks for Large-Scale Image Recognition.\"",
        "Given above information, for an author who has written the paper with the title \"Fashion IQ: A New Dataset Towards Retrieving Images by Natural Language Feedback\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00633": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Relating Architectural Decay and Sustainability of Software Systems':",
        "Document: \"DISCOA: architectural adaptations for security and QoS. Modern distributed systems have greatly benefited from developments such as model-driven development, and architectural description languages. Abstract models of components (e.g., IDL) and models of interconnection (e.g., architectural description languages, or ADLs) provide important software engineering advantages, such as explicit design models, type-checked integration across machine and language boundaries (with generated marshaling and dispatch code), the possibility of third-party components, and automated verification of design artifacts. But, when distributed systems are enhanced to provide security features, many of these advantages do not apply. Security features are hand-written into almost every part of the system; there is no explicit component or architectural model, or separable \"security component\" security code fragments are scattered and tangled through the different distributed elements of the system, and are often reduced to communicating through lowest-common denominator fragments (like raw bytes) since they are not represented in the model.In this paper, we describe DISCOA, a proposed extension of our earlier work on DADO [23] to handle security features in distributed systems, using explicit architectural models with aspect-oriented extensions.\"",
        "Document: \"Reconciling software requirements and architectures with intermediate models. Little guidance and few methods are avail- able for the refinement of software requirements into an architecture satisfying those requirements. Part of the challenge stems from the fact that requirements and ar- chitectures use different terms and concepts to capture the model elements relevant to each. In this paper we will present CBSP, a lightweight approach intended to pro- vide a systematic way of reconciling requirements and architectures using intermediate models. CBSP lever- ages a simple set of architectural concepts (components, connectors, overall systems, and their properties) to re- cast and refine the requirements into an intermediate model facilitating their mapping to architectures. Fur- thermore, the intermediate CBSP model eases captur- ing and maintaining arbitrarily complex relationships be- tween requirements and architectural model elements, as well as among CBSP model elements. We have applied CBSP within the context of different requirements and architecture definition techniques. We leverage that ex- perience in this paper to demonstrate the CBSP method and tool support using a large-scale example.\"",
        "Document: \"Support for Disconnected Operation via Architectural Self-Reconfiguration. In distributed and mobile environments, the connections among the hosts on which a software system is running are often unstable. As a result of connectivity losses, the overall availability of the system decreases. The distribution of software components onto hardware nodes (i.e., deployment architecture) may be ill-suited for the given state of the target hardware environment and may need to be altered to improve the software system\u00fds availability. In this paper, we present a flexible, software architecture-based solution for disconnected operation that increases the availability of a system during disconnection by allowing the system to monitor and automatically redeploy itself.\"",
        "Document: \"Component-based perspective on software mismatch detection and resolution. Existing approaches to modeling software systems all too often neglect the issue of component mismatch identification and resolution. The traditional view of software development over-emphasizes synthesis at the expense of analysis - the latter frequently being seen as a problem one only needs to deal with during the integration stage towards the end of a development project. This paper discusses three software modeling and analysis techniques, all tool supported, and emphasizes the vital role analysis can play in identifying and resolving risks early on. This work also combines model based development with component based development (e.g., COTS and legacy systems) and shows how their mismatch detection capabilities complement each other in providing a more comprehensive coverage of development risks.\"",
        "Document: \"Detecting event anomalies in event-based systems. Event-based interaction is an attractive paradigm because its use can lead to highly flexible and adaptable systems. One problem in this paradigm is that events are sent, received, and processed nondeterministically, due to the systems\u2019 reliance on implicit invocation and implicit concurrency. This nondeterminism can lead to event anomalies, which occur when an event-based system receives multiple events that lead to the write of a shared field or memory location. Event anomalies can lead to unreliable, error-prone, and hard to debug behavior in an event-based system. To detect these anomalies, this paper presents a new static analysis technique, DEvA, for automatically detecting event anomalies. DEvA has been evaluated on a set of open-source event-based systems against a state-of-the-art technique for detecting data races in multithreaded systems, and a recent technique for solving a similar problem with event processing in Android applications. DEvA exhibited high precision with respect to manually constructed ground truths, and was able to locate event anomalies that had not been detected by the existing solutions.\"",
        "1 is \"Categorizing the Spectrum of Coordination Technology\", 2 is \"Dynamic Binding Framework for Adaptive Web Services\"",
        "Given above information, for an author who has written the paper with the title \"Relating Architectural Decay and Sustainability of Software Systems\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00803": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Aspect-Based Personalized Text Summarization':",
        "Document: \"A probabilistic framework for recognizing intention in information graphics. This paper extends language understanding and plan inference to information graphics. We identify the kinds of communicative signals that appear in information graphics, describe how we utilize them in a Bayesian network that hypothesizes the graphic's intended message, and discuss the performance of our implemented system. This work is part of a larger project aimed at making information graphics accessible to individuals with sight impairments.\"",
        "Document: \"Query expansion and query reduction in document retrieval. We investigate two seemingly incompatible approaches for improving document retrieval performance in the context of question answering: query expansion and query reduction. Queries are expanded by generating lexical paraphrases. Syntactic, semantic and corpus-based frequency information is used in this process. Queries are reduced by removing words that may detract from retrieval performance. Features that identify these words were obtained from decision graphs. These approaches were evaluated using a subset of queries from TREC8, 9 and 10. Our evaluation shows that each approach in isolation improves retrieval performance, and both approaches together yield substantial improvements. Specifically, query expansion followed by reduction improved the average number of correct documents retrieved by 21.7% and the average number of queries that can be answered by 15%.\"",
        "Document: \"Inferences, suppositions and explanatory extensions in argument interpretation. We describe a probabilistic approach for the interpretation of user arguments that integrates three aspects of an interpretation: inferences, suppositions and explanatory extensions. Inferences fill in information that connects the propositions in a user's argument, suppositions postulate new information that is likely believed by the user and is necessary to make sense of his or her argument, and explanatory extensions postulate information the user may have implicitly considered when constructing his or her argument. Our system receives as input an argument entered through a web interface, and produces an interpretation in terms of its underlying knowledge representation--a Bayesian network. Our evaluations show that suppositions and explanatory extensions are necessary components of interpretations, and that users consider appropriate the suppositions and explanatory extensions postulated by our system.\"",
        "Document: \"Personalised viewing-time prediction in museums. People are often overwhelmed by the large amount of information provided in museum spaces, which makes it difficult for them to select exhibits of potential interest. As a first step in recommending exhibits where a visitor may wish to spend some time, this article investigates predictive user models for personalised prediction of museum visitors' viewing times at exhibits. We consider two content-based models and a nearest-neighbour collaborative filter, and develop a collaborative model based on the theory of spatial processes which relies on a notion of distance between exhibits. We discuss models of exhibit distance derived from viewing-time similarity, semantic similarity and walking distance. The results from our evaluation with a real-world dataset of visitor pathways collected at Melbourne Museum (Melbourne, Australia) suggest that utilising walking and semantic distances between exhibits enables more accurate predictions of a visitor's viewing times of unseen exhibits than using distances derived from observed exhibit viewing times. Our results also show that all models outperform a non-personalised baseline, that content-based viewing time prediction yields better results than nearest-neighbour collaborative prediction, and that our collaborative model based on spatial processes attains the highest predictive accuracy overall.\"",
        "Document: \"The automated understanding of simple bar charts. While identifying the intention of an utterance has played a major role in natural language understanding, this work is the first to extend intention recognition to the domain of information graphics. A tenet of this work is the belief that information graphics are a form of language. This is supported by the observation that the overwhelming majority of information graphics from popular media sources appear to have some underlying goal or intended message. As Clark noted, language is more than just words. It is any ''signal'' (or lack of signal when one is expected), where a signal is a deliberate action that is intended to convey a message (Clark, 1996 [15]). As a form of language, information graphics contain communicative signals that can be used in a computational system to identify the message that the graphic conveys. We identify the communicative signals that appear in simple bar charts, and present an implemented Bayesian network methodology for reasoning about these signals and hypothesizing a bar chart's intended message. Once the message conveyed by an information graphic has been inferred, it can then be used to facilitate access to this information resource for a variety of users, including 1) users of digital libraries, 2) visually impaired users, and 3) users of devices where graphics are impractical or inaccessible.\"",
        "1 is \"COLLAGEN: A Collaboration Manager for Software Interface Agents\", 2 is \"Automatic labeling of multinomial topic models\"",
        "Given above information, for an author who has written the paper with the title \"Aspect-Based Personalized Text Summarization\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00832": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Data fitting via chaotic ant swarm':",
        "Document: \"Some improved results on communication between information systems. To study the communication between information systems, Wang et al. [C. Wang, C. Wu, D. Chen, Q. Hu, C. Wu, Communicating between information systems, Information Sciences 178 (2008) 3228-3239] proposed two concepts of type-1 and type-2 consistent functions. Some properties of such functions and induced relation mappings have been investigated there. In this paper, we provide an improvement of the aforementioned work by disclosing the symmetric relationship between type-1 and type-2 consistent functions. We present more properties of consistent functions and induced relation mappings and improve upon several deficient assertions in the original work. In particular, we unify and extend type-1 and type-2 consistent functions into the so-called neighborhood-consistent functions. This provides a convenient means for studying the communication between information systems based on various neighborhoods.\"",
        "Document: \"New Constructions Of Binary Sequences With Optimal Autocorrelation Magnitude Based On Interleaving Technique. Recently, Yu, Gong and Tang found new constructions of binary sequences of period 4N with optimal autocorrelation magnitude by different interleaved structure of sequences and sequences which had special correlation property, respectively. In this paper, we derive more results on binary sequences of period 4N which also have optimal autocorrelation.\"",
        "Document: \"2-Adic Complexity of a Sequence Obtained from a Periodic Binary Sequence by Either Inserting or Deleting k Symbols within One Period. In this paper, we propose a method to get the lower bounds of the 2-adic complexity of a sequence obtained from a periodic sequence over by either inserting or deleting symbols within one period. The results show the variation of the distribution of the 2-adic complexity becomes as increases. Particularly, we discuss the lower bounds when respectively.\"",
        "Document: \"Bound on local unambiguous discrimination between multipartite quantum states. We investigate the upper bound on unambiguous discrimination by local operations and classical communication. We demonstrate that any set of linearly independent multipartite pure quantum states can be locally unambiguously discriminated if the number of states in the set is no more than $$\\max \\{d_{i}\\}$$max{di}, where the space spanned by the set can be expressed in the irreducible form $$\\otimes _{i=1}^{N}d_{i}$$ i=1Ndi and $$d_{i}$$di is the optimal local dimension of the $$i\\hbox {th}$$ith party. That is, $$\\max \\{d_{i}\\}$$max{di} is an upper bound. We also show that it is tight, namely there exists a set of $$\\max \\{d_{i}\\}+1$$max{di}+1 states, in which at least one of the states cannot be locally unambiguously discriminated. Our result gives the reason why the multiqubit system is the only exception when any three quantum states are locally unambiguously distinguished.\"",
        "Document: \"Revisiting the security of secure direct communication based on ping-pong protocol[Quantum Inf. Process. 8, 347 (2009)]. A. Chamoli and C.M. Bhandari presented a secure direct communication based on ping-pong protocol[Quantum Inf. Process. 8, 347 (2009)]. M.Naseri analyzed its security and pointed out that in this protocol any dishonest party can obtain all the other one's secret message with zero risk of being detected by using fake entangled particles (FEP attack) [M. Naseri, Quantum Inf. Process. online]. In this letter, we reexamine the protocol's security and discover that except the FEP attack, using a special property of GHZ states, any one dishonest party can also take a special attack, i.e., double-CNOT(Controlled NOT) attack. Finally, a denial-of-service attack is also discussed.\"",
        "1 is \"PayWord and MicroMint: Two Simple Micropayment Schemes\", 2 is \"Reflections on `Health Care in the Information Society - a Prognosis for the Year 2013`\"",
        "Given above information, for an author who has written the paper with the title \"Data fitting via chaotic ant swarm\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00925": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Grid-Based Metadata Services':",
        "Document: \"Orthogonal Persistence for Java? - A Mid-term Report. The experience of applying the principles of orthogonal persistence to the Java programming language is described in the context of the PJama prototype implementation. The model for checkpointing the state of a computation, including live threads, is analyzed and related to a transactional approach. The problem of dealing with state that is external to the PJama environment is explained and the solutions outlined. The difficult problem of system evolution is identified as the major barrier to deploying orthogonal persistence for the Java language.\"",
        "Document: \"Federated enactment of workflow patterns. In this paper we address two research questions concerning workflows: 1) how do we abstract and catalogue recurring workflow patterns?; and 2) how do we facilitate optimisation of the mapping from workflow patterns to actual resources at runtime? Our aim here is to explore techniques that are applicable to large-scale workflow compositions, where the resources could change dynamically during the lifetime of an application. We achieve this by introducing a registry-based mechanism where pattern abstractions are catalogued and stored. In conjunction with an enactment engine, which communicates with this registry, concrete computational implementations and resources are assigned to these patterns, conditional to the execution parameters. Using a data mining application from the life sciences, we demonstrate this new approach.\"",
        "Document: \"Accessing data in grids using OGSA-DAI. The grid provides a vision in which resources, including storage and data, can be shared across organisational boundaries. The original emphasis of grid computing lay in the sharing of computational resources but technological and scientific advances have led to an ongoing data explosion in many fields. However, data is stored in many different storage systems and data formats, with different schema, access rights, metadata attributes, and ontologies all of which are obstacles to the access, integration and management of this information. In this chapter we examine some of the ways in which these differences can be addressed by grid technology to enable the meaningful sharing of data. In particular, we present an overview of the OGSA-DAI (Open Grid Service Architecture - Data Access and Integration) software, which provides a uniform, extensible framework for accessing structured and semi-structured data and provide some examples of its use in other projects.\"",
        "Document: \"Issues Raised by Three Years of Developing PJama: An Orthogonally Persistent Platform for Java. Orthogonal persistence is based on three principles that have been understood for nearly 20 years. PJama is a publically available prototype of a Java platform that supports orthogonal persistence. It is already capable of supporting substantial applications. The experience of applying the principles of orthogonal persistence to the Java programming language is described in the context of PJama. For example, issues arise over achieving orthogonality when there are classes that have a special relationship with the Java Virtual Machine. The treatment of static variables and the definition of reachability for classes and the handling of the keyword transient also pose design problems. The model for checkpointing the state of a computation, including live threads, is analyzed and related to a transactional approach. The problem of dealing with state that is external to the PJama environment is explained and the solutions outlined. The difficult problem of system evolution is identified as a major barrier to deploying orthogonal persistence for the Java language. The predominant focus is on semantic issues, but with concern for reasonably efficient implementation. We take the opportunity throughout the paper and in the conclusions to identify directions for further work.\"",
        "Document: \"Introduction to OGSA-DAI services. In today's large collaborative environments, potentially composed of multiple distinct organisations, uniform controlled access to data has become a key requirement if these organisations are to work together as Virtual Organisations. We refer to such an integrated set of data resources as a virtual data warehouse. The Open Grid Services Architecture \u2013 Data Access and Integration (OGSA-DAI) project was established to produce a common middleware solution, aligned with the Global Grid Forum's (GGF) OGSA vision [OGSA] to allow uniform access to data resources using a service based architecture. In this paper the service infrastructure provided by OGSA-DAI is presented providing a snapshot of its current state, in an evolutionary process, which is attempting to build infrastructure to allow easy integration and access to distributed data using grids or web services. More information about OGSA-DAI is available from the project web site: www.ogsadai.org.\"",
        "1 is \"Granules: A Lightweight, Streaming Runtime For Cloud Computing With Support For Map-Reduce\", 2 is \"Web caching with consistent hashing\"",
        "Given above information, for an author who has written the paper with the title \"Grid-Based Metadata Services\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00964": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Visible Reverse k-Nearest Neighbor Queries':",
        "Document: \"A straw shows which way the wind blows: ranking potentially popular items from early votes. Prediction of popular items in online content sharing systems has recently attracted a lot of attention due to the tremendous need of users and its commercial values. Different from previous works that make prediction by fitting a popularity growth model, we tackle this problem by exploiting the latent conforming and maverick personalities of those who vote to assess the quality of on-line items. We argue that the former personality prompts a user to cast her vote conforming to the majority of the service community while on the contrary the later personality makes her vote different from the community. We thus propose a Conformer-Maverick (CM) model to simulate the voting process and use it to rank top-k potentially popular items based on the early votes they received. Through an extensive experimental evaluation, we validate our ideas and find that our proposed CM model achieves better performance than baseline solutions, especially for smaller k.\"",
        "Document: \"Location recommendation for location-based social networks. In this paper, we study the research issues in realizing location recommendation services for large-scale location-based social networks, by exploiting the social and geographical characteristics of users and locations/places. Through our analysis on a dataset collected from Foursquare, a popular location-based social networking system, we observe that there exists strong social and geospatial ties among users and their favorite locations/places in the system. Accordingly, we develop a friend-based collaborative filtering (FCF) approach for location recommendation based on collaborative ratings of places made by social friends. Moreover, we propose a variant of FCF technique, namely Geo-Measured FCF (GM-FCF), based on heuristics derived from observed geospatial characteristics in the Foursquare dataset. Finally, the evaluation results show that the proposed family of FCF techniques holds comparable recommendation effectiveness against the state-of-the-art recommendation algorithms, while incurring significantly lower computational overhead. Meanwhile, the GM-FCF provides additional flexibility in tradeoff between recommendation effectiveness and computational overhead.\"",
        "Document: \"Spatial search for K diverse-near neighbors. To many location-based service applications that prefer diverse results, finding locations that are spatially diverse and close in proximity to a query point (e.g., the current location of a user) can be more useful than finding the k nearest neighbors/locations. In this paper, we investigate the problem of searching for the k Diverse-Near Neighbors (kDNNs)} in spatial space that is based upon the spatial diversity and proximity of candidate locations to the query point. While employing a conventional distance measure for proximity, we develop a new and intuitive diversity metric based upon the variance of the angles among the candidate locations with respect to the query point. Accordingly, we create a dynamic programming algorithm that finds the optimal kDNNs. Unfortunately, the dynamic programming algorithm, with a time complexity of O(kn3), incurs excessive computational cost. Therefore, we further propose two heuristic algorithms, namely, Distance-based Browsing (DistBrow) and Diversity-based Browsing (DivBrow) that provide high effectiveness while being efficient by exploring the search space prioritized upon the proximity to the query point and spatial diversity, respectively. Using real and synthetic datasets, we conduct a comprehensive performance evaluation. The results show that DistBrow and DivBrow have superior effectiveness compared to state-of-the-art algorithms while maintaining high efficiency.\"",
        "Document: \"Monitoring minimum cost paths on road networks. On a road network, the minimum cost path (or min-cost path for short) from a source location to a destination is a path with the smallest travel cost among all possible paths. Despite that min-cost path queries on static networks have been well studied, the problem of monitoring min-cost paths on a road network in presence of updates is not fully explored. In this paper, we present PathMon, an efficient system for monitoring min-cost paths in dynamic road networks. PathMon addresses two important issues of the min-cost path monitoring problem, namely, (i) path invalidation that identifies min-cost paths returned to path queries affected by network changes, and (ii) path update that replaces invalid paths with new ones for those affected path queries. For (i), we introduce the notion of query scope, based on which a query scope index (QSI) is developed to identify affected path queries. For (ii), we devise a partial path computation algorithm (PPCA) to quickly recompute the updated paths. Through a comprehensive performance evaluation by simulation, QSI and PPCA are demonstrated to be effective on the path invalidation and path update issues.\"",
        "Document: \"The Design and Evaluation of Task Assignment Algorithms for GWAP-based Geospatial Tagging Systems. Geospatial tagging (geotagging) is an emerging and very promising application that can help users find a wide variety of location-specific information, and thereby facilitate the development of advanced location-based services. Conventional geotagging systems share some limitations, such as the use of a two-phase operating model and the tendency to tag popular objects with simple contexts. To address these problems, a number of geotagging systems based on the concept of `Games with a Purpose' (GWAP) have been developed recently. In this study, we use analysis to investigate these new systems. Based on our analysis results, we design three metrics to evaluate the system performance, and develop five task assignment algorithms for GWAP-based systems. Using a comprehensive set of simulations under both synthetic and realistic mobility scenarios, we find that the Least-Throughput-First Assignment algorithm (LTFA) is the most effective approach because it can achieve competitive system utility, while its computational complexity remains moderate. We also find that, to improve the system utility, it is better to assign as many tasks as possible in each round. However, because players may feel annoyed if too many tasks are assigned at the same time, it is recommended that multiple tasks be assigned one by one in each round in order to achieve higher system utility.\"",
        "1 is \"Efficient subgraph similarity search on large probabilistic graph databases\", 2 is \"The K-D-B-tree: a search structure for large multidimensional dynamic indexes\"",
        "Given above information, for an author who has written the paper with the title \"Visible Reverse k-Nearest Neighbor Queries\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00984": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Nonstationary kernel combination':",
        "Document: \"SVM-Fold: a tool for discriminative multi-class protein fold and superfamily recognition. Predicting a protein's structural class from its amino acid sequence is a fundamental problem in computational biology. Much recent work has focused on developing new representations for protein sequences, called string kernels, for use with support vector machine (SVM) classifiers. However, while some of these approaches exhibit state-of-the-art performance at the binary protein classification problem, i.e. discriminating between a particular protein class and all other classes, few of these studies have addressed the real problem of multi-class superfamily or fold recognition. Moreover, there are only limited software tools and systems for SVM-based protein classification available to the bioinformatics community.We present a new multi-class SVM-based protein fold and superfamily recognition system and web server called SVM-Fold, which can be found at http://svm-fold.c2b2.columbia.edu. Our system uses an efficient implementation of a state-of-the-art string kernel for sequence profiles, called the profile kernel, where the underlying feature representation is a histogram of inexact matching k-mer frequencies. We also employ a novel machine learning approach to solve the difficult multi-class problem of classifying a sequence of amino acids into one of many known protein structural classes. Binary one-vs-the-rest SVM classifiers that are trained to recognize individual structural classes yield prediction scores that are not comparable, so that standard \"one-vs-all\" classification fails to perform well. Moreover, SVMs for classes at different levels of the protein structural hierarchy may make useful predictions, but one-vs-all does not try to combine these multiple predictions. To deal with these problems, our method learns relative weights between one-vs-the-rest classifiers and encodes information about the protein structural hierarchy for multi-class prediction. In large-scale benchmark results based on the SCOP database, our code weighting approach significantly improves on the standard one-vs-all method for both the superfamily and fold prediction in the remote homology setting and on the fold recognition problem. Moreover, our code weight learning algorithm strongly outperforms nearest-neighbor methods based on PSI-BLAST in terms of prediction accuracy on every structure classification problem we consider.By combining state-of-the-art SVM kernel methods with a novel multi-class algorithm, the SVM-Fold system delivers efficient and accurate protein fold and superfamily recognition.\"",
        "Document: \"QVALITY: non-parametric estimation of q-values and posterior error probabilities. Qvality is a C program for estimating two types of standard statistical confidence measures: the q-value, which is an analog of the p-value that incorporates multiple testing correction, and the posterior error probability (PEP, also known as the local false discovery rate), which corresponds to the probability that a given observation is drawn from the null distribution. In computing q-values, qvality employs a standard bootstrap procedure to estimate the prior probability of a score being from the null distribution; for PEP estimation, qvality relies upon non-parametric logistic regression. Relative to other tools for estimating statistical confidence measures, qvality is unique in its ability to estimate both types of scores directly from a null distribution, without requiring the user to calculate p-values.\"",
        "Document: \"Learning gene functional classifications from multiple data types. In our attempts to understand cellular function at the molecular level, we must be able to synthesize information from disparate types of genomic data. We consider the problem of inferring gene functional classifications from a heterogeneous data set consisting of DNA microarray expression measurements and phylogenetic profiles from whole-genome sequence comparisons. We demonstrate the application of the support vector machine (SVM) learning algorithm to this functional inference task. Our results suggest the importance of exploiting prior information about the heterogeneity of the data. In particular, we propose an SVM kernel function that is explicitly heterogeneous. In addition, we describe feature scaling methods for further exploiting prior knowledge of heterogeneity by giving each data type different weights.\"",
        "Document: \"Support vector machine classification on the web. The support vector machine (SVM) learning algorithm has been widely applied in bioinformatics. We have developed a simple web interface to our implementation of the SVM algorithm, called Gist. This interface allows novice or occasional users to apply a sophisticated machine learning algorithm easily to their data. More advanced users can download the software and source code for local installation. The availability of these tools will permit more widespread application of this powerful learning algorithm in bioinformatics.\"",
        "Document: \"Automated Validation of Polymerase Chain Reaction Amplicon Melting Curves. Abstract PCR, the polymerase chain reaction, is a fundamental tool of molecular biology. Quantitative PCR is the gold- standard methodology for determination of DNA copy num- bers, quantitating transcription, and numerous other ap- plications. A major barrier to large-scale application of PCR for quantitative genomic analyses is the current re- quirement for manual,validation of individual PCR reac- tions to ensure generation of a single product. This typically requires visual inspection either of gel electrophoreses or temperature dissociation (\u00ecmelting\u00ee) curves of individual PCR reactions\u00f3a time-consuming and costly process. Here we describe a robust computational solution to this fundamental problem. Using a training set of 10,080 reac- tions comprising multiple quantitative PCR reactions from each of 1,728 unique human genomic amplicons, we de- veloped a support vector machine classier capable of dis-\"",
        "1 is \"Toward Optimal Feature Selection\", 2 is \"PISCES: a protein sequence culling server.\"",
        "Given above information, for an author who has written the paper with the title \"Nonstationary kernel combination\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00989": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Impact of research on practice in the field of inspections, reviews and walkthroughs: learning from successful industrial uses':",
        "Document: \"Experimentation as a vehicle for software technology transfer-A family of software reading techniques. This paper presents a short report on the invited lecture given by Dr. H.D. Rombach at the EASE-97 conference. In this lecture Dr. Rombach described his view of the importance of experimentation to the introduction of new techniques and methods. He demonstrated how a series of experimental activities were used to take a reading technique (i.e. reading by stepwise abstraction) from initial conception into widespread use. In providing this report, I will use a selection of the information from the slides presented by Dr. Rombach, linked by my own summaries. As such any misrepresentations of Dr. Rombach's views are my mistakes.\"",
        "Document: \"Optimizing cost and quality by integrating inspection and test processes. Inspections and testing are two of the most commonly performed software quality assurance processes today. Typically, these processes are applied in isolation, which, however, fails to exploit the benefits of systematically combining and integrating them. Expected benefits of such process integration are higher defect detection rates or reduced quality assurance effort. Moreover, when conducting testing without any prior information regarding the system's quality, it is often unclear which parts or which defect types should be prioritized. Existing approaches do not explicitly use information from inspections in a systematical way to focus testing processes. In this article, we present an integrated two-stage approach that routes inspection data to test processes in order to prioritize code classes and defect types. While an initial version of the approach focused on prioritizing code classes, this article focuses on the prioritization of defect types for testing. Results from a case study where the approach was applied on the code level show that those defect types could be prioritized before the testing that afterwards actually showed up most often during the test process. In addition, an overview of related work and an outlook on future research directions are given.\"",
        "Document: \"The Role of Measurement in ISEEs. The main objective of software engineering is to support the development of quality software in a cost-effective way. It is long agreed within the software engineering community that more effective software processes and more effective automated support via integrated software engineering environments (ISEEs) are needed. The TAME ISEE project at the University of Maryland is based on the assumption that there is a basically experimental nature to software development. As such we need to treat software development projects as experiments from which we can learn and improve the way in which we develop software. Learning and improvement require a development model which not only addresses the construction of software products, but also the planning of the construction processes, the control of the construction processes, and the necessary learning from each project in order do it better next time. I present the improvement-oriented software development model which has been developed as part of the TAME project, and suggest that future ISEEs should be instantiations of this model. I develop a scheme for classifying ISEEs and survey five current ISEE research projects. Finally, I list several (mainly measurement-oriented) ISEE requirements and demonstrate how these are being addressed in our first prototype TAME system.\"",
        "Document: \"Software inspections, reviews & walkthroughs. Presents some of the history of software inspections, walkthroughs and reviews. This shows that inspections are related to research efforts back in the 1970s. An example success story is briefly described to illustrate how research has impacted industrial software development practice in this area. The success story was the result of researchers and practitioners working closely together. Finally, challenges and questions as well as areas for further work are outlined. As a summary statement, one can say that, in the inspection area, research did have and still has impact on industrial practice.\"",
        "Document: \"A taxonomy for combining software engineering and human-computer interaction measurement approaches: towards a common framework. The rapid development of any field of knowledge brings with it unavoidable fragmentation and proliferation of new disciplines. The development of computer science is no exception. Software engineering (SE) and human-computer interaction (HCI) are both relatively new disciplines of computer science. Furthermore, as both names suggest, they each have strong connections with other subjects. SE is concerned with methods and tools for general software development based on engineering principles. This discipline has its roots not only in computer science but also in a number of traditional engineering disciplines. HCI is concerned with methods and tools for the development of human-computer interfaces, assessing the usability of computer systems and with broader issues about how people interact with computers. It is based on theories about how humans process information and interact with computers, other objects and other people in the organizational and social contexts in which computers are used. HCI draws on knowledge and skills from psychology, anthropology and sociology in addition to computer science. Both disciplines need ways of measuring how well their products and development processes fulfil their intended requirements. Traditionally, SE has been concerned with \"how software is constructed\" and HCI with \"how people use software\". Given the different histories of the disciplines and their different objectives, it is not surprising that they take different approaches to measurement, Thus, each has its own distinct \"measurement culture\". In this paper we analyse the differences and the commonalities of the two cultures by examining the measurement approaches used by each. We then argue the need for a common measurement taxonomy and framework, which is derived from our analyses of the two disciplines. Next we demonstrate the usefulness of the taxonomy and framework via specific example studies drawn from our own work and that of others and show that, in fact, the two disciplines have many important similarities as well as differences and that there is some evidence to suggest that they are growing closer. Finally, we discuss the role of the taxonomy as a framework to support: reuse, planning future studies, guiding practice and facilitating communication between the two disciplines.\"",
        "1 is \"Data Mining for Very Busy People\", 2 is \"Software Defect Reduction Top 10 List\"",
        "Given above information, for an author who has written the paper with the title \"Impact of research on practice in the field of inspections, reviews and walkthroughs: learning from successful industrial uses\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001050": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Distinctive Features Should Be Learned':",
        "Document: \"Context-aware search using cooperative agents in a smart environment. In this paper we present the design of a decentralized vision-based object search system that can be used for elder care in a smart environment. In our approach, each autonomous search agent maintains separate estimates of the probability density function (PDF) of the object location and makes independent decisions about its search process. Asynchronous cooperative search is achieved by transmitting perceptual information among the agents. Our work also investigates how context such as the detection history and density of activity by people influence the estimation of the prior PDF of the target and the use of this information to improve the search efficiency. Our experimental results demonstrate that the proposed cooperative search strategy is efficient and the methods we use to incorporate contextual information into the target's posterior PDF can improve the efficiency further.\"",
        "Document: \"Tracing Patterns and Attention: Humanoid Robot Cognition. As robots increasingly become part of our everyday lives, programming and interacting with them must become simpler and more natural. This article describes an approach for structuring and controlling complex robotic systems, focusing on humanoid robots, ...\"",
        "Document: \"Learning Generalizable Control Programs. In this paper, we present a framework for guiding autonomous learning in robot systems. The paradigm we introduce allows a robot to acquire new skills according to an intrinsic motivation function that finds behavioral affordances. Affordances\u2014in the sense of (Gibson, Toward and Ecological Psychology, Hillsdale, NJ, 1977)\u2014describe the latent possibilities for action in the environment and provide a direct means of organizing functional knowledge in embodied systems. We begin by showing how a robot can assemble closed-loop action primitives from its sensory and motor resources, and then show how these primitives can be sequenced into multi-objective policies. We then show how these policies can be assembled hierarchically to support incremental and cumulative learning. The main contribution of this paper demonstrates how the proposed intrinsic motivator for affordance discovery can cause a robot to both acquire such hierarchical policies using reinforcement learning and then to generalize these policies to new contexts. As the framework is described, its effectiveness and applicability is demonstrated through a longitudinal learning experiment on a bimanual robot.\"",
        "Document: \"Static Analysis Of Contact Forces With A Mobile Manipulator. Most mobile robots in existence today have large, heavy, statically stable bases. To enable manipulation, actuated arms and effectors are attached to the base providing limited postural control. On the other hand, some of the latest developments in humanoid robotics have succeeded in reducing the footprint and raising the center of mass of a robot using whole body postural control and balancing. By comparing the magnitude of various forces that can be applied to the environment, we demonstrate the advantages of whole body postural control. The results presented suggest that for pushing, pulling, or carrying types of tasks, using whole body postural control can lead to higher performance by allowing a platform to apply more force to the environment.\"",
        "Document: \"User intentions funneled through a human-robot interface. We describe a method for predicting user intentions as part of a human-robot interface. In particular, we show that funnels, i.e., geometric objects that partition an input space, provide a convenient means for discriminating individual objects and for clustering sets of objects for hierarchical tasks. One advantage of the proposed implementation is that a simple parametric model can be used to specify the shape of a funnel, and a straightforward heuristic for setting initial parameter values appears promising. We discuss the possibility of adapting the user interface with machine learning techniques, and we illustrate the approach with a humanoid robot performing a variation of a standard peg-insertion task.\"",
        "1 is \"Constrained Parametric Min-Cuts for Automatic Object Segmentation.\", 2 is \"Wireless Sensor Networks for Habitat Monitoring\"",
        "Given above information, for an author who has written the paper with the title \"Distinctive Features Should Be Learned\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001059": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A Cross-Layer Optimization Framework for Multihop Multicast in Wireless Mesh Networks':",
        "Document: \"Probing-based anypath forwarding routing algorithms in wireless mesh networks. Existing routing protocols for Wireless Mesh Networks (WMNs) are generally optimized with statistical link measures, without focusing on the intrinsic uncertainty of wireless links. We show evidence that, with the transient link uncertainties at PHY and MAC layers, a pseudo-deterministic routing protocol that relies on average or historic statistics can hardly explore the full potentials of a multi-hop wireless mesh. We study optimal WMN routing using probing-based anypath forwarding, with explicit consideration of transient link uncertainties. Starting from a two-state link capacity model, we show the underlying connection between WMN routing and the classic Canadian Traveller Problem (CTP) [1]. Inspired by a stochastic recoverable version of CTP (SRCTP), we develop an practical SRCTP-based online routing algorithm under link uncertainties. We study how dynamic next-hop selection can be done with low cost, and derive a systematic selection order for minimizing transmission delay. We further extend our solution to a general multi-rate link model, and present a Stopping Theory (ST) based solution, which naturally degrades to the SRCTP algorithm in the two-state link model. We conduct simulation studies to verify the effectiveness of the SRCTP and ST algorithms under diverse network configurations. In particular, compared to deterministic routing, reduction of end-to-end delay (51.15-73.02% for two-state links, 5.16% for multi-rate links) and improvement on packet delivery ratio (99.76% for two-state links, 94.44% for multi-rate links) are observed. By using RTS/CTS as the online probing tool in ns3 simulator, we observed both significant goodput improvements (29.9% than EXOR, 61.8% than HWMP) and much less packet arriving jitter (14.27 times less than EXOR, 45% less than HWMP) as compared to EXOR and HWMP routing protocols.\"",
        "Document: \"An Auction Approach to Spectrum Management in HetNets. The growing demand in mobile Internet access calls for high capacity and energy efficient cellular access with better cell coverage. The in-band relaying solution, proposed in LTE-Advanced, improves coverage without requiring additional spectrum for backhauling, making its deployment more economical and practical. However, in-band relay without careful management incurs low spectrum utilization and reduces the system capacity. We propose auction-based solutions that aim at dynamic spectrum resource sharing, maximizing the utilization of precious spectrum resources. We first present a truthful auction that ensures a theoretical performance guarantee in terms of social welfare. Then in an extended system model that focuses on addressing the heterogeneity of resource blocks, we design a more practical auction mechanism. We implement our proposed auctions under large scale real-world settings. Simulation results verify the efficacy of proposed auctions, showing improvements in both cell coverage and spectrum efficiency.\"",
        "Document: \"Novel Criteria for Deterministic Remote State Preparation via the Entangled Six-Qubit State. In this paper, our concern is to design some criteria for deterministic remote state preparation for preparing an arbitrary three-particle state via a genuinely entangled six-qubit state. First, we put forward two schemes in both the real and complex Hilbert space, respectively. Using an appropriate set of eight-qubit measurement basis, the remote three-qubit preparation is completed with unit success probability. Departing from previous research, our protocol has a salient feature in that the serviceable measurement basis only contains the initial coefficients and their conjugate values. By utilizing the permutation group, it is convenient to provide the permutation relationship between coefficients. Second, our ideas and methods can also be generalized to the situation of preparing an arbitrary N-particle state in complex case by taking advantage of Bell states as quantum resources. More importantly, criteria satisfied conditions for preparation with 100% success probability in complex Hilbert space is summarized. Third, the classical communication costs of our scheme are calculated to determine the classical recourses required. It is also worth mentioning that our protocol has higher efficiency and lower resource costs compared with the other papers.\"",
        "Document: \"The flattening internet topology: natural evolution, unsightly barnacles or contrived collapse?. In this paper we collect and analyze traceroute measurements to show that large content providers (e.g., Google, Microsoft, Yahoo!) are deploying their own wide-area networks, bringing their networks closer to users, and bypassing Tier-1 ISPs on many paths. This trend, should it continue and be adopted by more content providers, could flatten the Internet topology, and may result in numerous other consequences to users, Internet Service Providers (ISPs), content providers, and network researchers.\"",
        "Document: \"Demand Response in Smart Grids: A Randomized Auction Approach. The smart grid is a modern power grid that achieves high efficiency and robustness through sophisticated information and communications technology. Demand response has great potential in helping balance demand and supply in a smart grid, cutting generation cost and carbon footprint, and improving system stability. Auctions represent a natural and efficient approach for carrying out demand response between the power grid and large electricity users, microgrids, and electricity storage devices. This work explores the modeling and design space of demand response auctions, targeting expressive power, truthful information revelation, computational efficiency, and economic efficiency. We present a randomized auction that explores the underlying problem structure of demand response, and prove that it is truthful, runs in polynomial time, and achieves (1 + )-optimal social cost for an arbitrarily small constant . The key technique lies in the marriage of smoothed analysis and randomized reduction, which makes its debut in this work among literature on mechanism design, and can be applied to problems where social welfare optimization is NP-hard but admits a smoothed polynomial-time algorithm.\"",
        "1 is \"Low complexity resource allocation with opportunistic feedback over downlink OFDMA networks\", 2 is \"Combinatorial Auction-Based Dynamic VM Provisioning and Allocation in Clouds\"",
        "Given above information, for an author who has written the paper with the title \"A Cross-Layer Optimization Framework for Multihop Multicast in Wireless Mesh Networks\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001097": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Lattice-based sums':",
        "Document: \"Sorted Multi-adjoint Logic Programs: Termination Results and Applications. A general framework of logic programming allowing for the combination of several adjoint lattices of truth-values is presented. The main contribution is a new sufficient condition which guarantees termination of all queries for the fixpoint semantics for an interesting class of programs. Several extensions of these conditions are presented and related to some well-known formalisms for probabilistic logic programming.\"",
        "Document: \"FCA Attribute Reduction in Information Systems. One of the main targets in formal concept analysis (FCA) and in rough set theory (RST) is the reduction of redundant information. Feature selection mechanisms have been studied separately in many works. In this paper, we analyse the result of applying the reduction mechanisms given in FCA to RST, and give interpretations of such reductions.\"",
        "Document: \"Syntax and semantics of multi-adjoint normal logic programming. Multi-adjoint logic programming is a general framework with interesting features, which involves other positive logic programming frameworks such as monotonic and residuated logic programming, generalized annotated logic programs, fuzzy logic programming and possibilistic logic programming. One of the most interesting extensions of this framework is the possibility of considering a negation operator in the logic programs, which will improve its flexibility and the range of real applications. This paper introduces multi-adjoint normal logic programming, which is an extension of multi-adjoint logic programming including a negation operator in the underlying lattice. Beside the introduction of the syntax and semantics of this paradigm, we will provide sufficient conditions for the existence of stable models defined on a convex compact set of an euclidean space. Finally, we will consider a particular algebraic structure in which sufficient conditions can be given in order to ensure the unicity of stable models of multi-adjoint normal logic programs.\"",
        "Document: \"Multi-adjoint Logic Programming with Continuous Semantics. Considering different implication operators, such as \"\u0141ukasiewicz, G\u00f6del or product implication in the same logic program, naturally leads to the allowance of several adjoint pairs in the lattice of truthvalues. In this paper we apply this idea to introduce multi-adjoint logic programs as an extension of monotonic logic programs. The continuity of the immediate consequences operators is proved and the assumptions required to get continuity are further analysed.\"",
        "Document: \"Multi-lattices as a basis for generalized fuzzy logic programming. A prospective study of the use of ordered multi-lattices as underlying sets of truth-values for a generalised framework of logic programming is presented. Specifically, we investigate the possibility of using multi-lattice-valued interpretations of logic programs and the theoretical problems that this generates with regard to its fixed point semantics.\"",
        "1 is \"A value for bi-cooperative games\", 2 is \"Fuzzy Logic Programming for Implementing a Flexible XPath-based Query Language\"",
        "Given above information, for an author who has written the paper with the title \"Lattice-based sums\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001098": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Brief Announcement: Communication-Efficient Self-stabilizing Protocols for Spanning-Tree Construction':",
        "Document: \"On the Probabilistic Omission Adversary. This paper newly proposes a novel round-based synchronous system suffering crash and probabilistic omission failures. In this\n model, a novel class of adversaries, called p-probabilistic omission adversary (p-POA) is introduced. In addition to the ability of complete control of the crash-failure behavior, p-POA can select any subset of all transmitted messages as omission candidates. Then, each message in the omission candidates is lost with probability p. This paper investigates the feasiblity and complexity of the consensus problem under p-POA. We first show two impossibility results that (1) for any p\u2009>\u20090, there exists no uniform consensus algorithm tolerating more than or equal to n/2 crash failures, and that (2) for any p\u2009>\u20090, any uniform consensus algorithm cannot halt. We also show two consensus algorithms CPO and F-CPO. Both algorithms work under (1/2)-POA and respectively have distinct advantages. The algorithm CPO can tolerate at most n/2\u2009\u2212\u20091 crash failures and achieves O(f) expected round complexity, where f is the actual number of crash failures. This implies that CPO has maximum crash-failure resiliency. While the second algorithm F-CPO assumes the maximum number of crash failures less than n/3, it achieves f\u2009+\u2009O(1) round compexity in expectation. Since it is known that the lower bound for crash-tolerant consensus is f\u2009+\u20091, this result implies that only a constant number of extra rounds is nessesary to tolerate a drastic number of message\n omissions.\n \"",
        "Document: \"Elimination Techniques of Redundant Data Transfers Among GPUs and CPU on Recursive Stream-Based Applications. Applying the stream-based computing approach, the general purpose computing on graphics processing units has become to be considered as a breakthrough to overcome the performance bottleneck as seen in the recent CPU architecture. However, the program potentially includes the data transfer overhead if it has recursive I/Os. During the recursive operation in the GPU-based program, the output streams are copied to the input ones and this overhead degrades the performance. This paper proposes the best method to eliminate the transfer overheads and shows design and implementation of the method based on CUDA and OpenCL. The experimental evaluation using realistic applications shows the method eliminates the transfer overhead and the method exploits the potential performance of GPU.\"",
        "Document: \"Highly fault-tolerant routings and fault-induced diameter for generalized hypercube graphs. Consider a communication network G in which a limited number of link and/or node faults F might occur. A routing \u03c1 for the network (a fixed path between each pair of nodes) must be chosen without knowing which components might become faulty. The diameter of the surviving route graph R ( G , \u03c1)/ F , where the surviving route graph R ( G , \u03c1)/ F is a directed graph consisting of all nonfaulty nodes in G with a directed edge from x to y iff there are no faults on the route from x to y , could be one of the fault-tolerant measures for the routing \u03c1. In this paper, we show that we can construct efficient and highly fault-tolerant routings on a k -dimensional generalized d -hypercube C ( d , k ) such that the diameter of the surviving route graph is bounded by constant for the case that the number of faults exceeds the connectivity of C ( d , k ).\"",
        "Document: \"Optimal Fault-Tolerant ATM-Routings for Biconnected Graphs. We study the problem of designing fault-tolerant virtual path layouts for an ATM network which is a biconnected network of n processors in the surviving route graph model. The surviving route graph for a graph G, a routing p and a set of faults F is a directed graph consisting of nonfaulty nodes with a directed edge from a node x to a node y iff there are no faults on the route from x to y. The diameter of the surviving route graph could be one of the fault-tolerance measures for the graph G and the routing p. When a routing is considered as a virtual path layout, we can discuss the fault tolerance of virtual path layouts in the ATM network. In this paper, we show that we construct three routings for any biconnected graph such that the diameter of the surviving route graphs is optimal and they satisfy some desirable properties of virtual path layouts in ATM networks.\"",
        "Document: \"Performance evaluation of offloading software modules to cluster network. A design of software to offload user-defined software modules to Maestro2 cluster network, named Maestro dynamic offloading mechanism(MDO), and results of its performance evaluation are described. Maestro2 is a high-performance network for clusters. The network interface and the switch of Maestro2 have a general-purpose processor tightly coupled with dedicated communication hard-ware. MDO enables the users to offload software modules to both the network interface and the switch. The effectiveness of MDO are discussed through the evaluation of offloading collective communications and parallel Gauss-Jordan elimination benchmark.\"",
        "1 is \"A single source shortest path algorithm for a planar distributed network\", 2 is \"Fast polygonal approximation of digitized curves\"",
        "Given above information, for an author who has written the paper with the title \"Brief Announcement: Communication-Efficient Self-stabilizing Protocols for Spanning-Tree Construction\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001114": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'High-dimensional integration: The quasi-Monte Carlo way':",
        "Document: \"THE ANOVA DECOMPOSITION OF A NON-SMOOTH FUNCTION OF INFINITELY MANY VARIABLES CAN HAVE EVERY TERM SMOOTH. The pricing problem for a continuous path-dependent option results in a path integral which can be recast into an infinite-dimensional integration problem. We study ANOVA decomposition of a function of infinitely many variables arising from the Brownian bridge formulation of the continuous option pricing problem. We show that all resulting ANOVA terms can be smooth in this infinite-dimensional case, despite the non-smoothness of the underlying payoff function. This result may explain why quasi-Monte Carlo methods or sparse grid quadrature techniques work for such option pricing problems.\"",
        "Document: \"Parameter Choice Strategies for Least-squares Approximation of Noisy Smooth Functions on the Sphere. We consider a polynomial reconstruction of smooth functions from their noisy values at discrete nodes on the unit sphere by a variant of the regularized least-squares method of An et al. [SIAM J. Numer. Anal., 50 (2012), pp. 1513-1534]. As nodes we use the points of a positive-weight cubature formula that is exact for all spherical polynomials of degree up to 2M, where M is the degree of the reconstructing polynomial. We first obtain a reconstruction error bound in terms of the regularization parameter and the penalization parameters in the regularization operator. Then we discuss a priori and a posteriori strategies for choosing these parameters. Finally, we give numerical examples illustrating the theoretical results.\"",
        "Document: \"Good Lattice Rules in Weighted Korobov Spaces with General Weights. We study the problem of multivariate integration and the construction of good lattice rules in weighted Korobov spaces with general weights. These spaces are not necessarily tensor products of spaces of univariate functions. Sufficient conditions for tractability and strong tractability of multivariate integration in such weighted function spaces are found. These conditions are also necessary if the weights are such that the reproducing kernel of the weighted Korobov space is pointwise non-negative. The existence of a lattice rule which achieves the nearly optimal convergence order is proven. A component-by-component (CBC) algorithm that constructs good lattice rules is presented. The resulting lattice rules achieve tractability or strong tractability error bounds and achieve nearly optimal convergence order for suitably decaying weights. We also study special weights such as finite-order and order-dependent weights. For these special weights, the cost of the CBC algorithm is polynomial. Numerical computations show that the lattice rules constructed by the CBC algorithm give much smaller worst-case errors than the mean worst-case errors over all quasi-Monte Carlo rules or over all lattice rules, and generally smaller worst-case errors than the best Korobov lattice rules in dimensions up to hundreds. Numerical results are provided to illustrate the efficiency of CBC lattice rules and Korobov lattice rules (with suitably chosen weights), in particular for high-dimensional finance problems.\"",
        "Document: \"Multivariate integration for analytic functions with Gaussian kernels. We study multivariate integration of analytic functions defined on R-d. These functions are assumed to belong to a reproducing kernel Hilbert space whose kernel is Gaussian, with nonincreasing shape parameters. We prove that a tensor product algorithm based on the univariate Gauss-Hermite quadrature rules enjoys exponential convergence and computes an epsilon-approximation for the d-variate integration using an order of (ln epsilon(-1))(d) function values as epsilon goes to zero. We prove that the exponent d is sharp by proving a lower bound on the minimal (worst case) error of any algorithm based on finitely many function values. We also consider four notions of tractability describing how the minimal number n(epsilon, d) of function values needed to find an epsilon-approximation in the d-variate case behaves as a function of d and ln epsilon(-1). One of these notions is new. In particular, we prove that for all positive shape parameters, the minimal number n(epsilon, d) is larger than any polynomial in d and ln epsilon(-1) as d and epsilon(-1) go to infinity. However, it is not exponential in d(t) and ln epsilon(-1) whenever t > 1.\"",
        "Document: \"Tractability of Approximation for Weighted Korobov Spaceson Classical and Quantum Computers.  \nWe study the approximation problem (or problem of optimal recovery in the\n$L_2$-norm) for weighted Korobov spaces with smoothness\nparameter $\\a$. The weights $\\gamma_j$ of the Korobov spaces moderate\nthe behavior of periodic functions with respect to successive variables.\nThe nonnegative smoothness parameter $\\a$ measures the decay\nof Fourier coefficients. For $\\a=0$, the Korobov space is the\n$L_2$ space, whereas for positive $\\a$, the Korobov space\nis a space of periodic functions with some smoothness \nand the approximation problem\ncorresponds to a compact operator. The periodic functions are defined on \n$[0,1]^d$ and our main interest is when the dimension $d$ varies and\nmay be large. We consider algorithms using two different \nclasses of information.\nThe first class $\\lall$ consists of arbitrary linear functionals.\nThe second class $\\lstd$ consists of only function values\nand this class is more realistic in practical computations.\n\nWe want to know when the approximation problem is\ntractable. Tractability means that there exists an algorithm whose error\nis at most $\\e$ and whose information cost is bounded by a polynomial \nin the dimension $d$ and in $\\e^{-1}$. Strong tractability means that \nthe bound does not depend on $d$ and is polynomial in $\\e^{-1}$. \nIn this paper we consider the worst case, randomized, and quantum\nsettings. In each setting, the concepts of error and cost are defined\ndifferently and,  therefore, tractability and strong tractability \ndepend on the setting and on the class of information. \n\nIn the worst case setting, we apply known results to prove\nthat strong tractability and tractability in the class $\\lall$ \nare equivalent. This holds\nif and only if $\\a>0$ and the sum-exponent $s_{\\g}$ of weights is finite, where \n$s_{\\g}= \\inf\\{s>0 : \\xxsum_{j=1}^\\infty\\g_j^s\\,<\\,\\infty\\}$.\n\nIn the worst case setting for the class $\\lstd$ we must assume \nthat $\\a>1$ to guarantee that\nfunctionals from $\\lstd$ are continuous. The notions of strong\ntractability and tractability are not equivalent. In particular,\nstrong tractability holds if and only if $\\a>1$ and\n$\\xxsum_{j=1}^\\infty\\g_j<\\infty$.\n\nIn the randomized setting, it is known that randomization does not\nhelp over the worst case setting in the class $\\lall$. For the class\n$\\lstd$, we prove that strong tractability and tractability\nare equivalent and this holds under the same assumption\nas for the class $\\lall$ in the worst case setting, that is,\nif and only if $\\a>0$ and $s_{\\g} < \\infty$.\n\nIn the quantum setting, we consider only upper bounds for the class\n$\\lstd$ with $\\a>1$. We prove that $s_{\\g}<\\infty$ implies strong\ntractability. \n\nHence for $s_{\\g}>1$, the randomized and quantum settings\nboth break worst case intractability of approximation for\nthe class $\\lstd$.\n \nWe indicate cost bounds on algorithms with error at\nmost $\\e$. Let $\\cc(d)$ denote the cost of computing $L(f)$ for\n$L\\in \\lall$ or $L\\in \\lstd$, and let the cost of one arithmetic \noperation be taken as unity. \nThe information cost bound in the worst case setting for the \nclass $\\lall$ is of order $\\cc (d) \\cdot \\e^{-p}$\nwith $p$ being roughly equal to $2\\max(s_\\g,\\a^{-1})$.\nThen for the class $\\lstd$\nin the randomized setting, \nwe present an algorithm with error at most $\\e$ and whose total cost is \nof order $\\cc(d)\\e^{-p-2} + d\\e^{-2p-2}$, which for small $\\e$ is roughly\n$$\nd\\e^{-2p-2}.\n$$ \nIn the quantum setting, we present a quantum algorithm \nwith error at most $\\e$ that \nuses about only  $d + \\log \\e^{-1}$ qubits\nand whose total cost is of order\n$$\n(\\cc(d) +d)   \\e^{-1-3p/2}.\n$$ \nThe ratio of the costs of the algorithms in the quantum setting and \nthe randomized setting is of order\n$$\n\\frac{d}{\\cc(d)+d}\\,\\left(\\frac1{\\e}\\right)^{1+p/2}.\n$$\nHence, we have a polynomial speedup of order $\\e^{-(1+p/2)}$.\nWe stress that $p$ can be arbitrarily large, and in this case\nthe speedup is huge. \n\n\"",
        "1 is \"Duality theory and propagation rules for higher order nets\", 2 is \"On choosing \"",
        "Given above information, for an author who has written the paper with the title \"High-dimensional integration: The quasi-Monte Carlo way\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001143": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Polynomially graded logic I. A graded version of system T':",
        "Document: \"Breaking and fixing public-key Kerberos. We report on a man-in-the-middle attack on PKINIT, the public key extension of the widely deployed Kerberos 5 authentication protocol. This flaw allows an attacker to impersonate Kerberos administrative principals (KDC) and end-servers to a client, hence breaching the authentication guarantees of Kerberos. It also gives the attacker the keys that the KDC would normally generate to encrypt the service requests of this client, hence defeating confidentiality as well. The discovery of this attack caused the IETF to change the specification of PKINIT and Microsoft to release a security update for some Windows operating systems. We discovered this attack as part of an ongoing formal analysis of the Kerberos protocol suite, and we have formally verified several possible fixes to PKINIT-including the one adopted by the IETF-that prevent our attack as well as other authentication and secrecy properties of Kerberos with PKINIT.\"",
        "Document: \"Bounded memory Dolev-Yao adversaries in collaborative systems. This paper extends existing models for collaborative systems. We in- vestigate how much damage can be done by insiders alone, without collusion with an outside adversary. In contrast to traditional intruder models, such as in protocol security, all the players inside our system, including potential adversaries, have similar capabilities. They have bounded storage capacity, that is, they can only re- member at any moment a bounded number of facts. This is technically imposed by only allowing balanced actions, that is, actions that have the same number of facts in their pre and post conditions. On the other hand, the adversaries inside our system have many capabilities of the standard Dolev-Yao intruder, namely, they are able, within their bounded storage capacity, to compose, decompose, overhear, and intercept messages as well as update values with fresh ones. We in- vestigate the complexity of the decision problem of whether or not an adversary is able to discover secret data. We show that this problem is PSPACE-complete when all actions are balanced and can update values with fresh ones. As an appli- cation we turn to security protocol analysis and demonstrate that many protocol anomalies, such as the Lowe anomaly in the Needham-Schroeder public key ex- change protocol, can also occur when the intruder is one of the insiders with bounded memory.\"",
        "Document: \"Formal Analysis of Multiparty Contract Signing. We analyze the multi-party contract-signing protocols of Garay and MacKenzie (GM) and of Baum and Waidner (BW). We use a finite-state tool, MOCHA, which allows specification of protocol properties in a branching-time temporal logic with game semantics. While our analysis does not reveal any errors in the BW protocol, in the GM protocol we discover serious problems with fairness for four signers and an oversight regarding abuse-freeness for three signers. We propose a complete revision of the GM subprotocols in order to restore fairness.\"",
        "Document: \"Uniform Proofs as a Foundation for Logic Programming. A proof-theoretic characterization of logical languages that form suitable bases for Prolog-like programming languages is provided. This characterization is based on the principle that the declarative meaning of a logic program, provided by provability in a logical system, should coincide with its operational meaning, provided by interpreting logical connectives as simple and fixed search instruments. The operational semantics is formalized by the identification of a class of cut-free sequent proofs called {\\em uniform proofs.} A uniform proof is one that can be found by a goal-directed search that respects the interpretation of the logical connectives as search instructions. The concept of a uniform proof is used to define the notion of an {\\em abstract logic programming language,} and it is shown that first-order and higher-order Horn clauses with classical provability are examples of such a language. Horn clauses are then generalized to {\\em hereditary Harrop formulas} and it is shown that first-order and higher-order versions of this new class of formulas are also abstract logic programming languages if the inference rules are those of either intuitionistic or minimal logic. The programming language significance of the various generalizations to first-order Horn clauses is briefly discussed.\"",
        "Document: \"Relating State-Based and Process-Based Concurrency through Linear Logic. This paper has the purpose of reviewing some of the established relationships between logic and concurrency, and of exploring new ones. Concurrent and distributed systems are notoriously hard to get right. Therefore, following an approach that has proved highly beneficial for sequential programs, much effort has been invested in tracing the foundations of concurrency in logic. The starting points of such investigations have been various idealized languages of concurrent and distributed programming, in particular the well-established state-transformation model inspired to Petri nets and multiset rewriting, and the prolific process-based models such as the @p-calculus and other process algebras. In nearly all cases, the target of these investigations has been linear logic, a formal language that supports a view of formulas as consumable resources. In the first part of this paper, we review some of these interpretations of concurrent languages into linear logic. In the second part of the paper, we propose a completely new approach to understanding concurrent and distributed programming as a manifestation of logic, which yields a language that merges those two main paradigms of concurrency. Specifically, we present a new semantics for multiset rewriting founded on an alternative view of linear logic. The resulting interpretation is extended with a majority of linear connectives into the language of @w-multisets. This interpretation drops the distinction between multiset elements and rewrite rules, and considerably enriches the expressive power of standard multiset rewriting with embedded rules, choice, replication, and more. Derivations are now primarily viewed as open objects, and are closed only to examine intermediate rewriting states. The resulting language can also be interpreted as a process algebra. For example, a simple translation maps process constructors of the asynchronous @p-calculus to rewrite operators, while the structural equivalence corresponds directly to logically-motivated structural properties of @w-multisets (with one exception).\"",
        "1 is \"WAM Algebras - A Mathematical Study of Implementation, Part 2\", 2 is \"Compiling intensional sets in CLP\"",
        "Given above information, for an author who has written the paper with the title \"Polynomially graded logic I. A graded version of system T\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001171": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Supporting and accelerating reproducible empirical research in software evolution and maintenance using TraceLab Component Library':",
        "Document: \"From Requirements Monitoring To Diagnosis Support In System Of Systems. Context and motivation: Complex industrial software systems are often systems of systems (SoS) whose behavior only fully emerges during operation. Techniques such as requirements monitoring thus have to be used to observe such systems at runtime to detect deviations from their requirements. Question/problem: However, the focus of existing monitoring approaches is mainly on detecting violations of expected behavior, while support for subsequent diagnosis of violations is rather limited and often even neglected. Diagnosis is particularly challenging in SoS, which are characterized by complex heterogeneous architectures and a slew of different development and testing tools. Principal ideas/results: In this research preview paper we discuss the required capabilities for diagnosis support in SoS and outline a tool-supported framework based on a runtime artifact model and pre-defined diagnosis actions. Contribution: We describe our ongoing development of the framework and tools for supporting diagnosis in SoS and provide a research agenda.\"",
        "Document: \"Reverse Engineering Product Lines In Agile Environments: Lesson Learned And Challenges. In order to meet competitive market deadlines and to reduce development costs, families of software systems are increasingly developed as Software Product Lines [2]. Adopting agile practices for product-line development brings the promise of faster time-to-market and less costly delivery, while maintaining or even improving safety. Therefore, agile practices are often adopted even though many product lines, such as medical infusion pumps, pacemakers, and flight-control systems, operate in safety-critical domains. This introduces non-trivial risks related to the safe reuse of components across multiple products. The goal is to dynamically compose demonstrably safe products within the constraints of a fast-moving, incrementally delivered project.   This talk describes these challenges, with illustrations drawn from Dronology -- a cyber physical environment for managing and coordinating the flight of Unmanned Aerial Vehicles (UAVs) [3]. Dronology was designed to support UAV-based search-and-rescue, environmental data collection, fire reconnaissance, commercial product delivery and other such applications. It was initially targeted toward river-rescue scenarios; and is currently being reverse engineered into a product line.   Reverse engineering a product line from a single product, especially one with safety implications, is a challenging task [1]. Within the context of an agile project, the goal is to introduce variability points that bring immediate value to the project stakeholders without breaking the system and without sacrificing safety or other system qualities. On the other hand, by strategically looking ahead, the architecture can be extended incrementally to support the desired variability points. To address incremental delivery, feature models can be evolved over time by adding new common and variable features. As features are planned for current or upcoming sprints, corresponding functional, architectural, and safety stories can be added to the backlog. Practical traceability solutions can be employed from early phases of the project to support product line engineering in order to facilitate the metamorphosis from a single product to a full-fledged product line.   These ideas, challenges, and solutions, are presented in this talk, with rich examples drawn from the Dronology system.\"",
        "Document: \"Ready-set-transfer: exploring the technology transfer readiness of academic research projects (panel). Software engineering research is undertaken to propose innovative solutions, to develop concepts, algorithms, processes, and technologies, to validate effective solutions for important software engineering problems, and ultimately to support the transition of important findings to practice. However prior studies have shown that successful projects often take from 20-25 years to reach the stage of full industry adoption, while many other projects fizzle out and never advance beyond the initial research phase. This panel provides the opportunity for practitioners and academics to engage in a meaningful discussion around the topic of technology transfer. In this fourth offering of the Ready-Set-Transfer panel, three research groups will present products that they believe to be industry-ready to a panel of industrial practitioners. Each team will receive feedback from the panelists. The long-term goal of the panel is to increase technology transfer in the software engineering domain.\"",
        "Document: \"A pattern system for tracing architectural concerns. A software architecture is carefully designed to satisfy the quality concerns of its stakeholders, and as such, represents a systematic and intricately balanced set of design decisions which deliver required qualities such as performance, reliability, and safety. In practice, architectural degradation tends to occur over the lifetime of the software system, as developers make ongoing and incremental maintenance changes to the system without knowledge of its underlying design decisions. Fortunately, this problem can be alleviated by establishing traceability between concrete elements in the architecture and their associated design decisions, and then using these traceability links to keep developers informed of relevant architectural tactics, styles, and design patterns throughout the development process. This paper focuses on the task of creating and using such traceability links. We present six trace creation patterns describing techniques and supporting structures for creating architecturally significant traceability links, and two usage patterns describing techniques for using the created links to help preserve qualities in the architectural design. The patterns described in this paper emerged from our experiences and observations of tracing architectural concerns in safety critical systems.\"",
        "Document: \"Supporting the partitioning of distributed systems with Function-Class Decomposition. Function-Class Decomposition is a hybrid method that integrates structured analysis with an object-oriented approach to decompose a system. The task of class identification is performed in parallel to the decomposition of the system into a hierarchy of functional modules. This hierarchy provides the infrastructure for a systematic approach to partitioning components for distribution and for evaluating key attributes of the resulting architecture. Complexity is reduced by the fact that partitioning decisions are made along the boundaries of previously identified groupings. Early evaluation of the resulting component distribution is also a key factor in mitigating the risks associated with developing distributed applications.\"",
        "1 is \"Template-based reconstruction of complex refactorings\", 2 is \"Automatic extraction of semantic relationships for wordnet by means of pattern learning from wikipedia\"",
        "Given above information, for an author who has written the paper with the title \"Supporting and accelerating reproducible empirical research in software evolution and maintenance using TraceLab Component Library\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001257": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Adaptive fuzzy systems for multichannel signal processing':",
        "Document: \"Convergence properties of Gauss-Newton iterative algorithms in nonlinear image restoration. Nonlinear image restoration is a complicated problem that is receiving increasing attention. Since every image formation system involves a built-in nonlinearity, nonlinear image restoration finds applications in a wide variety of research areas. Iterative algorithms have been well established in the corresponding linear restoration problem. In this paper, a generalized analysis regarding the convergence properties of nonlinear iterative algorithms is introduced. Moreover, the applications of the iterative Gauss-Newton (GN) algorithm in nonlinear image restoration are considered. The convergence properties of a general class of nonlinear iterative algorithms are rigorously studied through the Global Convergence Theorem (GCT). The derivation of the convergence properties is based on the eigen-analysis, rather than on the norm analysis. This approach offers a global picture of the evolution and the convergence properties of an iterative algorithm. Moreover, the generalized convergence-analysis introduced may be interpreted as a link towards the integration of minimization and projection algorithms. The iterative GN algorithm for the solution of the least-squares optimization problem is introduced. The computational complexity of this algorithm is enormous, making its implementation very difficult in practical applications. Structural modifications are introduced, which drastically reduce the computational complexity while preserving the convergence rate of the GN algorithm. With the structural modifications, the GN algorithm becomes particularly useful in nonlinear optimization problems. The convergence properties of the algorithms introduced are readily derived, on the basis of the generalized analysis through the GCT. The application of these algorithms on practical problems, is demonstrated through several examples.\"",
        "Document: \"Environment and Movement Model for Mobile Terminal Location Tracking. Mobile terminal position location, tracking and prediction are becomingimportant areas of research for advanced cellular communications.Methods for mobile terminal location are evaluated using simulations. To obtain accurate simulation results, the simulation environment and terminalmotion model must be as realistic as possible. This paper describes a simulation system for mobile terminals located within vehicles in dense urban environments. These are the mobiles with the greatest need for locationpredictions in the environments of greatest interest to network providers. Theradio propagation model is based on well known multipath radio propagationmodels. The motion model combines an accurate kinematic model for vehicular motion with a driver decision model to mimic human driving decisions.Simulatedmobile terminal motion tracks are presented, showing how realistic motionsare generated.\"",
        "Document: \"Query feedback for interactive image retrieval. From a perceptual standpoint, the subjectivity inherent in understanding and interpreting visual content in multimedia indexing and retrieval motivates the need for online interactive learning. Since efficiency and speed are important factors in interactive visual content retrieval, most of the current approaches impose restrictive assumptions on similarity calculation and learning algorithms. Specifically, content-based image retrieval techniques generally assume that perceptually similar images are situated close to each other within a connected region of a given space of visual features. This paper proposes a novel method for interactive image retrieval using query feedback. Query feedback learns the user query as well as the correspondence between high-level user concepts and their low-level machine representation by performing retrievals according to multiple queries supplied by the user during the course of a retrieval session. The results presented in this paper demonstrate that this algorithm provides accurate retrieval results with acceptable interaction speed compared to existing methods.\"",
        "Document: \"Flexible architectures for morphological image processing and analysis. An architecture for the efficient and high-speed realization of morphological filters is presented. Since morphological filtering can be described in terms of erosion and dilation, two basic building units performing these functions are required for the realization of any morphological filter. Dual architectures for erosion and dilation are proposed and their operations are described. Their structure, similar to the systolic array architecture as used in the implementation of linear digital filters, is highly modular and suitable for efficient very-large-scale integration (VLSI) implementation. A decomposition scheme is proposed to facilitate the implementation of two-dimensional morphological filters based on one-dimensional structuring elements constructed using the dual architectures. The proposed architectures, which also allow the processing of gray-scale images, are appropriate for applications where speed, size, and cost are of critical significance\"",
        "Document: \"Regularization studies of linear discriminant analysis in small sample size scenarios with application to face recognition. It is well-known that the applicability of linear discriminant analysis (LDA) to high-dimensional pattern classification tasks such as face recognition often suffers from the so-called ''small sample size'' (SSS) problem arising from the small number of available training samples compared to the dimensionality of the sample space. In this paper, we propose a new LDA method that attempts to address the SSS problem using a regularized Fisher's separability criterion. In addition, a scheme of expanding the representational capacity of face database is introduced to overcome the limitation that the LDA-based algorithms require at least two samples per class available for learning. Extensive experiments performed on the FERET database indicate that the proposed methodology outperforms traditional methods such as Eigenfaces and some recently introduced LDA variants in a number of SSS scenarios.\"",
        "1 is \"Rank filters in digital image processing\", 2 is \"Limits on Super-Resolution and How to Break Them\"",
        "Given above information, for an author who has written the paper with the title \"Adaptive fuzzy systems for multichannel signal processing\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001281": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Centralized Micro-clouds: An Infrastructure for Service Distribution in Collaborative Smart Devices.':",
        "Document: \"Mobile Advertisement in Vehicular Ad-Hoc Networks. Mobile Advertisement is a location-aware dissemination solution built on top of a vehicular ad-hoc network. We envision a network of WiFi access points that dynamically disseminate data to clients running on the car's smart device. The approach can be considered an alternative to the static advertisement billboards and can be useful to business companies wanting to dynamically advertise their products and offers to people driving their car. The clients can subscribe to information based on specific topics. We present design solutions that use access points as emitters for transmitting messages to wireless-enabled devices equipped on vehicles. We also present implementation details for the evaluation of the proposed solution using a simulator designed for VANET application. The results show that the application can be used for transferring a significant amount of data even under difficult conditions, such as when cars are moving at increased speeds, or the congested Wi-Fi network causes significant packet loss.\"",
        "Document: \"Energy Consumption Optimization Using Social Interaction in the Mobile Cloud. This paper addresses the issue of resource offloading for energy usage optimization in the cloud, using the centrality principle of social networks. Mobile users take advantage of the mobile opportunistic cloud, in order to increase their reliability in service provision by guaranteeing sufficient resources for the execution of mobile applications. This work elaborates on the improvement of the energy consumption for each mobile device, by using a social collaboration model that allows for a cooperative partial process offloading scheme. The proposed scheme uses social centrality as the underlying mobility and connectivity model for process offloading within the connected devices to maximize the energy usage efficiency, node availability and process execution reliability. Furthermore, this work considers the impact of mobility on the social-oriented offloading, by allowing partitionable resources to be executed according to the social interactions and the associated mobility of each user during the offloading process. The proposed framework is thoroughly evaluated through event driven simulations, towards defining the validity and offered efficiency of the proposed offloading policy in conjunction to the energy consumption of the wireless devices.\"",
        "Document: \"A Platform to Support Context-Aware Mobile Applications. Today smartphones and tablet PCs are gaining more popularity due to cutting edge technology added on top of wearability, thus creating the scene for context-aware applications that are capable to sense and actively use context to provide the user with valuable information whenever and wherever is needed, even while on the move. In this paper we describe CAPIM, a platform designed to support the construction of such context-aware mobile applications. The platform provides capabilities for sensing and collecting data from sensors and external sources. It includes a layer where the raw context data is aggregated and derived into higher-level information. A dedicated rule execution engine is offered to support context-aware workflows. CAPIM integrates context-aware services that are dynamically configurable and use the user's location, identity, preferences, profile, and relations with individuals, as well as capabilities of the mobile devices to aggregate and semantically organize the context data. They react based on dynamically defined context-oriented workflows. We present the platform's architecture, implementation details, and present case study scenarios which show its potential to handle a variety of context-aware situations. CAPIM is fully functional and can be used in a variety of context-aware situations.\"",
        "Document: \"Reaching for the clouds: contextually enhancing smartphones for energy efficiency. Energy efficiency has gradually become a compulsory need in mobile computing as the processing requirements for smartphones have increased exponentially. Moreover, the current demand is stretching beyond the extents of modern battery technology. In this sense, we introduce a novel collaboration solution for mobile devices based on a contextual search entitled Hybrid Contextual Cloud for Ubiquitous Platforms comprising of Smartphones (HYCCUPS). HYCCUPS takes advantage of the pervasive nature of smartphones and of current wireless communication technologies as to offer offloading execution of mobile applications in an opportunistic on-the-fly hybrid computing cloud. We design an adaptive contextual search algorithm for schedulling mobile code execution in smartphone communities based on predicting the availability and mobility of devices in the near vicinity. We emulate the HYCCUPS framework based on real user traces and we prove that it improves battery health, maximizes power save, minimizes overall execution time of mobile applications and it preserves or even enhances user experience.\"",
        "Document: \"Scalable Chord-Based, Cluster-Enhanced Peer-to-Peer Architecture Supporting Range Queries. Over the Internet today, computing and communications environments are more complex and chaotic than classical distributed systems, lacking any centralized organization or hierarchical control. Peer-to-Peer network overlays provide a good substrate for creating large-scale data sharing, content distribution and application-level multicast applications. We present a scalable, cluster-enhanced P2P overlay network designed to share large sets of replicated distributed objects in the context of large-scale highly dynamic infrastructures. The system extends an existing architecture with an original solution designed to achieve optimal implementation results for range queries, as well as provide a fault-tolerant substrate. It also optimizes message routing in hop-count and throughput, whilst providing an adequate consistency among replicas.\"",
        "1 is \"CoAP: An Application Protocol for Billions of Tiny Internet Nodes\", 2 is \"Fairness issue in message delivery in delay- and Disruption-Tolerant Networks for disaster areas\"",
        "Given above information, for an author who has written the paper with the title \"Centralized Micro-clouds: An Infrastructure for Service Distribution in Collaborative Smart Devices.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001297": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A logic for strategic reasoning':",
        "Document: \"Model checking strategic abilities of agents under incomplete information. In this paper we introduce and study the complexity of model checking alternating-time temporal logic (atp) with imperfect information, using a fine-structured complexity measure. While atp model checking with perfect information is linear in the size of the model when the number of agents is considered fixed, this is no longer true when the number of agents is considered parameters of the problem (fine structure). Combining it with results from our previous papers, we get the surprising result that checking strategic abilities of agents under both perfect and imperfect information belong to the same complexity class: both problems are $\\sum^P_{2}$-complete.\"",
        "Document: \"Constructive knowledge: what agents can achieve under imperfect information. We propose a non-standard semantics for Alternating-time Temporal Logic with incomplete information, for which no commonly accepted semantics has been pro- posed yet. In our semantics, formulae are interpreted over sets of states rather than single states. We also propose a new epistemic operator for \"practical\" or \"con- structive\" knowledge, and we show that the new language is strictly more expres- sive than existing solutions, while it retains the same model checking complexity. Finally, we study properties of constructive knowledge and other operators in a non-standard semantics like this.\"",
        "Document: \"Rational play and rational beliefs under uncertainty. Alternating-time temporal logic (atl) is one of the most influential logics for reasoning about agents' abilities. Constructive Strategic Logic (csl) is a variant of atl for imperfect information games that allows to express strategic and epistemic properties of coalitions under uncertainty. In this paper, we propose a logic that extends csl with a notion of plausibility that can be used for reasoning about the outcome of rational behavior (in the game-theoretical sense). Moreover, we show how a particular notion of beliefs can be defined on top of plausibility. The resulting logic, cslp, turns out to be very expressive. We show that beliefs satisfy axioms KD45 in the logic. We also demonstrate how solution concepts for imperfect information games can be characterized and used in cslp and that the model checking complexity increases only slightly when plausibility and rational beliefs are added.\"",
        "Document: \"Multi-Valued Verification of Strategic Ability. Some multi-agent scenarios call for the possibility of evaluating specifications in a richer domain of truth values.Examples include runtime monitoring of a temporal property over a growing prefix of an infinite path, inconsistency analysis in distributed databases, and verification methods that use incomplete anytime algorithms, such as bounded model checking. In this paper, we present multi-valued ATL* (mv-ATLo), an expressive logic to specify strategic abilities in multi-agent systems.We show that our general method for model-independent translation from multi-valued to two-valued model checking cannot be directly extended to ATLo*. We also propose two ways of overcoming the problem. Firstly, we identify constraints on ATLo* formulas for which the model-independent translation can be suitably adapted. Secondly, we present a model-dependent reduction that can be applied to all formulas of ATLo*. We show that, in all cases, the complexity of verification increases only polynomially when new truth values are added to the evaluation domain. We also consider several examples that show possible applications of mv-ATLo and motivate its use for model checking MAS.\"",
        "Document: \"On module checking and strategies. Two decision problems are very close in spirit: module checking of CTL/CTL* and model checking of ATL/ATL*. The latter appears to be a natural multi-agent extension of the former, and it is commonly believed that model checking of ATL(*) subsumes module checking of CTL(*) in a straightforward way. Perhaps because of that, the exact relationship between the two has never been formally established. A more careful look at the known complexity results, however, makes one realize that the relationship is somewhat suspicious. In particular, module checking of CTL is Exptime-complete, while model checking of ATL is only time-complete. Thus, the (seemingly) less expressive framework yields significantly higher computational complexity than the (seemingly) more expressive one. This suggests that the relationship may not be as simple as believed. In this paper, we show that the difference is indeed fundamental. The way in which behavior of the environment is understood in module checking cannot be equivalently characterized in ATL(*). Conversely, if one wants to embed module checking in ATL(*) then its semantics must be extended with two essential features, namely nondeterministic strategies and long-term commitment to strategies.\"",
        "1 is \"Logical Preference Representation and Combinatorial Vote\", 2 is \"A logic for reasoning about time and reability\"",
        "Given above information, for an author who has written the paper with the title \"A logic for strategic reasoning\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001303": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Flexible integration of multimedia sub-queries with qualitative preferences':",
        "Document: \"On the Region Proximity in Metric Spaces. The problem of defining and measuring proximity of generic metric space regions is investigated. Though the proposed probabilistic approach is valid for arbitrary regions, specific ready-to-use formulas are developed for the important case of ball regions, taking into account the arbitrary distance distributions. Given a measure of proximity the concepts of similarity and selectivity of regions are also discussed\"",
        "Document: \"Flexible integration of multimedia sub-queries with qualitative preferences. Complex multimedia queries, aiming to retrieve from large databases those objects that best match the query specification, are usually processed by splitting them into a set of m simpler sub-queries, each dealing with only some of the query features. To determine which are the overall best-matching objects, a rule is then needed to integrate the results of such sub-queries, i.e., how to globally rank the m-dimensional vectors of matching degrees, or partial scores ,t hat objects obtain on the m sub-queries. It is a fact that state-of-the-art approaches all adopt as integration rule a scoring function ,s uch as weighted average, that aggregates the m partial scores into an overall (numerical) similarity score, so that objects can be linearly ordered and only the highest scored ones returned to the user. This choice however forces the system to compromise between the different sub-queries and can easily lead to miss relevant results. In this paper we explore the potentialities of a more general approach, based on the use of qualitative preferences, able to define arbitrary partial (rather than only linear) orders on database objects, so that a larger flexibility is gained in shaping what the user is looking for. For the purpose of efficient evaluation, we propose two integration algorithms able to work with any (monotone) partial order (thus also with scoring functions): MPO, which delivers objects one layer of the partial order at a time, and iMPO, which can incrementally return one object at a time, thus also suitable for processing top k queries. Our analysis demonstrates that using qualitative preferences pays off. In particular, using Skyline and Region-prioritized Skyline preferences for queries on a real image database, we show that the results we get have a precision comparable to that obtainable using scoring functions, yet they are obtained much faster, saving up to about 70% database accesses.\"",
        "Document: \"M-tree: An Efficient Access Method for Similarity Search in Metric Spaces. A new access method, called M-tree, is proposed to organize and search large data sets from a generic \"metric space\", i.e. where object proximity is only defined by a distance function satisfying the positivity, symmetry, and triangle inequality postulates. We detail algorithms for insertion of objects and split management, which keep the M-tree always balanced - several heuristic split alternatives are considered and experimentally evaluated. Algorithms for similarity (range and k-nearest neighbors) queries are also described. Results from extensive experimentation with a prototype system are reported, considering as the performance criteria the number of page I/O's and the number of distance computations.' The results demonstrate that the M-tree indeed extends the domain of applicability beyond the traditional vector spaces, performs reasonably well in high-dimensional data spaces, and scales well in case of growing files.\"",
        "Document: \"Bounding the cardinality of aggregate views through domain-derived constraints. Accurately estimating the cardinality of aggregate views is crucial for logical and physical design of data warehouses. This paper proposes an approach based on cardinality constraints, derived a-priori from the application domain, which may bound either the cardinality of a view or the ratio between the cardinalities of two views. We face the problem by first computing satisfactory bounds for the cardinality, then by capitalizing on these bounds to determine a good probabilistic estimate for it. In particular, we propose a bounding strategy which achieves an effective trade-off between the tightness of the bounds produced and the computational complexity.\"",
        "Document: \"PIBE: Manage Your Images the Way You Want. A customizable system for image browsing, named PIBE, is proposed. In details, PIBE provides the user with a set of browsing and personalization facilities that enable an ef- fective and efficient exploration of the image collection. The approach is novel and appealing because: 1) the personal- ization actions over the hierarchical organization of images are local, 2) the storage of the browsing structure is persis- tent, and 3) the provided GUI makes browsing and person- alization facilities extremely intuitive and \"easy-to-use\".\"",
        "1 is \"Approximate similarity retrieval with M-trees\", 2 is \"Efficient Video Retrieval by Locality Sensitive Hashing\"",
        "Given above information, for an author who has written the paper with the title \"Flexible integration of multimedia sub-queries with qualitative preferences\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001319": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Development Of Intelligent Service Robots':",
        "Document: \"A Distributed Architecture for Autonomous Robots. In this paper we propose a distributed architecture for intelligent robotic systems. The architecture is specific to this domain because it intends to provide support for the type of applications that share particular functional requirements: concurrent perception and action, task and plan execution, reasoning, \u201cintelligent behaviours\u201d. The architecture also aims to improve re-usability and integration of different software components. It allows the transparent distribution of processes on different computers in a network to take advantage of the increased computational power and so overcome the vehicles on-board limitations. We focus on the related cognitive model on the internal structure of the architecture and of its components. We also examine in detail the EIE protocol we defined to exchange information within the distributed system. Finally we indicate some experimental results\"",
        "Document: \"A Cognitive Hybrid Model for Autonomous Navigation. Action representation and planning is one among the most important research fields in which it has been experienced the failure of single paradigms in isolation to solve real, complex problems. The goal of this paper is to present a system for action representation and reasoning in complex, real-world, and real time scenarios, characterised by the integration of different representation paradigms: symbolic, diagrammatic, and procedural. In this sense the system is called hybrid. The paper focuses on the cognitive model and on the representation and reasoning system. A realistic navigation system for the guidance and control of autonomous mobile robots is used as an example to describe the potentiality of the system in solving real complex problems and it is currently being tested in an indoor environment.\"",
        "Document: \"Context assessment strategies for ubiquitous robots. This paper presents an architecture for context-aware Ubiquitous Robotics applications, where mobile robots cooperate with intelligent environments to fulfill their tasks. Specifically, the work is focused on distributed knowledge representation issues and context assessment strategies, and introduces a technique for on-line context recognition in highly dynamic environments. Experimental validation, performed in a civillian hospital building, is described and discussed.\"",
        "Document: \"Describing and Classifying Situations with Description Logics in Ubiquitous Robotics. The article describes a system for context assessment, which is based on an ontology described through the Description Logics formalism and implemented in OWL, the Web Ontology Language. The approach is different from all other works in the literature since the system does not require an external reasoning engine, but relies only on the base mechanism for ontology reasoning. Experiments performed in two different scenarios are described, i.e., a Smart Home and an autonomous mobile robot operating within a partially automated building.\"",
        "Document: \"Control Strategies in the Eye-Head Coordination System. The aim of this paper is to outline a model of the eye-head system and some of its control strategies. To formulate such a model, the eye-head system was stimulated with random or periodic visual targets. Coupling between the eye and head motor commands was evident in the experiments performed, and this was also found to be the case for acoustic and tactile stimuli. Further experiments were performed to investigate whether the central nervous system (CNS) would be able to overcome the relative lack offlexibility shown by the model of the eye-head control system initially proposed. These experiments, in which it was required to execute independent eye-head movements towards two simultaneous targets (visual and acoustic), showed an impaired performance, indicating that an overcoming action by the CNS is not always possible. The findings are interpreted in terms of an interaction between a \"hardware\" low-level controller and a \"software\" high-level decisionmaker.\"",
        "1 is \"Keeping the resident in the loop: adapting the smart home to the user\", 2 is \"Issues and approaches in the design of collective autonomous agents\"",
        "Given above information, for an author who has written the paper with the title \"Development Of Intelligent Service Robots\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001324": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Tracking @stemxcomet: teaching programming to blind students via 3D printing, crisis management, and twitter':",
        "Document: \"Designing for discovery: opening the hood for open-source end user tinkering. According to the Free Software Movement, the user ought to have \"the freedoms to make changes, and to publish improved versions\" and \"to study how the program works, and adapt it to your needs\". The Open Source Initiative expects users to access source code, explaining that \"you can't evolve programs without modifying them. Since our purpose is to make evolution easy, we require that modification be made easy\". These philosophies can shape a unique perspective on software usability that has not been addressed thoroughly in the open-source domain. That is: how to design user-interfaces and tools that facilitate access to source code and encourage the behaviors envisioned above, namely, to improve the code, to personalize it, to learn from it, and to share it. And, as the Open Source Initiative recommends, to make this easy. In addition to presenting this research perspective, we suggest some fruitful approaches to answering these questions and our current and future steps.\"",
        "Document: \"Audio-Based Feedback Techniques for Teaching Touchscreen Gestures. While sighted users may learn to perform touchscreen gestures through observation (e.g., of other users or video tutorials), such mechanisms are inaccessible for users with visual impairments. As a result, learning to perform gestures without visual feedback can be challenging. We propose and evaluate two techniques to teach touchscreen gestures to users with visual impairments: (1) gesture sonification to generate sound based on finger touches, creating an audio representation of a gesture; and (2) corrective verbal feedback that combined automatic analysis of the user's drawn gesture with speech feedback. To refine and evaluate the techniques, we conducted three controlled laboratory studies. The first study, with 12 sighted participants, compared parameters for sonifying gestures in an eyes-free scenario. We identified pitch+stereo panning as the best combination. In the second study, ten blind and low-vision participants completed gesture replication tasks for single-stroke, multistroke, and multitouch gestures using the gesture sonification feedback. We found that multistroke gestures were more difficult to understand in sonification, but that playing each finger sound serially may improve understanding. In the third study, six blind and low-vision participants completed gesture replication tasks with both the sonification and corrective verbal feedback techniques. Subjective data and preliminary performance findings indicated that the techniques offer complementary advantages: although verbal feedback was preferred overall primarily due to the precision of its instructions, almost all participants appreciated the sonification for certain situations (e.g., to convey speed). This article extends our previous publication on gesture sonification by extending these techniques to multistroke and multitouch gestures. These findings provide a foundation for nonvisual training systems for touchscreen gestures.\"",
        "Document: \"Collaborative Accessibility: How Blind and Sighted Companions Co-Create Accessible Home Spaces. In recent decades, great technological strides have been made toward enabling people who are blind to live independent, successful lives. However, there has been relatively little progress towards understanding the social, collaborative needs of this population, particularly in the domestic setting. We conducted semi-structured interviews in the homes of 10 pairs of close companions in which one partner was blind and one was not. We found that partners engaged in collaborative accessibility by taking active roles in co-creating an accessible environment. Due to their different visual abilities, however, partners sometimes encountered difficulties managing divergent needs and engaging in shared experiences. We describe outstanding challenges to creating accessible shared home spaces and outline new research and technology opportunities for supporting collaborative accessibility in the home.\"",
        "Document: \"A web accessibility report card for top international university web sites. University web pages play a central role in the activities of current and prospective postsecondary students. University sites that are not accessible may exclude people with disabilities from participation in educational, social and professional activities. In order to assess the current state of university web site accessibility, we performed a multi-method analysis of the home pages of 100 top international universities. Each site was analyzed for compliance with accessibility standards, image accessibility, alternate-language and text-only content, and quality of web accessibility statements. Results showed that many top universities continue to have accessibility problems. University web site accessibility also varies greatly across different countries and geographic regions. Remaining obstacles to universal accessibility for universities include low accessibility in non-English-speaking countries and absent or low-quality accessibility policies.\"",
        "Document: \"\"At times avuncular and cantankerous, with the reflexes of a mongoose\": Understanding Self-Expression through Augmentative and Alternative Communication Devices. Amyotrophic Lateral Sclerosis (ALS) is a disease that causes individuals to lose their ability to control their muscles, eventually leaving them unable to speak or write. People with ALS often transition to using an augmentative and alternative communication device (AAC), which requires both the AAC user and their conversation partners to adjust to new and different communication patterns. We conducted interviews with seven individuals with ALS and their partners, focusing on how AAC use has impacted their communication and personal expression. Our participants experienced a range of communication difficulties, including conversational pacing, personality expression, and interacting with unfamiliar conversational partners. Participants worked to adapt their communication behaviors to the AAC device, but still experienced challenges in expressing themselves, and sometimes felt compelled to withdraw from social interaction. By improving our understanding of how people transition to using AAC, we may inform improved designs for future AAC devices.\"",
        "1 is \"Exploring non-speech auditory feedback at an interactive multi-user tabletop\", 2 is \"Legion scribe: real-time captioning by non-experts\"",
        "Given above information, for an author who has written the paper with the title \"Tracking @stemxcomet: teaching programming to blind students via 3D printing, crisis management, and twitter\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001333": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Opportunistic Periodic MAC Protocol for Cognitive Radio Networks':",
        "Document: \"Data harvesting with mobile elements in wireless sensor networks. In recent studies, using mobile elements (MEs) as mechanical carriers to relay data has been shown to be an effective way of prolonging sensor network life time and relaying information in partitioned networks. As the data generation rates of sensors may vary, some sensors need to be visited more frequently than others. In this paper, a partitioning-based algorithm is presented that schedules the movements of MEs in a sensor network such that there is no data loss due to buffer overflow. Simulation results show that the proposed Partitioning-Based Scheduling (PBS) algorithm performs well in terms of reducing the minimum required ME speed to prevent data loss, providing high predictability in inter-visit durations, and minimizing the data loss rate for the cases when the ME is constrained to move slower than the minimum required ME speed.\"",
        "Document: \"A node-based CSMA algorithm for improved delay performance in wireless networks. Recent studies in wireless scheduling have shown that CSMA can be made throughput optimal by optimizing over activation rates. However, those throughput optimal CSMA algorithms were found to suffer from poor delay performance, especially at high throughputs where the delay can potentially grow exponentially in the size of the network. Motivated by these shortcomings, in this paper we propose a node-based version of the throughput optimal CSMA (NB-CSMA) as opposed to previous link-based CSMA algorithms, where links were treated as separate entities. Our algorithm is fully distributed and corresponds to Glauber dynamics with \\\"Block updates\\\". We show analytically and via simulations that NB-CSMA outperforms conventional link-based CSMA in terms of delay for any fixed-size network. We also characterize the fraction of the capacity region for which the average queue lengths (and the average delay) grow polynomially in the size of the network, for networks with bounded-degree conflict graphs. This fraction is no smaller than the fraction known for link-based CSMA, and is significantly larger for a special class of wireless ad-hoc networks.\"",
        "Document: \"Scheduling in Multihop Wireless Networks Without Back-Pressure. This paper focuses on scheduling in multihop wireless networks where flows are associated with fixed routes. The well-known back-pressure scheduling algorithm is throughput-optimal, but requires constant exchange of queue length information among neighboring nodes for calculating the \u201cback-pressure.\u201d Moreover, previous research shows that the total queue length along a route increases quadratically as the route length under the back-pressure algorithm, resulting in poor delay performance. In this paper, we propose a self-regulated MaxWeight scheduling, which does not require back-pressure calculation. We prove that the self-regulated MaxWeight scheduling is throughput-optimal (an algorithm is said to be throughput-optimal if it can stabilize any traffic that can be stabilized by any other algorithm). In the simulation part, we show that the self-regulated MaxWeight scheduling has a much better delay performance than the back-pressure algorithm.\"",
        "Document: \"Performance optimization of interference-limited multihop networks. The performance of a multihop wireless network is typically affected by the interference caused by transmissions in the same network. In a statistical fading environment, the interference effects become harder to predict. Information sources in a multihop wireless network can improve throughput and delay performance of data streams by implementing interference-aware packet injection mechanisms. Forcing packets to wait at the head of queues and coordinating packet injections among different sources enable effective control of copacket interference. In this paper, throughput and delay performance in interference-limited multihop networks is analyzed. Using nonlinear probabilistic hopping models, waiting times which jointly optimize throughput and delay performances are derived. Optimal coordinated injection strategies are also investigated as functions of the number of information sources and their separations. The resulting analysis demonstrates the interaction of performance constraints and achievable capacity in a wireless multihop network.\"",
        "Document: \"Voluntary Spectrum Handoff: A Novel Approach to Spectrum Management in CRNs. In this paper, a new spectrum management scheme for CRNs called Voluntary Spectrum Handoff (VSH) is introduced. The two mechanisms proposed under VSH estimate opportune times to initiate unforced spectrum handoff events to facilitate setup and signaling of alternative channels without having communication disruption, which occurs when a secondary user is forced out of an operating spectrum due to primary user activity. VSH has been evaluated through extensive simulations. Simulation results indicate that VSH significantly reduces the communication disruption duration due to handoffs.\"",
        "1 is \"Study on QoS Support in 802.11e-based Multi-hop Vehicular Wireless Ad Hoc Networks\", 2 is \"Cross-layer modeling for quality of service guarantees over wireless links\"",
        "Given above information, for an author who has written the paper with the title \"Opportunistic Periodic MAC Protocol for Cognitive Radio Networks\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001354": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Advances In Peer-To-Peer Content Search':",
        "Document: \"Context-Aware Identity Management in Pervasive Ad-hoc Environments. Contextual information and Identity Management (IM) is of paramount importance in the growing use of portable mobile devices for sharing information and communication between emergency services in pervasive ad-hoc environments. Mobile Ad-hoc Networks (MANets) play a vital role within such a context. The concept of ubiquitous/pervasive computing is intrinsically tied to wireless communications. Apart from many remote services, proximity services (context-awareness) are also widely available, and people rely on numerous identities to access these services. The inconvenience of these identities creates significant security vulnerability as well as user discomfort, especially from the network and device point of view in MANet environments. In this article, the authors address how contextual information is represented to facilitate IM and present a User-centered and Context-aware Identity Management (UCIM) framework for MANets.\"",
        "Document: \"NetHost-Sensor: A Novel Concept in Intrusion Detection Systems. An intrusion detection systems (IDS) mainpurpose is to monitor a resource and notifysomeone in the event of a specific occurrencefor an appropriate response. Based on thesources of audit data, an IDS can be classifiedinto a Host-Based Intrusion Detection System(HBIDS) or a Network-Based Intrusion DetectionSystem (NBIDS).In this paper we focus on NBIDS and proposea novel concept in IDSs called the NetHost-Sensor.We describe the NetHost-Sensor abilityto thwart end-to-end encryption, Denial ofServices (DoS) attacks, and reduces falsepositives. This paper presents our experimentalprocedures and results in designing the NetHost-Sensor.\"",
        "Document: \"Game Based Learning Framework for Virtual 3D Dinosaurs Knowledge. In this paper we propose a constructivist theory, model and application for the design and development of a Game-Based learning application introducing virtual 3D dinosaurs science. How to design effective learning opportunities? Why is learning by experience often more efficient than learning by studying? - are some of the questions we discuss in this research. Using computer games and games in general for educational purposes offers a variety of knowledge presentations and creates opportunities to apply the knowledge within a virtual world, thus supporting and facilitating the learning process. The development is carried out using Homura 3D game engine which facilitates the development and deployment (online and mobile) of such 3D games.\"",
        "Document: \"Component-based security system (COMSEC) with QoS for wireless sensor networks. In the last decade, many security solutions have been proposed to fulfil the security requirements of wireless sensor networks (WSNs). However, these solutions are specifically designed for particular security issues, based on different assumptions, and limited to certain WSNs applications. Can these security solutions work together to handle multiple problems at the same time? It is an interesting and difficult question. We believe good solutions in various security areas do not mean they can work together and deliver similar results, i.e. occurrence of any security weakness or attack in a particular security solution could expose the vulnerabilities of other solutions. Using these solutions together might also degrade WSN quality of service. To deal with the aforementioned issues, we therefore propose a novel component-based security system (COMSEC) based on proactive and reactive components. Each component looks after a particular security issue and is integrated with others. The proposed system provides better secure communication, Sybil attack detection, secure data aggregation and resilience against node capture attacks and replication attacks. COMSEC has been evaluated and compared against existing schemes. Evaluation results show a significant improvement in resilience against node capture attacks, Sybil attack detection data confidentiality, privacy, memory overhead and connectivity. Copyright (c) 2012 John Wiley & Sons, Ltd.\"",
        "Document: \"A New Self-Detection Scheme For Sensor Network Boundary Recognition. One of the exigent problems in wireless sensor networks is the recognition of network boundary and the detection of holes within the network. In this paper, we propose an algorithm in which every node in the network self-detects whether it is a boundary node or an inner node by utilizing the available connectivity information and making no assumptions about the location awareness. The algorithm is efficient than existing schemes in terms of accuracy and energy consumption. It does not need high degree of connectivity as compare to other existing schemes. The simulation results prove the efficiency and accuracy of our algorithm.\"",
        "1 is \"Capacity of digital watermarks subjected to an optimal collusion attack\", 2 is \"Blobworld: A System for Region-Based Image Indexing and Retrieval\"",
        "Given above information, for an author who has written the paper with the title \"Advances In Peer-To-Peer Content Search\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001363": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Competitive Diffusion on Weighted Graphs.':",
        "Document: \"The minimum vulnerability problem on specific graph classes. Suppose that each edge e of an undirected graph G is associated with three nonnegative integers $$\\\\mathsf{cost}(e)$$cost(e), $$\\\\mathsf{vul}(e)$$vul(e) and $$\\\\mathsf{cap}(e)$$cap(e), called the cost, vulnerability and capacity of e, respectively. Then, we consider the problem of finding $$k$$k paths in G between two prescribed vertices with the minimum total cost; each edge e can be shared without any cost by at most $$\\\\mathsf{vul}(e)$$vul(e) paths, and can be shared by more than $$\\\\mathsf{vul}(e)$$vul(e) paths if we pay $$\\\\mathsf{cost}(e)$$cost(e), but cannot be shared by more than $$\\\\mathsf{cap}(e)$$cap(e) paths even if we pay the cost for e. This problem generalizes the disjoint path problem, the minimum shared edges problem and the minimum edge cost flow problem for undirected graphs, and it is known to be NP-hard. In this paper, we study the problem from the viewpoint of specific graph classes, and give three results. We first show that the problem is NP-hard even for bipartite outerplanar graphs, 2-trees, graphs with pathwidth two, complete bipartite graphs, and complete graphs. We then give a pseudo-polynomial-time algorithm for bounded treewidth graphs. Finally, we give a fixed-parameter algorithm for chordal graphs when parameterized by the number $$k$$k of required paths.\"",
        "Document: \"Finding Independent Spanning Trees in Partial k-Trees. Spanning trees rooted at a vertex r of a graph G are independent if, for each vertex v in G, all the paths connecting v and r in the trees are pairwise internally disjoint. In this paper we give a linear-time algorithm to find the maximum number of independent spanning trees rooted at any given vertex r in partial k-trees G, that is, graphs G with tree-width bounded by a constant k.\"",
        "Document: \"Reconfiguration Of Steiner Trees In An Unweighted Graph. We study a reconfiguration problem for Steiner trees in an unweighted graph, which determines whether there exists a sequence of Steiner trees that transforms a given Steiner tree into another one by exchanging a single edge at a time. In this paper, we show that the problem is PSPACE-complete even for split graphs (and hence for chordal graphs), while solvable in linear time for interval graphs.\"",
        "Document: \"Finding optimal edge-rankings of trees. No abstract available.\n\n\"",
        "Document: \"Energy-efficient threshold circuits computing mod functions. We prove that the modulus function MODm of n variables can be computed by a threshold circuit C of energy e and size s = O(e(n/m)1/(e\u22121)) for any integer e \u2265 2, where the energy e is defined to be the maximum number of gates outputting \"1\" over all inputs to C, and the size s to be the number of gates in C. Our upper bound on the size s almost matches the known lower bound s = \u03a9(e(n/m)1/e).\"",
        "1 is \"Automatic Generation of Linear-Time Algorithms from Predicate Calculus Descriptions of Problems on Recursively Constructed Graph Families\", 2 is \"Isomorphism on Subgraph-Closed Graph Classes: A Complexity Dichotomy and Intermediate Graph Classes.\"",
        "Given above information, for an author who has written the paper with the title \"Competitive Diffusion on Weighted Graphs.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001422": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A Unified Approach to Planning, Sensing and Navigation for Mobile Robots':",
        "Document: \"An architecture for distributed cooperative planning in a behaviour-based multi-robot system. The Architecture for Behaviour-Based Agents (ABBA) is an architecture designed to illustrate that situated agents can exhibit sophisticated planning while retaining reactivity, without resorting to hybrid architectures. In particular, unified planning for spatial and topological navigation, cooperation and communication can be achieved using an appropriate action selection scheme. Joint-planning of cooperative behaviour in a multi-robot system arises as a natural consequence of considering cooperative planning as an extension of the action selection problem facing individual agents. This paper describes ABBA and illustrates the efficacy of our approach by presenting a solution to a cooperative cleaning task with two autonomous mobile robots. (C) 1999 Elsevier Science B.V. All rights reserved.\"",
        "Document: \"Visual Human-Machine Interaction. It is envisaged that computers of the future will have smart interfaces such as speech and vision, which will facilitate natural and easy human-machine intereiction. Gestures of the face and hands could become a natural way to control the operations of a computer or a machine, such as a robot. In this paper, we present a vision-based interface that in real-time tracks a person's facial features and the gaze point of the eyes. The system can robustly track facial features, can detect tracking failures and has an automatic mechanism for error recovery. The system is insensitive to lighting changes and occulsions or distortion of the facial features. The system is user independent eind can automatically calibrate for each different user. An application using this technology for driver fatigue detection eind the evaluation of ergonomic design of motor vehicles has been developed. Our human-machine interface has an enormous potential in other apphcations that allow the control of machines and processes, and measure human performance. For example, product possibilities exist for assisting the disabled and in video game entertainment.\"",
        "Document: \"Q-Learning in Continuous State and Action Spaces. Q-learning can be used to learn a control policy that maximises a scalar reward through interaction with the environment. Q- learning is commonly applied to problems with discrete states and actions. We describe a method suitable for control tasks which require continuous actions, in response to continuous states. The system consists of a neureil network coupled with a novel interpolator. Simulation results are presented for a non-holonomic control task. Advantage Learning, a variation of Q-learning, is shown enhance learning speed and reliability for this task.\"",
        "Document: \"Real-Time Visual Recognition of Facial Gestures for Human-Computer Interaction. People naturally express themselves through facial gestures and expressions. Our goal is to build a facial gesture human-computer interface for use in robot applications. We have implemented an interface that tracks a person's facial features in real time (30Hz). Our system does not require special illumination nor facial makeup. By using multiple Kalman filters we accurately predict and robustly track facial features. This is despite disturbances and rapid movements of the head (including both translational and rotational motion). Since we reliably track the face in real-time we are also able to recognize motion gestures of the face. Our system can recognize a large set of gestures (13) ranging from \"yes\", \"no\" and \"may be\" to detecting winks, blinks and sleeping.\"",
        "Document: \"Finger Track - A Robust and Real-Time Gesture Interface. Real-time computer vision combined with robust gesture recognition provides a natural alternative to traditional computer interfaces. Human users have plenty of experience with actions and the manipulation of objects requiring finger movement. In place of a mouse, users could use their hands to select and manipulate data. This paper presents a first step in this approach using a finger as a pointing and selection device. A major feature of a successful tracking system is robustness. The system must be able to acquire tracked features upon startup, and r eacquire them if lost during tracking. Re- acquisition should be fast and accurate (i.e. it s hould pick up the correct feature). Intelligent search algorithms are needed for speedy, accurate acquisition of lost features with the frame. The prototype interface presented in this paper is based on finger tracking as a means of i nput to applications. The focus of the discussion is how the system can be made to perform robustly in real-time. Dynamically distributed search windows are defined for searching within the frame. The location and number of search windows are dependent on the confidence in the tracking of features. Experimental results showing the effectiveness of these techniques are presented.\"",
        "1 is \"Module-Based Reinforcement Learning: Experiments with a Real Robot\", 2 is \"Unknown Object Grasping Using Statistical Pressure Models\"",
        "Given above information, for an author who has written the paper with the title \"A Unified Approach to Planning, Sensing and Navigation for Mobile Robots\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001431": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Closures, cycles, and paths':",
        "Document: \"2-factors and hamiltonicity. We prove the following generalization of a result of Faudree and van den Heuvel. Let G be a 2-connected graph with a 2-factor. If d ( u ) + d ( v ) \u2a7e n \u2212 2 for all pairs of non-adjacent vertices u , v contained in an induced K 1,3 , in an induced K 1,3 + e or as end-vertices in an induced P 4 , then G is Hamiltonian.\"",
        "Document: \"On the chromatic number of 2K2-free graphs. In this paper, we study the chromatic number of 2K2-free graphs. We show linear \u03c7-binding functions for several subclasses of 2K2-free graphs, namely (2K2,H)-free graphs where H\u2208{house,gem,2K1+Kp,(K1\u222aK2)+Kp}. We will also discuss \u03c7-binding functions for (2K2,claw)-free graphs. Finally, we will present extensions of our results to subclasses of P5-free graphs.\"",
        "Document: \"A New Bound on the Domination Number of Graphs with Minimum Degree Two. For a graph G, let gamma(G) denote the domination number of G and let delta(G) denote the minimum degree among the vertices of G. A vertex x is called a bad-cut-vertex of G if G-x contains a component, C-x, which is an induced 4-cycle and x is adjacent to at least one but at most three vertices on C-x. A cycle C is called a special-cycle if C is a 5-cycle in G such that if u and v are consecutive vertices on C, then at least one of u and v has degree 2 in G. We let bc(G) denote the number of bad-cut-vertices in G, and sc(G) the maximum number of vertex disjoint special-cycles in G that contain no bad-cut-vertices. We say that a graph is (C-4, C-5)-free if it has no induced 4-cycle or 5-cycle. Bruce Reed [14] showed that if G is a graph of order n with delta(G) >= 3, then gamma(G) >= 3n/8. In this paper, we relax the minimum degree condition from three to two. Let G be a connected graph of order n >= 14 with delta(G) >= 2. As an application of Reed's result, we show that gamma(G) <= 1/8(3n + sc(G) + bc(G)). As a consequence of this result, we have that (i) gamma(G) <= 2n/5; (ii) if G contains no special-cycle and no bad-cut-vertex, then gamma(G) <= 3n/8; (iii) if G is (C-4, C-5)-free, then gamma(G) <= 3n/8; (iv) if G is 2-connected and d(G)(u) + d(G)(v) >= 5 for every two adjacent vertices u and v, then gamma(G) <= 3n/8. All bounds are sharp.\"",
        "Document: \"Non-path spectrum sets. A path of a graph is called maximal if it is not a proper subpath of any other path of the graph. The path spectrum of a graph G, denoted by ps(G), is the set of lengths of all maximal paths in the graph. A set S of positive integers is called a path spectrum if there is a connected graph G such that ps(G) = S. Jacobson et al. showed that all sets of positive integers with cardinality of 1 or 2 are path spectrum sets. Their results raised the question of whether all sets of positive integers are path spectra. We show that, for every positive integer k \u2265 3, there are infinitely many sets of k positive integers which are not path spectra. A set S of positive integers is called an absolute path spectrum if there are infinitely many connected graphs G such that ps(G) = S. We completely characterize absolute path spectra S for |S| \u2264 2. \u00a9 2008 Wiley Periodicals, Inc. J Graph Theory 58: 329\u2013350, 2008\"",
        "Document: \"The k-SATISFIABILITY problem remains NP-complete for dense families. We consider the k - SATISFIABILITY problem ( k -SAT): Given a family F of n clauses c 1 , \u2026, c n in conjunctive normal form, each consisting of k literals corresponding to k different variables of a set of r \u2a7e k 1 boolean variables, is F satisfiable? By k -SAT(> n 0 ) we denote the k -SAT problem restricted to families with n > n 0 ( r ) clauses. We prove that for each k \u2a7e3 and each integer l \u2a7e4 such that r \u2a7e lk 2 , the k- SAT (>( r k ) (2 k \u22121\u22124/l)) problem is NP-complete.\"",
        "1 is \"On-line arbitrarily vertex decomposable trees\", 2 is \"3-connected graphs with non-cut contractible edge covers of size k\"",
        "Given above information, for an author who has written the paper with the title \"Closures, cycles, and paths\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001481": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Self organizing maps in NLP: exploration of coreference feature space':",
        "Document: \"A multi\u2014directional multiple\u2014path recognition scheme for complex objects applied to the domain of a wooden toy kit. Recognition and description of complex objects require the representation and processing of various aspects and features. In the paper, we propose an architecture which combines the advantages of di\u00c4erent paradigms. Voting and Bayesian networks enable an integrated approach for knowledge based and probabilistic reasoning. \" 1999 Elsevier Science B.V. All rights reserved.\"",
        "Document: \"A very concise feature representation for time series classification understanding. One major problem of time series analysis, particularly of a multivariate time series, is to find their feature representations. Especially, with the emerging of deep recurrent neural networks (RNNs), researchers opt to train the networks with raw signals by using an end-to-end framework to achieve the highest classification accuracy. Their works focus on modifying the network models and fine-tuning millions of hyperparameters; however, they lack the required level of understanding of the intrinsic properties of the data. In our work, we adopted a technique for dimensionality reduction of non-time-series to transform the time series data into small sets of feature representations. Our proposed technique allows the analyst to easily visualize the feature representations of the data and detect an instance which has a potential to cause a test failure. We demonstrated the robustness of our technique by subjecting the extracted features to a conventional classification approach such as Random Forest. The datasets used for the evaluation of this task are from the known benchmarking of 15 multivariate time series datasets and two Motion Caption datasets of 27 and 65 actions. The classification results were compared with the outputs from the Echo State Networks (ESNs) and the deep Bidirectional Neural Networks (BRNNs).\"",
        "Document: \"Interactive schematic summaries for exploration of surveillance video. We present a new and scalable technique to explore surveillance videos by scatter/gather browsing of trajectories of moving objects. The proposed approach facilitates interactive clustering of trajectories by an effective way of cluster visualization that we term schematic summaries. This novel visualization illustrates cluster summaries in a schematic, non-photorealistic style. To reduce visual clutter, we introduce the trajectory bundling technique. The fusion of schematic summaries and user interaction leads to efficient hierarchical exploration of video data. Examples of different browsing scenarios demonstrate the effectiveness of the proposed method.\"",
        "Document: \"Focus-of-attention from local color symmetries. In this paper, a continuous valued measure for local color symmetry is introduced. The new algorithm is an extension of the successful gray value-based symmetry map proposed by Reisfeld et al. The use of color facilitates the detection of focus points (FPs) on objects that are difficult to detect using gray-value contrast only. The detection of FPs is aimed at guiding the attention of an object recognition system; therefore, FPs have to fulfill three major requirements: stability, distinctiveness, and usability. The proposed algorithm is evaluated for these criteria and compared with the gray value-based symmetry measure and two other methods from the literature. Stability is tested against noise, object rotation, and variations of lighting. As a measure for the distinctiveness of FPs, the principal components of FP-centered windows are compared with those of windows at randomly chosen points on a large database of natural images. Finally, usability is evaluated in the context of an object recognition task.\"",
        "Document: \"The long-range saliency of edge- and corner-based salient points. A major goal of salient-point (SP) detection is increasing computational efficiency. Therefore, methods which can detect saliency of a large region by evaluation of only a small local patch are of particular interest. This paper checks for well-known detectors whether saliency outreaches the actual SPs.\"",
        "1 is \"Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion\", 2 is \"Unsupervised Segmentation of Color-Texture Regions in Images and Video\"",
        "Given above information, for an author who has written the paper with the title \"Self organizing maps in NLP: exploration of coreference feature space\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001484": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Time follower's vision: a teleoperation interface with past images.':",
        "Document: \"Development and Control of a High Maneuverability Wheeled Robot with Variable-Structure Functionality. Common wheeled robots face many difficulties when traveling in complex environments, especially its incapability to climb over obstacles higher than its wheel radius. Many alterations were made to these robots to allow them to travel in rough terrains or conquer high obstacles, nevertheless, most designs are complex in nature and lost several original wheeled robot advantages by gaining others. In this paper, we propose a novel wheeled type robot with variable structure functionality. It is designed with simple structure consisting of three main robot parts, the main body, the left and right wheel-arm units. The robot could achieve five locomotion modes that allows the robot to travel in various environments and climb over high obstacles. Moreover, the fast-speed advantage in flat floor condition that common wheeled robot originally hold is still remained. The confirmation of the effectiveness of the robot's maneuverability is shown by several experimental results\"",
        "Document: \"Communication using pheromone field for multiple robots. In this paper, we consider a issue that the reliable and the inexpensive communication method in swarm robotics. The ants forage for preys by using pheromone trails. They lay down the pheromone trails between preys and a nest. By detecting the trail pheromone, they can find the preys. Though they do not have excellent intelligence, they can communicate with each other and cooperate by adding information to the environment, like a pheromone. This communication method has a merit that an agent does not need to memorize the place of the preys. We consider to answer the issue that ldquoHow do the swarm robots communicate using pheromone trail?rdquo. We construct a swarm behavior simulator and develop swarm robots that communicate using the pheromone trail. We demonstrate the effectiveness of the communication using the pheromone trail by computer simulations and experiments using swarm robots. To realize this purpose, we design a swarm behavior algorithm, based on 4 perceptual signs (stimuli) and 3 effector signs (actions). In the simulations, an experimental field is discretized by computational grids, and evaporation and diffusion are phenomena of the pheromone modeled by discretized equations. The proposed algorithm is demonstrated by the simulation. Simulation result shows that proposed algorithms act effectively. Based on the simulation results, we set three robots, one nest and one prey in the flat experimental field. We observe three robotspsila behavior and the state of the environment for 20 minutes. The robots laid down the pheromone trail between the nest and the prey, and reinforced the pheromone trail many times. This fact means that swarm robots can realize the function of the chemical, indirect, plastic and local communication like ants by using the pheromone trail.\"",
        "Document: \"Control Of Redundant 3d Snake Robot Based On Kinematic Model. In this paper, we derive a kinematic model and a control law for 3D snake robots which have wheeled link mechanism. We define the redundancy controllable system and find that introduction of links without wheels makes the system redundancy controllable. Using redundancy, it becomes possible to accomplish both main objective of controlling the position and the posture of the snake robot head, and sub-objective of the singular configuration avoidance. Computer simulations demonstrate the effectiveness of the proposed control law.\"",
        "Document: \"Attitude Control Of A Space Robot With Initial Angular Momentum. In this paper, we propose attitude control law of a space robot with initial angular momentum. We consider a class of the 3-state and 2-input affine nonholonomic systems with a drift term as general system representation for a planar two-link space, robot with initial angular momentum. We propose the coordinate and input transformation algorithm which converts an attine system with a drift term into the time-state control form). and propose the controller design method using the exact linearization. Ali asteroid sample-return robot with nonzero initial angular momentum is considered as ail example. By using the transformation algorithm, we obtain the time-state control form for the robot, The attitude controller of the robot is designed oil the basis of the time-state control form, and simulation and experiment have been carried out.\"",
        "Document: \"Proposition of twisting mode of locomotion and GA based motion planning for transition of locomotion modes of 3-dimensional snake-like robot. We present a type of locomotion mode, path planning of shape transition, and design and implementation of a 3-dimensional hyper-redundant snake-like robot. First we explain the multiple locomotion modes, for example ring mode, inching mode, bridge mode, and so on. We propose a new type of locomotion \"twisting mode\" and analyze the principle of the locomotion based on statics. Next, the motion planning using genetic algorithm for transition of locomotion modes is explained and simulation results for the GA research is shown. Finally, we explain the designed prototype system and experimental results are shown.\"",
        "1 is \"Range: exploring implicit interaction through electronic whiteboard design\", 2 is \"EOG-based Human-Computer Interface system development\"",
        "Given above information, for an author who has written the paper with the title \"Time follower's vision: a teleoperation interface with past images.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001487": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Extracting probable command and control signatures for detecting botnets':",
        "Document: \"Shady paths: leveraging surfing crowds to detect malicious web pages. The web is one of the most popular vectors to spread malware. Attackers lure victims to visit compromised web pages or entice them to click on malicious links. These victims are redirected to sites that exploit their browsers or trick them into installing malicious software using social engineering. In this paper, we tackle the problem of detecting malicious web pages from a novel angle. Instead of looking at particular features of a (malicious) web page, we analyze how a large and diverse set of web browsers reach these pages. That is, we use the browsers of a collection of web users to record their interactions with websites, as well as the redirections they go through to reach their final destinations. We then aggregate the different redirection chains that lead to a specific web page and analyze the characteristics of the resulting redirection graph. As we will show, these characteristics can be used to detect malicious pages. We argue that our approach is less prone to evasion than previous systems, allows us to also detect scam pages that rely on social engineering rather than only those that exploit browser vulnerabilities, and can be implemented efficiently. We developed a system, called SpiderWeb, which implements our proposed approach. We show that this system works well in detecting web pages that deliver malware.\"",
        "Document: \"Steal this movie - automatically bypassing DRM protection in streaming media services. Streaming movies online is quickly becoming the way in which users access video entertainment. This has been powered by the ubiquitous presence of the Internet and the availability of a number of hardware platforms that make access to movies convenient. Often, video-on-demand services use a digital rights management system to prevent the user from duplicating videos because much of the economic model of video stream services relies on the fact that the videos cannot easily be saved to permanent storage and (illegally) shared with other customers. In this paper, we introduce a general memory-based approach that circumvents the protections deployed by popular video-on-demand providers. We apply our approach to four different examples of streaming services: Amazon Instant Video, Hulu, Spotify, and Netflix and we demonstrate that, by using our technique, it is possible to break DRM protection in a semi-automated way.\"",
        "Document: \"You are what you include: large-scale evaluation of remote javascript inclusions. JavaScript is used by web developers to enhance the interactivity of their sites, offload work to the users' browsers and improve their sites' responsiveness and user-friendliness, making web pages feel and behave like traditional desktop applications. An important feature of JavaScript, is the ability to combine multiple libraries from local and remote sources into the same page, under the same namespace. While this enables the creation of more advanced web applications, it also allows for a malicious JavaScript provider to steal data from other scripts and from the page itself. Today, when developers include remote JavaScript libraries, they trust that the remote providers will not abuse the power bestowed upon them. In this paper, we report on a large-scale crawl of more than three million pages of the top 10,000 Alexa sites, and identify the trust relationships of these sites with their library providers. We show the evolution of JavaScript inclusions over time and develop a set of metrics in order to assess the maintenance-quality of each JavaScript provider, showing that in some cases, top Internet sites trust remote providers that could be successfully compromised by determined attackers and subsequently serve malicious JavaScript. In this process, we identify four, previously unknown, types of vulnerabilities that attackers could use to attack popular web sites. Lastly, we review some proposed ways of protecting a web application from malicious remote scripts and show that some of them may not be as effective as previously thought.\"",
        "Document: \"Automated Spyware Collection and Analysis. Various online studies on the prevalence of spyware attest overwhelming numbers (up to 80%) of infected home computers. However, the term spyware is ambiguous and can refer to anything from plug-ins that display advertisements to software that records and leaks user input. To shed light on the true nature of the spyware problem, a recent measurement paper attempted to quantify the extent of spyware on the Internet. More precisely, the authors crawled the web and analyzed the executables that were downloaded. For this analysis, only a single anti-spyware tool was used. Unfortunately, this is a major shortcoming as the results from this single tool neither capture the actual amount of the threat, nor appropriately classify the functionality of suspicious executables in many cases. For our analysis, we developed a fully-automated infrastructure to collect and install executables from the web. We use three different techniques to analyze these programs: an online database of spyware-related identifiers, signature-based scanners, and a behavior-based malware detection technique. We present the results of a measurement study that lasted about ten months. During this time, we crawled over 15 million URLs and downloaded 35,853 executables. Almost half of the spyware samples we found were not recognized by the tool used in previous work. Moreover, a significant fraction of the analyzed programs (more than 80%) was incorrectly classified. This underlines that our measurement results are more comprehensive and precise than those of previous approaches, allowing us to draw a more accurate picture of the spyware threat.\"",
        "Document: \"MalGene: Automatic Extraction of Malware Analysis Evasion Signature. Automated dynamic malware analysis is a common approach for detecting malicious software. However, many malware samples identify the presence of the analysis environment and evade detection by not performing any malicious activity. Recently, an approach to the automated detection of such evasive malware was proposed. In this approach, a malware sample is analyzed in multiple analysis environments, including a bare-metal environment, and its various behaviors are compared. Malware whose behavior deviates substantially is identified as evasive malware. However, a malware analyst still needs to re-analyze the identified evasive sample to understand the technique used for evasion. Different tools are available to help malware analysts in this process. However, these tools in practice require considerable manual input along with auxiliary information. This manual process is resource-intensive and not scalable. In this paper, we present MalGene, an automated technique for extracting analysis evasion signatures. MalGene leverages algorithms borrowed from bioinformatics to automatically locate evasive behavior in system call sequences. Data flow analysis and data mining techniques are used to identify call events and data comparison events used to perform the evasion. These events are used to construct a succinct evasion signature, which can be used by an analyst to quickly understand evasions. Finally, evasive malware samples are clustered based on their underlying evasive techniques. We evaluated our techniques on 2810 evasive samples. We were able to automatically extract their analysis evasion signatures and group them into 78 similar evasion techniques.\"",
        "1 is \"Effective inter-component communication mapping in Android with Epicc: an essential step towards holistic security analysis\", 2 is \"Efficient classification across multiple database relations: a CrossMine approach\"",
        "Given above information, for an author who has written the paper with the title \"Extracting probable command and control signatures for detecting botnets\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001632": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Low Normalized Energy Derivation Asynchronous Circuit Synthesis Flow Through Fork-Join Slack Matching For Cryptographic Applications':",
        "Document: \"An acoustic noise suppression system with reduced musical artifacts. In this paper, we propose an acoustic noise suppression system with reduced musical artifacts for digital hearing instruments (aids). The proposed system features the capabilities to detect, estimate and suppress the acoustic noise corrupting an input speech. The algorithms in the system consist of two noise detections, an enhanced parametric spectral subtraction, a noise attenuation and transition smoothing window, and an automatic gain control. Simulation results on several stationary and nonstationary noise show that our acoustic noise suppression system is capable of improving signal-to-noise ratio by > 9 dB and reducing musical artifacts\"",
        "Document: \"Low delay-variation sub-/near-threshold asynchronous-to-synchronous interface controller for GALS Network-on-Chips. We propose an Asynchronous-to-Synchronous Interface Controller (A2S-IC) with low delay-variation towards Process, Voltage and Temperature (PVT) variations for sub-threshold/near-threshold operation in low power applications. This A2S-IC is targeted for a full-range Dynamic Voltage Scaling (DVS) Global-Asynchronous-Local-Synchronous (GALS) Network-on-Chip (NoC). There are three key attributes in this proposed A2S-IC. First, it is realized using static-logic (over dynamic-logic), hence is more appropriate for DVS (and sub-threshold operation). Second, it is implemented using gate-level standard-cell to simplify the implementation efforts. Third, it is designed to share some internal nodes, hence reducing the redundant switching for data validity checking. The proposed A2S-IC is compared against its reported dynamic-logic counterpart; both are implemented in the same 65nm CMOS process. Based on the simulations conducted at 27 C, our proposed A2S-IC is more throughput-efficient at near- and sub-threshold operations, featuring ~19% and ~66% faster throughput at FDD =0.5V and FDD =0.3V respectively. When the temperature variation (0\u00b0C to 100\u00b0C) is considered at the sub-threshold operation, the proposed A2S-IC demonstrates 140% faster throughput than the reported design, the former only features up to 1.6x delay-variation but the latter exhibits up to 4x delay-variation. The proposed A2S-IC is able to operate at the voltage as low as 0.15V (as opposed to 0.3V for the reported design).\"",
        "Document: \"Asynchronous-Logic QDI Quad-Rail Sense-Amplifier Half-Buffer Approach for NoC Router Design. We propose a low area overhead and power-efficient asynchronous-logic quasi-delay-insensitive (QDI) sense-amplifier half-buffer (SAHB) approach with quad-rail (i.e., 1-of-4) data encoding. The proposed quad-rail SAHB approach is targeted for area- and energy-efficient asynchronous network-on-chip (ANoC) router designs. There are three main features in the proposed quad-rail SAHB approach. First, t...\"",
        "Document: \"Extracting functional modules from flattened gate-level netlist. A generic and highly versatile method for extracting functional modules from a flattened gate-level netlist is proposed. The proposed method requires no prior knowledge about the netlist under analysis and is applicable to circuits targeting diverse applications. It is fully automated and employs a highly compact module library containing only single generic model for each common function type with arbitrary data width. Experiment results depict the efficacy of the proposed method and its embodied techniques.\"",
        "Document: \"Energy-Efficient Synchronous-Logic and Asynchronous-Logic FFT/IFFT Processors. Two 128-point 16-bit radix-2 FFT/IFFT processors based on synchronous-logic (sync) and asynchronous-logic (async) for low voltage (1.1-1.4 V) energy-critical low-speed hearing aids are described. The two processors herein are designed with the same function and similar architecture, and the emphasis is energy efficacy. The async approach, on average, features ~37% lower energy per FFT/IFFT computation than the sync approach but with ~10% larger IC area penalty and an inconsequential 1.4 times worse delay; the async design can be designed to be 0.24 times faster and with largely the same energy dissipation if the matched delay elements and the latch controllers therein are better optimized. In this low-speed application, the lower energy feature of the async design is not attributed to the absence of the clock infrastructure but instead due to the adoption of established and proposed async circuit designs, resulting in reduced redundant operations and reduced spurious/glitch switching, and to the use of latches. The prototype async FFT/IFFT processor (in a 0.35-mum CMOS process) can be operated at 1.0 V and dissipates 93 nJ.\"",
        "1 is \"A 0.018% THD+N, 88-dB PSRR PWM Class-D Amplifier for Direct Battery Hookup.\", 2 is \"A Low-Power SRAM Using Bit-Line Charge-Recycling\"",
        "Given above information, for an author who has written the paper with the title \"Low Normalized Energy Derivation Asynchronous Circuit Synthesis Flow Through Fork-Join Slack Matching For Cryptographic Applications\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001672": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'VoIP Intrusion Detection Through Interacting Protocol State Machines':",
        "Document: \"E-commerce Reputation Manipulation: The Emergence of Reputation-Escalation-as-a-Service. In online markets, a store's reputation is closely tied to its profitability. Sellers' desire to quickly achieve high reputation has fueled a profitable underground business, which operates as a specialized crowdsourcing marketplace and accumulates wealth by allowing online sellers to harness human laborers to conduct fake transactions for improving their stores' reputations. We term such an underground market a seller-reputation-escalation (SRE) market. In this paper, we investigate the impact of the SRE service on reputation escalation by performing in-depth measurements of the prevalence of the SRE service, the business model and market size of SRE markets, and the characteristics of sellers and offered laborers. To this end, we have infiltrated five SRE markets and studied their operations using daily data collection over a continuous period of two months. We identified more than 11,000 online sellers posting at least 219,165 fake-purchase tasks on the five SRE markets. These transactions earned at least $46,438 in revenue for the five SRE markets, and the total value of merchandise involved exceeded $3,452,530. Our study demonstrates that online sellers using SRE service can increase their stores' reputations at least 10 times faster than legitimate ones while only 2.2% of them were detected and penalized. Even worse, we found a newly launched service that can, within a single day, boost a seller's reputation by such a degree that would require a legitimate seller at least a year to accomplish. Finally, armed with our analysis of the operational characteristics of the underground economy, we offer some insights into potential mitigation strategies.\"",
        "Document: \"Your Remnant Tells Secret: Residual Resolution in DDoS Protection Services. The increasing prevalence of Distributed Denial of Service (DDoS) attacks on the Internet has led to the wide adoption of DDoS Protection Service (DPS), which is typically provided by Content Delivery Networks (CDNs) and is integrated with CDN's security extensions. The effectiveness of DPS mainly relies on hiding the IP address of an origin server and rerouting the traffic to the DPS provider's distributed infrastructure, where malicious traffic can be blocked. In this paper, we perform a measurement study on the usage dynamics of DPS customers and reveal a new vulnerability in DPS platforms, called residual resolution, by which a DPS provider may leak origin IP addresses when its customers terminate the service or switch to other platforms, resulting in the failure of protection from future DPS providers as adversaries are able to discover the origin IP addresses and launch the DDoS attack directly to the origin servers. We identify that two major DPS/CDN providers, Cloudflare and Incapsula, are vulnerable to such residual resolution exposure, and we then assess the magnitude of the problem in the wild. Finally, we discuss the root causes of residual resolution and the practical countermeasures to address this security vulnerability.\"",
        "Document: \"PowerTracer: tracing requests in multi-tier services to diagnose energy inefficiency. As energy has become one of the key operating costs in running a data center and power waste commonly exists, it is essential to observe and reduce energy inefficiency inside data centers. In this paper, we develop an innovative framework, called PowerTracer, for diagnosing energy-inefficiency. Inside the framework, we first present a resource tracing method based on request tracing in multi-tier services of black boxes. Then, we propose a generalized methodology of applying a request tracing approach for energy-inefficiency diagnosis in multi-tier service systems. With insights into service performance and resource consumption of individual requests, we develop a bottleneck diagnosis tool that pinpoints the root causes of energy inefficiency. We implement the prototype and conduct experiments to validate its effectiveness.\"",
        "Document: \"All Your DNS Records Point to Us: Understanding the Security Threats of Dangling DNS Records. In a dangling DNS record (Dare), the resources pointed to by the DNS record are invalid, but the record itself has not yet been purged from DNS. In this paper, we shed light on a largely overlooked threat in DNS posed by dangling DNS records. Our work reveals that Dare can be easily manipulated by adversaries for domain hijacking. In particular, we identify three attack vectors that an adversary can harness to exploit Dares. In a large-scale measurement study, we uncover 467 exploitable Dares in 277 Alexa top 10,000 domains and 52 edu zones, showing that Dare is a real, prevalent threat. By exploiting these Dares, an adversary can take full control of the (sub)domains and can even have them signed with a Certificate Authority (CA). It is evident that the underlying cause of exploitable Dares is the lack of authenticity checking for the resources to which that DNS record points. We then propose three defense mechanisms to effectively mitigate Dares with little human effort.\"",
        "Document: \"An Entropy-Based Approach to Detecting Covert Timing Channels. The detection of covert timing channels is of increasing interest in light of recent exploits of covert timing channels over the Internet. However, due to the high variation in legitimate network traffic, detecting covert timing channels is a challenging task. Existing detection schemes are ineffective at detecting most of the covert timing channels known to the security community. In this paper, we introduce a new entropy-based approach to detecting various covert timing channels. Our new approach is based on the observation that the creation of a covert timing channel has certain effects on the entropy of the original process, and hence, a change in the entropy of a process provides a critical clue for covert timing channel detection. Exploiting this observation, we investigate the use of entropy and conditional entropy in detecting covert timing channels. Our experimental results show that our entropy-based approach is sensitive to the current covert timing channels and is capable of detecting them in an accurate manner.\"",
        "1 is \"Covers for Functional Independencies\", 2 is \"Securing web service by automatic robot detection\"",
        "Given above information, for an author who has written the paper with the title \"VoIP Intrusion Detection Through Interacting Protocol State Machines\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001700": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Learning to Remember More with Less Memorization.':",
        "Document: \"Learning Boltzmann Distance Metric for Face Recognition. We introduce a new method for face recognition using a versatile probabilistic model known as Restricted Boltzmann Machine (RBM). In particular, we propose to regularise the standard data likelihood learning with an information-theoretic distance metric defined on intra-personal images. This results in an effective face representation which captures the regularities in the face space and minimises the intra-personal variations. In addition, our method allows easy incorporation of multiple feature sets with controllable level of sparsity. Our experiments on a high variation dataset show that the proposed method is competitive against other metric learning rivals. We also investigated the RBM method under a variety of settings, including fusing facial parts and utilising localised feature detectors under varying resolutions. In particular, the accuracy is boosted from 71.8% with the standard whole-face pixels to 99.2% with combination of facial parts, localised feature extractors and appropriate resolutions.\"",
        "Document: \"Hyper-community detection in the blogosphere. Most existing work on learning community structure in social network is graph-based whose links among the members are often represented as an adjacency matrix, encoding direct pairwise associations between members. In this paper, we propose a method to group online communities in blogosphere based on the topics learnt from the content blogged. We then consider a different type of online community formulation - the sentiment-based grouping of online communities. The problem of sentiment-based clustering for community structure discovery is rich with many interesting open aspects to be explored. We propose a novel approach for addressing hyper-community detection based on users' sentiment. We employ a nonparametric clustering to automatically discover hidden hyper-communities and present the results obtained from a large dataset.\"",
        "Document: \"MCMC for Hierarchical Semi-Markov Conditional Random Fields. Deep architecture such as hierarchical semi-Markov models is an important class of models for nested sequential data. Current exact inference schemes either cost cubic time in sequence length, or exponential time in model depth. These costs are prohibitive for large-scale problems with arbitrary length and depth. In this contri- bution, we propose a new approximation technique that may have the potential to achieve sub-cubic time complexity in length and linear time depth, at the cost of some loss of quality. The idea is based on two well-known methods: Gibbs sam- pling and Rao-Blackwellisation. We provide some simulation-based evaluation of the quality of the RGBS with respect to run time and sequence length.\"",
        "Document: \"Mixed-Variate Restricted Boltzmann Machines.   Modern datasets are becoming heterogeneous. To this end, we present in this paper Mixed-Variate Restricted Boltzmann Machines for simultaneously modelling variables of multiple types and modalities, including binary and continuous responses, categorical options, multicategorical choices, ordinal assessment and category-ranked preferences. Dependency among variables is modeled using latent binary variables, each of which can be interpreted as a particular hidden aspect of the data. The proposed model, similar to the standard RBMs, allows fast evaluation of the posterior for the latent variables. Hence, it is naturally suitable for many common tasks including, but not limited to, (a) as a pre-processing step to convert complex input data into a more convenient vectorial representation through the latent posteriors, thereby offering a dimensionality reduction capacity, (b) as a classifier supporting binary, multiclass, multilabel, and label-ranking outputs, or a regression tool for continuous outputs and (c) as a data completion tool for multimodal and heterogeneous data. We evaluate the proposed model on a large-scale dataset using the world opinion survey results on three tasks: feature extraction and visualization, data completion and prediction. \"",
        "Document: \"AdaBoost.MRF: Boosted Markov Random Forests and Application to Multilevel Activity Recognition. Activity recognition is an important issue in building intelligent monitoring systems. We address the recognition of multilevel activities in this paper via a conditional Markov random field (MRF), known as the dynamic conditional random field (DCRF). Parameter estimation in general MRFs using maximum likelihood is known to be computationally challenging (except for extreme cases), and thus we propose an efficient boosting-based algorithm AdaBoost.MRF for this task. Distinct from most existing work, our algorithm can handle hidden variables (missing labels) and is particularly attractive for smarthouse domains where reliable labels are often sparsely observed. Furthermore, our method works exclusively on trees and thus is guaranteed to converge. We apply the AdaBoost.MRF algorithmto a home video surveillance application and demonstrate its efficacy.\"",
        "1 is \"Sparsity and Robustness in Face Recognition\", 2 is \"Predicting Consumer Behavior With Web Search\"",
        "Given above information, for an author who has written the paper with the title \"Learning to Remember More with Less Memorization.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001735": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Variable message sign and dynamic regional traffic guidance':",
        "Document: \"Comprehensive Global Typography Extraction System for Electronic Book Documents. In this paper, we present a robust system to accurately detect and localize texts in natural scene images. For text detection, a region-based method utilizing multiple features and cascade AdaBoost classifier is adopted. For text localization, a window ...\"",
        "Document: \"Formula Citation Graph Based Mathematical Information Retrieval. Nowadays, with the quick availability and growth of formulae on the Web, the question of how to effectively retrieve the relevant documents about formulae, namely formula retrieval, has attracted much attention from the researchers of mathematical information retrieval (MIR). Existing MIR search engines have explored much information of formulae such as characters, layout structure, the formula context. However, little attention has been paid to the link or citation relations of formulae among different documents, while these relations are helpful for searching some related formulae whose appearances are not similar to the query formula. Therefore, in this paper, we design a Formula Citation Graph (FCG) to 'dig out' the link or citation relations between formulae. FCG has two main advantages: 1) The graph could generate the descriptive keywords of formulae to enrich the semantics of formula queries. 2) The graph is employed to balance the ranking results between the text and structure matching. The experimental results demonstrate that the link or citation relations among formulae are helpful for MIR.\"",
        "Document: \"Context-aware Geometric Object Reconstruction for Mobile Education. The solid geometric objects in the educational geometric books are usually illustrated as 2D line drawings accompanied with description text. In this paper, we present a method to recover the geometric objects from 2D to 3D. Unlike the previous methods, we not only use the geometric information from the line drawing itself, but also the textual information extracted from its context. The essential of our method is a cost function to mix the two types of information, and we optimize the cost function to identify the geometric object and recover its 3D information. Our method can recover various types of solid geometric objects including straight-edge manifolds and curved objects such as cone, cylinder and sphere. We show that our method performs significantly better compared to the previous ones.\"",
        "Document: \"Comic storyboard extraction via edge segment analysis. Comic storyboard extraction aims to decompose the comic image into several storyboards (or frames), which is the key technique to produce the digital comic documents suitable for mobile reading. Previous methods fail either to detect overlapped storyboards or to produce storyboards without blank margins. To tackle these problems, we propose a novel comic storyboard extraction method based on edge segment analysis. First, we extract edge segments (i.e. contiguous chains of Canny edge points) from the input comic image; second, we detect line segments within each obtained edge segment with a top-down scheme; third, we detect storyboards through line segments combination and storyboard validation, and perform post-processing to handle some special cases. We test the proposed method on two datasets comprising 2237 comic pages from 11 printed comic series. Experimental results demonstrate that the proposed method achieves satisfactory results and outperformed the existing methods on the storyboard and page level.\"",
        "Document: \"Web-based citation parsing, correction and augmentation. Considering the tremendous value of citation metadata, many methods have been proposed to automate Citation Metadata Extraction (CME). The existing methods primarily rely on the content analysis of citation text. However, the results from such content-based methods are often unreliable. Moreover, the extracted citation metadata is only a small part of the relevant metadata that spreads across the Internet. As opposed to the content-based CME methods, this paper proposes a Web-based CME approach and a citation enriching system, called as BibAll, which is capable of correcting the parsing results of content-based CME methods and augmenting citation metadata by leveraging relevant bibliographic data from digital repositories and cited-by publications on the Web. BibAll consists of four main components: citation parsing, Web-based bibliographic data retrieval, irrelevant bibliographic data filtering, and relevant bibliographic data integration. The system has been tested on the publicly available FLUX-CIM dataset. Experimental results show that BibAll significantly improves the citation parsing accuracy and augments the metadata of the original citation.\"",
        "1 is \"Dynamic Bayesian Network Based Speech Recognition with Pitch and Energy    as Auxiliary Variables\", 2 is \"Decomposition of complex line drawings with hidden lines for 3D planar-faced manifold object reconstruction.\"",
        "Given above information, for an author who has written the paper with the title \"Variable message sign and dynamic regional traffic guidance\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001799": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Distributed Spanner Construction With Physical Interference: Constant Stretch and Linear Sparseness.':",
        "Document: \"An Invocation Cost Optimization Method for Web Services in Cloud Environment. The advent of cloud computing technology has enabled users to invoke various web services in a \"pay-as-you-go\" manner. However, due to the flexible pricing model of web services in cloud environment, a cloud user' service invocation cost may be influenced by many factors (e.g., service invocation time), which brings a great challenge for cloud users' cost-effective web service invocation. In view of this challenge, in this paper, we first investigate the multiple factors that influence the invocation cost of a cloud service, for example, user's job size, service invocation time, and service quality level; and afterwards, a novel Cloud Service Cost Optimization Method named CS-COMis put forward, by considering the above multiple impact factors. Finally, a set of experiments are designed, deployed, and tested to validate the feasibility of our proposal in terms of cost optimization. The experiment results show that our proposed CS-COM method outperforms other related methods.\"",
        "Document: \"An Energy-Aware Distributed Unequal Clustering Protocol For Wireless Sensor Networks. Due to the imbalance of energy consumption of nodes in wireless sensor networks (WSNs), some local nodes die prematurely, which causes the network partitions and then shortens the lifetime of the network. The phenomenon is called \"hot spot\" or \"energy hole\" problem. For this problem, an energy-aware distributed unequal clustering protocol (EADUC) in multihop heterogeneous WSNs is proposed. Compared with the previous protocols, the cluster heads obtained by EADUC can achieve balanced energy, good distribution, and seamless coverage for all the nodes. Moreover, the complexity of time and control message is low. Simulation experiments show that EADUC can prolong the lifetime of the network significantly.\"",
        "Document: \"Addressing the Threats of Inference Attacks on Traits and Genotypes from Individual Genomic Data. The decreasing cost of DNA-sequencing empowers high availability of genetic-oriented services, which further promote growing number of genomes and traits of individuals being accessible online. Notoriously, these data are sensitive and may further lead to more sensitive data leakage. In this paper, we formulate the trait and genotype inference problem and develop an efficient inference method based on factor graph and belief propagation. An adversary then can infer the potential traits and genotypes of the victims whose portions of data are observed, depending on trait/SNP associations available from GWAS catalog. To protect against such inference attacks, we detail privacy and utility metrics then propose a genomic data-sanitization method that can effectively tradeoff genomic data openness and privacy.\"",
        "Document: \"Edge Dominating Capability based Backbone Construction in Wireless Networks.   Constructing a connected dominating set as the virtual backbone plays an important role in wireless networks. In this paper, we propose two novel approximate algorithms for dominating set and connected dominating set in wireless networks, respectively. Both of the algorithms are based on edge dominating capability which is a novel notion proposed in this paper. Simulations show that each of proposed algorithm has good performance especially in dense wireless networks . \"",
        "Document: \"On the fractional f-chromatic index of a graph. For a graph G, we assign a positive integer f(v) to each vertex v\u2208V(G). The f-chromatic index of G, denoted by [image omitted]\u00a0, is the minimum number of f-matchings needed to partition E(G), where an f-matching of G is the edge set of a (0, f)-factor of G. In this paper, we give the exact value of the fractional f-chromatic index of a graph.\"",
        "1 is \"Probabilistic Reliable Dissemination in Large-Scale Systems\", 2 is \"Spectrally-efficient relay selection with limited feedback\"",
        "Given above information, for an author who has written the paper with the title \"Distributed Spanner Construction With Physical Interference: Constant Stretch and Linear Sparseness.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001932": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Attacks on the (enhanced) Yang-Shieh authentication':",
        "Document: \"Exploiting the Intrinsic Irreversibility of Adaptive Technologies to Enhance the Security of Digital Watermarking. To overcome the reversibility of the widely researched adaptive watermarking, the existing method based on cryptography is first directly revised for adaptive systems. The intrinsic irreversibility of some adaptive technologies is then disclosed and evaluated. Under these technologies, attackers have great difficulty dividing a released version into their claimed original data and scaled watermarks, and in the meantime making the latter be the particular adaptive results based on the former. Their reversed solutions are violently perturbed and perceptually unacceptable. The condition number of the coefficient matrix of the reverse equations can be employed to assess the degree of the perturbation. The proposed method is more efficient because it disposes of any cryptographic processing, such as hashing and stream generation. The experiments on images support the theoretic conclusion well.\"",
        "Document: \"Analysis and Improvements of Two Identity-Based Perfect Concurrent Signature Schemes. One of the tasks of data mining is classification, which provides a mapping from attributes (observations) to pre-specified classes. Classification models are built by using underlying data. In principle, the models built with more data yield better ...\"",
        "Document: \"Homomorphic Linear Authentication Schemes for Proofs of Retrievability. In a proof of retrievability (POR) system, interactive POR protocols are executed between a storage server and clients, so that clients can be convinced that their data is available at the storage server, ready to be retrieved when needed. In an interactive POR protocol, clients initiate challenges to the server, and the server feedbacks responses to clients with input of the stored data. Retrievability means that it should be possible for a client to extract the his/her data from the server's valid responses. An essential step-stone leading to retrievability is server's unforgeability of valid responses, i.e, any server coming up valid responses to a client's challenges is actually storing the client's data with overwhelming probability. Unforgeability can be achieved with authentication schemes like MAC, Digital Signature, etc. With homomorphic linear authentication schemes, the authenticators can be aggregated into one tag for the challenges, hence reducing the communication complexity. In this paper, we explore some new homomorphic linear authenticator schemes in POR to provide unforgeability. Compared with the recent work of Shacham and Waters, our scheme enjoys the same shortest responses, but reduces the local storage from O(s) to O(1).\"",
        "Document: \"Attribute-Based Key-Insulated Encryption. Attribute-based encryption (ABE) is an exciting alternative to public-key encryption, as ABE develops encryption systems with high expressiveness, without the need for a public key infrastructure (PKI) that makes publicly available the mapping between identities (sets of attributes), public keys, and validity of the latter. Any setting, PKI or attribute-based, must provide a means to revoke users from the system. To mitigate the limitation of ABE with regard to revocation, we propose an attribute-based key-insulated encryption (ABKIE) scheme, which is a novel ABE scheme. In our ABKIE scheme, a private key can be renewed without having to make changes to its public key (a set of attributes). The scheme is secure against adaptive chosen ciphertext attacks. The formal proof of security is presented under the Selective-ID security model, i.e. without random oracles, assuming the decision Bilinear Diffie-Hellman problem is computationally hard. To the best of our knowledge, this is the first ABKIE scheme up to now. Further, this is also the first concrete ABE construction with regard to revocation.\"",
        "Document: \"Sender-Equivocable Encryption Schemes Secure against Chosen-Ciphertext Attacks Revisited. Abstract Fehr et al. 2010 proposed the first sender-equivocable encryption scheme secure against chosen-ciphertext attacks NCCCA and proved that NC-CCA security implies security against selective opening chosen-ciphertext attacks SO-CCA. The NC-CCA security proof of the scheme relies on security against substitution attacks of a new primitive, the \\\"crossauthentication code\\\". However, the security of the cross-authentication code cannot be guaranteed when all the keys used in the code are exposed. Our key observation is that, in the NC-CCA security game, the randomness used in the generation of the challenge ciphertext is exposed to the adversary. Based on this observation, we provide a security analysis of Fehr et al.'s scheme, showing that its NC-CCA security proof is flawed. We also point out that the scheme of Fehr et al. encrypting a single-bit plaintext can be refined to achieve NC-CCA security, free of the cross-authentication code. Furthermore, we propose the notion of \\\"strong cross-authentication code\\\", apply it to Fehr et al.'s scheme, and show that the new version of the latter achieves NC-CCA security for multi-bit plaintexts.\"",
        "1 is \"Relations among Notions of Security for Public-Key Encryption Schemes.\", 2 is \"Multiparty computation for interval, equality, and comparison without bit-decomposition protocol\"",
        "Given above information, for an author who has written the paper with the title \"Attacks on the (enhanced) Yang-Shieh authentication\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001978": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Improved prediction of the number of residue contacts in proteins by recurrent neural networks.':",
        "Document: \"Reconstruction of 3D Structures from Protein Contact Maps. Proteins are large organic compounds made of amino acids arranged in a linear chain (primary structure). Most proteins fold into unique three-dimensional (3D) structures called interchangeably tertiary, folded, or native structures. Discovering the tertiary structure of a protein (Protein Folding Problem) can provide important clues about how the protein performs its function and it is one of the most important problems in Bioinformatics. A contact map of a given protein P is a binary matrix M such that M-i,M-j = 1 iff the physical distance between amino acids i and j in the native structure is less than or equal to a pre-assigned threshold t. The contact map of each protein is a distinctive signature of its folded structure. Predicting the tertiary structure of a protein directly from its primary structure is a very complex and still unsolved problem. An alternative and probably more feasible approach is to predict the contact map of a protein from its primary structure and then to compute the tertiary structure starting from the predicted contact map. This last problem has been recently proven to be NP-Hard [6]. In this paper we give a heuristic method that is able to reconstruct in a few seconds a 3D model that exactly matches the target contact map. We wish to emphasize that our method computes an exact model for the protein independently of the contact map threshold. To our knowledge, our method outperforms all other techniques in the literature [5,10,17,19] both for the quality of the provided solutions and for the running times. Our experimental results are obtained on a non-redundant data set consisting of 1760 proteins which is by far the largest benchmark set used so far. Average running times range from 3 to 15 seconds depending on the contact map threshold and on the size of the protein. Repeated applications of our method (starting from randomly chosen distinct initial solutions) show that the same contact map may admit (depending on the threshold) quite different 3D models. Extensive experimental results show that contact map thresholds ranging from 10 to 18 Angstrom allow to reconstruct 3D models that are very similar to the proteins native structure. Our Heuristic is freely available for testing on the web at the following url: http://vassura.web.cs.unibo.it/cmap23d/.\"",
        "Document: \"On The Reconstruction Of Three-Dimensional Protein Structures From Contact Maps. The problem of protein structure prediction is one of the long-standing goals of Computational Biology. Although we are still not able to provide first principle solutions, several shortcuts have been discovered to compute the protein three-dimensional structure when similar protein sequences are available (by means of comparative modeling and remote homology detection). Nonetheless, these approaches can assign structures only to a fraction of proteins in genomes and ab-initio methods are still needed. One relevant step of ab-initio prediction methods is the reconstruction of the protein structures starting from inter-protein residue contacts. In this paper we review the methods developed so far to accomplish the reconstruction task in order to highlight their differences and similarities. The different approaches are fully described and their reported performances, together with their computational complexity, are also discussed.\"",
        "Document: \"A three-state prediction of single point mutations on protein stability changes. A basic question of protein structural studies is to which extent mutations affect the stability. This question may be addressed starting from sequence and/or from structure. In proteomics and genomics studies prediction of protein stability free energy change (DeltaDeltaG) upon single point mutation may also help the annotation process. The experimental DeltaDeltaG values are affected by uncertainty as measured by standard deviations. Most of the DeltaDeltaG values are nearly zero (about 32% of the DeltaDeltaG data set ranges from -0.5 to 0.5 kcal/mole) and both the value and sign of DeltaDeltaG may be either positive or negative for the same mutation blurring the relationship among mutations and expected DeltaDeltaG value. In order to overcome this problem we describe a new predictor that discriminates between 3 mutation classes: destabilizing mutations (DeltaDeltaG<-1.0 kcal/mol), stabilizing mutations (DeltaDeltaG>1.0 kcal/mole) and neutral mutations (-1.0</=DeltaDeltaG</=1.0 kcal/mole).In this paper a support vector machine starting from the protein sequence or structure discriminates between stabilizing, destabilizing and neutral mutations. We rank all the possible substitutions according to a three state classification system and show that the overall accuracy of our predictor is as high as 56% when performed starting from sequence information and 61% when the protein structure is available, with a mean value correlation coefficient of 0.27 and 0.35, respectively. These values are about 20 points per cent higher than those of a random predictor.Our method improves the quality of the prediction of the free energy change due to single point protein mutations by adopting a hypothesis of thermodynamic reversibility of the existing experimental data. By this we both recast the thermodynamic symmetry of the problem and balance the distribution of the available experimental measurements of free energy changes. This eliminates possible overestimations of the previously described methods trained on an unbalanced data set comprising a number of destabilizing mutations higher than stabilizing ones.\"",
        "Document: \"AlignBucket: a tool to speed up \u2018all-against-all\u2019 protein sequence alignments optimizing length constraints. Motivation: The next-generation sequencing era requires reliable, fast and efficient approaches for the accurate annotation of the ever-increasing number of biological sequences and their variations. Transfer of annotation upon similarity search is a standard approach. The procedure of all-against-all protein comparison is a preliminary step of different available methods that annotate sequences based on information already present in databases. Given the actual volume of sequences, methods are necessary to pre-process data to reduce the time of sequence comparison. Results: We present an algorithm that optimizes the partition of a large volume of sequences (the whole database) into sets where sequence length values (in residues) are constrained depending on a bounded minimal and expected alignment coverage. The idea is to optimally group protein sequences according to their length, and then computing the all-against-all sequence alignments among sequences that fall in a selected length range. We describe a mathematically optimal solution and we show that our method leads to a 5-fold speed-up in real world cases.\"",
        "Document: \"Prediction of Signal Peptide in Proteins with Neural Networks. In this paper we present a new Neural-Network-based predictor trained and tested on a set of well annotated proteins to tackle the problem of predicting the signal peptide in protein sequences. The method trained on a set of experimentally derived signal peptides from Eukaryotes and Prokaryotes, identifies the presence of the sorting signal and predicts their cleavage sites. The accuracy in cross-validation is comparable with previously presented programs reaching the 97%, 96% and 95% for Gram negative, Gram positive and Eukaryotes, respectively.\"",
        "1 is \"MODBASE, a database of annotated comparative protein structure models.\", 2 is \"Learning capability and storage capacity of two-hidden-layer feedforward networks.\"",
        "Given above information, for an author who has written the paper with the title \"Improved prediction of the number of residue contacts in proteins by recurrent neural networks.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001988": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Unknown-tolerance analysis and test-quality control for test response compaction using space compactors':",
        "Document: \"Signal transition graph transformations for initializability. We present a method of transforming a functionally uninitializable signal transition graph (STG) into a functionally initializable STG. The design of a trigger module is described to illustrate the transformations\"",
        "Document: \"Design of testable sequential circuits by repositioning flip-flops. This paper presents a technique to enhance the testability of sequential circuits by repositioning flip-flops. A novel retiming for testability technique is proposed that reduces cycle lengths in the dependency graph, converts sequential redundancies into combinational redundancies, and yields retimed circuits that usually require fewer scan flip-flops to break all cycles (except self-loops) as compared to the original circuit. Our technique is based on a new minimum cost flow formulation that simultaneously considers the interactions among all strongly connected components (SCCs) of the circuit graph to minimize the number of flip-flops in the SCCs. A circuit graph has a vertex for every gate, primary input and primary output. If gatea has a fanout to gateb, then the circuit graph has an arc from vertexa to vertexb. Experimental results on several large sequential circuits demonstrate the effectiveness of the proposed retiming for testability technique in reducing the number of partial scan flip-flops.\"",
        "Document: \"Managing the Quality vs. Efficiency Trade-off Using Dynamic Effort Scaling. Several current and emerging applications do not have a unique result for a given input; rather, functional correctness is defined in terms of output quality. Recently proposed design techniques exploit the inherent resilience of such applications and achieve improved efficiency (energy or performance) by foregoing correct execution of all the constituent computations. Hardware and software systems that are thus designed may be viewed as scalable effort systems, since they offer the capability to modulate the effort that they expend towards computation, thereby allowing for trade-offs between output quality and efficiency. We propose the concept of Dynamic Effort Scaling (DES), which refers to dynamic management of the control knobs that are exposed by scalable effort systems. We argue the need for DES by observing that the degree of resilience often varies significantly across applications, across datasets, and even within a dataset. We propose a general conceptual framework for DES by formulating it as a feedback control problem, wherein the scaling mechanisms are regulated with the goal of maintaining output quality at or above a specified limit. We present an implementation of Dynamic Effort Scaling for recognition and mining applications and evaluate it for the support vector machines and K-means clustering algorithms under various application scenarios and datasets. Our results clearly demonstrate the benefits of the proposed approach---statically setting the scaling mechanisms leads to either significant error overshoot or significant opportunities for energy savings left on the table unexploited. In contrast, DES is able to effectively regulate the output quality while maximally exploiting the time-varying resiliency in the workload.\"",
        "Document: \"Software transformations for sequential test generation. This paper presents software (model) transformations that can be used to effectively generate high fault coverage test sets. Unlike synthesis or design for testability methods which involve hardware modifications, this approach does not modify the hardware design. Instead, it transforms a software model of the design into a new software model that has desirable testability properties. A sequential test generator generates tests for the new model. The new model may not be functionally equivalent to the original design but our transformations guarantee that the tests generated for the new model can always be inverse mapped to serve as tests for the original design. Experimental results show that significantly high fault coverage can be achieved by using this approach.\"",
        "Document: \"Partial scan design for technology mapped circuits. For a vast majority of production VLSI designs, the synthesis pipeline is interrupted and technology mapping is performed manually. Here, designers map functional specifications directly onto a more richer set of library blocks that include counters and registers. Typically, these blocks have more than one memory element. The scan version of such a block has all flip-flops chained into a shift register during test mode. For such designs, we show that existing partial scan selection methods may produce sub-optimal solutions. We then propose a new method of selecting scan flip-flops in mapped designs. Our algorithm is based on a new formulation that models the presence of multiple memory elements in a library block and also takes into account both area and performance penalties of scan. We also extend a recently proposed integer linear program (ILP) formulation. A graph transformation that was effective in solving the scan selection problem for large synthesized (or unmapped) designs is shown to be inapplicable for mapped designs. We then develop a new transformation that provably preserves optimum solutions for these mapped designs. Experimental results on three production VLSI circuits having 12,000 to over 50,000 gates are reported.\"",
        "1 is \"Topic 3 Scheduling and Load-Balancing\", 2 is \"Memristor-based memory: The sneak paths problem and solutions\"",
        "Given above information, for an author who has written the paper with the title \"Unknown-tolerance analysis and test-quality control for test response compaction using space compactors\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001994": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Inferring Generative Model Structure with Static Analysis':",
        "Document: \"A Deep-learning Approach for Prognosis of Age-Related Macular Degeneration Disease using SD-OCT Imaging Biomarkers. We propose a hybrid sequential deep learning model to predict the risk of AMD progression in non-exudative AMD eyes at multiple timepoints, starting from short-term progression (3-months) up to long-term progression (21-months). Proposed model combines radiomics and deep learning to handle challenges related to imperfect ratio of OCT scan dimension and training cohort size. We considered a retrospective clinical trial dataset that includes 671 fellow eyes with 13,954 dry AMD observations for training and validating the machine learning models on a 10-fold cross validation setting. The proposed RNN model achieved high accuracy (0.96 AUCROC) for the prediction of both short term and long-term AMD progression, and outperformed the traditional random forest model trained. High accuracy achieved by the RNN establishes the ability to identify AMD patients at risk of progressing to advanced AMD at an early stage which could have a high clinical impact as it allows for optimal clinical follow-up, with more frequent screening and potential earlier treatment for those patients at high risk.\"",
        "Document: \"Expanding a radiology lexicon using contextual patterns in radiology reports. Objective: Distributional semantics algorithms, which learn vector space representations of words and phrases from large corpora, identify related terms based on contextual usage patterns. We hypothesize that distributional semantics can speed up lexicon expansion in a clinical domain, radiology, by unearthing synonyms from the corpus. Materials and Methods: We apply word2vec, a distributional semantics software package, to the text of radiology notes to identify synonyms for RadLex, a structured lexicon of radiology terms. We stratify performance by term category, term frequency, number of tokens in the term, vector magnitude, and the context window used in vector building. Results: Ranking candidates based on distributional similarity to a target term results in high curation efficiency: on a ranked list of 775 249 terms, >50% of synonyms occurred within the first 25 terms. Synonyms are easier to find if the target term is a phrase rather than a single word, if it occurs at least 100x in the corpus, and if its vector magnitude is between 4 and 5. Some RadLex categories, such as anatomical substances, are easier to identify synonyms for than others. Discussion: The unstructured text of clinical notes contains a wealth of information about human diseases and treatment patterns. However, searching and retrieving information from clinical notes often suffer due to variations in how similar concepts are described in the text. Biomedical lexicons address this challenge, but are expensive to produce and maintain. Distributional semantics algorithms can assist lexicon curation, saving researchers time and money.\"",
        "Document: \"Using automatically extracted information from mammography reports for decision-support. Display Omitted We use an NLP system, extracting information from mammography reports, to provide inputs to a DSS.We demonstrate Bayesian network reasoning based on variety of features extracted from the text reports.We evaluate the impact of imperfect extracted information on the performance of the DSS.We evaluate the NLP-DSS by considering both qualitative and quantitative DSS outputs. ObjectiveTo evaluate a system we developed that connects natural language processing (NLP) for information extraction from narrative text mammography reports with a Bayesian network for decision-support about breast cancer diagnosis. The ultimate goal of this system is to provide decision support as part of the workflow of producing the radiology report. Materials and methodsWe built a system that uses an NLP information extraction system (which extract BI-RADS descriptors and clinical information from mammography reports) to provide the necessary inputs to a Bayesian network (BN) decision support system (DSS) that estimates lesion malignancy from BI-RADS descriptors. We used this integrated system to predict diagnosis of breast cancer from radiology text reports and evaluated it with a reference standard of 300 mammography reports. We collected two different outputs from the DSS: (1) the probability of malignancy and (2) the BI-RADS final assessment category. Since NLP may produce imperfect inputs to the DSS, we compared the difference between using perfect (\\\"reference standard\\\") structured inputs to the DSS (\\\"RS-DSS\\\") vs NLP-derived inputs (\\\"NLP-DSS\\\") on the output of the DSS using the concordance correlation coefficient. We measured the classification accuracy of the BI-RADS final assessment category when using NLP-DSS, compared with the ground truth category established by the radiologist. ResultsThe NLP-DSS and RS-DSS had closely matched probabilities, with a mean paired difference of 0.004\u017a0.025. The concordance correlation of these paired measures was 0.95. The accuracy of the NLP-DSS to predict the correct BI-RADS final assessment category was 97.58%. ConclusionThe accuracy of the information extracted from mammography reports using the NLP system was sufficient to provide accurate DSS results. We believe our system could ultimately reduce the variation in practice in mammography related to assessment of malignant lesions and improve management decisions.\"",
        "Document: \"On combining image-based and ontological semantic dissimilarities for medical image retrieval applications. \u2022We propose an image retrieval system that considers the semantic of medical images.\u2022It relies on automatic annotation of the images and evaluation of their dissimilarities.\u2022Dissimilarity is computed using visual and ontological relations among image annotations.\u2022It provides a means of capturing the semantic correlations between image contents.\u2022Evaluation on the retrieval of liver CT images proved the interest of this system.\"",
        "Document: \"The caBIGTM Annotation and Image Markup Project. Image annotation and markup are at the core of medical interpretation in both the clinical and the research setting. Digital\n medical images are managed with the DICOM standard format. While DICOM contains a large amount of meta-data about whom, where,\n and how the image was acquired, DICOM says little about the content or meaning of the pixel data. An image annotation is the\n explanatory or descriptive information about the pixel data of an image that is generated by a human or machine observer.\n An image markup is the graphical symbols placed over the image to depict an annotation. While DICOM is the standard for medical\n image acquisition, manipulation, transmission, storage, and display, there are no standards for image annotation and markup.\n Many systems expect annotation to be reported verbally, while markups are stored in graphical overlays or proprietary formats.\n This makes it difficult to extract and compute with both of them. The goal of the Annotation and Image Markup (AIM) project\n is to develop a mechanism, for modeling, capturing, and serializing image annotation and markup data that can be adopted as\n a standard by the medical imaging community. The AIM project produces both human- and machine-readable artifacts. This paper\n describes the AIM information model, schemas, software libraries, and tools so as to prepare researchers and developers for\n their use of AIM.\"",
        "1 is \"Noise reduction for magnetic resonance images via adaptive multiscale products thresholding.\", 2 is \"Flipper: A Systematic Approach to Debugging Training Sets.\"",
        "Given above information, for an author who has written the paper with the title \"Inferring Generative Model Structure with Static Analysis\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002119": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'HCI-enriched approach for DSS development: the UP/U approach':",
        "Document: \"A Rule-Based Approach to the Ergonomic \"Static\" Evaluation of Man-Machine Graphic Interface in Industrial Processes. This paper aims at describing a rule-based approach for the ergonomic \"static\" evaluation of man-machine graphic interface in industrial processes. In the first part, we present some research works that directly or indirectly contribute to the ergonomical design or evaluation of man-machine interfaces. In the second part, we propose an ergonomic methodology for man-machine interface design. In a way to improve the information presentation, this methodology includes an expert system called SYNOP. This system is described in the third part of the paper. Finally, the last part discusses about the possible interest of such a rule- based approach.\"",
        "Document: \"Evaluation of Agent-based Interactive Systems: Proposal of an Electronic Informer Using Petri Nets. The evaluation of interactive systems has been an active subject of research for many years. Many methods and tools have been proposed but most of them do not take architectural specificities of agent-based interactive systems into account. In addition, electronic informers are popular evaluation tools but current ones have often some limits. In order to solve these problems, we propose an electronic informer to evaluate agent-based interactive systems. This tool captures interaction events occurred in agent-based interactive systems and then, based on such captured data, it realizes treatments such as calculations, statistics and generates Petri Nets (PNs) to assist evaluators in evaluating three aspects of the system: user interface, non-functional properties (e.g. response time, reliability, etc.) and user's properties (e.g. abilities, preferences, etc.). The approach has been validated by applying it to evaluate an agent-based interactive system used for the supervision of urban transport network.\"",
        "Document: \"Towards multimodal user interfaces composition based on UsiXML and MBD principles. In software design, the reuse issue brings the increasing of web services, components and others techniques. These techniques allow reusing code associated to technical aspect (as software component). With the development of business components which can integrate technical aspect with HCI, the composition issue has appeared. Our previous work concerned the GUI composition based on an UIDL as UsiXML. With the generalization of Multimodal User Interfaces (MUI), MUI composition principles have to be studied. This paper aims at extend existing basic composition principles in order to treat multimodal interfaces. The same principle as in the previous work, based on the tree algebra, can be used in another level (AUI) of the UsiXML framework to support the Multimodal User Interfaces composing. This paper presents a case study on the food ordering system based on multimodal (coupling GUI and MUI). A conclusion and the future works in the HCI domain are presented.\"",
        "Document: \"Integrating the SE and HCI models in the human factors engineering cycle for re-engineering Computerized Physician Order Entry systems for medications: basic principles illustrated by a case study. The integration of Human Factors is still insufficient in the design and implementation phases of complex interactive systems such as Computerized Physician Order Entry (CPOE) systems. One of the problems is that human factors specialists have difficulties to communicate their data and to have them properly understood by the computer scientists in the design and implementation phases. This paper presents a solution to this problem based on the creation of common documentation supports using Software Engineering (SE) and Human-Computer Interaction (HCI) methods.The integration of SE and HCI methods and models is an interesting means for modelling an organization's activities, with software applications being part of these activities. Integrating these SE and HCI methods and models allows case studies to be seen from the technical, organizational and ergonomic perspectives, and also makes it easier to compare current and future work situations.The exploitation of these techniques allows the creation of common work supports that can be easily understandable by computer scientists and relevant for re-engineering or design. In this paper, the basic principles behind such communication supports are described and illustrated by a real case study.\"",
        "Document: \"Architecture modelling and evaluation of agent-based interactive systems. This paper presents an architecture for agent-based interactive systems as well as an evaluation method adapted to such an architecture. In order to position the proposed architecture, the current state of the art concerning architectures used for traditional interactive systems is presented, along with that for agent-based interactive systems. The advantages and drawbacks of these architectures are discussed. There are two main types of architecture: those with functional components and those with structural components. in this article, the proposed agent-oriented architecture is considered as being mixed (it is both functional and structural). It creates a separation into three functional components, respectively known as: the interface with the application, the dialogue controller and the presentation. By using this architecture as a basis, we will propose an evaluation tool called \"electronic informer\" which will allow us to obtain and analyse the human-machine interactions in agent-based environments. This electronic informer is made up of several informer agents taken from the evaluation of the system architecture and more particularly from the multi-agent presentation system. It is based essentially on the acquisition of data coining from the evaluated global system.\"",
        "1 is \"A cluster validity index for fuzzy clustering\", 2 is \"Identifying Usability Issues For Personalization During Formative Evaluations: A Comparison Of Three Methods\"",
        "Given above information, for an author who has written the paper with the title \"HCI-enriched approach for DSS development: the UP/U approach\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002165": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Result merging strategies for a current news metasearcher':",
        "Document: \"Ad hoc retrieval with the Persian language. This paper describes our participation to the Persian ad hoc search during the CLEF 2009 evaluation campaign. In this task, we suggest using a light suffix-stripping algorithm for the Farsi (or Persian) language. The evaluations based on different probabilistic models demonstrated that our stemming approach performs better than a stemmer removing only the plural suffixes, or statistically better than an approach ignoring the stemming stage (around +4.5%) or a n-gram approach (around +4.7%). The use of a blind query expansion may significantly improve the retrieval effectiveness (between +7% to +11%). Combining different indexing and search strategies may further enhance the MAP (around +4.4%).\"",
        "Document: \"Consid\u00e9rations sur l'\u00e9valuation de la robustesse en recherche d'information. This paper describes and evaluates vector-space, probabilistic and language IR models used to retrieve news articles from a corpus written in the French language. Based on three CLEF test-collections and 151 topics, we analyze the retrieval effectiveness of these approaches and analyze the poor retrieval results of hard topics. An appropriate robust evaluation is not easy because both the mean average precision (MAP) or the geometric mean (GMAP) present some drawbacks. In order to obtain a better picture, we suggest using the First Relevant Score (or FRS, based on the rank of the first relevant item). We evaluate and compare these three measures in particular when using blind query expansion technique.\"",
        "Document: \"Statistical inference in retrieval effectiveness evaluation. Evaluation methodology, and particularly its statistical tests associated, plays a central role in the information retrieval domain which maintains a strong empirical tradition. In an effort to evaluate the retrieval effectiveness of a search algorithm, this paper focuses on the average precision over a set of fixed recall values. After reviewing traditional evaluation methodology through the use of examples, this study suggests applying another statistical inference methodology called bootstrap, within which no particular assumption is needed about the distribution of the observations. Moreover, this scheme may be used to assert the accuracy of virtually any statistic, to build approximate confidence interval, and to verify whether a statistically significant difference exists between two retrieval schemes, even when dealing with a relatively small sample size. This study also suggests selecting the sample median rather than the sample mean in evaluating retrieval effectiveness where the justification for this choice is based on the nature of the information retrieval data.\"",
        "Document: \"Information retrieval strategies for digitized handwritten medieval documents. This paper describes and evaluates different IR models and search strategies for digitized manuscripts. Written during the thirteenth century, these manuscripts were digitized using an imperfect recognition system with a word error rate of around 6%. Having access to the internal representation during the recognition stage, we were able to produce four automatic transcriptions, each introducing some form of spelling correction as an attempt to improve the retrieval effectiveness. We evaluated the retrieval effectiveness for each of these versions using three text representations combined with five IR models, three stemming strategies and two query formulations. We employed a manually-transcribed error-free version to define the ground-truth. Based on our experiments, we conclude that taking account of the single best recognition word or all possible top-k recognition alternatives does not provide the best performance. Selecting all possible words each having a log-likelihood close to the best alternative yields the best text surrogate. Within this representation, different retrieval strategies tend to produce similar performance levels.\"",
        "Document: \"Simple and efficient classification scheme based on specific vocabulary. Assuming a binomial distribution for word occurrence, we propose computing a standardized Z score to define the specific vocabulary of a subset compared to that of the entire corpus. This approach is applied to weight terms (character n-gram, word, stem, lemma or sequence of them) which characterize a document. We then show how these Z score values can be used to derive a simple and efficient categorization scheme. To evaluate this proposition and demonstrate its effectiveness, we develop two experiments. First, the system must categorize speeches given by B. Obama as being either electoral or presidential speech. In a second experiment, sentences are extracted from these speeches and then categorized under the headings electoral or presidential. Based on these evaluations, the proposed classification scheme tends to perform better than a support vector machine model for both experiments, on the one hand, and on the other, shows a better performance level than a Na\u00efve Bayes classifier on the first test and a slightly lower performance on the second (10-fold cross validation).\"",
        "1 is \"A comparison of Chinese document indexing strategies and retrieval models\", 2 is \"Robust Classification for Imprecise Environments\"",
        "Given above information, for an author who has written the paper with the title \"Result merging strategies for a current news metasearcher\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002220": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Bimanual Interaction with Interscopic Multi-Touch Surfaces':",
        "Document: \"Presence-enhancing real walking user interface for first-person video games. For most first-person video games it is important that players have a high level of feeling presence in the displayed game environment. Virtual reality (VR) technologies have enormous potential to enhance gameplay since players can experience the game immersively from the perspective of the player's virtual character. However, the VR technology itself, such as tracking devices and cabling, has until recently restricted the ability of users to really walk over long distances. In this paper we introduce a VR-based user interface for presence-enhancing gameplay with which players can explore the game environment in the most natural way, i. e., by real walking. While the player walks through the virtual game environment, we guide him/her on a physical path which is different from the virtual path and fits into the VR laboratory space. In order to further increase the VR experience, we introduce the concept of transitional environments. Such a transitional environment is a virtual replica of the laboratory environment, where the VR experience starts and which enables a gradual transition to the game environment. We have quantified how much humans can unknowingly be redirected and whether or not a gradual transition to a first-person game via a transitional environment increases the user's sense of presence.\"",
        "Document: \"Navigation through Geospatial Environments with a Multi-Touch enabled Human-Transporter Metaphor. Geospatial environments often provide users with complex and de- tailed 3D data sets. Although, many different visualization tech- niques allow interesting insights, intuitive and natural exploration approaches are often missing such that only domain-experts are able to efficiently explore these data sets. In this paper we present an interaction metaphor, which allows even non-expert to naturally navigate through a virtual 3D city model based on a real-world metaphor. Recent developments in the area of interactive surfaces enable the construction of low-cost multi-touch displays and relatively inexpensive sensor technology to detect foot gestures, which allows to explore these input modal- ities. We demonstrate how multi-touch hand gestures in combina- tion with foot gestures can be used to perform navigation tasks in interactive 3D geospatial environments. We describe the 3D nav- igation metaphor as well as the corresponding hardware and im- plementation. Furthermore, we have performed an experiment in which we observed and analyzed users using our navigation tech- nique.\"",
        "Document: \"A Universal Virtual Locomotion System: Supporting Generic Redirected Walking And Dynamic Passive Haptics Within Legacy 3d Graphics Applications. In this paper we introduce a virtual locomotion system that allows navigation within any large-scale virtual environment (VE) by real walking. In contrast to [5] we have developed generic redirected walking concepts by combining motion compression, i.e., scaling the real distance users walk, rotation gains, which make the real turns smaller or larger, and curvature gains, which bend the user's walking direction such that s/he walks on a curve (see Figure I (left)). Furthermore, we introduce the new concept of dynamic passive haptics which extends passive haptics [3, 4] in such a way that any number of virtual objects can be sensed by means of real proxy objects having similar haptic capabilities, i.e., size, shape and surface structure. We have evaluated these concepts and explain technical details regarding their integration into legacy 3D graphics applications.\"",
        "Document: \"Interactive Position-dependent Customization of Transfer Function Classification Parameters in Volume Rendering. In direct volume rendering (DVR) and related techniques a basic operation is the classification of data values by mapping (mostly scalar) intensities to color values using a transfer function. However, in some cases this kind of mapping might not suffice to achieve satisfying rendering results, for instance when intensity homogeneities occur in the volume data due to technical restrictions of the scanner technology. In this case it might be desirable to customize one or more parameters of the visualization depending on the position within the volume. In this paper we propose a novel approach for an interactive position-dependent customization of arbitrary parameters of the transfer function classification. Our method can easily be integrated into existing volume rendering pipelines by incorporating an additional operation during the classification step. It allows the user to interactively modify the rendering result by specifying reference points within the data set and customizing their associated visualization parameters while receiving direct visual feedback. Since the additional memory requirements of our method do not depend on the size of the visualized data our approach is applicable to large data sets, for instance in the field of ultra microscopy.\"",
        "Document: \"Haptic props: semi-actuated tangible props for haptic interaction on the surface. While multiple methods to extend the expressiveness of tangible interaction have been proposed, e. g., self-motion, stacking and transparency, providing haptic feedback to the tangible prop itself has rarely been considered. In this poster we present a semi-actuated, nano-powered, tangible prop, which is able to provide programmable friction for interaction with a tabletop setup. We have conducted a preliminary user study evaluating the users' acceptance for the device and their ability to detect changes in the programmed level of friction and received some promising results.\"",
        "1 is \"Supporting I/O-efficient scientific computation in TPIE\", 2 is \"Impacts of Forced Serious Game Play on Vulnerable Subgroups\"",
        "Given above information, for an author who has written the paper with the title \"Bimanual Interaction with Interscopic Multi-Touch Surfaces\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002264": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'i-Hand: An intelligent robotic hand for fast and accurate assembly in electronic manufacturing':",
        "Document: \"Autonomous robot photographer with KL divergence optimization of image composition and human facial direction. In this paper, we propose a robot photography system that can autonomously search for the optimal target viewpoint. Two technical issues of scene composition evaluation and viewpoint selection are solved by this system. A composition evaluation method for photos is developed using well-known composition rules based on Kullback\u2013Leibler (KL) divergence, considering the directional information of each target. To reduce the calculation cost of the composition evaluation in the case where the number of targets is large, automatic target grouping is conducted via variational Bayes. The optimal viewpoint with respect to the composition is selected from a number of candidate viewpoints around the targets based on KL divergence. Finally, the fact that better composed photos can be autonomously photographed by the proposed system is validated via experiments and human evaluations.\"",
        "Document: \"Cane-supported walking by humanoid robot and falling-factor-based optimal cane usage selection. In this study, we propose a scheme that enables a humanoid robot to perform cane-supported walking and to select the optimal cane usage depending on its condition. A reaction force is generated through the cane-ground interaction. Although the optimal cane-ground contact position leads to the most effective reaction force from the viewpoint of the walking capability, the cane position is constrained during such contact. Thus, different cane utilities can be achieved based on the priority assigned to the reaction force and cane position. In this light, we propose cyclic, leg-like, and preventive usages, each of which has different priorities. These different priorities lead to different utilities such as the expansion of the support polygon, load distribution, and posture recovery. The robot selects the most suitable of these three cane usages using a selection algorithm for locomotion that is combined with Q-learning (Us-SAL). Through simulations, the robot is made to learn the affinities between these cane usages and falling factors as Q-values and to accordingly select the optimal cane usage. As a result, the robot can perform the desired walking by selecting the optimal cane usage depending on its condition. We conducted walking experiments for each cane usage and found that each improved the walking stability for a suitable falling factor. Furthermore, we experimentally validated Us-SAL; the robot selected the optimal cane usage depending on its condition and walked in a complex environment without falling. We propose a scheme that enables humanoid robots to perform cane-supported walking.Cyclic, leg-like, and preventive cane usages are designed to adapt falling factors.The robot selects the optimal cane usage according to estimated falling factor.Affinities between cane usages and falling factors are used for the selection.Q-learning makes the robot learn the affinities via dynamics simulations.\"",
        "Document: \"3-D biped walking over rough terrain based on the assumption of point-contact. This paper describes a 3-D biped walking over rough terrain. The robot is modeled as the special 3-D inverted pendulum that can change the length. The dynamics of the 3-D inverted pendulum is modeled as 2-D autonomous system by applying the Passive Dynamic Autonomous Control (PDAC) that is based on the assumption of point-contact of the robot foot and the virtual holonomic constraint as to robot joints. We analyze the stability of the 2-D autonomous system by use of Poincare\u0301e map, and derive the stable range over rough terrain. By applying the virtual compliance control to an actual robot \u201cGorilla Robot III\u201d, the angle of the pendulum is modified. Finally, the 3-D biped walking over rough terrain is realized by use of the Gorilla Robot III.\"",
        "Document: \"A PSO-based Mobile Sensor Network for Odor Source Localization in Dynamic Environment: Theory, Simulation and Measurement. This paper presents a problem of odor source localization in a dynamic environment, which means the odor distribution is changing over time. Most work on chemical sensing with mobile robots assume an experimental setup that minimizes the influence of turbulent transport by either minimizing the source-to-sensor distance in trail following or by assuming a strong unidirectional air stream in the environment, including our previous work. However, not much attention has been paid to the natural environment problem. Modification Particle Swarm Optimization is a well-known algorithm, which can continuously track a changing optimum over time. PSO can be improved or adapted by incorporating the change detection and responding mechanisms for solving dynamic problems. Charged PSO, which is another extension of the PSO, has also been applied to solve dynamic problems. Odor source localization is an interesting application in dynamic problems. We will adopt two types of PSO modification concepts to develop a new algorithm in order to control autonomous vehicles. Before applying the algorithm for real implementation, some important hardware conditions must be considered. Firstly, to reduce the possibility of robots leaving the search space, a limit to the value of velocity vector is needed. The value of vector velocity can be clamped to the range [-Vmax, Vmax]; in our case for the MK-01 Robot, the maximum velocity is 0.05 m/s. Secondly, in the standard PSO algorithm there is no collision avoidance mechanism. To avoid the collision among robot we add some collision avoidance functions. Finally, we also add some sensor noise, delay and threshold value to model the sensor response. Then we develop odor localization algorithm, and simulations to show that the new approach can solve such dynamic environment problems.\"",
        "Document: \"A genetic algorithm for subtask allocation within human and robot coordinated assembly. In human and robot collaborative assembly cell (Hybrid Cell), it is important to develop automaic subtask allocation method for human and robot in usage both offline and online way. In this research, first, a logic mathematic method is developed to describe this discrete event system by considering the system trade-off between the assembly time cost and payment cost. A genetic based algorithm is developed for realtime and reliable subtask allocation while maintaining this trade-off. The performance of proposed algorithm is analyzed from the experiment data study about an assembly case.\"",
        "1 is \"Capacity of Ad Hoc wireless networks\", 2 is \"Steering of Multisegment Continuum Manipulators Using Rigid-Link Modeling and FBG-Based Shape Sensing.\"",
        "Given above information, for an author who has written the paper with the title \"i-Hand: An intelligent robotic hand for fast and accurate assembly in electronic manufacturing\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002279": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Possibility and complexity of probabilistic reliable communication in directed networks':",
        "Document: \"Privacy Preserving Shortest Path Computation in Presence of Convex Polygonal Obstacles. Shortest path computation in presence of obstacles has been a subject of study since long and so has been the study of privacy preserving algorithms. In this paper we design efficient privacy preserving algorithms for computing the shortest path circumventing convex polygonal obstacles. Specifically, we assume that a party A has source sand destination d while another party B has the list of convex polygonal obstacles and A wishes to find the shortest path from s to d without compromising each others\u2019 privacy. That is, at the end of the protocol, A must not know anything else about the obstacles other that what may be revealed by the shortest path that is output and B must not know anything about A\u2019s source/destination.\"",
        "Document: \"On reporting the L1 metric closest pair in a query rectangle. In this work, we consider the problem of finding the closest pair (in L\"1 metric) of points in an orthogonal query rectangle. Given a set of n static points on a UxU grid, we preprocess these points into a data structure of size O(mf(m)log^2m) that can be queried in time O((g(m)+loglogm)log^3m), for m=O(nlogU) and (i) f(m)=O(1) and g(m)=O(log^@em); (ii) f(m)=O(loglogm) and g(m)=O(loglogm); (iii) f(m)=O(log^@em) and g(m)=O(1). Here @e: 0\"",
        "Document: \"Interplay between (im)perfectness, synchrony and connectivity: the case of reliable message transmission. For unconditionally reliable message transmission (URMT) in synchronous directed networks of n nodes, a subset of which may be malicious, it is well-known that the minimum connectivity requirements for zero-error (perfect) protocols to exist is strictly higher than those where a negligible yet non-zero error probability is allowed (Monte Carlo protocols) [12]. In this work, we study the minimum connectivity requirements for the existence of (a) synchronous Las Vegas, (b) asynchronous Monte Carlo, and (c) asynchronous Las Vegas protocols for URMT. Interestingly, we prove that in any network, a synchronous Las Vegas URMT protocol exists if and only if an asynchronous Monte Carlo URMT protocol exists too. We further show that in any network, an asynchronous Las Vegas URMT protocol exists if and only if a synchronous perfect protocol exists as well. Thus, our results establish an interesting interplay between (im)perfectness, synchrony and connectivity for the case of URMT.\"",
        "Document: \"Choosing and Working of an Anonymous Leader. We revisit the problem of anonymous communication where nodes communicate without revealing their identity. We propose its use in the choosing of a node as an anonymous leader and subsequent communication to and from this leader such that its identity is not revealed. We give indistinguishability definitions for anonymous leader and prove it to be secure in an arbitrary reliable network.\"",
        "Document: \"Allowing Multiple Rounds in the Shared Whiteboard Model: Some More Impossibility Results. The shared whiteboard model for distributed computing is one of the recent interesting models to be proposed (See Becker et al. (SPAA 2012)). In its basic form, this model allows all nodes to write a message of no more than O(log n) bits on a whiteboard that every node can read. However, each node can write at most once. In this model, a variety of problems from graphs are shown to be either possible or impossible. In this paper we extend the work of Becker et al. to allow for nodes to write on the shared whiteboard more than once. However, each node can write at most O(log n) bits at any one instant. Interestingly, in this model, we show that allowing just two rounds of writing on the whiteboard, one can color the vertices of a d-degenerate graph using d+1 colors. Similarly, we show that two rounds suffice to find maximal independent set (MIS), whereas 2-ruling sets can be computed in one round in simultaneous synchronous models. Finally, we show that for finding connected components in a graph, even O(1) rounds is not enough in general. We show that any deterministic algorithm that follows certain rules requires at least Omega(log nlog log n) rounds to find the connected components of an n-vertex graph. At the same time, we show the existence of a O(log n) round algorithm for the same. Thus, our results indicate that the multi-round shared whiteboard model has interesting consequences.\"",
        "1 is \"Efficient statistical asynchronous verifiable secret sharing with optimal resilience\", 2 is \"Computing an st-numbering\"",
        "Given above information, for an author who has written the paper with the title \"Possibility and complexity of probabilistic reliable communication in directed networks\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002319": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A suboptimal feedback control for nonlinear time-varying systems with continuous inequality constraints.':",
        "Document: \"Variable structure adaptive fuzzy control for a class of nonlinear time-delay systems. In this paper, the problem of variable structure adaptive fuzzy control for a class of nonlinear time-delay systems is investigated. Sliding surface dependent of the time delay is constructed based on the Lyapunov\u2013Krasovskii method. Two adaptive fuzzy logic systems are proposed to approximate two unknown continuous nonlinear functions containing the current state and the delayed state, respectively. The corresponding reaching law is designed, which can drive the state trajectory of the closed system onto the sliding surface within limited time. Simulation investigation verifies the effect of the main results of the paper.\"",
        "Document: \"Time domain solution to descriptor variable systems. The time domain solution to the fast subsystem of the descriptor variable system of the form Ex\u02d9=Ax+Bu with E singular is given by Laplace inverse transformation. A new and significant feature of the solution is that it contains impulse terms excited by the values of sufficiently smooth input u and its derivatives at the initial time point. Based on the solution, the notions of consistent initial conditions, classical solutions and impulse-free solutions are discussed perfectly. The impulse controllability is discussed also which admits the same interpretation as the controllability at infinity in frequency domain.\"",
        "Document: \"Stabilization via Fully Actuated System Approach: A Case Study. In this note, a benchmark example system which is not stabilizable by a smooth state feedback controller is considered with the fully actuated system (FAS) approach. It is shown that a smooth controller exists which drives the trajectories starting from a large domain in the initial value space to the origin exponentially. Such a result brings about a generalization of Lyapunov asymptotical stability, which is termed as global exponential sub-stability. The region of attraction is allowed to be an unbounded open set of the initial values with closure containing the origin. This sub-stability result may be viewed to be superior to some local stability results in the Lyapunov sense because the region of attraction is much larger than any finite ball containing the origin and meanwhile the feasible trajectories are always driven to the origin exponentially. Based on this sub-stabilization result, globally asymptotically stabilizing controllers for the system can be provided in two general ways, one is through combination with existing globally stabilizing controllers, and the other is by using a pre-controller to first move an initial point which is not within the region of attraction into the region of attraction.\"",
        "Document: \"Robust H-Infinity Filter Design For 2d Discrete Systems In Roesser Model. This paper investigates the robust H-infinity filtering problem for uncertain two-dimensional (2D) systems described by the Roesser model. The parameter uncertainties considered in this paper are assumed to be of polytopic type. A new structured polynomially parameter-dependent method is utilized, which is based on homogeneous polynomially parameter-dependent matrices of arbitrary degree. The proposed method includes results in the quadratic framework and the linearly parameter-dependent framework as special cases for zeroth degree and first degree, respectively. A numerical example illustrates the feasibility and advantage of the proposed filter design methods.\"",
        "Document: \"On solutions of matrix equations V\u2212AVF=BW and V\u2212AV\u0304F=BW. With the help of the Kronecker map, a complete, general and explicit solution to the Yakubovich matrix equation V \u2212 A V F = B W , with F in an arbitrary form, is proposed. The solution is neatly expressed by the controllability matrix of the matrix pair ( A , B ) , a symmetric operator matrix and an observability matrix. Some equivalent forms of this solution are also presented. Based on these results, explicit solutions to the so-called Kalman\u2013Yakubovich equation and Stein equation are also established. In addition, based on the proposed solution of the Yakubovich matrix equation, a complete, general and explicit solution to the so-called Yakubovich-conjugate matrix is also established by means of real representation. Several equivalent forms are also provided. One of these solutions is neatly expressed by two controllability matrices, two observability matrices and a symmetric operator matrix.\"",
        "1 is \"H2 gain scheduled observer based controllers for rational LPV systems\", 2 is \"Adjoint polynomials and chromatically unique graphs\"",
        "Given above information, for an author who has written the paper with the title \"A suboptimal feedback control for nonlinear time-varying systems with continuous inequality constraints.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002356": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Acoustic sensor network design for position estimation':",
        "Document: \"On Source Dependency Models for Reliable Social Sensing: Algorithms and Fundamental Error Bounds. This paper develops a simplified dependency model for sources on social networks that is shown to improve the quality of fact-finding -- assessing veracity of observations shared on social media. Recent literature developed a mathematical approach for exploiting social networks, such as Twitter, as noisy sensor networks that report observations on the state of the physical world. It was shown that the quality of state estimation from such noisy data, known as fact-finding, was a function of assumptions made regarding the independence of sources or lack thereof. When sources propagate information they hear from others (without verification), correlated errors may arise that degrade fact-finding performance. This work advances the state of the art by developing a simplified model of dependencies between sources and designing an improved dependency-aware estimator to assess veracity of observations, taking into account the observed dependency structure. A fundamental error bound is derived for this estimator to understand the gap in its performance from optimal. It is shown that the new estimator outperforms state of the art fact-finders and, in some cases, yields an accuracy close to the fundamental error bound.\"",
        "Document: \"Pruned Multiangle Resolution Fast Beamforming. Delay-and-sum (DS) beamforming is a simple processing method that can estimate the direction-of-arrival from multiple signal sources. The major advantage of DS beamforming is that it can handle wideband as well as narrowband signals. However, DS beamforming exhibits high computational complexity. The multiangle resolution fast beamformer was proposed as a computationally efficient approximation of DS beamforming, reducing the computational order of complexity from  Open image in new window  to  Open image in new window . In this paper, we introduce the pruned multiangle resolution fast beamformer to further reduce the computational complexity. The new algorithm includes an energy detector at intermediate stages of the fast beamformer to prune sectors that do not exhibit increasing energy consistent with coherent integration. Simulations are provided to assess the performance of the pruned fast beamformer. One use for the estimates from the pruned fast beamformer is to initialize high-resolution direction-of-arrival (DOA) estimators such as coherent signal subspace methods.\"",
        "Document: \"Extending self-similarity for fractional Brownian motion. The fractional Brownian motion (fBm) model has proven to be valuable in modeling many natural processes because of its persistence for large time lags. However, the model is characterized by one single parameter that cannot distinguish between short- and long-term correlation effects. This article investigates the idea of extending self-similarity to create a correlation model that generalizes discrete fBm referred to as asymptotic fBm (afBm). Namely, afBm is parameterized by variables controlling short- and long-term correlation effects. It proposes a fast parameter estimation algorithm for afBm based on the Haar transform, and demonstrates the performance of this parameter estimation algorithm with numerical simulations\"",
        "Document: \"Multi-sensor data fusion: An unscented least squares approach. This manuscript provides an approach to solve the nonlinear least squares problem that arises in decentralized fusion. Even though almost all sensor noise can be modeled as additive noise, the additive nature of the measurement noise is lost when the signal is processed at the sensor node. The proposed approach employs the unscented transformation before the estimation problem at the central node is posed as a nonlinear least squares problem. Numerical simulations indicate that the proposed unscented transformation based approach yields desired results.\"",
        "Document: \"Social Fusion: Integrating Twitter and Instagram for Event Monitoring. This paper describes the implementation of a service to identify and geo-locate real world events that may be present as social activity signals in two different social networks. Specifically, we focus on content shared by users on Twitter and Instagram in order to design a system capable of fusing data across multiple networks. Past work has demonstrated that it is indeed possible to detect physical events using various social network platforms. However, many of these signals need corroboration in order to handle events that lack proper support within a single network. We leverage this insight to design an unsupervised approach that can correlate event signals across multiple social networks. Our algorithm can detect events and identify the location of the event occurrence. We evaluate our algorithm using both simulations and real world datasets collected using Twitter and Instagram. The results indicate that our algorithm significantly improves false positive elimination and attains high precision compared to baseline methods on real world datasets.\"",
        "1 is \"Nonlinear Approximation by Sums of Exponentials and Translates\", 2 is \"Histograms of Oriented Gradients for Human Detection\"",
        "Given above information, for an author who has written the paper with the title \"Acoustic sensor network design for position estimation\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002361": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'On sum rate and power consumption of multi-user distributed antenna system with circular antenna layout':",
        "Document: \"Fast Parallel Garner Algorithm for Chinese Remainder Theorem. This paper presents a fast parallel garner algorithm for Chinese remainder theorem. The variables in garner algorithm are divided into public parameters that are constants for fixed module and private parameters that represent random input integers. We design the parallel garner algorithm by analyzing the data dependencies of these arithmetic operations for computing public variables and private variables. Time complexities and speedup ratios of the parallel algorithm and the sequential algorithm are calculated to make the quantitative comparison based on our previous work about some fundamental parallel algorithms. The performance evaluation shows high efficiency of the proposed parallel algorithm compared to the sequential one. \u00a9 IFIP International Federation for Information Processing 2012.\"",
        "Document: \"A DNA Computing System of Modular-Multiplication over Finite Field GF(2n). The enormous parallel computing ability and high memory density of DNA computing bring potential challenges and opportunities to traditional cryptography. Finite field GF(2n) is one of the most commonly used mathematic sets for cryptography. It is an open problem that how to implement the arithmetic operations over finite field GF(2n) based on DNA computing. Existing research has the problem that the lengths of parameters in the DNA tile assembly process could not match each other strictly. This paper proposes a parallel molecular computing system to compute the modular-multiplication, an operation combining multiplication and reduction over finite field GF(2n). The multiplication and the reduction are executed simultaneously in this system. One concrete example of <InlineEquation ID=\\\"IEq1\\\" <EquationSource Format=\\\"TEX\\\"$1100 \\\\cdot 1001 \\\\ mod \\\\ 10011$</EquationSource> </InlineEquation> is proposed to show the details of our tile assembly system. The time complexity of this system is \u0398(n) and the space complexity is \u0398(n 2). This system requires 210 types of computation tiles and 17 types of boundary tiles.\"",
        "Document: \"Optimization of AP placement in indoor fingerprint positioning. There has been significant development in indoor positioning techniques based on location fingerprinting in wireless networks. AP placement has an impact on indoor fingerprint positioning precision. In this study, a max-min RSS distance method to optimize AP placement in KNN fingerprint positioning is proposed for location accuracy and the simulation results are presented based on IEEE 802.11 TGn channel models.\"",
        "Document: \"Opportunistic spectrum access based on channel quality under general collision constraints. This paper studies the problem of cognitive access in a hierarchical network with practical time varying channels. We propose an access policy which is not only based on the sensing outcomes about the occupation states of the channel, but also based on the channel quality situations. The contribution in this paper is as follows. First we propose an access policy which takes advantages of the characteristics of the channel and thereby fully utilizes the access opportunities that channel could offer. The optimization objective of cognitive access is designed to maximize the effective throughput subject to collision constraints imposed by channels. It is shown that our proposed algorithm can achieve high throughput performance by grasping the access opportunities which have good channel qualities. Second we extend the throughput region achieved by secondary users under general collision constraints. Specifically, our algorithm can achieve better throughput performance not only when collision constraints are tight, but also under loose collision constraints, which has not been studied in previous related work. Besides, We also give the practical algorithms to implement our policy, which makes it easily to be deployed in practice. In order to validate the theoretical analysis, numerical results are presented to show that by introducing the proposed policy, which can fully utilize the characteristics of application scenarios and take the advantages of access opportunities of the channel when it is of high quality, the effective throughput can be improved significantly under general collision constraints.\"",
        "Document: \"Utility-Based Adaptive Multi-Frame Iterative Algorithm For Resource Scheduling In Ofdma Networks. This paper focuses on the downlink resource scheduling with quality-of-service (QoS) provisioning in orthogonal-requency-division multiplexing (OFDM) networks. In homogeneous networks, the multi-user diversity gain is achievable through exploiting the independent fading among users. However, in heterogeneous networks, the different priorities and QoS constraints of heterogeneous services, especially delay sensitive services, bring to inferior system performance. To solve this problem, a utility-based adaptive multi-frame iterative (AMI) algorithm aware of both channel and queue state information is proposed. The AMI algorithm contains two parts: the core and the iterative shell. In the core, the problem of dynamic subcarrier assignment (DSA) is modeled as the weighted independent set (WIS) problem in graph theory. Through joint optimization among subcarriers, a higher spectral efficiency is achieved. In the iterative shell, intra-frame and inter-frame iterations exploit the time diversity, thus a lower packet loss is achieved.\"",
        "1 is \"Spatial Subgroup Mining Integrated in an Object-Relational Spatial Database\", 2 is \"Achieving near-capacity at low SNR on a multiple-antenna multiple-user channel\"",
        "Given above information, for an author who has written the paper with the title \"On sum rate and power consumption of multi-user distributed antenna system with circular antenna layout\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002431": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Terminological difficulties in fuzzy set theory\u2014The case of \u201cIntuitionistic Fuzzy Sets\u201d':",
        "Document: \"An extended numerical analysis of an intuitionistic fuzzy classifier for imbalanced classes. Recognizing relatively smaller classes (called imbalanced classes) from data is an important task both from a theoretical and practical points of view. In many real world problems smaller classes are usually more interesting from the user point of view but they are more difficult to obtain by a classifier. This paper, which is a continuation of our previous works, discusses a classifier that is based on Atanassovs intuitionistic fuzzy sets (A-IFSs, for short) and shows that it can be a good tool for recognizing imbalanced classes. Our considerations are illustrated on benchmark examples. Special attention is paid to a detailed behavior of the classifier proposed (different measures besides the general accuracy are examined). A simple cross validation method is applied (with 10 experiments). Results are compared with a fuzzy classifier reported to be good in the literature. Also the influence of the type granulation, symmetrical or asymmetrical, and of the number of intervals is considered.\"",
        "Document: \"A New Insight into the Linguistic Summarization of Time Series Via a Degree of Support: Elimination of Infrequent Patterns. We extend our previous works on using a fuzzy logic based calculus of linguistically quantified propositions for linguistic summarization of time series (cf. Kacprzyk, Wilbik and Zadrozny [4, 5, 6, 7, 8, 9, 10, 11, 12, 13]. That approach, using the classic degree of truth (validity) to be maximized, is here extended by adding a degree of support. On the one hand, this can reflect in natural language the essence of traditional statistical approaches, and on the other hand, can help discard linguistic summaries with a high degree of truth but a low degree of support so that they concern infrequently occurring patterns and may be uninteresting. We show an application to the absolute performance analysis of an investment (mutual) fund.\"",
        "Document: \"On bilateral matching between fuzzy sets. In the paper, we describe the new measure of matching fuzzy sets. The introduced measure of perturbation of one fuzzy set by another fuzzy set is considered instead of commonly used distance between two fuzzy sets. The operations known in the fuzzy set theory are used and the perturbation of one fuzzy set by another fuzzy set is understood as a measure describing changes of the first fuzzy set after adding the second one. Obviously, the opposite case can also be considered wherein the second fuzzy set is perturbed by the first one. In general, the new measure is asymmetric and can provide more information compare to a distance between fuzzy sets. The values of such measures of fuzzy sets perturbation are in range between 0 and 1. In this paper several mathematical properties of the measure of fuzzy sets perturbation are studied, and the measure of sets perturbation is compared to other selected measures.\"",
        "Document: \"Linguistic Summarization of Time Series by Using the Choquet Integral. We further extend a new approach to a linguistic summarization of time series proposed in our previous works (cf. Kacprzyk, Wilbik and Zadro\u017cny [1,2,3,4,5]) in which we put forward the use of a fuzzy linguistic quantifier driven aggregation of trends (partial scores) via the traditional Zadeh calculus of linguistically quantified propositions and the Sugeno integral. Here we use for this purpose the Choquet integral that has been widely advocated for many decision analytic and economic problems. The results are intuitively appealing and the method is effective and efficient.\"",
        "Document: \"Compound bipolar queries: The case of data with a variable quality. We further develop our concept of a compound query (cf. Kacprzyk and Zadroi\u017eny [23]) in which in a bipolar query comprising of a required and desired condition aggregated via a non-conventional operator corresponding to \u201cand if possible\u201d the particular required and desired conditions are by themselves queries with fuzzy linguistic quantifiers. We use our approach to the dealing with data quality (trustworthiness), originally developed for the queries with fuzzy linguistic quantifiers (cf. Kacprzyk and Zadrozny [25]), employ it for the required and desired conditions (queries with fuzzy linguistic quantifiers), and then implant into the compound query, i.e. a bipolar query with the required and desired conditions being queries with fuzzy linguistic quantifiers. A new conceptual quality, functionality and human consistency is therefore obtained.\"",
        "1 is \"Validity Queries and Completeness Queries\", 2 is \"Inclusion Measures in Intuitionistic Fuzzy Set Theory\"",
        "Given above information, for an author who has written the paper with the title \"Terminological difficulties in fuzzy set theory\u2014The case of \u201cIntuitionistic Fuzzy Sets\u201d\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002464": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Runtime compositional analysis of track-based traffic control systems.':",
        "Document: \"Coordinated actor model of self-adaptive track-based traffic control systems. Abstract   Self-adaptation is a well-known technique to handle growing complexities of software systems, where a system autonomously adapts itself in response to changes in a dynamic and unpredictable environment. With the increasing need for developing self-adaptive systems, providing a model and an implementation platform to facilitate integration of adaptation mechanisms into the systems and assuring their safety and quality is crucial. In this paper, we target Track-based Traffic Control Systems (TTCSs) in which the traffic flows through pre-specified sub-tracks and is coordinated by a traffic controller. We introduce a coordinated actor model to design self-adaptive TTCSs and provide a general mapping between various TTCSs and the coordinated actor model. The coordinated actor model is extended to build large-scale self-adaptive TTCSs in a decentralized setting. We also discuss the benefits of using Ptolemy II as a framework for model-based development of large-scale self-adaptive systems that supports designing multiple hierarchical MAPE-K feedback loops interacting with each other. We propose a template based on the coordinated actor model to design a self-adaptive TTCS in Ptolemy II that can be instantiated for various TTCSs. We enhance the proposed template with a predictive adaptation feature. We illustrate applicability of the coordinated actor model and consequently the proposed template by designing two real-life case studies in the domains of air traffic control systems and railway traffic control systems in Ptolemy II.\"",
        "Document: \"HPobSAM for modeling and analyzing IT Ecosystems - Through a case study. The next generation of software systems includes systems composed of a large number of distributed, decentralized, autonomous, interacting, cooperating, organically grown, heterogeneous, and continually evolving subsystems, which we call IT Ecosystems. Clearly, we need novel models and approaches to design and develop such systems which can tackle the long-term evolution and complexity problems. In this paper, our framework to model IT Ecosystems is a combination of centralized control (top-down) and self-organizing (bottom-up) approach. We use a flexible formal model, HPobSAM, that supports both behavioral and structural adaptation/evolution. We use a detailed, close to real-life, case study of a smart airport to show how we can use HPobSAM in modeling, analyzing and developing an IT Ecosystem. We provide an executable formal specification of the model in Maude, and use LTL model checking and bounded state space search provided by Maude to analyze the model. We develop a prototype of our case study designed by HPobSAM using Java and Ponder2. Due to the complexity of the model, we cannot check all properties at design time using Maude. We propose a new approach for run-time verification of our case study, and check different types of properties which we could not verify using model checking. As our model uses dynamic policies to control the behavior of systems which can be modified at runtime, it provides us a suitable capability to react to the property violation by modification of policies.\"",
        "Document: \"Formal Semantics and Analysis of Component Connectors in Reo. We present an operational semantics for a component composition language called Reo. Reo connectors exogenously compose and coordinate the interactions among individual components that comprise a complex system, into a coherent collaboration. The formal semantics we present here paves the way for a rigorous study of the behavior of component composition mechanisms. To demonstrate the feasibility of such a rigorous approach, we give a faithful translation of Reo semantics into the Maude term rewriting language. This translation allows us to exploit the rewriting engine and the model-checking module in the Maude tool-set to symbolically run and model-check the behavior of Reo connectors.\"",
        "Document: \"Formal Verification of the IEEE 802.1D Spanning Tree Protocol Using Extended Rebeca. The STP (Spanning Tree Protocol) which is standardized as IEEE 802.1D has been used in many bridges and switches of networks. This algorithm tries to eliminate loops in bridged networks. In this study the correctness of STP algorithm is formally verified using Extended Rebeca. In order to not to be confined to a specific case or set of cases we used a compositional verification approach. This allows us to gain generality in verifying the algorithm. The clarity and convenience in model checking by means of Extended Rebeca suggests that this language can be used for verifying more network protocols in future.\"",
        "Document: \"Modeling and analysis of Reo connectors using alloy. Reo is an exogenous coordination language based on a calculus of channel composition. Different formal models have been developed for this language. In this paper, we present a new approach to modeling and analysis of Reo connectors using Alloy which is a lightweight modeling language based on first-order relational logic. We provide a reusable library of Reo channels in Alloy that can be used to create a model of a Reo connector in Alloy. The model is simple and reflects the original structure of the connector. Furthermore, the model of a connector can be reused as a component for constructing more complex connectors. Using the Alloy Analyzer tool, properties expressed as predicates can be verified by automatically analyzing the execution traces of the Reo connector. We handle the context-sensitive behavior of channels as well as optional constraints on the interactions with environment. Our compositional model can be used as an alternative to other existing approaches, and is supported by a well known tool with a rich set of features such as counterexample generation.\"",
        "1 is \"Aggregation and Degradation in JetStream: Streaming Analytics in the Wide Area.\", 2 is \"Natjam: design and evaluation of eviction policies for supporting priorities and deadlines in mapreduce clusters\"",
        "Given above information, for an author who has written the paper with the title \"Runtime compositional analysis of track-based traffic control systems.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002473": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'On Performance Loss of Some CoMP Techniques under Channel Power Imbalance and Limited Feedback':",
        "Document: \"MIMO adaptive codebook for cross-polarized antenna arrays. In this paper we address the design of MIMO double codebook structure for cross-polarized antenna arrays. Double codebooks consist of two codebook components, a wide-band long-term component and a short-term sub-band component. Herein we study particularly the two layer short-term sub-band component and provide the complete set of structure types leading to orthogonal constant modulus codewords and cross-polarization phase of M-PSK alphabet. By 3GPP LTE system simulations we show that a novel, two layer structure type, provides significant improvements in channels with large angular spreads.\"",
        "Document: \"Mimo-Ofdm Channel Estimation With Eigenbeamforming And User-Specific Reference Signals. MIMO-OFDM systems with channel reciprocity allow spatial multiplexing using Eigenbeamforming. However, Eigenbeamforming requires singular value decomposition, which is phase ambiguous and causes the precoded channel to become phase discontinuous in frequency domain. This changes statistical properties of channel components, which degrades the performance of channel estimation.In this paper we propose phase unifying method to improve channel statistics and consequently decrease the error in channel estimation. In addition, we investigate the 2D-Wiener channel estimation with respect to different channel frequency correlation functions. The performance is evaluated using LTE link simulator.\"",
        "Document: \"On The Analysis And Design Of Practical Quantization For Opportunistic Beamforming. We propose a closed-loop scheme that combines opportunistic beamforming (OBF) and closed-loop (CL) transmit-diversity (TD) techniques. This quantized OBF scheme is compatible with CL TD algorithms, and allows to adaptively control the amount of feedback overhead according to the number of participating users. Closed-form expressions for the SNR gain of the quantized OBF proposal are derived. Important reductions in the amount of feedback overhead are observed without affecting the SNR gain considerably, when compared to CL TD techniques.\"",
        "Document: \"On Number of Almost Blank Subframes in Heterogeneous Cellular Networks. In heterogeneous cellular scenarios with macrocells, femtocells or picocells users may suffer from significant co-channel cross-tier interference. To manage this interference 3GPP introduced almost blank subframe (ABSF), a subframe in which the interferer tier is not allowed to transmit data. Vulnerable users thus get a chance to be scheduled in ABSFs with reduced cross-tier interference. We analyze downlink scenarios using stochastic geometry and formulate a condition for the required number of ABSFs based on base station placement statistics and user throughput requirement. The result is a semi-analytical formula that serves as a good initial estimate and offers an easy way to analyze impact of network parameters. We show that while in macro/femto scenario the residue ABSF interference can be well managed, in macro/pico scenario it affects the number of required ABSFs strongly. The effect of ABSFs is subsequently demonstrated via user throughput simulations. Especially in the macro/pico scenario, we find that using ABSFs is advantageous for the system since victim users no longer suffer from poor performance for the price of relatively small drop in higher throughput percentiles.\"",
        "Document: \"Data-aided CFO estimators based on the averaged cyclic autocorrelation. Wireless communication systems typically employ a repetitive preamble in each slot which is used for parameter acquisition. The repetitive preamble is useful for estimating the carrier frequency offset (CFO), usually based on the autocorrelation of the received signal. In this paper, we derive a family of novel data-aided CFO estimators. The proposed estimators are based on a new autocorrelation function which is defined using cyclostationary properties of the repetitive preamble. In contrast to previous approaches, the new estimators make use of high-order noise terms leading to an improved performance. We present a detailed analysis of the proposed estimators and provide closed-form expressions for the variance of the estimators. The new estimators are shown to outperform the existing estimators obtaining a moderate improvement at high signal to noise ratio (SNR) and a considerable improvement at low SNR, by means of a reasonable increase in computational complexity.\"",
        "1 is \"Cognitive Radio Resource Management for QoS Guarantees in Autonomous Femtocell Networks\", 2 is \"Optimally combining sampling techniques for Monte Carlo rendering\"",
        "Given above information, for an author who has written the paper with the title \"On Performance Loss of Some CoMP Techniques under Channel Power Imbalance and Limited Feedback\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002489": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Playing the Smart Grid Game: Performance Analysis of Intelligent Energy Harvesting and Traffic Flow Forecasting for Plug-In Electric Vehicles':",
        "Document: \"UBMR-CA: Utility-based multicast routing and channel assignment with varying traffic demands in multi-radio multi-channel wireless mesh networks. Over the years, a wireless mesh network (WMN) has been considered as a leading technology for providing Quality of Service (QoS) aware services to end users thanks to its features such as low deployment and maintenance costs. To obtain the maximum throughput in a WMN, an efficient routing and channel assignment is required. But there are certain constraints in the WMN such as end-to-end delay, interference, and available bandwidth which limits the QoS aware service availability to the end users. Moreover, predicting traffic demand is challenging due to the varying needs of the users which further limits the throughput. Hence, to overcome these difficulties, in this paper we propose a utility-based multicast routing and channel assignment (UBMR-CA) scheme. The two main objectives of the proposed scheme are the loop free routing and minimization of the total utility from a source to a destination at any time. The problem is formulated as a linear programming (LP) problem along with the constraints. The utility for a link is calculated using a utility weight metric (UWM) which is used for sending multicast messages from a source to destinations. Once the utility for all the links is calculated, these links are assigned to a particular channel keeping in view of various constraints such as capacity, bandwidth, and interference between different links. The performance of the proposed scheme is evaluated using extensive simulation with respect to various parameters such as aggregate throughput, fairness, delay, packet loss and collision probabilities, and total execution time. The proposed UWM is also compared with existing routing metrics in the literature. The results obtained from our simulation show that the proposed scheme outperforms existing schemes with respect to the above defined parameters.\"",
        "Document: \"Comparative Handover Performance Analysis Of Ipv6 Mobility Management Protocols. IPv6 mobility management is one of the most challenging research topics for enabling mobility service in the forthcoming mobile wireless ecosystems. The Internet Engineering Task Force has been working for developing efficient IPv6 mobility management protocols. As a result, Mobile IPv6 and its extensions such as Fast Mobile IPv6 and Hierarchical Mobile IPv6 have been developed as host-based mobility management protocols. While the host-based mobility management protocols were being enhanced, the network-based mobility management protocols such as Proxy Mobile IPv6 (PMIPv6) and Fast Proxy Mobile IPv6 (FPMIPv6) have been standardized. In this paper, we analyze and compare existing IPv6 mobility management protocols including the recently standardized PMIPv6 and FPMIPv6. We identify each IPv6 mobility management protocol's characteristics and performance indicators by examining handover operations. Then, we analyze the performance of the IPv6 mobility management protocols in terms of handover latency, handover blocking probability, and packet loss. Through the conducted numerical results, we summarize considerations for handover performance.\"",
        "Document: \"Performance analysis of distributed mapping system in ID/locator separation architectures. An ID/locator separation architecture is one of the most recognized technologies that enable the Future Internet. In ID/locator separation architecture, an ID/locator mapping system is indispensable to provide location management in mobile environments. This paper conducts a comparative study on two different ID/locator mapping approaches: centralized and distributed ID/locator mapping systems. We develop analytical models on the signaling cost incurred in location update and location query procedures of the centralized and distributed ID/locator mapping systems. Numerical results demonstrate that the distributed ID/locator mapping system with enhanced distributed hash table (DHT) has comparable signaling cost to the centralized ID/locator mapping system while providing higher scalability and robustness.\"",
        "Document: \"Performance Analysis of Route Optimization on Proxy Mobile IPv6. In the Mobile IPv6 (MIPv6), a mobile node (MN) directly manages its signaling related mobility because the MIPv6 is a host based mobility protocol. On the other hands, the Proxy Mobile IPv6 (PMIPv6) provides a network based mobility in where special network entities manage all gnaling related mobility for supporting mobility service of MN. The Route Optimization (RO) mechanism for PMIPv6 is proposed while the specification of PMIPv6 is still focusing on basic operations. The RO mechanism establishes the enhanced communication path between MNs so that it can reduce transmission delay and network burden. In this paper, we analyze the performance of RO mechanism compared with the basic PMIPv6. The presented results confirm that the RO mechanism provide the improved performance during the MN communicates with the CN, due to the established communication path.\"",
        "Document: \"Privacy-preserving data aggregation scheme against internal attackers in smart grids. With fast advancements of communication, systems and information technologies, a smart grid (SG) could bring much convenience to users because it could provide a reliable and efficient energy service. The data aggregation (DA) scheme for the SG plays an important role in evaluating information about current energy usage. To achieve the goal of preserving users\u2019 privacy, many DA schemes for the SG have been proposed in last decade. However, how to withstand attacks of internal adversaries is not considered in those schemes. To enhance preservation of privacy, Fan et al. proposed a DA scheme for the SG against internal adversaries. In Fan et al.\u2019s DA scheme, blinding factors are used in evaluating information about current energy usage and the aggregator cannot get the consumption information of any individual user. Fan et al. demonstrated that their scheme was secure against various attacks. However, we find that their scheme suffers from the key leakage problem, i.e., the adversary could extract the user\u2019s private key through the public information. To overcome such serious weakness, this paper proposes an efficient and privacy-preserving DA scheme for the SG against internal attacks. Analysis shows that the proposed DA scheme not only overcome the key leakage problem in Fan et al.\u2019s DA scheme, but also has better performance.\"",
        "1 is \"Efficient and secure authenticated key exchange using weak passwords\", 2 is \"LACAS: learning automata-based congestion avoidance scheme for healthcare wireless sensor networks\"",
        "Given above information, for an author who has written the paper with the title \"Playing the Smart Grid Game: Performance Analysis of Intelligent Energy Harvesting and Traffic Flow Forecasting for Plug-In Electric Vehicles\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002515": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Improving the Performance of Parallel Triangularization of a Sparse Matrix Using a Reconfigurable Multicomputer':",
        "Document: \"Direct coherence: bringing together performance and scalability in shared-memory multiprocessors. Traditional directory-based cache coherence protocols suffer from long-latency cache misses as a consequence of the indirection introduced by the home node, which must be accessed on every cache miss before any coherence action can be performed. In this work we present a new protocol that moves the role of storing up-to-date coherence information (and thus ensuring totally ordered accesses) from the home node to one of the sharing caches. Our protocol allows most cache misses to be directly solved from the corresponding remote caches, without requiring the intervention of the home node. In this way, cache miss latencies are reduced. Detailed simulations show that this protocol leads to improvements in total execution time of 8% on average over a highly optimized MOESI directory-based protocol.\"",
        "Document: \"Owner prediction for accelerating cache-to-cache transfer misses in a cc-NUMA architecture. Cache misses for which data must be obtained from a remote cache (cache-to-cache transfer misses) account for an important fraction of the total miss rate. Unfortunately, cc-NUMA designs put the access to the directory information into the critical path of 3-hop misses, which significantly penalizes them compared to SMP designs. This work studies the use of owner prediction as a means of providing cc-NUMA multiprocessors with a more efficient support for cache-to-cache transfer misses. Our proposal comprises an effective prediction scheme as well as a coherence protocol designed to support the use of prediction. Results indicate that owner prediction can significantly reduce the latency of cache-to-cache transfer misses, which translates into speed-ups on application performance up to 12%. In order to also accelerate most of those 3-hop misses that are either not predicted or mispredicted, the inclusion of a small and fast directory cache in every node is evaluated, leading to improvements up to 16% on the final performance.\"",
        "Document: \"Reducing 3D Fast Wavelet Transform Execution Time Using Blocking and the Streaming SIMD Extensions. The video compression algorithms based on the 3D wavelet transform obtain excellent compression rates at the expense of huge memory requirements, that drastically affects the execution time of such applications. Its objective is to allow the real-time video compression based on the 3D fast wavelet transform. We show the hardware and software interaction for this multimedia application on a general-purpose processor. First, we mitigate the memory problem by exploiting the memory hierarchy of the processor using several techniques. As for instance, we implement and evaluate the blocking technique. We present two blocking approaches in particular: cube and rectangular, both of which differ in the way the original working set is divided. We also put forward the reuse of previous computations in order to decrease the number of memory accesses and floating point operations. Afterwards, we present several optimizations that cannot be applied by the compiler due to the characteristics of the algorithm. On the one hand, the Streaming SIMD Extensions (SSE) are used for some of the dimensions of the sequence (y and time), to reduce the number of floating point instructions, exploiting Data Level Parallelism. Then, we apply loop unrolling and data prefetching to specific parts of the code. On the other hand, the algorithm is vectorized by columns, allowing the use of SIMD instructions for the y dimension. Results show speedups of 5x in the execution time over a version compiled with the maximum optimizations of the Intel C/C++ compiler, maintaining the compression ratio and the video quality (PSNR) of the original encoder based on the 3D wavelet transform. Our experiments also show that, allowing the compiler to perform some of these optimizations (i.e. automatic code vectorization), causes performance slowdown, demonstrating the effectiveness of our optimizations.\"",
        "Document: \"Modeling the impact of permanent faults in caches. The traditional performance cost benefits we have enjoyed for decades from technology scaling are challenged by several critical constraints including reliability. Increases in static and dynamic variations are leading to higher probability of parametric and wear-out failures and are elevating reliability into a prime design constraint. In particular, SRAM cells used to build caches that dominate the processor area are usually minimum sized and more prone to failure. It is therefore of paramount importance to develop effective methodologies that facilitate the exploration of reliability techniques for caches. To this end, we present an analytical model that can determine for a given cache configuration, address trace, and random probability of permanent cell failure the exact expected miss rate and its standard deviation when blocks with faulty bits are disabled. What distinguishes our model is that it is fully analytical, it avoids the use of fault maps, and yet, it is both exact and simpler than previous approaches. The analytical model is used to produce the miss-rate trends (expected miss-rate) for future technology nodes for both uncorrelated and clustered faults. Some of the key findings based on the proposed model are (i) block disabling has a negligible impact on the expected miss-rate unless probability of failure is equal or greater than 2.6e-4, (ii) the fault map methodology can accurately calculate the expected miss-rate as long as 1,000 to 10,000 fault maps are used, and (iii) the expected miss-rate for execution of parallel applications increases with the number of threads and is more pronounced for a given probability of failure as compared to sequential execution.\"",
        "Document: \"Genetic algorithm for planning cable telecommunication networks. Optical fiber networks over synchronous digital hierarchy operating on ATM mode appear as one of the best alternatives for the new high-speed telecommunications networks. The network management requires planning studies so that the designed networks allow efficient operation in real time. A quantitative model evaluating the global topological design of an optical fiber network over synchronous digital hierarchy is described. The model includes the economic aspects involved in the project, the civil works, the capacity evaluation costs and the introduction of reliability conditions. A genetic approach based on the exploitation of the Kuhn\u2013Tucker optimality conditions is proposed to solve the model.\"",
        "1 is \"Distributed Resource Management for Parallel Applications in Networks of Workstations\", 2 is \"Virtual clock: a new traffic control algorithm for packet switching networks\"",
        "Given above information, for an author who has written the paper with the title \"Improving the Performance of Parallel Triangularization of a Sparse Matrix Using a Reconfigurable Multicomputer\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002534": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'On Common-Mode Skewed-Load and Broadside Tests':",
        "Document: \"Forward-looking fault simulation for improved static compaction. Fault simulation of a test set in an order different from the order of generation (e.g., reverse- or random-order fault simulation) is used as a fast and effective method to drop unnecessary tests from a test set in order to reduce its size. We propose an improvement to this type of fault simulation process that makes it even more effective in reducing the test-set size. The proposed improvement allows us to drop tests without simulating them based on the fact that the faults they detect will be detected by tests that will be simulated later, hence the name of the improved procedure: forward-looking fault simulation. We present experimental results to demonstrate the effectiveness of the proposed improvement\"",
        "Document: \"Primary Input Vectors to Avoid in Random Test Sequences for Synchronous Sequential Circuits. Random test sequences may be used for manufacturing testing as well as for simulation-based design verification. This paper studies one of the reasons for the fact that random primary input sequences achieve very low fault coverage for synchronous sequential circuits. It is shown that a synchronous sequential circuit may have input cubes, or incompletely specified input vectors, that synchronize a subset of its state variables, i.e., it forces them to certain specified values. When an input cube c that synchronizes the subset of state variables S(c) has a small number of specified inputs, the input vectors covered by it may appear often in a random primary input sequence. As a result, the sequence will force the same values on the state variables in S(c) repeatedly. This may limit the fault coverage that the sequence can obtain. To address this issue, a procedure is described for modifying a random primary input sequence to eliminate the appearance of input vectors that synchronize subsets of state variables. It is demonstrated that this procedure has a significant effect on the fault coverage that can be achieved by random primary input sequences.\"",
        "Document: \"Static test compaction for circuits with multiple independent scan chains. This study describes a static test compaction procedure for transition faults in circuits with multiple scan chains where each scan chain can operate independently in functional or shift mode. The procedure mixes parts of different broadside and skewed-load tests, where the parts coincide with the scan chains, in order to create new tests that detect more faults. This allows the number of tests to be reduced without reducing the fault coverage. By mixing parts of tests with different types, different scan chains are assigned different modes of operation within the same test. Experimental results are presented to demonstrate that this allows the number of tests to be reduced below the number of tests in a compact test set that consists of broadside and skewed-load tests.\"",
        "Document: \"Path-Oriented Transition Fault Test Generation Considering Operating Conditions. We describe a test generation procedure for path-oriented transition faults that takes into account the fact that operating conditions may change during circuit operation. A path-oriented transition fault is detected through the longest sensitizable path that goes through the fault site. The operating conditions we consider are junction temperature and power supply voltage. Since path delays change with operating conditions, the longest path through a fault site may be different under different conditions. We show that test generation using nominal delays is not sufficient for covering the complete range of operating conditions, even if N-detection test generation is used. Therefore, operating conditions need to be addressed explicitly during test generation. However, since temperature and voltage are continuous variables and represent an infinite number of values in the range, test generation must concentrate on a small selected set of operating conditions. We discuss the selection of these conditions and demonstrate that N-detection test generation with multiple operating conditions is effective in covering the range of operation conditions almost completely.\"",
        "Document: \"Generation of Multi-Cycle Broadside Tests. The use of multi-cycle (or multi-pattern) tests for delay faults can reduce the number of clock cycles required for test application, and enhance the ability of a test set to detect delay defects. This is achieved by exercising the circuit in functional mode for several clock cycles as part of each test. This advantage is especially important for multi-pattern functional broadside tests, which guarantee normal functional operation conditions during the functional clock cycles of the test. This paper describes a procedure for generating multi-pattern broadside tests. The procedure extends a two-pattern test set gradually to increase the number of patterns included in each test while reducing the number of tests. Experimental results demonstrate that significant reductions in the numbers of clock cycles are possible with the proposed procedure for both functional and arbitrary broadside test sets.\"",
        "1 is \"Simultaneous Power and Thermal Integrity Driven Via Stapling in 3D ICs\", 2 is \"On identifying undetectable and redundant faults in synchronous sequential circuits\"",
        "Given above information, for an author who has written the paper with the title \"On Common-Mode Skewed-Load and Broadside Tests\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002538": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Nested codes for secure transmission':",
        "Document: \"On The Achievable Secrecy Throughput Of Block Fading Channels With No Channel State Information At Transmitter. We study the block fading wire-tap channel, where a transmitter sends confidential messages to a legitimate receiver over a block fading channel in the presence of an eavesdropper, which listens to the transmission through another independent block fading channel. We assume that the transmitter has no channel state information (CSI) available from either the main channel or the eavesdropper channel. The transmitter uses an in advance given Wyner secrecy code (instead of adapting the code based on CSI). In this case, both reliability and perfect secrecy can be achieved only for a subset of channel states. We identify this channel state set and provide an achievable average secrecy throughput of the block fading wire-tap channel for given reliability and secrecy outage probabilities.\"",
        "Document: \"Spectral and Spatial Properties of Antennas for Transmitting and Receiving UWB Signals. The spectral and spatial properties of antennas for transmitting and receiving ultrawideband (UWB) pulses and the corresponding filter response designs for receiving such pulses over severe multipath are studied in this paper. Based on the transmit and receive transfer functions of a UWB antenna, we analyze the UWB link behavior. A metric that is based on the receiver output signal-to-noise ratio ...\"",
        "Document: \"Radio scene analysis using trilinear decomposition. We consider a scenario with multiple radio sources performing packet based transmissions. The sources belong to heterogeneous networks and their signals may overlap in time and frequency. Each source is characterized by its power spectral density and an on/off activity sequence. A network of sensors performs measurements, where each sensor computes spectrogram of the received signal with certain time and frequency resolution. Spectrograms from different sensors are collected and arraigned in a three-way array, whose three dimensions correspond to space, time, and frequency indices. We show that, under certain rank conditions of the three-way array, it is possible to recover sources to sensors channel gain coefficients, power spectral densities and on/off activity sequences of the sources by decomposing the three-way array into rank-one components. The recovery process is illustrated with simulation examples involving 802.11b/g and Bluetooth sources whose signals overlap in time and frequency.\"",
        "Document: \"Characterization Of The Orbit Indoor Testbed Radio Environment. We perform a set of measurements of channel frequency responses at different points in the room that ac commodates the ORBIT indoor testbed using a vector network analyzer (VNA). To validate the data collected with VNA we use spectrum analyzer (SA) measurements, as well. Four measurements are performed over non overlapping 100 MHz bands in the industrial, scientific, and medical (ISM) band, and in the unlicensed national information infrastructure (UNII) band. The fifth measurement spans the coarsely sampled .4 to 6 GHz band. From the measured frequency responses we calculate path loss model parameters. The path loss exponent is between 1.1 and 2 and the dynamic range of the signal is around 25 dB across different bands. Based on the frequency responses measured over the .4 to 6 GHz band we determine multipath intensity profiles; (MIP) with fine time granularity. The comparison of MIPs to the ray tracing simulations generated with WiSE software indicates that the surfaces perpendicular to the plane defined by the transmit and receive antennas represent a significant source of reflections. However, the reflections from the floor, dropped coiling, and roof are suppressed by the antenna elevation patterns.\"",
        "Document: \"Punctured turbo code ensembles. We analyze the asymptotic performance of punctured turbo codes. The analysis is based on the union bound on the word error probability of maximum likelihood decoding for punctured turbo code ensembles averaged over all possible puncturing patterns and interleavers. By using special probabilistic puncturing, we prove that, for a given mother turbo code ensemble, [C], with a finite noise threshold, c0[C], if the asymptotic puncturing turing rate, \u03bb, satisfies log \u03bb < -c0[C], there exists a finite noise threshold, c0[Cp], for the punctured turbo code ensemble which is bounded by a function of c0[C] and \u03bb. Based on this result, we prove that, on any binary-input memoryless channel whose Bhattacharyya noise distance is greater than c0[Cp], the average ML decoding word error probability of the punctured turbo code ensemble approaches zero at least as fast as n-\u03b2, where \u03b2 is the well known \"interleaver gain\" exponent. This enables us to answer an important question in the practice of HARQ (hybrid ARQ) schemes, namely, up to which puncturing rate \"good\" turbo codes give rise to \"good\" punctured codes.\"",
        "1 is \"No bad local minima: Data independent training error guarantees for multilayer neural networks.\", 2 is \"User cooperation diversity-part I: system description\"",
        "Given above information, for an author who has written the paper with the title \"Nested codes for secure transmission\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002566": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Fuzzy multi-period portfolio selection with different investment horizons.':",
        "Document: \"Original Article: A three phase supplier selection method based on fuzzy preference degree. As competition is growing high on this globalized world, the companies are imposing more and more importance on the process of supplier selection. After the foundation of fuzzy logic, the problem of supplier selection has been treated from the viewpoint of uncertainty. The present work reviews and classifies different approaches towards this problem. A new fuzzy preference degree between two triangular fuzzy numbers is introduced and a new approach is prescribed to solve the problem using this preference degree. The weights of the Decision Makers are considered and a methodology is proposed to determine the weights. Moreover, a unique process of classifying the suppliers in different groups is proposed. The methodologies are exemplified by a suitable case study.\"",
        "Document: \"Supplier Selection Using Ranking Interval Type-2 Fuzzy Sets. In face of global competition, supplier management is emerging as crucial issue to any companies striving for business success. This paper develops a framework for selecting suitable outsourced suppliers of upstream supply chain in uncertain environment. Emerging supply risk arising from outsourcing are analyzed to reduce cost and increase the sustainability of supply chain network. The study applies ranking based interval type-2 fuzzy set exploring the risk factors and ranking supplier companies. The performance rating weights of risk criteria in supply chain are evaluated based on decision makers. Finally, an empirical study is conducted for Indian Oil Corporation Limited (IOCL) to demonstrate the applicability of the proposed algorithm to select the suitable crude oil supplier(s).\"",
        "Document: \"Group decision making in medical system: An intuitionistic fuzzy soft set approach. \u2022Group decision making for medical diagnosis.\u2022Intuitionistic fuzzy soft set and fuzzy soft matrix.\u2022Hamming distance and Euclidean approach.\u2022Cardinal of intuitionistic fuzzy soft set to compute the weight.\u2022Viral fever related diagnosis.\"",
        "Document: \"Fixed charge transportation problem with type-2 fuzzy variables. This paper considers two fixed charge transportation problems with type-2 fuzzy parameters. Unit transportation costs, fixed costs in the first problem and unit transportation costs, fixed costs, supplies and demands in the second problem are type-2 fuzzy variables. For the first problem, to get corresponding defuzzified values of the type-2 fuzzy cost parameters, first critical value (CV)-based reduction methods are applied to reduce type-2 fuzzy variables into type-1 fuzzy variables and then centroid method is used for complete defuzzification. Besides this, we also apply geometric defuzzification method to the type-2 fuzzy cost parameters in the first problem to provide a comparison of the results. Coming to the second problem, a chance-constrained programming model is formulated using generalized credibility measure for the objective function as well as the constraints with the CV-based reductions of corresponding type-2 fuzzy parameters. Next, the reduced model is turned into equivalent parametric programming problem. The deterministic problems so obtained are then solved by using the standard optimization solver - LINGO. We have provided numerical examples illustrating the proposed models and techniques. Some sensitivity analyzes for the second model are also presented.\"",
        "Document: \"Group decision making using neutrosophic soft matrix: An algorithmic approach. This article proposes an algorithmic approach for group decision making (GDM) problems using neutrosophic soft matrix (NSM) and relative weights of experts. NSM is the matrix representation of neutrosophic soft sets (NSSs), where NSS is the combination of neutrosophic set and soft set. We propose a new idea for assigning relative weights to the experts based on cardinalities of NSSs. The relative weight is assigned to each of the experts based on their preferred attributes and opinions, which reduces the chance of unfairness in the decision making process. Firstly we introduce choice matrix and combined choice matrix using neutrosophic sets. Multiplying combined choice matrices with the individual NSMs, this study develops product NSMs, which are aggregated to find out the collective NSM. Then neutrosophic cross-entropy measure is used to rank the alternatives and for selecting the most desirable one(s). This study also provides a comparative analysis of the proposed weight based approach with the normal procedure, where weight is not considered. Finally, a case study illustrates the applicability of the proposed approach.\"",
        "1 is \"Recent advances on multicue object tracking: a survey\", 2 is \"Stochastic uncapacitated hub location\"",
        "Given above information, for an author who has written the paper with the title \"Fuzzy multi-period portfolio selection with different investment horizons.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002655": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Large Margin Nearest Neighbor Embedding for Knowledge Representation.':",
        "Document: \"An open/free database and Benchmark for Uyghur speaker recognition. Few research has been conducted on Uyghur speaker recognition. Among the limited works, researchers usually collect small speech databases and publish results based on their own private data. This `close-door evaluation' makes most of the publications doubtable. This paper publishes an open and free speech database THUYG-20 SRE and a benchmark for Uyghur speaker recognition. The database is based on the THUYG-20 speech corpus we recently released, and the benchmark involves recognition tasks with various training/enrollment/test conditions. We provide a complete description for the database as well as the benchmark, and present an i-vector baseline system constructed using the Kaldi toolkit.\"",
        "Document: \"Deep Speaker Verification: Do We Need End to End?. End-to-end learning treats the entire system as a whole adaptable black box, which, if sufficient data are available, may learn a system that works very well for the target task. This principle has recently been applied to several prototype research on speaker verification (SV), where the feature learning and classifier are learned together with an objective function that is consistent with the evaluation metric. An opposite approach to end-to-end is feature learning, which firstly trains a feature learning model, and then constructs a back-end classifier separately to perform SV. Recently, both approaches achieved significant performance gains on SV, mainly attributed to the smart utilization of deep neural networks. However, the two approaches have not been carefully compared, and their respective advantages have not been well discussed. In this paper, we compare the end-to-end and feature learning approaches on a text-independent SV task. Our experiments on a dataset sampled from the Fisher database and involving 5,000 speakers demonstrated that the feature learning approach outperformed the end-to-end approach. This is a strong support for the feature learning approach, at least with data and computation resources similar to ours.\"",
        "Document: \"Effectiveness of n-gram fast match for query-by-humming systems. To achieve a good balance between matching accuracy and computation efficiency is a key challenge for query-by-humming (QBH) system. In this paper, we propose an approach of n-gram based fast match. Our n-gram method uses a robust statistical note transcription as well as error compensation method based on the analysis of frequent transcription errors. The effectiveness of our approach has been evaluated on a relatively large melody database with 5223 melodies. The experimental results show that when the searching space was reduced to only 10% of the whole size, 90% of the target melodies were preserved in the candidates, and 88% of the match accuracy of system was kept. Meanwhile, no obvious additional computation was applied.\"",
        "Document: \"A Dialectal Chinese Speech Recognition Framework. A framework for dialectal Chinese speech recognition is proposed and studied, in which a rela- tively small dialectal Chinese (or in other words Chinese influenced by the native dialect) speech corpus and dialect-related knowledge are adopted to transform a standard Chinese (or Putonghua, abbreviated as PTH) speech recognizer into a dialectal Chinese speech recognizer. Two kinds of knowledge sources are explored: one is experts knowledge and the other is a small dialectal Chinese corpus. These knowledge sources provide information on four levels: phonetics level, lexicon level, language level, and the acoustic decoder level. This paper takes Wu dialectal Chinese (WDC) as an example target language. Our goal was to establish a WDC speech recognizer from an existing PTH speech recognizer based on the Initial-Final structure of the Chinese language and a study of how dialectal Chinese speakers speak Putonghua, we propose to use context-independent PTH-IF mappings (where IF means either a Chinese Initial or a Chi- nese Final), context-independent WDC-IF mappings, and syllable-dependent WDC-IF mappings (ob- tained from either experts or data), and combine them with the supervised maximum likelihood linear re- gression (MLLR) acoustic model adaptation method. To reduce the size of the multi-pronunciation lexi- con introduced by the IF mappings, which might also enlarge the lexicon confusion and hence lead to the performance degradation, a Multi-Pronunciation Expansion (MPE) method based on the accumulated uni-gram probability (AUP) is proposed. In addition, some commonly used WDC words are selected and added to the lexicon. Compared with the original PTH speech recognizer, the resulting WDC speech rec- ognizer achieves 10-18% absolute Character Error Rate (CER) reduction when recognizing WDC, with only a 0.62% CER increase when recognizing PTH. The proposed framework and methods are expected to work not only for Wu dialectal Chinese but also for other dialectal Chinese languages and even other languages.\"",
        "Document: \"System Combination for Short Utterance Speaker Recognition. For text-independent short-utterance speaker recognition (SUSR), the performance often degrades dramatically. This paper presents a combination approach to the SUSR tasks with two phonetic-aware systems: one is the DNN-based i-vector system and the other is our recently proposed subregion-based GMM-UBM system. The former employs phone posteriors to construct an i-vector model in which the shared statistics offers stronger robustness against limited test data, while the latter establishes a phone-dependent GMM-UBM system which represents speaker characteristics with more details. A score-level fusion is implemented to integrate the respective advantages from the two systems. Experimental results show that for the text-independent SUSR task, both the DNN-based i-vector system and the subregion-based GMM-UBM system outperform their respective baselines, and the score-level system combination delivers performance improvement.\"",
        "1 is \"In search of better pronunciation models for speech recognition\", 2 is \"An improved extraction pattern representation model for automatic IE pattern acquisition\"",
        "Given above information, for an author who has written the paper with the title \"Large Margin Nearest Neighbor Embedding for Knowledge Representation.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002685": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'The Visual Object Tracking VOT2016 Challenge Results':",
        "Document: \"Probabilistic fusion-based parameter estimation for visual tracking. In object tracking, visual features may not be discriminative enough to estimate high dimensional motion parameters accurately, and complex motion estimation is computationally expensive due to a large search space. To tackle these problems, a reasonable strategy is to track small components within the target independently in lower dimensional motion parameter spaces (e.g., translation only) and then estimate the overall high dimensional motion (e.g., translation, scale and rotation) by statistically integrating the individual tracking results. Although tracking each component in a lower dimensional space is more reliable and faster, it is not trivial to combine the local motion information and estimate global parameters in a robust way because the individual component motions are frequently inconsistent. We propose a robust fusion algorithm to estimate the complex motion parameters using variable-bandwidth mean-shift. By employing correlation-based uncertainty modeling and fusion of individual components, the motion parameter that is robust to outliers can be detected with variable-bandwidth density-based fusion (VBDF) algorithm. In addition, we describe a method to update target appearance model for each component adaptively based on the component motion consistency. We present various tracking results and compare the performance of our algorithm with others using real video sequences.\"",
        "Document: \"Occlusion detection using horizontally segmented windows for vehicle tracking. This paper proposes an efficient algorithm for detecting occlusions in a video sequences of ground vehicles using color information. The proposed method uses a rectangular window to track a target vehicle, and the window is horizontally divided into several sub-regions of equal width. Each region is determined to be occluded or not based on the color histogram similarity to the corresponding region of the target. The occlusion detection results are used in likelihood computation of the conventional tracking algorithm based on particle filtering. Experimental results in real scenes show that the proposed method finds the occluded region successfully and improves the performance of the conventional trackers.\"",
        "Document: \"Density-based multifeature background subtraction with support vector machine. Background modeling and subtraction is a natural technique for object detection in videos captured by a static camera, and also a critical preprocessing step in various high-level computer vision applications. However, there have not been many studies concerning useful features and binary segmentation algorithms for this problem. We propose a pixelwise background modeling and subtraction technique using multiple features, where generative and discriminative techniques are combined for classification. In our algorithm, color, gradient, and Haar-like features are integrated to handle spatio-temporal variations for each pixel. A pixelwise generative background model is obtained for each feature efficiently and effectively by Kernel Density Approximation (KDA). Background subtraction is performed in a discriminative manner using a Support Vector Machine (SVM) over background likelihood vectors for a set of features. The proposed algorithm is robust to shadow, illumination changes, spatial variations of background. We compare the performance of the algorithm with other density-based methods using several different feature combinations and modeling techniques, both quantitatively and qualitatively.\"",
        "Document: \"Robust Observations For Object Tracking. It is a difficult task to find an observation model that will perform well for long-term visual tracking. In this paper, we propose an adaptive observation enhancement technique based on likelihood images which are derived from multiple visual features. The most discriminative likelihood image is extracted by Principal Component Analysis (PCA) and incrementally updated frame by frame to reduce temporal tracking error. In the particle filter framework, the feasibility of each sample is computed using this most discriminative likelihood image before the observation process. Integral image is employed for efficient computation of the feasibility of each sample. We illustrate how our enhancement technique contributes to more robust observations through demonstrations.\"",
        "Document: \"BranchOut: Regularization for Online Ensemble Tracking with Convolutional Neural Networks. We propose an extremely simple but effective regularization technique of convolutional neural networks (CNNs), referred to as BranchOut, for online ensemble tracking. Our algorithm employs a CNN for target representation, which has a common convolutional layers but has multiple branches of fully connected layers. For better regularization, a subset of branches in the CNN are selected randomly for online learning whenever target appearance models need to be updated. Each branch may have a different number of layers to maintain variable abstraction levels of target appearances. BranchOut with multi-level target representation allows us to learn robust target appearance models with diversity and handle various challenges in visual tracking problem effectively. The proposed algorithm is evaluated in standard tracking benchmarks and shows the state-of-the-art performance even without additional pretraining on external tracking sequences.\"",
        "1 is \"UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild\", 2 is \"Image sequence denoising via sparse and redundant representations.\"",
        "Given above information, for an author who has written the paper with the title \"The Visual Object Tracking VOT2016 Challenge Results\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002724": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Efficient outsourcing of secure k-nearest neighbour query over encrypted database.':",
        "Document: \"A New Architecture for User Authentication and Key Exchange Using Password for Federated Enterprises. The rapid rise of federated enterprises entails a new way of trust management by the fact that an enterprise can account for partial trust of its affiliating organizations. On the other hand, password has historically been used as a main means for user authentication because of operational simplicity. We are thus motivated to explore the use of short password for user authentication and key exchange in the context of federated enterprises. Exploiting the special structure of a federated enterprise, our proposed new architecture comprises an external server managed by each affiliating organization and a central server managed by the enterprise headquarter. We are concerned with the development of an efficient authentication and key exchange protocol using password, built over the new architecture. The architecture together with the protocol well addresses off-line dictionary attacks initiated at the server side, a problem rarely considered in prior effort.\"",
        "Document: \"Privacy-Preserving k-Nearest Neighbour Query on Outsourced Database. Cloud computing brought a shift from the traditional client-server model to DataBase as a Service DBaaS, where the data owner outsources her database as well as the data management function to the cloud service provider. Although cloud services relieve the clients from the data management burdens, a significant concern about the data privacy remains. In this work, we focus on privacy-preserving k-nearest neighbour k-NN query, and provide the first sublinear solution with preprocessing with computational complexity $$\\\\tilde{O}k\\\\text {log}^4n$$ in the honest-but-curious adversarial setting. Our constructions use the data structure called kd-tree to achieve sublinear query complexity. In order to protect data access patterns, garbled circuits are used to simulate Oblivious RAM ORAM for accessing data in the kd-tree. Compared to the existing solutions, our scheme imposes little overhead on both the data owner and the querying client.\"",
        "Document: \"Privacy and ownership preserving of outsourced medical data. The demand for the secondary use of medical data is increasing steadily to allow for the provision of better quality health care. Two important issues pertaining to this sharing of data have to be addressed: one is the privacy protection for individuals referred to in the data; the other is copyright protection over the data. In this paper, we present a unified framework that seamlessly combines techniques of binning and digital watermarking to attain the dual goals of privacy and copyright protection. Our binning method is built upon an earlier approach of generalization and suppression by allowing a broader concept of generalization. To ensure data usefulness, we propose constraining binning by usage metrics that define maximal allowable information loss, and the metrics can be enforced off-line. Our watermarking algorithm watermarks the binned data in a hierarchical manner by leveraging on the very nature of the data. The method is resilient to the generalization attack that is specific to the binned data, as well as other attacks intended to destroy the inserted mark. We prove that watermarking could not adversely interfere with binning, and implemented the framework. Experiments were conducted, and the results show the robustness of the proposed framework.\"",
        "Document: \"A Generic Scheme for Secure Data Sharing in Cloud. Working in various service models ranging from SaaS, PaaS, to IaaS, cloud computing is a new revolution in IT, and could reshape the business model of how the IT industry works today. Storage services are a fundamental component of the cloud computing paradigm. By exploiting the storage services, users outsource their data to the cloud so as to enjoy the reduced upfront maintenance and capital costs. However, a security challenge associated with data outsourcing is how to prevent data abuses by the cloud. It has been commonly accepted that data encryption offers a good solution to this problem. With data encryption, an issue arises when the data owner who outsourced the data wants to revoke some data consumers' access privileges, which normally involves key re-distribution and data re-encryption. In this work, we propose a generic scheme to enable fine-grained data sharing over the cloud, which does not require key-redistribution and data re-encryption whatsoever. The main primitives we make use of are attribute-based/predicate encryption and proxy re-encryption, but our construction is not restricted to any specific scheme of its kind. Our scheme has a number of advantages over other similar proposals in the literature.\"",
        "Document: \"AutoPrivacy: Automatic privacy protection and tagging suggestion for mobile social photo. \u2022AutoPrivacy distinguishes intended and unintended human faces with their temporal and spatial characteristics.\u2022We propose a novel encryption scheme to conceal unintended human faces such that authorized users can access blur regions.\u2022AutoPrivacy utilizes recognition model to classify intended human faces based on existing tagged faces database.\u2022Implement AutoPrivacy at Android platform and verify its performance.\"",
        "1 is \"Efficient identification and signatures for smart cards\", 2 is \"Threshold and Identity-based Key Management and Authentication for Wireless Ad Hoc Networks\"",
        "Given above information, for an author who has written the paper with the title \"Efficient outsourcing of secure k-nearest neighbour query over encrypted database.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002744": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Real-time EEG-based emotion monitoring using stable features.':",
        "Document: \"Visual Clustering and Boundary Detection of Time-Dependent Datasets. Visual clustering should be one of the basic tools for time-dependent data analysis in cyberworlds. This paper describes a novel approach to spatial clustering and boundary detection based on geometric modeling and visualization. Datasets and boundaries of clusters are visualized as 3D points and surfaces of reconstructed solids changing over time. Our approach applies the concepts of geometric solid modeling and uses density as clustering criteria that comes from traditional density-based clustering techniques. Visual clustering allows the user to analyze results of clustering the data changing over time and to interactively choose appropriate parameters.\"",
        "Document: \"Real-time EEG-based emotion monitoring using stable features. In human\u2013computer interaction (HCI), electroencephalogram (EEG) signals can be added as an additional input to computer. An integration of real-time EEG-based human emotion recognition algorithms in human\u2013computer interfaces can make the users experience more complete, more engaging, less emotionally stressful or more stressful depending on the target of the applications. Currently, the most accurate EEG-based emotion recognition algorithms are subject-dependent, and a training session is needed for the user each time right before running the application. In this paper, we propose a novel real-time subject-dependent algorithm with the most stable features that gives a better accuracy than other available algorithms when it is crucial to have only one training session for the user and no re-training is allowed subsequently. The proposed algorithm is tested on an affective EEG database that contains five subjects. For each subject, four emotions (pleasant, happy, frightened and angry) are induced, and the affective EEG is recorded for two sessions per day in eight consecutive days. Testing results show that the novel algorithm can be used in real-time emotion recognition applications without re-training with the adequate accuracy. The proposed algorithm is integrated with real-time applications \u201cEmotional Avatar\u201d and \u201cTwin Girls\u201d to monitor the users emotions in real time.\"",
        "Document: \"Towards a Definition of Virtual Objects Using Partial Differential Equations. We propose an efficient alternative to commonly used parametric surfaces such as NURBS surfaces for definition of complex geometry in shared virtual spaces. Our mathematical model allows to define objects by only providing coordinates of the section curves in 3-space. The resulting parametric functions allow fast calculation of the coordinates of the points on the surface of the objects. We devise an algorithm which evaluates the coefficients of these functions in real time. Given the small size of the resulting formulas and interactive rates for their calculation, we are able to efficiently use such PDE-based models for making virtual objects in shared virtual spaces. We describe the modeling framework and illustrate the proposed theoretical concepts with our function-based extension of VRML and X3D.\"",
        "Document: \"Problems of Human-Computer Interaction in Cyberworlds. Created intentionally or spontaneously, cyberworlds are information spaces and communities that immensely augment the way we interact, participate in business and receive information throughout the world. This paper reports position statements presented at the plenary panel of the 2015th International Conference on Cyberworlds. First, the problems of enhancing creativity in cyberworlds using new interfaces are considered. It follows by the discussions on using biometric interfaces in on-line services. Finally, the challenges of using brain-computer interfaces and emotion recognition using electroencephalograms are considered.\"",
        "Document: \"Real-time Adaptive Prediction Method for Smooth Haptic Rendering. In this paper, we propose a real-time adaptive prediction method to calculate smooth and accurate haptic feedback in complex scenarios. Smooth haptic feedback is an important task for haptic rendering with complex virtual objects. However, commonly the update rate of the haptic rendering may drop down during multi-point contact in complex scenarios because high computational cost is required for collision detection and physically-based dynamic simulation. If the haptic rendering is done at a lower update rate, it may cause discontinuous or instable force/torque feedback. Therefore, to implement smooth and accurate haptic rendering, the update rate of force/torque calculation should be kept in a high and constant frequency. In the proposed method, the auto-regressive model with real-time coefficients update is proposed to predict interactive forces/torques during the physical simulation. In addition, we introduce a spline function to dynamically interpolate smooth forces/torques in haptic display according to the update rate of physical simulation. In the experiments, we show the feasibility of the proposed method and compare its performance with other methods and algorithms. The result shows that the proposed method can provide smooth and accurate haptic force feedback at a high update rate for complex scenarios.\"",
        "1 is \"Interactive Function-Based Shape Modeling for Cyberworlds\", 2 is \"Variational Bayes for continuous hidden Markov models and its application to active learning.\"",
        "Given above information, for an author who has written the paper with the title \"Real-time EEG-based emotion monitoring using stable features.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002801": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Model checking service compositions under resource constraints':",
        "Document: \"Visual methods for web application design. The paper outlines a tool -supported approach to the design of Web applications. Behavioural models are augmented with web-based simulations of user interfaces to permit validation and usability assessment of systems by end users in advance of implementation. The goal is to correct architectural design decisions that adversely impact usability early in the design cycle when correction is relatively inexpensive. The behavioural model of a system captures the interactions between the different users roles and the set of components that constitute the application. A visual scenario-based language is used to specify interactions and the tool LTSA-MSC is used to synthesise the required behavioural model. The tool supports a visual representation of this model that is animated in response to user -interaction with the simulated Web interface. The combination of these facilities permits agile incremental elaboration of a system design.\"",
        "Document: \"A foundation for behavioural conformance in software product line architectures. Software product lines or families represent an emerging paradigm that is enabling companies to engineer applications with similar functionality and user requirements more effectively. Behaviour modelling at the architecture level has the potential for supporting behaviour analysis of entire product lines, as well as defining optional and variable behaviour for different products of a family. However, to do so rigorously, a well defined notion of behavioural conformance of a product to its product line must exist. In this paper we provide a discussion on the shortcomings of traditional behaviour modelling formalisms such as Labelled Transition Systems for characterising conformance and propose Modal Transition Systems as an alternative. We discuss existing semantics for such models, exposing their limitations and finally propose a novel semantics for Modal Transition Systems, branching semantics, that can provide the formal underpinning for a notion of behaviour conformance for software product line architectures.\"",
        "Document: \"Using a Rigorous Approach for Engineering Web Service Compositions: A Case Study. In this paper we discuss a case study for the UK Police IT Organisation (PITO) on using a model-based approach to verifying web service composition interactions for a coordinated service-oriented architecture. The move towards implementing web service compositions by multiple interested parties as a form of distributed system architecture promotes the ability to support 1) early verification of service implementations against design specifications and 2) that compositions are built with compatible interfaces for differing scenarios in such a collaborative environment. The approach uses finite state machine representations of web service orchestrations and distributed process interactions. The described approach is supported by an integrated tool environment for for providing verification and validation results from checking designated properties of service models.\"",
        "Document: \"Towards accurate probabilistic models using state refinement. Probabilistic models are useful in the analysis of system behaviour and non-functional properties. Reliable estimates and measurements of probabilities are needed to annotate behaviour models in order to generate accurate predictions. However, this may not be sufficient, and may still lead to inaccurate results when the system model does not properly reflect the probabilistic choices made by the environment. Thus, not only should the probabilities be accurate in properly reflecting reality, but also the model that is being used. In this paper we identify and illustrate this problem showing that it can lead to inaccuracies and both false positive and false negative property checks. We propose state refinement as a technique to mitigate this problem, and present a framework for iteratively improving the accuracy of a probabilistically annotated behaviour model.\"",
        "Document: \"Engage: Engineering Service Modes with WS-Engineer and Dino. In this demonstration we present an approach to engineering service brokering requirements and capabilities using the concepts of Service Modes. The demonstration illustrates building service modes in UML2 with Rational Software Modeller, transforming modes in WS-Engineer and generating artefacts for runtime service brokering.\"",
        "1 is \"Model Checking of Message Sequence Charts\", 2 is \"The IceCube approach to the reconciliation of divergent replicas\"",
        "Given above information, for an author who has written the paper with the title \"Model checking service compositions under resource constraints\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002819": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Using public displays to stimulate passive engagement, active engagement, and discovery in public spaces':",
        "Document: \"WMSC '16: second workshop on mobile and situated crowdsourcing. The proposed workshop seeks to build upon the success of previous workshops at UbiComp 2010 and 2011 on Ubiquitous Crowdsourcing, and UbiComp 2015 on Mobile and Situated Crowdsourcing. Increasingly, researchers and practitioners alike are turning towards crowdsourcing with ubiquitous technologies due to their affordances and potential to circumvent limitations with online crowdsourcing platforms. Hence, this workshop's main objectives are to investigate the current state of the art of mobile and situated crowdsourcing, and foster collaborations by bringing together researchers of this thriving research agenda.\"",
        "Document: \"Donating Context Data to Science: The Effects of Social Signals and Perceptions on Action-Taking. Abstract It is becoming increasingly easy for researchers to develop context-aware applications for smartphones. A perennial challenge, however, is to convince a large number of people to install them and donate contextual data for scientific purposes. Our empirical study seeks to address this challenge by investigating how people&#39;s perception and attitude affect their willingness to donate contex...\"",
        "Document: \"Kinship verification from facial images and videos: human versus machine. Automatic kinship verification from facial images is a relatively new and challenging research problem in computer vision. It consists in automatically determining whether two persons have a biological kin relation by examining their facial attributes. In this work, we compare the performance of humans and machines in kinship verification tasks. We investigate the state-of-the-art methods in automatic kinship verification from facial images, comparing their performance with the one obtained by asking humans to complete an equivalent task using a crowdsourcing system. Our results show that machines can consistently beat humans in kinship classification tasks in both images and videos. In addition, we study the limitations of currently available kinship databases and analyzing their possible impact in kinship verification experiment and this type of comparison.\"",
        "Document: \"This is not classified: everyday information seeking and encountering in smart urban spaces. We present a multipronged comparative study of citizens' self-proclaimed information needs and actual information seeking behavior in smart urban spaces. We first conducted several user studies to identify the types of information services that citizens believed to be useful in urban setting utilizing methods ranging from contextual inquiry with lo-fi prototypes to \"card sorting\" exercise with a separate set of participants, and finally to implementing selected services. We then made a sizeable constructive intervention into the urban space by deploying in a city center 12 large, interactive public displays called \"hotspots\" to offer a wide range of previously identified information services. We collected comprehensive qualitative and quantitative data on the usage of the hotspots and their services by the general public during 13\u807dmonths. Our study reveals discrepancies between a priori and a posteriori information seeking strategies extracted from the self-proclaimed information needs and the actual usage of the hotspots.\"",
        "Document: \"Application discoverability on multipurpose public displays: popularity comes at a price. An important step in developing multipurpose public displays is understanding application discoverability: the effort required to locate or \"discover\" an application amongst others. Discoverability can affect the adoption and potential success applications. Here we investigate the effects of application discoverability on two aspects of application use: relative utility and conversion rate. We do so by testing three conditions that provide incremental discoverability to an application. Our results indicate that increased discoverability leads to higher relative utility but lower conversion rates. We discuss the implications our findings have on evaluating applications on multipurpose displays, and finally we show how our results contribute to understanding the economics of discoverability mechanisms.\"",
        "1 is \"Second international workshop on ubiquitous crowdsourcing: towards a platform for crowd computing\", 2 is \"Lockr: social access control for web 2.0\"",
        "Given above information, for an author who has written the paper with the title \"Using public displays to stimulate passive engagement, active engagement, and discovery in public spaces\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002829": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Mutual-Information Optimized Quantization for LDPC Decoding of Accurately Modeled Flash Data':",
        "Document: \"Feedback Communication Systems with Limitations on Incremental Redundancy.   This paper explores feedback systems using incremental redundancy (IR) with noiseless transmitter confirmation (NTC). For IR-NTC systems based on {\\em finite-length} codes (with blocklength $N$) and decoding attempts only at {\\em certain specified decoding times}, this paper presents the asymptotic expansion achieved by random coding, provides rate-compatible sphere-packing (RCSP) performance approximations, and presents simulation results of tail-biting convolutional codes.   The information-theoretic analysis shows that values of $N$ relatively close to the expected latency yield the same random-coding achievability expansion as with $N = \\infty$. However, the penalty introduced in the expansion by limiting decoding times is linear in the interval between decoding times. For binary symmetric channels, the RCSP approximation provides an efficiently-computed approximation of performance that shows excellent agreement with a family of rate-compatible, tail-biting convolutional codes in the short-latency regime. For the additive white Gaussian noise channel, bounded-distance decoding simplifies the computation of the marginal RCSP approximation and produces similar results as analysis based on maximum-likelihood decoding for latencies greater than 200. The efficiency of the marginal RCSP approximation facilitates optimization of the lengths of incremental transmissions when the number of incremental transmissions is constrained to be small or the length of the incremental transmissions is constrained to be uniform after the first transmission. Finally, an RCSP-based decoding error trajectory is introduced that provides target error rates for the design of rate-compatible code families for use in feedback communication systems. \"",
        "Document: \"Adaptive bit-interleaved coded modulation. Adaptive coded modulation is a powerful method for achieving a high spectral efficiency over fading channels. Previously proposed adaptive schemes have employed set-partitioned trellis-coded modulation (TCM) and have adapted the number of uncoded bits on a given symbol based on the corresponding channel estimate. However, these adaptive TCM schemes do not perform well in systems where channel estimates are unreliable, since uncoded bits are not protected from unexpected finding. In this paper, adaptive bit-interleaved coded modulation (BICM) is introduced. Adaptive BICM schemes remove the need for parallel branches in the trellis-even when adapting the constellation size, thus making these schemes robust to errors made in the estimation of the current channel fading value. This motivates the design of adaptive BICM schemes, which will lead to adaptive systems that can support users with higher mobility than those considered in previous work. In such systems, numerical results demonstrate that the proposed schemes achieve a moderate bandwidth efficiency gain over previously proposed adaptive schemes and conventional (nonadaptive) schemes of similar complexity\"",
        "Document: \"Convolutional-Code-Specific CRC Code Design. Cyclic redundancy check (CRC) codes check if a codeword is correctly received. This paper presents an algorithm to design CRC codes that are optimized for the code-specific error behavior of a specified feedforward convolutional code. The algorithm utilizes two distinct approaches to computing undetected error probability of a CRC code used with a specific convolutional code. The first approach enumerates the error patterns of the convolutional code and tests if each of them is detectable. The second approach reduces complexity significantly by exploiting the equivalence of the undetected error probability to the frame error rate of an equivalent catastrophic convolutional code. The error events of the equivalent convolutional code are exactly the undetectable errors for the original concatenation of CRC and convolutional codes. This simplifies the computation because error patterns do not need to be individually checked for detectability. As an example, we optimize CRC codes for a commonly used 64-state convolutional code for information length k=1024, demonstrating significant reduction in undetected error probability compared to the existing CRC codes with the same degrees. For a fixed target undetected error probability, the optimized CRC codes typically require 2 fewer bits.\"",
        "Document: \"Multi-input multi-output fading channel tracking and equalizationusing Kalman estimation. This paper addresses the problem of channel tracking and equalization for multi-input multi-output (MIMO) time-varying frequency-selective channels. These channels model the effects of inter-symbol interference (ISI), co-channel in- terference (CCI), and noise. A low-order autoregressive model approximates the MIMO channel variation and facilitates tracking via a Kalman filter. Hard decisions to aid Kalman tracking come from a MIMO finite-length minimum-mean-squared-error de- cision-feedback equalizer (MMSE-DFE), which performs the equalization task. Since the optimum DFE for a wide range of channels produces decisions with a delay , the Kalman filter tracks the channel with a delay. A channel prediction module bridges the time gap between the channel estimates produced by the Kalman filter and those needed for the DFE adaptation. The proposed algorithm offers good tracking behavior for multiuser fading ISI channels at the expense of higher complexity than conventional adaptive algorithms. Applications include syn- chronous multiuser detection of independent transmitters, as well as coordinated transmission through many transmitter/receiver antennas, for increased data rate.\"",
        "Document: \"Speech transmission using rate-compatible trellis codes and embedded source coding. This paper presents bandwidth-efficient speech transmission systems using rate-compatible channel coders and variable bitrate embedded source coders. Rate-compatible punctured convolutional codes (RCPC) are often used to provide unequal error protection (UEP) via progressive bit puncturing. RCPC codes are well suited for constellations for which Euclidean and Hamming distances are equivalent (BPSK...\"",
        "1 is \"Processor Design in 3D Die-Stacking Technologies\", 2 is \"A* based algorithm for reduced complexity ML decoding of tailbiting codes\"",
        "Given above information, for an author who has written the paper with the title \"Mutual-Information Optimized Quantization for LDPC Decoding of Accurately Modeled Flash Data\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002952": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Convergence Models of Genetic Algorithm Selection Schemes':",
        "Document: \"Convergence Models of Genetic Algorithm Selection Schemes. We discuss the use of normal distribution theory as a tool to model the convergence characteristics of different GA selection schemes. The models predict the proportion of optimal alleles in function of the number of generations when optimizing the bit-counting function. The selection schemes analyzed are proportionate selection, tournament selection, truncation selection and elitist recombination. Simple yet accurate models are derived that have only a slight deviation from the experimental results. It is argued that this small difference is due to the build-up of covariances between the alleles \u2014 a phenomenon called linkage disequilibrium in quantitative genetics. We conclude with a brief discussion of this linkage disequilibrium.\"",
        "Document: \"Are multiple runs of genetic algorithms better than one?. There are conflicting reports over whether multiple independent runs of genetic algorithms (GAs) with small populations can reach solutions of higher quality or can find acceptable solutions faster than a single run with a large population. This paper investigates this question analytically using two approaches. First, the analysis assumes that there is a certain fixed amount of computational resources available, and identifies the conditions under which it is advantageous to use multiple small runs. The second approach does not constrain the total cost and examines whether multiple properly-sized independent runs can reach the optimal solution faster than a single run. Although this paper is limited to additively-separable functions, it may be applicable to the larger class of nearly decomposable functions of interest to many GA users. The results suggest that, in most cases under the constant cost constraint, a single run with the largest population possible reaches a better solution than multiple independent runs. Similarly, a single large run reaches the global faster than multiple small runs. The findings are validated with experiments on functions of varying difficulty.\"",
        "Document: \"Fluctuating crosstalk, deterministic noise, and GA scalability. This paper extends previous work showing how fluctuating crosstalk in a deterministic fitness function introduces noise into genetic algorithms. In that work, we modeled fluctuating crosstalk or nonlinear interactions among building blocks via higher-order Walsh coefficients. The fluctuating crosstalk behaved like exogenous noise and could be handled by increasing the population size and run duration. This behavior held until the strength of the crosstalk far exceeded the underlying fitness variance by a certain factor empirically observed. This paper extends that work by considering fluctuating crosstalk effects on genetic algorithm scalability using smaller-ordered Walsh coefficients on two extremes of building block scaling: uniformly-scaled and exponentially-scaled building blocks. Uniformly-scaled building blocks prove to be more sensitive to fluctuating crosstalk than do exponentially-scaled building blocks in terms of function evaluations and run duration but less sensitive to population sizing for large building-block interactions. Our results also have implications for the relative performance of building-block-wise mutation over crossover.\"",
        "Document: \"Genetic Algorithm Difficulty and the Modality of Fitness Landscapes.  We assume that the modality (i.e., number of local optima) of a fitness landscape is related to thedifficulty of finding the best point on that landscape by evolutionary computation (e.g., hillclimbers andgenetic algorithms (GAs)). We first examine the limits of modality by constructing a unimodal functionand a maximally multimodal function. At such extremes our intuition breaks down. A fitness landscapeconsisting entirely of a single hill leading to the global optimum proves to be hard for ... \"",
        "Document: \"Hierarchical Problem Solving and the Bayesian Optimization Algorithm.  The paper discusses three major issues. First, it discusses why it makes sense to approachproblems in a hierarchical fashion. It defines the class of hierarchically decomposable functionsthat can be used to test the algorithms that approach problems in this fashion. Finally, theBayesian optimization algorithm (BOA) is extended in order to solve the proposed class ofproblems.1 IntroductionRecently, the connection between human innovation and genetic algorithms has been discussed... \"",
        "1 is \"A Comprehensive Survey of Evolutionary-Based Multiobjective Optimization Techniques\", 2 is \"Comparison of multi-modal optimization algorithms based on evolutionary algorithms\"",
        "Given above information, for an author who has written the paper with the title \"Convergence Models of Genetic Algorithm Selection Schemes\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002987": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'On the Performance Regularity of Web Servers':",
        "Document: \"Stochastic analysis of distributed deadlock scheduling. Deadlock detection scheduling is an important, yet oft-overlooked problem that can significantly affect the overall performance of deadlock handling.An excessive initiation of deadlock detection increases overall message usage, resulting in degraded system performance in the absence of deadlocks; while a deficient initiation of deadlock detection increases the deadlock persistence time, resulting in an increased deadlock resolution cost in the presence of deadlocks. Such a performance tradeoff, however, is generally missing in literature. In this paper we study the impact of deadlock detection scheduling on the system performance, and show that there exists an optimal deadlock detection frequency that yields the minimum long-run mean average cost associated with the message complexity of deadlock detection and resolution algorithms, and the rate of deadlock formation, \u03bb. Based on the up-to-date deadlock detection and resolution algorithms, we show that the asymptotically optimal frequency of deadlock detection scheduling that minimizes the message overhead is cal O((\u03bb n)1/3), when the total number of processes n is sufficiently large. Furthermore, we show that in general fully distributed (uncoordinated) deadlock detection scheduling can not be performed as efficiently as centralized (coordinated) deadlock detection scheduling.\"",
        "Document: \"Drawing micro learning into MOOC: Using fragmented pieces of time to enable effective entire course learning experiences. Recently the massive open online course (MOOC) is an emerging trend that attracts many educators' and researchers' attentions. Based on our pilot study focusing on the development and operation of MOOC in Australia, we found MOOC is featured with mastery learning and blended learning, but it suffers from low completion rates. Brining micro learning into MOOC can be a feasible solution to improve current MOOC delivery and learning experience. We design a system which aims to provide adaptive micro learning contents as well as learning path identifications customized for each individual learner. To investigate how micro learning can impact learning experience and knowledge acquisitions of learners participated in MOOC, we suggest a potential scheme including hypotheses to evaluate our proposed approach.\"",
        "Document: \"An incrementally deployable path address scheme. The research community has proposed numerous network security solutions, each dealing with a specific problem such as address spoofing, denial-of-service attacks, denial-of-quality attacks, reflection attacks, viruses, or worms. However, due to the lack of fundamental support from the Internet, individual solutions often share little common ground in their design, which causes a practical problem: deploying all these vastly different solutions will add exceedingly high complexity to the Internet routers. In this paper, we propose a simple generic extension to the Internet, providing a new type of information, called path addresses, that simplify the design of security systems for packet filtering, fair resource allocation, packet classification, IP traceback, filter push-back, etc. IP addresses are owned by end hosts; path addresses are owned by the network core, which is beyond the reach of the hosts. We describe how to enhance the Internet protocols for path addresses that meet the uniqueness requirement, completeness requirement, safety requirement, and incrementally deployable requirement. We evaluate the performance of our scheme both analytically and by simulations, which show that, at small overhead, the false positive ratio and the false negative ratio can both be made negligibly small.\"",
        "Document: \"Pack Up Cloud: Recursive Datacenter Resource Management And Experimental Studies. Today's virtualized resource management in datacenters is typically VM-centric, which has serious problems in scalability. We propose a shift from the traditional flat, fine-grained model towards a hierarchical, abstract model that facilitates decomposition of the complex resource allocation problem in a divide-and-conquer approach. In particular, this paper elaborates a pack-centric framework in datacenter resource management, provides a case study with a recursive algorithm, and presents detailed results from experimental studies on VM-PM mapping through recursive resource allocation, which demonstrates the performance and scalability of the pack-centric framework.\"",
        "Document: \"Persistent Traffic Measurement Through Vehicle-to-Infrastructure Communications. Measuring point traffic volume and point-to-point traffic volume in a road system has important applications in transportation engineering. The connected vehicle technologies integrate wireless communications and computers into transportation systems, allowing wireless data exchanges between vehicles and road-side equipment, and enabling large-scale, sophisticated traffic measurement. This paper investigates the problems of persistent point traffic measurement and persistent point-to-point traffic measurement, which were not adequately studied in the prior art, particularly in the context of intelligent vehicular networks. We propose two novel estimators for privacy-preserving persistent traffic measurement: one for point traffic and the other for point-to-point traffic. The estimators are mathematically derived from the join result of traffic records, which are produced by the electronic roadside units with privacy-preserving data structures. We evaluate our estimation methods using simulations based on both real transportation traffic data and synthetic data. The numerical results demonstrate the effectiveness of the proposed methods in producing high measurement accuracy and allowing accuracy-privacy tradeoff through parameter setting.\"",
        "1 is \"Optimal fully adaptive minimal wormhole routing for meshes\", 2 is \"Deadlock detection and resolution in a CODASYL based data management system\"",
        "Given above information, for an author who has written the paper with the title \"On the Performance Regularity of Web Servers\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003030": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Educating Reflective Systems Developers':",
        "Document: \"Managing Risks: Post-Merger Integration of Information Systems. Mergers and acquisitions (M&A) require organizations to blend together different information system (IS) configurations. Unfortunately, less than 50 percent of M&A's achieve their goals, with IS integration being a major problem. Here, the authors offer a framework to help managers prepare for, analyze, and mitigate risks during post-merger IS integration. They identify key risks relating to IS integration content, process, and context, and present five strategies for mitigating those risks. Their framework aims to help managers proactively reduce the impact of adverse events. Adopting the framework supported by their templates is straightforward and the time and resources required are minimal. When properly executed, adoption increases the likelihood of successful merger outcomes; the framework is thus a valuable addition to the management tool box and can be applied in collaboration with key stakeholders at the start of - and at several points throughout - a post-merger IS integration.\"",
        "Document: \"The Issue Of Gender Within Computing: Reflections From The Uk And Scandinavia. This paper explores some of the reasons that may underlie the gender segregation and declining levels of female participation within the field of computing in Europe during the 1990s in both the professional (industrial) and academic spheres. The interrelationships between three areas - communicative processes, social networks and legitimizing claims to knowledge overlaid by gendered-power relations - are used to analyse and explain the existing situation. The paper draws upon statistical data to explore the extent of gender segregation and then focuses on the authors' own experiences within the UK and Scandinavia in order to explore some of the underlying causes. While direct discrimination does still occur, the paper suggests that indirect, deep-rooted discrimination is the major reason for the situation that currently exists. Drawing upon our own experiences in academia and business and acknowledging the importance of the institutional context, the paper offers a number of recommendations as to how the current situation may be improved. We suggest first that consideration is given to the pedagogical design and marketing of computing courses so that individuals are initially attracted to computing from far more diverse backgrounds, approaches and interests than at present. Second, we suggest that those with influence in the field reflect upon the constitution and behaviours of the informal networks in which they are involved and seek to include female researchers more actively here. Finally we suggest that consideration is given in more general terms to how the field may become more gender neutral and, thus, more inclusive in the future. Masculine discourses and 'hard' skills have dominated within computing for too long and contribute significantly to the declining participation of women within computing.\"",
        "Document: \"Managing risk in software process improvement: an action research approach. Many software organizations engage in software process improvement (SPI) initiatives to increase their capability to develop quality solutions at a competitive level. Such efforts, however, are complex and very demanding. A variety of risks makes it difficult to develop and implement new processes. We studied SPI in its organizational context through collaborative practice research (CPR), a particular form of action research. The CPR program involved close collaboration between practitioners and researchers over a three-year period to understand and improve SPI initiatives in four Danish software organizations. The problem of understanding and managing risks in SPI teams emerged in one of the participating organizations and led to this research. We draw upon insights from the literature on SPI and software risk management as well as practical lessons learned from managing SPI risks in the participating software organizations. Our research offers two contributions. First, we contribute to knowledge on SPI by proposing an approach to understand and manage risks in SPI teams. This risk management approach consists of a framework for understanding risk areas and risk resolution strategies within SPI and a related process for managing SPI risks. Second, we contribute to knowledge on risk management within the information systems and software engineering disciplines. We propose an approach to tailor risk management to specific contexts. This approach consists of a framework for understanding and choosing between different forms of risk management and a process to tailor risk management to specific contexts.\"",
        "Document: \"Embracing digital innovation in incumbent firms: how volvo cars managed competing concerns. AbstractPast research provides instructive yet incomplete answers as to how incumbent firms can address competing concerns as they embrace digital innovation. In particular, it offers only partial explanations of why different concerns emerge, how they manifest, and how firms can manage them. In response, we present a longitudinal case study of Volvo Cars'connected car initiative. Combining extant literature with insights from the case, we argue that incumbent firms face four competing concerns\u2014capability (existing versus requisite), focus (product versus process), collaboration (internal versus external), and governance (control versus flexibility)\u2014and that these concerns are systemically interrelated. Firms must therefore manage these concerns cohesively by continuously balancing new opportunities and established practices.\"",
        "Document: \"Organizational Path Constitution in Technological Innovation: Evidence from Rural Telehealth. Path constitution theory has emerged as a promising combination of two contrasting perspectives on technological innovation: path dependence, which focuses on historically embedded, contingent processes that are more or less beyond the control of actors, and path creation, which emphasizes mindful contributions from powerful actors. However, the current path constitution literature focuses on macro- and multi-level inquiry without addressing the specific processes, opportunities, and challenges related to organizational (micro-level) technological innovation. Against this backdrop, we draw on the innovation and path literature as well as a case study of telehealth innovation in a public health organization to theorize how technological innovation paths constitute in organizational contexts. The proposed theory distinguishes between innovation path status and innovation path trajectory to help researchers understand and explain how organizations transform and reinforce path constitution patterns, how innovation paths may merge with or separate from other paths, and how organizations may arrive at a lock-in that challenges them to break out from dominant and seemingly irreversible action patterns.\"",
        "1 is \"Collaboration, peer review and open source software\", 2 is \"Analyzing The Social Capital Value Chain In Community Network Interfaces\"",
        "Given above information, for an author who has written the paper with the title \"Educating Reflective Systems Developers\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003035": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Self-protection in p2p networks: choosing the right neighbourhood':",
        "Document: \"Gaining insight on friendly jamming in a real-world IEEE 802.11 network. Frequency jamming is the fiercest attack tool to disrupt wireless communication and its malicious aspects have received much attention in the literature. Yet, several recent works propose to turn the table and employ so-called friendly jamming for the benefit of a wireless network. For example, recently proposed friendly jamming applications include hiding communication channels, injection attack defense, and access control. This work investigates the practical viability of friendly jamming by applying it in a real-world network. To that end, we implemented a reactive and frame-selective jammer on a consumer grade IEEE 802.11 access point. Equipped with this, we conducted a three weeks real-world study on the jammer's performance and side-effects on legitimate traffic (the cost of jamming) in a university office environment. Our results provide detailed insights on crucial factors governing the trade-off between the effectiveness of friendly jamming (we evaluated up to 13 jammers) and its cost. In particular, we observed -- what we call the power amplification phenomenon -- an effect that aggravates the known hidden station problem when the number of jammers increases. However, we also find evidence that this effect can be alleviated by collaboration between jammers, which again enables effective and minimally invasive friendly jamming.\"",
        "Document: \"There are many apps for that: quantifying the availability of privacy-preserving apps. The adage \\\"there's an app for that\\\" holds true in modern app stores. Indeed, app stores usually go further and provide multiple apps with very similar functionality; examples range from flashlight apps to alarm clocks. We call these functionally-similar apps. When searching for these apps, users are often presented with a vast array of choices, but no distinction is made in the user interface to highlight the relative privacy risks inherent in choosing one app over another. Yet the availability of many functionally-similar apps raises the question of whether some apps are significantly less invasive than others. In this paper, we take several steps toward answering this question. We begin by enumerating 2 500 groups of functionally-similar apps in the Google Play Store. Within groups of apps, we use static analysis to understand the real-world risks coming from apps with aggressive permission usage. By leveraging an established ranking system, and combining it with real-world data from over 28 000 Android devices, we quantify the improvements that can be made if users installed apps with privacy in mind. We observe that at least 25.6% of apps contain libraries that gratuitously exploit available permissions and find that 43.5% of apps could be swapped for comparable alternatives that require fewer permissions. Permissions saved may deliver important privacy and security improvements, including preventing access to the calendar (in 24% of cases), sending text messages (12%) and recording audio (8%). This is particularly important for apps which embed third-party libraries, since library code executes with the same permissions as the app itself.\"",
        "Document: \"Self-protection in p2p networks: choosing the right neighbourhood. In unstructured peer-to-peer networks, as in real life, a good neighbourhood is not only crucial for a peaceful sleep, but also for an exchange of important gossips and for finding good service. This work investigates self-protection mechanisms based on reputation in unstructured peer-to-peer networks. We use a simple approach where each peer rates the service provided by others and exchanges the collected knowledge with its direct neighbours. Based on reputation values peers manage their connections to direct neighbours and make service provisioning decisions. To quantify the impact of our proposed scheme, we implement a simple protocol in a fully unstructured peer-to-peer network. We show that free riding and the impact of malicious peers trying to poison the network with bad files is minimised. Furthermore, we show that a good neighbourhood protects peers from selecting bad files, while free riders suffer in a bad neighbourhood of malicious peers.\"",
        "Document: \"Looks Like Eve: Exposing Insider Threats Using Eye Movement Biometrics. We introduce a novel biometric based on distinctive eye movement patterns. The biometric consists of 20 features that allow us to reliably distinguish users based on differences in these patterns. We leverage this distinguishing power along with the ability to gauge the users\u2019 task familiarity, that is, level of knowledge, to address insider threats. In a controlled experiment, we test how both time and task familiarity influence eye movements and feature stability, and how different subsets of features affect the classifier performance. These feature subsets can be used to tailor the eye movement biometric to different authentication methods and threat models. Our results show that eye movement biometrics support reliable and stable continuous authentication of users. We investigate different approaches in which an attacker could attempt to use inside knowledge to mimic the legitimate user. Our results show that while this advance knowledge is measurable, it does not increase the likelihood of successful impersonation. In order to determine the time stability of our features, we repeat the experiment twice within 2 weeks. The results indicate that we can reliably authenticate users over the entire period. We show that lower sampling rates provided by low-cost hardware pose a challenge, but that reliable authentication is possible even at the rate of 50Hz commonly available with consumer-level devices. In a second set of experiments, we evaluate how our authentication system performs across a variety of real-world tasks, including reading, writing, and web browsing. We discuss the advantages and limitations of our approach in detail and give practical insights on the use of this biometric in a real-world environment.\"",
        "Document: \"Generating Secret Keys from Biometric Body Impedance Measurements. Growing numbers of ubiquitous electronic devices and services motivate the need for effortless user authentication and identification. While biometrics are a natural means of achieving these goals, their use poses privacy risks, due mainly to the difficulty of preventing theft and abuse of biometric data. One way to minimize information leakage is to derive biometric keys from users' raw biometric measurements. Such keys can be used in subsequent security protocols and ensure that no sensitive biometric data needs to be transmitted or permanently stored. This paper is the first attempt to explore the use of human body impedance as a biometric trait for deriving secret keys. Building upon Randomized Biometric Templates as a key generation scheme, we devise a mechanism that supports consistent regeneration of unique keys from users' impedance measurements. The underlying set of biometric features are found using a feature learning technique based on Siamese networks. Compared to prior feature extraction methods, the proposed technique offers significantly improved recognition rates in the context of key generation. Besides computing experimental error rates, we tailor a known key guessing approach specifically to the used key generation scheme and assess security provided by the resulting keys. We give a very conservative estimate of the number of guesses an adversary must make to find a correct key. Results show that the proposed key generation approach produces keys comparable to those obtained by similar methods based on other biometrics.\"",
        "1 is \"An investigation of geographic mapping techniques for internet hosts\", 2 is \"Hardware support for code integrity in embedded processors\"",
        "Given above information, for an author who has written the paper with the title \"Self-protection in p2p networks: choosing the right neighbourhood\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003067": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Distributed artificial intelligence for group decision support: integration of problem solving, coordination, and learning':",
        "Document: \"The effect of synergy enhancement on information technology portfolio selection. This paper investigates how firms can use synergy to optimize their information technology portfolios. We begin by developing a framework for the portfolio selection by identifying three types of information technology synergy. Next, we use this framework to examine the impact of different types of synergy on the portfolio selection. Analytical models are developed to illustrate the roles of different types of the synergy, and analytical and computational methods are used to investigate the impact of the synergy. The analysis in this paper provides conditions in which synergy enhancement results in a more efficient or a less efficient portfolio. Our study establishes that firms with higher risk thresholds are more likely to obtain more efficient information technology portfolios by enhancing synergy, whereas firms with lower risk thresholds are less likely to benefit from enhancing synergy.\"",
        "Document: \"Portfolio Selection Model for Enhancing Information Technology Synergy. This paper presents a methodological framework for IT project selection that focuses on the impact of synergy enhancement on portfolio return and risk. As the size of firms&#39; IT investments continues to increase, the demand for strong methodologies for IT portfolio selection has been increasing. Using the mean-variance efficient frontier as a tool to balance portfolio return and portfolio risk, we ...\"",
        "Document: \"The Online Retailing Challenge: Forward Integration and E-Backend Development. This paper aims to categorize some of the fundamental information systems (IS) challenges with the development of consumer-oriented electronic commerce or online retailing. With the emergence of new online intermediaries such as America Online, Amazon and eBay, many traditional firms are at risk and need to respond. Despite their size and retailing experience most of the incumbent retailers such as Wal-Mart in the U.S. or Metro in Europe have been slow to develop successful online channels. IS-based forward integration into customer activities, such as product and price search, and the development of electronic backends (\"e-backends\") have been identified as the two major and distinct inhibitors to online retailing success. Our analysis and findings draw from information system design work with retailers in the U.S. and Europe from 1997 through 1999. I. INTRODUCTION In 1996, Swiss-based Metro Group, the world's second largest retailer in 1998, recognized the emergence of a new retail channel, something called electronic commerce. To expand or at least protect its traditional retail business, Metro purchased a small Internet service provider and targeted the German market with an aggressively- priced Internet service, saying, \"In the bricks-and-mortar world, we build our customers a parking lot so that they can reach us. Internet access is like a parking lot.\" Metro, like many traditional retailers entering electronic commerce, looked first to invest in online elements with which they could somehow associate with the world of parking lots, shelf space and cash registers. A few of the earliest interactive grocery stores even had customers wander down virtual isles and pull virtual boxes of corn flakes from virtual shelves. It is an interesting observation that in the beginning of Internet commerce, as it still is today, the online retail success stories, such as Amazon, eBay or CDNow, are not from the world of retailing. Success in consumer electronic commerce appears to be less determined by things like parking lots and shelf space. Furthermore, profitability, as we will illustrate, will not simply be a function of substituting store rent with rented server space and Internet access. Instead our observations suggest that success may depend on IS design, integration and management skills. Our insights have been developed while working with large retailers and manufacturers in the U.S. and Europe on electronic commerce business development and IS architecture design.\"",
        "Document: \"Consumer Cost Differences For Traditional And Internet Markets. In this paper we address research issues related to the economics of electronic, Internet-based markets. First, what are the consumer cost-based differences for traditional and electronic markets! Second, what revenue implications does increased electronic market utilization have for sellers and transaction intermediaries! Based on an empirical, survey-based study of an electronic market in the sports trading card industry we find that prices, search costs, and sales taxes are lower in the electronic markets, while risk costs, distribution costs, and market costs are lower in traditional markets. We discuss the implications this has for seller, intermediary and government revenue sources.\"",
        "Document: \"Knowledge management and data mining for marketing. Due to the proliferation of information systems and technology, businesses increasingly have the capability to accumulate huge amounts of customer data in large databases. However, much of the useful marketing insights into customer characteristics and their purchase patterns are largely hidden and untapped. Current emphasis on customer relationship management makes the marketing function an ideal application area to greatly benefit from the use of data mining tools for decision support. A systematic methodology that uses data mining and knowledge management techniques is proposed to manage the marketing knowledge and support marketing decisions. This methodology can be the basis for enhancing customer relationship management.\"",
        "1 is \"Reasoning in incomplete domains\", 2 is \"Managing metadata for distributed information servers\"",
        "Given above information, for an author who has written the paper with the title \"Distributed artificial intelligence for group decision support: integration of problem solving, coordination, and learning\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003085": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Similarity Search in Time Series of Dynamical Model-based Systems':",
        "Document: \"Incremental Reverse Nearest Neighbor Ranking in Vector Spaces. In this paper, we formalize the novel concept of incremental reverse nearest neighbor ranking and suggest an original solution for this problem. We propose an efficient approach for reporting the results incrementally without the need to restart the search from scratch. Our approach can be applied to a multi-dimensional feature database which is hierarchically organized by any R-tree like index structure. Our solution does not assume any preprocessing steps which makes it applicable for dynamic environments where updates of the database frequently occur. Experiments show that our approach reports the ranking results with much less page accesses than existing approaches designed for traditional reverse nearest neighbor search applied to the ranking problem.\"",
        "Document: \"Knowledge-Enriched Route Computation. Directions and paths, as commonly provided by navigation systems, are usually derived considering absolute metrics, e.g., finding the shortest or the fastest path within an underlying road network. With the aid of Volunteered Geographic Information (VGI), i.e., geo-spatial information contained in user generated content, we aim at obtaining paths that do not only minimize distance but also lead through more popular areas. Based on the importance of landmarks in Geographic Information Science and in human cognition, we extract a certain kind of VGI, namely spatial relations that define closeness (nearby, next to) between pairs of points of interest (POIs), and quantify them following a probabilistic framework. Subsequently, using Bayesian inference we obtain a crowd-based closeness confidence score between pairs of POIs. We apply this measure to the corresponding road network based on an altered cost function which does not exclusively rely on distance but also takes crowdsourced geo-spatial information into account. Finally, we propose two routing algorithms on the enriched road network. To evaluate our approach, we use Flickr photo data as a ground truth for popularity. Our experimental results - based on real world datasets - show that the paths computed w.r.t. our alternative cost function yield competitive solutions in terms of path length while also providing more \"popular\" paths, making routing easier and more informative for the user.\"",
        "Document: \"MARiO: multi-attribute routing in open street map. In recent years, the Open Street Map (OSM) project collected a large repository of spatial network data containing a rich variety of information about traffic lights, road types, points of interest etc.. Formally, this network can be described as a multi-attribute graph, i.e. a graph considering multiple attributes when describing the traversal of an edge. In this demo, we present our framework for Multi-Attribute Routing in Open Street Map (MARiO). MARiO includes methods for preprocessing OSM data by deriving attribute information and integrating additional data from external sources. There are several routing algorithms already available and additional methods can be easily added by using a plugin mechanism. Since routing in a multi-attribute environment often results in large sets of potentially interesting routes, our graphical frontend allows various views to interactively explore query results.\"",
        "Document: \"Spatial Join for High-Resolution Objects. Modern database applications including computer-aided design (CAD), medical imaging, molecular biology, or Multimedia Information Systems impose new requirementson efficient spatial query processing. One of themost common query types in Spatial Database ManagementSystems is the spatial join. In this paper, we investigate spatialjoin processing for two sets of very complex spatial objects.We present an approach that is based on a fast filterstep performing the spatial join on simple primitives whichconservatively approximate the objects. Our main attentionis focused on the problem how to generate approximationsadequate for high-resolution objects. In this paper, we introducegray approximations as a general concept whichhelps to range between replicating and non-replicating objectapproximations. The key idea of our approach is tobuild these replications based on statistical informationtaking the data distribution of the respective join-partnerrelation into account. Furthermore, our approach usescompression techniques for the effective storage and retrievalof the decomposed spatial objects. We demonstratethe benefits of our new method for the spatial intersectionjoin on high resolution data. The experimental evaluationon real-world test data points out that our new concept acceleratesthe spatial intersection join considerably.\"",
        "Document: \"Similarity Search in Time Series of Dynamical Model-based Systems. Similarity search in time series is usually based on an assessment of the geometric similarity of the time series curves. In bioinformatics, dynamical model-based analysis and processing is used, where the curve itself is not meaningful. However, some internal features based on a model extracted from time series are meaningful. Therefore, the similarity is based on a dynamical model explaining the observation instead of being based merely on the superficial observation. There currently exist no methods for meaningful similarity search on such time series data emerging in bioinformatics. In this paper, we introduce a new similarity search method for time series based on similarity of internal features, called the perturbation method.\"",
        "1 is \"High dimensional similarity search with space filling curves\", 2 is \"Discovering Similar Multidimensional Trajectories\"",
        "Given above information, for an author who has written the paper with the title \"Similarity Search in Time Series of Dynamical Model-based Systems\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003119": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Criteria on Proximity Graphs for Boundary Extraction and Spatial Clustering':",
        "Document: \"Efficient wayfinding in complex environments: derivation of a continuous space shortest path. This paper discusses the problem of wayfinding in complex environments. Movement is complicated in many ways, including the existence of obstacles that impede travel across continuous space. To support route planning and navigation for the blind, shipping, robotics, etc., methods have been developed to identify the shortest or most efficient path. This paper details a GIS based method for finding an optimal path for this problem. The developed approach offers substantial computational advances over existing approaches, making it appealing for real time travel support. Application results are presented to demonstrate the effectiveness of the approach.\"",
        "Document: \"Criteria on Proximity Graphs for Boundary Extraction and Spatial Clustering. Proximity and density information modeling of 2D point-data by Delaunay Diagrams has delivered a powerful exploratory and argument-free clustering algorithm [6] for geographical data mining [13]. The algorithm obtains cluster boundaries using a Short-Long criterion and detects non-convex clusters, high and low density clusters, clusters inside clusters and many other robust results. Moreover, its computation is linear in the size of the graph used. This paper demonstrates that the criterion remains effective for exploratory analysis and spatial data mining where other proximity graphs are used. It also establishes a hierarchy of the modeling power of several proximity graphs and presents how the argument free characteristic of the original algorithm can be traded for argument tuning. This enables higher than 2 dimensions by using linear size proximity graphs like k-nearest neighbors.\"",
        "Document: \"A Coverage Model for Improving Public Transit System Accessibility and Expanding Access. Accessible and efficient service are vital features of well-utilized public transit systems. Modeling approaches to support the evaluation of transit operations for management and planning are essential for continued improvement. A hybrid coverage model is developed in this paper for simultaneously expanding service access and increasing accessibility. This paper discusses the use and integration of this model in a geographic information system environment for strategic planning. Analysis is presented for public transit service in Brisbane, Australia. The structured model provides flexibility in developing viable policies for addressing system improvements and service expansion, all of which are likely to promote increased utilization of public transit.\"",
        "Document: \"Accessibility tradeoffs in public transit planning. .\u00a0\u00a0\u2002Spatial accessibility is a critical consideration in the provision of services, both public and private. In public transit\n planning, accessibility is comprised of access and geographic coverage. Interestingly, these two considerations are somewhat\n at odds with each other. Access is important because it is the process associated with getting to and departing from the service.\n Such access is typically perceived of in spatial terms as the physical proximity to transit stops or stations. Additional\n stops along a route usually mean greater access, because a stop is more likely to be within an acceptable walking/driving\n standard for a larger number of people. On the other hand, more stops and greater access slow transit travel speeds, thereby\n decreasing the area of service reachable given a travel time budget. More stops along a route translate to greater service\n interruption and longer travel times. The faster the travel time, the more desirable the service. Further, if travel times\n become excessive, then user demand for service will decrease. All of this means that stop spacing along a route is central\n to accessibility, as it is a tradeoff of access (more stops) and geographic coverage (service efficiency through less stops).\n This paper details modeling approaches for addressing accessibility concerns in an integrated fashion. Bus-based transit service\n in Columbus, Ohio will be utilized to illustrate the usefulness of these approaches in transit planning.\"",
        "Document: \"Commercial Gis Location Analytics: Capabilities And Performance. An essential location analytic method is GIS, able to support map-based display along with geographic data creation, management, manipulation in addition to functions for suitability evaluation. Location models have proven to be critical as well, with many prominent approaches found in popular GIS software. This paper reviews a class of location models and provides an overview of the methods used to solve these models. This is significant because of the broad use and application of location models in GIS to address important problems and issues facing society. Spatial analytical insights are essential. The implications of models and methods available through GIS are explored, particularly notions of solution quality. Case studies are offered to highlight ease of access to location models in GIS along with observed computational performance.\"",
        "1 is \"Machine learning for targeted display advertising: transfer learning in action\", 2 is \"The priority R-tree: A practically efficient and worst-case optimal R-tree\"",
        "Given above information, for an author who has written the paper with the title \"Criteria on Proximity Graphs for Boundary Extraction and Spatial Clustering\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003152": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Dynamic selection of tile sizes':",
        "Document: \"Precise memory leak detection for java software using container profiling. A memory leak in a Java program occurs when object references that are no longer needed are unnecessarily maintained. Such leaks are difficult to detect because static analysis typically cannot precisely identify these redundant references, and existing dynamic leak detection tools track and report fine-grained information about individual objects, producing results that are usually hard to interpret and lack precision. In this article we introduce a novel container-based heap-tracking technique, based on the fact that many memory leaks in Java programs occur due to incorrect uses of containers, leading to containers that keep references to unused data entries. The novelty of the described work is twofold: (1) instead of tracking arbitrary objects and finding leaks by analyzing references to unused objects, the technique tracks only containers and directly identifies the source of the leak, and (2) the technique computes a confidence value for each container based on a combination of its memory consumption and its elements' staleness (time since last retrieval), while previous approaches do not consider such combined metrics. Our experimental results show that the reports generated by the proposed technique can be very precise: for two bugs reported by Sun, a known bug in SPECjbb 2000, and an example bug from IBM developerWorks, the top containers in the reports include the containers that leak memory.\"",
        "Document: \"Evaluating the imprecision of static analysis. This work discusses two non-traditional approaches for evaluating the imprecision of static analysis. The approaches are based on proofs of feasibility or infeasibility that are constructed manually by the experimenters. We also describe our initial experience with these techniques.\"",
        "Document: \"Effective automatic parallelization of stencil computations. Performance optimization of stencil computations has been widely studied in the literature, since they occur in many computationally intensive scientific and engineering applications. Compiler frameworks have also been developed that can transform sequential stencil codes for optimization of data locality and parallelism. However, loop skewing is typically required in order to tile stencil codes along the time dimension, resulting in load imbalance in pipelined parallel execution of the tiles. In this paper, we develop an approach for automatic parallelization of stencil codes, that explicitly addresses the issue of load-balanced execution of tiles. Experimental results are provided that demonstrate the effectiveness of the approach.\"",
        "Document: \"LeakChecker: Practical Static Memory Leak Detection for Managed Languages. Static detection of memory leaks in a managed language such as Java is attractive because it does not rely on any leak-triggering inputs, allowing compile-time tools to find leaks before software is released. A long-standing issue that prevents practical static memory leak detection for Java is that it can be very expensive to statically determine object liveness in large applications. We present a novel (and the first practical) static leak detection technique that bypasses this problem by considering a common leak pattern. In many cases severe leaks occur in loops where, in each iteration, some objects created by the iteration are unnecessarily referenced by objects external to the loop. These unnecessary references are never used in later loop iterations. Based on this insight, we shift our focus from computing liveness, which is very difficult to achieve precisely and efficiently for large programs, to the easier goal of identifying objects that flow out of a loop but never flow back in. We formalize this analysis using a type and effect system and present its key properties. The analysis was implemented in a tool called LeakChecker and used to detect leaks in eight real-world programs, such as Eclipse, Derby, and log4j. LeakChecker not only identified known leaks, but also discovered new ones whose causes were unknown beforehand, while exhibiting a false positive rate suitable for practical use.\n\n\"",
        "Document: \"Scalable Runtime Bloat Detection Using Abstract Dynamic Slicing. Many large-scale Java applications suffer from runtime bloat. They execute large volumes of methods and create many temporary objects, all to execute relatively simple operations. There are large opportunities for performance optimizations in these applications, but most are being missed by existing optimization and tooling technology. While JIT optimizations struggle for a few percent improvement, performance experts analyze deployed applications and regularly find gains of 2\u00d7 or more. Finding such big gains is difficult, for both humans and compilers, because of the diffuse nature of runtime bloat. Time is spread thinly across calling contexts, making it difficult to judge how to improve performance. Our experience shows that, in order to identify large performance bottlenecks in a program, it is more important to understand its dynamic dataflow than traditional performance metrics, such as running time. This article presents a general framework for designing and implementing scalable analysis algorithms to find causes of bloat in Java programs. At the heart of this framework is a generalized form of runtime dependence graph computed by abstract dynamic slicing, a semantics-aware technique that achieves high scalability by performing dynamic slicing over bounded abstract domains. The framework is instantiated to create two independent dynamic analyses, copy profiling and cost-benefit analysis, that help programmers identify performance bottlenecks by identifying, respectively, high-volume copy activities and data structures that have high construction cost but low benefit for the forward execution. We have successfully applied these analyses to large-scale and long-running Java applications. We show that both analyses are effective at detecting inefficient operations that can be optimized for better performance. We also demonstrate that the general framework is flexible enough to be instantiated for dynamic analyses in a variety of application domains.\"",
        "1 is \"Solving Dense Linear Systems on Graphics Processors\", 2 is \"Simulating reachability using first-order logic with applications to verification of linked data structures\"",
        "Given above information, for an author who has written the paper with the title \"Dynamic selection of tile sizes\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003164": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Secure Transmission for Heterogeneous Cellular Networks With Wireless Information and Power Transfer.':",
        "Document: \"Coverage and capacity in WiMAX and TD-SCDMA evolution: a comparative study. In this paper, fundamental practical performance comparison between WiMAX and TDD-HSDPA, the evolution variant of TD-SCDMA is provided. Comparisons are based on fair assumptions that guarantee the minimum QoS for both systems. It shows that the intermediate evolution of TD-SCDMA can provide better spectrum efficiency and coverage than those can be offered by WiMAX under the assumed circumstance, such as the deployment of beamforming with very high accuracy assumption and taking very high frequency re-use by the WiMAX system.\"",
        "Document: \"Optimal Incentive Design for Cloud-Enabled Multimedia Crowdsourcing. Multimedia crowdsourcing possesses a huge potential to actualize many new applications that are expected to yield tremendous benefits in diverse fields including environment monitoring, emergency rescues during natural catastrophes, online education, sports, and entertainment. Nonetheless, multimedia crowdsourcing unfolds new challenges such as big data acquisition and processing, more stringent quality of service requirements, and heterogeneity of crowdsensors. Consequently, incentive mechanisms specifically tailored to multimedia crowdsourcing applications need to be developed to fully utilize the potential of multimedia crowdsourcing. In this paper, we design an optimal incentive mechanism for the smartphone contributors to participate in a cloud-enabled multimedia crowdsourcing scheme. We establish a condition that determines whether the smartphones are eligible to participate, and provide a close form expression for the optimal duration of service from the contributors, for a given reward from the crowdsourcer. Consequently, we derive the conditions for existence of an optimal reward for the contributors from the crowdsourcer, and prove its uniqueness. We numerically illustrate the performance of our model considering logarithmic and linear cost functions for the cloud resources. The similarity of the results for different cost models corroborates the validity of our model and the results, whereas the difference in the magnitudes suggests that the strategy of the crowdsourcer as well as the strategies of the smartphone participants considerably depend on the cloud cost model.\"",
        "Document: \"A Fast Formation Flocking Scheme for a Group of Interactive Distributed Mobile Nodes in Autonomous Networks. Ubi-Com promises to provide a diversity of services at anytime and anyplace. The mobile nodes are envisioned to cooperate freely and move on a plane with the help of the efficient control and distributed coordination strategies. To achieve this goal, mobile nodes group communications in Ubi-Com is becoming increasing important and challenging. In this paper, we focus on the formation flocking in a group of autonomous mobile nodes. In the scenario, there are two kinds of nodes: the leader node and the follower nodes. The follower nodes are required to follow the leader node wherever it goes (following), while keeping a particular formation they are given in input (flocking). A novel scheme is proposed on the basis of the relative motion theory. Extensive theoretical analysis and simulation results have demonstrated that this scheme provides the follower nodes an efficient and timely method to follow the leader with the shortest path and the shortest time. In addition, the reported scheme is scalable in the sense that the processing load in each node is not increasing with more nodes in a group.\"",
        "Document: \"Modeling and characterization of transmission energy consumption in Machine-to-Machine networks. In future, a massive number of devices are expected to communicate for pervasive monitoring and measurement, industrial automation, and home/building energy management. Nevertheless, such Machine-to-Machine (M2M) communications are prone to failure due to depletion of machines energy if the communication system is not designed properly. A key step in building energy-efficient protocols for large-scale M2M communications is to assess, model or characterize a network energy consumption profile. To meet this need, we develop a theoretical and numerical framework to evaluate the cumulative distribution function (CDF) of the total energy consumption by fully exploiting the properties of stochastic geometry. Unlike the other existing approaches, we model the transmission energy as a function of transmission power, packet size, and link affordable capacity that is a logarithmic function of experienced Signal to Interference plus Noise Ratio (SINR). Since it is very difficult, if not impossible, to derive a closed-form expression for the CDF, we derive numerically computable first- and second-order moments of energy consumption. Applying these moments we then propose Log-normal and Log-logistic distributions to approximate the CDF. Our simulation results show that Log-logistic almost precisely approximates the exact CDF.\"",
        "Document: \"Authentication Overhead in Wireless Networks. Access authentication is an indispensable component in wireless mobile networks for providing network resilience and ensuring secure communications. Normally, more complicated secure algorithms will consume more resources, including bandwidth, power, computation time and storage space. This will result in higher authentication overhead, which, however, is not fully studied in the literature. In this paper, we will analyze and exam the authentication overhead in wireless networks. The number of user authentication request (UAR) is formulated as the performance metric to demonstrate the authentication traffic load. The probability mass function and the average number of UAR are developed. In deriving the formula, Registration Area residence time is relaxed to follow a general probability density function. In addition, an efficient recursive algorithm is developed to significantly reduce the computation complexity. Numerical examples are presented to investigate the interaction between authentication overhead and the key tele-traffic parameters. The analysis has been validated by the simulation.\"",
        "1 is \"Process cruise control: event-driven clock scaling for dynamic power management\", 2 is \"Mixed Chroma Sampling-Rate High Efficiency Video Coding for Full-Chroma Screen Content\"",
        "Given above information, for an author who has written the paper with the title \"Secure Transmission for Heterogeneous Cellular Networks With Wireless Information and Power Transfer.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003256": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Alternative Essences Of Intelligence':",
        "Document: \"A point-and-click interface for the real world: laser designation of objects for mobile manipulation. We present a novel interface for human-robot interaction that enables a human to intuitively and unambiguously select a 3D location in the world and communicate it to a mobile robot. The human points at a location of interest and illuminates it (``clicks it'') with an unaltered, off-the-shelf, green laser pointer. The robot detects the resulting laser spot with an omnidirectional, catadioptric camera with a narrow-band green filter. After detection, the robot moves its stereo pan/tilt camera to look at this location and estimates the location's 3D position with respect to the robot's frame of reference. Unlike previous approaches, this interface for gesture-based pointing requires no instrumentation of the environment, makes use of a non-instrumented everyday pointing device, has low spatial error out to 3 meters, is fully mobile, and is robust enough for use in real-world applications. We demonstrate that this human-robot interface enables a person to designate a wide variety of everyday objects placed throughout a room. In 99.4% of these tests, the robot successfully looked at the designated object and estimated its 3D position with low average error. We also show that this interface can support object acquisition by a mobile manipulator. For this application, the user selects an object to be picked up from the floor by ``clicking'' on it with the laser pointer interface. In 90% of these trials, the robot successfully moved to the designated object and picked it up off of the floor.\"",
        "Document: \"Visual odometry and control for an omnidirectional mobile robot with a downward-facing camera. An omnidirectional Mecanum base allows for more flexible mobile manipulation. However, slipping of the Mecanum wheels results in poor dead-reckoning estimates from wheel encoders, limiting the accuracy and overall utility of this type of base. We present a system with a downward-facing camera and light ring to provide robust visual odometry estimates. We mounted the system under the robot which allows it to operate in conditions such as large crowds or low ambient lighting. We demonstrate that the visual odometry estimates are sufficient to generate closed-loop PID (Proportional Integral Derivative) and LQR (Linear Quadratic Regulator) controllers for motion control in three different scenarios: waypoint tracking, small disturbance rejection, and sideways motion. We report quantitative measurements that demonstrate superior control performance when using visual odometry compared to wheel encoders. Finally, we show that this system provides high-fidelity odometry estimates and is able to compensate for wheel slip on a four-wheeled omnidirectional mobile robot base.\"",
        "Document: \"The domesticated robot: design guidelines for assisting older adults to age in place. Many older adults wish to remain in their own homes as they age [16]. However, challenges in performing home upkeep tasks threaten an older adult's ability to age in place. Even healthy independently living older adults experience challenges in maintaining their home [13]. Challenges with home tasks can be compensated through technology, such as home robots. However, for home robots to be adopted by older adult users, they must be designed to meet older adults' needs for assistance and the older users must be amenable to robot assistance for those needs. We conducted a needs assessment to (1) assess older adults' openness to assistance from robots; and (2) understand older adults' opinions about using an assistive robot to help around the home. We administered questionnaires and conducted structured group interviews with 21 independently living older adults (ages 65-93). The questionnaire data suggest that older adults prefer robot assistance for cleaning and fetching/organizing tasks overall. However their assistance preferences discriminated between tasks. The interview data provided insight as to why they hold such preferences. Older adults reported benefits of robot assistance (e.g., the robot compensating for limitations, saving them time and effort, completing undesirable tasks, and performing tasks at a high level of performance). Participants also reported concerns such as the robot damaging the environment, being unreliable at or incapable of doing a task, doing tasks the older adult would rather do, or taking up too much space/storage. These data, along with specific comments from participant interviews, provide the basis for preliminary recommendations for designing mobile manipulator robots to support aging in place.\"",
        "Document: \"Domestic Robots for Older Adults: Attitudes, Preferences, and Potential. The population of older adults in America is expected to reach an unprecedented level in the near future. Some of them have difficulties with performing daily tasks and caregivers may not be able to match pace with the increasing need for assistance. Robots, especially mobile manipulators, have the potential for assisting older adults with daily tasks enabling them to live independently in their homes. However, little is known about their views of robot assistance in the home. Twenty-one independently living older Americans (65-93 years old) were asked about their preferences for and attitudes toward robot assistance via a structured group interview and questionnaires. In the group interview, they generated a diverse set of 121 tasks they would want a robot to assist them with in their homes. These data, along with their questionnaire responses, suggest that the older adults were generally open to robot assistance but were discriminating in their acceptance of assistance for different tasks. They preferred robot assistance over human assistance for tasks related to chores, manipulating objects, and information management. In contrast, they preferred human assistance to robot assistance for tasks related to personal care and leisure activities. Our study provides insights into older adults' attitudes and preferences for robot assistance with everyday living tasks in the home which may inform the design of robots that will be more likely accepted by older adults.\"",
        "Document: \"Finding and navigating to household objects with UHF RFID tags by optimizing RF signal strength. We address the challenge of finding and navigating to an object with an attached ultra-high frequency radio-frequency identification (UHF RFID) tag. With current off-the-shelf technology, one can affix inexpensive self-adhesive UHF RFID tags to hundreds of objects, thereby enabling a robot to sense the RF signal strength it receives from each uniquely identified object. The received signal strength indicator (RSSI) associated with a tagged object varies widely and depends on many factors, including the object's pose, material properties and surroundings. This complexity creates challenges for methods that attempt to explicitly estimate the object's pose. We present an alternative approach that formulates finding and navigating to a tagged object as an optimization problem where the robot must find a pose of a directional antenna that maximizes the RSSI associated with the target tag. We then present three autonomous robot behaviors that together perform this optimization by combining global and local search. The first behavior uses sparse sampling of RSSI across the entire environment to move the robot to a location near the tag; the second samples RSSI over orientation to point the robot toward the tag; and the third samples RSSI from two antennas pointing in different directions to enable the robot to approach the tag. We justify our formulation using the radar equation and associated literature. We also demonstrate that it has good performance in practice via tests with a PR2 robot from Willow Garage in a house with a variety of tagged household objects.\"",
        "1 is \"Affective Personalization of a Social Robot Tutor for Children's Second Language Skills.\", 2 is \"Galileo: Perceiving Physical Object Properties by Integrating a Physics Engine with Deep Learning\"",
        "Given above information, for an author who has written the paper with the title \"Alternative Essences Of Intelligence\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003279": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Validity of the Independence Assumption for the Separation of Instantaneous and Convolutive Mixtures of Speech and Music Sources':",
        "Document: \"Blind Separation of Noisy Mixtures of Non-stationary Sources Using Spectral Decorrelation. In this paper, we propose a new approach for blind separation of noisy, over-determined, linear instantaneous mixtures of non-stationary sources. This approach is an extension of a new method based on spectral decorrelation that we have recently proposed. Contrary to classical second-order blind source separation (BSS) algorithms, our proposed approach only requires the non-stationary sources and the stationary noise signals to be instantaneously mutually uncorrelated. Thanks to this assumption, it works even if the noise signals are auto-correlated. The simulation results show the much better performance of our approach in comparison to some classical BSS algorithms.\"",
        "Document: \"Self-adaptive separation of convolutively mixed signals with a recursive structure. Part I: stability analysis and optimization of asymptotic behavior. Dans cet article, nous traitons le probl\u00e8me de la s\u00e9paration auto-adaptative de sources pour des m\u00e9langes convolutifs. L\u2019approche propos\u00e9e utilise une structure r\u00e9currente adapt\u00e9e par une r\u00e8gle g\u00e9n\u00e9rique bas\u00e9e sur des fonctions s\u00e9paratrices arbitraires. On effectue d\u2019abord une analyse de la stabilit\u00e9 de cet algorithme. Elle s\u2019applique notamment \u00e0 plusieurs r\u00e8gles classiques pour des m\u00e9langes instantan\u00e9s ou convolutifs qui n\u2019ont \u00e9t\u00e9 que partiellement analys\u00e9es dans la litt\u00e9rature. L\u2019expression de la variance de l\u2019erreur asymptotique est ensuite d\u00e9termin\u00e9e dans le cas de m\u00e9langes strictement causaux. Ceci permet de calculer les fonctions s\u00e9paratrices optimales au sens de la minimisation de la variance de l\u2019erreur. On montre qu\u2019elles ne d\u00e9pendent que de la densit\u00e9 de probabilit\u00e9 des sources. Pour r\u00e9aliser cette minimisation d\u2019erreur, deux proc\u00e9dures de normalisation qui am\u00e9liorent les propri\u00e9t\u00e9s de l\u2019algorithme sont propos\u00e9es. Leurs conditions de stabilit\u00e9 et leurs performances asymptotiques sont analys\u00e9es.\"",
        "Document: \"Differential source separation: Concept and application to a criterion based on differential normalized kurtosis. This paper concerns the underdetermined case of the blind source separation problem, i.e. the situation when the number of observed mixed signals is lower than the number of sources. The general concept that we propose in this case consists of a differential source separation approach, which uses optimization criteria based on differential parameters, so as to make some sources invisible in these criteria and to perform an exact separation of the other sources only. We illustrate this partial source separation concept on a new criterion based on the \"differential normalized kurtosis\" that we introduce to this end. We then validate the performance of this criterion by means of experimental tests.\"",
        "Document: \"A Blind Source Separation Method Based on Output Nonlinear Correlation for Bilinear Mixtures. In this paper, a blind source separation method for bilinear mixtures of two source signals is presented, that relies on nonlinear correlation between separating system outputs. An estimate of each source is created by linearly combining observed mixtures and maximizing a cost function based on the correlation between the element-wise product of the estimated sources and the corresponding quadratic term. A proof of the method separability, i.e. of the uniqueness of the solution to the cost function maximization problem, is also given. The algorithm used in this work is also presented. Its effectiveness is demonstrated through tests with artificial mixtures created with real Earth observation spectra. The proposed method is shown to yield much better performance than a state-of-the-art method.\"",
        "Document: \"Hypersharpening by Joint-Criterion Nonnegative Matrix Factorization. Hypersharpening aims at combining an observable low-spatial resolution hyperspectral image with a high-spatial resolution remote sensing image, in particular a multispectral one, to generate an unobservable image with the high spectral resolution of the former and the high spatial resolution of the latter. In this paper, two such new fusion methods are proposed. These methods, related to linear sp...\"",
        "1 is \"Differential of the Mutual Information\", 2 is \"Gammatone sub-band magnitude-domain dereverberation for ASR\"",
        "Given above information, for an author who has written the paper with the title \"Validity of the Independence Assumption for the Separation of Instantaneous and Convolutive Mixtures of Speech and Music Sources\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003343": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Transmit delay structure design for blind channel estimation over multipath channels':",
        "Document: \"On Efficient Packet-Switched Wireless Networking: A Markovian Approach to Trans-Layer Design and Optimization of ROHC. In packet-switched radio links, the little known Robust Header Compression (ROHC) has become an integral part of many wireless and particularly cellular communication networks. To strengthen existing schemes, this paper aims to improve ROHC performance in terms of payload efficiency for U-mode compression under poor wireless channel conditions. We first consider the parameter optimization of curre...\"",
        "Document: \"A Resource Allocation Scheme for Heterogeneous Networks Using Dynamic Programming Approach. In this paper, we propose a resource allocation scheme for interference management in heterogeneous networks. We particularly consider downlink interference from a Home eNB (HeNB) to macrocell user equipments (MUEs) in the coverage of the traditional macrocell basestation (MBS). By overhearing uplink feedback information from MUEs and Home User Equipments (HUEs) together with downlink control information (DCI) from the MBS, the HeNB formulates a dynamic programming problem with the objective of maximizing the total reward over an infinite horizon. By exploiting the feedback information as well as the DCI at the HeNB, we show that the solution of the dynamic programming problem follows a greedy policy at the HeNB, which simplifies the solution of the infinite horizon dynamic programming problem. We also examine the effect of uncertainty in the feedback information on the performance of our proposed scheme.\"",
        "Document: \"Offset and Power Optimization for DCO-OFDM in Visible Light Communication Systems. Direct-current-biased optical orthogonal frequency division multiplexing (DCO-OFDM) is used in visible light communications (VLC) for high rate transmission. Due to the unipolarity of optical signals, a direct current (DC) offset along with proper clipping is adopted in DCO-OFDM. A high DC-offset causes a waste of power and a low DC-offset causes a severe clipping distortion. Thus, the DC-offset and information-carrying power shall be jointly optimized to balance clipping distortion and power consumption. We consider the offset and power optimization problem under the optical and/or electrical power constraints over flat and dispersive channels. In the flat case, we analytically characterize the optimal solution, based on which several important insights are provided. We also investigate the relation and impact of the two constraints, and propose efficient algorithms to obtain the optimal solution. In the dispersive case, we identify the scope of the optimal solution, and provide efficient algorithms to jointly optimize the offset and power. The optimality and superiority of the proposed methods are validated by numerical results.\"",
        "Document: \"Efficient Algorithms for Resource Allocation in Heterogeneous OFDMA Networks. We consider a heterogeneous multiuser OFDMA wireless network serving both QoS-constrained high-priority users and best-effort users. We propose several efficient algorithms for subcarrier/cluster and power allocation. Our strategy is to maximize the total network utility of the best-effort user while satisfying the high-priority users. We define the best-effort user utility function in two different ways and propose two different novel, efficient and optimal cluster allocation algorithms. We also propose an efficient and optimal power allocation algorithm based on the modification of existing algorithms.\"",
        "Document: \"On Number of Almost Blank Subframes in Heterogeneous Cellular Networks. In heterogeneous cellular scenarios with macrocells, femtocells or picocells users may suffer from significant co-channel cross-tier interference. To manage this interference 3GPP introduced almost blank subframe (ABSF), a subframe in which the interferer tier is not allowed to transmit data. Vulnerable users thus get a chance to be scheduled in ABSFs with reduced cross-tier interference. We analyze downlink scenarios using stochastic geometry and formulate a condition for the required number of ABSFs based on base station placement statistics and user throughput requirement. The result is a semi-analytical formula that serves as a good initial estimate and offers an easy way to analyze impact of network parameters. We show that while in macro/femto scenario the residue ABSF interference can be well managed, in macro/pico scenario it affects the number of required ABSFs strongly. The effect of ABSFs is subsequently demonstrated via user throughput simulations. Especially in the macro/pico scenario, we find that using ABSFs is advantageous for the system since victim users no longer suffer from poor performance for the price of relatively small drop in higher throughput percentiles.\"",
        "1 is \"Channel Estimation Of Long-Code Cdma Systems Utilizing Transmission Induced Cyclostationarity\", 2 is \"Iterative Joint Detection, Decoding, and Channel Estimation in Turbo-Coded MIMO-OFDM\"",
        "Given above information, for an author who has written the paper with the title \"Transmit delay structure design for blind channel estimation over multipath channels\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003410": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Asymptotic behavior of the Weber location problem on the plane':",
        "Document: \"A distribution map for the one-median location problem on a network. In this paper we consider the Weber (one-median) location problem on a network. The weights at the nodes are drawn from a multivariate normal distribution. We find the probability that the optimal location is at each of the nodes. This set of probabilities is the distribution map. The methodology is illustrated on two networks of 20 nodes each.\"",
        "Document: \"An asynchronous algorithm for scattering information between the active nodes of a multicomputer system. In this paper we present an asynchronous algorithm for scattering information between the active nodes of a multicomputer system, having a large number of independent computers and workstations that are interconnected by a local area communication network. This algorithm is useful when it is desired to reduce the number of messages and the time delay necessary to transmit information from any node to all the active nodes of the system. The algorithm that we develop is based on one-way messages which are sent by each node, every unit of time, to a randomly selected node. The main advantage of this routing is that it overcomes inactive or faulty machines. We show that for an N node multicomputer in which n nodes are active, it is possible to scatter information to all the active nodes in approximately (1.693 + 1.414(1 \u2212 n N )) log 2 n steps.\"",
        "Document: \"Gossip-Based Distributed Algorithms For Estimating The Average Load Of Scalable Clusters And Grids. We present gossip-based distributed algorithms by which each node of a cluster or a grid can find an estimate of the global average load of the system. Knowledge of this estimate can improve the performance by allowing each node to make better scheduling decisions. Our algorithms are based on asynchronous exchanges (gossip) of load messages between either a random or a fixed pairs of nodes. We show that based on these messages, each node can find an estimate of the average load of all the nodes. We present the algorithms and prove their convergence.\"",
        "Document: \"Constructing a DC decomposition for ordered median problems. In this paper we show how to express ordered median problems as a difference between two convex functions (DC). Such an expression can be exploited in solving ordered median problems by using the special methodology available for DC optimization. The approach is demonstrated for solving ordered one median problems in the plane. Computational experiments demonstrated the effectiveness of the approach.\"",
        "Document: \"Solving planar location problems by global optimization. In this paper we review global optimization techniques and their application to location problems. The following techniques are reviewed: Big Square Small Square, Big Cube Small Cube, Big Triangle Small Triangle, Big Segment Small Segment, DC Optimization and the Ordered Median formulation. These techniques are described, and examples for their implementation for various location problems are given.\"",
        "1 is \"The big cube small cube solution method for multidimensional facility location problems\", 2 is \"Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems\"",
        "Given above information, for an author who has written the paper with the title \"Asymptotic behavior of the Weber location problem on the plane\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003483": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Real-Time And Distributed Av Content Analysis System For Consumer Electronics Networks':",
        "Document: \"A multi-resolution particle filter tracking in a multi-camera environment. This paper presents a novel tracking method with the multi-resolution technique and a Kolmogrov-Smirnov test for model update to track a non-rigid target in an uncalibrated multi-camera environment. It is based on particle filter method using color appearance model. Compared to the related work, our method improves the tracking performance by proposing: i) a multi-resolution technique to rapidly locate the estimate of the target state and refine it gradually, ii) the Kolmogrov-Smirnov test to evaluate the reliability of the estimate so as to take the decision on further updating/ reinitialization of the estimate, as well as iii) an interaction of cameras approach to reinitialize the estimate by information detected in other cameras in case of tracking failures. After being tested in a multi-camera environment for one person tracking, our system is shown to give a better tracking result in comparison with mono-camera tracking, especially when occlusions occur.\"",
        "Document: \"Detection of human faces in color image sequences with arbitrary motions for very low bit-rate videophone coding. The problem of human face detection is a focus of interest in image analysis, image databases and video coding. A new multi-resolution method using color and motion information and shape model is developed to detect human faces in videophone QCIF sequences for efficient encoding. The method is based on color segmentation and multiresolution propagation of a geometrical model. A new measure of motion activity is proposed to validate the choice of candidates. (C) 1997 Elsevier Science B.V.\"",
        "Document: \"Semantic Event Fusion of Different Visual Modality Concepts for Activity Recognition. Combining multimodal concept streams from heterogeneous sensors is a problem superficially explored for activity recognition. Most studies explore simple sensors in nearly perfect conditions, where temporal synchronization is guaranteed. Sophisticated fusion schemes adopt problem-specific graphical representations of events that are generally deeply linked with their training data and focused on a...\"",
        "Document: \"Alzheimer's disease diagnosis on structural MR images using circular harmonic functions descriptors on hippocampus and posterior cingulate cortex. Recently, several pattern recognition methods have been proposed to automatically discriminate between patients with and without Alzheimer's disease using different imaging modalities: sMRI, fMRI, PET and SPECT. Classical approaches in visual information retrieval have been successfully used for analysis of structural MRI brain images. In this paper, we use the visual indexing framework and pattern recognition analysis based on structural MRI data to discriminate three classes of subjects: normal controls (NC), mild cognitive impairment (MCI) and Alzheimer's disease (AD). The approach uses the circular harmonic functions (CHFs) to extract local features from the most involved areas in the disease: hippocampus and posterior cingulate cortex (PCC) in each slice in all three brain projections. The features are quantized using the Bag-of-Visual-Words approach to build one signature by brain (subject). This yields a transformation of a full 3D image of brain ROIs into a 1D signature, a histogram of quantized features. To reduce the dimensionality of the signature, we use the PCA technique. Support vector machines classifiers are then applied to classify groups. The experiments were conducted on a subset of ADNI dataset and applied to the \"Bordeaux-3City\" dataset The results showed that our approach achieves respectively for ADNI dataset and \"Bordeaux-3City\" dataset; for AD vs NC classification, an accuracy of 83.77% and 78%, a specificity of 88.2% and 80.4% and a sensitivity of 79.09% and 74.7%. For NC vs MCI classification we achieved for the ADNI datasets an accuracy of 69.45%, a specificity of 74.8% and a sensitivity of 62.52%. For the most challenging classification task (AD vs MCI), we reached an accuracy of 62.07%, a specificity of 75.15% and a sensitivity of 49.02%. The use of PCC visual features description improves classification results by more than 5% compared to the use of hippocampus features only. Our approach is automatic, less time-consuming and does not require the intervention of the clinician during the disease diagnosis. (C) 2015 Elsevier Ltd. All rights reserved.\"",
        "Document: \"Multi-layer local graph words for object recognition. In this paper, we propose a new multi-layer structural approach for the task of object based image retrieval. In our work we tackle the problem of structural organization of local features. The structural features we propose are nested multi-layered local graphs built upon sets of SURF feature points with Delaunay triangulation. A Bag-of-Visual-Words (BoVW) framework is applied on these graphs, giving birth to a Bag-of-Graph-Words representation. The multi-layer nature of the descriptors consists in scaling from trivial Delaunay graphs - isolated feature points - by increasing the number of nodes layer by layer up to graphs with maximal number of nodes. For each layer of graphs its own visual dictionary is built. The experiments conducted on the SIVAL and Caltech-101 data sets reveal that the graph features at different layers exhibit complementary performances on the same content. The combination of all layers, yields significant improvement of the object recognition performance.\"",
        "1 is \"Split-screen dynamically accelerated video summaries\", 2 is \"Robust estimation of correlation with applications to computer vision\"",
        "Given above information, for an author who has written the paper with the title \"Real-Time And Distributed Av Content Analysis System For Consumer Electronics Networks\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003494": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Boosting local binary pattern (LBP)-Based face recognition':",
        "Document: \"Visual gesture recognition for human robot interaction using dynamic movement primitives. In this paper a method to address the efficiency and robustness of dynamic hand gesture recognition for human robot interaction is proposed. By using on-board monocular camera and specialized gesture detection algorithms, the humanoid robot is able to detect gestures fast. To model the dynamics of gestures, the dynamic movement primitives (DMP) model is employed, which well characterizes both spatial and temporal evolutions of gestures. The invariance properties of the DMP model against different spatiotemporal scales also offer expected robustness to handle the variances in gestures. To cope with the diversity and noise of gestures, an efficient adaptive DMP learning method is further proposed. Since the learnt weights of the DMP compactly represent the original gestures, they serve as ideal feature vectors for building a classifier to recognize new gestures. To evaluate the proposed method, a nine-class human gestures recognition task on a real humanoid robot is performed and 98.06% accuracy is obtained. Experimental results demonstrate the effectiveness of our method.\"",
        "Document: \"A morphology-based Chinese word segmentation method. This paper proposes a novel method of Chinese word segmentation utilizing morphology information. The method introduces morphology into statistical model to capture structural relationship within word. It improves the conventional Conditional Random Fields (CRFs) models on the ability of representing the structure information. Firstly, a word-segmented Chinese corpus is annotated with morphology tags by a semi-automatic method. The resulting structure-related tags are integrated into the CRFs model. Secondly, a joint CRFs model is trained, which generates both morphology tags and word boundaries. Experiments are carried out on several SIGHAN Bakeoff corpus and show that the morphology information can improve the performance of Chinese word segmentation significantly, especially for the segmentation of out-of-vocabulary words.\"",
        "Document: \"Learning basic unit movements for humanoid arm motion control. Manipulation skill is important for humanoid robots to live and work with humans, and arm motion control is essential for the manipulation accomplishment. In our research, we hope our robot execute a manipulation task by combining basic unit movements (BUMs), thus making the manipulation easier and more robust. So in this paper, we firstly define BUMs which actually can be regarded as basic components of any arm motion. Then we propose a learning approach for the robot to execute BUMs, which means knowing the current state, the robot learns how to move his arm to accomplish the given BUM. Considering the complexity and inaccuracy problems in solving the inverse kinematics, the proposed approach is basically building an internal inverse model and the robot directly learns in the motor space without any inverse kinematics. Taking advantages of the powerful capacity of Deep Neural Networks (DNN) in extracting inherent features, the auto-encoder is employed to formalize our model. Experimental results on MATLAB simulation as well as PKU-HR5II humanoid robot reveal the effectiveness of the proposed approach. The robot can successfully execute almost all the BUMs in the whole workspace of his right arm with the accuracy of 98.49%.\"",
        "Document: \"Maximum entropy based tone modeling for mandarin speech recognition. To explore the potential of prosody for Mandarin speech recognition, this paper addresses the tone modeling problem and its integration issue. This study adopts the maximum entropy approach to capture both acoustic and lexical characteristics of tones due to its flexibility in handling multiple interacting features. Moreover, considering the phoneme factor, besides a tone model, a phoneme dependent model is also constructed. With regard to the model integration, the presented models are integrated into the recognizer under the one-pass decoding framework, where they are used to prune the active word-final states during beam search. Experimental results on the HUB-4 evaluation material reveal the effectiveness of the presented models. They significantly improve the performance of speech recognition with 7.6% and 11.1% relative reduction of character error rate.\"",
        "Document: \"Discriminative GMM-HMM acoustic model selection using two-level bayesian ying-yang harmony learning. This paper proposes a two-level Bayesian Ying-Yang (BYY) harmony learning based acoustic model discriminative training method. In this method, a rival penalized competitive learning (RPCL) simplified BYY harmony learning based discriminative training is conducted at the HMM state level to optimizing the state boundaries, while a BYY based model selection is conducted at the Gaussian mixture components level to determine the Gaussian mixture components within the same HMM state. Two levels of learning work coordinately and have good convergence. Experiments show that the trained model is more discriminative with better recognition performance, and also more compact with smaller number of Gaussian components.\"",
        "1 is \"A Background Layer Model for Object Tracking Through Occlusion\", 2 is \"Using prosodic features in language models for meetings\"",
        "Given above information, for an author who has written the paper with the title \"Boosting local binary pattern (LBP)-Based face recognition\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003595": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'An application-specific protocol architecture for wireless microsensor networks':",
        "Document: \"Sensor selection cost function to increase network lifetime with QoS support. Single-hop centralized wireless sensor networks are widely used for applications ranging from security and surveillance to medical monitoring. Often the goal of these networks is to provide satisfactory quality of service (QoS) to the application under different system states, but it is difficult to determine how the appropriate sensor sets should be selected over time, given the knowledge of all possible sensor sets to support the application QoS requirements. To address this problem, in this paper we propose a novel cost function called SUI (Sensor Usage Index) that is based on a sensor's relative ideal lifetime (SRIL) and can be used to select sensor sets so as to meet application QoS requirements for extended periods of time. Simulation results show that utilizing the proposed cost function for sensor set selection enables the network to meet application QoS requirements for longer than using other standard sensor selection schemes. In fact, our scheme approaches the optimal network lifetime, which can be found using global knowledge of the sensors and the system dynamics.\"",
        "Document: \"A Survey Of Visual Sensor Networks. Visual sensor networks have emerged as an important class of sensor-based distributed intelligent systems, with unique performance, complexity, and quality of service challenges. Consisting of a large number of low-power camera nodes, visual sensor networks support a great number of novel vision-based applications. The camera nodes provide information from a monitored site, performing distributed and collaborative processing of their collected data. Using multiple cameras in the network provides different views of the scene, which enhances the reliability of the captured events. However, the large amount of image data produced by the cameras combined with the network's resource constraints require exploring new means for data processing, communication, and sensormanagement. Meeting these challenges of visual sensor networks requires interdisciplinary approaches, utilizing vision processing, communications and networking, and embedded processing. In this paper, we provide an overview of the current state-of-the-art in the field of visual sensor networks, by exploring several relevant research directions. Our goal is to provide a better understanding of current research problems in the different research fields of visual sensor networks, and to show how these different research fields should interact to solve the many challenges of visual sensor networks. Copyright (C) 2009 S. Soro andW. Heinzelman. This is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.\"",
        "Document: \"Schedule Adaptation of Low-Power-Listening Protocols for Wireless Sensor Networks. Many recent advances in MAC protocols for wireless sensor networks have been proposed to reduce idle listening, an energy wasteful state of the radio. Low-Power-Listening (LPL) protocols transmit packets for t_i {\\rm s} (the \u201cinterlistening interval\u201d), thereby, allowing nodes to sleep for long periods of time between channel probes. The interlistening interval as well as the particular type of LPL protocol should be well matched to the network conditions. In this paper, we propose network-aware adaptation of the specific succession of repeated packets over the t_i interval (the \u201cMAC schedule\u201d), which yields significant energy savings. Moreover, some LPL protocols interrupt communication between the sender and the receiver after the data packet has been successfully received. We propose a new and simple adaptation of the \u201ctransmit/receive schedule\u201d to synchronize nodes on a slowly changing path so that energy consumption and delay are further reduced, at no cost of overhead in most cases. Our results show that using network-aware adaptation of the MAC schedule provides up to 30 percent increase in lifetime for different traffic scenarios. Additional adaptation of the transmit/receive schedule to automatically synchronize the nodes can reduce packet delivery delays by up to 50 percent, providing an additional decrease in energy consumption of 18 percent.\"",
        "Document: \"Motion Sensor and Camera Placement Design for In-Home Wireless Video Monitoring Systems. Motion sensors and cameras can be used together to build in-home video monitoring systems, where the cameras are only activated when the motion sensors detect motion. This saves both bandwidth (reduced video storage and/or transmission) as well as energy (if the cameras are battery-operated). However, motion sensors and cameras have different fields of view (FoV), and thus it is not clear that attaching motion sensors to cameras provides the most efficient system. Therefore, it is necessary to evaluate the performance of systems where the motion sensors are attached to the cameras (attached scenario) as well as where the motion sensors are detached from the cameras (detached scenario). We consider the motion sensor and camera placement problem under these two scenarios, with an additional hybrid scenario, and we formulate optimization problems to achieve the minimum energy consumption, longest network lifetime, or lowest cost. Simulation results show the tradeoffs between different objectives for all three scenarios.\"",
        "Document: \"Improving QoS under lossy channels through adaptive redundancy. Quality of service (QoS) of a network-wide broadcast (NWB) protocol is one of the most important performance metrics, especially in mobile ad hoc networks, where channel conditions and network topology change frequently. We propose a mesh networking inspired approach to overcome the performance degradation caused by lossy channels. We show that our adaptive approach, whereby the amount of redundancy is adjusted to the current link conditions, can achieve good performance while simultaneously reducing unnecessary energy dissipation.\"",
        "1 is \"Sensitivity of wireless network simulations to a two-state Markov model channel approximation\", 2 is \"Ultra-wideband communications: an idea whose time has come\"",
        "Given above information, for an author who has written the paper with the title \"An application-specific protocol architecture for wireless microsensor networks\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003631": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Semi-supervised clustering of graph objects: a subgraph mining approach':",
        "Document: \"Mining protein family specific residue packing patterns from protein structure graphs. Finding recurring residue packing patterns, or spatial motifs, that characterize protein structural families is an important problem in bioinformatics. We apply a novel frequent subgraph mining algorithm to three graph representations of protein three-dimensional (3D) structure. In each protein graph, a vertex represents an amino acid. Vertex-residues are connected by edges using three approaches: first, based on simple distance threshold between contact residues; second using the Delaunay tessellation from computational geometry, and third using the recently developed almost-Delaunay tessellation approach.Applying a frequent subgraph mining algorithm to a set of graphs representing a protein family from the Structural Classification of Proteins (SCOP) database, we typically identify several hundred common subgraphs equivalent to common packing motifs found in the majority of proteins in the family. We also use the counts of motifs extracted from proteins in two different SCOP families as input variables in a binary classification experiment. The resulting models are capable of predicting the protein family association with the accuracy exceeding 90 percent. Our results indicate that graphs based on both almost-Delaunay and Delaunay tessellations are sparser than the contact distance graphs; yet they are robust and efficient for mining protein spatial motif.\"",
        "Document: \"Biomedical text categorization with concept graph representations using a controlled vocabulary. Recent work using graph representations for text categorization has shown promising performance over conventional bag-of-words representation of text documents. In this paper we investigate a graph representation of texts for the task of text categorization. In our representation we identify high level concepts extracted from a database of controlled biomedical terms and build a rich graph structure that contains important concepts and relationships. This procedure ensures that graphs are described with a regular vocabulary, leading to increased ease of comparison. We then classify document graphs by applying a set-based graph kernel that is intuitively sensible and able to deal with the disconnectedness of the constructed concept graphs. We compare this approach to standard approaches using non-graph, text-based features. We also do a comparison amongst different kernels that can be used to see which performs better.\"",
        "Document: \"Non-stationary bayesian networks based on perfect simulation. Non-stationary Dynamic Bayesian Networks (Non-stationary DBNs) are widely used to model the temporal changes of directed dependency structures from multivariate time series data. However, the existing change-points based non-stationary DBNs methods have several drawbacks including excessive computational cost, and low convergence speed. In this paper we proposed a novel non-stationary DBNs method. Our method is based on the perfect simulation model. We applied this approach for network structure inference from synthetic data and biological microarray gene expression data and compared it with other two state-of-the-art non-stationary DBNs methods. The experimental results demonstrated that our method outperformed two other state-of-the-art methods in both computational cost and structure prediction accuracy. The further sensitivity analysis showed that once converged our model is robust to large parameter ranges, which reduces the uncertainty of the model behavior.\"",
        "Document: \"Graph Database Indexing Using Structured Graph Decomposition. We introduce a novel method of indexing graph databases in order to facilitate subgraph isomorphism and similarity queries. The index is comprised of two major data structures. The primary structure is a directed acyclic graph which contains a node for each of the unique, induced subgraphs of the database graphs. The secondary structure is a hash table which cross-indexes each subgraph for fast isomorphic lookup. In order to create a hash key independent of isomorphism, we utilize a code-based canonical representation of adjacency matrices, which we have further refined to improve computation speed. We validate the concept by demonstrating its effectiveness in answering queries for two practical datasets. Our experiments show that for subgraph isomorphism queries, our method outperforms existing methods by more than an order of magnitude.\"",
        "Document: \"When Additional Views are Not Free: Active View Completion for Multi-view Semi-Supervised Learning. Multi-view semi-supervised learning methods exploit the combination of multiple data views and unlabeled data in order to learn better predictive functions with limited labeled data. However, their applicability is limited since typically one data view is readily available but additional views may be costly to obtain. Here we explore a new research direction at the intersection of active learning and multi-view semi-supervised learning: active view completion. The goal is to actively select which instances to obtain missing view data for, for the purposes of enabling effective multi-view semi-supervised learning. Recent work has shown an active selection strategy for view completion can be more effective than a random one. Here a better understanding of active approaches is sought, and it is demonstrated that the effectiveness of an active selection strategy over a random one can depend on the relationship between views. We present new algorithms, theoretical results, and experimental study to elucidate the conditions for and extent to which active approaches can be beneficial in this scenario.\"",
        "1 is \"Fast and Simple Relational Processing of Uncertain Data\", 2 is \"PDTD: a web-accessible protein database for drug target identification.\"",
        "Given above information, for an author who has written the paper with the title \"Semi-supervised clustering of graph objects: a subgraph mining approach\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003697": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Animating organizational processes Insight eases change':",
        "Document: \"Participative Modelling for Understanding: Facilitating Organizational Change with GSS. The involvement of stakeholder groups in organizational change efforts seems a fertile application area for GSS. In an action research study at the Amsterdam Municipal Police Force, we investigated the use of electronic meetings software and joint modeling facilities for the construction of models of the police force's current situation. The findings of this study show the potential of GSS to facilitate organizational change both in terms of efficiency and in quality of the resulting models. A number of issues for future research are identified.\"",
        "Document: \"The collaboration engineering approach for designing collaboration processes. Collaboration Engineering is an approach to design and deploy collaboration processes that can be executed by practitioners for high value recurring tasks. A collaboration engineer designs collaboration processes and transfers them to practitioners in an organization. Through the recurring nature of the task, combined with lower investment in training, the approach is more likely to be successful in organizations because it is easier to adopt and sustain collaboration support in this way. In order to be successful, collaboration engineers need to develop collaboration process designs that have many more functions and requirements than traditional process agenda's of facilitators. This paper describes a step-by-step approach for the design of such collaboration processes. The approach was evaluated in a number of iterations. The evaluation results provide support for the usefulness of the approach.\"",
        "Document: \"Collaboration Engineering For Incident Response Planning: Process Development and Validation. Many organizations have plans for incident response strategies. Despite Incident Response Planning (IRP) being an essential ingredient in conjuring security planning procedures in organizations, extensive literature reviews have revealed that there are no collaborative processes in place for such a crucial activity. This study proposes a design for a facilitated incident response planning process using technology such as GSS. Three sessions were conducted and an analysis of the sessions revealed that the facilitated IRP process design held up strongly in terms of efficiency, goal attainment, and session participant satisfaction. Future research implications entail devising an all-encompassing integrative general approach that would be applicable to any form of corporate security development planning process.\"",
        "Document: \"Collaboration in Virtual Worlds: The Role of the Facilitator. Virtual worlds (VWs) are becoming a popular medium for meetings and collaborative problem solving efforts. However, complex VW communication tools and challenges in managing online social interactions are likely to complicate VW collaboration efforts. The purpose of our study therefore was to investigate the role of the facilitator when collaboration is conducted in a virtual environment. We developed a questionnaire based on major issues in real world collaboration and interviewed 14 subject-matter experts. Participants were asked to identify what key differences facilitators perceive between virtual and real world collaboration. In response, participants provided many insights, such as the new interpersonal management challenges that arise from the absence of face-to-face communication. Participants also warned of the challenges associated with the introduction of more technology to the collaboration process. Further, they identified credibility and trust issues that arise due to facilitators' avatar manipulation skills and avatar appearance. Suggestions for avoiding pitfalls and optimizing collaboration are provided.\"",
        "Document: \"Coding for Unique Ideas and Ambiguity: Measuring the Effects of a Convergence Intervention on the Artifact of an Ideation Activity. Groups can generate large numbers of ideas as part of a decision-making process. These ideas may become too numerous for the group to process effectively. Ideas may also need to be clarified to facilitate this processing. Convergence patterns of group behavior help to reduce the number of ideas to a manageable set and at the same time clarify these ideas. Research aimed at understanding convergence is in the beginning stages. Researchers are developing methods of quantifying convergence. In this paper, we present a method for quantifying the reduction and clarification that has occurred through convergence using an assessment of pre- and postconvergence artifacts. The method characterizes artifacts obtained in the field when facilitators led groups of managers through convergence activities to solve an actual business problem. The workshops utilized the FastFocus thinkLet, as part of a larger group process. We present the results of this measurement or coding method.\"",
        "1 is \"Use of business process simulation: A survey of practitioners\", 2 is \"Modeling and Simulation for Mission Operations Work System Design\"",
        "Given above information, for an author who has written the paper with the title \"Animating organizational processes Insight eases change\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003709": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'MuVi: Multiview Video Aware Transmission Over MIMO Wireless Systems.':",
        "Document: \"A Novel Peer-to-Peer Intrusion Detection System. MANETs are composed of mobile nodes without any infrastructure and nodes cooperate to set up routes for network communications. Because of these characters, MANETs operate in open medium, so they are particularly vulnerable to intrusions. In this paper, we present an efficient intrusion detection system called MAPIDS (Mobile Agent-based Peer-to-peer Intrusion Detection System). On detecting a suspicious activity, MAPIDS initiates a voting approach to make a collective decision and take further action. In contrast to other intrusion detection system based on collective decision, MAPIDS saves more bandwidth and energy and it is immune to sparse nodes problem.\"",
        "Document: \"Decluster: a complex network model-based data center network topology. To cope with increasing demands of computation and storage, data centers should follow the pace of the rapid growth of data size. It is necessary for a data center with a scalability property of which each expansion of a data center network is done with a few modifications. Besides the scalability property, we also need a data center to have good performance, such as high throughput. For these purposes, we propose Decluster, a complex network model-based data center network topology. The complex network model of Decluster is derived from a random network. Such a model just satisfies the requirement of scalability. Decluster employs a complex network model to achieve high throughput via reducing the variance of local clustering coefficients. We have carried out extensive simulations to demonstrate that Decluster enjoys good performance while keeping scalability.\"",
        "Document: \"PCube: Improving Power Efficiency in Data Center Networks. To alleviate the growing concern of energy waste in networked devices, we present PCube, a server-centric data center structure that conserves energy by varying bandwidth availability based on traffic demand. PCube not only supports a low-power mode for existing data centers offering full bisection bandwidth without hardware modification or re-wiring, but also provides an alternative for new data centers at a lower construction cost. The bandwidth demand could be dynamic and scalable in the optimal way when considering conserving the energy waste. To further reduce the cost, we take advantage of traffic locality, and propose Hybrid PCube, in order to offer different bandwidth to different servers. We use samples of traffic from a real-world production data center with full bisection bandwidth across thousands of servers, and evaluate our proposed algorithm using these collected samples. Our experimental results have shown convincing evidence that the proposed algorithm is able to substantially reduce energy costs incurred by networked devices in data centers.\"",
        "Document: \"A Grid Middleware DISQ for Data Integration. The Grid offers new opportunities and raises new challenges for data integration. The Grid communities have paid great attention to data integration middle ware in grid environment. But few of those projects actually establish semantic connections among heterogeneous data sources[1]. Recently, several resolutions of data integration in grid environment have been proposed. However most of them put their emphasis in a particular data integration domain such as medical data integration [2] than a common grid data integration middle ware. So in this paper we provide a grid middle ware DISQ for semantic data integration based on a data model which is extended for data inconsistency solution to improve the quality of data integration by introducing data source quality criteria into the data model. It provides application interfaces to user and to semantically integrate data sources based on our data model, it encapsulates services provided by OGSA-DAI which only shields data sources' physical access heterogeneity.\"",
        "Document: \"Performance Analysis of Multimedia Retrieval Workloads Running on Multicores. Multimedia data has become a major data type in the Big Data era. The explosive volume of such data and the increasing real-time requirement to retrieve useful information from it have put significant pressure in processing such data in a timely fashion. However, while prior efforts have done in-depth analysis on architectural characteristics of traditional multimedia processing and text-based retrieval algorithms, there has been no systematic study towards the emerging multimedia retrieval applications. This may impede the architecture design and system evaluation of these applications. In this paper, we make the first attempt to construct a multimedia retrieval benchmark suite (MMRBench for short) that can be used to evaluate architectures and system designs for multimedia retrieval applications. MMRBench covers modern multimedia retrieval algorithms with different versions (sequential, parallel and distributed). MMRBench also provides a series of flexible interfaces as well as certain automation tools. With such a flexible design, the algorithms in MMRBench can be used both in individual kernel-level evaluation and in integration to form a complete multimedia data retrieval infrastructure for full system evaluation. Furthermore, we use performance counters to analyze a set of architecture characteristics of multimedia retrieval algorithms in MMRBench, including the characteristics of core level, chip level and inter-chip level. The study shows that micro-architecture design in current processor is inefficient (both in performance and power) for these multimedia retrieval workloads, especially in core resources and memory systems. We then derive some insights into the architecture design and system evaluation for such multimedia retrieval algorithms.\"",
        "1 is \"Clustering Spatial Data when Facing Physical Constraints\", 2 is \"Max-Contribution: On Optimal Resource Allocation in Delay Tolerant Networks\"",
        "Given above information, for an author who has written the paper with the title \"MuVi: Multiview Video Aware Transmission Over MIMO Wireless Systems.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003792": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Better Effectiveness Metrics for SERPs, Cards, and Rankings.':",
        "Document: \"Evaluating search systems using result page context. We introduce a method for evaluating the relevance of all visible components of a Web search results page, in the context of that results page. Contrary to Cranfield-style evaluation methods, our approach recognizes that a user's initial search interaction is with the result page produced by a search system, not the landing pages linked from it. Our key contribution is that the method allows us to investigate aspects of component relevance that are difficult or impossible to judge in isolation. Such contextual aspects include component-level information redundancy and cross-component coherence. We report on how the method complements traditional document relevance measurement and its support for comparative relevance assessment across multiple search engines. We also study possible issues with applying the method, including brand presentation effects, inter-judge agreement, and comparisons with document-based relevance judgments. Our findings show this is a useful method for evaluating the dominant user experience in interacting with search systems.\"",
        "Document: \"Merging Results From Isolated Search Engines.  . Two new techniques for merging search results are introduced:Feature Distance ranking algorithms and Reference Statistics.These techniques are compared with other published methods, usingTREC effectiveness evaluations based on human relevance judgementsand input rankings from 5 different search engines over 5 disjoint documentcollections. The new techniques are found to be more effective thanexisting methods in an isolated-server environment such as the WorldWide Web. In... \"",
        "Document: \"ANU/ACSys TREC-6 Experiments.  A number of experiments conducted within the framework of the TREC-6 conferenceand using a completely re-engineered version of the PArallel Document Retrieval Engine(PADRE97) are reported. Passage-based pseudo relevance feedback combined with a variantof City University's Okapi BM25 scoring function achieved best average precision, best recalland best precision@20 in the Long-topic Automatic Adhoc category. The same basic methodwas used as the basis for successful submissions in the Manual ... \"",
        "Document: \"Multimodal analysis of vocal collaborative search: a public corpus and results.  Intelligent agents have the potential to help with many tasks. Information seeking and voice-enabled search assistants are becoming very common. However, there remain questions as to the extent by which these agents should sense and respond to emotional signals. We designed a set of information seeking tasks and recruited participants to complete them using a human intermediary. In total we collected data from 22 pairs of individuals, each completing five search tasks. The participants could communicate only using voice, over a VoIP service. Using automated methods we extracted facial action, voice prosody and linguistic features from the audio-visual recordings. We analyzed the characteristics of these interactions that correlated with successful communication and understanding between the pairs. We found that those who were expressive in channels that were missing from the communication channel (e.g., facial actions and gaze) were rated as communicating poorly, being less helpful and understanding. Having a way of reinstating nonverbal cues into these interactions would improve the experience, even when the tasks are purely information seeking exercises. The dataset used for this analysis contains over 15 hours of video, audio and transcripts and reported ratings. It is publicly available for researchers at: http://aka.ms/MISCv1. \n\n\"",
        "Document: \"Results and challenges in Web search evaluation. A frozen 18.5 million page snapshot of part of the Web has been created to enable and encourage meaningful and reproducible evaluation of Web search systems and techniques. This collection is being used in an evaluation framework within the Text Retrieval Conference (TREC) and will hopefully provide convincing answers to questions such as, \"Can link information result in better rankings?\", \"Do longer queries result in better answers?\", and, \"Do TREC systems work well on Web data?\" The snapshot and associated evaluation methods are described and an invitation is extended to participate. Preliminary results are presented for an effectivess comparison of six TREC systems working on the snapshot collection against five well-known Web search systems working over the current Web. These suggest that the standard of document rankings produced by public Web search engines is by no means state-of-the-art. \u00a9 1999 Published by Elsevier Science B.V. All rights reserved.\"",
        "1 is \"CLEF-IP 2009: Retrieval Experiments in the Intellectual Property Domain.\", 2 is \"Personalizing web search results by reading level\"",
        "Given above information, for an author who has written the paper with the title \"Better Effectiveness Metrics for SERPs, Cards, and Rankings.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003806": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Noise Robustness Analysis of Point Cloud Descriptors':",
        "Document: \"Supervised method to build an atlas database for multi-atlas segmentation-propagation. Multi-atlas based segmentation-propagation approaches have been shown to obtain accurate parcelation of brain structures. However, this approach requires a large number of manually delineated atlases, which are often not available. We propose a supervised method to build a population specific atlas database, using the publicly available Internet Brain Segmentation Repository (IBSR). The set of atlases grows iteratively as new atlases are added, so that its segmentation capability may be enhanced in the multi-atlas based approach. Using a dataset of 210 MR images of elderly subjects (170 elderly controls, 40 Alzheimer's disease) from the Australian Imaging, Biomarkers and Lifestyle (AIBL) study, 40 MR images were segmented to build a population specific atlas database for the purpose of multi-atlas segmentation-propagation. The population specific atlases were used to segment the elderly population of 210 MR images, and were evaluated in terms of the agreement among the propagated labels. The agreement was measured by using the entropy H of the probability image produced when fused by voting rule and the partial moment mu(2) of the histogram. Compared with using IBSR atlases, the population specific atlases obtained a higher agreement when dealing with images of elderly subjects.\"",
        "Document: \"A survey of prostate segmentation methodologies in ultrasound, magnetic resonance and computed tomography images. Prostate segmentation is a challenging task, and the challenges significantly differ from one imaging modality to another. Low contrast, speckle, micro-calcifications and imaging artifacts like shadow poses serious challenges to accurate prostate segmentation in transrectal ultrasound (TRUS) images. However in magnetic resonance (MR) images, superior soft tissue contrast highlights large variability in shape, size and texture information inside the prostate. In contrast poor soft tissue contrast between prostate and surrounding tissues in computed tomography (CT) images pose a challenge in accurate prostate segmentation. This article reviews the methods developed for prostate gland segmentation TRUS, MR and CT images, the three primary imaging modalities that aids prostate cancer diagnosis and treatment. The objective of this work is to study the key similarities and differences among the different methods, highlighting their strengths and weaknesses in order to assist in the choice of an appropriate segmentation methodology. We define a new taxonomy for prostate segmentation strategies that allows first to group the algorithms and then to point out the main advantages and drawbacks of each strategy. We provide a comprehensive description of the existing methods in all TRUS, MR and CT modalities, highlighting their key-points and features. Finally, a discussion on choosing the most appropriate segmentation strategy for a given imaging modality is provided. A quantitative comparison of the results as reported in literature is also presented.\"",
        "Document: \"3D reconstruction of transparent objects exploiting surface fluorescence caused by UV irradiation. In this paper, we present a novel approach exploiting fluorescence imaging to estimate the shape of transparent objects. Classical inspection systems require users to coat transparent objects with some powder before measurement. Methods suggested in literature through non contact measurement do not effectively deal with the refraction problem, thus, providing inaccuracies. The proposed method handles the scanning of transparent objects without using any powder and solving the refraction problem using UV environment. A classical triangulation method based on stereovision scheme using fixed stereoscopic visible range cameras with a fixed UV (Ultra Violet) laser source is implemented. Transparent object surface irradiated by UV laser emits fluorescence i.e. a visible white light in a diffused manner. Images consisting of fluorescent points are thereafter analyzed to extract 3D information of the transparent object surface with a stereovision scheme. The object is moved for scanning different parts and the above procedure is repeated.\"",
        "Document: \"Automatic Assessment of Depression Based on Visual Cues: A Systematic Review. Automatic depression assessment based on visual cues is a rapidly growing research domain. The present exhaustive review of existing approaches as reported in over sixty publications during the last ten years focuses on image processing and machine learning algorithms. Visual manifestations of depression, various procedures used for data collection, and existing datasets are summarized. The review outlines methods and algorithms for visual feature extraction, dimensionality reduction, decision methods for classification and regression approaches, as well as different fusion strategies. A quantitative meta-analysis of reported results, relying on performance metrics robust to chance, is included, identifying general trends and key unresolved issues to be considered in future studies of automatic depression assessment utilizing visual cues alone or in combination with vocal or verbal cues.\"",
        "Document: \"A Thin-Plate Spline Based Multimodal Prostate Registration with Optimal Correspondences. Accurate extraction of prostate biopsy samples during Transectal Ultra Sound (TRUS) guided prostate biopsy is facilitated with the registration of pre-acquired Magnetic Resonance (MR) images with the Ultrasound (US) images. This paper proposes a novel method of generating optimal correspondences to register the MR and US images using Thin-Plate Splines (TPS) transformation. The correspondence generation method exploits the prostate shape geometry in both the modalities and is fully automatic. Normalized Mutual Information (NMI) is employed for the quantitative determination of optimal number of correspondences in terms of maximization of registration similarity. Qualitative registration results, that conform to the NMI measures are also shown for different numbers of correspondences. Shepard\u2019s interpolation method is used with the TPS in order to deal with the interpolation error of backward TPS transformation. The accuracy of our method of correspondence generation is qualitatively evaluated in comparison with two intuitive geometric contour sampling methods. An average Dice Similarity Coefficient (DSC) value of 0.97 \u5364 0.01 for 4 patient datasets is obtained for the TPS registration using our novel method of correspondences.\"",
        "1 is \"Optimization and development of concurrent EEG-fMRI data acquisition setup for understanding neural mechanisms of brain\", 2 is \"A shape-based approach to the segmentation of medical imagery using level sets.\"",
        "Given above information, for an author who has written the paper with the title \"Noise Robustness Analysis of Point Cloud Descriptors\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003948": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Using a CISC microcontroller to test embedded memories.':",
        "Document: \"Testability of the Philips 80C51 Micro-controller. This paper presents the research results of thesequential testability of the Philips 80C51 micro-controller [14]. The motivations for this research wereto save chip area and test application time (i.e., re-ducing the production costs), and to evaluate the effectiveness and efficiency of the Delft Automatic Test(DAT) generation system for sequential circuits [10] onreal industrial sequential circuits, such as the 80C51.ATPG has been performed on a fully sequential version (non-scan), and on several partial-scan versions ofthe 80C51. The stuck-at fault coverage of the full-scanversion is above 91%, while the fault coverage of thenon-scan version is almost zero. Therefore, partial-scanversions of the 80C51 have been developed to achievethe fault coverage level of the full-scan version. Experimental results demonstrate that almost 50% of theFFs have to be scannable in order to approach the faultcoverage of the full-scan version. The fault coverage isreduced by \\pm10%, when \\pm30% of the FFs have beenselected for scan.\"",
        "Document: \"March SS: a test for all static simple RAM faults. This paper presents all simple (i.e., not linked) static fault models that have been shown to exist for random access memories (RAMs), and shows that none of the current industrial march tests has the capability to detect all these faults. It therefore introduces a new test (March SS), with a test length of 22n, that detects all realistic simple static faults in RAMs.\"",
        "Document: \"Consequences of RAM bitline twisting for test coverage. In order to reduce coupling effects between bit-lines in static or dynamic RAMs, bitline twisting can be used in the design. For testing, however, this has consequences for the to-be-used data backgrounds. A generic twisting scheme is introduced and the involved fault models are identified.\"",
        "Document: \"Multiprocessing memory subsystem. This paper presents a central memory subsystem for a tightly coupled multimicroprocessor system. The design supports high-speed single-cycle and burst-mode data transfers. It supports a high level of data integrity through autonomous retries upon error detection, and provides a high level of test support. The multiprocessor system employs a distributed cache using the VMEbus. The memory subsystem has special facilities to guarantee data consistency in the different caches.\"",
        "Document: \"Industrial Evaluation of DRAM SIMM Tests. This paper describes the results of testing 50 single inlinememory modules (SIMMs), each containing 16 16MbitDRAM chips (DUTs); 39 SIMMs failed, and of the 800DUTs, 116 failed. In total 54 different test algorithms havebeen applied, using up to 168 different stress combinationsfor each test. The results show that GAL9R is the best test.Furthermore, it is shown that burst mode tests detect a completelydifferent class of faults as compared with traditionalword mode tests, and that tests with address scrambling enableddetect more faults.\"",
        "1 is \"High-level synthesis with reconfigurable datapath components\", 2 is \"Objective comparison of 32-bit buses\"",
        "Given above information, for an author who has written the paper with the title \"Using a CISC microcontroller to test embedded memories.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004019": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'An unlinkably divisible and intention attachable ticket scheme for runoff elections':",
        "Document: \"Identifying MMORPG bots: a traffic analysis approach. MMORPGs have become extremely popular among network gamers. Despite their success, one of MMORPG's great- est challenges is the increasing use of game bots, i.e., auto- playing game clients. The use of game bots is considered unsportsmanlike and is therefore forbidden. To keep games in order, game police, played by actual human players, of- ten patrol game zones and question suspicious players. This practice, however, is labor-intensive and ineffective. To ad- dress this problem, we analyze the traffic generated by hu- man players vs. game bots and propose solutions to auto- matically identify game bots. Taking Ragnarok Online, one of the most popular MMOGs, as our subject, we study the traffic generated by mainstream game bots and human players. We find that their traffic is distinguishable by: 1) the regularity in the release time of client commands, 2) the trend and magnitude of traffic burstiness in multiple time scales, and 3) the sensitivity to network conditions. We propose four strategies and two in- tegrated schemes to identify bots. For our data sets, the conservative scheme completely avoids making false accusa- tions against bona fide players, while the progressive scheme tracks game bots down more aggressively. Finally, we show that the proposed methods are generalizable to other games and robust against counter-measures from bot developers.\"",
        "Document: \"An on-line page-structure approximation scheme for Web proxies. To render a Web page, a browser must first download an HTML document, parse it, and then issue a sequence of additional requests to fetch the embedded objects according to the content of the HTML document. Therefore, it should be straightforward for Web proxies to accurately predict future client requests by considering the characteristics of such regular behavior. However, the strong bindings between embedded objects and their containing documents are often ignored by modern Web proxies because there still exists no efficient solution for Web proxies to obtain the knowledge of page structures without performing the computation-intensive operations of HTML parsing. In this paper, we propose an effective and low-overhead scheme for Web proxies to approximate page structures and refine the approximation as new client requests arrive. The results of simulation show that the approximation converges quickly and reaches high accuracy after a relatively small number of incoming requests have been processed.\"",
        "Document: \"Blind Threshold Signatures Based on Discrete Logarithm.  In this paper, we propose two group-oriented (t; n) blind threshold signature schemes based on the discrete logarithm problem. By these schemes, any t out of n signers in a group can represent the group to sign blind threshold signatures. In our schemes, the size of a threshold signature is the same as the size of an individual signature and the signature verification process is simplified by means of a group public key. Our proposed schemes do not require the assistance of a mutually trusted... \"",
        "Document: \"A location-ID sensitive key establishment scheme in static wireless sensor networks. Sensor networks are usually consist of thousands of resource-limited nodes and are deployed in a designated area without any fixed infrastructure. While the establishment of the pairwise keys between any pair of adjacent nodes to build a secure link remains the main concern in the design of key management protocols, malicious attacks aim at routing information, exhaust node's resource, and compromised secrets can misdirect the data flow or denial the network service with relatively small effort. Many mission-critic sensor network applications demand an effective, light, and flexible algorithm yet robust under attacks. Based on the LEAP+ scheme, we propose an improved LEAP+ by adding location information into the key establishment phase. By identifying the correctness of the id-location pair, our scheme effectively limits the Sybil attack and mitigates the damage of HELLO flood attack and node cloning attack. We furthermore propose an authentication phase in our scheme to defend possible replay attacks. The analysis shows that our scheme is more robust than LEAP+ with only minor increase of computation overhead.\"",
        "Document: \"A High Performance Dynamic Token-Based Distributed Synchronization Algorithm. In this paper we propose a new dynamic token-based distributed synchronization algorithm that utilizes a new technique called optimistic broadcasting (optcasting) to improve efficiency. Briefly, an optcast message is a reliable unicast one that can also be heard by nodes other than its designated destination. Our algorithm manages pending token requesters by a distributed queue, and optcasts a direction towards the current queue end to help new requesters finding the queue end more quickly. Simulated experimental results indicate that our optcast algorithm outperforms the already fast Chang-Singhal-Liu (CSL) algorithm by up to 36%, especially for large systems of many processor nodes and under high synchronization loads. In addition, optcasting is highly robust and resistant to message loss, retaining at least 63% coverage even when the message loss rate approaches 100%.\"",
        "1 is \"A new delegation-based authentication protocol for use in portable communication systems\", 2 is \"Group Signature Schemes with Membership Revocation for Large Groups\"",
        "Given above information, for an author who has written the paper with the title \"An unlinkably divisible and intention attachable ticket scheme for runoff elections\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004057": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Explaining the Adoption of Mobile Information Services from a Cultural Perspective':",
        "Document: \"VASP: virtualization assisted security monitor for cross-platform protection. Numerous operating systems have been designed to manage and control system resources with large and complicated features, so they need high security protection. However, previous security applications can not provide adequate protection due to the untrusted execution environment. Furthermore, these security strategies cannot support a universal cross-platform system security requirements. This paper presents VASP, a hypervisor based monitor which allows a trusted execution environment to monitor various malicious behaviors in the operating system. This is achieved by taking advantage of \u00d786 hardware virtualization and self-transparency technology, and providing a unified security protection to unmodified operating systems such as Linux and Windows. Our design is targeted at establishing a security monitor which resides completely outside of the target OS environment with a negligible overhead. According to the security analysis and performance experiment result, our approach can effectively protect applications and the kernel at a modest overhead of only 0.9% average in Windows XP and 2.6% average in Linux.\"",
        "Document: \"Novel Attacks In Ospf Networks To Poison Routing Table. Link State Advertisement (LSA) reflects the current status of all incident links of a router in an Autonomous System (AS). A fake LSA with false link status information will pollute the view of the network topology on routers. In this paper, we present two novel attacks that inject malicious Link State Advertisements (LSAs) to modify the routing tables: adjacency spoofing and single path injection. Adjacency spoofing attack makes attacker access to routing networks by disguising as a legitimate router. Single path injection attack evades the \"fightback\" mechanism and affects routing advertisements of routers. Unlike existing LSA injection attacks, which need to be launched by malicious routers, a common host can launch these attacks and control the transmission path of data traffic in an AS. Simulation and real-world experiment results show that these two attacks can efficiently modify the routing tables of routers, and further lead to DNS spoofing, phishing Website, eavesdropping, and man-in-the-middle attacks. Furthermore, we also implement a security vulnerability detection system to detect the existing vulnerabilities of routing protocol deployed in real-world routers.\"",
        "Document: \"Explaining the Adoption of Mobile Information Services from a Cultural Perspective. Little research has been done to explore the adoption of mobile information services from a cultural perspective. This research is designed to study mobile information services adoption from a cultural perspective. Based on the three cultural dimensions (individualism/collectivism, uncertainty avoidance, and power distance), three research hypotheses are presented. To examine these hypotheses, an exploratory study is carried out with a mobile information service called eMSIS with both Norwegian students and Chinese students. Support was found for the three hypotheses. The findings indicate that the cultural dimensions play important roles in how mobile information services are used and adopted in two different cultural settings: the Norwegian culture and the Chinese culture. The results also highlight the relevance of the cultural dimensions (individualism/collectivism, uncertainty avoidance, and power distance) as the factors affecting the adoption of mobile information services.\"",
        "Document: \"A study on Chinese consumers' concern about books on online bookstores. This study examines Chinese consumers' concern about books on online bookstores. To address this, four research questions are proposed. The data used in this study are collected from the book best-seller lists from the most popular online bookstores in China, which is Dangdang.com. It is found that the most Chinese consumers are mainly concerned with fiction, children's books, inspirational books and books about life. Secondly, the exponential Smoothing is used to predict the book short-term attention. And the reasonable results prove that the prediction method chosen is appropriate. Lastly, based on the data on consumers' preference on themes of the books from 2004 to 2011, we find that fiction is always ranked in the first place. At the same time, the other popular top-ranking themes are children book, health/mental health, and management.\"",
        "Document: \"Supporting adaptive learning in hypertext environments: a high level timed Petri net-based approach. A problem for hypertext-based learning application is to control learning paths for different learning activities. This paper first introduces related concepts of hypertext learning state space and high level Petri Nets (PNs), then proposes a high level timed PN based approach used to providing kinds of adaptation for learning activities by adjusting time attributes of targeted learning state space. Examples are given while explaining ways to realising adaptive instructions. Possible future directions are also discussed at the end of this paper.\"",
        "1 is \"Enterprise architecture governance: the need for a business-to-IT approach\", 2 is \"Over the top video: the gorilla in cellular networks\"",
        "Given above information, for an author who has written the paper with the title \"Explaining the Adoption of Mobile Information Services from a Cultural Perspective\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004165": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A Review of the Application of Multiobjective Evolutionary Fuzzy Systems: Current Status and Further Directions':",
        "Document: \"Prototype selection for nearest neighbor classification: taxonomy and empirical study. The nearest neighbor classifier is one of the most used and well-known techniques for performing recognition tasks. It has also demonstrated itself to be one of the most useful algorithms in data mining in spite of its simplicity. However, the nearest neighbor classifier suffers from several drawbacks such as high storage requirements, low efficiency in classification response, and low noise tolerance. These weaknesses have been the subject of study for many researchers and many solutions have been proposed. Among them, one of the most promising solutions consists of reducing the data used for establishing a classification rule (training data) by means of selecting relevant prototypes. Many prototype selection methods exist in the literature and the research in this area is still advancing. Different properties could be observed in the definition of them, but no formal categorization has been established yet. This paper provides a survey of the prototype selection methods proposed in the literature from a theoretical and empirical point of view. Considering a theoretical point of view, we propose a taxonomy based on the main characteristics presented in prototype selection and we analyze their advantages and drawbacks. Empirically, we conduct an experimental study involving different sizes of data sets for measuring their performance in terms of accuracy, reduction capabilities, and runtime. The results obtained by all the methods studied have been verified by nonparametric statistical tests. Several remarks, guidelines, and recommendations are made for the use of prototype selection for nearest neighbor classification.\"",
        "Document: \"GP-COACH: Genetic Programming-based learning of COmpact and ACcurate fuzzy rule-based classification systems for High-dimensional problems. In this paper we propose GP-COACH, a Genetic Programming-based method for the learning of COmpact and ACcurate fuzzy rule-based classification systems for High-dimensional problems. GP-COACH learns disjunctive normal form rules (generated by means of a context-free grammar) coded as one rule per tree. The population constitutes the rule base, so it is a genetic cooperative-competitive learning approach. GP-COACH uses a token competition mechanism to maintain the diversity of the population and this obliges the rules to compete and cooperate among themselves and allows the obtaining of a compact set of fuzzy rules. The results obtained have been validated by the use of non-parametric statistical tests, showing a good performance in terms of accuracy and interpretability.\"",
        "Document: \"Replacing pooling functions in Convolutional Neural Networks by linear combinations of increasing functions. Traditionally, Convolutional Neural Networks make use of the maximum or arithmetic mean in order to reduce the features extracted by convolutional layers in a downsampling process known as pooling. However, there is no strong argument to settle upon one of the two functions and, in practice, this selection turns to be problem dependent. Further, both of these options ignore possible dependencies among the data. We believe that a combination of both of these functions, as well as of additional ones which may retain different information, can benefit the feature extraction process. In this work, we replace traditional pooling by several alternative functions. In particular, we consider linear combinations of order statistics and generalizations of the Sugeno integral, extending the latter\u2019s domain to the whole real line and setting the theoretical base for their application. We present an alternative pooling layer based on this strategy which we name \u201cCombPool\u201d layer. We replace the pooling layers of three different architectures of increasing complexity by CombPool layers, and empirically prove over multiple datasets that linear combinations outperform traditional pooling functions in most cases. Further, combinations with either the Sugeno integral or one of its generalizations usually yield the best results, proving a strong candidate to apply in most architectures.\"",
        "Document: \"Online entropy-based discretization for data streaming classification. Data quality is deemed as determinant in the knowledge extraction process. Low-quality data normally imply low-quality models and decisions. Discretization, as part of data preprocessing, is considered one of the most relevant techniques for improving data quality.\"",
        "Document: \"IPADE: Iterative Prototype Adjustment for Nearest Neighbor Classification. Nearest prototype methods are a successful trend of many pattern classification tasks. However, they present several shortcomings such as time response, noise sensitivity, and storage requirements. Data reduction techniques are suitable to alleviate these drawbacks. Prototype generation is an appropriate process for data reduction, which allows the fitting of a dataset for nearest neighbor (NN) classification. This brief presents a methodology to learn iteratively the positioning of prototypes using real parameter optimization procedures. Concretely, we propose an iterative prototype adjustment technique based on differential evolution. The results obtained are contrasted with nonparametric statistical tests and show that our proposal consistently outperforms previously proposed methods, thus becoming a suitable tool in the task of enhancing the performance of the NN classifier.\"",
        "1 is \"A review on the design and optimization of interval type-2 fuzzy controllers\", 2 is \"Gene perturbation and intervention in context-sensitive stochastic Boolean networks.\"",
        "Given above information, for an author who has written the paper with the title \"A Review of the Application of Multiobjective Evolutionary Fuzzy Systems: Current Status and Further Directions\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004170": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Automatically discovering word senses':",
        "Document: \"Word-for-word glossing with contextually similar words. Many corpus-based machine translation systems require parallel corpora. In this paper, we present a word-for-word glossing algorithm that requires only a source language corpus. To gloss a word, we first identify its similar words that occurred in the same context in a large corpus. We then determine the gloss by maximizing the similarity between the set of contextually similar words and the different translations of the word in a bilingual thesaurus.\"",
        "Document: \"Discriminative learning of selectional preference from unlabeled text. We present a discriminative method for learning selectional preferences from unlabeled text. Positive examples are taken from observed predicate-argument pairs, while negatives are constructed from unobserved combinations. We train a Support Vector Machine classifier to distinguish the positive from the negative instances. We show how to partition the examples for efficient training with 57 thousand features and 6.5 million training instances. The model outperforms other recent approaches, achieving excellent correlation with human plausibility judgments. Compared to Mutual Information, it identifies 66% more verb-object pairs in unseen text, and resolves 37% more pronouns correctly in a pronoun resolution experiment.\"",
        "Document: \"Induction of semantic classes from natural language text. Many applications dealing with textual information require classification of words into semantic classes (or concepts). However, manually constructing semantic classes is a tedious task. In this paper, we present an algorithm, UNICON, for UNsupervised Induction of CONcepts. Some advantages of UNICON over previous approaches include the ability to classify words with low frequency counts, the ability to cluster a large number of elements in a high-dimensional space, and the ability to classify previously unknown words into existing clusters. Furthermore, since the algorithm is unsupervised, a set of concepts may be constructed for any corpus.\"",
        "Document: \"A probability model to improve word alignment. Word alignment plays a crucial role in statistical machine translation. Word-aligned corpora have been found to be an excellent source of translation-related knowledge. We present a statistical model for computing the probability of an alignment given a sentence pair. This model allows easy integration of context-specific features. Our experiments show that this model can be an effective tool for improving an existing word alignment.\"",
        "Document: \"University of Manitoba: description of the NUBA system as used for MUC-5. Abduction is the inference to the best explanation. Many tasks in natural language understanding such as word-sense disambiguity [1], local pragmatics [4], metaphor interpretation [3], and plan recognition [5, 8], can be viewed as abduction.\"",
        "1 is \"Extracting Patterns and Relations from the World Wide Web\", 2 is \"Authoritative sources in a hyperlinked environment\"",
        "Given above information, for an author who has written the paper with the title \"Automatically discovering word senses\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004168": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Breaching IM session privacy using causality':",
        "Document: \"Scalable Mpeg-4 Video Coding With Graceful Packet-Loss Resilience Over Bandwidth-Varying Networks. In this paper, we evaluate the packet loss resilience of the recently developed MPEG-4 Fine-Granular-Scalability (FGS) video coding method for Internet streaming applications. Since unrecoverable packet losses are very common over the Internet, the focus of this study is to determine how robust the MPEG-4 FGS coding tool is under unrecoverable IP packet-losses over a wide range of connection qualities (i.e. at various bit-rates). Our extensive study was performed upon an entire set of sequences, bit-rates and packet-loss rates. As shown in this paper, under unequal Packet-loss Protection, FGS provides a clear advantage over non-scalable coding. If Equal Packet-loss Protection is employed, FGS performance is still considerably better, in particular under moderate-to-high packet-loss ratios.\"",
        "Document: \"Network-embedded FEC for optimum throughput of multicast packet video. Forward error correction (FEC) schemes have been proposed and used successfully for multicasting realtime video content to groups of users. Under traditional IP multicast, application-level FEC can only be implemented on an end-to-end basis between the sender and the clients. Emerging overlay and peer-to-peer (p2p) networks open the door for new paradigms of network FEC. The deployment of FEC within these emerging networks has received very little attention (if any). In this paper, we analyze and optimize the impact of network-embedded FEC (NEF) in overlay and p2p multimedia multicast networks. Under NEF, we place FEC codecs in selected intermediate nodes of a multicast tree. The NEF codecs detect and recover lost packets within FEC blocks at earlier stages before these blocks arrive at deeper intermediate nodes or at the final leaf nodes. This approach significantly reduces the probability of receiving undecodable FEC blocks. In essence, the proposed NEF codecs work as signal regenerators in a communication system and can reconstruct most of the lost data packets without requiring retransmission. We develop an optimization algorithm for the placement of NEF codecs within random multicast trees. Based on extensive H.264 video simulations, we show that this approach provides significant improvements in video quality, both visually and in terms of PSNR values.\"",
        "Document: \"Performance Evaluation: Priority Transmission Using Network Coding With Multi-Generation Mixing. In this paper, we evaluate the priority transmission characteristic of Network Coding (NC) with Multi-Generation Mixing (MGM). MGM supports priority transmission by providing enhanced reliability for delivering different groups of sender packets. MGM is a generalized approach for practical network coding that enhances its performance. With MGM, sender packets are grouped in generations that constitute mixing sets. By employing the novel inter-generation network coding approach, each generation within a mixing set can be considered as a priority layer. This is due to the varying levels of data protection provided to the different generations within a mixing set. Traditionally, priority transmission is done by increasing the level of FEC protection assigned to sender data of higher priority. This incurs transmission overhead that consumes bandwidth. MGM supports priority transmission by network encoding data of higher priority in more sender packets so that more packets support its recovery. At the same time there is no increase in the total number of packets transmitted and hence no additional transmission overhead.\"",
        "Document: \"Compressive Demosaicing For Periodic Color Filter Arrays. The utility of Compressed Sensing (CS) for demosaicing of images captured using random panchromatic color filter arrays (CFA) has been investigated in [1]. Meanwhile, most camera manufacturers employ periodic CFAs such as the popular Bayer CFA. In this paper, we derive a CS-based solution to demosaicing images captured using the general class of periodic CFAs. It is well known that periodic CFAs can be designed to effectively separate luminance and chrominance frequency bands [2, 3]. We employ this ability to reduce artifacts associated with luminance-chrominance overlap at the solver side. We show that the modified compressive demosaicing method coupled with the additional constraint that chrominance channels have smooth surfaces achieves further improved results for most periodic CFAs.\"",
        "Document: \"Identifying Leaders and Followers in Online Social Networks. Identifying leaders and followers in online social networks is important for various applications in many domains such as advertisement, community health campaigns, administrative science, and even politics. In this paper, we study the problem of identifying leaders and followers in online social networks using user interaction information. We propose a new model, called the Longitudinal User Centered Influence (LUCI) model, that takes as input user interaction information and clusters users into four categories: introvert leaders, extrovert leaders, followers, and neutrals. To validate our model, we first apply it to a data set collected from an online social network called Everything2. Our experimental results show that our LUCI model achieves an average classification accuracy of up to 90.3% in classifying users as leaders and followers, where the ground truth is based on the labeled roles of users. Second, we apply our LUCI model on a data set collected from Facebook consisting of interactions among more than 3 million users over the duration of one year. However, we do not have ground truth data for Facebook users. Therefore, we analyze several important topological properties of the friendship graph for different user categories. Our experimental results show that different user categories exhibit different topological characteristics in the friendship graph and these observed characteristics are in accordance with the expected ones based on the general definition of the four roles.\"",
        "1 is \"Distributed Source Coding: Symmetric Rates and Applications to Sensor Networks\", 2 is \"Algorithmic approaches to redesigning tcam-based systems\"",
        "Given above information, for an author who has written the paper with the title \"Breaching IM session privacy using causality\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004199": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Matched Drawings of Planar Graphs':",
        "Document: \"Computing straight-line 3D grid drawings of graphs in linear volume. This paper investigates the basic problem of computing crossing-free straight-line 3D grid drawings of graphs such that the overall volume is small. Motivated by their relevance in the literature, we focus on families of graphs having constant queue number and on k-trees. We present algorithms that compute drawings of these families of graphs on 3D grids consisting of a constant number of parallel lines and such that the overall volume is linear. Lower bounds on the number of such grid lines are also provided. Our results extend and improve similar ones already described in the literature.\"",
        "Document: \"Fan-planarity: Properties and complexity. In a fan-planar drawing of a graph an edge can cross only edges with a common end-vertex. Fan-planar drawings have been recently introduced by Kaufmann and Ueckerdt [35], who proved that every n-vertex fan-planar drawing has at most 5n\u221210 edges, and that this bound is tight for n\u226520. We extend their result from both the combinatorial and the algorithmic point of view. We prove tight bounds on the density of constrained versions of fan-planar drawings and study the relationship between fan-planarity and k-planarity. Also, we prove that testing fan-planarity in the variable embedding setting is NP-complete.\"",
        "Document: \"Drawing Bipartite Graphs on Two Parallel Convex Curves. Let G be a bipartite graph, and let \u201ae;\u201ai be two parallel convex curves; we study the question about whether G admits a planar straight-line drawing such that the vertices of one partite set of G lie on \u201ae and the vertices of the other partite set lie on \u201ai. A characterization is presented that gives rise to linear time testing algorithm. We also describe a drawing algorithm that runs in linear time if the curves are two concentric circles and the real RAM model of computation is adopted.\"",
        "Document: \"Colored Point-set Embeddings of Acyclic Graphs. We show that any planar drawing of a forest of three stars whose vertices are constrained to be at fixed vertex locations may require (varOmega (n^frac{2}{3})) edges each having (varOmega (n^frac{1}{3})) bends in the worst case. The lower bound holds even when the function that maps vertices to points is not a bijection but it is defined by a 3-coloring. In contrast, a constant number of bends per edge can be obtained for 3-colored paths and for 3-colored caterpillars whose leaves all have the same color. Such results answer to a long standing open problem.\"",
        "Document: \"Fan-Planar Graphs: Combinatorial Properties And Complexity Results. In a fan-planar drawing of a graph an edge can cross only edges with a common end-vertex. Fan-planar drawings have been recently introduced by Kaufmann and Ueckerdt, who proved that every n-vertex fan-planar drawing has at most 5n - 10 edges, and that this bound is tight for n >= 20. We extend their result from both the combinatorial and the algorithmic point of view. We prove tight bounds on the density of constrained versions of fan-planar drawings and study the relationship between fan-planarity and k-planarity. Also, we prove that testing fan-planarity in the variable embedding setting is NP-complete.\"",
        "1 is \"Encompassing colored planar straight line graphs\", 2 is \"Strip Planarity Testing for Embedded Planar Graphs\"",
        "Given above information, for an author who has written the paper with the title \"Matched Drawings of Planar Graphs\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004373": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Routing metrics for best-effort traffic':",
        "Document: \"Log-Logistic Software Reliability Growth Model. Finite failure NHPP models proposed in the literature exhibit either constant, monotonic increasing or monotonic decreasing failure occurrence rates per fault, and are in- adequate to describe the failure process underlying cer- tain failure data sets. In this paper, we propose the log- logistic reliability growth model, which can capture the increasing decreasing nature of the failure occurrence rate per fault. Equations to estimate the parameters of the exist- ing finite failure NHPP models, as well as the log-logistic model, based on failure data collected in the form of inter- failure times are developed. We also present an analysis of two data sets, where the underlying failure process could not be adequately described by the existing models, which motivated the development of the log-logistic model.\"",
        "Document: \"Specification: level integration of simulation and dependability analysis. Software architectural choices have a profound influence on the quality attributes supported by a system. Architecture analysis can be used to evaluate the influence of design decisions on important quality attributes such as maintainability, performance and dependability. As software architecture gains appreciation as a critical design level for software systems, techniques and tools to support testing, understanding, debugging and maintaining these architectures are expected to become readily available. In addition to providing the desired support, data collected from these tools also provides a rich source of information from the point of view of performance and dependability analysis of the architecture. This paper presents a performance and dependability analysis methodology which illustrates the use of such data. The methodology thus seeks a three way integration of distinct and important areas, namely, formal specification, specification simulation/testing and performance and dependability analysis. We illustrate the key steps in the methodology with the help of a case study.\"",
        "Document: \"A classification framework for web robots. The behavior of modern web robots varies widely when they crawl for different purposes. In this article, we present a framework to classify these web robots from two orthogonal perspectives, namely, their functionality and the types of resources they consume. Applying the classification framework to a year-long access log from the UConn SoE web server, we present trends that point to significant differences in their crawling behavior. \u00a9 2012 Wiley Periodicals, Inc.\"",
        "Document: \"Metrics for Quantifying the Disparity, Concentration, and Dedication between Program Components and Features. One of the most important steps towards effective software maintenance of a large complicated system is to understand how program features are spread over the entire system and their interactions with the program components. However, we must first be able to represent an abstract feature in terms of some concrete program components.In this paper, we use an execution slice-based technique to identify the basic blocks which are used to implement a program feature. Three metrics are then defined, based on this identification, to determine quantitatively, the disparity between a program component and a feature, the concentration of a feature in a program component, and the dedication of a program component to a feature. The computations of these metrics are automated by incorporating them in a tool (chi-Suds), which makes the use of our metrics immediately applicable in real-life contexts. We demonstrate the effectiveness of our technique by experimenting with a reliability and performance evaluator.Results of our study suggest that these metrics can provide an indication of the closeness between a feature and a program component which is very useful for software programmers and maintainers to better understand the system at hand.\"",
        "Document: \"A multiplicative model of software defect repair times. We hypothesize that software defect repair times can be characterized by the Laplace Transform of the Lognormal (LTLN) distribution. This hypothesis is rooted in the observation that software defect repair times are influenced by the multiplicative interplay of several factors, and the lognormal distribution is a natural choice to model rates of occurrence of such phenomenon. Conversion of the lognormal rate distribution to an occurrence time distribution yields the LTLN. We analyzed a total of more than 10,000 software defect repair times collected over nine products at Cisco Systems to confirm our LTLN hypothesis. Our results also demonstrate that the LTLN distribution provides a statistically better fit to the observed repair times than either of the two most widely used repair time distributions, namely, the lognormal and the exponential. Moreover, we show that the repair times of subsets of defects, partitioned according to the Orthogonal Defect Classification (ODC) scheme also follow the LTLN distribution. Finally, we describe how the insights that lead to the LTLN repair time model allow us to consider and evaluate alternative process improvement strategies.\"",
        "1 is \"Improving the throughput of point-to-multipoint ARQ protocols through destination set splitting\", 2 is \"Real life, real users, and real needs: a study and analysis of user queries on the web\"",
        "Given above information, for an author who has written the paper with the title \"Routing metrics for best-effort traffic\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004399": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Geodec: Enabling Geospatial Decision Making':",
        "Document: \"Point cloud matching based on 3D self-similarity. Point cloud is one of the primitive representations of 3D data nowadays. Despite that much work has been done in 2D image matching, matching 3D points achieved from different perspective or at different time remains to be a challenging problem. This paper proposes a 3D local descriptor based on 3D self-similarities. We not only extend the concept of 2D self-similarity [1] to the 3D space, but also establish the similarity measurement based on the combination of geometric and photometric information. The matching process is fully automatic i.e. needs no manually selected land marks. The results on the LiDAR and model data sets show that our method has robust performance on 3D data under various transformations and noises.\"",
        "Document: \"Orientation Tracking for Outdoor Augmented Reality Registration. The biggest single obstacle to building effective augmented reality (AR) systems is the lack of accurate wide-area sensors for tracking the locations and orientations of objects in an environment. Active (sensor-emitter) tracking technologies require powered-device installation, limiting their use to prepared areas that are relatively free of natural or man-made interference sources. Vision-based systems can use passive landmarks, but they are more computationally demanding and often exhibit erroneous behavior due to occlusion or numerical instability. Inertial sensors are completely passive, requiring no external devices or targets, however, their drift rates in portable strapdown configurations are too great for practical use. In this paper, we present a hybrid approach to orientation tracking that integrates inertial and vision-based sensing. We exploit the complementary nature of the two technologies to compensate for the weaknesses in each component. Analysis and experimental results demonstrate the effectiveness of this approach.\"",
        "Document: \"Texture Painting from Video. Texture mapping is an important research topic in computer graphics. Traditional static texture-maps are limiting for capturing a dynamic and up-to-date picture of the environment. This paper presents a new technique called texture painting from video. By employing live video as the texture resource, we are not only able to create an accurate and photo-realistic rendering of the scene, but also can support dynamic spatio-temporal update in the structure of texture model, database, and rendering system. We present our approaches towards the system requirements and experimental results for both simulation and real datasets.\"",
        "Document: \"Automatic 3D industrial point cloud modeling and recognition. 3D modeling of point clouds is an important but time-consuming process, inspiring extensive research in automatic methods. Prior efforts focus on primitive geometry, street structures or indoor objects, but industrial data has rarely been pursued. Our work presents a method for automatic modeling and recognition of 3D industrial site point clouds, dividing the task into 3 separate sub-problems: pipe modeling, plane classification, and object recognition. The results are integrated to obtain the complete model, revealing some issues during the integration, solved by utilizing information gained from each individual process. Experiments show that the presented method automatically models large and complex industrial scenes with a quality that outperforms leading commercial modeling software and is comparable to professional hand-made models.\"",
        "Document: \"A Robust Hybrid Tracking System for Outdoor Augmented Reality. We present a real-time hybrid tracking system thatintegrates gyroscopes and line-based vision trackingtechnology. Gyroscope measurements are used topredict orientation and image line positions.Gyroscope drift is corrected by vision tracking.System robustness is achieved by using a heuristiccontrol system to evaluate measurement quality andselect measurements accordingly. Experiments showthat the system achieves robust, accurate, and real-timeperformance for outdoor augmented reality.\"",
        "1 is \"A Survey of Augmented Reality\", 2 is \"The Strength of Weak Learnability\"",
        "Given above information, for an author who has written the paper with the title \"Geodec: Enabling Geospatial Decision Making\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004406": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Parallel Computational Intelligence-Based Multi-Camera Surveillance System':",
        "Document: \"Automatic Schaeffer's gestures recognition system. Schaeffer's sign language consists of a reduced set of gestures designed to help children with autism or cognitive learning disabilities to develop adequate communication skills. Our automatic recognition system for Schaeffer's gesture language uses the information provided by an RGB-D camera to capture body motion and recognize gestures using dynamic time warping combined with k-nearest neighbors methods. The learning process is reinforced by the interaction with the proposed system that accelerates learning itself thus helping both children and educators. To demonstrate the validity of the system, a set of qualitative experiments with children were carried out. As a result, a system which is able to recognize a subset of 11 gestures of Schaeffer's sign language online was achieved.\"",
        "Document: \"Where Are We After Five Editions?: Robot Vision Challenge, a Competition that Evaluates Solutions for the Visual Place Classification Problem. This article describes the Robot Vision challenge, a competition that evaluates solutions for the visual place classification problem. Since its origin, this challenge has been proposed as a common benchmark where worldwide proposals are measured using a common overall score. Each new edition of the competition introduced novelties, both for the type of input data and subobjectives of the challenge. All the techniques used by the participants have been gathered up and published to make it accessible for future developments. The legacy of the Robot Vision challenge includes data sets, benchmarking techniques, and a wide experience in the place classification research that is reflected in this article.\"",
        "Document: \"Object recognition in noisy RGB-D data using GNG. Object recognition in 3D scenes is a research field in which there is intense activity guided by the problems related to the use of 3D point clouds. Some of these problems are influenced by the presence of noise in the cloud that reduces the effectiveness of a recognition process. This work proposes a method for dealing with the noise present in point clouds by applying the growing neural gas (GNG) network filtering algorithm. This method is able to represent the input data with the desired number of neurons while preserving the topology of the input space. The GNG obtained results which were compared with a Voxel grid filter to determine the efficacy of our approach. Moreover, since a stage of the recognition process includes the detection of keypoints in a cloud, we evaluated different keypoint detectors to determine which one produces the best results in the selected pipeline. Experiments show how the GNG method yields better recognition results than other filtering algorithms when noise is present.\"",
        "Document: \"Junction detection and grouping with probabilistic edge models and Bayesian A\u2217. In this paper, we propose and integrate two Bayesian methods, one of them for junction detection, and the other one for junction grouping. Our junction detection method relies on a probabilistic edge model and a log-likelihood test. Our junction grouping method relies on finding connecting paths between pairs of junctions. Path searching is performed by applying a Bayesian A\u2217 algorithm. Such algorithm uses both an intensity and geometric model for defining the rewards of a partial path and prunes those paths with low rewards. We have extended such a pruning with an additional rule which favors the stability of longer paths against shorter ones. We have tested experimentally the efficiency and robustness of the methods in an indoor image sequence.\"",
        "Document: \"A robust and fast method for 6DoF motion estimation from generalized 3D data. Nowadays, there is an increasing number of robotic applications that need to act in real three-dimensional (3D) scenarios. In this paper we present a new mobile robotics orientated 3D registration method that improves previous Iterative Closest Points based solutions both in speed and accuracy. As an initial step, we perform a low cost computational method to obtain descriptions for 3D scenes planar surfaces. Then, from these descriptions we apply a force system in order to compute accurately and efficiently a six degrees of freedom egomotion. We describe the basis of our approach and demonstrate its validity with several experiments using different kinds of 3D sensors and different 3D real environments.\"",
        "1 is \"Summarising contextual activity and detecting unusual inactivity in a supportive home environment\", 2 is \"Coarse-to-Fine vision-based localization by indexing scale-invariant features.\"",
        "Given above information, for an author who has written the paper with the title \"Parallel Computational Intelligence-Based Multi-Camera Surveillance System\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004410": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A Dataset for Persistent Multi-target Multi-camera Tracking in RGB-D.':",
        "Document: \"Temporal video segmentation and classification of edit effects. The process of shot break detection is a fundamental component in automatic video indexing, editing and archiving. This paper introduces a novel approach to the detection and classification of shot transitions in video sequences including cuts, fades and dissolves. It uses the average inter-frame correlation coefficient and block-based motion estimation to track image blocks through the video sequence and to distinguish changes caused by shot transitions from those caused by camera and object motion. We present a number of experiments in which we achieve better results compared with two established techniques.\"",
        "Document: \"Multiple human tracking in RGB-depth data: a survey. Multiple human tracking (MHT) is a fundamental task in many computer vision applications. Appearance-based approaches, primarily formulated on RGB data, are constrained and affected by problems arising from occlusions and/or illumination variations. In recent years, the arrival of cheap RGB-depth devices has led to many new approaches to MHT, and many of these integrate colour and depth cues to im...\"",
        "Document: \"Perceptual Smoothing and Segmentation of Colour Textures. An approach for perceptual segmentation of colour image textures is described. A multiscale representation of the texture image, generated by a multiband smoothing algorithm based on human psychophysical measurements of colour appearance is used as the input. Initial segmentation is achieved by applying a clustering algorithm to the image at the coarsest level of smoothing. Using these isolated \\em core clusters 3D colour histograms are formed and used for probabilistic assignment of all other pixels to the core clusters to form larger clusters and categorise the rest of the image. The process of setting up colour histograms and probabilistic reassignment of the pixels is then propagated through finer levels of smoothing until a full segmentation is achieved at the highest level of resolution.\"",
        "Document: \"Rectifying perspective views of text in 3D scenes using vanishing points. Documents may be captured at any orientation when viewed with a hand-held camera. Here, a method of recovering fronto-parallel views of perspectively skewed text documents in single images is presented, useful for \u2018point-and-click\u2019 scanning or when generally seeking regions of text in a scene. We introduce a novel extension to the commonly used 2D projection profiles in document recognition to locate the horizontal vanishing point of the text plane. Following further analysis, we segment the lines of text to determine the style of justification of the paragraphs. The change in line spacings exhibited due to perspective is then used to locate the document's vertical vanishing point. No knowledge of the camera focal length is assumed. Using the vanishing points, a fronto-parallel view is recovered which is then suitable for OCR or other high-level recognition. We provide results demonstrating the algorithm's performance on documents over a wide range of orientations.\"",
        "Document: \"A head-mounted device for recognizing text in natural scenes. We present a mobile head-mounted device for detecting and tracking text that is encased in an ordinary flat-cap hat. The main parts of the device are an integrated camera and audio webcam together with a simple remote control system, all connected via a USB hub to a laptop. A near to real-time text detection algorithm (around 14 fps for 640\u00d7480 images) which uses Maximal Stable Extremal Regions (MSERs) for image segmentation is proposed. Comparative text detection results against the ICDAR 2003 text locating competition database along with performance figures are presented.\"",
        "1 is \"A system for learning statistical motion patterns.\", 2 is \"Text Detection and Recognition on Traffic Panels From Street-Level Imagery Using Visual Appearance.\"",
        "Given above information, for an author who has written the paper with the title \"A Dataset for Persistent Multi-target Multi-camera Tracking in RGB-D.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004423": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A new Informative Generic Base of Association Rules':",
        "Document: \"Yet another approach for completing missing values. When tackling real-life datasets, it is common to face the existence of scrambled missing values within data. Considered as \"dirty data\", it is usually removed during the pre-processing step of the KDD process. Starting from the fact that \"making up this missing data is better than throwing it away\", we present a new approach trying to complete the missing data. The main singularity of the introduced approach is that it sheds light on a fruitful synergy between generic basis of association rules and the topic of missing values handling. In fact, beyond interesting compactness rate, such generic association rules make it possible to get a considerable reduction of conflicts during the completion step. A new metric called \"Robustness\" is also introduced, and aims to select the robust association rule for the completion of a missing value whenever a conflict appears. Carried out experiments on benchmark datasets confirm the soundness of our approach. Thus, it reduces conflict during the completion step while offering a high percentage of correct completion accuracy.\"",
        "Document: \"MAD-IDS: novel intrusion detection system using mobile agents and data mining approaches. Intrusion Detection has been investigated for many years and the field reached the maturity. Nevertheless, there are still important challenges, e.g., how an Intrusion Detection System (IDS) can detect distributed attacks. To tackle this problem, we propose a novel distributed IDS, based on the desirable features provided by the mobile agent methodology and the high accuracy offered by the data mining techniques.\"",
        "Document: \"Leveraging Temporal Query-Term Dependency for Time-Aware Information Access. Incorporating the temporal property of queries into time-aware information access methods has been shown to have a significant positive effect on a large number of search tasks, such as over microblogs and news archive. Recent work on time-aware search mostly rely on time-based relevance models that are built upon the language model framework. However, in this model, query terms are often assumed to be generated independently from each other. In this paper, we observe through a time series analysis that, query terms are temporally dependent and are frequently occurring within similar time periods when they deal with the same topics. In contrast to existing work, we propose a method that naturally extends the effective temporal language model and exploits this dependency at the term granularity level. Moreover, we reframe the task as a rank aggregation problem that fully exploits the temporal features of query terms. Experiments using the large-scale TREC Temporal Summarization 2013 and 2014 standard datasets empirically show that our method leads to significant performance improvements, when compared to state-of-the-art temporal ranking models.\"",
        "Document: \"Missing Values: Proposition of a Typology and Characterization with an Association Rule-Based Model. Handling missing values when tackling real-world datasets is a great challenge arousing the interest of many scientific communities. Many works propose completion methods or implement new data mining techniques tolerating the presence of missing values. It turns out that these tasks are very hard. In this paper, we propose a new typology characterizing missing values according to relationships within the data. These relationships are automatically discovered by data mining techniques using generic bases of association rules. We define four types of missing values from these relationships. The characterization is made for each missing value. It differs from the well-known statistical methods which apply a same treatment for all missing values coming from a same attribute. We claim that such a local characterization enables us perceptive techniques to deal with missing values according to their origins: the way in which we deal with the missing values should depend on their origins (e.g., attribute meaningless w.r.t. other attributes, missing values depending on other data, missing values by accident). Experiments on a real-world medical dataset highlight the interests of such a characterization.\"",
        "Document: \"A neural-based approach for extending OLAP to prediction. In the Data Warehouse (DW) technology, On-line Analytical Processing (OLAP) is a good applications package that empowers decision makers to explore and navigate into a multidimensional structure of precomputed measures, which is referred to as a Data Cube. Though, OLAP is poorly equipped for forecasting and predicting empty measures of data cubes. Usually, empty measures translate inexistent facts in the DW and in most cases are a source of frustration for enterprise managements, especially when strategic decisions need to be taken. In the recent years, various studies have tried to add prediction capabilities to OLAP applications. For this purpose, generally, Data Mining and Machine Learning methods have been widely used to predict new measures' values in DWs. In this paper, we introduce a novel approach attempting to extend OLAP to a prediction application. Our approach operates in two main stages. The first one is a preprocessing one that makes use of the Principal Component Analysis (PCA) to reduce the dimensionality of the data cube and then generates ad hoc training sets. The second stage proposes a novel OLAP oriented architecture of Multilayer Perceptron Networks (MLP) that learns from each training set and comes out with predicted measures of inexistent facts. Carried out experiments demonstrate the effectiveness of our proposal and the performance of its predictive capabilities.\"",
        "1 is \"Supervised Machine Learning: A Review of Classification Techniques\", 2 is \"GraphX: a resilient distributed graph system on Spark\"",
        "Given above information, for an author who has written the paper with the title \"A new Informative Generic Base of Association Rules\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004438": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Satisfying requirements for pervasive service compositions':",
        "Document: \"Using Pre-Requirements Tracing To Investigate Requirements Based On Tacit Knowledge. Pre-requirements specification tracing concerns the identification and maintenance of relationships between requirements and the knowledge and information used by analysts to inform the requirements' formulation. However, such tracing is often not performed as it is a time-consuming process. This paper presents a tool for retrospectively identifying pre-requirements traces by working backwards from requirements to the documented records of the elicitation process such as interview transcripts or ethnographic reports. We present a preliminary evaluation of our tools performance using a case study. One of the key goals of our work is to identify requirements that have weak relationships with the source material. There are many possible reasons for this, but one is that they embody tacit knowledge. Although we do not investigate the nature of tacit knowledge in RE we believe that even helping to identify the probable presence of tacit knowledge is useful. This is particularly true for circumstances when requirements' sources need to be understood during, for example, the handling of change requests.\"",
        "Document: \"MOG user interface builder: a mechanism for integrating application and user interface. Tools which provide graphical editing techniques for the design of user interface presentations are increasingly commonplace. Such tools vary widely in the mechanisms used to define user interfaces and while some are general purpose, others are targeted at particular application domains. Designers faced with varying requirements must choose one tool and live with its shortcomings, purchase a number of different tools, or implement their own. The paper describes an approach to facilitating the latter by providing a library of augmented user interface components called MOG objects which embody both end-user and editing semantics. User interface design tools based on this approach need only provide mechanisms for composing MOG objects into user interfaces and the addition of any other, higher-level functionality. MOG-based user interfaces retain an in-built editing capability and are inherently tailorable.\"",
        "Document: \"Run-time resolution of uncertainty. Requirements awareness should help optimize requirements satisfaction when factors that were uncertain at design time are resolved at runtime. We use the notion of claims to model assumptions that cannot be verified with confidence at design time. By monitoring claims at runtime, their veracity can be tested. If falsified, the effect of claim negation can be propagated to the system's goal model and an alternative means of goal realization selected automatically, allowing the dynamic adaptation of the system to the prevailing environmental context.\"",
        "Document: \"The case for dumb requirements engineering tools. [Context and Motivation] This paper notes the advanced state of the natural language (NL) processing art and considers four broad categories of tools for processing NL requirements documents. These tools are used in a variety of scenarios. The strength of a tool for a NL processing task is measured by its recall and precision. [Question/Problem] In some scenarios, for some tasks, any tool with less than 100% recall is not helpful and the user may be better off doing the task entirely manually. [Principal Ideas/Results] The paper suggests that perhaps a dumb tool doing an identifiable part of such a task may be better than an intelligent tool trying but failing in unidentifiable ways to do the entire task. [Contribution] Perhaps a new direction is needed in research for RE tools.\"",
        "Document: \"Exposing Tacit Knowledge via Pre-Requirements Tracing. Pre-requirements specification tracing concerns the identification and maintenance of relationships between requirements and the knowledge and information used by analysts to inform the requirements' formulation. Some of the knowledge used is tacit and therefore hard to identify and associate with requirements. This paper presents a tool for retrospectively identifying pre-requirements traces from requirements to their respective source material. We posit that poorly sourced requirements may indicate a requirement based on tacit knowledge. We present a preliminary evaluation of our tools performance using a case study.\"",
        "1 is \"Automatic Initiation of an Ontology\", 2 is \"WiSeKit: A Distributed Middleware to Support Application-Level Adaptation in Sensor Networks\"",
        "Given above information, for an author who has written the paper with the title \"Satisfying requirements for pervasive service compositions\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004458": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A credit manager for traffic regulation in high-speed networks: a queueing analysis':",
        "Document: \"On the Combination of Cooperative Diversity and Network Coding for Wireless Uplink Transmissions. Cooperative diversity has been recognized as an effective and low-cost technique to combat multipath fading and enhance transmission reliability. Motivated by the fact that many existing cooperative protocols suffer some loss of ergodic capacity, network coding, which is a technique that is well known for its capability to increase system throughput, is proposed in this paper to be combined with c...\"",
        "Document: \"Relay-aided interference alignment: Feasibility conditions and algorithm. Consider a (1 \u00d7 1, \u00bd)K symmetric wireless interference network where K single-antenna user-pairs want to achieve \u00bd degrees of freedom each. It has been proved that it is almost surely infeasible to achieve interference alignment without symbol extension. While it was proved relays can not increase the degree of freedom of wireless interference networks, we show this does not preclude the usefulness of using relays to construct practical solutions to approach interference alignment with finite symbol extensions. Feasibility conditions for relay-aided interference alignment are analyzed and an example about how to design the relaying functions to approach interference alignment is given. Simulation results also justify the use of relays as a practical means to do interference alignment with finite symbol extensions.\"",
        "Document: \"On the Application of Cooperative Transmission to Wireless Broadcast Channels. In this paper, we study the application of cooperative diversity to wireless broadcast channels, a fundamental building block of wireless communication networks. Several cooperative broadcast protocols will be proposed, and information theoretic metrics are developed to facilitate performance evaluation. Provided that there is no direct S-D link, the proposed protocols can achieve a multiplexing gain close to one, whereas the traditional two-hop scheme can only achieve the diversity gain 1/2. Provided that there are direct S-D links, the proposed protocol can still outperform the comparable scheme, particularly at high multiplexing gains.\"",
        "Document: \"On optimal monitor placement for localizing node failures via network tomography. We investigate the problem of placing monitors to localize node failures in a communication network from binary states (normal/failed) of end-to-end paths, under the assumption that a path is in normal state if and only if it contains no failed nodes. To uniquely localize failed nodes, the measurement paths must show different symptoms (path states) under different failure events. Our goal is to deploy the minimum set of monitors to satisfy this condition for a given probing mechanism. We consider three families of probing mechanisms, according to whether measurement paths are (i) arbitrarily controllable, (ii) controllable but cycle-free, or (iii) uncontrollable (i.e., determined by the default routing protocol). We first establish theoretical conditions that characterize network-wide failure identifiability through a per-node identifiability measure that can be efficiently evaluated for the above three probing mechanisms. Leveraging these results, we develop a generic monitor placement algorithm, applicable under any probing mechanism, that incrementally selects monitors to optimize the per-node measure. The proposed algorithm is shown to be optimal for probing mechanism\u00bf(i), and provides upper and lower bounds on the minimum number of monitors required by the other probing mechanisms. In the special case of single-node failures, we develop an improved monitor placement algorithm that is optimal for probing mechanism\u00bf(ii) and has linear time complexity. Using these algorithms, we study the impact of the probing mechanism on the number of monitors required for uniquely localizing node failures. Our results based on real network topologies show that although more complicated to implement, probing mechanisms that allow monitors to control measurement paths substantially reduce the required number of monitors.\"",
        "Document: \"A General Framework of Wiretap Channel With Helping Interference and State Information. This paper considers a general framework of the wiretap channel with helping interference and state information (WT-HI-SI), where a transmitter-receiver pair wishes to keep the message secret from a passive eavesdropper in the presence of an interferer and a random state. The interferer is to help the legitimate transceivers to enhance their security level, and the state information is available at the transmitter, but not at the eavesdropper. For the discrete memoryless WT-HI-SI, an achievable scheme is proposed by combining the noise forward scheme and the double binning coding scheme. Some previously proposed schemes can be viewed as special cases of the proposed scheme. Then, the achievable scheme is applied to two special channels, the Gaussian WT-HI-SI and the Gaussian WT-HI, respectively. For the Gaussian WT-HI-SI, there exists an external random state noncausally available to the transmitter in advance. But for the Gaussian WT-HI, there does not exist any external random state. In this case, we propose a novel achievable scheme that requires the transmitter to artificially generate the random state whose power can be adjusted adaptively according to dynamic channel conditions. Both the analytic and numerical results are provided to demonstrate that the use of the state information can generally improve the secrecy performance. A more important contribution of this paper is that even for the scenario where no external state information is available to the transmitter, the proposed scheme with the artificial state can still achieve a strictly larger secrecy rate in comparison with existing interference assisted schemes.\"",
        "1 is \"Request-Adaptive Packet Dissemination for Context-Aware Services in Vehicular Networks\", 2 is \"On the general defective channel with informed encoder and capacities of some constrained memories\"",
        "Given above information, for an author who has written the paper with the title \"A credit manager for traffic regulation in high-speed networks: a queueing analysis\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004463": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Multiversion locking protocol with freezing for secure real-time database systems':",
        "Document: \"On real-time databases: concurrency control and scheduling. In addition to maintaining database consistency as in conventional databases, real-time database systems must also handle transactions with timing constraints. While transaction response time and throughput are usually used to measure a conventional database system, the percentage of transactions satisfying the deadlines or a time-critical value function is often used to evaluate a real-time database system. Scheduling real-time transactions is far more complex than traditional real-time scheduling in the sense that (1) worst case execution times are typically hard to estimate, since not only CPU but also I/O requirement is involved; and (2) certain aspects of concurrency control may not integrate well with real-time scheduling. In this paper, we first develop a taxonomy of the underlying design space of concurrency control including the various techniques for achieving serializability and improving performance. This taxonomy provides us with a foundation for addressing the real-time issues. We then consider the integration of concurrency control with real-time requirements. The implications of using run policies to better utilize real-time scheduling in a database environment are examined. Finally, as timing constraints may be more important than data consistency in certain hard realtime database applications, we also discuss several approaches that explore the nonserializable semantics of real-time transactions to meet the hard deadlines\"",
        "Document: \"Prediction-Based QoS Management for Real-Time Data Streams. With the emergence of large wired and wireless sensor networks, many real-time applications need to operate on continuous unbounded data streams. At the same time, many of these systems have inherent timing constraints. Providing deadline guarantees for queries over dynamic data streams is a challenging problem due to bursty data stream arrival rates and time-varying stream contents. In this paper, we propose a prediction-based quality-of-service (QoS) management scheme for periodic queries over dynamic data streams. Our QoS management scheme features novel query workload estimators, which predict the query workload using execution time profiling and input data sampling, and adjusts the query QoS levels based on online query execution time prediction. We implement our QoS management algorithm on a real-time data stream query system prototype called RTStream. Our experimental evaluation of the scheme shows that our query workload estimator performs very well even with workload fluctuations and our QoS management scheme yields better overall system utility than the existing approaches for QoS management\"",
        "Document: \"An adaptable security manager for real-time transactions. The rising demand for real-time services over networks, such as Web-based information services, requires new approaches for balancing competing demands on limited resources. The BeeHive database system proposes a novel solution to this need by the use of adaptive real time, fault tolerance, quality of service and security services based on rules embedded in individual objects. These rules prescribe tradeoffs of alternate levels of service (and cost) when resource contention becomes a problem. The approach momentarily trades off the level of security to achieve the required real-time performance. In many situations, this is an acceptable, and even preferred, solution. We have developed an adaptable security manager to provide alternate levels of communications security to multiple users and to dynamically adapt to real-time performance conditions. In this paper, we present the design and evaluation of the proposed security manager that utilizes the notion of adaptable security services\"",
        "Document: \"Power- and time-aware buffer cache management for real-time embedded databases. Due to the explosive increases of data from both the cyber and physical worlds, the demand for database support in embedded systems is increasing. Databases for embedded systems, or embedded databases, are expected to provide timely in situ data services under various resource constraints, such as limited energy. However, traditional buffer cache management schemes, in which the primary goal is to minimize the number of I/O operations, is problematic since they do not consider the constraints of modern embedded devices such as limited energy and distinctive underlying storage. In particular, due to asymmetric read/write characteristics of flash memory-based storage of modern embedded devices, minimum buffer cache misses neither coincide with minimum power consumption nor minimum I/O deadline misses. In this paper we propose a novel power- and time-aware buffer cache management scheme for embedded databases. A novel multi-dimensional feedback control architecture is proposed and the characteristics of underlying storage of modern embedded devices is exploited for the simultaneous support of the desired I/O power consumption and the I/O deadline miss ratio. We have shown through an extensive simulation that our approach satisfies both power and timing requirements in I/O operations under a variety of workloads while consuming significantly smaller buffer space than baseline approaches.\"",
        "Document: \"Modeling and Analyzing Real-Time Data Streams. Achieving situation awareness is especially challenging for real-time data stream applications because they i) operate on continuous unbounded streams of data, and ii) have inherent real-time requirements. In this paper we show how formal data stream modeling and analysis can be used to better understand stream behavior, evaluate query costs, and improve application performance. We use MEDAL, a formal specification language based on Petri nets, to model the data stream queries and the Quality-of-Service (QoS) management mechanisms in a data stream system. MEDAL's ability to combine query logic and data admission control in one model allows us to design a single comprehensive model of the system. This model can be used to perform a large set of analyses to help improve the application's performance and QoS.\"",
        "1 is \"Parsing English with a Link Grammar\", 2 is \"Real-time garbage collection for flash-memory storage systems of real-time embedded systems\"",
        "Given above information, for an author who has written the paper with the title \"Multiversion locking protocol with freezing for secure real-time database systems\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004488": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Underdetermined blind source separation based on sparse representation':",
        "Document: \"Global robust stability and stabilization of Boolean network with disturbances. Based on semi-tensor product of matrices, global robust stability and stabilization of Boolean network (BN) with disturbances are investigated. Firstly, the definition of global robust stability for BN with disturbances is presented. By using the algebraic state space representation of disturbed BN, some necessary and sufficient criteria are obtained to ensure the global robust stability with respect to (w.r.t.) a fixed point or w.r.t. a limit cycle. In contrast, if a given disturbed BN is not globally robust stable w.r.t. a fixed point or w.r.t. a limit cycle, system can achieve stability by a matrix transformation technique. However, it is a difficult task to find such a suitable matrix transformation. In this paper, a pinning state feedback control design is proposed to find a suitable transformation. The obtained results are well illustrated by numerical example.\"",
        "Document: \"Brief Bezout identity related to reduced-order observer-based controllers for singular systems. A design method of causal reduced-order observer-based controllers for singular systems is presented. A new state-space representation of the Bezout identity for singular systems is developed. A parameterization of all causal reduced-order controllers is also given. The general results given here can be extended to various cases including minimal and full order observer-based controllers.\"",
        "Document: \"Stabilization of discrete-time stochastic systems via sliding mode technique. This paper considers the problem of sliding mode control for discrete-time stochastic systems with parameter uncertainties and state-dependent noise perturbation. An integral-like sliding surface is chosen and a discrete-time sliding mode controller is designed. The key feature in this work is that both the reachability of the quasi-sliding mode and the stability of system states are simultaneously analyzed, due to the existence of state-dependent noise perturbation. By utilizing an Lyapunov function involving system states and sliding mode variables, the sufficient condition for reachability is obtained. Finally, numerical simulation results are provided.\"",
        "Document: \"Underdetermined blind source separation based on sparse representation. This paper discusses underdetermined (i.e., with more sources than sensors) blind source separation (BSS) using a two-stage sparse representation approach. The first challenging task of this approach is to estimate precisely the unknown mixing matrix. In this paper, an algorithm for estimating the mixing matrix that can be viewed as an extension of the DUET and the TIFROM methods is first developed. Standard clustering algorithms (e.g., K-means method) also can be used for estimating the mixing matrix if the sources are sufficiently sparse. Compared with the DUET, the TIFROM methods, and standard clustering algorithms, with the authors' proposed method, a broader class of problems can be solved, because the required key condition on sparsity of the sources can be considerably relaxed. The second task of the two-stage approach is to estimate the source matrix using a standard linear programming algorithm. Another main contribution of the work described in this paper is the development of a recoverability analysis. After extending the results in , a necessary and sufficient condition for recoverability of a source vector is obtained. Based on this condition and various types of source sparsity, several probability inequalities and probability estimates for the recoverability issue are established. Finally, simulation results that illustrate the effectiveness of the theoretical results are presented.\"",
        "Document: \"Robust H\u221e control for a class of nonlinear discrete time-delay stochastic systems with missing measurements. This paper is concerned with the problem of robust H\"~ output feedback control for a class of uncertain discrete-time delayed nonlinear stochastic systems with missing measurements. The parameter uncertainties enter into all the system matrices, the time-varying delay is unknown with given low and upper bounds, the nonlinearities satisfy the sector conditions, and the missing measurements are described by a binary switching sequence that obeys a conditional probability distribution. The problem addressed is the design of an output feedback controller such that, for all admissible uncertainties, the resulting closed-loop system is exponentially stable in the mean square for the zero disturbance input and also achieves a prescribed H\"~ performance level. By using the Lyapunov method and stochastic analysis techniques, sufficient conditions are first derived to guarantee the existence of the desired controllers, and then the controller parameters are characterized in terms of linear matrix inequalities (LMIs). A numerical example is exploited to show the usefulness of the results obtained.\"",
        "1 is \"The Dual Voronoi Diagrams with Respect to Representational Bregman Divergences\", 2 is \"Finite-time stochastic stabilization for BAM neural networks with uncertainties\"",
        "Given above information, for an author who has written the paper with the title \"Underdetermined blind source separation based on sparse representation\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004491": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Knowledge and data engineering':",
        "Document: \"Discovery of Actionable Patterns in Databases: The Action Hierarchy Approach. An approach to defining actionability as a measure of interestingness of patterns is proposed. This approach is based on the concept of an action hierarchy which is defined as a tree of actions with patterns and pat- tern templates (data mining queries) assigned to its nodes. A method for discovering actionable patterns is presented and various techniques for optimizing the discovery process are proposed.\"",
        "Document: \"On Characterization and Discovery of Minimal Unexpected Patterns in Rule Discovery. A drawback of traditional data-mining methods is that they do not leverage prior knowledge of users. In prior work, we proposed a method that could discover unexpected patterns in data by using domain knowledge in a systematic manner. In this paper, we present new methods for discovering a minimal set of unexpected patterns by combining the two independent concepts of minimality and unexpectedness, both of which have been well-studied in the KDD literature. We demonstrate the strengths of this approach experimentally using a case study in a marketing domain.\"",
        "Document: \"Multidimensional Recommender Systems: A Data Warehousing Approach. In this paper, we present a new data-warehousing-based approach to recommender systems. In particular, we propose to extend traditional two-dimensional user/item recommender systems to support multiple dimensions, as well as comprehensive profiling and hierarchical aggregation (OLAP) capabilities. We also introduce a new recommendation query language RQL that can express complex recommendations taking into account the proposed extensions. We describe how these extensions are integrated into a framework that facilitates more flexible and comprehensive user interactions with recommender systems.\"",
        "Document: \"The long tail of recommender systems and how to leverage it. The paper studies the Long Tail problem of recommender systems when many items in the Long Tail have only few ratings, thus making it hard to use them in recommender systems. The approach presented in the paper splits the whole itemset into the head and the tail parts and clusters only the tail items. Then recommendations for the tail items are based on the ratings in these clusters and for the head items on the ratings of individual items. If such partition and clustering are done properly, we show that this reduces the recommendation error rates for the tail items, while maintaining reasonable computational performance.\"",
        "Document: \"Personalization technologies: a process-oriented perspective. By leveraging customer reactions to personalized products and services, companies continuously improve their personalization processes through an iterative feedback loop resulting in the `virtuous cycle' of personalization.\"",
        "1 is \"Towards a Pipelined Prolog Processor.\", 2 is \"Personalized active learning for collaborative filtering\"",
        "Given above information, for an author who has written the paper with the title \"Knowledge and data engineering\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004571": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Waterloo Exploration Database: New Challenges for Image Quality Assessment Models':",
        "Document: \"Polyview fusion: a strategy to enhance video-denoising algorithms. We propose a simple but effective strategy that aims to enhance the performance of existing video denoising algorithms, i.e., polyview fusion (PVF). The idea is to denoise the noisy video as a 3-D volume using a given base 2-D denoising algorithm but applied from multiple views (front, top, and side views). A fusion algorithm is then designed to merge the resulting multiple denoised videos into one, so that the visual quality of the fused video is improved. Extensive tests using a variety of base video-denoising algorithms show that the proposed PVF method leads to surprisingly significant and consistent gain in terms of both peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) performance, particularly at high noise levels, where the improvement over state-of-the-art denoising algorithms is often more than 2 dB in PSNR.\"",
        "Document: \"SSIM-inspired image restoration using sparse representation. Recently, sparse representation based methods have proven to be successful towards solving image restoration problems. The objective of these methods is to use sparsity prior of the underlying signal in terms of some dictionary and achieve optimal performance in terms of mean-squared error, a metric that has been widely criticized in the literature due to its poor performance as a visual quality predictor. In this work, we make one of the first attempts to employ structural similarity (SSIM) index, a more accurate perceptual image measure, by incorporating it into the framework of sparse signal representation and approximation. Specifically, the proposed optimization problem solves for coefficients with minimum \n\n\n\u2112\n\n\n0\n\n\nOpen image in new window norm and maximum SSIM index value. Furthermore, a gradient descent algorithm is developed to achieve SSIM-optimal compromise in combining the input and sparse dictionary reconstructed images. We demonstrate the performance of the proposed method by using image denoising and super-resolution methods as examples. Our experimental results show that the proposed SSIM-based sparse representation algorithm achieves better SSIM performance and better visual quality than the corresponding least square-based method.\"",
        "Document: \"Cooperation Enhancement for Message Transmission in VANETs. As one special case of the Mobile Ad Hoc Networks (MANET), vehicular ad-hoc networking (VANET) is featured by its high mobility and constantly changing topology. In VANET, nodes can work properly only if the participating vehicles cooperate with each other during communications. However, as a distributed network, individual vehicles might be non-cooperative for their own benefits. In order to prevent non-cooperative vehicles from tampering packet relaying in the network, we propose a cooperation enhancement mechanism using \"Neighborhood WatchDog\" to generate \"Trust Token\" based on the first-hand observation. Therefore, trust relationships and packet-acceptance decisions of the receiving nodes are based on the instant observation and the token-proved relaying behavior of the benign neighboring vehicles. With the inherit mapping between the Electronic ID of one vehicle and its public key, keys can be distributed on-the-fly. As a network layer solution, the cooperation enhancement mechanism proposed in this paper is built on the top of our previous proposed Media Access Control (MAC) protocol: Relative Position Based-MAC (RPB-MAC).\"",
        "Document: \"A Patch-Structure Representation Method for Quality Assessment of Contrast Changed Images. Contrast is a fundamental attribute of images that plays an important role in human visual perception of image quality. With numerous approaches proposed to enhance image contrast, much less work has been dedicated to automatic quality assessment of contrast changed images. Existing approaches rely on global statistics to estimate contrast quality. Here we propose a novel local patch-based objecti...\"",
        "Document: \"An Adaptive Linear System Framework For Image Distortion Analysis. We describe a framework for decomposing the distortion between two images into a linear combination of components. Unlike conventional linear bases such as those in Fourier or wavelet decompositions, a subset of the components in our representation are not fixed, but are adaptively computed from the input images. We show that this framework is a generalization of a number of existing image comparison approaches. As an example of a specific implementation, we select the components based on the structural similarity principle, separating the overall image distortions into non-structural distortions (those that do not change the structures of the objects in the scene) and the remaining structural distortions. We demonstrate that the resulting measure is effective in predicting image distortions as perceived by human observers.\"",
        "1 is \"Channel estimation techniques based on pilot arrangement in OFDM systems\", 2 is \"Interactive local adjustment of tonal values\"",
        "Given above information, for an author who has written the paper with the title \"Waterloo Exploration Database: New Challenges for Image Quality Assessment Models\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004631": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Formal Analysis of the Genetic Toggle':",
        "Document: \"A Big-Data Layered Architecture For Analyzing Molecular Communications Systems In Blood Vessels. We present a novel architecture for analyzing molecular communications systems in blood vessels for drug delivery and monitoring. This architecture leverages a big data platform for simultaneously using data produced by the existing simulation platforms, health records, and medical data acquisition systems. An included machine learning engine may provide useful insight for medical purposes.\"",
        "Document: \"Risk perception and disease spread on social networks. Social networks have become vital in the modeling of disease spread in populations, but so far these models have failed to take into account human risk perception: the fact that human beings, when aware of a disease outbreak, will take precautions to reduce their susceptibility. We study an existing model of risk perception on a social network and note the changes in the efficiency of the disease spread to previous studies that failed to take human behavior into account. We also discuss the accounts that need to be taken into consideration in the construction of accurate models of the population. (C) 2010 Published by Elsevier Ltd.\"",
        "Document: \"Osteoporosis: a multiscale modeling viewpoint. Our work focuses on bone remodeling with a multiscale breadth that ranges from modeling intracellular and intercellular RANK/RANKL signaling to tissue dynamics. Several important findings provide clear evidences of the multiscale properties of bone formation and of the links between RANK/RANKL and bone density in health and disease conditions. Recent studies indicate that the circulating levels of OPG and RANKL are inversely related to bone turnover and bone mineral density (BMD) and contribute to the development of osteoporosis in postmenopausal women, and thalassemia-induced osteoporosis. We make use of a spatial process algebra, the Shape Calculus, to control stochastic cell agents that are continuously remodeling the bone. We found that our description is effective for such a multiscale, multilevel process and that RANKL signaling small dynamic concentration defects are greatly amplified by the continuous alternation of absorption and formation resulting in large structural bone defects.\"",
        "Document: \"Bottleneck genes and community structure in the cell cycle network of S. pombe. The identification of cell cycle-related genes is still a difficult task, even for organisms with relatively few genes such as the fission yeast. Several gene expression studies have been published on S. pombe showing similarities but also discrepancies in their results. We introduce a network in which the weight of each link is a function of the phase difference between the expression peaks of two genes. The analysis of the stability of the clustering through the computation of an entropy parameter reveals a structure made of four clusters, the first one corresponding to a robustly connected M-G1 component, the second to genes in the S phase, and the third and fourth to two G2 components. They are separated by bottleneck structures that appear to correspond to cell cycle checkpoints. We identify a number of genes that are located on these bottlenecks. They represent a novel group of cell cycle regulatory genes. They all show interesting functions, and they are supposed to be involved in the regulation of the transition from one phase to the next. We therefore present a comparison of the available studies on the fission yeast cell cycle and a general statistical bioinformatics methodology to find bottlenecks and gene community structures based on recent developments in network theory.\"",
        "Document: \"Biological principles for future internet architecture design. Currently, a large number of activities on Internet redesign are being discussed in the research community. While today&#39;s Internet was initially planned as a datagram-oriented communication network among research facilities, it has grown and evolved to accommodate unexpected diversity in services and applications. For the future Internet this trend is anticipated to continue even more. Such develo...\"",
        "1 is \"Automated symbolic reachability analysis: with application to delta-notch signaling automata\", 2 is \"Collaborating as Normal: Detecting Systemic Anomalies in Your Partner.\"",
        "Given above information, for an author who has written the paper with the title \"Formal Analysis of the Genetic Toggle\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004665": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Nonparametric spectral analysis with missing data via the EM algorithm':",
        "Document: \"Differential space-code modulation for interference suppression. Space-time coding has been receiving much attention due to its potentials offered by fully exploiting the spatial and temporal diversities of multiple transmit and receive antennas. A differential space-time modulation (DSTM) scheme was previously proposed for demodulation without channel state information, which is attractive in fast fading channels where accurate channel estimates are difficult to obtain. However, this technique is sensitive to interference and is likely to deteriorate or even break down in a wireless environment, where interference (including intentional and unintentional jamming) signals exist. We propose a new coding and modulation scheme, referred to as the differential space-code modulation (DSCM), which is interference resistant. Our focus is on single-user communications. We show that DSCM outperforms DSTM significantly when interference is present. This advantage is achieved at the cost of a lower data rate or a wider bandwidth or a combination of both. To alleviate this problem, a high-rate DSCM (HR-DSCM) scheme is also presented, which increases the data rate considerably at the cost of a slightly higher bit-error rate (BER), while still maintaining the interference suppression capability\"",
        "Document: \"High resolution imaging for MIMO forward looking ground penetrating radar. Forward-Looking Ground Penetrating Radar (FLGPR) can be used for detecting landmines. The detection process involves generating synthetic aperture radar (SAR) images using the standard backprojection (BP). The BP approach suffers from poor resolution and high sidelobe problems. This paper focuses on enhancing imaging resolution and reducing sidelobes using the Sparse Iterative Covariance-based Estimation (SPICE). A MIMO FLGPR developed by the Army Research Laboratory (ARL) is used for analysis.\"",
        "Document: \"Automatic Target Recognition Using Discrimination Based On Optimal Transport. The use of distances based on optimal transportation has recently shown promise for discrimination of power spectra. In particular, spectral estimation methods based on l(1) regularization as well as covariance based methods can be shown to be robust with respect to such distances. These transportation distances provide a geometric framework where geodesics corresponds to smooth transition of spectral mass, and have been useful for tracking.In this paper we investigate the use of these distances for automatic target recognition. We study the use of the Monge-Kantorovich distance compared to the standard l(2) distance for classifying civilian vehicles based on SAR images. We use a version of the Monge-Kantorovich distance that applies also for the case where the spectra may have different total mass, and we formulate the optimization problem as a minimum flow problem that can be computed using efficient algorithms.\"",
        "Document: \"A soft detector with good performance/complexity trade-off for a MIMO system. We present a hybrid soft detector that has a good performance/complexity trade-off for a multiple-input multiple-output (MIMO) wireless communication system with known channel information. The new soft detector combines the merits of a simple unstructured least-squares (LS)-based soft detector and a list sphere decoder (LSD)-based soft detector for data bit detection. The former is computationally much more efficient than the latter at the cost of poorer performance. The poor performance of the former occurs mainly when the channel matrix is ill-conditioned. Whenever this happens, we use the LSD-based soft detector in the hybrid soft detector; otherwise, we use the LS-based one. Moreover, we provide a tight radius for a sphere decoder, a hard detector, via using the output of an LS-based hard detector. These two hard detectors are needed to determine if LS or LSD should be used in the hybrid soft detector. As an application example, we consider doubling the maximum data rate of the IEEE 802.11a conformable wireless local area networks by a MIMO system with two transmit and two receive antennas. For this application, the new soft detector is about 10 times faster than the LSD-based one and is about 10 times slower than the LS-based one. Yet the packet error rate due to using the new soft detector is quite close to that of using the LSD-based one.\"",
        "Document: \"Multiple-symbol double-differential detection based on least-squares and generalized-likelihood ratio criteria. Two algorithms for double-differential detection of multiple phase-shift keying modulation are proposed, based on a least-squares criterion and a generalized-likelihood ratio test, respectively. While both algorithms take advantage of the performance gain obtained by observing the received signal over an observation interval longer than that required for symbol-by-symbol detection, the former has the important advantage of reduced implementation complexity, whereas the latter offers better performance.\"",
        "1 is \"Capacity performance of multicell large-scale antenna systems.\", 2 is \"Minimum probability of error for asynchronous Gaussian multiple-access channels\"",
        "Given above information, for an author who has written the paper with the title \"Nonparametric spectral analysis with missing data via the EM algorithm\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004702": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Statistical Model of OFDMA Cellular Networks Uplink Interference Using Lognormal Distribution':",
        "Document: \"Position Uncertainties In Range-Free Wireless Sensor Network Localization. Evaluating position uncertainties is a fundamental problem of wireless sensor network localization. A constraint set, including both positive and negative constraints, is constructed to bound sensor position. By projecting the feasible region of this constraint set onto a 2D plane, the feasible scope of sensor position is computed to evaluate node position uncertainty. The projection result, called feasible geographic region (FGR), is approximated by its inner and outer polygon. The polygon approximation will converge to the actual FGR if we incrementally add more polygon vertices. A distributed algorithm is proposed to compute FGR. Finally, we study the impact of node position uncertainty upon a typical network application, target event detection. The feasible scope of target event position is computed even though the sensor position is not certain.\"",
        "Document: \"A measurement study of a massive multi-player online first person shooter game in play-station networks. Massive multi-player online games (MMOGs) have been attracting thousands of millions of participants on the Internet in the past decades. The newly emerging smart phones and game consoles together with PCs have enlarged the player base to an even larger scale. The increasing game traffic may generate significant real-time traffic across different ISP networks. In this paper, we conducted a measurement study of the traffic locality property of a popular online game, Call-of-Duty (CoD), which is a hybrid peer-to-peer (P2P) client/server massive multi-player online first person shooter (MMOFPS) game in the play station network (PSN) over the Internet. To facility our measurement, we designed and implemented a peer crawler over the PSN. Our instrumented crawler applies the principle of the ARP poisoning attack in our justified scenario so that our crawler is able to penetrate the PSN to harvest player's information successfully. We analyzed the measurement results for finer granularity at the autonomous system (AS) level compared with previous measurement studies. Our results show that the sessions in this CoD game are constructed with players' locality in mind. Nevertheless, optimized locality-aware game sessions are yet to be found. Insights obtained from this study may be valuable for the development and deployment of future P2P online gaming systems.\"",
        "Document: \"Adaptive congestion control for DSRC vehicle networks. Dedicated short range communications (DSRC) was proposed for collaborative safety applications (CSA) in vehicle communications. In this article we propose two adaptive congestion control schemes for DSRC-based CSA. A cross-layer design approach is used with congestion detection at the MAC layer and traffic rate control at the application layer. Simulation results show the effectiveness of the proposed rate control scheme for adapting to dynamic traffic loads.\"",
        "Document: \"A delay estimation approach in stochastic overlay networks. Overlay networks are resilient in transferring data through intermediate nodes. The dynamic stochastic shortest path (DSSP) can be utilized in overlay networks to find the optimized relay paths; however, DSSP depends on the link delay properties/states (i.e., delay distribution and delay average range). Nevertheless, it is difficult to acquire accurate link delay states to approximate the link characteristics due to possible measurement errors. In this paper, we first proposed a convenient DSSP estimation approach to approximate the link stochastic delay considering the tradeoff between the immediate delay and historical delay samples in stochastic overlay networks. Then, in order to evaluate the performance of DSSP, we conducted a comprehensive simulation study to compare DSSP with the shortest path computed using classic routing algorithms with the average delay values and delay errors due to the variation of delay distributions and updating intervals. The experiment results show that the proposed DSSP delay estimation method is more reliable and outperforms the conventional shortest path routing with the delay estimation using average delay and delay errors. In addition, we also proposed a refined heuristic K-shortest stochastic path routing algorithm using the proposed delay estimation method. In two typical overlay relay network scenarios, the simulation results show that the proposed stochastic routing algorithm outperforms the classic routing algorithms in reducing the average delay by 20% - 40% and the packet loss for nearly 50%.\"",
        "Document: \"An identity-based proxy ring signature scheme from bilinear pairings. We propose an identity-based proxy ring signature scheme from bilinear pairings which combines the advantages of proxy signature and of ring signature. Furthermore, our scheme can prevent the original signer form generating the proxy ring signature, thus the profits of the proxy signer are guaranteed. We introduce bilinear pairings to minimize the computation overhead and to improve the related performance of our scheme. As compared with Zhang's scheme, our scheme is a computational efficiency improvement for signature verification because the computational cost of bilinear pairings required is reduced form O(n) to O(1). In addition, the proxy ring signature presented in This work is signer ambiguous, nonforgeable, verifiable, nonrepudiable and identifiable.\"",
        "1 is \"Routing and link-layer protocols for multi-channel multi-interface ad hoc wireless networks\", 2 is \"DOMINO: Detecting MAC Layer Greedy Behavior in IEEE 802.11 Hotspots\"",
        "Given above information, for an author who has written the paper with the title \"Statistical Model of OFDMA Cellular Networks Uplink Interference Using Lognormal Distribution\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004787": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Wireless Physical Layer Security with Imperfect Channel State Information: A Survey':",
        "Document: \"Time-frequency analysis compensating missing data for Atrial Fibrillation ECG assessment. We propose a novel algorithm for temporal tracking of the fibrillatory frequency for Atrial Fibrillation (AF) ECG. Both Atrial activity extraction and fibrillatory frequency tracking are combined into a single algorithm and analytical expressions are derived for final time-frequency distributions, thus the proposed method is computationally efficient. It can also serve the general purpose of time-frequency analysis of a non-stationary signal with missing or corrupted data segments. The method uses short-time expansion of an orthogonal basis set, and regularized least squares solution used to compute a stable coefficients for the basis. A simplified and computationally efficient time-frequency distribution is derived based only on the coefficient vector. The algorithm is successfully applied to simulated as well as real ECG with AF, and its advantages over conventional average beat subtraction method are discussed.\"",
        "Document: \"Spatial Correlation For Correlated Scatterers. This paper investigates the correlations between sensor signals in multipath environments created by correlated scatterers. We derive a closed form expression for the correlation in fields created by arbitrary scatterer correlations and scatterer powers, using Fourier techniques, and propose reasonable function forms for scatterer correlation. Simulations show notable differences from the uncorrelated scatterer case.\"",
        "Document: \"Functional analysis guided approach for sound field reproduction with flexible loudspeaker layouts. This paper proposes a design of multiple circular and partial circular loudspeaker arrays for reproducing sound fields originated from a limited spatial region. We apply a functional analysis framework to formulate the sound field reproduction problem in closed form. Analytical solutions are derived for a circular secondary source arrangement, from which circular arc layouts are investigated and the design of placing multiple loudspeaker arrays over the limited region of interest is proposed. Such a design allows for non-spherical and non-uniform loudspeaker placement and thus provides flexibility to suit reproduction in real audio environments. The reproduction using the proposed method are illustrated by numerical simulations in comparison with the Least-squares based schemes.\"",
        "Document: \"Broadband DOA Estimation Using Sensor Arrays on Complex-Shaped Rigid Bodies. Sensor arrays mounted on complex-shaped rigid bodies are a common feature in many practical broadband direction of arrival (DOA) estimation applications. The scattering and reflections caused by these rigid bodies introduce complexity and diversity in the frequency domain of the channel transfer function, which presents several challenges to existing broadband DOA estimators. This paper presents a novel high resolution broadband DOA estimation technique based on signal subspace decomposition. We describe how broadband signals can be decomposed into narrow subband components, and combined such that the frequency domain diversity is retained. The DOA estimation performance is compared with existing techniques using a uniform circular array and a sensor array on a hypothetical rigid body. An improvement in closely spaced source resolution of up to 6 dB is observed for the sensor array on the hypothetical rigid body, in comparison to the uniform circular array. The results suggest that frequency domain diversity, introduced by complex-shaped rigid bodies, can provide higher resolution and clearer separation of closely spaced broadband sound sources.\"",
        "Document: \"3D exterior soundfield capture using pressure and gradient microphone array on 2D plane. Two-dimensional (2D) planar array of first order microphones have been utilised in spherical harmonic decomposition based interior soundfield analysis. This paper proposes an efficient method in designing two-dimensional planar arrays of first order microphones that are capable of completely capturing three-dimensional (3D) spatial exterior soundfields. First order microphones are utilised within the array for measurements of pressure gradients, allowing the microphone array to capture soundfield components that conventional planar omni-directional microphone arrays are unable to detect. While spherical microphone arrays are capable of detecting all soundfield components, they have drawbacks in feasibility due to their 3D geometric configuration. The proposed planar array of first-order microphone provides the same functionality as a large spherical array which needs to encompass all sound sources while having a scalable geometry that lies purely in a two-dimensional horizontal plane. Simulations show the accuracy and feasibility of the proposed microphone array design in capturing a fully developed exterior soundfield.\"",
        "1 is \"Cooperative diversity in wireless networks: Efficient protocols and outage behavior\", 2 is \"Ultra-wide bandwidth time-hopping spread-spectrum impulse radio for wireless multiple-access communications\"",
        "Given above information, for an author who has written the paper with the title \"Wireless Physical Layer Security with Imperfect Channel State Information: A Survey\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004813": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Configurable computing for high-security/high-performance ambient systems':",
        "Document: \"Reconfigurable Computing for Digital Signal Processing: A Survey. Steady advances in VLSI technology and design tools have extensively expanded the application domain of digital signal processing over the past decade. While application-specific integrated circuits (ASICs) and programmable digital signal processors (PDSPs) remain the implementation mechanisms of choice for many DSP applications, increasingly new system implementations based on reconfigurable computing are being considered. These flexible platforms, which offer the functional efficiency of hardware and the programmability of software, are quickly maturing as the logic capacity of programmable devices follows Moore's Law and advanced automated design techniques become available. As initial reconfigurable technologies have emerged, new academic and commercial efforts have been initiated to support power optimization, cost reduction, and enhanced run-time performance.This paper presents a survey of academic research and commercial development in reconfigurable computing for DSP systems over the past fifteen years. This work is placed in the context of other available DSP implementation media including ASICs and PDSPs to fully document the range of design choices available to system engineers. It is shown that while contemporary reconfigurable computing can be applied to a variety of DSP applications including video, audio, speech, and control, much work remains to realize its full potential. While individual implementations of PDSP, ASIC, and reconfigurable resources each offer distinct advantages, it is likely that integrated combinations of these technologies will provide more complete solutions.\"",
        "Document: \"Robust metastability-based TRNG design in nanometer CMOS with sub-vdd pre-charge and hybrid self-calibration. In this work, we study the impact of sub-vdd pre-charge operation of metastability-based True Random Number Generator (TRNG) and propose a hybrid self-calibration to improve the statistics of the TRNG in the presence increasing intra-die variation. Circuits designed in deep submicron technologies are susceptible to process variation. The variability may affect the circuit performance, power and reliability. Numerous pre-silicon design methodologies and post-silicon circuit tuning mechanisms have been studied in literature. We propose a sub-vdd pre-charge technique to improve the tolerance of the TRNG to device mismatch. This is followed by a hybrid self-detection and calibration technique based on algorithmic post processing and circuit tuning to mitigate the effects of variability. The cryptographic metric of `bit entropy' is used to validate the proposed techniques. The TRNG circuit and the proposed techniques are implemented using 45nm PDK. Results show that variation in fabrication process affect the reliability of TRNG circuits. Pre-charging the TRNG to 0.7V for a typical supply voltage of 1.1V reduces the impact of device mismatch on the circuit by 2X for device mismatch as large as 4-5%. The hybrid self-calibration further improves the bit entropy by ~120% across a range of 5% intra-die variation. The simple control logic has an estimated area of 128 um2 and results in a negligible energy overhead of 0.82 fJ/bit.\"",
        "Document: \"A VLSI systolic array architecture for Lempel-Ziv-based data compression. We present a parallel algorithm, architecture, and implementation for Lempel-Ziv-based data compression. The parallel algorithm exhibits a regular structure and is well suited for parallel VLSI array implementation. Based on our parallel algorithm, a word-parallel systolic array has been developed using systematic design methodologies. Compared to a recent systolic architecture, our array structure is substantially faster, with latency of N/2+M compared to 2N+M where M is the maximum allowable length of symbols to be encoded at each encoding step and N is the length of symbols in an encoding buffer which have already been encoded. Furthermore the architecture consumes significantly less area and has a faster clock rate\"",
        "Document: \"Synchro-Tokens: Eliminating Nondeterminism to Enable Chip-Level Test of Globally-Asynchronous Locally-Synchronous SoC's. Globally asynchronous locally synchronous (GALS) clocking applied to a system-on-a-chip (SoC) results in a design in which each core is a synchronous block (SB) of logic with a locally generated clock. Inter-core communication is asynchronous and controlled by wrapper logic around the cores. The nondeterministic synchronization used by most GALS architectures makes chip-level silicon debug and functional test difficult and costly. Deterministic GALS methodologies make dataflow assumptions which are only valid for a very limited set of applications. This paper describes a novel deterministic GALS methodology called \"synchro-tokens\" whose parameterized wrappers are flexible enough to be useful for a wide range of applications while supporting synchronous debug and test methodologies such as1149.1 and P1500. The validation of determinism, estimation of area overhead, and analysis of performance impact are detailed.\"",
        "Document: \"High-efficiency protection solution for off-chip memory in embedded systems. Abstract This paper proposes a complete hardware solution for embedded systems that fully protects o-chip,memory. Our security core is based on one-time pad (OTP) encryption and a CRC32 integrity check module. These modules safeguard external memories,for embedded,processors against a series of well-known attacks, including replay attacks, spoofing attacks and relocation attacks. The implementation limits memory,space overhead to about 18.75% and reduces memory,latency from 14 cycles for a alternate approach to 3 clock cycles. A FPGA-based implementation of the security core has been completed to gauge the security overhead and to compare our approach with existing solutions. Keywords: embedded systems, security, cryptography\"",
        "1 is \"An information-theoretic framework for optimal temperature sensor allocation and full-chip thermal monitoring\", 2 is \"The security and performance of the galois/counter mode (GCM) of operation\"",
        "Given above information, for an author who has written the paper with the title \"Configurable computing for high-security/high-performance ambient systems\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004894": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A hybrid approach to medical decision support systems: combining feature selection, fuzzy weighted pre-processing and AIRS.':",
        "Document: \"Ensemble adaptive network-based fuzzy inference system with weighted arithmetical mean and application to diagnosis of optic nerve disease from visual-evoked potential signals. This paper presents a new method based on combining principal component analysis (PCA) and adaptive network-based fuzzy inference system (ANFIS) to diagnose the optic nerve disease from visual-evoked potential (VEP) signals. The aim of this study is to improve the classification accuracy of ANFIS classifier on diagnosis of optic nerve disease from VEP signals. With this aim, a new classifier ensemble based on ANFIS and PCA is proposed.The VEP signals dataset include 61 healthy subjects and 68 patients suffered from optic nerve disease. First of all, the dimension of VEP signals dataset with 63 features has been reduced to 4 features using PCA. After applying PCA, ANFIS trained using three different training-testing datasets randomly with 50-50% training-testing partition.The obtained classification results from ANFIS trained separately with three different training-testing datasets are 96.87%, 98.43%, and 98.43%, respectively. And then the results of ANFIS trained with three different training-testing datasets randomly with 50-50% training-testing partition have been combined with three different ways including weighted arithmetical mean that proposed firstly by us, arithmetical mean, and geometrical mean. The classification results of ANFIS combined with three different ways are 98.43%, 100%, and 100%, respectively. Also, ensemble ANFIS has been compared with ANN ensemble. ANN ensemble obtained 98.43%, 100%, and 100% prediction accuracy with three different ways including arithmetical mean, geometrical mean and weighted arithmetical mean.These results have shown that the proposed classifier ensemble approach based on ANFIS trained with different train-test datasets and PCA has produced very promising results in the diagnosis of optic nerve disease from VEP signals.\"",
        "Document: \"Improved binary dragonfly optimization algorithm and wavelet packet based non-linear features for infant cry classification. \u2022This paper proposes a new feature set using wavelet packet transform based energy and non-linear entropies.\u2022Improved Binary Dragonfly optimization (IBDFO) based feature selection was proposed to select the most salient features.\u2022Several two-class and multi-class experiments have been successfully performed by means of this proposed method.\u2022We report very promising classification accuracies using the selected features by IBDFO for the dataset under test compared to the existing works in the literature.\"",
        "Document: \"Sleep spindles recognition system based on time and frequency domain features. Sleep spindle is the one of important components determining N-REM (Non-Rapid Eye Movement) stage 2 in the sleep stages. The symptoms of N-REM stage 2 are sleep spindle and K-complex and here sleep spindles are automatically recognized by using time and frequency domain features belonging to EEG (Electroencephalograph) signals obtained from three patient subjects. In this study, the proposed method consists of two steps. In the first step, six time domain features have been extracted from raw EEG signals. As for the extraction of frequency domain features from raw EEG signals, Welch spectral analysis has been used and applied to raw EEG signals. By this way, 65 frequency domain features have been extracted and reduced from 65 to 4 features by using statistical measures including minimum, maximum, standard deviation, and mean values. Three feature sets including only time domain, only frequency domain, and both time and frequency domain features have been used and the numbers of these feature sets are 6, 4, and 10, respectively. In the second step, artificial neural network (ANN) with LM (Levenberg-Marquardt) has been used to classify the sleep spindles evaluated beforehand by sleep expert physicians. The obtained classification accuracies for three features sets in the classification of sleep spindles are 100%, 56.86%, and 93.84% by using LM-ANN (for ten node in hidden layer). The obtained results have presented that the proposed recognition system could be confidently used in the automatic classification of sleep spindles.\"",
        "Document: \"A cascade learning system for classification of diabetes disease: Generalized Discriminant Analysis and Least Square Support Vector Machine. The aim of this study is to diagnosis of diabetes disease, which is one of the most important diseases in medical field using Generalized Discriminant Analysis (GDA) and Least Square Support Vector Machine (LS-SVM). Also, we proposed a new cascade learning system based on Generalized Discriminant Analysis and Least Square Support Vector Machine. The proposed system consists of two stages. The first stage, we have used Generalized Discriminant Analysis to discriminant feature variables between healthy and patient (diabetes) data as pre-processing process. The second stage, we have used LS-SVM in order to classification of diabetes dataset. While LS-SVM obtained 78.21% classification accuracy using 10-fold cross validation, the proposed system called GDA-LS-SVM obtained 82.05% classification accuracy using 10-fold cross validation. The robustness of the proposed system is examined using classification accuracy, k-fold cross-validation method and confusion matrix. The obtained classification accuracy is 82.05% and it is very promising compared to the previously reported classification techniques.\"",
        "Document: \"Automatic detection of respiratory arrests in OSA patients using PPG and machine learning techniques. Obstructive sleep apnea is a syndrome which is characterized by the decrease in air flow or respiratory arrest depending on upper respiratory tract obstructions recurring during sleep and often observed with the decrease in the oxygen saturation. The aim of this study was to determine the connection between the respiratory arrests and the photoplethysmography (PPG) signal in obstructive sleep apnea patients. Determination of this connection is important for the suggestion of using a new signal in diagnosis of the disease. Thirty-four time-domain features were extracted from the PPG signal in the study. The relation between these features and respiratory arrests was statistically investigated. The Mann\u2013Whitney U test was applied to reveal whether this relation was incidental or statistically significant, and 32 out of 34 features were found statistically significant. After this stage, the features of the PPG signal were classified with k-nearest neighbors classification algorithm, radial basis function neural network, probabilistic neural network, multilayer feedforward neural network (MLFFNN) and ensemble classification method. The output of the classifiers was considered as apnea and control (normal). When the classifier results were compared, the best performance was obtained with MLFFNN. Test accuracy rate is 97.07 % and kappa value is 0.93 for MLFFNN. It has been concluded with the results obtained that respiratory arrests can be recognized through the PPG signal and the PPG signal can be used for the diagnosis of OSA.\"",
        "1 is \"Comparison of the autoregressive modeling and fast Fourier transformation in demonstrating Doppler spectral waveform changes in the early phase of atherosclerosis.\", 2 is \"A note on the confinement problem\"",
        "Given above information, for an author who has written the paper with the title \"A hybrid approach to medical decision support systems: combining feature selection, fuzzy weighted pre-processing and AIRS.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004899": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Query Languages For Domain Specific Information From Ptf Astronomical Repository':",
        "Document: \"Hierarchical Dual-Net: A Flexible Interconnection Network and Its Routing Algorithm. In this paper, we propose a flexible interconnection network, called hierarchical dual-net (HDN), with low node degree and short diameter for constructing a supercomputer of large scale. The HDN is constructed based on a symmetric product graph (base network). A $\\bm{k}$-level hierarchical dual-net, HDN($\\bm{B,k,S}$), contains $\\bm{(2N_0)^{2^k}/(2\\times \\prod_{i=1}^{k}s_i)}$ nodes, where $\\bm{S=\\{s_i|1\\leq i\\leq k\\}}$ is the set of integers with each $\\bm{s_i}$ representing the number of nodes in a super-node at the level $\\bm{i}$ for $\\bm{1 \\leq i \\leq k}$, and $\\bm{N_0}$ is the number of nodes in the base network $\\bm{B}$. The node degree of HDN($\\bm{B,k,S}$) is $\\bm{d_0+k}$, where $\\bm{d_0}$ is the node degree of the base network. The benefit of the HDN is that we can select suitable $\\bm{s_i}$ to control the growing speed of the number of nodes for constructing a supercomputer of the desired scale. We investigate the topological properties of the HDN and compare them to that of other networks and give efficient routing and broadcasting algorithms for the hierarchical dual-net.\"",
        "Document: \"Prefix Computation and Sorting in Dual-Cube. In this paper, we describe two algorithmic techniques for the design of efficient algorithms in dual-cube. The first uses cluster structure of dual-cube, and the second uses recursive structure of the dual-cube. We propose efficient algorithms for parallel prefix computation and sorting in dual-cube based on the two techniques, respectively. For a dual-cube D_n with 2^{2n-1} nodes and n links per node, the communication and computation times of the algorithm for parallel prefix computation are at most 2n+1 and 4n-2, respectively; and those of the algorithm for sorting are at most 6n^2 and 2n^2, respectively.\"",
        "Document: \"Optimal Algorithms for Finding a Trunk on a Tree Network and its Applications. Given an edge-weighted tree T, a trunk is a path P in T which minimizes the sum of the distances of all vertices in T from P plus the weight of path P. In this paper, we give efficient algorithms for finding a trunk of T. The first algorithm is a sequential algorithm which runs in O(n) time, where n is the number of vertices in T. The second algorithm is a parallel algorithm which runs in O(log n) time using O(n/log n) processors on the EREW PRAM model. We present an application of trunk on mobile ad hoc networks for efficient multicasting.\"",
        "Document: \"Parallel prefix computation in the recursive dual-net. In this paper, we propose an efficient algorithm for parallel prefix computation in recursive dual-net, a newly proposed network The recursive dual-net RDNk(B) for k0 has ${(2n_0)^{2^k}/2}$ nodes and d0+k links per node, where n0 and d0 are the number of nodes and the node-degree of the base network B, respectively Assume that each node holds one data item, the communication and computation time complexities of the algorithm for parallel prefix computation in RDNk(B), k0, are ${2^{k+1}-2+2^k*T_{comm}(0)}$ and ${2^{k+1}-2+2^k*T_{comp}(0)}$, respectively, where Tcomm(0) and Tcomp(0) are the communication and computation time complexities of the algorithm for parallel prefix computation in the base network B, respectively.\"",
        "Document: \"Handling Domain Specific Document Repositories for Application of Query Languages. Domain specific information is increasingly available on the Web in form of document repositories. In specialized domains such as agriculture, bio-medical sciences and health-care, this information is required by various domain experts. Health-care experts such as researchers and practitioners require it during health-care delivery and for educational purposes. These users differ from the Web users and database users. Most of the existing document repositories on the Web have alphabetical and keyword based searches. These are not sufficient for the expert users with precise and complex queries, who require in-depth results within time constraints. Their information needs can be supported by providing user-level schema. Such a schema can support database-style high-level query languages over these repositories. Seeking specialized domain-specific information through queries is gaining importance. In this paper, a model for on-line document repositories is proposed. Queries can be performed with in-depth results. The model can be replicated to similarly structured document repositories in any given domain.\"",
        "1 is \"Pfair Scheduling of Generalized Pinwheel Task Systems\", 2 is \"TNet: a reliable system area network\"",
        "Given above information, for an author who has written the paper with the title \"Query Languages For Domain Specific Information From Ptf Astronomical Repository\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004897": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Secure communication in cellular networks: The benefits of millimeter wave mobile broadband':",
        "Document: \"Cognitive Single-Carrier Systems: Joint Impact of Multiple Licensed Transceivers. In this paper, the impact of interference from multiple licensed transceivers on cognitive underlay single-carrier systems is examined. Specifically, the situation is considered in which the secondary network is limited by three key parameters: 1) maximum transmit power at the secondary transmitter, 2) peak interference power at the primary receivers, and 3) interference power from the primary transmitters. For this cognitive underlay single-carrier system, the signal-to-interference ratio (SIR) of the secondary network is obtained for transmission over frequency-selective fading channels. Based on this, a new closed-form expression for the cumulative distribution function of the SIR is evaluated, from which the outage probability and the ergodic capacity are derived. Further insights are established by analyzing the asymptotic outage probability and the asymptotic ergodic capacity in the high-transmission-power regime. In particular, it is corroborated that the asymptotic outage diversity gain is equal to the multipath gain of the frequency-selective channel in the secondary network. The asymptotic ergodic capacity also gives new insight into the additional power cost for different network parameters while maintaining a specified target ergodic capacity. Illustrative numerical examples are presented to validate the outage probability and ergodic capacity under different interference power profiles.\"",
        "Document: \"Multi-pair amplify-and-forward relaying with very large antenna arrays. We consider a multi-pair relay channel where multiple sources simultaneously communicate with destinations using a relay. Each source or destination has only a single antenna, while the relay is equipped with a very large antenna array. We investigate the power efficiency of this system when maximum ratio combining/maximal ratio transmission (MRC/MRT) or zero-forcing (ZF) processing is used at the relay. Using a very large array, the transmit power of each source or relay (or both) can be made inversely proportional to the number of relay antennas while maintaining a given quality-of-service. At the same time, the achievable sum rate can be increased by a factor of the number of source-destination pairs. We show that when the number of antennas grows to infinity, the asymptotic achievable rates of MRC/MRT and ZF are the same if we scale the power at the sources. Depending on the large scale fading effect, MRC/MRT can outperform ZF or vice versa if we scale the power at the relay.\"",
        "Document: \"Scheduling performance enhancement by network coding in wireless mesh networks. When a wireless mesh network accommodates interactive applications with quality of service requirements, schedulebased protocols are more suitable than contention-based protocols. In this paper, the problem of determining an appropriate schedule assignment for multiple group transmissions within a spatial time division multiple access link scheduling network is referred to as an integrated multiple-group communication and link scheduling problem. A polynomial-time scheduling algorithm, designated as a source-parallel-aware assignment (SPAA), is proposed to increase the spatial utilisation within each time slot in order to enhance the network throughput. Furthermore, an advanced version of SPAA, designated as joint sourceparallel- aware assignment with network coding (JSANC), is proposed to reduce the effects of bottleneck paths on the schedule frame length by flexibly applying conventional or opportunistic network coding approaches. Simulation results show that the proposed algorithms achieve a better network throughput than existing flow-based or particular order-based scheduling schemes.\"",
        "Document: \"OSTBC Transmission in MIMO AF Relay Systems with Keyhole and Spatial Correlation Effects. In this paper, we investigate the degenerative effects of antenna correlation and keyhole on the performance of multiple-input multiple-output (MIMO) amplify-and-forward (AF) relay networks. In particular, we considered a downlink MIMO AF relay system consisting of an nS-antenna base station S, an nR-antenna relay station R, and an nD-antenna mobile station D, in which the signal propagation originated from the mobile station suffers from keyhole and spatial correlation effects. We have derived an exact expression for the moment generation function of the instantaneous signal-to-noise ratio which enables us to analyze the symbol error probability and outage probability of the considered system. We have shown that although the mobile station is in a poor scattering environment, i.e., keyhole, the relay channel (S \u2192 R \u2192 D link) still achieves a cooperative diversity order of min(nR, nD) provided that the channel from the source (S \u2192 R link) is keyhole-free. This result is important to radio system designers since under such a severe scenario it is unnecessary to deploy a large number of antennas on the relay station (nR) but only requires nR = nD to obtain the maximum achievable diversity gain.\"",
        "Document: \"Proactive Relay Selection with Joint Impact of Hardware Impairment and Co-channel Interference. In this paper, we investigate the end-to-end performance of dual-hop proactive decode-and-forward relaying networks with Nth best relay selection in the presence of two practical deleterious effects: i) hardware impairment and ii) cochannel interference. In particular, we derive new exact and asymptotic closed-form expressions for the outage probability and average channel capacity of Nth best partial and opportunistic relay selection schemes over Rayleigh fading channels. Insightful discussions are provided. It is shown that, when the system cannot select the best relay for cooperation, the partial relay selection scheme outperforms the opportunistic method under the impact of the same co-channel interference (CCI). In addition, without CCI but under the effect of hardware impairment, it is shown that both selection strategies have the same asymptotic channel capacity. Monte Carlo simulations are presented to corroborate our analysis.\"",
        "1 is \"Secrecy Wireless Information and Power Transfer With MISO Beamforming\", 2 is \"Downlink Rate Distribution in Heterogeneous Cellular Networks under Generalized Cell Selection\"",
        "Given above information, for an author who has written the paper with the title \"Secure communication in cellular networks: The benefits of millimeter wave mobile broadband\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004928": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Challenges And Futuristic Perspective Of Cdma Technologies: Occ-Cdma/Os For 4g Wireless Networks':",
        "Document: \"Two-stage hybrid decision feedback equalization for DS-CDMA systems. This letter proposes a hybrid decision-feedback equalizer (HDFE) for DS-CDMA systems. The proposed HDFE is carried out in two stages to improve the accuracy of the feedback signals by exploiting the spreading gain in the feedback filter. The spread signals for a symbol are buffered and then detected, after which re-spreading is applied to generate feedback signals at the chip level. Through the use of symbol detection for feedback filtering, more accurate feedback signals are achieved. Simulation results demonstrate the superior error performance of the proposed scheme over the frequency domain linear equalization (FD-LE) and RAKE schemes in highly dispersive channels.\"",
        "Document: \"Design and Analysis of an Interference Cancellation Algorithm for AF, DF and DMF Relay Protocol in Multiuser MIMO Scenario Based on the LTE-Advanced System. The analysis and design of relay protocols is a hot issue in 3GPP Long Term Evolution--Advanced. In this paper, we discuss interference cancellation in a multiuser MIMO environment using Amplify-and-Forward (AF), Decode-and-Forward (DF) and De-Modulate-and-Forward (DMF) as relay protocols, and using Thomilson Harashima Precoding and Dirty Paper Coding as precoding techniques, with Zero-Forcing, Minimum Mean Square Error, Successive Interference Cancellation and Ordered Successive Interference Cancellation detection techniques. By using a combination of classical precoding schemes and detection techniques with weighted matrix, we propose a new interference cancellation technique that is capable of cancelling interference. The interference cancellation is managed by AF, DF and DMF relay node protocols and the interference free codeword is transmitted to the selected User Equipment. The proposed algorithm when used with DMF protocol shows best performance, compared to the conventional system or the no-relay system case, it gives best performance. The observation results shows that DMF protocol gives the best results for BER and Throughput performance in a high interference environment.\"",
        "Document: \"New Relay Protocols with AMC Scheme for Throughput Enhancement in LTE-Advanced System. We propose an adaptive modulation and coding (AMC) scheme using relay protocols AF, DF and DMF. The AMC scheme is used for improving the throughput and reliability of a communication system, using different modulation and coding schemes. We analyze the performance of relay protocols with the AMC scheme and observe that relay protocols with the AMC scheme are capable of providing better average throughput at a lower signal to noise ratio (SNR) level as compared to the conventional scheme with no AMC. We perform Monte Carlo simulations based on 3GPP long term evolution-advanced parameters to prove the performance comparison of adaptive modulation and coding scheme (MCS) relay protocols with non-adaptive MCS relay protocols. The simulation results of the proposed system with adaptive MCS prove that among the amplify-and-forward (AF), decode-and-forward (DF) and de-modulate-and-forward (DMF), the DMF protocol performs best, at a lower SNR value and higher average throughput.\"",
        "Document: \"Distributed and control theoretic approach to intrusion detection. Ad hoc wireless networks are more vulnerable to malicious attacks than traditional wired networks due to the silent nature of these attacks and the inability of the conventional intrusion detection systems (IDS) to detect them. These attacks operate under the threshold boundaries during an intrusion attempt and can only be identified by profiling the complete system activity in relation to a normal behavior. In this paper we discuss a control-theoretic Hidden Markov Model (HMM) strategy for intrusion detection using distributed observations across multiple nodes. This model consists of a distributed HMM engine that executes in a randomly selected monitor node and functions as a part of the feedback control engine. This drives the defensive response based on hysteresis to reduce the frequency of false positives, thereby avoiding inappropriate ad hoc responses.\"",
        "Document: \"Self-Organization of Wireless Sensor Network for Autonomous Control in an IT Server Platform. Information technology systems face considerable challenges in seamless integration of telemetry and control information. These are essential to various autonomic management functions related to power, thermal, reliability, predictability, survivability, and adaptability. Sensors and control agents supporting this telemetry are a part of large multiprocessor environments that are scattered across the platform. The conventional approaches to support distributed observability and control using wired solutions are static, expensive, and non-scalable. We present an alternative approach for this unique environment that replaces static wired sensors with dynamically reconfigurable wireless sensors. It employs a genetic algorithm based approach to optimize sensor node function assignment, clustering decisions, resource distribution, and route establishment for improved control quality. Based on this new, wireless sensor network approach, we evaluate the average data-flow delay characteristics between sensor and control end-points. We also investigate the \"quality of control\" by measuring the conformity of the controlled objective to platform policy specifications (like power limits).\"",
        "1 is \"Temporal accountability and anonymity in medical sensor networks\", 2 is \"A fully distributed opportunistic network coding scheme for cellular relay networks\"",
        "Given above information, for an author who has written the paper with the title \"Challenges And Futuristic Perspective Of Cdma Technologies: Occ-Cdma/Os For 4g Wireless Networks\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004931": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Towards formal description and automatic generation of programming environments':",
        "Document: \"Multipath Key Exchange on P2P Networks. Distributed-hash-table (DHT) has been proposed to solve the problem of scaling for P2P networks. However, there are some problems of security in P2P networks. One of the problem is that key exchange is not performed preventing against endangering security. This reason may be the fact that P2P networks have no trusted server. Although certification and authentication are able to protect key exchange from spoofing and man-in-the-middle attacks, these cannot be applied because of that. Therefore, an attacker can easily compromise key exchange since P2P networks should accept any node whether it is malicious or not, and every node of P2P networks plays a role of router. Until today, for the above reasons, there is no way to exchange the secret key on P2P networks protecting against encountering security. In this paper, we will propose a key exchange method on P2P networks protecting against spoofing and man-in-the-middle attacks. For the purpose of protecting key exchange from encountering security, we will present the enhanced routings which are directional and probabilistic routings.\"",
        "Document: \"QOMB: A wireless network emulation testbed. In this paper we present QOMB, a testbed we designed and implemented for the evaluation of wireless network systems, protocols and applications. The testbed uses the wireless network emulation set of tools QOMET so as to reproduce in a wired network, in real time, the wireless network conditions corresponding to a given scenario. In this context QOMET also provides support for features such as realistic virtual 3D environments, and node mobility generation. The infrastructure of QOMB is StarBED, the large-scale network experiment testbed at the National Institute of Information and Communications Technology, Hokuriku Research Center, in Ishikawa, Japan. The multi-hop wireless network emulation experimental results related to OLSR performance analysis in mesh networks and MANETs illustrate the main features and the usability of QOMB.\"",
        "Document: \"Shared Data Management Mechanism for Distributed Software Development Based on a Reflective Object-Oriented Model. In this paper, we propose a new object-oriented mechanism to manage shared data in distributed software development with following features.\"",
        "Document: \"Performance analysis of IEEE 802.11 in multi-hop wireless networks. Multi-hop wireless networks provide a quick and easy way for networking when we need a temporary network or when cabling is difficult. The 802.11 Medium Access Control (MAC) plays an important role in the achievable system performance. There have been many studies on analytic modeling of single-hop 802.11 wireless networks but only a few on the analysis of multihop wireless networks. Furthermore, the object of these researches is an homogeneous ad-hoc wireless networks; therefore they are not appropriate for a network with structure such as wireless mesh networks. This paper introduces an analytic model of throughput performance for the IEEE 802.11 multi-hop networks, which allows us to compute the achievable throughput on a given path in multi-hop wireless networks. The model shows that there is an optimal point at which throughput is maximized. Using this model and a Markov model for modeling the operation of the IEEE 802.11 DCF we can determine the amount of data that each node should inject to the network to get the best throughput performance.\"",
        "Document: \"Educational environment on StarBED: case study of SOI Asia 2008 Spring Global E-Workshop. Hands-on experiences are indispensable to train IT operators. However, it is often difficult to conduct hands-on practices in many locations due to lack of resources such as PCs and network equipments. The cost of gathering lecturers and participants at one hands-on site is another possible problem in this situation. Utilization of remotely located hands-on environment is one of the solution to solve these problems. StarBED is a testbed facility for conducting network experiments. Installation of any OSes, adjusting the network topologies according to the requirements can be easily realized utilizing StarBED and SpringOS. A software suite, SpringOS, was developed exclusively for automating the experiment setups in StarBED. This facility could be used to address the problems of lack of resources and high cost for traveling. This research conducted a region-wide remote hands-on workshop utilizing the StarBED inviting 42 participants from 10 Asian countries. The workshop was designed so that the remote participants do not have to travel to single place, and prepare same specification PCs at each site. Through the collected data and feedback, the proposed workshop model was proved to be feasible and effective. This paper describes the requirements and approaches for region-wide remote hands-on workshop utilizing StarBED for building target hands-on environment. As the contents of the conducted workshop could be applied to the general IT human resource development programs, this paper could be useful for the future remote hands-on programs to train IT operators.\"",
        "1 is \"Car-level congestion and position estimation for railway trips using mobile phones\", 2 is \"Attributed Translations\"",
        "Given above information, for an author who has written the paper with the title \"Towards formal description and automatic generation of programming environments\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005001": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'BabyExp: Constructing a Huge Multimodal Resource to Acquire Commonsense Knowledge Like Children Do':",
        "Document: \"Spectral Moment Features Augmented by Low Order Cepstral Coefficients for Robust ASR. We propose a novel Automatic Speech Recognition (ASR) front-end, that consists of the first central Spectral Moment time-frequency distribution Augmented by low order Cepstral coefficients (SMAC). We prove that the first central spectral moment is proportional to the spectral derivative with respect to the filter's central frequency. Consequently, the spectral moment is an estimate of the frequency domain derivative of the speech spectrum. However information related to the entire speech spectrum, such as the energy and the spectral tilt, is not adequately modeled. We propose adding this information with few cepstral coefficients. Furthermore, we use a mel-spaced Gabor filterbank with 70% frequency overlap in order to overcome the sensitivity to pitch harmonics. The novel SMAC front-end was evaluated for the speech recognition task for a variety of recording conditions. The experimental results have shown that SMAC performs at least as well as the standard MFCC front-end in clean conditions, and significantly outperforms MFCCs in noisy conditions.\"",
        "Document: \"Robust AM-FM features for speech recognition. In this letter, a nonlinear AM-FM speech model is used to extract robust features for speech recognition. The proposed features measure the amount of amplitude and frequency modulation that exists in speech resonances and attempt to model aspects of the speech acoustic information that the commonly used linear source-filter model fails to capture. The robustness and discriminability of the AM-FM f...\"",
        "Document: \"A supervised approach to movie emotion tracking. In this paper, we present experiments on continuous time, continuous scale affective movie content recognition (emotion tracking). A major obstacle for emotion research has been the lack of appropriately annotated databases, limiting the potential for supervised algorithms. To that end we develop and present a database of movie affect, an notated in continuous time, on a continuous valence-arousal scale. Supervised learning methods are proposed to model the continuous affective response using hidden Markov Models (independent) in each dimension. These models classify each video frame into one of seven discrete categories (in each dimension); the discrete-valued curves are then converted to continuous values via spline interpolation. A variety of audio-visual features are investigated and an optimal feature set is selected. The potential of the method is experimentally verified on twelve 30-minute movie clips with good precision at a macroscopic level.\"",
        "Document: \"Detecting emotional state of a child in a conversational computer game. The automatic recognition of user's communicative style within a spoken dialog system framework, including the affective aspects, has received increased attention in the past few years. For dialog systems, it is important to know not only what was said but also how something was communicated, so that the system can engage the user in a richer and more natural interaction. This paper addresses the problem of automatically detecting ''frustration'', ''politeness'', and ''neutral'' attitudes from a child's speech communication cues, elicited in spontaneous dialog interactions with computer characters. Several information sources such as acoustic, lexical, and contextual features, as well as, their combinations are used for this purpose. The study is based on a Wizard-of-Oz dialog corpus of 103 children, 7-14 years of age, playing a voice activated computer game. Three-way classification experiments, as well as, pairwise classification between polite vs. others and frustrated vs. others were performed. Experimental results show that lexical information has more discriminative power than acoustic and contextual cues for detection of politeness, whereas context and acoustic features perform best for frustration detection. Furthermore, the fusion of acoustic, lexical and contextual information provided significantly better classification results. Results also showed that classification performance varies with age and gender. Specifically, for the ''politeness'' detection task, higher classification accuracy was achieved for females and 10-11 years-olds, compared to males and other age groups, respectively.\"",
        "Document: \"Audio salient event detection and summarization using audio and text modalities. This paper investigates the problem of audio event detection and summarization, building on previous work [1, 2] on the detection of perceptually important audio events based on saliency models. We lake a synergistic approach to audio summarization where saliency computation of audio streams is assisted by using the text modality as well. Auditory saliency is assessed by auditory and perceptual cues such as Teager energy, loudness and roughness; all known to correlate with attention and human hearing. Text analysis incorporates part-of-speech tagging and affective modeling. A computational method for the automatic correction of the boundaries of the selected audio events is applied creating summaries that consist not only of salient but also meaningful and semantically coherent events. A non-parametric classification technique is employed and results are reported on the MovSum movie database using objective evaluations against ground-truth designating the auditory and semantically salient events.\"",
        "1 is \"MIMIC: an adaptive mixed initiative spoken dialogue system for information queries\", 2 is \"BootCaT: Bootstrapping Corpora and Terms from the Web\"",
        "Given above information, for an author who has written the paper with the title \"BabyExp: Constructing a Huge Multimodal Resource to Acquire Commonsense Knowledge Like Children Do\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005089": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Scalable Proactive Event-Driven Decision Making':",
        "Document: \"Effect of Head-Mounted Displays on Posture. Objective: The aim of the present study was to determine if a wearable system based on a head-mounted display (HMD) causes users to alter their head position and adopt postures that place greater stress on the musculoskeletal system. Background: HMDs are common output devices used with wearable computers. HMDs provide the wearer with visual information by projecting computer-generated virtual images in front of the eyes. Deviations of neck posture from a neutral upright position increase the stresses on the musculoskeletal system of the head and neck. Method: Seven paramedics simulated the treatment of a patient under a normal condition and when using an HMD wearable computer system. During the simulations a posture analysis was performed using the Rapid Upper Limb Assessment method. Results: The postures adopted when wearing an HMD, as compared with a normal condition, scored significantly higher for the neck (z = 2.463, p < .05) and for overall body posture (left side of the body: z = 2.447, p < .05; right side of the body: z = 2.895, p < .05). Conclusion: Wearing an HMD can force the wearers to modify their neck posture. As such, the musculoskeletal system of the head and neck may be placed under increased levels of stress. Application: Potential users should be made aware that HMDs could dictate modifications in neck posture, which may have detrimental effects and may compound the weight effect of the HMD.\"",
        "Document: \"Distributed situation awareness in an Airborne Warning and Control System: application of novel ergonomics methodology. This paper applies a distributed theory of situation awareness based upon the analysis of interactions between agents (both human and non-human) in an Airborne Warning and Control System (Boeing E3D Sentry). The basic tenet of this approach is that agents within a system each hold their own component(s) of situation awareness, which may be very different from, but compatible with, other agent\u2019s view of the situation. However, it is argued that it is not always necessary to have complete sharing of this awareness, as different system agents have different purposes. Situation awareness is regarded as a dynamic and collaborative process that binds agents together on tasks on a moment-by-moment basis. Situation awareness is conceptualised as residing at a system, not an individual level. Data were collected from crew-members in theE3D during a series of simulated air battles. These data pertained to task structure, communications between the crew and the collection and analysis of crew actions at critical decision points. All phases of operations were considered. From these data propositional networks were developed in which key knowledge objects were identified. Analysis of these networks clearly shows how the location and nature of distributed situation awareness changes across agents with regard to the phase of operation/air battle.\"",
        "Document: \"Distributed cognition at the crime scene. The examination of a scene of crime provides both an interesting case study and analogy for consideration of Distributed Cognition. In this paper, Distribution is defined by the number of agents involved in the criminal justice process, and in terms of the relationship between a Crime Scene Examiner and the environment being searched.\"",
        "Document: \"A Cognitive Model of How People Make Decisions Through Interaction with Visual Displays. In this paper we report a cognitive model of how people make decisions through interaction. The model is based on the assumption that interaction for decision making is an example of a Partially Observable Markov Decision Process (POMDP) in which observations are made by limited perceptual systems that model human foveated vision and decisions are made by strategies that are adapted to the task. We illustrate the model by applying it to the task of determining whether to block a credit card given a number of variables including the location of a transaction, its amount, and the customer history. Each of these variables have a different validity and users may weight them accordingly. The model solves the POMDP by learning patterns of eye movements (strategies) adapted to different presentations of the data. We compare the model behavior to human performance on the credit card transaction task.\"",
        "Document: \"Creating and using interactive narratives: reading and writing branching comics. In this paper we describe the design and development of a multi-touch surface and software that challenges current approaches to the production and consumption of comics. Authorship of the comics involves drawing the 'top level' of the story directly onto paper and projecting lower-level narrative elements, such as objects, characters, dialogue, descriptions and/or events onto the paper via a multi-touch interface. In terms of the impact this has upon the experience of reading and writing, the implementation of paper is intended to facilitate the creation of high-level overviews of stories, while the touch surface allows users to generate branches through the addition of artifacts in accordance with certain theories about interactive narratives. This provides the opportunity to participate in the reading and authoring of both traditional, paper-based texts and interactive, digital scenarios. Prototype comics are used to demonstrate this approach to reading and writing top-level and low-level narratives.\"",
        "1 is \"A New Decomposition Method for Multiuser DC-Programming and Its Applications\", 2 is \"Modeling embodied visual behaviors\"",
        "Given above information, for an author who has written the paper with the title \"Scalable Proactive Event-Driven Decision Making\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005092": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Throughput Analysis for a Multi-User, Multi-Channel ALOHA Cognitive Radio System':",
        "Document: \"Multi-access radio resource management using multi-agent system. Coexistence of heterogeneous networks such as cellular, wireless local area network (WLAN), ultra-wideband (UWB) etc. brings new challenges. It is perceived that current radio resource management mechanism cannot meet the requirements of multi-radio access technologies (multi-RATs). This paper proposes a novel intelligent multi-agent radio resource management system, which is self-organized and distributed to ensure the coexistence of multi-RATs. Radio resource is managed by a macro control and management system using control factors and validation mechanism, instead of micro control for individual users. The goal is to increase radio resource utilization efficiency, maximize system capacity and meet the QoS requirements of different services\"",
        "Document: \"Adaptive routing considering the state of receiver for Ad hoc networks. Routing in MANET (Mobile Ad Hoc Network) is a challenging due to the dynamic nature of network topology and the resource constraints. To maximize the channel resource utilization and minimize the network transfer delay along the path, the shortest path with minimum hops scheme is often adopted. However, the quality of wireless channel among the mobile nodes is time varying due to fading, shadowing and pathloss. Considering adaptive channel coding and modulation scheme, the channel states can be characterized by different link throughput. If routing selection based on the link throughput is implemented, the minimum transfer delay from source to destination and the maximal throughput may be obtained. In our previous research, a channel adaptive shortest routing CASPR was presented, but it didn't consider receiver's state. In this paper, we take the condition of receiver into account, and present an adaptive routing (BR-CASPR). Simulation results show that if there is a proper weight between the channel quality and delay factor in total cost, the delay performance may be improved compared to CASPR.\"",
        "Document: \"Linear Precoding Design in Multi-User Cognitive MIMO Systems with Cooperative Feedback. In this letter, we present a linear precoder design scheme for a cognitive (CR) multi-input multi-output (MIMO) system, where a secondary broadcast channel uses the same spectrum with multiple primary users (PUs). Considering both cooperative feedback from the PUs and local feedback from the secondary users (SUs) to the secondary transmitter (ST), the ST can obtain the quantized information regarding the direction of the channels. The proposed scheme is based on the minimum mean square error (MMSE) criteria and is robust to the channel uncertainties caused by quantization error. The derivation is given under two different levels of interference constraints respectively. Simulation results show the effectiveness of the proposed precoder design scheme. \u00a9 2012 IEEE.\"",
        "Document: \"Analysis of Improved Cyclostationary Detector with Multiple Antennas over Fading Channels. A comprehensive performance analysis of the multi-cycle cyclostationary (MC) detection-based spectrum sensing over fading channels with multiple independent and correlated antennas is developed. We first proposed an improved MC detector, aiming to reduce the computational complexity of the conventional one. Compared with conventional MC detector, the proposed method is low-computational complexity and high-accuracy on sensing performance. Based on the proposed MC detector, for the multiple independent antennas case, the average detection probability by employing square-law combining (SLC) is derived for several fading channels such as Nakagami, Rayleigh and Rician by using the moment generation function (MGF) approach. For multiple correlated antenna case, with Nakagami fading and SLC scheme, expressions of detection probability are derived by the same approach as it in the independent antennas case. Special cases of a linear array of 2 and 4 arbitrarily correlated antennas are treated. Finally, illustrative and analytical results show that the reliability of our proposed MC detector and the degradation of sensing performance over correlation and fading. \u00a9 2013 ACADEMY PUBLISHER.\"",
        "Document: \"Topology Reconfiguration in Cognitive Radio Networks Using Ant Colony Optimization. Considering the inevitable trends for heterogeneous network convergence and self-adaptation ability, Cognitive Radio Network (CRN) concept has been proposed with some essential characteristics to achieve global end-to-end goals. CRNs are composed of cognitive devices which have the capable of changing network configurations based on the dynamic environment. This capability opens up the possibility of designing flexible and dynamic topology control strategies with the purpose of opportunistically reusing idle licensed spectrum and effectively achieving data transmission. This work focuses on the problem of designing effective topology reconfiguration algorithm to offer optimal routing solutions. We analyze the topology control for CRNs finding the topology problem can be formularized as a multi-objective optimization problem. In this context, as an intelligent technology for complex multi-objective optimization, the ant colony optimization (ACO) techniques are applied in topology reconfiguration for optimal decision making in this paper. Finally, the reconfiguration algorithm is simulated, and the simulation results with detailed are analyzed.\"",
        "1 is \"SIP: Session Initiation Protocol\", 2 is \"On the performance of MIMO RFID backscattering channels.\"",
        "Given above information, for an author who has written the paper with the title \"Throughput Analysis for a Multi-User, Multi-Channel ALOHA Cognitive Radio System\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005111": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Multi-hop cooperative transmission using fountain codes over Rayleigh fading channels':",
        "Document: \"Energy efficient cooperative LEACH protocol for wireless sensor networks. We develop a low complexity cooperative diversity protocol for low energy adaptive clustering hierarchy (LEACH) based wireless sensor networks. A cross layer approach is used to obtain spatial diversity in the physical layer. In this paper, a simple modification in clustering algorithm of the LEACH protocol is proposed to exploit virtual multiple-input multiple-output (MIMO) based user cooperation. In lieu of selecting a single cluster-head at network layer, we proposed M cluster-heads in each cluster to obtain a diversity order of M in long distance communication. Due to the broadcast nature of wireless transmission, cluster-heads are able to receive data from sensor nodes at the same time. This fact ensures the synchronization required to implement a virtual MIMO based space time block code (STBC) in cluster-head to sink node transmission. An analytical method to evaluate the energy consumption based on BER curve is presented. Analysis and simulation results show that proposed cooperative LEACH protocol can save a huge amount of energy over LEACH protocol with same data rate, bit error rate, delay and bandwidth requirements. Moreover, this proposal can achieve higher order diversity with improved spectral efficiency compared to other virtual MIMO based protocols.\"",
        "Document: \"An Approach Of Relay Ordering To Improve Ofdm-Based Cooperation. Multi-hop cooperative communication has been investigated in order to overcome disadvantages such as fading, obstruction and low power. In addition, with the goal of increasing access capacity, the orthogonal frequency division multiplexing (OFDM) modulation is being advanced as a solution. In this paper, we propose the approach of relay ordering in a Decode-and-Forward OFDM scheme. Combining techniques such as maximal ratio combining and selection combining are employed at receivers and approximate outage capacity probabilities are derived for evaluating system performance over frequency selective Rayleigh fading channels. Final, the expressions are validated by Monte-Carlo simulations, and are used to compare with the same scheme based relay selection.\"",
        "Document: \"Secondary Spectrum Access in Cognitive Radio Networks Using Rateless Codes over Rayleigh Fading Channels. In this paper, we propose secondary relaying schemes in cognitive spectrum leasing. In the proposed protocols, a primary transmitter uses rateless code to transmit its data to a primary receiver. In the secondary network, $$M$$ M secondary transmitters are ready to help the primary transmitter forward the data to a primary receiver so that they can find opportunities to transmit their data. For performance evaluation, we derive the average outage probability, the average number of encoded packets transmitted by the primary transmitter, the average number of remaining time slots for secondary network and the average capacity of the secondary network over Rayleigh fading channels. Various Monte-Carlo simulations are presented to verify the derivations.\"",
        "Document: \"Exact Outage Probability of Two-Way Decode-and-Forward Scheme with Opportunistic Relay Selection Under Physical Layer Security. The combination of cooperative communication and physical layer security is an effective approach to overcome the disadvantages of the fading environment as well as to increase the security capacity of the wireless network. In this paper, we propose a two-way decode-and-forward scheme with a relay selection method. In the proposed protocol, two source nodes communicate with each other with the help of intermediate relays under the eavesdropping of another wireless node, called the eavesdropper node. The best relay, which is chosen by the max---min strategy, uses digital network coding to enhance secure communication and spectrum use efficiency. The system performance is analyzed and evaluated in terms of the exact closed-form outage probability over Rayleigh fading channels. The Monte-Carlo simulation results are presented to verify the theoretical analysis. Finally, the proposed protocol is compared with the two-way protocol, which does not use digital network coding.\"",
        "Document: \"Outage Analysis Of Cognitive Spectrum Sharing For Two-Way Relaying Schemes With Opportunistic Relay Selection Over I.N.I.D. Rayleigh Fading Channels. In this letter, we analyze the outage performance of cognitive spectrum sharing in two-way relaying systems. We derive expressions of outage probability for the primary and secondary network over independent but not necessarily identically distributed (i.n.i.d.) Rayleigh fading channels. Monte Carlo simulations are presented to verify the theoretical analyses.\"",
        "1 is \"Investigation of Interference Margin for the Co-existence of Macrocell and Femtocell in Orthogonal Frequency Division Multiple Access Systems\", 2 is \"OFDMA Based Two-Hop Cooperative Relay Network Resources Allocation\"",
        "Given above information, for an author who has written the paper with the title \"Multi-hop cooperative transmission using fountain codes over Rayleigh fading channels\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005133": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Rogue access point detection using segmental TCP jitter':",
        "Document: \"Microbiome dynamics analysis using a novel multivariate vector autoregression model with weighted fusion regularization. In recent years, there are growing interests in developing novel approaches for inferring dynamic interactions in biological systems including gene transcription network and microbial interaction networks. Multivariate Vector Autoregression (MVAR) model is one of these efficient methods. Variants of MVAR with different penalties or regularizations can avoid the problem of over-fitting and provide great potential in high-dimensional data analysis. In this paper, we developed a novel regularization methods for MVAR via weighted fusion which consider the correlation among variables. The weighted fusion can potentially incorporate information redundancy among correlated variables for estimation and variable selection. Weighted fusion is also useful when the number of predictors p is larger than the number of observations n. In theory, we discuss the grouping effect of weighted fusion regularization for linear models. We then apply the proposed model on several time series data sets especially a time series dataset of human gut microbiomes. The experimental results indicate that the new approach has better performance that several other VAR-based models and we demonstrate its capability of extracting relevant microbial interactions.\"",
        "Document: \"Semi-automatic hot event detection. In this paper, we propose a method to detect hot event automatically. We use all the web pages from Jan 1st 2005 to Dec 31st 2005, and detect new events by using incremental TF-IDF model and incremental cluster algorithm. Based on analysis of the attributes of events, we propose a method to measure the activity of events, then filter and sort the event according to the activity of events; finally a hot event list can be derived.\"",
        "Document: \"Modeling semantic relations between visual attributes and object categories via dirichlet forest prior. In this paper, we deal with two research issues: the automation of visual attribute identification and semantic relation learning between visual attributes and object categories. The contribution is two-fold, firstly, we provide uniform framework to reliably extract both categorical attributes and depictive attributes. Secondly, we incorporate the obtained semantic associations between visual attributes and object categories into a text-based topic model and extract descriptive latent topics from external textual knowledge sources. Specifically, we show that in mining natural language descriptions from external knowledge sources, the relation between semantic visual attributes and object categories can be encoded as Must-Links and Cannot-Links, which can be represented by Dirichlet-Forest prior. To alleviate the workload of manual supervision and labeling in image categorization process, we introduce a semi-supervised training framework using soft-margin semi-supervised SVM classifier. We also show that the large-scale image categorization results can be significantly improved by combining automatically acquired visual attributes. Experimental results show that the proposed model achieves better ability in describing object-related attributes and makes the inferred latent topics more descriptive.\"",
        "Document: \"Affine Distortion Compensation for an Isolated Online Handwritten Chinese Character Using Combined Orientation Estimation and HMM-Based Minimax Classification. This paper presents a new approach to compensating affine distortion of an isolated online handwritten Chinese character. The input sample is first analyzed by using a character-structure-guided orientation estimation approach. If necessary, the orientation hypotheses are refined based on confidence evaluation of two pre-classifiers. Depending on the number of possible orientations, an HMM-based minimax classification approach is then used to estimate an affine transformation against either the original sample or the compensated sample with the previously identified orientation. The final compensated sample can be derived accordingly using the estimated affine transformation. The effectiveness of the proposed approach is demonstrated by recognition experiments using distorted samples generated artificially from the popular Nakayosi and Kuchibue Japanese character databases.\"",
        "Document: \"Incorporating Hierarchical Dirichlet Process into Tag Topic Model. The Latent Dirichlet Allocation (LDA) is a parametric approach and the number of topics must be predefined. So it is natural to try to capture uncertainty regarding the number of topics. This paper proposes a Tag Hierarchical Dirichlet Process (THDP) that automatically infers the number of topics while also leveraging the tag information associated with each document. In this model, we assume that an author is clear in his mind that the content will contains which aspects and for each aspect he will choose a tag to describe it, and then we consider problems involving groups of tag, where each tag within a group is a draw from a mixture model and it is desirable to share topic between groups. In this setting it is natural to consider Hierarchical Dirichlet Process, Where the well-known clustering property of the Dirichlet process provides a nonparametric prior for the number of topic within each tag. Experimental results on corpora demonstrate superior performance over the THDP model. \u00a9 2013 Springer-Verlag.\"",
        "1 is \"Understanding the Characteristics of Internet Short Video Sharing: A YouTube-Based Measurement Study\", 2 is \"Social tag prediction\"",
        "Given above information, for an author who has written the paper with the title \"Rogue access point detection using segmental TCP jitter\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005176": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Benchmarks for robotic soccer vision':",
        "Document: \"Interactive Learning of Continuous Actions from Corrective Advice Communicated by Humans. An interactive learning framework that allows non-expert humans to shape a policy through corrective advice, using a binary signal in the action domain of the robot/agent, is proposed. One of the most innovative features of COACH COrrective Advice Communicated by Humans, the proposed framework, is a mechanism for adaptively adjusting the amount of human feedback that a given action receives, taking into consideration past feedback. The performance of COACH is compared with the one of TAMER Teaching an Agent Manually via Evaluative Reinforcement, ACTAMER Actor-Critic TAMER, and an autonomous agent trained using SARSA\u03bb in two reinforcement learning problems: ball dribbling and Cart-Pole balancing. COACH outperforms the other learning frameworks in the reported experiments. In addition, results show that COACH is able to transfer successfully human knowledge to agents with continuous actions, being a complementary approach to TAMER, which is appropriate for teaching in discrete action domains.\"",
        "Document: \"A portable ground-truth system based on a laser sensor. State estimation is of crucial importance to mobile robotics since it determines in a great measure its ability to model the world from noisy observations. In order to quantitatively evaluate state-estimation methods, the availability of ground-truth data is essential since it provides a target that the result of the state-estimation methods should approximate. Most of the reported ground-truth systems require a complex assembly which limit their applicability and make their set-up long and complicated. Furthermore, they often require a long calibration procedure. Additionally, they do not present measures of their accuracy. This paper proposes a portable laser-based ground-truth system. The proposed system can be easily ported from one environment to other and requires almost no calibration. Quantitative results are presented with the purpose of encouraging future comparisons among different groundtruth systems. The presented method has shown to be accurate enough to evaluate state-estimation methods and works in real time.\"",
        "Document: \"Offline Signature Verification Using Local Interest Points and Descriptors. In this article, a new approach to offline signature verification, based on a general-purpose wide baseline matching methodology, is proposed. Instead of detecting and matching geometric, signature-dependent features, as it is usually done, in the proposed approach local interest points are detected in the signature images, then local descriptors are computed in the neighborhood of these points, and afterwards these descriptors are compared using local and global matching procedures. The final verification is carried out using a Bayes classifier. It is important to remark that the local interest points do not correspond to any signature-dependent fiducial point, but to local maxima in a scale-space representation of the signature images. The proposed system is validated using the GPDS signature database, where it achieves a FRR of 16.4% and a FAR of 14.2%.\"",
        "Document: \"A new neural network model for automatic generation of Gabor-like feature filters. The automatic selection of feature variables is a task of increasing interest in the field of pattern recognition. Neural models have recently been used for this purpose. Among other models, the adaptive-subspace SOM (ASSOM) stands out because of its simplicity and biological plausibility. However, the main drawback of its application in image processing systems is that a priori information is necessary to choose a suitable network size and topology in advance. This article introduces the adaptive-subspace growing cell structures (ASGCS) network, which corresponds to a further improvement of the ASSOM that overcomes its main drawbacks. The ASGCS network is described and some examples of automatic generation of Gabor-like feature filter are given\"",
        "Document: \"A Neural Architecture for Preattentive Segmentation of Sewage Pipes Video Images. This article describes a neural architecture for real time preattentive segmentation of sewage pipes video images, whose mechanisms are based on the mammalian early visual system. The architecture corresponds to a modified and simplified version of the Boundary Contour System tuned to take advantage of the circular symmetric characteristics of the pipes images. Remarkable aspects of the proposed architecture are the application of a spatial complex logarithmic mapping stage, and the use of cooperative receptive fields with noncollinear branches.\"",
        "1 is \"Cooperative behaviors in multi-robot systems through implicit communication\", 2 is \"Caffe con Troll: Shallow Ideas to Speed Up Deep Learning\"",
        "Given above information, for an author who has written the paper with the title \"Benchmarks for robotic soccer vision\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005189": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Integrating Heterogeneous Web Services with WebXcript':",
        "Document: \"Incremental consistency checking for pervasive context. Applications in pervasive computing are typically required to interact seamlessly with their changing environments. To provide users with smart computational services, these applications must be aware of incessant context changes in their environments and adjust their behaviors accordingly. As these environments are highly dynamic and noisy, context changes thus acquired could be obsolete, corrupted or inaccurate. This gives rise to the problem of context inconsistency, which must be timely detected in order to prevent applications from behaving anomalously. In this paper, we propose a formal model of incremental consistency checking for pervasive contexts. Based on this model, we further propose an efficient checking algorithm to detect inconsistent contexts. The performance of the algorithm and its advantages over conventional checking techniques are evaluated experimentally using Cabot middleware.\"",
        "Document: \"Checking safety properties using compositional reachability analysis. The software architecture of a distributed program can be represented by a hierarchical composition of subsystems, with interacting processes at the leaves of the hierarchy. Compositional reachability analysis (CRA) is a promising state reduction technique which can be automated and used in stages to derive the overall behavior of a distributed program based on its architecture. CRA is particularly suitable for the analysis of programs that are subject to evolutionary change. When a program evolves, only the behaviors of those subsystems affected by the change need be reevaluated. The technique however has a limitation. The properties available for analysis are constrained by the set of actions that remain globally observable. Properties involving actions encapsulated by subsystems may therefore not be analyzed. In this article, we enhance the CRA technique to check safety properties which may contain actions that are not globally observable. To achieve this, the state machine model is augmented with a special trap state labeled as &pgr;. We propose a scheme to transform, in stages, a property that involves hidden actions to one that involves only globally observable actions. The enhanced technique also includes a mechanism aiming at reducing the debugging effort. The technique is illustrated using a gas station system example.\"",
        "Document: \"Pivot: learning API-device correlations to facilitate Android compatibility issue detection. The heavily fragmented Android ecosystem has induced various compatibility issues in Android apps. The search space for such fragmentation-induced compatibility issues (FIC issues) is huge, comprising three dimensions: device models, Android OS versions, and Android APIs. FIC issues, especially those arising from device models, evolve quickly with the frequent release of new device models to the market. As a result, an automated technique is desired to maintain timely knowledge of such FIC issues, which are mostly undocumented. In this paper, we propose such a technique, PIVOT, that automatically learns API-device correlations of FIC issues from existing Android apps. PIVOT extracts and prioritizes API-device correlations from a given corpus of Android apps. We evaluated PIVOT with popular Android apps on Google Play. Evaluation results show that PIVOT can effectively prioritize valid API-device correlations for app corpora collected at different time. Leveraging the knowledge in the learned API-device correlations, we further conducted a case study and successfully uncovered ten previously-undetected FIC issues in open-source Android apps.\n\n\"",
        "Document: \"Fault-Based Testing of Database Application Programs with Conceptual Data Model. Database application programs typically contain program units that use SQL statements to manipulate records in database instances. Testing the correctness of data manipulation by these programs is challenging. When a tester provides a database instance to test such a program, the program unit may output faulty SQL statements and, hence, manipulate inappropriate database records. Nonetheless, these failures may only be revealed in very specific database instances. This paper proposes to integrate SQL statements and the conceptual data models of an application for faultbased testing. It proposes a set of mutation operators based on the standard types of constraint used in the enhanced entity-relationship model. These operators are semantic in nature. This semantic information guides the construction of affected attributes and join conditions of mutants. The usefulness of our proposal is illustrated by an example in which a missing-record fault is revealed.\"",
        "Document: \"Symbolic state validation through runtime data. Real world programs are typically built on top of many library functions. Symbolic analysis of these programs generally requires precise models of these functions? Application Programming Interfaces (APIs), which are mostly unavailable because these models are costly to construct. A variant approach of symbolic analysis is to over-approximate the return values of those APIs that have not been modeled. However, such approximation can induce many unreachable symbolic states, which are expensive to validate manually. In this paper, we propose a static approach to automatically validating the reported anomalous symbolic states. The validation makes use of the available runtime data of the un-modeled APIs collected from previous program executions. We show that the symbolic state validation problem can be cast as a MAX-SAT problem and solved by existing constraint solvers. Our approach is motivated by two observations. We may bind the symbolic parameters in un-modeled APIs based on observations made in former executions by other programs. The binding enables us to use the corresponding observed concrete return values of APIs to validate the symbolic states arising from the over-approximated return values of the un-modeled APIs. Second, some symbolic constraints can be accurately evaluated despite the imprecision of the over-approximated symbolic values. Our technique found 80 unreported bugs when it was applied to 10 popular programs with a total of 1.5 million lines of code. All of them can be confirmed by test cases. Our technique presents a promising way to apply the big data paradigm to software engineering. It provides a mechanism to validate the symbolic states of a project by leveraging the many concrete input-output values of APIs collected from other projects.\"",
        "1 is \"Socially Conscious Decision-Making\", 2 is \"Locating causes of program failures\"",
        "Given above information, for an author who has written the paper with the title \"Integrating Heterogeneous Web Services with WebXcript\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005364": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Distributed Markovian Bisimulation Reduction aimed at CSL Model Checking':",
        "Document: \"Arcade - A Formal, Extensible, Model-Based Dependability Evaluation Framework. This paper discusses the requirements that a suitable formalism for dependability modeling/evaluation should possess. We also discuss the outline of Arcade, an architectural dependability formalism that we are developing.\"",
        "Document: \"Specification of Data Centre Power Management Strategies. In recent work, we proposed a flexible simulation framework (using AnyLogic) for the trade-off analysis of power and performance in data centres. We now extend this framework with a versatile module to study the effect of advanced power management strategies based on both power and performance measurement data collected during system operation. This allows us to study the effect of power management in more detail by taking a wide variety of state information variables into account. The paper includes examples of several power management strategies presented in the literature (and extensions thereof). By enabling power management, our more advanced strategies show energy consumption reductions of up to 55% for a typical data centre workload for a small 30 servers cluster, while performance is kept intact and Service-Level Agreements violations are minimised.\"",
        "Document: \"Approximate Analysis of Networks of PH|PH|1|K Queues: Theory & Tool Support.  . We address the approximate analysis of open networks ofPHjPHj1 and PHjPHj1jK queues. We start from the analysis of openqueueing networks (QNs) as proposed by Whitt, where large QNs aredecomposed into individual GIjGj1 queues, characterized by the firstand second moment of the service and interarrival time distribution. Weextend this approach in two ways.First of all, we use PHjPHj1 queues, instead of GIjGj1 queues, so thatthe individual queues can be solved exactly, using... \"",
        "Document: \"Structural Decomposition and Serial Solution of SPN Models of the ATM GAUSS Switch. We address the performance, in particular, the cell loss ratio, of the ATM GAUSS switch under a variety of realistic video\n and constant bit rate traffic patterns.\n \n We describe the operation of the GAUSS switch and derive a stochastic Petri net model for it. One problem with this model,\n when subjected to realistic traffic, is that it is too large (in terms of states of the underlying Markov chain) to be analysed.\n We circumvent this largeness problem by structurally decomposing this model in a number of smaller models that can be solved\n in a serial fashion, thereby using analysis results of one another. This approach not only speeds up the solution process\n by several orders of magnitude, it also still yields accurate results.\n \n \n \n With respect to the GAUSS switch we show that under realistic traffic, the internal buffers need to be doubled in size, as\n opposed to analysis results under Poisson traffic, to yield acceptable cell-loss performance. Concluding, this paper serves\n three aims: (i) it shows the suitability of stochastic Petri nets in the context of ATM system analysis; (ii) it illustrates\n a structural decomposition method circumventing the state space explosion problem; and (iii) it derives more detailed performance\n results for the GAUSS switch than has been possible previously.\n \n \n \"",
        "Document: \"An adaptive resource control mechanism in multi-hop ad-hoc networks. This paper presents an adaptive resource control mechanism for multihop ad-hoc network systems, which avoids bottleneck problems caused by the node-fairness property of IEEE 802.11. In our proposal, the feedback information from the downstream bottleneck, derived from Request-To-Send (RTS) and Clear-To-Send (CTS) messages is utilized to control the Transmission Opportunity (TXOP) limit of the upstream nodes for traffic balancing. The proposed mechanism is modelled control-theoretically using the 20-sim control system modelling tool, which has the advantage that results can be obtained in a fast and efficient way. Compared to systems without resource control, a higher throughput and lower delay can be achieved under a variety of traffic load conditions as well as in dynamic network environments.\"",
        "1 is \"The Formal Verification of an Algorithm for Interactive Consistency under a Hybrid Fault Model\", 2 is \"The temporal logic of programs\"",
        "Given above information, for an author who has written the paper with the title \"Distributed Markovian Bisimulation Reduction aimed at CSL Model Checking\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005469": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Evolutionary Algorithm Based Immune Intrusion Analysis Engine':",
        "Document: \"Guess and Determine Attacks on Filter Generators\u2014Revisited. Although there are many different approaches used in cryptanalysis of nonlinear filter generators, the selection of tap positions has not received enough attention yet. In this paper we examine the security of nonlinear filter generators that output several bits at the time against a variant of a guess and determine attack that takes into account the tap positions of the generator. In difference to the filter state guessing attack (FSGA) introduced by Pasalic (2009), our approach further reduces the input preimage space by using a given placement of the tap positions. The new attack, though a simple generalization of the FSGA, in many cases outperforms both classical algebraic attacks and the FSGA. In particular, the new attack is much more efficiently applied against filter generators that use a vectorial Maiorana\u2013McFarland than classical algebraic attacks or the FSGA. As a proof of the concept we apply our attack to the stream cipher SOBER-t32 without stuttering and show that our attack performs slightly better than a guess and determine attack proposed by Babbage\"",
        "Document: \"NTRUSign With a New Perturbation. NTRUSign, a digital signature scheme, is suffering an effective attack. In this correspondence, we insert a new perturbation into NTRUSign primitive. With the new perturbation, each signature value is a linear combination of the private keys, and the combination coefficients have a hidden distribution. By a large number of signatures, the attacker obtains the value of some complicated function of the private keys. It appears as hard problem computing the private keys from this value. Our scheme is smaller in size, presents a higher efficiency, and provides a clearer security proof than NTRUSign with the old perturbation.\"",
        "Document: \"Constructions of 1-resilient Boolean functions on odd number of variables with a high nonlinearity. In this paper, we concentrate on the design of 1-resilient Boolean functions with desirable cryptographic properties. Firstly, we put forward a novel secondary construction to obtain 1-resilient functions. Next, we present the relationships between the properties of these constructed 1-resilient functions and that of the initial functions. Based on the construction and a class of bent functions on n variables, we can obtain a class of (n\u2009+\u20093)-variable 1-resilient non-separable cryptographic functions with a high algebraic immunity, whose nonlinearity is equal to the bent concatenation bound 2n\u2009+\u20092\u2009\u2212\u20092(n\u2009+\u20092)/2. Furthermore, we propose a set of 1-resilient non-separable functions on odd number of variables with an optimal algebraic degree, a high algebraic immunity, and a high nonlinearity. Copyright \u00a9 2012 John Wiley & Sons, Ltd.\"",
        "Document: \"Direct CCA secure identity-based broadcast encryption. In the previous works, the general transformation methods from a CPA(chosen-plaintext attacks) secure scheme to a CCA(chosen-ciphertext attacks) secure scheme are the hierarchical identity-based encryption, one-time signature and MAC. These folklore construction methods lead to the CCA secure schemes that are somewhat inefficient in the real life. In this paper, a new direct chosen-ciphertext technique is introduced and a practical identity-based broadcast encryption(IBBE) scheme that is CCA secure is proposed. The new scheme has many advantages over the available, such as constant size private keys and constant size ciphertexts, which solve the trade-off between the private keys size and ciphertexts size. In addition, under the standard model, the security of the new scheme is reduced to the hardness assumption-decision bilinear Diffie-Hellman exponent problem(DBDHE). This assumption is more natural than many of the hardness assumptions recently introduced to IBBE in the standard model.\"",
        "Document: \"Provably secure multi-proxy signature scheme with revocation in the standard model. Multi-proxy signature schemes are very useful tools when an original signer needs to delegate his signing capability to a group of proxy signers, and have been suggested in numerous applications. The proxy revocation problem is an essential issue of the proxy signature schemes, however, it is seldom considered in the multi-proxy signature schemes. In this paper, we give a formal definition and security model of the multi-proxy signature schemes with proxy revocation, and propose a multi-proxy signature scheme with proxy revocation. Our scheme can perform the immediate revocation by using a security mediator (SEM), who examines whether each proxy signer signs according to a warrant or its identity exists in a revocation list, and then decides if it issues a proxy token for each proxy signer. The proposed scheme is proven existentially unforgeable against chosen message/warrant attacks based on the computational Diffie-Hellman intractability assumption in the standard model. Furthermore, the size of a multi-proxy signature is constant and independent of the number of the proxy signers.\"",
        "1 is \"Reversible data hiding in encrypted images based on absolute mean difference of multiple neighboring pixels.\", 2 is \"Efficient Batch Verification of Short Signatures for a Single-Signer Setting without Random Oracles\"",
        "Given above information, for an author who has written the paper with the title \"Evolutionary Algorithm Based Immune Intrusion Analysis Engine\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005470": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'The Visual Object Tracking VOT2016 Challenge Results':",
        "Document: \"Application Of Papoulis\u2013Gerchberg Method In Image Super-Resolution and Inpainting. In this paper, we study the Papoulis\u2013Gerchberg (PG) method and its applications to domains of image restoration such as super-resolution (SR) and inpainting. We show that the method performs well under certain conditions. We then suggest improvements to the method to achieve better SR and inpainting results. The modification applied to the SR process also allows us to apply the method to a larger class of images by doing away with some of the restrictions inherent in the classical PG method. We also present results to demonstrate the performance of the proposed techniques.\"",
        "Document: \"Dynamic Data Driven Applications System Concept For Information Fusion. We present a framework to Information Fusion (IF) using the Dynamic Data Driven Applications Systems (DDDAS) concept. Existing literature at the intersection of these two topics supports environmental modeling (e.g. terrain understanding) for context enhanced applications. Taking advantage of sensor models, statistical methods, and situation-specific spatio-temporal fusion products derived from wide area sensor networks, DDDAS demonstrates robust multi-scale and multi-resolution geographical terrain computations. We highlight the complementary nature of these seemingly parallel approaches and propose a more integrated analytical framework in the context of a cooperative multimodal sensing application. In particular, we use a Wide-Area Motion Imagery (WAMI) application to draw parallels and contrasts between IF and DDDAS systems that warrants an integrated perspective. This elementary work is aimed at triggering a sequence of deeper insightful research towards exploiting sparsely sampled piecewise dense WAMI measurements - an application where the challenges of big-data with regards to mathematical fusion relationships and high-performance computation remain significant and will persist. Dynamic data-driven adaptive computations are required to effectively handle the challenges with exponentially increasing data volume for advanced information fusion systems solutions such as simultaneous target tracking and identification. (C) 2013 The Authors. Published by Elsevier B.V. Selection and peer review under responsibility of the organizers of the 2013 International Conference on Computational Science\"",
        "Document: \"Image Registration and Change Detection under Rolling Shutter Motion Blur. In this paper, we address the problem of registering a distorted image and a reference image of the same scene by estimating the camera motion that had caused the distortion. We simultaneously detect the regions of changes between the two images. We attend to the coalesced effect of rolling shutter and motion blur that occurs frequently in moving CMOS cameras. We first model a general image format...\"",
        "Document: \"Parallel Blob Extraction Using the Multi-core Cell Processor. The rapid increase in pixel density and frame rates of modern imaging sensors is accelerating the demand for fine-grained and embedded parallelization strategies to achieve real-time implementations for video analysis. The IBM Cell Broadband Engine (BE) processor has an appealing multi-core chip architecture with multiple programming models suitable for accelerating multimedia and vector processing applications. This paper describes two parallel algorithms for blob extraction in video sequences: binary morphological operations and connected components labeling (CCL), both optimized for the Cell-BE processor. Novel parallelization and explicit instruction level optimization techniques are described for fully exploiting the computational capacity of the Synergistic Processing Elements (SPEs) on the Cell processor. Experimental results show significant speedups ranging from a factor of nearly 300 for binary morphology to a factor of 8 for COL in comparison to equivalent sequential implementations applied to High Definition (HD) video.\"",
        "Document: \"Parallel implementation of the integral histogram. The integral histogram is a recently proposed preprocessing technique to compute histograms of arbitrary rectangular gridded (i.e. image or volume) regions in constant time. We formulate a general parallel version of the the integral histogram and analyse its implementation in Star Superscalar (StarSs). StarSs provides a uniform programming and runtime environment and facilitates the development of portable code for heterogeneous parallel architectures. In particular, we discuss the implementation for the multi-core IBM Cell Broadband Engine (Cell/B.E.) and provide extensive performance measurements and tradeoffs using two different scan orders or histogram propagation methods. For 640 \u00d7 480 images, a tile or block size of 28 \u00d7 28 and 16 histogram bins the parallel algorithm is able to reach greater than real-time performance of more than 200 frames per second.\"",
        "1 is \"A Riemannian Optimization Approach for Computing Low-Rank Solutions of Lyapunov Equations\", 2 is \"Multi-Target Tracking by Online Learning a CRF Model of Appearance and Motion Patterns\"",
        "Given above information, for an author who has written the paper with the title \"The Visual Object Tracking VOT2016 Challenge Results\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005474": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Convex Hull Based Approach For Multi-Oriented Character Recognition From Graphical Documents':",
        "Document: \"Signature Segmentation from Machine Printed Documents Using Conditional Random Field. Automatic separation of signatures from a document page involves difficult challenges due to the free-flow nature of handwriting, overlapping/touching of signature parts with printed text, noise, etc. In this paper, we have proposed a novel approach for the segmentation of signatures from machine printed signed documents. The algorithm first locates the signature block in the document using word level feature extraction. Next, the signature strokes that touch or overlap with the printed texts are separated. A stroke level classification is then performed using skeleton analysis to separate the overlapping strokes of printed text from the signature. Gradient based features and Support Vector Machine (SVM) are used in our scheme. Finally, a Conditional Random Field (CRF) model energy minimization concept based on approximated labeling by graph cut is applied to label the strokes as \"signature\" or \"printed text\" for accurate segmentation of signatures. Signature segmentation experiment is performed in \"tobacco\" dataset1 and we have obtained encouraging results.\"",
        "Document: \"Anomaly Detection in Road Traffic Using Visual Surveillance: A Survey. AbstractComputer vision has evolved in the last decade as a key technology for numerous applications replacing human supervision. Timely detection of traffic violations and abnormal behavior of pedestrians at public places through computer vision and visual surveillance can be highly effective for maintaining traffic order in cities. However, despite a handful of computer vision\u2013based techniques proposed in recent times to understand the traffic violations or other types of on-road anomalies, no methodological survey is available that provides a detailed insight into the classification techniques, learning methods, datasets, and application contexts. Thus, this study aims to investigate the recent visual surveillance\u2013related research on anomaly detection in public places, particularly on road. The study analyzes various vision-guided anomaly detection techniques using a generic framework such that the key technical components can be easily understood. Our survey includes definitions of related terminologies and concepts, judicious classifications of the vision-guided anomaly detection approaches, detailed analysis of anomaly detection methods including deep learning\u2013based methods, descriptions of the relevant datasets with environmental conditions, and types of anomalies. The study also reveals vital gaps in the available datasets and anomaly detection capability in various contexts, and thus gives future directions to the computer vision\u2013guided anomaly detection research. As anomaly detection is an important step in automatic road traffic surveillance, this survey can be a useful resource for interested researchers working on solving various issues of Intelligent Transportation Systems (ITS).\"",
        "Document: \"Bag-of-Visual-Words for Signature-Based Multi-Script Document Retrieval. An end-to-end architecture for multi-script document retrieval using handwritten signatures is proposed in this paper. The user supplies a query signature sample, and the system exclusively returns a set of documents that contain the query signature. In the first stage, a component-wise classification technique separates the potential signature components from all other components. A bag-of-visual-words powered by SIFT descriptors in a patch-based framework is proposed to compute the features and a support vector machine (SVM)-based classifier was used to separate signatures from the documents. In the second stage, features from the foreground (i.e., signature strokes) and the background spatial information (i.e., background loops, reservoirs etc.) were combined to characterize the signature object to match with the query signature. Finally, three distance measures were used to match a query signature with the signature present in target documents for retrieval. The \u2018Tobacco\u2019 (The Legacy Tobacco Document Library (LTDL). University of California, San Francisco, 2007. http://legacy.library.ucsf.edu/) document database and an Indian script database containing 560 documents of Devanagari (Hindi) and Bangla scripts were used for the performance evaluation. The proposed system was also tested on noisy documents, and the promising results were obtained. A comparative study shows that the proposed method outperforms the state-of-the-art approaches.\"",
        "Document: \"Smart Device Authentication Based on Online Handwritten Script Identification and Word Recognition in Indic Scripts Using Zone-Wise Features. AbstractSecure authentication is a vital component for device security. The most basic form of authentication is by using passwords. With the evolution of smart devices, selecting stronger and unbreakable passwords have become a challenging task. Such passwords if written in native languages tend to offer improved security since attackers having no knowledge of such scripts finding it hard to crack. This article proposes two zone-wise feature extraction approaches-zone-wise structural and directional ZSD and zone-wise slopes of dominant points ZSDP, to recognize online handwritten script and word in four major Indic scripts-Devanagari, Bengali, Telugu and Tamil. These features have been used separately and in combination in HMM-based platform for recognition purpose. The dimension reduction of the ZSD-ZSDP combination with factor analysis has shown the best performance in all the four scripts. This work can be utilized for setting up the authentication schemes with the Indic scripts' passwords thus rendering it difficult to crack by hackers having no knowledge of such scripts.\"",
        "Document: \"Detection of seal and signature entities with hierarchical recovery based on watermark self embedding in tampered digital documents. \u2022A framework for detection and security of authentication entities in the documents.\u2022The signatures are segmented based on the stroke information.\u2022The seals are detected based on the textual content of the seal.\u2022The integrity of authentication entities is ensured via three authentication bits.\u2022Forgeries are detected well with accurate localization and consequent restoration.\"",
        "1 is \"QSAR Models Using a Large Diverse Set of Estrogens.\", 2 is \"Off-line Arabic character recognition: the state of the art\"",
        "Given above information, for an author who has written the paper with the title \"Convex Hull Based Approach For Multi-Oriented Character Recognition From Graphical Documents\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005495": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Energy efficiency of an enhanced DCF access method using bidirectional communications for infrastructure-based IEEE 802.11 WLANs':",
        "Document: \"Modeling and analysis of reservation frame slotted-ALOHA in wireless machine-to-machine area networks for data collection. Reservation frame slotted-ALOHA (RFSA) was proposed in the past to manage the access to the wireless channel when devices generate long messages fragmented into small packets. In this paper, we consider an M2M area network composed of end-devices that periodically respond to the requests from a gateway with the transmission of fragmented messages. The idle network is suddenly set into saturation, having all end-devices attempting to get access to the channel simultaneously. This has been referred to as delta traffic. While previous works analyze the throughput of RFSA in steady-state conditions, assuming that traffic is generated following random distributions, the performance of RFSA under delta traffic has never received attention. In this paper, we propose a theoretical model to calculate the average delay and energy consumption required to resolve the contention under delta traffic using RFSA. We have carried out computer-based simulations to validate the accuracy of the theoretical model and to compare the performance for RFSA and FSA. Results show that there is an optimal frame length that minimizes delay and energy consumption and which depends on the number of end-devices. In addition, it is shown that RFSA reduces the energy consumed per end-device by more than 50% with respect to FSA under delta traffic.\"",
        "Document: \"Testing Cooperative Communication Schemes in a Virtual Distributed Testbed of Wireless Networks. It is expected that Next Generation Networks (NGNs) will offer seamless interoperability among heterogeneous access technologies in order to provide ubiquitous access. In such settings, short range technologies may be used in order to extend the coverage area of cellular systems while cooperative diversity can improve the efficiency of the wireless systems. An advanced, backward compatible, with the 802.11 standard, MAC protocol for cooperative ARQ scenarios in NGNs sets the research framework for this work. The functionalities of the RCSMA protocol 1 will be enhanced and the derived analytical models will be validated at the UNITE Virtual Distributed Testbed (VDT) (2).\"",
        "Document: \"Performance analysis of a cluster-based MAC protocol for wireless ad hoc networks. An analytical model to evaluate the non-saturated performance of the Distributed Queuing Medium Access Control Protocol for Ad Hoc Networks (DQMANs) in single-hop networks is presented in this paper. DQMAN is comprised of a spontaneous, temporary, and dynamic clustering mechanism integrated with a near-optimum distributed queuing Medium Access Control (MAC) protocol. Clustering is executed in a distributed manner using a mechanism inspired by the Distributed Coordination Function (DCF) of the IEEE 802.11. Once a station seizes the channel, it becomes the temporary clusterhead of a spontaneous cluster and it coordinates the peer-to-peer communications between the clustermembers. Within each cluster, a near-optimum distributed queuing MAC protocol is executed. The theoretical performance analysis of DQMAN in single-hop networks under non-saturation conditions is presented in this paper. The approach integrates the analysis of the clustering mechanism into the MAC layer model. Up to the knowledge of the authors, this approach is novel in the literature. In addition, the performance of an ad hoc network using DQMAN is compared to that obtained when using the DCF of the IEEE 802.11, as a benchmark reference.\"",
        "Document: \"On the Impact of Repeaters Deployment on WCDMA, Networks Planning. This paper addresses the analysis of WCDMA systems with repeaters deployment. The real different path delays, taking into account the repeaters presence and the finite nature of the time window of Rake receivers are considered. This allows an enhanced analysis with respect to classical approaches from a system level viewpoint. Results show that not taking into account these effects imply erroneous metrics. Indeed, some of the most relevant parameters in network performance evaluation are clearly affected.\"",
        "Document: \"RLNC-Aided Cooperative Compressed Sensing for Energy Efficient Vital Signal Telemonitoring. Wireless Body Area Networks (WBANs) are composed of sensors that either monitor and trans- mit vital signals or act as relays that forward the received data to a Body Node Coordinator (BNC). In this paper, we introduce an energy efficient vital signal telemonitoring scheme, which exploits Com- pressed Sensing (CS) for low-complexity signal com- pression/reconstruction and distributed cooperation for reliable data transmission to the BNC. More specif- ically, we introduce a Cooperative Compressed Sensing (CCS) approach, which increases the energy efficiency of WBANs by exploiting the benefits of Random Linear Network Coding (RLNC). We study the energy effi- ciency of RLNC and compare it with the Store-and- Forward (FW) protocol. Our mathematical analysis shows that the gain introduced by RLNC increases as the link failure rate increases, especially in practical scenarios with a limited number of relays. Furthermore, we propose a reconstruction algorithm that further enhances the benefits of RLNC by exploiting key characteristics of vital signals. With the aid of elec- trocardiographic (ECG) and electroencephalographic (EEG) data available in medical databases, extensive simulation results are illustrated, which validate our theoretical findings and show that the proposed re- covery algorithm increases the energy efficiency of the body sensor nodes by 40% compared to conventional CS-based reconstruction methods.\"",
        "1 is \"A New Genetic Algorithm for Scheduling for Large Communication Delays\", 2 is \"Advanced prototype platform for a wireless multimedia local area network\"",
        "Given above information, for an author who has written the paper with the title \"Energy efficiency of an enhanced DCF access method using bidirectional communications for infrastructure-based IEEE 802.11 WLANs\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005527": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Adaptive Output Feedback Control of Uncertain Nonlinear Systems With Hysteresis Nonlinearity':",
        "Document: \"Composite Adaptive Fuzzy Output Feedback Control Design for Uncertain Nonlinear Strict-Feedback Systems With Input Saturation. In this paper, a composite adaptive fuzzy output-feedback control approach is proposed for a class of single-input and single-output strict-feedback nonlinear systems with unmeasured states and input saturation. Fuzzy logic systems are utilized to approximate the unknown nonlinear functions, and a fuzzy state observer is designed to estimate the unmeasured states. By utilizing the designed fuzzy s...\"",
        "Document: \"Adaptive neural control of nonlinear MIMO systems with unknown time delays. In this paper, a novel adaptive NN control scheme is proposed for a class of uncertain multi-input and multi-output (MIMO) nonlinear time-delay systems. RBF NNs are used to tackle unknown nonlinear functions, then the adaptive NN tracking controller is constructed by combining Lyapunov-Krasovskii functionals and the dynamic surface control (DSC) technique along with the minimal-learning-parameters (MLP) algorithm. The proposed controller guarantees uniform ultimate boundedness (UUB) of all the signals in the closed-loop system, while the tracking error converges to a small neighborhood of the origin. An advantage of the proposed control scheme lies in that the number of adaptive parameters for each subsystem is reduced to one, triple problems of ''explosion of complexity'', ''curse of dimension'' and ''controller singularity'' are solved, respectively. Finally, a numerical simulation is presented to demonstrate the effectiveness and performance of the proposed scheme.\"",
        "Document: \"A simple adaptive fuzzy control for a class of strict-feedback SISO systems. A simple adaptive fuzzy control (SAFC) is proposed for a class of strict-feedback uncertain nonlinear systems with both unknown system nonlinearities and unknown virtual control gain nonlinearities. Combining the dynamic surface control (DSC) technique with minimal-learning-parameters (MLP) algorithm, a systematic procedure for synthesis of SAFC is developed base on the universal approximation of Takagi-Sugeno (T-S) fuzzy system. An important feature of the proposed algorithm is that the number of parameters updated on line for each subsystem is reduced dramatically to one, both problems of ldquoexplosion of complexityrdquo and ldquocurse of dimensionrdquo are avoided, such that the computation load is reduced drastically. It is shown that all closed-loop signals are semi-global uniform ultimate bound (SGUUB) via Lyapunov stability theory and the tracking error can be made arbitrary small. Finally, simulation results are presented to demonstrate the effectiveness and performance of the proposed scheme.\"",
        "Document: \"Asynchronous Frequency-Dependent Fault Detection for Nonlinear Markov Jump Systems Under Wireless Fading Channels. In this article, the asynchronous fault detection (FD) strategy is investigated in frequency domain for nonlinear Markov jump systems under fading channels. In order to estimate the system dynamics and meet the fact that not all the running modes can be observed exactly, a set of asynchronous FD filters is proposed. By using statistical methods and the Lynapunov stability theory, the augmented system is shown to be stochastic stable with a prescribed \n<inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$l_{2}$ </tex-math></inline-formula>\n gain even under fading transmissions. Then, a novel lemma is developed to capture the finite frequency performance. Some solvable conditions with less conservatism are subsequently deduced by exploiting novel decoupling techniques and additional slack variables. Besides, the FD filter gains could be calculated with the aid of the derived conditions. Finally, the effectiveness of the proposed method is shown by an illustrative example.\"",
        "Document: \"Cooperative Path Following Ring-Networked Under-Actuated Autonomous Surface Vehicles: Algorithms and Experimental Results. This paper addresses the cooperative path following the problem of ring-networked under-actuated autonomous surface vehicles on a closed curve. A cooperative guidance law is proposed at the kinematic level such that a symmetric formation pattern is achieved. Specifically, individual guidance laws of surge speed and angular rate are developed by using a backstepping technique and a line-of-sight guidance method. Then, a coordination design is proposed to update the path variables under a ring-networked topology. The equilibrium point of the closed-loop system has been proven to be globally asymptotically stable. The result is extended to the cooperative path following the lack of sharing of a global reference velocity, and a distributed observer is designed to recover the reference velocity to each vehicle. Moreover, the cooperative path following the presence of an unknown sideslip is considered, and an extended state observer is developed to compensate for the effect of the unknown sideslip. Both simulation and experimental results are provided to illustrate the effectiveness of the proposed cooperative guidance law for the path following over a closed curve.\"",
        "1 is \"Adaptive fuzzy output-feedback controller design for nonlinear systems via backstepping and small-gain approach.\", 2 is \"Identification of Wiener systems with binary-valued output observations\"",
        "Given above information, for an author who has written the paper with the title \"Adaptive Output Feedback Control of Uncertain Nonlinear Systems With Hysteresis Nonlinearity\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005550": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Semantic distance between specifications':",
        "Document: \"mucROSE: automated measurement of COSMIC-FFP for Rational Rose RealTime. During the last 10 years, many organizations have invested resources and energy in order to be rated at the highest level as possible according to some maturity models for software development. Since measures play an important role in these models, it is essential that CASE tools offer facilities to automatically measure the sizes of various documents produced using them. This paper introduces a tool, called \u03bccROSE, that automatically measures the functional software size, as defined by the COSMIC-FFP method, for Rational Rose RealTime models. \u03bccROSE streamlines the measurement process, ensuring repeatability and consistency in measurement while reducing measurement cost. It is the first tool to address automatic measurement of COSMIC-FFP and it can be integrated into the Rational Rose RealTime toolset.\"",
        "Document: \"Proving Non-interference on Reachability Properties: A Refinement Approach. This paper proposes an approach to prove interference freedom for a reach ability property of the form AG (psi = EF phi) in a B specification. Such properties frequently occur in security policies and information systems. Reach ability is proved by constructing using stepwise algorithmic refinement an abstract program that refines AG (psi = EF phi). We propose proof obligations to show non-interference, ie, to prove that other operations can be executed in interleaving with this program while preserving the reach ability property, to cater for the multi-user aspect of information systems. Proof obligations are discharged using conventional B provers (eg, Atelier B). Since refinement preserves these reach ability properties and non-interference, proofs can be conducted on abstract machines rather than implementation code.\"",
        "Document: \"Defining and detecting feature interactions.  We describe a relational method for specifying features and detecting feature interactions.The method allows for an independent specification of system features, and for a detection ofinteraction between features. The method is based on the lattice of relational specifications: thesystem specification is given as the conjunction (lattice operator meet) of the features; a featureinteraction is detected when the meet of the features does not exist. Examples of detection aregiven using logic... \"",
        "Document: \"Formal Modeling for Deploying Improvement and Innovation in Information Technology. In this paper, we show the importance of building precise models of basic concepts before conducting strategic information technology improvement and innovation. We use the notions of distinction and definition to build precise models. Our modelling approach mixes both natural language and mathematical definitions, to reach the appropriate level of precision. This modelling approach constitutes a form of knowledge management similar to approaches like enterprise architecture, but focusing on modelling the impacts of a given technology innovation and conducting projects in a more agile style.\"",
        "Document: \"Refinement patterns for ASTD. astd is a formal and graphical language specifically defined for information system specification. Up to now, a specifier had to build an astd specification from scratch and there were no refinement techniques for stepwise construction. This paper aims at introducing refinement patterns for astd, which are inspired from real case studies. For each pattern, proof obligations have been identified to define the refinement semantics we want to provide. The three refinement patterns presented in the paper are illustrated by an example of a basic complaint management system.\"",
        "1 is \"Experimenting Formal Proofs of Petri Nets Refinements\", 2 is \"Building application generators\"",
        "Given above information, for an author who has written the paper with the title \"Semantic distance between specifications\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005575": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Practical polar code construction over parallel channels.':",
        "Document: \"Optimal Power Control Game Algorithm for Cognitive Radio Networks with Multiple Interference Temperature Limits. Based on interference temperature model, the problem of secondary spectrum sharing can be formulated as a power optimization problem at physical layer. In this paper, we consider decentralized cognitive radio networks, and we especially focus on the spectrum sharing scenario where multiple measurement points are located in the licensed system. Game theory is used to investigate the distributed power control for providing the maximum throughput in cognitive radio networks. There are two aspects that should be considered in the design of the payoff function for each player, one is counteracting negative externalities in cognitive radio networks, and the other is satisfying all the interference temperature limits from measurement points. A tax-based power control game algorithm is introduced to implement power allocation optimization in a distributed mode, and guarantees the convergence to globally optimal power allocations.\"",
        "Document: \"Improved Message Passing Algorithms for Sparse Code Multiple Access. Sparse code multiple access (SCMA) is one of the nonorthogonal multiple access techniques for the 5G system. SCMA can provide different levels of overloading to meet the diverse traffic connectivity requirements. However, its relatively high computational complexity of multiuser detection is still a significant concern for practical implementation, even when the sparse structure has already been e...\"",
        "Document: \"A Novel Wavelet-Based Energy Detection for Compressive Spectrum Sensing. Wavelet transform has proved to be an attractive tool in terms of analyzing singularities and irregular structures, which can characterize irregular edges of signals. Thus, it is well motivated to apply the wavelet transform approach to wideband spectrum sensing. But the existing wavelet-based spectrum sensing schemes work under the assumption that the frequency response of the analog signal input at the sensing receiver is real. To make this method work for more types of signals, this paper develops a novel wavelet-based approach to compressive wide-band spectrum sensing. In the proposed scheme, the wide-band time domain signal is fed into a number of filters and the sub-Nyquist sampled outputs are utilized to detect the occupancy of spectrum via a wavelet-based edge detector. The filters' outputs are real and nonnegative, regardless of the style of the analog signal. Furthermore, through simply adding the measurement vectors at the fusion center, this scheme can be applied to the case of multiple cognitive radios (CRs), reducing the complexity compared with traditional joint recovery algorithms.\"",
        "Document: \"Criteria on Utility Designing of Convex Optimization in FDMA Networks. In this paper, we investigate the network utility maximization problem in FDMA systems. We summarize with a suite of criteria on designing utility functions so as to achieve the global optimization convex. After proposing the general form of the utility functions, we present examples of commonly used utility function forms that are consistent with the criteria proposed in this paper, which include the well-known proportional fairness function and the sigmoidal-like functions. In the second part of this paper, we use numerical results to demonstrate a case study based on the criteria mentioned above, which deals with the subcarrier scheduling problem with dynamic rate allocation in FDMA system.\"",
        "Document: \"Resource Allocation in Multicast OFDM Systems: Lower/Upper Bounds and Suboptimal Algorithm. This letter studies generalized resource allocation in multiple description coding multicast (MDCM) orthogonal frequency division multiplexing (OFDM) systems, where many multicast groups are served simultaneously by base station (BS). Due to non-tractable nature, the problem is reformulated to weighted-capacity maximization by resource distribution among virtual multicast groups (VMGs). Then, the relaxation of exclusively-used constraints allows us to provide both upper and lower bounds for the original problem. A suboptimal algorithm is also proposed based on Newton's method to reduce the computational complexity. Simulation results show the proposed algorithm approximately reaches the optimal capacity within 2 iterations for multicast waterfilling, and MDCM is more bandwidth-efficient for multicast services than conventional multicast (CVM).\"",
        "1 is \"GPS: a graph processing system\", 2 is \"On Lp Decoding Of Polar Codes\"",
        "Given above information, for an author who has written the paper with the title \"Practical polar code construction over parallel channels.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005590": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'How to play bowling in parallel on the grid':",
        "Document: \"Convergence Study Of Principal Component Analysis Algorithms. We investigate the convergence properties of two different principal component analysis algorithms, and analytically explain some commonly observed experimental results. We use two different methodologies to analyze the two algorithms. The first methodology uses the fact that both algorithms are stochastic approximation procedures. We use the theory of stochastic approximation, in particular the results of Fabian, to analyze the asymptotic mean square errors (AMSEs) of the algorithms. This analysis reveals the conditions under which the algorithms produce smaller AMSEs, and also the conditions under which one algorithm has a smaller AMSE than the other. We next analyze the asymptotic mean errors (AMEs) of the two algorithms in the neighborhood of the solution. This analysis establishes the conditions under which the AMEs of the minor eigenvectors go to zero faster. Furthermore, the analysis makes explicit that increasing the gain parameter up to an upper bound improves the convergence of all eigenvectors. We also show that the AME of one algorithm goes to zero faster than the other. Experiments with multi-dimensional Gaussian data corroborate the analytical findings presented here.\"",
        "Document: \"Collaborative Spam Filtering Using E-Mail Networks. A distributed spam-filtering system that leverages e-mail networks' topological properties is more efficient and scalable than client-server-based solutions. Large-scale simulations of a prototype system reveal that this approach achieves a near-perfect spam detection rate while minimizing bandwidth cost.\"",
        "Document: \"A General Framework for Scalability and Performance Analysis of DHT Routing Systems. In recent years, many DHT-based P2P systems have been proposed, analyzed, and certain deployments have reached a global scale with nearly one million nodes. One is thus faced with the question of which particular DHT system to choose, and whether some are inherently more robust and scalable. Toward developing such a comparative framework, we present the reachable component method (RCM) for analyzing the performance of different DHT routing systems subject to random failures. We apply RCM to five DHT systems and obtain analytical expressions that characterize their routability as a continuous function of system size and node failure probability. An important consequence is that in the large-network limit, the routability of certain DHT systems go to zero for any non-zero probability of node failure. These DHT routing algorithms are therefore unscalable, while some others, including Kademlia, which powers the popular eDonkey P2P system, are found to be scalable.\"",
        "Document: \"Comparison of Image Similarity Queries in P2P Systems. Given some of the recent advances in Distributed Hash Table (DHT) based Peer-To-Peer (P2P) systems we ask the following questions: Are there applications where unstructured queries are still necessary (i.e., the underlying queries do not efficiently map onto any structured framework), and are there unstructured P2P systems that can deliver the high bandwidth and computing performance necessary to support such applications. Toward this end, we consider an image search application which supports queries based on image similarity metrics, such as color histogram intersection, and discuss why in this setting, standard DHT approaches are not directly applicable. We then study the feasibility of implementing such an image search system on two different unstructured P2P systems: powerlaw topology with percolation search, and an optimized super-node topology using structured broadcasts. We examine the average and maximum values for node bandwidth, storage and processing requirements in the percolation and super-node models, and show that current high-end computers and high-speed links have sufficient resources to enable deployments of large-scale complex image search systems.\"",
        "Document: \"Scalable Percolation Search in Power Law Networks. We introduce a scalable searching algorithm for nding nodes and contents in random networks with Power-Law (PL) and heavy-tailed degree distributions. The network is searched using a prob- abilistic broadcast algorithm, where a query message is relayed on each edge with probability just above the bond percolation threshold of the network. We show that if each node caches its directory via a short random walk, then the total number of accessible contents exhibits a rst-or der phase transition, ensuring very high hit rates just above the percolation threshold. In any random PL network of size, N, and exponent, 2 < 3, the total trac per query scales sub-linearly, while the search time scales as O(log N). In a PL network with exponent, 2, any content or node can be located in the network with probability approaching one in time O(log N), while generating trac that scales as O(log2 N), if the maximum degree, kmax, is unconstrained, and as O(N 1 2 + ) (for any > 0) if kmax = O( p N). Extensive large-scale simulations show these scaling laws to be precise. We discuss how this percolation search algorithm can be directly adapted to solve the well- known scaling problem in unstructured Peer-to-Peer (P2P) networks. Simulations of the protocol on sample large-scale subnetworks of existing P2P services show that overall trac can be reduced by almost two-orders of magnitude, without any signican t loss in search performance.\"",
        "1 is \"On the Optimum Checkpoint Interval\", 2 is \"Capturing Social Data Evolution Using Graph Clustering\"",
        "Given above information, for an author who has written the paper with the title \"How to play bowling in parallel on the grid\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005602": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Design and implementation of TruMan, a Trust Manager Component for Distributed Systems':",
        "Document: \"A Portable Tool for Running MPI Applications in the Cloud. Cloud computing founds on the pay-per-use paradigm, and offers a simple way to acquire any kind of resources through an as-a-service approach. As a consequence, the cloud is currently seen as a viable and inexpensive alternative to customary parallel/distributed computing solutions, ranging from small clusters to GRIDs, which were widely used in the past. In this paper we present a portable cloud tool, cMe (cloud MPI enabler), able to run on any cloud provider, which builds up dedicated cloud-based MPI-enabled clusters. The objective is to help the end-users to choose dynamically both the cloud provider and the amount of the resources to be used. The cloud application performs simple job scheduling, based on the same principles adopted in the typical HPC environments (i.e., different jobs never share computing resources), but also enables the dynamical acquisition of resources. For user convenience, the application is provided with a simple web interface that helps the resource acquisition process.\"",
        "Document: \"The CloudGrid approach: Security analysis and performance evaluation. Both cloud and grid are computing paradigms that manage large sets of distributed resources, and the scientific community would benefit from their convergence. This paper proposes a novel computing model, cloudgrid, able to achieve full cloud and grid integration. After presenting its three-layer architecture, the security issues involved are analyzed, proposing a solution based on fine-grained access control mechanisms and identity federation that allows cooperation and interoperability among untrusted cloud resources. The overhead introduced by the multiple-layer architecture and by the security system are measured by extensive testing on a prototype implementation, and a trade-off analysis between security and performance is presented.\"",
        "Document: \"Cloud-Aware Development of Scientific Applications. The potential of cloud computing is still underutilized in the scientific computing field. Even if clouds probably are not fit for high-end HPC applications, they could be profitably used to bring the power of economic and scalable parallel computing to the masses. But this requires simple and friendly development environments, able to exploit cloud scalability and to provide fault tolerance. This paper presents a framework built on the top of a cloud-aware platform (mOSAIC) for the development of bag-of-tasks scientific applications.\"",
        "Document: \"Design and implementation of TruMan, a Trust Manager Component for Distributed Systems. Obtaining a measure of the trustability of a system in an open environment is a difficult task since trust concepts involve different quality criteria including non-technological aspects. Quality criteria and, above all, security mechanisms are commonly characterized by means of a set of statements making part of a policy. We are working on an evaluation methodology allowing automatic management for semi-formal policies and in this paper we propose a trust manager architecture (TruMan) enabling our methodology and an implementation of such architecture. We also present a proof-of-concept case study where TruMan is adopted in the context of service oriented architectures\"",
        "Document: \"A Framework for Mobile Agent Platform performance Evaluation. Mobile agents programming paradigm is an emerging approach for distributed computing, extremely suitable for mobile systems because of its adaptability to exploit the available resources. Optimization of mobile agents applications and system configuration are relevant above all when we deal handheld devices with limited capabilities. Classical approaches are hard to apply because new kinds of interaction facilities provided by agent platforms, such as cloning and migration, represent an additional software layer that affects the system performances. In this context identification and estimation of performance indexes are necessary to evaluate and foresee system dependability. In fact performance evaluation is exploited to address different issues according to which, different measurements are required. In practice, there is a need for tools and techniques for evaluation of the performances of the adopted mobile agent platforms. Related work proposes a set of performance indexes, slightly different one from the other; and measured using different approaches. We present here a framework that aims at supporting the development of ad-hoc solutions based on measurement agents. Framework architecture, prototype implementation and case studies and preliminary performance figures are presented in the following.\"",
        "1 is \"Simulation Based HPC Workload Analysis\", 2 is \"Bandwidth-Constrained Mapping of Cores onto NoC Architectures\"",
        "Given above information, for an author who has written the paper with the title \"Design and implementation of TruMan, a Trust Manager Component for Distributed Systems\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005629": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'An Efficient Observability Evaluation Algorithm Based on Factored Use-Def Chains.':",
        "Document: \"Accelerating Lightpath setup via broadcasting in binary-tree waveguide in Optical NoCs. In this paper, we propose a binary-tree waveguide connected Optical-Network-on-Chip (ONoC) to accelerate the establishment of the lightpath. By broadcasting the control data in the proposed power-efficient binary-tree waveguide, the maximal hops for establishing lightpath is reduced to two. With extensive simulations and analysis, we demonstrate that the proposed ONoC significantly reduces the setup time, and then the packet latency.\"",
        "Document: \"Observation Point Oriented Deterministic Diagnosis Pattern Generation (DDPG) for Chain Diagnosis. Scan is a widely used Design-for-Testability technique to improve test and diagnosis quality. Many defects may cause scan chains to fail. In this paper, an observation point oriented Deterministic Diagnostic Pattern Generation (DDPG) method was proposed for compound defects, which tolerates the system defects during scan chain diagnosis. Instead of sensitizing multiple paths proposed in our prior work, the proposed new DDPG method directly targets as many observation points as possible to observe the loading error occurred on the targeted scan cell. Experimental results on ISCAS\u201989 benchmark circuits show that the proposed DDPG method improves the effectiveness and efficiency of diagnosing compound defects, compared to our prior research\"",
        "Document: \"On-Chip Delay Sensor for Environments with Large Temperature Fluctuations. The precision of on-chip delay sensors is degraded by temperature fluctuations, which hinders these sensors from applying to on-line fault predicting and DVFS. We present a novel path delay measuring technique which is immune to large temperature fluctuation. The delay reference are generated by gate biasing temperature compensation devices in which the pull-up and pull-down network are tuned to set the measurement circuit working in temperature insensitive point, thereby eliminating the precision degradation due to temperature variations. Video image scaling IP is used as experimental circuit to validate the effectiveness of the proposed technique. Experimental results show that within temperature range of-55\u00b0C to 125\u00b0C, the measurement error is reduced from 19.56% to 0.5%, compared with the techniques without temperature resilience.\"",
        "Document: \"Acr: Enabling Computation Reuse For Approximate Computing. Approximate computing, which trades off computation quality (e.g, accuracy) and computation efforts, has becoming a promising technique to improve performance for many mission-non-critical and error-tolerant applications. The computations in such applications usually exhibit superior value locality, i.e, computations performed by a function or code region are very likely to reproduce \"similar\" results. Reusing the similar results can bypass redundant computations, as long as \"exact\" results are not mandatory. However, conventional computation reuse techniques are less effective in approximate computing paradigm. The input values of two computation instances have to be identical to reuse one for another, hence \"exact\" in nature. We propose ACR, an approximate computation reuse framework, to enable computation reuse for approximate computing. ACR relaxes the exact matching requirement in inputs to some extent regulated by \"similarity\" quantification, thereby shifting the exact computation reuse paradigm to its approximate counterpart. We furthermore propose an input significance-aware similarity quantification scheme through statistical approaches. Experimental result shows ACR could effectively exploit the potential of computation reuse for approximate computing and reduce 47.6% computations on average for a set of approximate applications.\"",
        "Document: \"OPUF: Obfuscation logic based physical unclonable function. The Physical Unclonable Function (PUF) has broad application prospects in the field of hardware security. The arbiter PUF is a typical kind of strong PUF. However, due to its deterministic logic, attackers can use modeling techniques to break it in short time. Therefore, this paper proposes an Obfuscation logic based PUF (OPUF) design. A Boolean obfuscation module is proposed to obfuscate the logic which is employed to select the path segments in the arbiter PUF. In this way, the nondeterminacy of PUF is improved, and the computation complexities of modeling attacks are significantly increased, making the OPUF much safer against modeling attack. Both the theoretical analysis and the experimental results show the proposed OPUF design has good stability and randomness.\"",
        "1 is \"An 8 Mb Multi-Layered Cross-Point ReRAM Macro With 443 MB/s Write Throughput.\", 2 is \"Mining data records in Web pages\"",
        "Given above information, for an author who has written the paper with the title \"An Efficient Observability Evaluation Algorithm Based on Factored Use-Def Chains.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005717": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'On the impact of the TCP acknowledgement frequency on energy efficient ethernet performance':",
        "Document: \"Using coordinated transmission with energy efficient ethernet. IEEE 802.3az Energy Efficient Ethernet (EEE) supports link active and sleep (idle) modes as a means of reducing the energy consumption of lightly utilized Ethernet links. A link wakes-up when an interface has packets to send and returns to idle when there are no packets. In this paper, we show how Coordinated Transmission (CT) in a 10GBASE-T link can allow for key physical layer (PHY) components to be shutdown to further reduce Ethernet energy consumption and enable longer cable lengths. CT is estimated to enable an additional 25% energy savings with a trade-off of an added frame latency of up to 40 \u00b5s, which is expected to have a negligible impact on most applications. The effective link capacity is approximately 4 Gb/s for symmetric traffic and close to 7 Gb/s for asymmetric traffic. This can be sufficient in many situations. Additionally a mechanism to switch to the normal full-duplex mode is proposed to allow for full link capacity when needed while retaining the additional energy savings when the link load is low.\"",
        "Document: \"Assembly admission control based on random packet selection at border nodes in Optical Burst-Switched networks. In Optical Burst-Switched (OBS) networks employing Just Enough Time (JET) signalling, Burst-Control Packets request resources\n at intermediate nodes following a one-way reservation protocol, that is, requested resources are not confirmed back to the\n source. Hence, data bursts are transmitted without any guarantees, and it sometimes occurs that these are dropped at a certain\n hop in the source\u2013destination path, hence wasting resources at previous hops. This effect is specially harmful if some connections\n are abusing of the global shared resources, violating their respective Service Level Agreements, thus causing: (1) global\n performance degradation; and, (2) unfair service received by other connections. This article proposes \u201cRandom Packet Assembly\n Admission Control\u201d, an admission control mechanism for OBS networks that moderates the two problems above. The mechanism monitors\n the network load status, detects which links are heavily loaded and decides which flows among the total traversing them require\n throughput decrease, on attempts to alleviate congestion and benefit other flows which are not abusing from the network. Such\n throughput decrease consists of preventive packet dropping during the assembly process at the ingress nodes of the OBS network,\n thus making no use of the network core. The numerical results show a substantial increase in the throughput experienced by\n well-behaved flows, and fundamental fairness achievement in the use of optical resources.\"",
        "Document: \"Blocking models of optical burst switches with shared wavelength converters: exact formulations and analytical approximations. Loss modeling of asynchronous optical burst switches with shared wavelength converters is considered. An exact analysis based\n on continuous time Markov chains is proposed and validated by comparison with simulation for balanced and unbalanced traffic.\n A computationally efficient approximated analysis is also proposed and compared with the exact model to find applicability\n conditions. Approximate loss performance evaluation is presented for ranges of values which are not tractable either by simulation\n or exact analysis.\"",
        "Document: \"Efficient soft error-tolerant adaptive equalizers. Soft errors are becoming an increasingly important issue for circuit reliability. Traditional techniques to protect against soft errors, like triple modular redundancy (TMR), have a large cost in terms of area and power. This has motivated the development of specific protection techniques for various types of circuits. In this paper, techniques to protect adaptive filters are presented, which provide reasonable reliability with reduced cost and power consumption. An adaptive equalizer case study is used to discuss and evaluate the proposed techniques in terms of both protection and cost.\"",
        "Document: \"An experimental power profile of Energy Efficient Ethernet switches. The access network is believed to account for 70-80% of the overall energy consumption of wired networks, attributable in part to the large number of small and inefficient switches deployed in typical homes and enterprises. In order to reduce the per-bit energy consumption of such devices, the Energy Efficient Ethernet (EEE) standard was approved as IEEE 802.3az in 2010 with the aim of making Ethernet devices more energy efficient. However, the potential for energy savings, and their dependence on traffic characteristics, is poorly understood. This paper undertakes a comprehensive study of the energy efficiency of EEE, and makes three new contributions: first, we perform extensive measurements on three commercial EEE switches, and show how their power consumption profile depends on factors such as port counts, traffic loads, packet sizes, and traffic burstiness. Second, we develop a simple yet powerful model that gives analytical estimates of the power consumption of EEE switches under various traffic conditions. Third, we validate the energy savings via experiments in typical deployment scenarios, and estimate the overall reduction in annual energy costs that can be realized with widespread adoption of EEE in the Internet.\"",
        "1 is \"Trends and challenges in VLSI circuit reliability\", 2 is \"Reducing Soft Errors through Operand Width Aware Policies\"",
        "Given above information, for an author who has written the paper with the title \"On the impact of the TCP acknowledgement frequency on energy efficient ethernet performance\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005755": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Exploring Robustness in Group Key Agreement':",
        "Document: \"Tradeoffs between security and communication performance in wireless mesh networks. In the context of wireless mesh networks (WMNs), we ask the question whether a high level of security can be achieved while providing good communication performance. To answer this question, we examine two techniques designed and shown to improve throughput performance in WMNs: high-throughput routing and network coding. Although the advantages of using these techniques hold in a benign setting, it is not clear whether they can be preserved under an adversarial setting. We analyze these techniques and reveal a wide range of security vulnerabilities. We then investigate whether alternative schemes can be designed to be secure and still preserve most of the advantages achieved in benign settings.\"",
        "Document: \"An on-demand secure routing protocol resilient to byzantine failures. An ad hoc wireless network is an autonomous self-organizing system ofmobile nodes connected by wireless links where nodes not in directrange can communicate via intermediate nodes. A common technique usedin routing protocols for ad hoc wireless networks is to establish therouting paths on-demand, as opposed to continually maintaining acomplete routing table. A significant concern in routing is theability to function in the presence of byzantine failures whichinclude nodes that drop, modify, or mis-route packets in an attempt todisrupt the routing service.We propose an on-demand routing protocol for ad hoc wireless networks that provides resilience to byzantine failures caused by individual or colluding nodes. Our adaptive probing technique detects a malicious link after log n faults have occurred, where n is the length of the path. These links are then avoided by multiplicatively increasing their weights and by using an on-demand route discovery protocol that finds a least weight path to the destination.\"",
        "Document: \"Secure network coding for wireless mesh networks: Threats, challenges, and directions. In recent years, network coding has emerged as a new communication paradigm that can significantly improve the efficiency of network protocols by requiring intermediate nodes to mix packets before forwarding them. Recently, several real-world systems have been proposed to leverage network coding in wireless networks. Although the theoretical foundations of network coding are well understood, a real-world system needs to solve a plethora of practical aspects before network coding can meet its promised potential. These practical design choices expose network coding systems to a wide range of attacks. We identify two general frameworks (inter-flow and intra-flow) that encompass several network coding-based systems proposed in wireless networks. Our systematic analysis of the components of these frameworks reveals vulnerabilities to a wide range of attacks, which may severely degrade system performance. Then, we identify security goals and design challenges in achieving security for network coding systems. Adequate understanding of both the threats and challenges is essential to effectively design secure practical network coding systems. Our paper should be viewed as a cautionary note pointing out the frailty of current network coding-based wireless systems and a general guideline in the effort of achieving security for network coding systems.\"",
        "Document: \"Why Do Adversarial Attacks Transfer? Explaining Transferability Of Evasion And Poisoning Attacks. Transferability captures the ability of an attack against a machine-learning model to be effective against a different, potentially unknown, model. Empirical evidence for transferability has been shown in previous work, but the underlying reasons why an attack transfers or not are not yet well understood. In this paper, we present a comprehensive analysis aimed to investigate the transferability of both test-time evasion and training-time poisoning attacks. We provide a unifying optimization framework for evasion and poisoning attacks, and a formal definition of transferability of such attacks. We highlight two main factors contributing to attack transferability: the intrinsic adversarial vulnerability of the target model, and the complexity of the surrogate model used to optimize the attack. Based on these insights, we define three metrics that impact an attack's transferability. Interestingly, our results derived from theoretical analysis hold for both evasion and poisoning attacks, and are confirmed experimentally using a wide range of linear and non-linear classifiers and datasets.\"",
        "Document: \"Plagiarizing smartphone applications: attack strategies and defense techniques. In this paper, we show how an attacker can launch malware onto a large number of smartphone users by plagiarizing Android applications and by using elements of social engineering to increase infection rate. Our analysis of a dataset of 158,000 smartphone applications meta-information indicates that 29.4% of the applications are more likely to be plagiarized. We propose three detection schemes that rely on syntactic fingerprinting to detect plagiarized applications under different levels of obfuscation used by the attacker. Our analysis of 7,600 smartphone application binaries shows that our schemes detect all instances of plagiarism from a set of real-world malware incidents with 0.5% false positives and scale to millions of applications using only commodity servers.\"",
        "1 is \"Admission Control in Peer Groups\", 2 is \"Linear cryptanalysis method for DES cipher\"",
        "Given above information, for an author who has written the paper with the title \"Exploring Robustness in Group Key Agreement\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005778": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Modifying Perceived Size of a Handled Object through Hand Image Deformation':",
        "Document: \"Inter-glow: interaction by controlling light. inter-glow is a system that facilitates close interaction and communication among users in real spaces by using multiplexed visible-light communication technology. By pointing light to shine on an object contains an embedded photo sensor, users can get information about the object. In addition, several users can communicate with each other intimately by shining light on the object at the same time. We built a prototype to show the technology plain. In our prototype, when users shine lamps on a table in a miniature living room, the system recognizes which lamps are illuminated the table. According to the combination of illuminating lamps, the system produces family conversations.\"",
        "Document: \"Digital display case using non-contact head tracking. In our research, we aim to construct the Digital Display Case system, which enables a museum exhibition using virtual exhibits using computer graphics technology, to convey background information about exhibits effectively. In this paper, we consider more practical use in museum, and constructed the system using head tracking, which doesn't need to load any special devices on users. We use camera and range camera to detect and track user's face, and calculate images on displays to enable users to appreciate virtual exhibits as if they were really in the virtual case.\"",
        "Document: \"Augmented endurance: controlling fatigue while handling objects by affecting weight perception using augmented reality. The main contribution of this paper is to develop a method for alleviating fatigue during handling medium-weight objects and augmenting our endurance by affecting our weight perception with augmented reality technology. To assist people to lift medium-weight objects without a complex structure or various costs, we focus on the phenomenon that our weight perception during handling objects is affected by visual properties. Our hypothesis is that this illusionary effect in weight perception can be applied to reduce fatigue while handling medium-weight objects without mechatronics-based physical assistance. In this paper, we propose an augmented reality system that changes the brightness value of an object in order to reduce fatigue while handling the object. We conducted two fundamental experiments to investigate the effectiveness of the proposed system. Our results suggested that the system eliminates the need to use excess energy for handling objects and reduces fatigue during the handling task.\"",
        "Document: \"Factors And Influences Of Body Ownership Over Virtual Hands. The sense that one's own body belongs to oneself is called \"Body Ownership\". Until now, body ownership is investigated in a psychology field. However, the investigation of the origins and functional role of the sense of one's own body not only explores the philosophical question, but it also examines the issues that should be investigated before VR becomes widely used in our daily life. In this paper, we surveyed the existing literature on body ownership and categorized factors into bottom-up and top-down processes. We also identified the perceptual, cognitive, and behavioral influences of body ownership. We then discussed some promising areas for future research in the VR field. As for factors, it is promising to consider the interaction between the body and the external world, and to develop a unified evaluation standard and theoretical framework for research in this field. As for influences, it is promising to consider the higher, longer-term cognitive influences in the social context, and to apply them in the context of body augmentation in the real world.\"",
        "Document: \"Influencing driver behavior through future expressway traffic predictions. Unlike trains or buses, automobiles are essentially uncontrollable for road management companies. However, we can influence driver behavior to a certain degree by displaying appropriate traffic information. When taking a break at the rest area of an expressway, many drivers are unaware of the future traffic conditions along the highway toward their destination. We propose a method for changing the driver's departure time by predicting the future traffic conditions and informing the driver on how long the driver will expect to wait in a traffic jam based on the time at which the driver chooses to depart from the rest area. We created a prototype system for implementing the proposed method using the driver's smartphone and big data regarding the traffic conditions on an expressway in real-time. In this paper, we report the survey results of an elementary questionnaire regarding the proposed method.\"",
        "1 is \"Walking walking-in-place flying, in virtual environments\", 2 is \"Labeling images with a computer game\"",
        "Given above information, for an author who has written the paper with the title \"Modifying Perceived Size of a Handled Object through Hand Image Deformation\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005789": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'MTCrossBit: A Dynamic Binary Translation System Using Multithreaded Optimization Framework':",
        "Document: \"Recursive histogram modification: establishing equivalency between reversible data hiding and lossless data compression. State-of-the-art schemes for reversible data hiding (RDH) usually consist of two steps: first construct a host sequence with a sharp histogram via prediction errors, and then embed messages by modifying the histogram with methods, such as difference expansion and histogram shift. In this paper, we focus on the second stage, and propose a histogram modification method for RDH, which embeds the message by recursively utilizing the decompression and compression processes of an entropy coder. We prove that, for independent identically distributed (i.i.d.) gray-scale host signals, the proposed method asymptotically approaches the rate-distortion bound of RDH as long as perfect compression can be realized, i.e., the entropy coder can approach entropy. Therefore, this method establishes the equivalency between reversible data hiding and lossless data compression. Experiments show that this coding method can be used to improve the performance of previous RDH schemes and the improvements are more significant for larger images.\"",
        "Document: \"A Statistics-Based Watermarking Scheme Robust to Print-and-Scan. This paper proposes a print-and-scan (PS) resilient watermarking scheme based on the mean of DCT coefficients (MDC) for a fixed AC frequency of image. We investigate the property of MDC in the PS process and find that the MDC is nearly unchanged after PS, which is approximately zero for natural (non-watermarked) images. The watermark bits are embedded by shifting these PS-invariants to be positive or negative. The embedding intensity of the scheme is determined by a JND model to avoid the visible distortion. Experimental results show that the scheme can successfully resist the PS attack in different printing status.\"",
        "Document: \"Steganalysis of a PVD-based content adaptive image steganography. Pixel-value-differencing (PVD) is a well-known technique for content adaptive steganography. By this technique, secret data are embedded into the differences of adjacent pixels. Recently, a new PVD-based steganographic method is proposed by Luo et al. Besides realizing adaptive embedding using PVD, the new method also exploits a pairwise modification mechanism to reduce the distortion. In this work, a targeted detector is devised to detect the new PVD-based steganography. We show that although content adaptive approach may enhance the stego-security, Luo et al.'s PVD-based scheme is not a good choice for realizing adaptive embedding since it contains a serious design flaw in data embedding procedure and this flaw can lead to possible attacks. More specifically, by counting the differences of adjacent pixels in both vertical and horizontal directions, a folded difference-histogram is generated and we show that Luo et al.'s PVD-based method may arise significant artifact to this histogram which can be exploited for reliable detection. Experimental results verify that Luo et al.'s PVD-based method can be detected by the proposed detector even at a low embedding rate of 0.05 bits per pixel.\"",
        "Document: \"Efficient reversible watermarking based on adaptive prediction-error expansion and pixel selection. Prediction-error expansion (PEE) is an important technique of reversible watermarking which can embed large payloads into digital images with low distortion. In this paper, the PEE technique is further investigated and an efficient reversible watermarking scheme is proposed, by incorporating in PEE two new strategies, namely, adaptive embedding and pixel selection. Unlike conventional PEE which embeds data uniformly, we propose to adaptively embed 1 or 2 bits into expandable pixel according to the local complexity. This avoids expanding pixels with large prediction-errors, and thus, it reduces embedding impact by decreasing the maximum modification to pixel values. Meanwhile, adaptive PEE allows very large payload in a single embedding pass, and it improves the capacity limit of conventional PEE. We also propose to select pixels of smooth area for data embedding and leave rough pixels unchanged. In this way, compared with conventional PEE, a more sharply distributed prediction-error histogram is obtained and a better visual quality of watermarked image is observed. With these improvements, our method outperforms conventional PEE. Its superiority over other state-of-the-art methods is also demonstrated experimentally.\"",
        "Document: \"Rappep: A Framework For Deploying Router-Assisted Congestion Control Protocols At Tcp Performance Enhancement Proxy. Router Assisted congestion control Protocols (RAPs) appear to be the most efficient solutions to the TCP performance degradation issue in high Bandwidth Delay Product (BDP) networks. Global deployment of RAPs such as XCP, VCP, and MPCP however, has been challenging due to their need for router support. In this paper, we propose RAPPEP a framework for deploying RAPs on potential congestion zones such as satellite links that are locally utilizing the architecture of TCP Performance Enhancement Proxy (PEP). Such a marriage allows for an immediate deployment of RAPs without the need for global router support, while still being able to take advantage of sophisticated RAPs. Beyond the deployed congestion zone, RAPPEP is completely transparent to the rest of the network including end nodes and other routers. Adapting from two implementations of RAPs and an implementation of TCP PEP (PEPSal), we implement and integrate RAPPEP in the Linux kernel and demonstrate its performance improvement compared to PEPSal through emulation studies.\"",
        "1 is \"A Passive-Blind Forgery Detection Scheme Based on Content-Adaptive Quantization Table Estimation\", 2 is \"Whole-system persistence\"",
        "Given above information, for an author who has written the paper with the title \"MTCrossBit: A Dynamic Binary Translation System Using Multithreaded Optimization Framework\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005794": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A component-level self-configuring personal agent platform for pervasive computing':",
        "Document: \"An effective signal strength-based wireless location estimation system for tracking indoor mobile users. Localization is an essential function for location-dependent services. Although various efficient localization methods have been proposed, many of them have not tested with practical applications. Different location-dependent applications may have very different performance and operation requirements such as the accuracy in localization and frequency of location tracking. In this paper, based on our previous works on the design of efficient localization methods, we study the important issues in the design and implementation of a location tracking system for monitoring people in an indoor environment within a large building such as in rehabit centres and schools. In estimating the current location of a mobile user, we adopt the received signal strength indicator (RSSI) approach. Enhancements are proposed to improve the accuracy in localization using RSSI. To ensure the reliability of the location results and the efficiency in location tracking, it is important to minimize the traffic of location data on the network, as well as the chance of overloading the estimation system. Various issues in the design of the system are discussed and the performance results of the system are provided to illustrate the efficiency and the limitations of the proposed methods.\"",
        "Document: \"Fast Setup and Robust WiFi Localization for the Exhibition Industry. With the prevalence of WiFi devices (e.g., smartphones), many new business opportunities are being spawned by exploiting the routes or locations of users. Nevertheless, most current approaches based on received signal strength indicator (RSSI) usually assume that the targeted (tracked) devices have the transmission characteristics similar to the devices used for system training. This is far from the reality and may lead to considerable errors. We propose a robust localization approach which automatically infers a customized signal-strength-to-distance function for every device on-the-fly, and simultaneously estimates the location of it. This is achieved by first approximating the function with a set of piecewise linear functions, for each targeted device, and the parameters of the linear functions are updated, with an Expectation Maximum (EM) algorithm, when more and more RSSI data of the device are available. Specifically, during the expectation step of the EM algorithm, the location of the targeted device is estimated. Whereas in the maximization step of the algorithm, the parameters of the linear functions customized for that device are updated. As the approach is capable of learning the parameters during localization, training process or system calibration is unnecessary and thus the system setup time can be shortened. This feature is practical for meeting the special needs of the exhibition industry: extremely tight schedules for setting up the site from scratch and extremely large venues. With our testbeds, experimental results show that the mean localization error can be kept about 1.7 meters for different mobile devices with different transmission characteristics. A real-world test at Hong Kong Convention and Exhibition Centre (HKCEC) was also conducted and various mobile devices can be tracked accurately with little setup time.\"",
        "Document: \"SmartMind: Activity Tracking and Monitoring for Patients with Alzheimer's Disease. In this paper, we introduce SmartMind, an activity tracking and monitoring system to help Alzheimer's diseases (AD) patients to live independently within their living rooms while providing emergent help and support when necessary. Allowing AD patients to handle their daily activities not only can release some of the burdens on their families and caregivers, but also is highly important to help them regain confidence towards a healthy life and reduce the degeneration rates of their memories. The daily activities of a patient captured from Smart Mind can also serve as important indicators to describe his/her normal living habit (NLH). By checking with NLH, the patient's current health status can be estimated on a daily basis.\"",
        "Document: \"Capturing and Analyzing Pervasive Data for SmartHealth. In this paper, we study how mobile computing and wireless technologies can be explored to provide effective ubiquitous healthcare services. Instead of reinventing the wheels, we make use of smartphones, off-the-shelf components, and existing technologies in ubiquitous computing (i.e. wireless and mobile positioning technologies, and data acquisition techniques and processing via sensors) to develop a middleware, and tools for the development of systems and applications to provide effective ubiquitous healthcare services. Two main tasks to be studied are: 1) Developing a framework, called Smart Health, to provide the infrastructure and architectural support for realizing ubiquitous healthcare services, and 2) Designing and developing ubiquitous healthcare applications by utilizing the SmartHelath framework to let users experience and benefit from the provided services. We use scenarios to illustrate how mobile/wireless and sensor technologies can enable ubiquitous healthcare services in Smart Health. Some of the examples included in Smart Health are: location tracking, vital signs and well-being data acquisition and analysis, fall detection and behavior monitoring, and sleep analysis. As a start, based on the Smart Health framework, we introduce a smartphone app, called Smart Mood, for tracking the mood of patients who are suffering mood disorder (i.e., manic and depression) to demonstrate how Smart Health can effectively enable ubiquitous healthcare services.\"",
        "Document: \"Using Lda Method To Provide Mobile Location Estimation Services Within A Cellular Radio Network. Mobile location estimation is becoming an important value-added service for a mobile phone operator. It is well-known that GPS can provide an accurate location estimation. But it is also a known fact that GPS does not perform well in urban areas like downtown New York and cities like Hong Kong. Then many mobile location estimation approaches based on the cellular radio networks have been proposed to compensate the problem of the lost of GPS signals for providing location services to mobile users in metropolitan areas, but there exists no general solution since each algorithm has its own advantage depending on specific terrain and environmental factors. In this paper, we propose a selector method with LDA among different kinds of mobile location estimation algorithms we had proposed in previous work to combine their merits, then provide a more accurate estimation for location services. And we build up a threelevel binary decision tree to classify these four algorithms. These three levels are named as Stat-Geo level, CG-nonCG level and CT-EPM level. And the success ratios of these three levels are 85.22%, 88.45% and 88.89% respectively. We have tested our selector method with real data taken in Hong Kong and the experiment results have shown that our selector method outperforms other existing location estimation algorithms among different kinds of terrains.\"",
        "1 is \"Fault tolerant deployment and topology control in wireless ad hoc networks\", 2 is \"Statistical characteristics and multiplexing of MPEG streams\"",
        "Given above information, for an author who has written the paper with the title \"A component-level self-configuring personal agent platform for pervasive computing\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005869": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'System-on-Chip Architecture Design for Intelligent Sensor Networks':",
        "Document: \"An effective chip implementation of a real-time eight-channel EEG signal processor based on on-line recursive ICA algorithm. This paper presents an effective chip implementation of a real-time eight-channel electroencephalogram signal processor based on on-line recursive independent component analysis (ORICA) algorithm. The system architecture is composed of a memory unit, a whitening unit, an ORICA training unit, and an ORICA computation unit. The proposed architecture is implemented using TSMC 90 nm CMOS technology. It occupies a core area of 800\u00d7800 \u03bcm2 and consumes 4.18 mW at a core supply voltage of 1.0 V and 50 MHz clock operating frequency. Simulated super and sub-Gaussian signals are used to verify the system. The separated signals match those obtained using off-line Matlab-based analysis.\"",
        "Document: \"Design Of Multi-Mode Depth Buffer Compression For 3d Graphics System. An innovative multi-mode depth buffer compression algorithm has been developed for 3D graphics system. It adaptively compresses the depth buffer data according to different scene changes by employing 19 compression modes generated from three compression algorithms including DDPCM, HA, and general DDPCM. Furthermore, this novel algorithm supports one-plane and two-plane compression modes and manipulates break points more efficiently. For 8x8 tile size and 16-bit depth values, the proposed multi-mode algorithm can achieve 1.91:1 compression ratio on average and improve 76.9% and 39.4% compared with the HA and the DDPCM compression methods, respectively. The modified efficient depth buffer compression can achieve 10.56:1 and 7.76:1 compression ratio in one-plane mode and two-plane mode, respectively. The modified DDPCM can achieve 6.48:1 and 5.39:1 compression ratio in one-plane mode and two-plane mode, respectively.\"",
        "Document: \"Reconfigurable Depth Buffer Compression Design for 3D Graphics System. Depth buffer bandwidth reduction is one of the most important issues in bandwidth-limited 3D computer graphics system. In this paper, we propose a reconfigurable algorithm for depth buffer compression. For 8x8 tile size and 16-bit depth values, the proposed algorithm can achieve 1.91:1 compression ratio on average and improve 76.9% and 39.4% compared with HA [12] and DDPCM [7], respectively. Furthermore, the devised algorithm supports one-plane and two-plane compression modes and manipulates break points more efficiently.\"",
        "Document: \"An efficient VLSI implementation of SVD processor of on-line recursive ICA for real-time EEG system. This paper presents an efficient VLSI implementation of a singular value decomposition (SVD) processor of on-line recursive independent component analysis (ORICA) for use in a real-time electroencephalography (EEG) system. ICA is a well-known method for blind source separation (BBS), which helps to obtain clear EEG signals without artifacts. In general, computations of ORICA are complicated and the critical computational latency is associated with the SVD process. Accordingly, the performance of the SVD processor should be prioritized. Going beyond previous research [1], this work presents a novel design of coordinate rotation digital computer (CORDIC) engine which is optimized and speeded up to avoid structural hazards. Finally, the processor is fabricated using TSMC 40nm CMOS technology in a 16-channel EEG system. The computation time of the SVD processor is reduced by 24.7% and the average correlation coefficient between original source signals and extracted ORICA signals is 0.95452.\"",
        "Document: \"Real-time computing of optical flow using adaptive VLSI neuroprocessors. The multilayer stochastic neural network and its associated VLSI array neuroprocessors are presented for VLSI optical flow computing. This network is well-suited to VLSI implementation due to the high parallelism and local connectivity. Instead of using deterministic scheme, a stochastic decision rule implemented with electronic annealing techniques is used to search optimal solutions. VLSI array neuroprocessor architecture is proved to be an effective supercomputing hardware for real-time optical flow applications. A prototype 25-neuron chip for this VLSI array neuroprocessors (called a velocity-selective hyperneuron chip) has been implemented using MOSIS 2-\u03bcm CMOS technology. A real-time optical flow machine is feasible by using arrays of hyperneuron chips\"",
        "1 is \"Larrabee: a many-core x86 architecture for visual computing\", 2 is \"A survey of passive technology for digital image forensics\"",
        "Given above information, for an author who has written the paper with the title \"System-on-Chip Architecture Design for Intelligent Sensor Networks\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005878": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Output tracking of probabilistic Boolean networks by output feedback control.':",
        "Document: \"Observing and Interpreting Correlations in Metabolic Networks. Motivation: Metabolite profiling aims at an unbiased identification and quantification of all the metabolites present in a biological sample. Based on their pair-wise correlations, the data obtained from metabolomic experiments are organized into metabolic correlation networks and the key challenge is to deduce unknown pathways based on the observed correlations. However, the data generated is fundamentally different from traditional biological measurements and thus the analysis is often restricted to rather pragmatic approaches, such as data mining tools, to discriminate between different metabolic phenotypes. Methods and Results: We investigate to what extent the data generated networks reflect the structure of the underlying biochemical pathways. The purpose of this work is 2-fold: Based on the theory of stochastic systems, we first introduce a framework which shows that the emergent correlations can be interpreted as a 'fingerprint' of the underlying biophysical system. This result leads to a systematic relationship between observed correlation networks and the underlying biochemical pathways. In a second step, we investigate to what extent our result is applicable to the problem of reverse engineering, i.e. to recover the underlying enzymatic reaction network from data. The implications of our findings for other bioinformatics approaches are discussed.\"",
        "Document: \"Classifying cardiac biosignals using ordinal pattern statistics and symbolic dynamics. The performance of (bio-)signal classification strongly depends on the choice of suitable features (also called parameters or biomarkers). In this article we evaluate the discriminative power of ordinal pattern statistics and symbolic dynamics in comparison with established heart rate variability parameters applied to beat-to-beat intervals. As an illustrative example we distinguish patients suffering from congestive heart failure from a (healthy) control group using beat-to-beat time series. We assess the discriminative power of individual features as well as pairs of features. These comparisons show that ordinal patterns sampled with an additional time lag are promising features for efficient classification.\"",
        "Document: \"A Complex Network-Based Broad Learning System for Detecting Driver Fatigue From EEG Signals. Driver fatigue detection is of great significance for guaranteeing traffic safety and further reducing economic as well as societal loss. In this article, a novel complex network (CN) based broad learning system (CNBLS) is proposed to realize an electroencephalogram (EEG)-based fatigue detection. First, a simulated driving experiment was conducted to obtain EEG recordings in alert and fatigue state. Then, the CN theory is applied to facilitate the broad learning system (BLS) for realizing an EEG-based fatigue detection. The results demonstrate that the proposed CNBLS can accurately differentiate the fatigue state from an alert state with high stability. In addition, the performances of the four existing methods are compared with the results of the proposed method. The results indicate that the proposed method outperforms these existing methods. In comparison to directly using EEG signals as the input of BLS, CNBLS can sharply improve the detection results. These results demonstrate that it is feasible to apply BLS in classifying EEG signals by means of CN theory. Also, the proposed method enriches the EEG analysis methods.\"",
        "Document: \"Secure and Energy-Efficient Data Transmission System Based on Chaotic Compressive Sensing in Body-to-Body Networks. Applications of wireless body area networks (WBANs) are extended from remote health care to military, sports, disaster relief, etc. With the network scale expanding, nodes increasing, and links complicated, a WBAN evolves to a body-to-body network. Along with the development, energy saving and data security problems are highlighted. In this paper, chaotic compressive sensing (CCS) is proposed to s...\"",
        "Document: \"On controllability of neuronal networks with constraints on the average of control gains. Control gains play an important role in the control of a natural or a technical system since they reflect how much resource is required to optimize a certain control objective. This paper is concerned with the controllability of neuronal networks with constraints on the average value of the control gains injected in driver nodes, which are in accordance with engineering and biological backgrounds....\"",
        "1 is \"Event-Triggered Control for the Disturbance Decoupling Problem of Boolean Control Networks.\", 2 is \"Adaptive fuzzy control of uncertain stochastic nonlinear systems with unknown dead zone using small-gain approach\"",
        "Given above information, for an author who has written the paper with the title \"Output tracking of probabilistic Boolean networks by output feedback control.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005950": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Computationally Inexpensive Tracking Control of High-Speed Trains With Traction/Braking Saturation':",
        "Document: \"A Cognitive Control Approach to Communication-Based Train Control Systems. Communication-based train control (CBTC) is an automated train control system using bidirectional train\u2013ground wireless communications to ensure the safe operation of rail vehicles. Due to unreliable wireless communications and train mobility, the train control performance can be significantly affected by wireless networks. Although some works have been done to study CBTC systems from both train\u2013ground communication and train control perspectives, these two important areas have traditionally been separately addressed. In this paper, with recent advances in cognitive dynamic systems, we take a cognitive control approach to CBTC systems considering both train\u2013ground communication and train control. In our approach, the notion of information gap is adopted to quantitatively describe the effects of train\u2013ground communication on train control. Moreover, unlike the existing works that use network capacity as the design measure, in this paper the linear quadratic cost for the train control performance in CBTC systems is considered in the performance measure. Reinforcement learning is applied to obtain the optimal policy based on the performance measure, which includes linear quadratic cost and information gap. In addition, the wireless channel is modeled as finite-state Markov chains with multiple state transition probability matrices, which can demonstrate the characteristics of both large-scale and small-scale fading. The channel state transition probability matrices are derived from real field measurement results. Simulation results show that the proposed cognitive control approach can significantly improve the train control performance in CBTC systems.\"",
        "Document: \"Recursive least squares estimation algorithm applied to a class of linear-in-parameters output error moving average systems. This letter deals with the identification problem of a class of linear-in-parameters output error moving average systems. The difficulty of identification is that there exist some unknown variables in the information vector. By means of the auxiliary model identification idea, an auxiliary model based recursive least squares algorithm is developed for identifying the parameters of the proposed system. The simulation results confirm the conclusion.\"",
        "Document: \"An energy-efficient control scheme for communication-based train control (CBTC) systems with random packet drops. Communication-based train control (CBTC) systems use wireless local area networks (WLANs) to transmit train status and control commands. Random transmission delays and packet drops are inevitable in train-ground communication, which could result in unnecessary traction, brake or even emergency brake of trains, loss of line capacity and passenger satisfaction. In this paper, we study the random packet drops in CBTC systems, analyze their impact on the performances of CBTC systems, and propose a control scheme to improve CBTC performances. We model the system to control a group of trains as a networked control system with packet drops in transmissions. Extensive simulation results are presented. We show that our proposed scheme can provide less energy consumption, better riding comfortability and total cost reduction.\"",
        "Document: \"An integrated error-detecting method based on expert knowledge for GPS data points measured in Qinghai-Tibet Railway. As there are huge amounts of Global Positioning System (GPS) data points measured in the Qinghai\u2013Tibet Railway (QTR) with a length of 1142km, it was inevitable that some measuring errors existed due to various situations in measurement. It is very important to develop a method to automatically detect the possible errors in all data points so as to modify them or measure them again to improve the reliability of GPS data. Four error patterns, including redundant measurement, sparse measurement, back-and-forth measurement, and big angle change, were obtained based on expert knowledge. Based on the four error patterns, four algorithms were developed to detect the corresponding possible errors in data points. To delete the repetitive errors by different algorithms and effectively display the possible errors, an integrated error-detecting method was developed by reasonably assembling the four algorithms. After four performance indices were given to evaluate the performance of the error-detecting method, six GPS track data sets between seven railway stations in the QTR were used to validate the method. Thirty-eight segments of some sequential points that are possibly wrong were found by the method and fourteen of them were confirmed by measurement experts. The detecting rate of the method was 100% and the duration time of the detecting process was less than half an hour compared with the 94h manual workload. The validation results show that the method is effective not only in decreasing workload, but also in ensuring correctness by integrating the domain expert knowledge to make the final decision.\"",
        "Document: \"Runtime Verification with Multi-valued Formula Rewriting. Runtime verification is a promising method that tries to bridge the gap between formal methods and traditional testing. In this paper, we present an improved runtime verification method via multi-valued formula rewriting. A 3-valued executable semantics for finite trace LTL is formally defined, and an algorithm based on this new semantics is proposed and implemented in Maude, which is a high performance rewriting system. To improve the efficiency of our algorithm, we introduce a novel approximation technique, which reduces rewriting steps by sacrificing some abilities of detecting the satisfactions of LTL properties. Moreover, this technique provides a quick procedure for distinguishing non-monitor able properties from those can be monitored. Finally, experiments are conducted to show the strength and weakness of the presented method.\"",
        "1 is \"An Ensemble ELM Based on Modified AdaBoost.RT Algorithm for Predicting the Temperature of Molten Steel in Ladle Furnace\", 2 is \"Distributed adaptive tracking control for synchronization of unknown networked Lagrangian systems.\"",
        "Given above information, for an author who has written the paper with the title \"Computationally Inexpensive Tracking Control of High-Speed Trains With Traction/Braking Saturation\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005984": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Investigating and improving a COTS-based software development':",
        "Document: \"An Empirical Study of Reliability Growth of Open versus Closed Source Software through Software Reliability Growth Models. The purpose of this study is to analyze the reliability growth of Open Source Software (OSS) versus software developed in-house (i.e. Closed Source Software, CSS) using Software Reliability Growth Model. This study uses 22 datasets containing failure data of CSS and 18 datasets containing defect data of OSS projects. The failure data are modelled by eight SRGM (Musa Okumoto, Inflection S-Shaped, Goel Okumoto, Delayed S-Shaped, Logistic, Gompertz, Yamada Exponential, and Generalized Goel Model). These models are chosen due to their widespread use. The results can be summarized as follows \u8072 All selected SRGM fit to defect data of the OSS projects in the same manner as that of CSS. OSS reliability grows similarly to that of CSS. \u8072 Musa Okumoto and Inflection S-Shaped perform well for CSS, while for OSS Inflection S-Shaped and Gompertz are good performers. \u8072 SRGM can be used for reliability characterization of OSS projects.\"",
        "Document: \"Building a software cost estimation model based on categorical data. This paper explores the possibility of generating a multi-organisational software cost estimation model by analysing the software cost data collected by the International Software Benchmarking Standards Group. This database contains data about recently developed projects characterised mostly by attributes of categorical nature such as the project business area, organisation type, application domain and usage of certain tools or methods. The generation of the model is based on a statistical technique which has been proposed as alternative to the standard regression approach, namely the categorical regression or regression with optimal scaling. This technique starts with the quantification of the qualitative attributes (expressed either on nominal or ordinal scale), that appear frequently within such data, and proceeds by using the obtained scores as independent variables of a regression model. The generated model is validated by measuring certain indicators of accuracy.\"",
        "Document: \"A framework for object oriented design and prototyping of manufacturing systems. Object-oriented programming, through the use of a library of standard classes and their inheritance relationships, offers a consistent framework to specify, design, and prototype the software architecture of complex discrete event control systems. This work applies such a framework to computer integrated manufacturing, proposing a unified view to represent the hierarchical control structure of a commonly accepted reference model. The results are a library of classes (called G++ and based on the C++ programming language) to express concurrency and to support an extended client server paradigm, and a new design methodology\"",
        "Document: \"Framework Based Software Development: Investigating the Learning Effect. We present a case study in framework based software development, with specific analysis of the effect of learning on productivity and defect density. The framework supports the development of multimedia, web-based services on a digital network. It uses a CORBA infrastructure, is developed in Java, and integrates COTS (Component Off-The-Shelf).The case study considers the development of the framework and the development of a number of applications reusing the framework. Some of the applications are also developed without the framework. The study uses a nested factors experimental design, and measures effort, application size, defects, productivity, reuse level.The main result of the study is the correlation found between framework learning and productivity increase; and between framework learning and defect density decrease. We also underline an impressive difference in productivity between traditional development, development of the framework and development with the framework.\"",
        "Document: \"Measurement processes are software, too. Software process improvement and measurement are closely linked: measures are the only way to prove improvements in a process. Despite this link, and the interest in process improvement, measurement is not widely applied in industrial software production. This paper describes a method designed to guide the definition, implementation and operation of measurement processes. The method, which builds upon Fenton's measurement framework and GQM, starts from the point that measuring a software process is in its turn a process in the software process. The three basic ideas of the method are derived from this assumption: the measurement process should reuse and suitably adapt the same phases of the software process: requirements definition, design, implementation, etc. A descriptive process model should be the essential starting point of a measurement process. Many concepts and tools which derive from the object oriented approach should be effectively used in the measurement process. An experimental application in an industrial process has shown that building the process model was the hardest part of the measurement process, and that it has improved the quality of measurement by reducing misunderstandings. Object oriented concepts and tools make it possible to automate certain tasks (for instance the definition of the schema of the measurement database) and to improve robustness against changes in the measurement process. (C) 1999 Elsevier Science Inc. All rights reserved.\"",
        "1 is \"Ultrascalable Implicit Finite Element Analyses in Solid Mechanics with over a Half a Billion Degrees of Freedom\", 2 is \"Natural Language Processing as a Foundation of the Semantic Web\"",
        "Given above information, for an author who has written the paper with the title \"Investigating and improving a COTS-based software development\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005987": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Dynamic MEMD Associated with Approximate Entropy in Patients' Consciousness Evaluation.':",
        "Document: \"Sequential nonnegative tucker decomposition on multi-way array of time-frequency transformed event-related potentials. Tensor factorization has exciting advantages to analyze EEG for simultaneously exploiting its information in the time, frequency and spatial domains as well as for sufficiently visualizing data in different domains concurrently. Event-related potentials (ERPs) are usually investigated by the group-level analysis, for which tensor factorization can be used. However, sizes of a tensor including time-frequency representation of ERPs of multiple channels of multiple participants can be immense. It is time-consuming to decompose such a tensor. The low-rank approximation based sequential nonnegative Tucker decomposition (LraSNTD) has been recently developed and shown to be computationally efficient with respect to some benchmark datasets. Here, LraSNTD is applied to decompose a fourth-order tensor representation of ERPs. We find that the decomposed results from LraSNTD and a benchmark nonnegative Tucker decomposition algorithm are very similar. So, LraSNTD is promising for ERP studies.\"",
        "Document: \"Two-Stage temporally correlated source extraction algorithm with its application in extraction of event-related potentials. To extract source signals with certain temporal structures, such as periodicity, we propose a two-stage extraction algorithm. Its first stage uses the autocorrelation property of the desired source signal, and the second stage exploits the independence assumption. The algorithm is suitable to extract periodic or quasi-periodic source signals, without requiring that they have distinct periods. It outperforms many existing algorithms in many aspects, confirmed by simulations. Finally, we use the proposed algorithm to extract the components of visual event-related potentials evoked by three geometrical figure stimuli, and the classification accuracy based on the extracted components achieves 93.2%.\"",
        "Document: \"A novel oddball paradigm for affective BCIs using emotional faces as stimuli. The studies of P300-based brain computer interfaces (BCIs) have demonstrated that visual attention to an oddball event can enhance the event-related potential (ERP) time-locked to this event. However, it was unclear that whether the more sophisticated face-evoked potentials can also be modulated by related mental tasks. This study examined ERP responses to objects, faces, and emotional faces when subjects performs visual attention, face recognition and categorization of emotional facial expressions respectively in an oddball paradigm. The results revealed the significant difference between target and non-target ERPs for each paradigm. Furthermore, the significant difference among three mental tasks was observed for vertex-positive potential (VPP) (ppp\"",
        "Document: \"Smooth PARAFAC Decomposition for Tensor Completion. In recent years, low-rank based tensor completion, which is a higher order extension of matrix completion, has received considerable attention. However, the low-rank assumption is not sufficient for the recovery of visual data, such as color and 3D images, when the ratio of missing data is extremely high. In this paper, we consider \u201csmoothness\u201d constraints as well as low-rank approximations and propose an efficient algorithm for performing tensor completion that is particularly powerful regarding visual data. The proposed method admits significant advantages, owing to the integration of smooth PARAFAC decomposition for incomplete tensors and the efficient selection of models in order to minimize the tensor rank. Thus, our proposed method is termed as \u201csmooth PARAFAC tensor completion (SPC).\u201d In order to impose the smoothness constraints, we employ two strategies, total variation (SPC-TV) and quadratic variation (SPC-QV), and invoke the corresponding algorithms for model learning. Extensive experimental evaluations on both synthetic and real-world visual data illustrate the significant improvements of our method, in terms of both prediction performance and efficiency, compared with many state-of-the-art tensor completion methods.\"",
        "Document: \"Patients' consciousness analysis using dynamic approximate entropy and MEMD method. Electroencephalography (EEG) based preliminary examination has been proposed in the clinical brain death determination. Multivariate empirical mode decomposition(MEMD) and approximate entropy(ApEn) are often used in the EEG signal analysis process. MEMD is an extended approach of empirical mode decomposition(EMD), in which it overcomes the problem of the decomposed number and frequency, and enables to extract brain activity features from multi-channel EEG simultaneously. ApEn as a complexity based method appears to have potential for the application to physiological and clinical time series data. In our previous studies, MEMD method and ApEn measure were always used severally, if MEMD and ApEn are used to analysis the same EEG signal simultaneously, the result of experiment will be more accurate. In this paper, we present MEMD method and ApEn measure based blind test without knowing about the clinical symptoms of patients beforehand. Features obtained from two typical cases indicate one patient being in coma and another in quasi-brain-death state.\"",
        "1 is \"Canonical correlation analysis for multilabel classification: a least-squares formulation, extensions, and analysis.\", 2 is \"Robust EEG channel selection across subjects for brain-computer interfaces\"",
        "Given above information, for an author who has written the paper with the title \"Dynamic MEMD Associated with Approximate Entropy in Patients' Consciousness Evaluation.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006007": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Burst loss and delay in optical buffers with offset-time management':",
        "Document: \"Buffer contents and cell delay in a rate adaptation buffer with Markovian arrivals. This paper investigates the performance of a rate adaptation buffer in the case that the arriving cell stream is generated by an on/off-source, where both the on-periods and the off-periods are geometrically distributed. The ratio between the input rate and the output rate takes an arbitrary integer value greater than one. Under the assumption of an infinite storage capacity, exact explicit expressions are obtained for the mean values and the tail distributions of the buffer contents and the cell delay. Furthermore, an approximation is derived for the cell loss ratio in a finite-capacity buffer. Some numerical results are presented and discussed.\"",
        "Document: \"Performance Analysis of a GI-G-1 Preemptive Resume Priority Buffer. In this paper, we have analyzed a discrete-time GI - G - 1 queue with a preemptive resume priority scheduling and two priority classes. We have derived the joint generating function of the system contents of both classes and the generating functions of the delay of both classes. These pgf's are not explicitly found, but we have proven that the moments of the distributions can be found explicitly in terms of the system parameters. We have shown the impact of priority scheduling on the performance characteristics by some numerical examples.\"",
        "Document: \"End-to-end delays in multistage ATM switching networks: approximate analytic derivation of tail probabilities. In this paper, an approximate method is developed to derive the delay performance of ATM switching networks consisting of multiple stages of elementary ATM switches with output queueing. The approach taken is to characterize the delay through each stage of the network as the delay of a Geo(N)/D/c discrete-time queueing system and to consider delays incurred at consecutive stages as independent. Closed-form expression are then obtained for the tail probabilities of the total delay of a packet through the whole network, in terms of the numbers of inputs/outputs of the switches in each intermediate stage, the numbers of servers and the loading factors of each output queue in the path of this packet through the network. The formulas are easy to evaluate and applicable for any number of switching stages in the network. The numerical complexity of the method is moderate and the results obtained are very accurate. Some numerical examples are discussed.\"",
        "Document: \"Characterisation of the output process of a discrete-time GI / D / 1 queue, and its application to network performance. In this paper we use the burst factor of a packet stream, which is defined in a general setting, to quantify the long-term variability, or burstiness, of such a stream. We briefly review some existing results to show that this parameter plays an important role in the performance assessment and dimensioning of buffers in network nodes, even in a non-Markovian setting. We then focus on the calculation of this parameter at the egress of a discrete-time GI / D / 1 queueing system, considering different routing scenarios, and show how it can be expressed in terms of the parameters that characterise the arrival process in such a queue. In addition, we demonstrate how these results can be applied to evaluate the buffer performance in the subsequent nodes of a network. The analytic results that are derived throughout this paper are supported by simulations.\"",
        "Document: \"Performance analysis of a single-server ATM queue with a priority scheduling. In this paper, we consider a discrete-time queueing system with head-of-line priority. First, we will give some general results on a GI-1-1 queue with priority scheduling. In particular, we will derive expressions for the probability generating function of the system contents and the cell delay. Some performance measures (such as mean, variance and approximate tail distributions) of these quantities will be derived, and used to illustrate the impact and significance of priority scheduling in an ATM output queueing switch.\"",
        "1 is \"Weak Regeneration in Modeling of Queueing Processes\", 2 is \"A framework for opportunistic scheduling in wireless networks\"",
        "Given above information, for an author who has written the paper with the title \"Burst loss and delay in optical buffers with offset-time management\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006042": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A Novel Definition of Spectrum Holes for Improved Spectrum Utilization Efficiency.':",
        "Document: \"An Improved Method For Density-Based Clustering. Knowledge discovery in large multimedia databases which usually contain large amounts of noise and high-dimensional feature vectors is an increasingly important research issue. Density-based clustering is proved to be much more efficient when dealing with such databases. However, its clustering quality mainly depends on the parameter setting. For the adequate choice of the parameters to be preset, it has difficulty in its operability without enough domain knowledge. To solve such problem, in this paper it proposed a new approach to immediately inference an appropriate value for one of the parameters named bandwidth. Based on the Bayesian Theorem, it is to infer the suitable parameter value by the constructed parameter estimation model. Then the user only has to preset the other parameter noise threshold. As a result, the clusters can be identified by the determined parameter values. The experimental results show that the proposed method has complementary advantages in the density-based clustering algorithm.\"",
        "Document: \"Asymmetric acoustic modeling of mixed language speech. We propose to improve speech recognition performance on speaker-independent, mixed language speech by asymmetric acoustic modeling. Mixed language is either inter-sentential code switching from the source matrix language to a foreign language or intra-sentential code mixing between the matrix language and embedded foreign words or phrases. In either case, the foreign phrases are pronounced by the matrix language speaker with varying degrees of accent. Our pro posed system using selective decision tree merging between a bilingual model and an accented embedded speech model outperforms previous approaches of either using a bilingual model with model retraining by 21.51%, or using adaptation by 15.88%. It outperforms all models on both code mixing and code switching cases. We successfully improved recognition on embedded foreign speech without degrading the performance on the matrix language speech.\"",
        "Document: \"Fast Spectrum Sharing for Cognitive Radio Networks: A Joint Time-Spectrum Perspective. Time efficiency is a basic characteristic for spectrum management and sharing in cognitive radio networks (CRN). In this paper, we introduce an incremental metric time into dynamic spectrum management and sharing problem, which could make the spectrum resource management more reasonable. On the one hand, to motivate the idle spectrum holders to share the spectrum more actively and regulate the spectrum leasing markets better , a spectrum management rule (SMR) is introduced. Accordingly, a spectrum lease rule (SLR) is proposed from the perspective of factual application requirements of unlicensed users (UUs). One the other hand, we formulate the spectrum sharing process as a two-dimensional packing problem, in which UUs utilize the spectrum resource through forming cooperative groups (CG). Finally, a distributed fast spectrum sharing algorithm (DFSS) is proposed to realize the process of CG forming promptly. Simulation demonstrates that DFSS algorithm can adapt to the cognitive radio networks effectively, and the average spectrum utilization ratio can reach about 83.42%.\"",
        "Document: \"Robust Symbolic Dual-View Facial Expression Recognition With Skin Wrinkles: Local Versus Global Approach. Simple cartoon facial expressions can be represented by emoticons, that is, a special sequence of symbols. This inspires us that a sketch of facial feature contour may be adequate to recognize expressions. Metrics of such sketches are easier to be calibrated under varying illumination and head pose. While skin wrinkles such as nasolabial folds, eye pouches, dimples, forehead, and chin furrows are not salient facial features, they may convey crucial subtle signals about an individual's emotion. Our experiments have shown that the side-view profile plus skin wrinkles can correctly differentiate nearly 70% expressions, and it contributes to the increase of overall recognition rate. Finally, we compare the accuracy and robustness of various local and global processing schemes, especially under the condition of partial occlusion.\"",
        "Document: \"Optimal Design of the Joint Network LDPC Codes for Half-Duplex Cooperative Multi-access Relay Channel. Optimization of degree distribution of the joint network LDPC code for orthogonal multiple-access relay channel (MARC) is investigated in this paper. We first calculated the code capacity of the source-destination link of orthogonal MARC based on a Gaussian approximation, where two sources communicate with one destination in the presence of one relay. Then, a new generalized differential evolution algorithm is developed to iteratively improve the degree distribution of the joint network LDPC code. Simulation results show that, the gap between bit error rate (BER) performance of our designed codes and the decoding threshold is about 0.4dB, and that between the capacity limit and decoding threshold is less than 0.7dB at BER = 10-6.\"",
        "1 is \"New Quaternary Sequences With Ideal Autocorrelation Constructed From Legendre Sequences\", 2 is \"Unsupervised Feature Selection Using Feature Similarity\"",
        "Given above information, for an author who has written the paper with the title \"A Novel Definition of Spectrum Holes for Improved Spectrum Utilization Efficiency.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006055": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Influence maximization on signed networks under independent cascade model':",
        "Document: \"An I-FFR algorithm for interference coordination in twin-layer femtocell networs. In LTE-A macro-femtocell networks, the dense deployment of femtocells may cause strong Inter-Cell Interference (ICI), which will lead to decrease Femto User Equipment (FUE) throughput and QoS, resulting in the degradation of the total throughput of the cell. Fractional Frequency Reuse is a conversation scheme which proves to be an efficient solutions to this problem. In this paper, we propose an Intelligent Fractional Frequency Reuse (I-FFR) algorithm for interference coordination, which combines genetic algorithm with graph-annealing algorithm to get the best CCR proportion and divides cell edge region (CER) into different clusters for the pursuit of high system throughput and low user outage probability. Simulation results show that, I-FFR algorithm can dynamically adjust the CCR proportional and reaches maximum system throughput compared to FFR-3 algorithm, and it also decreases the average outage probability of CER users from 55% to 30%.\"",
        "Document: \"A New Redundant Binary Partial Product Generator for Fast 2n-Bit Multiplier Design. The radix-4 Booth encoding or Modified Booth encoding (MBE) has been widely adopted in partial products generator to design high-speed redundant binary (RB) multipliers. Due to the existence of an error-correcting word (ECW) generated by MBE and RB encoding, the RB multiplier generates an additional RB partial product rows. An extra RB partial product accumulator (RBPPA) stage is needed for 2n-b RB MBE multiplier. The higher radix Booth algorithm than radix-4 can be adopted to reduce the number of partial products. However, the Booth encoding is not efficient because of the difficulty in generating hard multiples. The hard multiples problem in RB multiplier can be resolved by difference of two simple power-of-two multiples. This paper presents a new radix-16 RB Booth Encoding (RBBE-4) to avoid the hard multiple of high-radix Booth encoding without incurring any ECW. The proposed method leads to make high-speed and low-power RB multipliers. The experimental results show that the proposed RBBE-4 multiplier achieves significant improvement in delay and power consumption compared with the RB MBE multiplier and the current reported best RBBE-4 multipliers.\"",
        "Document: \"Influence maximization on signed networks under independent cascade model. Influence maximization problem is to find a subset of nodes that can make the spread of influence maximization in a social network. In this work, we present an efficient influence maximization method in signed networks. Firstly, we address an independent cascade diffusion model in the signed network (named SNIC) for describing two opposite types of influence spreading in a signed network. We define the independent propagation paths to simulate the influence spreading in SNIC model. Particularly, we also present an algorithm for constructing the set of spreading paths and computing their probabilities. Based on the independent propagation paths, we define an influence spreading function for a seed as well as a seed set, and prove that the spreading function is monotone and submodular. A greedy algorithm is presented to maximize the positive influence spreading in the signed network. We verify our algorithm on the real-world large-scale networks. Experiment results show that our method significantly outperforms the state-of-the-art methods, particularly can achieve more positive influence spreading.\"",
        "Document: \"Ant colony learning method for joint MCS and resource block allocation in LTE Femtocell downlink for multimedia applications with QoS guarantees. With the emergence of bandwidth-intensive online mobile multimedia applications in wireless networks, in order to make mobile users enjoy better Quality of Service (QoS) under the conditions of limited resources, efficient radio spectrum resource allocation schemes are always desirable. This paper addresses the problem of joint Resource Block (RB) allocation and Modulation-and-Coding Scheme (MCS) selection in LTE femtocell DownLink (DL) for mobile multimedia applications. We first formulate the problem as an Integer Linear Program (ILP) whose objective is to minimize the number of allocated RBs of a closed femtocell, while guaranteeing minimum throughput for each user. In view of the NP-hardness of the ILP, we then propose an intelligent optimization learning algorithm called ACO-HM algorithm with reduced polynomial time complexity. The Ant Colony Optimization (ACO) learning algorithm exhibits better performance in machine learning and supports parallel search for the RB allocation, while the Harmonic Mean (HM) method is to select a more appropriate MCS than the MINimum/MAXimum MCS selection schemes (MIN/MAX). Simulation results show that compared with the ACO-MIN algorithm and the ACO-MAX algorithm, the proposed ACO-HM learning algorithm achieves better performance with fewer RBs and better QoS guarantees.\"",
        "Document: \"Integrating Gaussian Process with Reinforcement Learning for Adaptive Service Composition. Service composition offers a powerful software paradigm to build complex and value-added applications by exploiting a service oriented architecture. However, the frequent changes in the internal and external environment demand adaptiveness of a composition solution. Meanwhile, the increasingly complex user requirements and the rapid growth of the composition space give rise to the scalability issue. To address these key challenges, we propose a new service composition scheme, integrating gaussian process with reinforcement learning for adaptive service composition. It uses kernel function approximation to predict the distribution of the objective function value with strong communication skills and generalization ability based on an off-policy Q-learning algorithm. The experimental results demonstrate that our method clearly outperforms the standard Q-learning solution for service composition.\"",
        "1 is \"Robust Pls Approach For Kpi-Related Prediction And Diagnosis Against Outliers And Missing Data\", 2 is \"Fast sum of absolute transformed difference based 4\u00d74 intra-mode decision of H.264/AVC video coding standard\"",
        "Given above information, for an author who has written the paper with the title \"Influence maximization on signed networks under independent cascade model\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006084": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Sharp Tactile Line Presentation Using Edge Stimulation Method.':",
        "Document: \"Identification of cutaneous detection thresholds against time-delay stimuli for tactile displays. Tactile display is a technology that gives an ar- tificial sense of touch to operators of information terminals or master-slave systems. The generation of tactile stimuli in response to the hand movements of the operators is associ- ated with active touch and is considered to be one of the effective display methods, however which inevitably causes delayed tactile feedbacks from tactile displays. The knowledge of the detection threshold of system latency between the hand movements and the stimuli is helpful in designing the tactile displays. In this study, the identification of the thresholds through psychophysical experiments of 13 participants revealed two types of thresholds. One was the time-delay at which the participants observed the existence of latency. The other was the minimal time-delay that could affect the subjective feelings of the operators while they were not conscious of the latency. The means of the thresholds were 41 ms and 59 ms, respectively. The participants reported that the time-delay stimuli caused various changes in their subjective feelings. The empirical results suggest that the two types of thresholds depend on different sensory processes. This paper also proposes a design policy for tactile display systems in terms of system latency.\"",
        "Document: \"Virtual Active Touch: Perception of Virtual Gratings Wavelength through Pointing-Stick Interface. Tactile feedback enhances the usability and enjoyment of human-computer interfaces. Many feedback techniques have been devised to present tactile stimuli corresponding to a user's hand movements taking account of the concept of active touch. However, hand movements may not necessarily be required for achieving natural tactile feedback. Here, we propose a virtual-active-touch method that achieves haptic perception without actual/direct hand movements. In this method, a cursor manipulated by a force-input device is regarded as a virtual finger of the operator on the screen. Tactile feedback is provided to the operator in accordance with cursor movements. To validate the translation of virtual roughness gratings, we compare the virtual-active-touch interface with an interface that involves actual hand movements. By using the appropriate force-to-velocity gain for the pointing-stick interface, we show that the virtual-active-touch method presents the surface wavelengths of the gratings, which is a fundamental property for texture roughness, and that the gain significantly influences the textures experienced by the operators. Furthermore, we find that the perceived wavelengths of objects scaled and viewed on a small screen are skewed. We conclude that although some unique problems remain to be solved, we may be able to perceive the surface wavelengths solely with the intentions of active touch through virtual-active-touch interfaces.\"",
        "Document: \"Shape estimation of flexible cable. In this paper, an estimation method using the flexible multi-body dynamics model, inertial/magnetic sensor system, and probabilistic state estimation is proposed for estimating the shape of a flexible cable. Sensors mounted on flexible cables, such as endoscopes, are often picked up noise, and their size and number are restricted. The proposed method can estimate motion using the flexible dynamics model if there are few sensors. In addition, this method is robust against noise because it can integrate some types of sensor information by probabilistic state estimation. The proposed method uses the Absolute Nodal Coordinate Formulation (ANCF) as a flexible dynamics model and the Unscented Kalman Filter (UKF) for probabilistic state estimation. It can model a greatly deformed cable at any point using ANCF, and it can fit nonlinear motion and achieve high versatility about models because of UKF. In the second half of this paper, experiments using Active Scope Camera (ASC) are conducted to evaluate the proposed method. The ASC is a snake-like rescue robot, and its sensor system picks up strong noise. In the experiments, the proposed method estimates three types of motion, and it shows the effectiveness if the number of sensors decreases.\"",
        "Document: \"Sharp Tactile Line Presentation Using Edge Stimulation Method. We provide supplemental data to a vibrator array tactile display, as well as additional data for application of the edge stimulation (ES) method proposed in our previous study. By vibrating two surfaces in different phases and touching their boundary, a strong continuous line sensation, not on the vibrators themselves, but along the boundary, is obtained. This vibrotactile edge is suitable for pre...\"",
        "Document: \"Remote vertical exploration by Active Scope Camera into collapsed buildings. Remote robotic explorations for collapsed buildings in a severe disaster are demanded. However, rescue robots cannot approach the rubble due to safety risks. This study proposes a remote vertical exploration system for collapsed buildings with a robotic inspection system hoisted by a crane. An Active Scope Camera (ASC) has many advantages for the vertical exploration such as a light and flexible continuum body to produce distributed driving forces. The purpose of this paper is to confirm the feasibility of the vertical exploration system with the ASC. The vertical explorations have proper problems related to contact and hanging conditions of the scope cable. We developed a new ASC that has a two-step bending mechanism to produce larger head movement in multi-DOF. We also evaluated the performances of the prototype when the contact areas were small. Finally, we conducted a remote vertical exploration experiments at the simulated collapsed building in 6 m height. The robot could explore in six different pathways by changing head directions and running the rubbles within seven trials. The experimental results showed that the proposed system has high potential to get inserted in the deep area in the rubble.\"",
        "1 is \"Direction of Arrival Based Spatial Covariance Model for Blind Sound Source Separation.\", 2 is \"Haptic telexistence\"",
        "Given above information, for an author who has written the paper with the title \"Sharp Tactile Line Presentation Using Edge Stimulation Method.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006089": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Story boundary detection in large broadcast news video archives: techniques, experience and trends':",
        "Document: \"Adaptive Learning for Multimodal Fusion in Video Search. Multimodal fusion had been shown prominent in video search for the sheer volume of video data. The state-of-the-art methods address the problem by query-dependent fusion, where modality weights vary across query classes (e.g., object, sports, scenes, people, etc.). However, provided the training queries, most of the prior methods rely on manually pre-defined query classes, ad-hoc query class classification, and heuristically determined fusion weights, which suffer from accuracy issues and are not scalable to large-scale data. Unlike prior methods, we propose an adaptive query learning framework for multimodal fusion. For each new query, we adopt ListNet to adaptively learn the fusion weights from its semantically-related training queries dynamically selected by K-nearest neighbor method. ListNet is efficient for optimizing the performance in search ranking rather than classification. In general, the proposed method has the following advantages: 1) No pre-defined query classes are needed. 2) The multimodal query weights are automatically and adaptively learned without ad-hoc hand-tuning. 3) The query training examples are selected according to the query semantics and require no noisy query classification. Experimenting in large-scale video benchmarks (i.e., TRECVID), we will show that the proposed method is scalable and competitive with prior query-dependent methods.\"",
        "Document: \"Learning by expansion: Exploiting social media for image classification with few training examples. Witnessing the sheer amount of user-contributed photos and videos, we argue to leverage such freely available image collections as the training images for image classification. We propose an image expansion framework to mine more semantically related training images from the auxiliary image collection provided with very few training examples. The expansion is based on a semantic graph considering both visual and (noisy) textual similarities in the auxiliary image collections, where we also consider scalability issues (e.g., MapReduce) as constructing the graph. We found the expanded images not only reduce the time-consuming (manual) annotation efforts but also further improve the classification accuracy since more visually diverse training images are included. Experimenting in certain benchmarks, we show that the expanded training images improve image classification significantly. Furthermore, we achieve more than 27% relative improvement in accuracy compared to the state-of-the-art training image crowdsourcing approaches by exploiting media sharing services (such as Flickr) for additional training images.\"",
        "Document: \"Predicting Viewer Affective Comments Based on Image Content in Social Media. Visual sentiment analysis is getting increasing attention because of the rapidly growing amount of images in online social interactions and several emerging applications such as online propaganda and advertisement. Recent studies have shown promising progress in analyzing visual affect concepts intended by the media content publisher. In contrast, this paper focuses on predicting what viewer affect concepts will be triggered when the image is perceived by the viewers. For example, given an image tagged with \\\"yummy food,\\\" the viewers are likely to comment \\\"delicious\\\" and \\\"hungry,\\\" which we refer to as viewer affect concepts (VAC) in this paper. To the best of our knowledge, this is the first work explicitly distinguishing intended publisher affect concepts and induced viewer affect concepts associated with social visual content, and aiming at understanding their correlations. We present around 400 VACs automatically mined from million-scale real user comments associated with images in social media. Furthermore, we propose an automatic visual based approach to predict VACs by first detecting publisher affect concepts in image content and then applying statistical correlations between such publisher affect concepts and the VACs. We demonstrate major benefits of the proposed methods in several real-world tasks - recommending images to invoke certain target VACs among viewers, increasing the accuracy of predicting VACs by 20.1% and finally developing a social assistant tool that may suggest plausible, content-specific and desirable comments when users view new images.\"",
        "Document: \"Multi-layer graph-based semi-supervised learning for large-scale image datasets using mapreduce. Semi-supervised learning is to exploit the vast amount of unlabeled data in the world. This paper proposes a scalable graph-based technique leveraging the distributed computing power of the MapReduce programming model. For a higher quality of learning, the paper also presents a multi-layer learning structure to unify both visual and textual information of image data during the learning process. Experimental results show the effectiveness of the proposed methods.\"",
        "Document: \"Topic Tracking Across Broadcast News Videos with Visual Duplicates and Semantic Concepts. Videos from distributed sources (e.g., broadcasts, podcasts, blogs, etc.) have grown exponentially. Topic threading is very useful for organizing such large-volume information sources. Current solu- tions primarily rely on text features only but encounter difficulty when text is noisy or unavailable. In this paper, we propose new representations and similarity measures for news videos based on low-level features, visual near-duplicates, and high-level semantic concepts automatically detected from videos. We develop a multi- modal fusion framework for estimating relevance of a new story to a known topic. Our extensive experiments using TRECVID 2005 data set (171 hours, 6 channels, 3 languages) confirm that near-duplicates consistently and significantly boost the tracking performance by up to 25%. In addition, we present information-theoretic analysis to as- sess the complexity of each semantic topic and determine the best subset of concepts for tracking each topic.\"",
        "1 is \"Person spotting: video shot retrieval for face sets\", 2 is \"Classification by Retrieval: Binarizing Data and Classifiers\"",
        "Given above information, for an author who has written the paper with the title \"Story boundary detection in large broadcast news video archives: techniques, experience and trends\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006101": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Formal Analysis of the Kerberos Authentication System':",
        "Document: \"Towards ASM-Based Formal Specification of Self-Adaptive Systems. This paper shows how to use multi-agent Abstract State Machines to specify self-adaptive behavior in a decentralized adaptation control system. A traffic monitoring system is taken as case study.\"",
        "Document: \"Consistent integration for sequential abstract state machines. In this paper we deal with the problem of integrating components described as Sequential Abstract State Machines. Two operations are defined to compose sequential ASMs and returning a machine consistent with respect to updates. Due to their nature, such operations can be used to analyze and handle \"behavioral inconsistencies\" and to prove component and system properties. The applicative aspects of the theoretical issues introduced in the paper are shown by means of three examples: the behavioral description of a telephone system presented in [7], the ASM specification of the Production Cell case study developed in [3], and the ASM solution of the railroad crossing problem given in [12].\"",
        "Document: \"Formal validation and verification of a medical software critical component. Medical device software malfunctioning can lead to injuries or death for humans and, therefore, its development should adhere to certification standards. However, these standards establish general guidelines on the use of common software engineering activities without any indication regarding methods and techniques to assure safety and reliability. This paper presents a formal development process, based on the Abstract State Machine method, that integrates most of the activities required by the standards. The process permits to obtain, through a sequence of refinements, more detailed models that can be formally validated and verified. Offline and online testing techniques permit to check the conformance of the implementation w.r.t. the specification. The process is applied to the validation of the SAM medical software, that is used to measure the patients' stereoacuity in the diagnosis of amblyopia.\"",
        "Document: \"Interactive Testing and Repairing of Regular Expressions. Writing a regular expression that exactly captures a set of desired strings is difficult, since regular expressions provide a compact syntax that makes it difficult to easily understand their meaning. Testing is widely used to validate regular expressions. Indeed, although a developer could have problems in writing the correct regular expression, (s)he can easily assess whether a string should be accepted or not. Starting from this observation, we propose an iterative mutation-based process that is able to test and repair a faulty regular expression. The approach consists in generating strings S that distinguish a regular expression r from its mutants, asking the user to assess the correct evaluation of S, and possibly substituting r with a mutant r' that evaluates S more correctly than r; we propose four variants of the approach which differ in the policy they employ to judge whether r' is better than r. Experiments show that the proposed approach is able to actually repair faulty regular expressions with a reasonable user's effort.\"",
        "Document: \"On formalizing UML state machines using ASMs. We present a transparent yet rigorous conceptual framework for defining the semantics of dynamic UML diagrams. We illustrate the method for UML state machines, making the \u201csemantic variation points\u201d of UML explicit, as well as various ambiguities and omissions in the official UML documents. This includes the event deferring and completion mechanism, the meaning of atomic and durative actions, concurrent internal activities and conflict situations which may arise through the concurrent behavior of active objects.\"",
        "1 is \"Abstracted Architecture for Smart Grid Privacy Analysis\", 2 is \"Testing Transition Systems: An Annotated Bibliography\"",
        "Given above information, for an author who has written the paper with the title \"Formal Analysis of the Kerberos Authentication System\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006115": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Power-Efficient Virtual Machine Placement and Migration in Data Centers':",
        "Document: \"An Energy-Aware Trust Derivation Scheme With Game Theoretic Approach in Wireless Sensor Networks for IoT Applications. Trust evaluation plays an important role in securing wireless sensor networks (WSNs), which is one of the most popular network technologies for the Internet of Things (IoT). The efficiency of the trust evaluation process is largely governed by the trust derivation, as it dominates the overhead in the process, and performance of WSNs is particularly sensitive to overhead due to the limited bandwidth and power. This paper proposes an energy-aware trust derivation scheme using game theoretic approach, which manages overhead while maintaining adequate security of WSNs. A risk strategy model is first presented to stimulate WSN nodes' cooperation. Then, a game theoretic approach is applied to the trust derivation process to reduce the overhead of the process. We show with the help of simulations that our trust derivation scheme can achieve both intended security and high efficiency suitable for WSN-based IoT networks.\"",
        "Document: \"Applying Spring-Relaxation Technique in Cellular Network Localization. This paper presents a simple solution suitable for the mandatory localization function for E-911 services specified by FCC. Our solution introduces zero-length spring technique to compute the estimated location based on received signal strength (RSS). The introduced zero-length spring concept permits a less detailed path loss model to use without significant impact to the location estimation. We show the stability of our algorithm by illustrating the convergence of the estimated locations computed by the algorithm. We then demonstrate with simulation the accuracy of the estimation with various settings, and compare this accuracy with two candidate localization techniques.\"",
        "Document: \"Online XOR packet coding: Efficient single-hop wireless multicasting with low decoding delay. In this paper we present a cross-layer solution to the problem of unreliability in IEEE 802.11 wireless multicast network, where an Access Point (AP) is multicasting a data file to a group of receivers over independent wireless erasure channels. We first present a practical scheme for collecting feedback frames from the receivers by means of simultaneous acknowledgment (ACK) frames collision. Based on these feedback frames, we design an online linear XOR coding algorithm to retransmit the lost packets. Through simulation results we first show that our proposed coding algorithm outperforms all the existing XOR coding algorithms in terms of retransmission rate. We further show that our proposed coding algorithm has the lowest average decoding delay of all the known network coding schemes. XOR coding and decoding only requires addition over GF(2), hence it enjoys lower encoding and decoding computational complexities. Because of these features such an online XOR coding algorithm is also of interest for delay-sensitive applications such as multicast audio video (AV) streaming, and in battery constrained devices such as smartphones.\"",
        "Document: \"Mitigating the Table-Overflow Attack in Software-Defined Networking. Software-defined networking (SDN) is a promising network paradigm for future Internet. The centralized controller and simplified switches replace the traditional complex forwarding devices, and make network management convenient. However, the switches in SDN currently have limited ternary content addressable memory to store specific routing rules from the controller. This bottleneck provokes cyber...\"",
        "Document: \"Throughput and delay analysis of the IEEE 802.11e EDCA saturation. In this paper, we introduce a simple model for the enhanced distributed channel access (EDCA) mechanism under saturation conditions. This model captures the operation of the AIFS (arbitration inter-frame space) and contention window differentiation of the EDCA mechanism. Using this model, we analyze the throughput and delay performance of EDCA. The results of our analytical model are then verified using simulations.\"",
        "1 is \"Assessing the impact of resource attack in Software Defined Network\", 2 is \"Modeling spatial and temporal dependencies of user mobility in wireless mobile networks\"",
        "Given above information, for an author who has written the paper with the title \"Power-Efficient Virtual Machine Placement and Migration in Data Centers\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006167": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Supporting Distributed Software Development with fine-grained Artefact Management':",
        "Document: \"Reproducing software failures by exploiting the action history of undo features. Bug reports seldom contain information about the steps to reproduce a failure. Therefore, failure reproduction is a time consuming, difficult, and sometimes impossible task for software developers. Users are either unaware of the importance of steps to reproduce, are unable to describe them, or do not have time to report them. Similarly, automated crash reporting tools usually do not capture this information. In order to tackle this problem, we propose to exploit the action history of undo features, i.e. the history of user actions captured by many applications in order to allow users to undo previous actions. As it is captured anyway, our approach does not introduce additional monitoring overhead. We propose to extract the action history upon occurrence of a failure and present it to developers during bug fixing. Our hypothesis is that information about user actions contained in the action history of undo features enables developers to reproduce failures. We support this hypothesis with anecdotal evidence from a small empirical study of bug reports. A thorough evaluation is necessary to investigate the applicability and impact of our approach and to compare it to existing capture/ replay approaches.\"",
        "Document: \"Practitioners' eye on continuous software engineering: an interview study. Continuous software engineering (CSE) emerged as a process that is increasingly applied by practitioners. However, different perceptions of CSE among practitioners might impede its adoption in industry. We aim to support practitioners by giving a comprehensive overview of current CSE practices. Our observations provide guidance for practice on how to establish, assess, and advance CSE in their company. We conducted an interview study with 24 practitioners from 17 companies during 20 interviews. Following a semi-structured approach, we asked for their definition of CSE, most relevant elements for CSE, their experiences, and plans for further additions to their CSE process. From the practitioners' statements, we identified five perspectives on CSE and found tool- and methodology-driven definitions most prevalent. Automated tests, involved users, and a shared ruleset are perceived as most relevant for CSE. Practitioners' positive experiences with CSE are more frequent than negative ones; however, more than half of the responses were neutral. Practitioners' future plans focus on enhancement, expansion, and on-demand adaption of current practices. We conclude that CSE remains partially difficult to capture for practitioners. Therefore, we structure CSE in a model, the Eye of CSE.\n\n\"",
        "Document: \"Applying a Video-based Requirements Engineering Technique to an Airport Scenario. In the development of software-intensive systems, the interaction between customer and supplier is usually text-based. We argue that with agile project management gaining momentum, the inclusion of end-user feedback and a better mutual understanding between customer and supplier on hardware and software design goals becomes increasingly important. We propose the use of video techniques, Video-based Requirements Engineering (VBRE), to support the communication between all stakeholders. The key ingredients of VBRE are user-centric videos and an exploratory environment for creating multi-path scenarios. In this workshop session, the participants will get hands-on experience with VBRE techniques and tools while working on a fictitious airport baggage handling system.\"",
        "Document: \"Pinocchio: conducting a virtual symphony orchestra. We present a system that allows users of any skill to conduct a virtual orchestra. Tempo and volume of the orchestra's performance are influenced with a baton. Pinocchio works with several types of batons, differing in tracking method and in algorithms for gesture recognition. The virtual orchestra can be configured, allowing the muting, hiding and positioning of individual musicians or instrument groups in 3D space. The audio and video material is based on a professional recording session with the Bavarian symphony orchestra. Pinocchio's long-term goal is the creation of a multi-modal, device independent framework for gesture-based applications which require motor skills or the control and operation of a complex set of sensors in intelligent house or car driver assistance systems. In this paper, we describe the current development status of the project, detail its usage and finally give an overview over our future project goals.\"",
        "Document: \"From pixels to bytes: evolutionary scenario based design with video. Change and user involvement are two major challenges in agile software projects. As change and user involvement usually arise spontaneously, reaction to change, validation and communication are thereby expected to happen in a continuous way in the project lifecycle. We propose Evolutionary Scenario Based Design, which employs video in fulfilling this goal, and present a new idea that supports video production using SecondLife-like virtual world technology.\"",
        "1 is \"Possibilities for the digital baton as a general-purpose gestural interface\", 2 is \"Evolutionary Optimization of Software Quality Modeling with Multiple Repositories\"",
        "Given above information, for an author who has written the paper with the title \"Supporting Distributed Software Development with fine-grained Artefact Management\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006219": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Blind Source Separation Exploiting Higher-Order Frequency Dependencies':",
        "Document: \"Multivariate analysis of fMRI group data using independent vector analysis. A multivariate non-parametric approach for the processing of fMRI group data is important to address variability of hemodynamic responses across subjects, sessions, and brain regions. Independent component analysis (ICA) has a limitation during the inference of group effects due to a permutation problem of independent components. In order to address this limitation, we present an independent vector analysis (IVA) for the processing of fMRI group data. Compared to the ICA, the IVA offers an extra dimension for the dependent parameters, which can be assigned for the automated grouping of dependent activation patterns across subjects. The IVA was applied to the fMRI data obtained from 12 subjects performing a left-hand motor task. In comparison with conventional univariate methods, IVA successfully characterized the group-representative activation time courses (as component vectors) without extra data processing schemes to circumvent the permutation problem, while effectively detecting the areas with hemodynamic responses deviating from canonical, model-driven ones.\"",
        "Document: \"Capturing nonlinear dependencies in natural images using ICA and mixture of Laplacian distribution. Capturing dependencies in images in an unsupervised manner is important for many image-processing applications and for understanding the structure of natural image signals. Data generative linear models such as principal component analysis (PCA) and independent component analysis (ICA) have shown to capture low level features such as oriented edges in images. However, those models capture only linear dependency and therefore its modeling capability is limited. We propose a new method for capturing nonlinear dependencies in images of natural scenes. This method is an extension of the linear ICA method and builds on a hierarchical representation. The model makes use of lower level linear ICA representation and a subsequent mixture of Laplacian distribution for learning the nonlinear dependencies in an image. The model parameters are learned via the expectation maximization (EM) algorithm and it can accurately capture variance correlation and other high order structures in a simple and consistent manner. We visualize the learned variance correlation structure and demonstrate applications to automatic image segmentation and image denoising.\"",
        "Document: \"Blind source separation in mobile environments using a priori knowledge. A speech enhancementscheme including blind source sepa- ration and background denoising based on minimum statis- tics is studied in mobile environments. To accommodate the dependence of the separated output signals on the spa- tial properties of the recorded source signals, these blind signal processing steps are complemented by an adaptive separated output channel selection stage using prior knowl- edge about the desired speaker speech content. The result- ing scheme performanceis illustrated by speech recognition experiments on real recordings corrupted by various noise sources and shown to outperform conventional beamform- ing and single channel denoising techniques as well as an equivalent scheme with fixed output channel selection.\"",
        "Document: \"Fast fixed-point independent vector analysis algorithms for convolutive blind source separation. A new type of independent component analysis (ICA) model showed excellence in tackling the blind source separation problem in the frequency domain. The new model, called independent vector analysis, is an extension of ICA for (independent) multivariate sources where the sources are mixed component-wise. In this work we examine available contrasts for the new formulation that can solve the frequency-domain blind source separation problem. Also, we introduce a quadratic Taylor polynomial in the notations of complex variables which is very useful in directly applying Newton's method to a contrast function of complex-valued variables. The use of the form makes the derivation of a Newton update rule simple and clear. Fast fixed-point blind source separation algorithms are derived and the performance is shown by experimental results.\"",
        "Document: \"Independent vector analysis (IVA): Multivariate approach for fMRI group study. Independent component analysis (ICA) of fMRI data generates session/individual specific brain activation maps without a priori assumptions regarding the timing or pattern of the blood-oxygenation-level-dependent (BOLD) signal responses. However, because of a random permutation among output components, ICA does not offer a straightforward solution for the inference of group-level activation. In this study, we present an independent vector analysis (IVA) method to address the permutation problem during fMRI group data analysis. In comparison to ICA, IVA offers an analysis of additional dependent components, which were assigned for use in the automated grouping of dependent activation patterns across subjects. Upon testing using simulated trial-based fMRI data, our proposed method was applied to real fMRI data employing both a single-trial task-paradigm (right hand motor clenching and internal speech generation tasks) and a three-trial task-paradigm (right hand motor imagery task). A generalized linear model (GLM) and the group ICA of the fMRI toolbox (GIFT) were also applied to the same data set for comparison to IVA. Compared to GLM, IVA successfully captured activation patterns even when the functional areas showed variable hemodynamic responses that deviated from a hypothesized response. We also showed that IVA effectively inferred group-activation patterns of unknown origins without the requirement for a pre-processing stage (such as data concatenation in ICA-based GIFT). IVA can be used as a potential alternative or an adjunct to current ICA-based fMRI group processing methods.\"",
        "1 is \"Minimax entropy principle and its application to texture modeling\", 2 is \"Random key predistribution schemes for sensor networks\"",
        "Given above information, for an author who has written the paper with the title \"Blind Source Separation Exploiting Higher-Order Frequency Dependencies\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006226": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'An Empirical Study of Multi-label Learning Methods for Video Annotation':",
        "Document: \"Dynamic ensemble pruning based on multi-label classification. Dynamic (also known as instance-based) ensemble pruning selects a (potentially) different subset of models from an ensemble during prediction based on the given unknown instance with the goal of maximizing prediction accuracy. This paper models dynamic ensemble pruning as a multi-label classification task, by considering the members of the ensemble as labels. Multi-label training examples are constructed by evaluating whether ensemble members are accurate or not on the original training set via cross-validation. We show that classification accuracy is maximized when learning algorithms that optimize example-based precision are used in the multi-label classification task. Results comparing the proposed framework against state-of-the-art dynamic ensemble pruning approaches in a variety of datasets using a heterogeneous ensemble of 200 classifiers show that it leads to significantly improved accuracy.\"",
        "Document: \"MOpiS: A Multiple Opinion Summarizer. Product reviews written by on-line shoppers is a valuable source of information for potential new customers who desire to make an informed purchase decision. Manually processing quite a few dozens, or even hundreds, of reviews for a single product is tedious and time consuming. Although there exist mature and generic text summarization techniques, they are focused primarily on article type content and do not perform well on short and usually repetitive snippets of text found at on-line shops. In this paper, we propose MOpiS, a multiple opinion summarization algorithm that generates improved summaries of product reviews by taking into consideration metadata information that usually accompanies the on-line review text. We demonstrate the effectiveness of our approach with experimental results.\"",
        "Document: \"Protein classification with multiple algorithms. Nowadays, the number of protein sequences being stored in central protein databases from labs all over the world is constantly increasing. From these proteins only a fraction has been experimentally analyzed in order to detect their structure and hence their function in the corresponding organism. The reason is that experimental determination of structure is labor-intensive and quite time-consuming. Therefore there is the need for automated tools that can classify new proteins to structural families. This paper presents a comparative evaluation of several algorithms that learn such classification models from data concerning patterns of proteins with known structure. In addition, several approaches that combine multiple learning algorithms to increase the accuracy of predictions are evaluated. The results of the experiments provide insights that can help biologists and computer scientists design high-performance protein classification systems of high quality.\"",
        "Document: \"A novel flow control and switching strategy for preventing hotspot congestion in multistage networks. Multistage packet switched interconnection networks have been proposed both for use in parallel processing systems and for communications switching. The performance of these networks has been known to degrade severely with non-uniformities in the traffic distribution. In this paper we present a combination of a flow control scheme with a novel switching strategy that reduces the performance degradation of such networks. We also present a switch model in order to support the proposed solution. Simulation results are presented to show that this method can successfully improve the overall network performance.\"",
        "Document: \"PolyA-iEP: A data mining method for the effective prediction of polyadenylation sites. This paper presents a study on polyadenylation site prediction, which is a very important problem in bioinformatics and medicine, promising to give a lot of answers especially in cancer research. We describe a method, called PolyA-iEP, that we developed for predicting polyadenylation sites and we present a systematic study of the problem of recognizing mRNA 3' ends which contain a polyadenylation site using the proposed method. PolyA-iEP is a modular system consisting of two main components that both contribute substantially to the descriptive and predictive potential of the system. In specific, PolyA-iEP exploits the advantages of emerging patterns, namely high understandability and discriminating power and the strength of a distance-based scoring method that we propose. The extracted emerging patterns may span across many elements around the polyadenylation site and can provide novel and interesting biological insights. The outputs of these two components are finally combined by a classifier in a highly effective framework, which in our setup reaches 93.7% of sensitivity and 88.2% of specificity. PolyA-iEP can be parameterized and used for both descriptive and predictive analysis. We have experimented with Arabidopsis thaliana sequences for evaluating our method and we have drawn important conclusions.\"",
        "1 is \"Multimedia ontology learning for automatic annotation and video browsing\", 2 is \"Maintaining views incrementally\"",
        "Given above information, for an author who has written the paper with the title \"An Empirical Study of Multi-label Learning Methods for Video Annotation\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006243": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A combined canonical variate analysis and Fisher discriminant analysis (CVA\u2013FDA) approach for fault diagnosis':",
        "Document: \"Fast model predictive control of sheet and film processes. Sheet and film processes are prevalent in the chemical and pulp and paper industries, and include paper coating, polymer film extrusion, and papermaking. A model predictive control algo- rithm is developed which is based on an off-line singular value de- composition of the plant. The input constraints are approximated by an ellipsoid whose size is optimized on-line to reduce conser- vatism. The controller has a structure proven to be robust to model inaccuracies and is computationally efficient enough for real-time implementation on large scale sheet and film processes (e.g., ma- nipulated variable settings computed for 200 actuators in less than ten CPU seconds). The algorithm is applied to a paper machine model constructed from industrial data.\"",
        "Document: \"Synchronizing microgrids to the utility through a series compensator. This paper presents a novel approach to deal with the synchronization of microgrids to the main grid. It is based on the use of a power electronic device derived from the original concept of the SSSC (Static Synchronous Series Compensator). The proposed scheme considers the connection of this device at the point-of-interconnection (POI) of the microgrid to the low voltage side of the substation transformer (main grid). The voltage originated by the series compensator is fully controlled in both amplitude and phase angle ensuring the conditions for the synchronization. The proposed scheme allows a smooth transition between the islanded operation and the grid connected operation overcoming typical problems observed when conventional methods are applied. Numerical results obtained for a hypothetical microgrid show the effectiveness of the proposed approach.\"",
        "Document: \"A hybrid multiscale kinetic Monte Carlo method for simulation of copper electrodeposition. A hybrid multiscale kinetic Monte Carlo (HMKMC) method for speeding up the simulation of copper electrodeposition is presented. The fast diffusion events are simulated deterministically with a heterogeneous diffusion model which considers site-blocking effects of additives. Chemical reactions are simulated by an accelerated (tau-leaping) method for discrete stochastic simulation which adaptively selects exact discrete stochastic simulation for the appropriate reaction whenever that is necessary. The HMKMC method is seen to be accurate and highly efficient.\"",
        "Document: \"Advanced control of crystallization processes. A key bottleneck in the production of pharmaceuticals and many other products is the formation of crystals from solution. The control of the crystal size distribution can be critically important for efficient downstream operations such as filtration and drying, and product effectiveness (e.g., bioavailability, tablet stability). This paper provides an overview of recent developments in the control of crystallization processes, including activities in sensor technologies, model identification, experimental design, process simulation, robustness analysis, and optimal control.\"",
        "Document: \"A multiscale systems approach to microelectronic processes. This paper describes applications of molecular simulation to microelectronics processes and the subsequent development of techniques for multiscale simulation and multiscale systems engineering. The progression of the applications of simulation in the semiconductor industry from macroscopic to molecular to multiscale is reviewed. Multiscale systems are presented as an approach that incorporates molecular and multiscale simulation to design processes that control events at the molecular scale while simultaneously optimizing all length scales from the molecular to the macroscopic. It is discussed how design and control problems in microelectronics and nanotechnology, including the targeted design of processes and products at the molecular scale, can be addressed using the multiscale systems tools. This provides a framework for addressing the \u201cgrand challenge\u201d of nanotechnology: how to move nanoscale science and technology from art to an engineering discipline.\"",
        "1 is \"Indoor segmentation and support inference from RGBD images\", 2 is \"Properties of the mixed /spl mu/ problem and its bounds\"",
        "Given above information, for an author who has written the paper with the title \"A combined canonical variate analysis and Fisher discriminant analysis (CVA\u2013FDA) approach for fault diagnosis\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006255": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Transmit Power Minimization and Base Station Planning for High-Speed Trains With Multiple Moving Relays in OFDMA Systems.':",
        "Document: \"DFT-Based Closed-Form Covariance Matrix and Direct Waveforms Design for MIMO Radar to Achieve Desired Beampatterns. In multiple-input multiple-output (MIMO) radar, for desired transmit beampatterns, appropriate correlated waveforms are designed. To design such waveforms, conventional MIMO radar methods use two steps. In the first step, the waveforms covariance matrix ${\\\\mathbf R}$ is synthesized to achieve the desired beampattern. Whereas in the second step, to realize the synthesized covariance matrix, actual waveforms are designed. Most of the existing methods use iterative algorithms to solve these constrained optimization problems. The computational complexity of these algorithms is very high, which makes them difficult to use in practice. In this paper, to achieve the desired beampattern, a low complexity discrete-Fourier-transform based closed-form covariance matrix design technique is introduced for an MIMO radar. The designed covariance matrix is then exploited to derive a novel closed-form algorithm to directly design the finite-alphabet constant-envelope waveforms for the desired beampattern. The proposed technique can be used to design waveforms for large antenna array to change the beampattern in real time. It is also shown that the number of transmitted symbols from each antenna depends on the beampattern and is less than the total number of transmit antenna elements.\"",
        "Document: \"Blind Source Separation Algorithms Using Hyperbolic and Givens Rotations for High-Order QAM Constellations. This paper addresses the problem of blind demixing of instantaneous mixtures in a multiple-input multiple-output communication system. The main objective is to present efficient blind source separation (BSS) algorithms dedicated to moderate or high-order quadratic-amplitude modulation (QAM) constellations. Four new iterative batch, BSS algorithms are presented dealing with the multimodulus (MM) an...\"",
        "Document: \"Distributed Robust Power Minimization for the Downlink of Multi-Cloud Radio Access Networks. Conventional cloud radio access networks (CRANs) assume single cloud processing and treat inter-cloud interference as background noise. This paper considers the downlink of a multi-CRAN where each cloud is connected to several base-stations (BSs) through limited-capacity wireline backhaul links. The set of BSs connected to each cloud, called cluster, serves a set of pre-known mobile users. The per...\"",
        "Document: \"A Game-Theoretic Framework for Network Coding Based Device-to-Device Communications. This paper investigates the delay minimization problem for instantly decodable network coding (IDNC) based device-to-device (D2D) communications. In D2D enabled systems, users cooperate to recover all their missing packets. The paper proposes a game theoretic framework as a tool for improving the distributed solution by overcoming the need for a central controller or additional signaling in the system. The session is modeled by self-interested players in a non-cooperative potential game. The utility functions are designed so as increasing individual payoff results in a collective behavior which achieves both a desirable system performance in a shared network environment and the Nash equilibrium. Three games are developed whose first reduces the completion time, the second the maximum decoding delay and the third the sum decoding delay. The paper, further, improves the formulations by including a punishment policy upon collision occurrence so as to achieve the Nash bargaining solution. Learning algorithms are proposed for systems with complete and incomplete information, and for the imperfect feedback scenario. Numerical results suggest that the proposed game-theoretical formulation provides appreciable performance gain against the conventional point-to-multipoint (PMP), especially for reliable user-to-user channels.\"",
        "Document: \"Opportunistic random beamforming with optimal precoding for spatially correlated channels. It has been shown recently that dirty paper coding (DPC) achieves optimum sum-rate capacity in a multi-antenna broadcast channel with full channel state (CSI) information at the transmitter. With only partial feedback, random beamforming (RBF) is able to match the sumrate of DPC for large number of users. However, in the presence of spatial correlation, RBF incurs an SNR hit as compared to DPC. In this letter, we explore precoding techniques to reduce the effect of correlation on RBF. We thus derive the optimum precoding matrix that minimizes the rate gap between DPC and RBF. Given the numerical complexity involved in calculating the optimum precoder, we derive approximate precoding matrices that are simple to calculate and close in performance to the optimum precoder.\"",
        "1 is \"RAR: Real-Time Acoustic Ranging in Underwater Sensor Networks.\", 2 is \"Bit allocation of WWAN scalable H.264 video multicast for heterogeneous cooperative peer-to-peer collective\"",
        "Given above information, for an author who has written the paper with the title \"Transmit Power Minimization and Base Station Planning for High-Speed Trains With Multiple Moving Relays in OFDMA Systems.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006282": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Semantic Interactions for Context-Aware and Service-Oriented Architecture':",
        "Document: \"A general performance model interchange format. A common, XML-based interface between specifications of quantitative system models and programmed solutions is developed and illustrated with several examples. It is based on the PMIF (Performance Model Interchange Format), which allows queueing network models to be specified in XML and solved by calling any appropriate modelling tool, such as Qnap. The definition of PMIF specifications is generalised by considering more abstract collections of interacting nodes, using concepts compatible with the Reversed Compound Agent Theorem (RCAT). The interactions are more general in that they synchronise transitions in a pair of nodes rather than being restricted to describing traffic flows. The generalised nodes are characterised by the interactions in which they participate, together with their rates and reversed rates, which may be implicit. In this way, generalised queueing networks with negative customers and triggers can be incorporated and fixpoint models can also be handled uniformly through the use of symbolic variables.\"",
        "Document: \"Performance modelling of computer networks. This paper intends to present an overview of the main problems that appear when we wish to estimate the performance of a non existing system or a future situation of an existing system. The main techniques used for representing the workload and the system performance will be also presented. It will start with a definition of the main problems existing in this field, then present the main techniques used for modelling computer networks and their workload and the methods used to obtain solutions from them. Finally a simple example will show the use of all of these techniques and methods.\"",
        "Document: \"Modeling Overhead in Servers with Transactional Workloads. Our research focuses on reducing the computing time in large simulation models of client-server architectures by replacing server models by load dependent servers (LDSs), whose solution requires less computational effort. As these LDSs include overhead information, it is necessary to design a set of measurements for collecting overhead data. We are proposing a methodology that indicates how to reduce the amount of measurements and how to model overhead by means of regression analysis. In this work we explain how the methodology is applied to a workload composed of TPC-C transactions that runs in a server with Solaris 2.5 operating system (OS). Also, we describe the construction of the LDS. At the end of the article, we compare the LDS predictions with measurements of the TPC-C benchmark.\"",
        "Document: \"PMIF extensions: increasing the scope of supported models. Performance model interchange formats are common representations for data that can be used to move models among modeling tools. In order to manage the research scope, the initial version of PMIF is limited to QNM that can be solved by efficient, exact solution algorithms. The overall model interoperability approach has now been demonstrated to be viable. This paper is a first step to broaden the scope of PMIF to represent models that can be solved with additional methods.\"",
        "Document: \"Correlation analysis of a discrete-time flexible arrival process. In this paper, a flexible discrete-time arrival process is introduced and its correlation properties are analyzed. The arrival process is the so-called batch-on/off model, an extension of the original on/off source used in the context of ATM networks. In the batch-on/off model, a group of arrivals may be generated at any given active slot. General distributions are assumed for the three input random variables characterizing the process: busy and idle periods, and batch size. The analysis focuses on two related processes: the process of counts and the sequence of interarrival times. For each process, an exact closed-form expression of its complete autocorrelation function is obtained. Explicit algorithms are provided to compute both autocorrelation functions, which are numerically evaluated for different distributions of the busy and idle periods and the batch size. The results provided in this paper reveal the analytical tractability of these models which, in addition to their flexibility, makes them very suitable for the performance evaluation of discrete-time communication systems and for general research in the area of queuing theory.\"",
        "1 is \"On Routing in Multichannel Wireless Mesh Networks: Challenges and Solutions\", 2 is \"Characterizing Superposition Arrival Processes in Packet Multiplexers for Voice and Data\"",
        "Given above information, for an author who has written the paper with the title \"Semantic Interactions for Context-Aware and Service-Oriented Architecture\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006309": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Explaining mobile community user participation from a social capital perspective':",
        "Document: \"Understanding Competing Web Application Platforms: An Extended Theory of Planned Behavior and its Relative Model. Despite many competing software systems being offered by different vendors, studies in information technology (IT) adoption rarely look at the adoption of competing products. We develop an extended theory of planned behavior (TPB), drawn from social psychology and marketing theories, to predict choice intention and subsequently the choice of two products. The extended TPB is used to compare the two products in a single analysis using relative values. The study is set in the context of two web application platforms: Microsoft and Java. A survey shows that the relative model can explain a high percentage of the variance in choice intention and actual choice. The results suggest that each product may have its own forte, with a different set of important factors. Applying the relative model to examine competing products can capture different dimensions and strengths to differentiate impacts on choice of alternative products. Factors that appear to be important for single-product analyses may not be important in a relative analysis. This highlights the importance of having a relative model study over separate single-product studies.\"",
        "Document: \"It-Enabled Dynamic Capability Creation: A Perspective On Exploitative Vs. Explorative It Utilization. By focusing on today's highly competitive and rapidly changing business environment, this study theorizes how organizations can create their dynamic capability from the utilization of information technology (IT) resources. The organizational exploitation and exploration perspective is adopted as the central theoretical basis of this study. A reflection on the exploitation and exploration of organizational IT management provides the possibility for theorizing multiple paths for IT-enabled dynamic capability creation. Under our theoretical development, the multiple paths involve different types of IT utilization capabilities that, in conjunction with organizational IT resources and other non-IT factors, lead to organizational dynamic capability. This study provides a theoretical basis for the role of IT in creating organizational dynamic capability. Specifically, it reveals the multiple types of interrelations between organizational IT resources and their utilization capabilities. This study serves as a basis for further empirical studies.\"",
        "Document: \"Knowledge management systems (KMS) continuance in organizations: a social relational perspective. This study explores knowledge management systems (KMS) continuance behavior in organizations. The study draws from the tenets of prior research on user acceptance and continuance of IS and the Social Capital Theory and suggest that both the technical and the situational social aspects of a KMS needs to be considered to understand KMS continuance. A conceptual model and a set of theoretical propositions are proposed as a foundation for further investigation.\"",
        "Document: \"Reducing status effects with computer-mediated communication: evidence from two distinct national cultures. Matching laboratory experiments were conducted in two distinct national cultures to investigate whether computer-mediated communication (CMC) can reduce status effects during group communication in both national cultures. Three independent variables were studied: national culture (Singapore versus U.S.), task type (intellective versus preference), and communication medium (unsupported versus CMC). Three different facets of status effects were measured as dependent variables: status influence, sustained influence, and perceived influence. Singapore groups reported higher sustained influence than U.S. groups. Preference task groups experienced higher status influence and sustained influence than intellective task groups. Unsupported groups also had higher status influence and sustained influence compared to CMC groups. In addition, Singapore groups that completed the preference task in the unsupported setting reported higher perceived influence than groups under other treatments. These results demonstrate that CMC appears to be able to reduce status effects during group communication, both in Singapore and in the United States. This is especially true when groups are working on a preference task. Moreover, status influence appears to be more sustainable in Singapore groups, where group members appear to be more conscious of its presence, than in U.S. groups.\"",
        "Document: \"Understanding Team Influence on Professionals' Acceptance of Large- Scale Systems. Abstract   This paper,highlights the importance,of team,influence in affecting professionals\u2019  acceptance,of large-scale information,systems. A longitudinal study involving 103  physicians was conducted with two data collections performed,three months,apart. The  results show that team influence has a significant effect on perceived usefulness, but not  on perceived ease of use, and that satisfaction with using the system of interest is  significantly affected by team influence, but not by perceived usefulness. The findings\"",
        "1 is \"To better stand on the shoulder of giants\", 2 is \"Predicting consumer intentions to use on-line shopping: the case for an augmented technology acceptance model\"",
        "Given above information, for an author who has written the paper with the title \"Explaining mobile community user participation from a social capital perspective\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006318": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Adaptive Decoupling Switching Control of the Forced-Circulation Evaporation System Using Neural Networks':",
        "Document: \"Multiscale asymmetric orthogonal wavelet kernel for linear programming support vector learning and nonlinear dynamic systems identification. Support vector regression for approximating nonlinear dynamic systems is more delicate than the approximation of indicator functions in support vector classification, particularly for systems that involve multitudes of time scales in their sampled data. The kernel used for support vector learning determines the class of functions from which a support vector machine can draw its solution, and the choice of kernel significantly influences the performance of a support vector machine. In this paper, to bridge the gap between wavelet multiresolution analysis and kernel learning, the closed-form orthogonal wavelet is exploited to construct new multiscale asymmetric orthogonal wavelet kernels for linear programming support vector learning. The closed-form multiscale orthogonal wavelet kernel provides a systematic framework to implement multiscale kernel learning via dyadic dilations and also enables us to represent complex nonlinear dynamics effectively. To demonstrate the superiority of the proposed multiscale wavelet kernel in identifying complex nonlinear dynamic systems, two case studies are presented that aim at building parallel models on benchmark datasets. The development of parallel models that address the long-term/mid-term prediction issue is more intricate and challenging than the identification of series-parallel models where only one-step ahead prediction is required. Simulation results illustrate the effectiveness of the proposed multiscale kernel learning.\"",
        "Document: \"Model predictive control allocation for overactuated systems - stability and performance. Overactuated systems often arise in automotive, aerospace, and robotics applications, where for reasons of redundancy or performance constraints, it is beneficial to equip a system with more control inputs than outputs. This necessitates control allocation methods that distribute control effort amongst many actuators to achieve a desired effect. Until recently, most methods have treated the control allocation as static in the sense that different dynamic authorities of the actuators were not taken into account. Recent advances have used model predictive control allocation (MPCA) to consider the dynamic authorities of the actuators over a receding horizon. In this paper, we consider the dynamic control allocation problem for overactuated systems where each actuator has different dynamic control authority and hard saturation limits. A modular control design approach is proposed, where the controller consists of an outer loop controller that synthesizes a desired virtual control input signal and an inner loop controller that uses MPCA to achieve the desired virtual control signal. We derive sufficient stability conditions for the composite feedback system and show how these conditions may be realized by imposing an additional constraint on the MPCA design. An automotive example is provided to illustrate the effectiveness of the proposed algorithm.\"",
        "Document: \"Robust Control of Linear Systems With Disturbances Bounded in a State Dependent Set. This technical note examines attractiveness and minimality of invariant sets for linear systems subject to additive disturbances confined in a state-dependent set. Existence of a minimal attractor is proved under the assumption that the state-dependent set, in which the disturbance is confined, is upper-semi-continuous. In many practical applications, the disturbance may evolve in a compact set while being generated by a dynamic process with a given model and bounded input. Our results for systems subject to general disturbances confined in state-dependent sets are applied to this case to prove the existence of a minimal robust invariant attractor.\"",
        "Document: \"Design of Multimode Power-Split Hybrid Vehicles - A Case Study on the Voltec Powertrain System. Planetary gears (PGs) have been used in power-split hybrid and plug-in hybrid vehicles, sometimes with clutches to enable multimode operations. Chevrolet Volt is a well-known production vehicle using such technologies; it achieves excellent performance in both charge-depleting (CD) and charge-sustaining modes. In this paper, a detailed analysis of the General Motors (GM) Voltec powertrain used in ...\"",
        "Document: \"Model predictive control allocation: design and experimental results on a thermal management system. In this paper, we consider the challenge of controlling an overactuated engine thermal management system where two actuators, with different dynamic authorities and saturation limits, are used to obtain tight temperature regulation. We propose a modular control strategy that combines model predictive control allocation (MPCA) with the concepts of model reference control to design an inner loop controller that closely matches a dynamic specification for the inner loop input-output performance while addressing actuator dynamics and saturation constraints. We present the design and implementation strategy and illustrate the effectiveness of the proposed solution through real-time simulation and experimental results.\"",
        "1 is \"Object tracking via appearance modeling and sparse representation\", 2 is \"Abnormal Interictal Gamma Activity May Manifest A Seizure Onset Zone In Temporal Lobe Epilepsy\"",
        "Given above information, for an author who has written the paper with the title \"Adaptive Decoupling Switching Control of the Forced-Circulation Evaporation System Using Neural Networks\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006344": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Online Anomaly Prediction for Robust Cluster Systems':",
        "Document: \"Siglm: Signature-Driven Load Management For Cloud Computing Infrastructures. Cloud computing has emerged as a promising platform that grants users with direct yet shared access to computing resources and services without worrying about the internal complex infrastructure. Unlike traditional batch service model, cloud service model adopts a pay-as-you-go form, which demands explicit and precise resource control. In this paper, we present SigLM, a novel Signature-driven Load Management system to achieve quality-aware service delivery in shared cloud computing infrastructures. SigLM dynamically captures fine-grained signatures of different application tasks and cloud nodes using time series patterns, and performs precise resource metering and allocation based on the extracted signatures. SigLM employs dynamic time warping algorithm and multi-dimensional time series indexing to achieve efficient signature pattern matching. Our experiments using real load traces collected on the PlanetLab show that SigLM can improve resource provisioning performance by 30-80% compared to existing approaches. SigLM is scalable and efficient, which imposes less than 1% overhead to the system and can perform signature matching within tens of milliseconds.\"",
        "Document: \"On Predictability of System Anomalies in Real World. As computer systems become increasingly complex, system anomalies have become major concerns in system management. In this paper, we present a comprehensive measurement study to quantify the predictability of different system anomalies. Online anomaly prediction allows the system to foresee impending anomalies so as to take proper actions to mitigate anomaly impact. Our anomaly prediction approach combines feature value prediction with statistical classification methods. We conduct extensive measurement study to investigate anomalous behavior of three systems in the real world: PlanetLab, SMART hard drive data, and IBM System S. We observe that real world system anomalies do exhibit predictability, which can be predicted with high accuracy and significant lead time.\"",
        "Document: \"Toward Predictive Failure Management for Distributed Stream Processing Systems. Distributed stream processing systems (DSPSs) have many important applications such as sensor data analysis, network security, and business intelligence. Failure management is essential for DSPSs that often require highly-available system operations. In this paper, we explore a new predictive failure management approach that employs online failure prediction to achieve more efficient failure management than previous reactive or proactive failure management approaches. We employ light-weight stream-based classification methods to perform online failure forecast. Based on the prediction results, the system can take differentiated failure preventions on abnormal components only. Our failure prediction model is tunable, which can achieve a desired tradeoff between failure penalty reduction and prevention cost based on a user-defined reward function. To achieve low-overhead online learning, we propose adaptive data stream sampling schemes to adaptively adjust measurement sampling rates based on the states of monitored components, and maintain a limited size of historical training data using reservoir sampling. We have implemented an initial prototype of the predictive failure management framework within the IBM System S distributed stream processing system. Experiment results show that our system can achieve more efficient failure management than conventional reactive and proactive approaches, while imposing low overhead to the DSPS.\"",
        "Document: \"Scalable Distributed Service Integrity Attestation for Software-as-a-Service Clouds. Software-as-a-service (SaaS) cloud systems enable application service providers to deliver their applications via massive cloud computing infrastructures. However, due to their sharing nature, SaaS clouds are vulnerable to malicious attacks. In this paper, we present IntTest, a scalable and effective service integrity attestation framework for SaaS clouds. IntTest provides a novel integrated attestation graph analysis scheme that can provide stronger attacker pinpointing power than previous schemes. Moreover, IntTest can automatically enhance result quality by replacing bad results produced by malicious attackers with good results produced by benign service providers. We have implemented a prototype of the IntTest system and tested it on a production cloud computing infrastructure using IBM System S stream processing applications. Our experimental results show that IntTest can achieve higher attacker pinpointing accuracy than existing approaches. IntTest does not require any special hardware or secure kernel support and imposes little performance impact to the application, which makes it practical for large-scale cloud systems.\"",
        "Document: \"Adaptive offloading inference for delivering applications in pervasive computing environments. Pervasive computing allows a user to access an application on heterogeneous devices continuously and consistently. However it is challenging to deliver complex applications on resource-constrained mobile devices, such as cellular telephones and PDA. Different approaches, such as application-based or system-based adaptations, have been proposed to address the problem. However existing solutions often require degrading application fidelity. We believe that this problem can be overcome by dynamically partitioning the application and offloading part of the application execution to a powerful nearby surrogate. This will enable pervasive application delivery to be realized without significant fidelity degradation or expensive application rewriting. Because pervasive computing environments are highly dynamic, the runtime offloading system needs to adapt to both application execution patterns and resource fluctuations. Using the fuzzy control model, we have developed an offloading inference engine to adaptively solve two key decision-making problems during runtime offloading: (1) timely triggering of adaptive offloading, and (2) intelligent selection of an application partitioning policy. Extensive trace-driven evaluations show the effectiveness of the offloading inference engine.\"",
        "1 is \"Optimizing Multiple Queries in Distributed Data Stream Systems\", 2 is \"Web-scale information extraction in knowitall: (preliminary results)\"",
        "Given above information, for an author who has written the paper with the title \"Online Anomaly Prediction for Robust Cluster Systems\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006352": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Algorithms and analysis of scheduling for low-power high-performance DSP on VLIW processors':",
        "Document: \"DLB: Dynamic lane borrowing for improving bandwidth and performance in Hybrid Memory Cube. The Hybrid Memory Cube (HMC) is an innovative DRAM architecture that adopts 3D-stacking to improve bandwidth and save energy. An HMC module adopts separate receive and transmit lanes and thus may achieve the maximal memory bandwidth only if data can be driven at full speed in both directions. However, due to the natural read and write imbalance in modern applications, the effective memory bandwidth utilization is often low, leading to suboptimal system performance. In this paper, we propose DLB (dynamic lane borrowing) that dynamically tracks link utilization and partitions the lanes in one link between receive and transmit directions. DLB allocates more lanes to transmit if servicing read-intensive applications. With more lanes allocated to either direction, DLB reduces the lane contention along that direction and thus the average memory access latency. Our experimental results show that DLB improves the bandwidth utilization by 10.4% on average, reduces the average utilization gap in two directions from 35.6% to 12.8%, and saves execution time by as much as 22.3%.\"",
        "Document: \"Architecting a common-source-line array for bipolar non-volatile memory devices. Traditional array organization of bipolar non-volatile memories such as STT-MRAM and memristor utilizes two bitlines for cell manipulations. With technology scaling, such bitline pair will soon become the bottleneck of density improvement. In this paper we propose a novel common-source-line array architecture, which uses a shared source-line along the row, leaving only one bitline per column. We also elaborate our design flow towards a reliable common-source-line array design, and demonstrate its effectiveness on STT-MRAM and memristor memory arrays. Our study results show that with comparable latency and energy, the proposed common-source-line array can save 33% and 21.8% area for Memristor-RAM and STT-MRAM respectively, comparing with corresponding traditional dual-bitline array designs.\"",
        "Document: \"Procedural level address offset assignment of DSP applications with loops. Automatic optimization of address offset assignment for DSP applications, which reduces the number of address arithmetic instructions to meet the tight memory size restrictions and performance requirements, received a lot of attention in recent years. However, most of current research focuses at the basic block level and does not distinguish different program structures, especially loops. Moreover, the effectiveness of modify register (MR) is not fully exploited since it is used only in the post optimization step. A novel address offset assignment approach is proposed at the procedural level. The MR is effectively used in the address assignment for loop structures. By taking advantage of MR, variables accessed in sequence within a loop are assigned to memory words of equal distances. Both static and dynamic addressing instruction counts are greatly reduced. For DSPSTONE benchmarks and on average, 9.9%, 17.1% and 21.8% improvements are achieved over address offset assignment [R. Leupers et al., (1996)] together with MR optimization when there is 1, 2 and 4 address registers respectively\"",
        "Document: \"A speculative arbiter design to enable high-frequency many-VC router in NoCs. High-performance network-on-chip routers usually prefer a large number of Virtual Channels (VC) for high throughput. However, the growth in VC count results in increased arbitration complexity and reduced router clock frequency. In this paper, we propose a novel high-frequency many-input arbiter design for many-VC routers. It is based on the speculation on short and thus fast arbitrations in case of high VC occupancy. We further enhance it to reduce arbitration latency and promote speculation opportunity. Simulation results show that using the proposed arbiter, a 16-VC router achieves almost the same performance as an ideal design, showing improvements of around 48% on zero-load latency and 100% on network throughput over a naive 16-VC design.\"",
        "Document: \"On the Restore Time Variations of Future DRAM Memory. As the de facto main memory standard, DRAM (Dynamic Random Access Memory) has achieved dramatic density improvement in the past four decades, along with the advancements in process technology. Recent studies reveal that one of the major challenges in scaling DRAM into the deep sub-micron regime is its significant variations on cell restore time, which affect timing constraints such as write recovery time. Adopting traditional approaches results in either low yield rate or large performance degradation. In this article, we propose schemes to expose the variations to the architectural level. By constructing memory chunks with different access speeds and, in particular, exploiting the performance benefits of fast chunks, a variation-aware memory controller can effectively mitigate the performance loss due to relaxed timing constraints. We then proposed restore-time-aware rank construction and page allocation schemes to make better use of fast chunks. Our experimental results show that, compared to traditional designs such as row sparing and Error Correcting Codes, the proposed schemes help to improve system performance by about 16% and 20%, respectively, for 20nm and 14nm technology nodes on a four-core multiprocessor system.\"",
        "1 is \"Instruction Scheduling for Low Power\", 2 is \"Near speed-of-light signaling over on-chip electrical interconnects\"",
        "Given above information, for an author who has written the paper with the title \"Algorithms and analysis of scheduling for low-power high-performance DSP on VLIW processors\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006429": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A relation-based services management mechanism for service computing':",
        "Document: \"Service-Controlled Networking: Dynamic In-Network Data Fusion for Heterogeneous Sensor Networks. Recent advances of Wireless Sensor Networks technologies has witnessed the dynamic changes of our daily life. With the sensors deployed everywhere, people may benefit from various services (weather service, traffic information, environment sensing, etc.) based on these sensor networks. However, how to collect/manage/process those various sensor dates from heterogeneous sensor networks still remain challenges. In this paper, we introduce our recent research effort and elaborate how we try to overcome these challenges with a large scale test bed environment. We first propose a novel concept named service-controlled networking (SCN), a network middleware which is capable to dynamically process data fusion at In-Network level. We then verify the usefulness by extensive experiment evaluation. Last, we discuss the future direction of SCN.\"",
        "Document: \"Phenomena-Based Management of Geotime-Tagged Contents on the Web. Recently, Geoweb services help people easily create and share geo-spatiotemporal contents through Internet. As a result, it becomes an emerging issue to manage Geoweb contents in exploding lots of spatiotemporal information and to obtain comprehensive knowledge about the real world using web contents. This paper proposes a novel approach for managing geotime-tagged contents on the basis of moving phenomena, such as typhoons, earthquakes, and flooding. In this study, a moving phenomenon is represented by a predefined three-dimensional object in a spatiotemporal domain with predefined meanings, and we define a phenomena-based predicate template named Relates as a combination of spatial, temporal, and thematic constraints. By using the Relates predicate, we can dynamically aggregate geotime-tagged contents along three aspects: location, time, and topic. Moreover, we present an application of the phenomena-based contents management, called k-Sticker. k-Sticker allows users to find top-k terms related to their own moving phenomenon through a three-dimensional (two spatial dimensions and one temporal dimension) visualization method.\"",
        "Document: \"Spoken Dialog System for Next Generation Knowledge Access. This paper described our development dialog system on Kyoto tourist information assistance. Dialog part of our system helped user to make an appropriate query. Information analysis part would be assisted for user to select the retrieved information. Nowadays we can get most information through the Internet. However, we have a trouble to pick up expected information from the huge results with conventional search engines. Especially in mobile terminal, we are confronted with great difficulties for two factors. One is that most of users cannot make an appropriate query because their request is vague with theirselves. The other is that the retrieved information has huge variation and mobile terminal has small area for displaying them. Therefore, we aim to develop technologies for the users to input their requests by familiar way and clarify what they want to know with displaying the retrieved information with suitable method.\"",
        "Document: \"Utterance Classification Using Linguistic and Non-linguistic Information for Network-Based Speech-to-Speech Translation Systems. Network-based mobile services, such as speech-to-speech translation and voice search, enable the construction of large-scale log database including speech. We have developed a smartphone application called VoiceTra for speech-to-speech translation and have collected 10, 000, 000 utterances so far. This huge corpus is unique in size and spatio-temporal information, it contains information on anonymized user locations. This spatio-temporal corpus can be used for improving the accuracy of its speech recognition and machine translation, and it will open the door for the study of the location dependency of vocabulary and new applications for location-based services. This paper first analyzes the corpus and then presents a novel method for classifying utterances using linguistic and non-linguistic information. L2-regularized Logistic Regression is used for utterance classification. Our experiments performed on the VoiceTra log corpus revealed that our proposed method outperformed baseline methods in terms of F measure.\"",
        "Document: \"Towards an Open Data Development Model for Linking Heterogeneous Data Sources. Open data is providing opportunities as well as challenges for research and development in the field of knowledge and systems engineering. The prospects for both researchers and application developers accessing and integrating publicly available data from various sources are unprecedented. However, many open data platforms are built in such a way that it is difficult or, sometimes, impossible to extract value from the data, integrate data from various sources, transform or improve the quality of the data through continuous use and reuse, track provenance, and share research findings. In this paper we present open data development model (ODDM) that addresses some these difficulties. The model shows how various stakeholders can check-out data from an open data repository, use and reuse the data, add new functionalities, appraise, transform, and publish or check-in the improved data for other researchers to benefit. We discuss how we intend to apply the model in practice to build a new generation of open data repository for interdisciplinary research, and highlight key issues for debate and discussion.\"",
        "1 is \"Evaluation of mobile information retrieval strategies\", 2 is \"An Efficient Decentralized Grid Service Discovery Approach based on Service Ontology\"",
        "Given above information, for an author who has written the paper with the title \"A relation-based services management mechanism for service computing\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006440": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A Framework For Developing Conversational User Interfaces':",
        "Document: \"The design of phoneme grouping for coarse phoneme recognition. Automatic speech recognition for real-world applications such as a robot should deal with speech under noisy environments. This paper presents coarse phoneme recognition which uses a phoneme group instead of a phoneme as a unit of speech recognition for such a real-world application. In coarse phoneme recognition, the design of the phoneme group is crucial. We, thus, introduce two types of phoneme groups - exclusive and overlapping phoneme groups, and evaluate coarse phoneme recognition with these two phoneme grouping methods under various kinds of noise conditions. The experimental results show that our proposed overlapping phoneme grouping improves the correct phoneme inclusion rate by 20 points on average.\"",
        "Document: \"Automatic allocation of training data for rapid prototyping of speech understanding based on multiple model combination. The optimal choice of speech understanding method depends on the amount of training data available in rapid prototyping. A statistical method is ultimately chosen, but it is not clear at which point in the increase in training data a statistical method become effective. Our framework combines multiple automatic speech recognition (ASR) and language understanding (LU) modules to provide a set of speech understanding results and selects the best result among them. The issue is how to allocate training data to statistical modules and the selection module in order to avoid overfitting in training and obtain better performance. This paper presents an automatic training data allocation method that is based on the change in the coefficients of the logistic regression functions used in the selection module. Experimental evaluation showed that our allocation method outperformed baseline methods that use a single ASR module and a single LU module at every point while training data increase.\"",
        "Document: \"Constraint projection: an efficient treatment of disjunctive feature descriptions. Unification of disjunctive feature descriptions is important for efficient unification-based parsing. This paper presents constraint projection, a new method for unification of disjunctive feature structures represented by logical constraints. Constraint projection is a generalization of constraint unification, and is more efficient because constraint projection has a mechanism for abandoning information irrelevant to a goal specified by a list of variables.\"",
        "Document: \"Selection of Unknown Objects Specified by Speech Using Models Constructed from Web Images. To select an object requested by human voice among several unknown objects is one of the important tasks for household robots that assist human's daily lives. In this paper, we propose a method that can achieve this task. Using image models which are constructed by Web images collected from the results of speech recognition, the proposed method enables a robot to select an object specified by a speech from several unknown objects in a real environment.\"",
        "Document: \"The dynamics of action corrections in situated interaction. In spoken communications, correction utterances, which are utterances correcting other participants utterances and behaviors, play crucial roles, and detecting them is one of the key issues. Previously, much work has been done on automatic detection of correction utterances in human-human and human-computer dialogs, but they mostly dealt with the correction of erroneous utterances. However, in many real situations, especially in communications between humans and mobile robots, the misunderstandings manifest themselves not only through utterances but also through physical actions performed by the participants. In this paper, we focus on action corrections and propose a classification of such utterances into Omission, Commission, and Degree corrections. We present the results of our analysis of correction utterances in dialogs between two humans who were engaging in a kind of on-line computer game, where one participant plays the role of the remote manager of a convenience store, and the other plays the role of a robot store clerk. We analyze the linguistic content, prosody as well as the timing of correction utterances and found that all features were significantly correlated with action corrections.\"",
        "1 is \"How Transferable are Neural Networks in NLP Applications?\", 2 is \"Locality-constrained Linear Coding for image classification\"",
        "Given above information, for an author who has written the paper with the title \"A Framework For Developing Conversational User Interfaces\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006479": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Another stride towards knowledge-based machine translation':",
        "Document: \"Inferring rules of Escherichia coli translational efficiency using an artificial neural network. Although the machinery for translation initiation in Escherichia coli is very complicated, the translational efficiency has been reported to be predictable from upstream oligonucleotide sequences. Conventional models have difficulties in their generalization ability and prediction nonlinearity and in their ability to deal with a variety of input attributions. To address these issues, we employed structural learning by artificial neural networks to infer general rules for translational efficiency. The correlation between translational activities measured by biological experiments and those predicted by our method in the test data was significant (r=0.78), and our method uncovered underlying rules of translational activities and sequence patterns from the obtained skeleton structure. The significant rules for predicting translational efficiency were (1) G- and A-rich oligonucleotide sequences, resembling the Shine\u2013Dalgarno sequence, at positions \u221210 to \u22127; (2) first base A in the initiation codon; (3) transport/binding or amino acid metabolism gene function; (4) high binding energy between mRNA and 16S rRNA at positions \u221215 to \u22125. An additional inferred novel rule was that C at position \u22121 increases translational efficiency. When our model was applied to the entire genomic sequence of E. coli, translational activities of genes for metabolism and translational were significantly high.\"",
        "Document: \"Kinetic simulation of signal transduction system in hippocampal long-term potentiation with dynamic modeling of protein phosphatase 2A. We modeled and analyzed a signal transduction system of long-term potentiation (LTP) in hippocampal post-synapse. Bhalla and Iyengar [Science 283(1999) 381] have developed a hippocampal LTP model. In the conventional model, the concentration of protein phosphatase 2A (PP2A) was fixed. However, it was reported that dynamic inactivation of PP2A was essential for LTP [J. Neurochem. 74 (2000) 807]. We introduced a dynamic modeling of PP2A; inactivation (phosphorylation) of PP2A by calcium/calmodulin-dependent protein kinase II (CaMKII) in the presence of calcium/calmodulin, self-activation (autodephosphorylation) of PP2A, and inactivation (dephosphorylation) of CaMKII by PP2A. This model includes complex feedback loops; both CaMKII and PP2A are autoactivated, while they inactivate each other. Moreover, we proposed an analysis strategy for model validation by applying the results of sensitivity analysis. In our system, calcineurin (CaN) played an essential role, rather than the activation of protein kinase C (PKC) as documented in the conventional model. From results of the analysis of our model, we found the following robustness as characteristics of bistability in our model: (1). PP2A reactions against calcium ion (Ca(2+)) perturbation; (2). PP2A inactivation against PP2A increase; (3). protein phosphatase 1 (PP1) activation against PF2A increase; and (4). PP2A reactions against PP2A initial concentration. These properties facilitated LTP induction in our system. We showed that another mechanism could introduce bistable behavior by adding dynamic reactions of PP2A.\"",
        "Document: \"Time-resolved metabolomics reveals metabolic modulation in rice foliage. To elucidate the interaction of dynamics among modules that constitute biological systems, comprehensive datasets obtained from \"omics\" technologies have been used. In recent plant metabolomics approaches, the reconstruction of metabolic correlation networks has been attempted using statistical techniques. However, the results were unsatisfactory and effective data-mining techniques that apply appropriate comprehensive datasets are needed.Using capillary electrophoresis mass spectrometry (CE-MS) and capillary electrophoresis diode-array detection (CE-DAD), we analyzed the dynamic changes in the level of 56 basic metabolites in plant foliage (Oryza sativa L. ssp. japonica) at hourly intervals over a 24-hr period. Unsupervised clustering of comprehensive metabolic profiles using Kohonen's self-organizing map (SOM) allowed classification of the biochemical pathways activated by the light and dark cycle. The carbon and nitrogen (C/N) metabolism in both periods was also visualized as a phenotypic linkage map that connects network modules on the basis of traditional metabolic pathways rather than pairwise correlations among metabolites. The regulatory networks of C/N assimilation/dissimilation at each time point were consistent with previous works on plant metabolism. In response to environmental stress, glutathione and spermidine fluctuated synchronously with their regulatory targets. Adenine nucleosides and nicotinamide coenzymes were regulated by phosphorylation and dephosphorylation. We also demonstrated that SOM analysis was applicable to the estimation of unidentifiable metabolites in metabolome analysis. Hierarchical clustering of a correlation coefficient matrix could help identify the bottleneck enzymes that regulate metabolic networks.Our results showed that our SOM analysis with appropriate metabolic time-courses effectively revealed the synchronous dynamics among metabolic modules and elucidated the underlying biochemical functions. The application of discrimination of unidentified metabolites and the identification of bottleneck enzymatic steps even to non-targeted comprehensive analysis promise to facilitate an understanding of large-scale interactions among components in biological systems.\"",
        "Document: \"An efficient context-free parsing algorithm for natural languages. In this paper we present a type inference method for Prolog programs. The new idea is to describe a superset of the success set by associating a type substitution (an assignment of sets of ground terms to variables) with each head of definite clause. ...\"",
        "Document: \"Mmmdb: Mouse Multiple Tissue Metabolome Database. The Mouse Multiple Tissue Metabolome Database (MMMDB) provides comprehensive and quantitative metabolomic information for multiple tissues from single mice. Manually curated databases that integrate literature-based individual metabolite information have been available so far. However, data sets on the absolute concentration of a single metabolite integrated from multiple resources are often difficult to be used when different metabolomic studies are compared because the relative balance of the multiple metabolite concentrations in the metabolic pathways as a snapshot of a dynamic system is more important than the absolute concentration of a single metabolite. We developed MMMDB by performing non-targeted analyses of cerebra, cerebella, thymus, spleen, lung, liver, kidney, heart, pancreas, testis and plasma using capillary electrophoresis time-of-flight mass spectrometry and detected 428 non-redundant features from which 219 metabolites were successfully identified. Quantified concentrations of the individual metabolites and the corresponding processed raw data; for example, the electropherograms and mass spectra with their annotations, such as isotope and fragment information, are stored in the database. MMMDB is designed to normalize users' data, which can be submitted online and used to visualize overlaid electropherograms. Thus, MMMDB allows newly measured data to be compared with the other data in the database. MMMDB is available at: http://mmmdb.iab.keio.ac.jp.\"",
        "1 is \"Classification models for the prediction of clinicians' information needs\", 2 is \"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\"",
        "Given above information, for an author who has written the paper with the title \"Another stride towards knowledge-based machine translation\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006523": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Identifying Contact Formations from Force Signals: A Comparison of Fuzzy and Neural Network Classifiers':",
        "Document: \"Context-Centric Needs Anticipation Using Information Needs Graphs. Effective agent teamwork requires information exchange to be conducted in a proactive, selective, and intelligent way. In the field of distributed artificial intelligence, there has been increasing number of research focusing on need-driven proactive communication, both theoretically and practically. Among these works, CAST has realized a team-oriented agent architecture where agents, based on a computational shared mental model, are able to anticipate teammates' information needs and proactively deliver relevant information to the needers in a timely manner. However, the first implementation of CAST takes little consideration of the dynamics of the anticipated information needs, which can change in various ways as the context develops. In this paper we describe a novel mechanism for organizing and managing the \"context\" of information needs. This allows agents to dynamically activate and deactivate information needs progressively. It has been shown that the two-level context-centric approach can enhance team performance considerably.\"",
        "Document: \"On Proactive Helping Behaviors In Teamwork. Teamwork has become increasingly important in diverse disciplines. Cognitive studies on teamwork have shown that team members in an effective team often have mutual expectations based on their shared mental models and proactively offer assistance to each other. We present a formal model called Role-Based Proactive Helping Behaviors (RoB-PHB) to enable proactive assistance among (sub)teams. Through a RoB-PHB, agents can dynamically identify others' help needs and provide helps by a course of actions on the fly. We have designed algorithms to implement our RoB-PHB formalism in a teamwork architecture called Role-Based Collaborative Agents for Simulating Teamwork (RoB-CAST). Our experiments on RoB-CAST have shown that the team with proactive helping behavior achieved better team performance.\"",
        "Document: \"Automatic evaluation of two-fingered grips. Grip determination is essential to any task-level planning process. The complete force/moment equations are presented for grasping employing a rigid two-fingered gripper with a thin elastic layer on the contacting surface. The surface contact is modeled as a linear pressure variation. The quality measure of a grip is taken to be the coefficient of friction needed to keep the held object from slipping between the fingers under applied forces and moments. The lower the coefficient of friction, the better the grip. Incorporation of this evaluation into general grip selection strategy is discussed, and several examples are given.\"",
        "Document: \"Recognizing partially hidden objects. In this paper, an approach is described for recognizing and locating partially hidden objects in an image. The method is based upon matching pairs of boundary segments of the template of an object with pairs of boundary segments in the image. Using a Bayesian based signal detection approach, pairs of segments are selected from the template of the object such that the probability of correctly identifying the object given that the pair is matched in the image is close to one. Assuming that models of all objects which might appear in the scene (a reasonable assumption for industrial applications) are known a priori, suitable pairs of segments can be determined a priori. Preliminary investigation suggests that the technique is robust and that subsecond recognition time can be achieved.\"",
        "Document: \"Teleautonomous systems: projecting and coordinating intelligent action at a distance. The authors have coined the term teleautonomous to describe methods for producing intelligent action at a distance. Teleautomation goes beyond autonomous control in that it blends in human intelligence and action as appropriate. It goes beyond teleoperation in that it incorporates as much autonomy as is possible or reasonable. A novel approach for solving one of the fundamental problems facing teleautonomous systems is discussed in detail: the need to overcome time delays due to telemetry and signal propagation. Concepts called time and position clutches are introduced; these allow the time and position frames, respectively, between the local user control and the remote device being controlled to be desynchronized. The design and implementation of these mechanisms lead to substantial telemanipulation performance improvements, including the novel result of improvements even in the absence of time delays. The controls also yield a simple protocol for handoffs of the control of manipulation tasks between local operators and remote systems\"",
        "1 is \"Reasoning about Beliefs, Observability, and Information Exchange in Teamwork\", 2 is \"Urban target classifications using time-frequency micro-Doppler signatures\"",
        "Given above information, for an author who has written the paper with the title \"Identifying Contact Formations from Force Signals: A Comparison of Fuzzy and Neural Network Classifiers\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006587": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Integrated Synthesis and Execution of Optimal Plans for Multi-Robot Systems in Logistics':",
        "Document: \"Ontology-based data access: An application to intermodal logistics. In this paper, we investigate ontology-based data access (OBDA) to build information systems whose purposes are (i) gathering data from a network of intermodal terminals, and (ii) computing performance indicators of the network. This application domain is characterized by large amounts of data and relatively simple data models, making it a natural challenge for logic-based knowledge representation and reasoning techniques. Considering relational database (RDB) technology as a yardstick, we show that careful engineering of OBDA can achieve RDB-like scalability even in demanding applications. To the best of our knowledge, this is the first study evaluating the potential of OBDA in a typical business-size application.\"",
        "Document: \"Automated Verification of Neural Networks: Advances, Challenges and Perspectives. Neural networks are one of the most investigated and widely used techniques in Machine Learning. In spite of their success, they still find limited application in safety- and security-related contexts, wherein assurance about networksu0027 performances must be provided. In the recent past, automated reasoning techniques have been proposed by several researchers to close the gap between neural networks and applications requiring formal guarantees about their behavior. In this work, we propose a primer of such techniques and a comprehensive categorization of existing approaches for the automated verification of neural networks. A discussion about current limitations and directions for future investigation is provided to foster research on this topic at the crossroads of Machine Learning and Automated Reasoning.\"",
        "Document: \"SAT-based planning in complex domains: concurrency, constraints and nondeterminism. Planning as satisfiability is a very efficient technique for classical planning, i.e., for planning domains in which both the effects of actions and the initial state are completely specified. In this paper we present C-SAT, a SAT-based procedure capable of dealing with planning domains having incomplete information about the initial state, and whose underlying transition system is specified using the highly expressive action language C. Thus, C-SAT allows for planning in domains involving (i) actions which can be executed concurrently; (ii) (ramification and qualification) constraints affecting the effects of actions; and (iii) nondeterminism in the initial state and in the effects of actions. We first prove the correctness and the completeness of C-SAT, discuss some optimizations, and then we present C-PLAN, a system based on C-SAT. C-PLAN works on any C planning problem, but some optimizations have not been fully implemented yet. Nevertheless, the experimental analysis shows that SAT-based approaches to planning with incomplete information are viable, at least in the case of problems with a high degree of parallelism.\"",
        "Document: \"On the Synthesis of Guaranteed-Quality Plans for Robot Fleets in Logistics Scenarios via Optimization Modulo Theories. In manufacturing, the increasing involvement of autonomous robots in production processes poses new challenges on the production management. In this paper we report on the usage of Optimization Modulo Theories (OMT) to solve certain multi-robot scheduling problems in this area. Whereas currently existing methods are heuristic, our approach guarantees optimality for the computed solution. We do not only present our final method but also its chronological development, and draw some general observations for the development of OMT-based approaches.\"",
        "Document: \"Designing a solver competition: the QBFEVAL'10 case study. In this paper we report about QBFEVALu002710, the seventh in a series of events established with the aim of assessing the advancements in reasoning about quantified Boolean formulas (QBFs). The paper discusses the results obtained and the evaluation setup, from the criteria used to select QBF instances down to the hardware infrastructure. We also discuss the current state-of-the-art in light of past challenges and we envision future research directions that are motivated by the results of QBFEVALu002710.\"",
        "1 is \"Sequent calculi for propositional nonmonotonic logics\", 2 is \"Polsat: A Portfolio LTL Satisfiability Solver.\"",
        "Given above information, for an author who has written the paper with the title \"Integrated Synthesis and Execution of Optimal Plans for Multi-Robot Systems in Logistics\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006676": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Strong edge-colorings of sparse graphs with large maximum degree.':",
        "Document: \"Korean text summarization using an aggregate similarity. In this paper, each document is represented by a weighted graph called a text relationship map. In the graph, each node represents a vector of nouns in a sentence, an undirected link connects two nodes if two sentences are semantically related, and a weight on the link is a value of the similarity between a pair of sentences. The vector similarity can be computed as the inner product between corresponding vector elements. The similarity is based on the word overlap between the corresponding sentences. The importance of a node on the map, called an aggregate similarity, is defined as the sum of weights on the links connecting it to other nodes on the map. In this paper, we present a Korean text summarization system using the aggregate similarity. To evaluate our system, we used two test collections: one collection (PAPER-InCon) consists of 100 papers in the domain of computer science; the other collection (NEWS) is composed of 105 articles in the newspapers. Under the compression rate of 20%, we achieved the recall of 46.6% (PAPER-InCon) and 30.5% (NEWS), and the precision of 76.9% (PAPER-InCon) and 42.3% (NEWS). Experiments show that our system outperforms two commercial systems.\"",
        "Document: \"Improper Coloring of Sparse Graphs with a Given Girth, II: Constructions. A graph G is -colorable if can be partitioned into two sets and so that the maximum degree of is at most j and of is at most k. While the problem of verifying whether a graph is 0, 0-colorable is easy, the similar problem with in place of 0, 0 is NP-complete for all nonnegative j and k with . Let denote the supremum of all x such that for some constant every graph G with girth g and for every is -colorable. It was proved recently that . In a companion paper, we find the exact value . In this article, we show that increasing g from 5 further on does not increase much. Our constructions show that for every g, . We also find exact values of for all g and all .\"",
        "Document: \"Proxy-Based Ipv6 Neighbor Discovery Scheme For Wireless Lan Based Mesh Networks. Multi-hop Wireless LAN-based mesh network (WMN) provides high capacity and self-configuring capabilities. Due to data forwarding and path selection based on MAC address, WMN requires additional operations to achieve global connectivity using IPv6 address. The neighbor discovery operation over WLAN mesh networks requires repeated all-node broadcasting and this gives rise to a big burden in the entire mesh networks. In this letter, we propose the proxy neighbor discovery scheme for optimized IPv6 communication over WMN to reduce network overhead and communication latency. Using simulation experiments, we show that the control overhead and communication setup latency can be significantly reduced using the proxy-based neighbor discovery mechanism.\"",
        "Document: \"Posfilter: an efficient filtering technique of XML documents based on postfix sharing. XML message filtering is to evaluate the path matching of a large number of registered path queries over a continuous stream of XML messages in real time. For this purpose, YFilter system has been suggested to exploit the prefix commonalities that exist among path expressions. Sharing such commonality gives the benefit of improving filtering performance through the tremendous reduction in filtering machine size. However, postfix sharing also can be useful for an XML filtering situation. For example, if a stream of XML messages does not have any defined DTD (or XML schema), the XPath queries beginning with the ancestor-descendant axis ('//') can be used often, e.g., '//buyer/name', '//seller/name', and '//name', and such query type is most likely to have the postfix sharing. Therefore, in this paper, we propose a bottom up filtering approach exploiting postfix sharing against the top down approach of YFilter exploiting prefix sharing. Some experimental results show that our method has better performance in the postfix-shared scenario.\"",
        "Document: \"Optimal construction of the city voronoi diagram. We address proximity problems in the presence of roads on the L1 plane. More specifically, we present the first optimal algorithm for constructing the city Voronoi diagram. We apply the continuous Dijkstra paradigm to obtain an optimal algorithm for building a shortest path map for a given source, and then it extends to that for the city Voronoi diagram. Moreover, the algorithm applies to other generalized situations including metric spaces induced by roads and obstacles together.\"",
        "1 is \"A note on Nordhaus-Gaddum inequalities for domination\", 2 is \"The maximum size of hypergraphs without generalized 4-cycles\"",
        "Given above information, for an author who has written the paper with the title \"Strong edge-colorings of sparse graphs with large maximum degree.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006690": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Performance Analysis of Random Access Channel in OFDMA Systems':",
        "Document: \"Evidence-Theory-Based Cooperative Spectrum Sensing With Efficient Quantization Method in Cognitive Radio. Sensing spectra in a reliable and efficient manner is fundamental to cognitive radio (CR). Ensuring cooperation among spectrum sensing devices is an appropriate method when a CR system is under deep shadowing and in a fading environment. In this paper, an enhanced scheme for cooperative spectrum sensing (CSS) based on efficient quantization and the Dempster-Shafer (D-S) theory of evidence is proposed. The proposed scheme includes an effective quantizer for the sensing data by utilizing special properties of the hypothesis distribution under different signal-to-noise ratios (SNRs) of the primary signal. As a result, the required bandwidth for the reporting channel is reduced while the advantage for combinations of the D-S theory is maintained. Simulation results revealed that significant improvements in the CSS gain, as well as a reduction in the system overhead, were achieved.\"",
        "Document: \"Erlang Capacity of CDMA Systems Using Optimized Sectoring. In this paper, we evaluate the Erlang capacity of CDMA systems according to two sector operation methods: optimized sectoring and fixed-angle sectoring. For the performance analysis, a multidimensional Markov channel model is developed. The improved Erlang capacity through optimized sectoring is quantified over fixed-angle sectoring. As a result, we can get about 20% capacity improvement for optimized sectoring over fixed-angle sectoring under the uniform traffic distribution among sectors. In addition, optimized sectoring provides a constant Erlang capacity improvement (about 3.6 times) over an omnidirectional case, even when traffic loads among sectors are nonuniformly distributed. On the other hand, fixed-angle sectoring results in the actual capacity improvement 1.8\u20133 times over an omnidirectional case, depending on the traffic distribution among sectors.\"",
        "Document: \"Cooperative Spectrum Sensing with Double Adaptive Energy Thresholds and Relaying Users in Cognitive Radio. Cognitive radio (CR) technology has been proposed to improve spectrum utilization by allowing cognitive radio users to opportunistically access under-utilized frequency band. The requirement of cognitive radio system is reliable detecting signals from licensed users to avoid harmful interference. However, due to the effects of the channel fading and shadowing, individual cognitive radio may not able to reliably detect the presence of a licensed user. In this paper, we propose a cooperative spectrum sensing scheme with double adaptive threshold and relaying users in order to improve sensing performance of both local sensing performance and global sensing performance in a CR network. In the proposed scheme, the detection threshold is changeable to adapt with the fluctuation of the received signal power in each local detector of cognitive user. In addition, the CR users with higher reliability act as a relaying user for the object of helping their neighbors to improve sensing performance. Finally, all the local observations will be sent to the fusion center to determine the global decision by using the Likelihood rule without any requirement on priori information of the licensed user.\"",
        "Document: \"Throughput Maximization for Sensor-Aided Cognitive Radio Networks with Continuous Energy Arrivals. We consider a Sensor-Aided Cognitive Radio Network (SACRN) in which sensors capable of harvesting energy are distributed throughout the network to support secondary transmitters for sensing licensed channels in order to improve both energy and spectral efficiency. Harvesting ambient energy is one of the most promising solutions to mitigate energy deficiency, prolong device lifetime, and partly reduce the battery size of devices. So far, many works related to SACRN have considered single secondary users capable of harvesting energy in whole slot as well as short-term throughput. In the paper, we consider two types of energy harvesting sensor nodes (EHSN): Type-I sensor nodes will harvest ambient energy in whole slot duration, whereas type-II sensor nodes will only harvest energy after carrying out spectrum sensing. In the paper, we also investigate long-term throughput in the scheduling window, and formulate the throughput maximization problem by considering energy-neutral operation conditions of type-I and -II sensors and the target detection probability. Through simulations, it is shown that the sensing energy consumption of all sensor nodes can be efficiently managed with the proposed scheme to achieve optimal long-term throughput in the window.\"",
        "Document: \"A secure distributed spectrum sensing scheme in cognitive radio. Distributed spectrum sensing provides an improvement for primary user detection but leads a new security threat into CR system. The spectrum sensing data falsification malicious users can decrease the cooperative sensing performance. In this paper, we propose a distributed scheme in which the presence and absence hypotheses distribution of primary signal is estimated based on past sensing received power data by robust statistics, and the data fusion are performed according to estimated parameters by Dempster-Shafer theory of evidence. Our scheme can achive a powerful capability of malicious user elimination due to the abnormality of the distribution of malicious users compared with that of other legitimate users. In addition, the performance of our data fusion scheme is enhanced by supplemented nodes' reliability weight.\"",
        "1 is \"Operational rate-distortion modeling for wavelet video coders\", 2 is \"Fusion of censored decisions in wireless sensor networks\"",
        "Given above information, for an author who has written the paper with the title \"Performance Analysis of Random Access Channel in OFDMA Systems\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006731": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Low power realization of finite state machines\u2014a decomposition approach':",
        "Document: \"Performance Analysis of Multiprocessor Systems Containing Functionally Dedicated Processors. General models of multiprocessor systems in which processors are functionally dedicated are described. In these models, processors are divided into different types. A task can be assigned only to a processor of certain types. Clearly, the model of multiprocessor systems with identical processors is a special case of our models. These models also include the job shop problem in which there is exactly one processor of each type. Worst case performance bounds of priority-driven schedules are obtained.\"",
        "Document: \"Optimal bipartite folding of PLA. The notion of a bipartite folding of a PLA is introduced. An efficient branch and bound algorithm is presented which finds an optimal bipartite folding of a PLA. The experimental results give additional justification to this folding technique.\"",
        "Document: \"An integrated algorithm for incremental data path synthesis. In this paper, we present an incremental synthesis procedure for a class of data path synthesis problems in which synthesis is carried out in a control step by control step fashion. The synthesis problems are classified according to the type of input dataflow graph (scheduled or unscheduled) and the type of storage units used (individual registers, single-port memory units, or multi-port memory units). These problems are solved optimally by employing either a network flow or 0\u20131 integer linear programming formulation. Experimental results on a number of benchmark problems are provided to demonstrate the effectiveness of the incremental synthesis algorithm.\"",
        "Document: \"Register Allocation\u2014A Hierarchical Reduction Approach. A new approach to the problem of register allocation in high-level synthesis is presented. The algorithm employs a bottom-up transformational approach\u2014sets of mutually exclusive variables in conditional branches are transformed into an \u201cequivalent\u201d set of nonmutuallyexclusive variables. The transformational approach is extended to the caseof data-flow graphs with loops. A new register allocation algorithm is thenused to produce an allocation for the nonmutually exclusive variables. Fromsuch an allocation, a corresponding allocation for the original set ofmutually exclusive variables is derived. Our approach is particularlyeffective when there is a large number of nested conditional branches andloops in a data-flow graph.\"",
        "Document: \"Timing-driven placement for regular architectures. We present a new iterative algorithm for timing-driven placement applicable to regular architectures such as field-programmable gate arrays (FPGAs). Our algorithm has two phases in each iteration: a compression phase and a relaxation phase. We employ a novel compression strategy based on the longest path tree of a cone for improving the timing performance of a given placement. Compression might cause a feasible placement to become infeasible. The concept of a slack neighborhood graph is introduced, and is used in the relaxation phase to transform an infeasible placement into a feasible one using a mincost maxflow formulation. The slack neighborhood graph approach used in the relaxation phase guarantees a bounded increase in delay during the relaxation phase. Our analytical results regarding the bounds on delay increase during relaxation are validated by the rapid convergence of our algorithm on benchmark circuits, We obtain placements that have 13% less critical path delay (on the average) than those generated by the Xilinx automatic place and route tool (apr) on technology-mapped MCNC benchmark circuits. The running time of our algorithm is significantly less than that of apr. Slack neighborhood graphs are of independent interest because they can also be used for timing-driven reconfiguration for yield enhancement and for handling incremental design changes efficiently\"",
        "1 is \"Some Basic Cryptographic Requirements For Chaos-Based Cryptosystems\", 2 is \"The McBOOLE logic minimizer\"",
        "Given above information, for an author who has written the paper with the title \"Low power realization of finite state machines\u2014a decomposition approach\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006749": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Framework for a NetFPGA-based Snort NIDS':",
        "Document: \"A game theoretical model for collaborative groups in social applications. \u2022Proposing an analytical model for collaborative groups based on repeated game theory.\u2022Simulating participants\u2019 behaviours using different existing Tit for Tat strategies.\u2022Analysing the impact of the free riders in collaborative groups survivability.\u2022Proposing the group reputation tit for tat strategy to motivate free riders.\"",
        "Document: \"Misbehavior Detection Framework for Community-Based Cloud Computing. The success and continuation of cloud computing depends to a large extent on the quality and performance of the offered services. We propose in this paper a novel architecture for cloud computing called Community-based Cloud Computing whose main goal is to improve the quality and performance of the cloud services. In this architecture, cloud services sharing the same domain of interest are partitioned into a set of communities led by a central entity called master. The advantages of such an architecture are (1) facilitating the discovery of cloud services, (2) providing efficient means for better QoS management and resources utilization, and (3) easing intra-layer and cross-layer compositions. However, one of the serious challenges against the success of such an architecture is the presence of malicious services that launch attacks either against the whole community or against some partners in that Community. Therefore, we address this problem by proposing a misbehavior detection framework based on the Support Vector Machine (SVM) learning technique. In this framework, the master of the community monitors the behavior of its community members to populate the training set of the classifier. Thereafter, SVM is used to analyze this set and predict the final classes of the cloud services. Simulation results show that our framework is able to produce highly accurate classifiers, while maximizing the attack detection rate and minimizing the false alarms. They show also that the framework is quite resilient to the increase in the number of malicious services.\"",
        "Document: \"An Intrusion Detection Game Theoretical Model. We propose, in this article, a game theoretical model for Host-based Intrusion Detection Systems (HIDS). The main drawbacks of existing HIDSs, such as Swatch, are the high computation cost in detection and the generation of false alarms. These limitations are not acceptable in resource-limited systems such as wireless mobile devices. We address these issues by applying game theory to HIDSs, so that HIDSs can discover potential attackers with lower computation cost and false alarm rates. To achieve that, we explore two solutions: The Bayesian model and the Dempster-Shafer (DS) model where the identity of the attacker is unknown. Such type of models can be used in Mobile Ad hoc Networks (MANET) where the sender identity is unknown. They help to determine the belief value that determines whether a sender is misbehaving or not. Our novel contribution in this article is a hybrid model that combines the Bayesian and DS models for decreasing false positives and detecting accurately an attacker. Our simulation results show the benefits from the combination of these two models regarding the posterior belief function that is used for increasing the possibility of intrusion detection.\"",
        "Document: \"AOMD approach for context-adaptable and conflict-free Web services composition. \u2022We provide aspect-oriented constructs for context-adaptable Web services composition.\u2022We afford aspect-oriented constructs that can assure conflict-free composition.\u2022We present new model that allows high level specification of AOP\u2013BPEL aspects.\u2022We formally verify consistency, accuracy, conflicts and deadlocks in the composition.\u2022We provide a plugin integrated in Eclipse BPEL environment.\"",
        "Document: \"A Markov Decision Process Model for High Interaction Honeypots. Honeypots, which are traps designed to resemble easy-to-compromise computer systems, have become essential tools for security professionals and researchers because of their significant contribution in disclosing the underworld of cybercrimes. However, recent years have witnessed the development of several anti-honeypot technologies. Botmasters can exploit the fact that honeypots should not participate in illegal actions by commanding the compromised machine to act maliciously against specific targets which are used as sensors to measure the execution of these commands. A machine that is not allowing the execution of such attacks is more likely to be a honeypot. Consequently, honeypot operators need to choose the optimal response that balances between being disclosed and being liable for participating in illicit actions. In this paper, we consider the optimal response strategy for honeypot operators. In particular, we model the interaction between botmasters and honeypots by a Markov Decision Process MDP and then determine the optimal policy for honeypots responding to the commands of botmasters. The model is then extended using a Partially Observable Markov Decision Process POMDP which allows operators of honeypots to model the uncertainty of the honeypot state as determined by botmasters. The analysis of our model confirms that exploiting the legal liability of honeypots allows botmasters to have the upper hand in their conflict with honeypots. Despite this deficiency in current honeypot designs, our model can help operators of honeypots determine the optimal strategy for responding to botmasters\u2019 commands. We also provide simulation results that show the honeypots\u2019 optimal response strategies and their expected rewards under different attack scenarios.\"",
        "1 is \"Design and Analysis of a Leader Election Algorithm for Mobile Ad Hoc Networks\", 2 is \"Collaborative Security: A Survey and Taxonomy\"",
        "Given above information, for an author who has written the paper with the title \"Framework for a NetFPGA-based Snort NIDS\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006755": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'The Decision View's Role in Software Architecture Practice':",
        "Document: \"Sustainable Architectural Design Decisions. Software architects must sustain design decisions to endure throughout software evolution. Several criteria can help them assess decisions' sustainability. In addition, industry and research projects have applied different techniques to make architectural design decisions sustainable; their examples offer solutions and lessons learned.\"",
        "Document: \"Processes for creating and exploiting architectural design decisions with tool support. Software architectures suffer of a serious lack of documented design decisions, but also an explicit definition of the processes needed to create and exploit such architectural knowledge. To address these issues, we focus on the specification of those activities that we believe should be implemented to support the creation and use of design rationale with tool support.\"",
        "Document: \"A layered coordination framework for optimizing resource allocation in adapting cloud-based applications. In this paper we propose a framework that adapts a cloud-based software application by providing an enhanced assembly of resources using the Pareto-optimal solution to optimize the resource allocation with tight cooperation between the cloud layers.\"",
        "Document: \"Dynamic Variability Meets Robotics. The complexity necessary for robotics software systems to dynamically respond to or handle a variety of evolving scenarios translates into serious development costs and engineering challenges. Dynamic variability is one new strategy for supporting the dynamic behavior of robotics control systems.\"",
        "Document: \"Current Research Topics and Trends in the Software Architecture Community: ICSA 2017 Workshops Summary. This summary reports the workshops accepted in the 1st International Conference on Software Architecture (ICSA 2017), held by Chalmers University at Gothenburg (Sweden). We gather the description of current and new research trends in different software architecture topics to provide a wide view to researchers and practitioners about the current status and trends in the field. ICSA is a premier software architecture conference that encompasses WICSA and COMPARCH conferences in one single event.\"",
        "1 is \"Design decision rationale: experiences and steps ahead towards systematic use\", 2 is \"Facets of Adaptivity\"",
        "Given above information, for an author who has written the paper with the title \"The Decision View's Role in Software Architecture Practice\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006817": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Metrics for evaluating video streaming quality in lossy IEEE 802.11 wireless networks':",
        "Document: \"Exploiting Multiple-Antenna Diversity for Shared Secret Key Generation in Wireless Networks. Generating a secret key between two parties by extracting the shared randomness in the wireless fading channel is an emerging area of research. Previous works focus mainly on single-antenna systems. Multiple-antenna devices have the potential to provide more randomness for key generation than single-antenna ones. However, the performance of key generation using multiple-antenna devices in a real environment remains unknown. Different from the previous theoretical work on multiple-antenna key generation, we propose and implement a shared secret key generation protocol, Multiple-Antenna KEy generator (MAKE) using off-the-shelf 802.11n multiple-antenna devices. We also conduct extensive experiments and analysis in real indoor and outdoor mobile environments. Using the shared randomness extracted from measured Received Signal Strength Indicator (RSSI) to generate keys, our experimental results show that using laptops with three antennas, MAKE can increase the bit generation rate by more than four times over single-antenna systems. Our experiments validate the effectiveness of using multi-level quantization when there is enough mutual information in the channel. Our results also show the trade-off between bit generation rate and bit agreement ratio when using multi-level quantization. We further find that even if an eavesdropper has multiple antennas, she cannot gain much more information about the legitimate channel.\"",
        "Document: \"Online learning for unreliable passive monitoring in multi-channel wireless networks. Passive network monitoring is important for the critical applications of network diagnosis and criminal investigation. As in multi-channel wireless networks, the sniffer-channel assignment problem faces a tradeoff between exploitation and exploration. In this paper, we investigate this problem in a practical scenario. Different from the existing literature, we assume that the knowledge of the users' activities is not known a priori, and there exists capture uncertainty due to unreliable monitoring conditions. Furthermore, we consider the case of sniffer redundancy deployment, which enables multiple sniffers to monitor one channel to enhance capture reliability. Our problem is then formulated as a combinatorial multi-arm bandit problem. We propose an online learning policy, in which sniffer-channel assignment is dynamically decided based on the learning results of the users' activities. We further develop a greedy algorithm to achieve the channel assignment decision in polynomial time. Our solution is evaluated by both theoretical analysis and numerical simulations. Simulation results show that our policy achieves logarithmic regret in time and outperforms the learning policy without consideration of sniffer redundancy deployment.\"",
        "Document: \"Prediction-time Efficient Classification Using Feature Computational Dependencies. As machine learning methods are utilized in more and more real-world applications involving constraints on computational budgets, the systematic integration of such constraints into the process of model selection and model optimization is required to an increasing extent. A specific computational resource in this regard is the time needed for evaluating predictions on test instances. There is meanwhile a substantial body of work concerned with the joint optimization of accuracy and test-time efficiency by considering the time costs of feature generation and model prediction. During the feature generation process, significant redundant computations across different features occur in many applications. Although the elimination of such redundancies would reduce the time cost substantially, there has been little research in this area due to substantial technical challenges involved, especially: 1) the lack of an effective formulation for feature computation dependency; and 2) the nonconvex and discrete nature of the optimization over feature computation dependency. In order to address these problems, this paper first proposes a heterogeneous hypergraph to represent the feature computation dependency, after which a framework is proposed that jointly optimizes the accuracy and the exact test-time cost based on a given feature computational dependency. A continuous tight approximation to this original problem is proposed based on a non-monotone nonconvex regularization term. Finally, an effective nonconvex optimization algorithm is proposed to solve the problem, along with a theoretical analysis of the convergence conditions. Extensive experiments on eight synthetic datasets and six real-world datasets demonstrate the proposed models' outstanding performance in terms of both accuracy and prediction-time cost.\n\n\"",
        "Document: \"Efficient data capturing for network forensics in cognitive radio networks. Network forensics is an emerging interdiscipline used to track down cyber crimes and detect network anomalies for a multitude of applications. Efficient capture of data is the basis of network forensics. Compared to traditional networks, data capture faces significant challenges in cognitive radio networks. In traditional wireless networks, usually one monitor is assigned to one channel for traffic capture. This approach will incur very high cost in cognitive radio networks because it typically has a large number of channels. Furthermore, due to the uncertainty of the primary user's behavior, cognitive radio devices change their operating channels dynamically, which makes data capturing more difficult. In this paper, we propose a systematic method to capture data in cognitive radio networks with a small number of monitors. We utilize incremental support vector regression to predict packet arrival time and intelligently switch monitors between channels. We also propose a protocol that schedules multiple monitors to perform channel scanning and packet capturing in an efficient manner. Monitors are reused in the time domain, and geographic coverage is taken into account. The real-world experiments and simulations show that our method is able to achieve the packet capture rate above 70% using a small number of monitors, which outperforms the random scheme by 200%-300%.\"",
        "Document: \"Physical layer multi-user key generation in wireless networks. Secret key generation by extracting the shared randomness in the wireless fading channel from physical layer is an interesting topic of practical value. Previous works have focused on the study of physical layer key generation with two nodes from the view point of key generation rate (KGR). Information theoretic limits and the KGRs in implementation have been derived. However, in real-world applications, the physical layer key generation problem involving multiple nodes is the common case, which lacks sufficient study so far. Multi-node case differs from two-node case in that there are two more important considerations: (1) the trade-off between KGR and probing efficiency at individual node pair; (2) channel probing schedule among multiple node pairs. This paper aims at minimizing the  (shorten as OWT) through the optimization of probing rates at individual node pair and channel probing schedule. The theoretical lower bound of OWT is derived first, then a practical method (MUKEM) is proposed to compute reasonable probing rates and channel probing schedule for multiple node pairs to obtain a short OWT. Simulations are conducted to evaluate the effectiveness of our method. The results show that 70\u00a0% of OWT can be reduced by using our method comparing with one-by-one key generations; while it is only about 8\u00a0% longer than the lower bound of OWT.\"",
        "1 is \"SPINS: security protocols for sensor networks\", 2 is \"Cicada: introducing predictive guarantees for cloud networks\"",
        "Given above information, for an author who has written the paper with the title \"Metrics for evaluating video streaming quality in lossy IEEE 802.11 wireless networks\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006877": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Model checking mobile stochastic logic':",
        "Document: \"Investigating Fluid-Flow Semantics Of Asynchronous Tuple-Based Process Languages For Collective Adaptive Systems. Recently, there has been growing interest in nature-inspired interaction paradigms for Collective Adaptive Systems, for modelling and implementation of adaptive and context-aware coordination, among which the promising pheromone-based interaction paradigm. System modelling in the context of such a paradigmmay be facilitated by the use of languages in which adaptive interaction is decoupled in time and space through asynchronous buffered communication, e.g. asynchronous, repository-or tuple-based languages. In this paper we propose a differential semantics for such languages. In particular, we consider an asynchronous, repository based modelling kernel-language which is a restricted version of LINDA, extended with stochastic information about action duration. We provide stochastic formal semantics for both an agent-based view and a population-based view. We then derive an ordinary differential equation semantics from the latter, which provides a fluid-flow deterministic approximation for the mean behaviour of large populations. We show the application of the language and the ODE analysis on a benchmark example of foraging ants.\"",
        "Document: \"A Process Algebra Approach To Fuzzy Reasoning. Fuzzy systems address the imprecision of the input and output variables, which formally describe notions like \"rather warm\" or \"pretty cold\", while provide a behaviour that depends on fuzzy data. This class of systems are classically represented by means of Fuzzy Inference Systems (FIS), a computing framework based on the concepts of fuzzy if-then rules and fuzzy reasoning. Even if FIS are largely used, these lack in compositionality. Moreover, the analysis of modeled behaviuors needs complex analytic tools. In this paper we propose a process algebraic approach to specification and analysis of fuzzy behaviours. Indeed, we introduce a Fuzzy variant of CCS (Calculus of Communicating Processes), that permits compositionally describing fuzzy behaviours. Moreover, we also show how standard process algebra formal tools, like modal logics and behavioural equivalences, can be used for supporting fuzzy reasoning.\"",
        "Document: \"Carma Eclipse Plug-In: A Tool Supporting Design And Analysis Of Collective Adaptive Systems. Collective Adaptive Systems (CAS) are heterogeneous populations of autonomous task-oriented agents that cooperate on common goals forming a collective system. This class of systems is typically composed of a huge number of interacting agents that dynamically adjust and combine their behaviour to achieve specific goals. Existing tools and languages are typically not able to describe the complex interactions that underpin such systems, which operate in a highly dynamic environment. For this reason, recently, new formalisms have been proposed to model CAS. One such is Carma, a process specification language that is equipped with linguistic constructs specifically developed for modelling and programming systems that can operate in open-ended and unpredictable environments. In this paper we present the Carma Eclipse plugin, a toolset integrated in Eclipse, developed to support the design and analysis of CAS.\"",
        "Document: \"A Companion of \"Relating Strong Behavioral Equivalences for Processes with Nondeterminism and Probabilities\".   In the paper \"Relating Strong Behavioral Equivalences for Processes with Nondeterminism and Probabilities\" to appear in TCS, we present a comparison of behavioral equivalences for nondeterministic and probabilistic processes. In particular, we consider strong trace, failure, testing, and bisimulation equivalences. For each of these groups of equivalences, we examine the discriminating power of three variants stemming from three approaches that differ for the way probabilities of events are compared when nondeterministic choices are resolved via deterministic schedulers. The established relationships are summarized in a so-called spectrum. However, the equivalences we consider in that paper are only a small subset of those considered in the original spectrum of equivalences for nondeterministic systems introduced by Rob van Glabbeek. In this companion paper we we enlarge the spectrum by considering variants of trace equivalences (completed-trace equivalences), additional decorated-trace equivalences (failure-trace, readiness, and ready-trace equivalences), and variants of bisimulation equivalences (kernels of simulation, completed-simulation, failure-simulation, and ready-simulation preorders). Moreover, we study how the spectrum changes when randomized schedulers are used instead of deterministic ones. \"",
        "Document: \"MarCaSPiS: a Markovian Extension of a Calculus for Services. Service Oriented Computing (SOC) is a design paradigm that has evolved from earlier paradigms including object-orientation and component-based software engineering. Important features of services are compositionality, context-independence, encapsulation and re-usability. To support the formal design and analysis of SOC applications recently a number of Service Oriented Calculi have been proposed. Most of them are based on process algebras enriched with primitives specific of service orientation such as operators for manipulating semi-structured data, mechanisms for describing safe client-service interactions, constructors for composing possibly unreliable services and techniques for services query and discovery. In this paper we show a versatile technique for the definition of Structural Operational Semantics of MarCaSPiS, a Markovian extension of one of such calculi, namely the Calculus of Sessions and Pipelines, CaSPiS. The semantics deals in an elegant way with a stochastic version of two-party synchronisation, typical of a service-oriented approach, and with the problem of transition multiplicity while preserving highly desirable mathematical properties such as associativity and commutativity of parallel composition. We also show how the proposed semantics can be naturally used for defining a bisimulation-based behavioural equivalence for MarCaSPiS terms that induces the same equalities as those obtained via Strong Markovian Equivalence.\"",
        "1 is \"VIS: A System for Verification and Synthesis\", 2 is \"A Testing Equivalence for Reactive Probabilistic Processes\"",
        "Given above information, for an author who has written the paper with the title \"Model checking mobile stochastic logic\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006882": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Survey on distributed mobility management schemes for Proxy mobile IPv6':",
        "Document: \"An Energy-Aware Trust Derivation Scheme With Game Theoretic Approach in Wireless Sensor Networks for IoT Applications. Trust evaluation plays an important role in securing wireless sensor networks (WSNs), which is one of the most popular network technologies for the Internet of Things (IoT). The efficiency of the trust evaluation process is largely governed by the trust derivation, as it dominates the overhead in the process, and performance of WSNs is particularly sensitive to overhead due to the limited bandwidth and power. This paper proposes an energy-aware trust derivation scheme using game theoretic approach, which manages overhead while maintaining adequate security of WSNs. A risk strategy model is first presented to stimulate WSN nodes' cooperation. Then, a game theoretic approach is applied to the trust derivation process to reduce the overhead of the process. We show with the help of simulations that our trust derivation scheme can achieve both intended security and high efficiency suitable for WSN-based IoT networks.\"",
        "Document: \"A hybrid localization and tracking system in camera sensor networks. AbstractPosition information is of vital importance in the various applications in energy-constrained wireless sensor networks. It has to design a localization mechanism considering both precision and energy consumption factors. In this paper, we propose a hybrid localization system in wireless sensor networks, which is composed of coarse-grained localization system and fine-grained localization system. The coarse-grained localization system takes the wireless signal strength as the reference for distance and gets the rough region as the unknown node. The fine-grained localization system is in charge of location refinement that takes image to localize the unknown node with camera sensor nodes. On the basis of the hybrid localization system, we furthermore design a hybrid tracking system for localizing a moving object. Finally, we build up a test bed to conduct experiments with our developed sensor network. The experiment results show that the proposed hybrid localization and tracking system can achieve high position precision and low energy consumption. Copyright \u00a9 2012 John Wiley & Sons, Ltd.\"",
        "Document: \"Management of Private Data: Addressing User Privacy and Economic, Social, and Ethical Concerns. Coordinated Web services can help alleviate user's privacy and economic, social, and ethical concerns that arise from third parties' access and use of user private data. This paper focuses on the requirements and design of such services in support of a client-side private data management system. Appropriate management of private data on the client side can both educate and assure users that their privacy is well guarded, and that their private data is being used by entities which satisfy economic and/or ethical user concerns. Our solutions describe novel Web services, interaction with P3P agents, and a client-side privacy architecture. A preliminary prototype implementation of our Web services using standard UDDI, SOAP, and WSDL technologies and rudimentary delay estimates are briefly discussed.\"",
        "Document: \"Admission control in IEEE 802.11e wireless LANs. Although IEEE 802.11 based wireless local area networks have become more and more popular due to low cost and easy deployment, they can only provide best effort services and do not have quality of service supports for multimedia applications. Recently, a new standard, IEEE 802.11e, has been proposed, which introduces a so-called hybrid coordination function containing two medium access mechanisms: contention-based channel access and controlled channel access. In this article we first give a brief tutorial on the various MAC-layer QoS mechanisms provided by 802.11e. We show that the 802.11e standard provides a very powerful platform for QoS supports in WLANs. Then we provide an extensive survey of recent advances in admission control algorithms/protocols in IEEE 802.11e WLANs. Our survey covers the research work in admission control for both EDCA and HCCA. We show that the new MAC-layer QoS schemes and parameters provided in EDCA and HCCA can be well utilized to fulfill the requirements of admission control so that QoS for multimedia applications can be provided in WLANs. Last, we give a summary of the design of admission control in EDCA and HCCA, and point out the remaining challenges.\"",
        "Document: \"A multilevel information fusion approach for road congestion detection in VANETs. As city road congestion problems become more serious, many researchers have started to use the technique of vehicle ad hoc networks (VANETs) for road congestion detection. However, various on-board sensors equipped in vehicles may generate lots of atomic messages, which usually cause serious channel competition problems. In this paper, we propose a multilevel information fusion approach by combining the fuzzy clustering-based feature level information fusion (FCMA) and the modified Dempster\u2013Shafer evidence reasoning-based decision level information fusion (D-SEMA). The FCMA can extract the key features from atomic messages, thereby greatly reducing the network traffic load. Furthermore, the D-SEMA mechanism is used to judge whether the road congestion event occurs. Performance analysis and simulation results under ONE simulator show that the proposed multilevel information fusion approach can detect road congestion efficiently with low bandwidth consumption.\"",
        "1 is \"Distributed Control Algorithms For Service Differentiation In Wireless Packet Networks\", 2 is \"Maintaining Strong Cache Consistency for the Domain Name System\"",
        "Given above information, for an author who has written the paper with the title \"Survey on distributed mobility management schemes for Proxy mobile IPv6\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006918": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'LILAC - Learn from Internet: Log, Annotation, and Content.':",
        "Document: \"Monitoring Patients with Mental Disorders. Mental disorders impose significant socio-economic and geo-political challenges which, if not addressed, have the potential to overwhelm the ability of healthcare systems globally to accommodate the growing demands both in human and resource management terms. From a treatment perspective there is a need to implement multi-modal systems where patients can be diagnosed, treated, and monitored. Such systems must incorporate triage and treatment capabilities in both hospital settings with monitoring in the community. This paper considers the practical challenges in realising the goal of achieving the effective monitoring of patients with mental disorders in Smart-Psychiatric Intensive Care Units and in the community. Illustrative scenarios are presented. We conclude that effective patient monitoring will provide benefits for all stakeholders in the management of mental disorders.\"",
        "Document: \"LILAC - Learn from Internet: Log, Annotation, and Content. This paper summarizes an user study designed to evaluate various models of how users browse the web while working on their day-to-day tasks, in their office or at home. We use these models to predict which pages contain information the user will find useful, and provide empirical data that these learned models are effective. \u00a9 2009, Association for the Advancement of Artificial Intelligence.\"",
        "Document: \"Ontology-based context modeling for emotion recognition in an intelligent web. We describe an ontological model for representation and integration of electroencephalographic (EEG) data and apply it to detect human emotional states. The model (BIO_EMOTION) is an ontology-based context model for emotion recognition and acts as a basis for: (1) the modeling of users' contexts, including user profiles, EEG data, the situation and environment factors, and (2) supporting reasoning on the users' emotional states. Because certain ontological concepts in the EEG domain are ill-defined, we formally represent and store these concepts, their taxonomies and high-level representation (i.e., rules) in the model. To evaluate the effectiveness for inferring emotional states, DEAP dataset is used for model reasoning. Result shows that our model reaches an average recognition ratio of 75.19 % on Valence and 81.74 % on Arousal for eight participants. As mentioned above, the BIO-EMOTION model acts like a bridge between users' emotional states and low-level bio-signal features. It can be integrated in user modeling techniques, and be used to model web users' emotional states in human-centric web aiming to provide active, transparent, safe and reliable services to users. This work aims at, in other words, creating an ontology-based context model for emotion recognition using EEG. Particularly, this model completely implements the loop body of the W2T data cycle once: from low-level EEG feature acquisition to emotion recognition. A long-term goal for the study is to complete this model to implement the whole W2T data cycle.\"",
        "Document: \"A PET study of discrimination of cerebral glucose metabolism in Alzheimer's disease and mild cognitive impairment. Imaging cerebral glucose metabolism with positron emission tomography (PET) has been widely used in studying Alzheimer's disease (AD) and mild cognitive impairment (MCI). In this study, we used fluoro-deoxyglucose (FDG) PET images to investigate reduced glucose metabolism in 90 AD subjects, 90 MCI subjects and 90 healthy elderly normal controls (NC). Compared to NC, the AD showed a significant hypometabolism in left and right middle temporal, left cingulate gyrus, medial frontal gyrus and left parahippocampal gyrus. Compared to NC, the MCI showed a significant hypometabolism in the right inferior temporal gyrus and right fusiform gyrus. Compared to MCIs, the AD also showed a significant hypometabolism in left and right middle temporal, left cingulate gyrus, left angular gyrus and right parahippocampal gyrus. This study demonstrates the different cerebral metabolic patterns of AD, MCI and controls. It also shows that glucose metabolism is a sensitive measure of change in cognition and functional ability in AD and MCI, and that might be valuable in predicting future cognitive decline.\"",
        "Document: \"Nearest Neighbor Method Based on Local Distribution for Classification. The k-nearest-neighbor (kNN) algorithm is a simple but effective classification method which predicts the class label of a query sample based on information contained in its neighborhood. Previous versions of kNN usually consider the k nearest neighbors separately by the quantity or distance information. However, the quantity and the isolated distance information may be insufficient for effective classification decision. This paper investigates the kNN method from a perspective of local distribution based on which we propose an improved implementation of kNN. The proposed method performs the classification task by assigning the query sample to the class with the maximum posterior probability which is estimated from the local distribution based on the Bayesian rule. Experiments have been conducted using 15 benchmark datasets and the reported experimental results demonstrate excellent performance and robustness for the proposed method when compared to other state-of-the-art classifiers.\"",
        "1 is \"Randomized vs.Deterministic Decision Tree Complexity for Read-Once Boolean Functions.\", 2 is \"USMART: An Unsupervised Semantic Mining Activity Recognition Technique\"",
        "Given above information, for an author who has written the paper with the title \"LILAC - Learn from Internet: Log, Annotation, and Content.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006917": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Double Window Cancellation and Combining for OFDM in Time-Invariant Large Delay Spread Channel':",
        "Document: \"Subcarrier Mapping For Single-User Sc-Fdma Relay Communications. A combination of single-carrier frequency-division multiple-access (SC-FDMA) and relay transmission is effective for performance improvement in uplink transmission. In SC-FDMA, a mapping strategy of user's spectrum has an enormous impact on system performance. In the relay communication, the optimum mapping strategy may differentiate from that in direct communication because of the independently distributed channels among nodes. In this letter, how each link should be considered in subcarrier mapping is studied and the impact of mapping strategies on the average bit error rate (BER) performance of single-user SC-FDMA relay communications will be given.\"",
        "Document: \"Mimo Receiver With Antenna Subset Selection: Algorithm And Hardware Implementation. Multiple-input multiple-output (MIMO) systems with antenna selection are practical in that they can alleviate the computational complexity at the receiver and achieve good reception performance. Channel correlation, not just carrier-to-noise ratio (CNR), has a great impact on reception performance in MIMO channels. We propose a practical receive antenna subset selection algorithm with reduced complexity that uses the condition number of the partial channel matrix and a predetermined CNR threshold. This paper describes the algorithm and its performance evaluation by both computer simulation and indoor experiments using a prototype receiver and received signals obtained in an actual mobile outdoor experiment. The results confirm that our proposed method provides good bit error rate performance by setting the CNR threshold properly.\"",
        "Document: \"Method Of Reducing Search Area For Localization In Sensor Networks. One typical use of sensor networks is monitoring targets. The sensor networks classify, detect, locate, and track targets. The ML (Maximum likelihood) algorithm is one of the estimation algorithms of target location and has high accuracy to estimate target location. However, the calculation amount of the ML estimation algorithm is large. Energy-Ratios Source Localization Nonlinear Least Square (ER-NLS) is proposed to realize the ML algorithm. ER-NLS is the algorithm of estimating source location by using the ratio of sensors, receiving energies. However, ER-NLS has to search all the areas, so that the calculation amount of ER-NLS is large. In this paper we propose a method of reducing search area for localization. The proposed method uses the ratio of sensors, receiving energies. It can be used with the ML algorithm. We show that the proposed method with the ML algorithm can reduce the search areas to estimate the target location and thus reduce the complexity, while achieving the RMSE (root mean square error) close to that of the ML algorithm.\"",
        "Document: \"Development of MMSE Macro-Diversity Receiver with Delay Difference Correction Technique. We have been developing a minimum mean square error macro-diversity (MMSE-MD) reception system using distributed remote antennas and radio-on-fiber (RoF) links for use in live broadcasts of road races. For the system to be feasible, we have to consider the propagation delay differences among diversity branches due to RoF links and radio propagation. This paper describes a hardware implementation of an MMSE-MD receiver embodying our delay difference correction (DDC) technique and indoor and outdoor performance evaluations. Our prototype receiver perfectly corrected the propagation delay difference among diversity branches in less than 11 symbols duration in a time-varying multipath fading environment, and it was capable of the MD reception without outage on an actual road race course.\"",
        "Document: \"Human Activity Classification And Localization Algorithm Based On Temporal-Spatial Virtual Array. We propose a human activity classification and localization algorithm based on temporal-spatial virtual array. By analyzing the synthetic wave of the echo reflected from each human body part, we can classify various human activities. In addition by using the estimation result of target direction with temporal-spatial phase shift that depends on relative target velocity and position, the proposed algorithm can estimate the position of a human being simultaneously with a smaller amount of computational complexity than the conventional least-square approach. We demonstrate the effectiveness of the proposed algorithm via simulation results.\"",
        "1 is \"Opportunistic transmission scheduling with resource-sharing constraints in wireless networks\", 2 is \"Experimental evaluation of throughput performance in broadband packet wireless access based on VSF-OFCDM and VSF-CDMA.\"",
        "Given above information, for an author who has written the paper with the title \"Double Window Cancellation and Combining for OFDM in Time-Invariant Large Delay Spread Channel\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006928": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'The Need for Open Source Software in Machine Learning':",
        "Document: \"Batch-incremental versus instance-incremental learning in dynamic and evolving data. Many real world problems involve the challenging context of data streams, where classifiers must be incremental: able to learn from a theoretically-infinite stream of examples using limited time and memory, while being able to predict at any point. Two approaches dominate the literature: batch-incremental methods that gather examples in batches to train models; and instance-incremental methods that learn from each example as it arrives. Typically, papers in the literature choose one of these approaches, but provide insufficient evidence or references to justify their choice. We provide a first in-depth analysis comparing both approaches, including how they adapt to concept drift, and an extensive empirical study to compare several different versions of each approach. Our results reveal the respective advantages and disadvantages of the methods, which we discuss in detail.\"",
        "Document: \"Scalable and efficient multi-label classification for evolving data streams. Many challenging real world problems involve multi-label data streams. Efficient methods exist for multi-label classification in non-streaming scenarios. However, learning in evolving streaming scenarios is more challenging, as classifiers must be able to deal with huge numbers of examples and to adapt to change using limited time and memory while being ready to predict at any point.This paper proposes a new experimental framework for learning and evaluating on multi-label data streams, and uses it to study the performance of various methods. From this study, we develop a multi-label Hoeffding tree with multi-label classifiers at the leaves. We show empirically that this method is well suited to this challenging task. Using our new framework, which allows us to generate realistic multi-label data streams with concept drift (as well as real data), we compare with a selection of baseline methods, as well as new learning methods from the literature, and show that our Hoeffding tree method achieves fast and more accurate performance.\"",
        "Document: \"Stream data mining using the MOA framework. Massive Online Analysis (MOA) is a software framework that provides algorithms and evaluation methods for mining tasks on evolving data streams. In addition to supervised and unsupervised learning, MOA has recently been extended to support multi-label classification and graph mining. In this demonstrator we describe the main features of MOA and present the newly added methods for outlier detection on streaming data. Algorithms can be compared to established baseline methods such as LOF and ABOD using standard ranking measures including Spearman rank coefficient and the AUC measure. MOA is an open source project and videos as well as tutorials are publicly available on the MOA homepage.\"",
        "Document: \"Efficient data stream classification via probabilistic adaptive windows. In the context of a data stream, a classifier must be able to learn from a theoretically-infinite stream of examples using limited time and memory, while being able to predict at any point. Many methods deal with this problem by basing their model on a window of examples. We introduce a probabilistic adaptive window (PAW) for data-stream learning, which improves this windowing technique with a mechanism to include older examples as well as the most recent ones, thus maintaining information on past concept drifts while being able to adapt quickly to new ones. We exemplify PAW with lazy learning methods in two variations: one to handle concept drift explicitly, and the other to add classifier diversity using an ensemble. Along with the standard measures of accuracy and time and memory use, we compare classifiers against state-of-the-art classifiers from the data-stream literature.\"",
        "Document: \"Optimizing the Induction of Alternating Decision Trees.  The alternating decision tree brings comprehensibility to theperformance enhancing capabilities of boosting. A single interpretabletree is induced wherein knowledge is distributed across the nodes andmultiple paths are traversed to form predictions. The complexity of thealgorithm is quadratic in the number of boosting iterations and thismakes it unsuitable for larger knowledge discovery in database tasks. Inthis paper we explore various heuristic methods for reducing this... \"",
        "1 is \"StreamDM: Advanced Data Mining in Spark Streaming.\", 2 is \"Feedback channel in pixel domain Wyner-Ziv video coding: Myths and realities\"",
        "Given above information, for an author who has written the paper with the title \"The Need for Open Source Software in Machine Learning\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006947": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Two-timescale simultaneous perturbation stochastic approximation using deterministic perturbation sequences':",
        "Document: \"Applying model reference adaptive search to American-style option pricing. This paper considers the application of stochastic optimization methods to American-style option pricing. We apply a randomized optimization algorithm called Model Reference Adaptive Search (MRAS) to pricing American-style options by parameterizing the early exercise boundary. Numerical results are provided for pricing American-style call and put options written on underlying assets following geometric Brownian motion and Merton jump-diffusion processes. The results from the MRAS algorithm are also compared with the Cross-Entropy (CE) method.\"",
        "Document: \"Estimating customer service in a two-location continuous review inventory model with emergency transshipments. In this paper, an approximate analytical two-location inventory transshipment model is developed that combines the popular order-quantity, reorder-point (Q,R) continuous review ordering policy with a third parameter, the hold-back amount, which limits the level of outgoing transshipments. The degree to which transshipments improve both Type I (no-stockout probability) and Type II (fill rate) customer service levels can be calculated using the model. Simulation studies conducted to test the validity of the approximations in the analytical model indicate that it performs very well over a wide range of inputs.\"",
        "Document: \"A dynamic framework for statistical selection problems. For the statistical selection problem we formulate a general framework comprising both sequential sampling allocation and optimal design selection, for which the traditional probability of correct selection measure is inadequate. Therefore, we introduce the integrated probability of correct selection to better characterize the objective. In this framework, the usual selection policy of choosing the design with the largest sample mean as the estimate of the best is no longer optimal. Rather, the optimal selection policy is to choose the design that maximizes the posterior integrated probability of correct selection, which is a function of both the posterior mean and the correlation structure induced by the posterior variance. \u00a9 2013 IEEE.\"",
        "Document: \"A New Hybrid Stochastic Approximation Algorithm. We introduce Secant-Tangents AveRaged (STAR) Stochastic Approximation (SA), a new SA algorithm that estimates the gradient using a hybrid estimator, which is a convex combination of a symmetric finite difference and an average of two direct gradient estimators. For the deterministic weight sequence that minimizes the variance of the STAR gradient, we prove that for quadratic functions, the mean squared error (MSE) of the STAR-SA algorithm using this weight sequence is strictly less than that of the classical SA methods of Robbins-Monro (RM) and Kiefer-Wolfowitz (KW). We also prove convergence of the STAR-SA algorithm for general concave functions. Furthermore, we illustrate its effectiveness through numerical experiments by comparing the MSE of the STAR-SA algorithm against RM and KW for simple quadratic functions with various steepness and noise levels.\"",
        "Document: \"Financial derivatives and real options: hedging beyond duration and convexity. Hedging of fixed income securities remains one of the most challenging problems faced by financial institutions. The predominantly used measures of duration and convexity do not completely capture the interest rate risks borne by the holder of these securities. Using historical data for the entire yield curve, we perform a principal components analysis and find that the first four factors capture over 99.99% of the yield curve variation. Incorporating these factors into the pricing of arbitrary fixed income securities via Monte Carlo simulation, we derive perturbation analysis (PA) estimators for the price sensitivities with respect to the factors. Computational results for mortgage-backed securities (MBS) indicate that using these sensitivity measures in hedging provides far more protection against interest risk exposure than the conventional measures of duration and convexity.\"",
        "1 is \"PAC Bounds for Multi-armed Bandit and Markov Decision Processes\", 2 is \"A Natural Policy Gradient\"",
        "Given above information, for an author who has written the paper with the title \"Two-timescale simultaneous perturbation stochastic approximation using deterministic perturbation sequences\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006962": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'GRAPHITE: A Visual Query System for Large Graphs':",
        "Document: \"A hybrid phish detection approach by identity discovery and keywords retrieval. Phishing is a significant security threat to the Internet, which causes tremendous economic loss every year. In this paper, we proposed a novel hybrid phish detection method based on information extraction (IE) and information retrieval (IR) techniques. The identity-based component of our method detects phishing webpages by directly discovering the inconsistency between their identity and the identity they are imitating. The keywords-retrieval component utilizes IR algorithms exploiting the power of search engines to identify phish. Our method requires no training data, no prior knowledge of phishing signatures and specific implementations, and thus is able to adapt quickly to constantly appearing new phishing patterns. Comprehensive experiments over a diverse spectrum of data sources with 11449 pages show that both components have a low false positive rate and the stacked approach achieves a true positive rate of 90.06% with a false positive rate of 1.95%.\"",
        "Document: \"A Delay-Dependent Approach to Passivity Analysis for Uncertain Neural Networks with Time-varying Delay. This paper deals with the problem of passivity analysis for neural networks with time-varying delay, which is subject to norm-bounded time-varying parameter uncertainties. The activation functions are supposed to be bounded and globally Lipschitz continuous. Delay-dependent passivity condition is proposed by using the free-weighting matrix approach. These passivity conditions are obtained in terms of linear matrix inequalities, which can be investigated easily by using standard algorithms. Two illustrative examples are provided to demonstrate the effectiveness of the proposed criteria.\"",
        "Document: \"You're getting warmer!: how proximity information affects search behavior in physical spaces. This paper describes the results of a Wizard of Oz study of people's search behavior using BuddySystem, a proximity-sensing system designed to help end-users locate people, places, and things. BuddySystem uses distance estimation based on signal strength alone, since direction is difficult to obtain in ad-hoc radio-based systems. Overall findings indicate that the BuddySystem changed people's search behavior to reduce walking area, but may increase search times if the system demands too much of the user's attention, suggesting that reducing distractions and adjusting search strategies could improve search effectiveness of proximity-based tracking systems in physical spaces.\"",
        "Document: \"Who's viewed you?: the impact of feedback in a mobile location-sharing application. Feedback is viewed as an essential element of ubiquitous computing systems in the HCI literature for helping people manage their privacy. However, the success of online social networks and existing commercial systems for mobile location sharing which do not incorporate feedback would seem to call the importance of feedback into question. We investigated this issue in the context of a mobile location sharing system. Specifically, we report on the findings of a field de-ployment of Locyoution, a mobile location sharing system. In our study of 56 users, one group was given feedback in the form of a history of location requests, and a second group was given no feedback at all. Our major contribution has been to show that feedback is an important contributing factor towards improving user comfort levels and allaying privacy concerns. Participants' privacy concerns were reduced after using the mobile location sharing system. Additionally,our study suggests that peer opinion and technical savviness contribute most to whether or not participants thought they would continue to use a mobile location technology.\"",
        "Document: \"Smartphones, Teddy Bears, and Toys. This installment of Notes from the Community is all about technology for locating, sensing, and interacting. Oh, and teddy bears.\"",
        "1 is \"Operators for propagating trust and their evaluation in social networks\", 2 is \"When a city tells a story: urban topic analysis\"",
        "Given above information, for an author who has written the paper with the title \"GRAPHITE: A Visual Query System for Large Graphs\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006978": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Utility-Based Intelligent Network Selection In Beyond 3g Systems':",
        "Document: \"A statistical analysis of IP packet delay and jitter in cellular networks. A novel methodology is proposed for the analysis of the IP packet delay performance of SR-ARQ mechanisms in a generalized wireless system. A simulation model of the system including a novel channel model is described and results are obtained for a range of IP packet size. To demonstrated the efficacy of the methodology, the test scenario is tailored to the transmission of small packets containing real-time data carried over an EGPRS system. The results show that the use of a mean value for IP packet delay estimation is of limited use when small packets are considered.\"",
        "Document: \"Performance engineering for cloud computing. Cloud computing potentially solves some of the major challenges in the engineering of large software systems. With the promise of infinite capacity coupled with the ability to scale at the same speed as the traffic changes, it may appear that performance engineering will become redundant. Organizations might believe that there is no need to plan for the future, to optimize applications, or to worry about efficient operation. This paper argues that cloud computing is an area where performance engineering must be applied and customized. It will not be possible to \"cloud wash\" performance engineering by just applying previous methods. Rather it is essential to both understand the differences between the cloud and previous systems, and the applicability of proposed performance engineering methods.\"",
        "Document: \"Automated WAIT for Cloud-Based Application Testing. Cloud computing is causing a paradigm shift in the provision and use of software. This has changed the way of obtaining, managing and delivering computing services and solutions. Similarly, it has brought new challenges to software testing. A particular area of concern is the performance of cloud-based applications. This is because the increased complexity of the applications has exposed new areas of potential failure points, complicating all performance-related activities. This situation makes the performance testing of cloud environments very challenging. Similarly, the identification of performance issues and the diagnosis of their root causes are time-consuming and complex, usually require multiple tools and heavily rely on expertise. To simplify these tasks, hence increasing the productivity and reducing the dependency on human experts, this paper presents a lightweight approach to automate the usage of expert tools in the performance testing of cloud-based applications. In this paper, we use a tool named Whole-system Analysis of Idle Time to demonstrate how our research work solves this problem. The validation involved two experiments, which assessed the overhead of the approach and the time savings that it can bring to the analysis of performance issues. The results proved the benefits of the approach by achieving a significant decrease in the time invested in performance analysis while introducing a low overhead in the tested system.\"",
        "Document: \"ABI: A mechanism for increasing video delivery quality in multi-radio Wireless Mesh Networks. Wireless Mesh Networks (WMNs) are becoming increasingly popular mostly due to their ease of deployment. One of the main drawbacks of these networks is that they suffer with respect to Quality of Service (QoS) provisioning to their clients. Equipping wireless mesh nodes with multiple radios in order to increase the available network bandwidth has become a common practice nowadays due to the low cost of the wireless chipsets. As the available bandwidth increases with each radio deployed on the mesh node, the energy consumed for transmission increases accordingly. Thus, efficient usage of the radio interfaces is a key aspect for keeping the energy consumption at low levels while offering high QoS level for video deliveries to the mesh network's clients. In WMN context, this paper proposes the Available Bandwidth Increase (ABI), a mesh node-based mechanism for efficient usage of the available bandwidth, which manages the wireless radio interfaces by activating them only when needed such as the energy consumption is maintained low. The proposed ABI is thoroughly evaluated and it is shown that it can provide video deliverie at good QoS level and at low energy consumption.\"",
        "Document: \"An Energy-Efficient Mechanism For Increasing Video Quality Of Service In Wireless Mesh Networks. The continuous growth in user demand for highquality rich media services puts pressure on Wireless Mesh Network (WMN) resources. Solutions such as those which increase the capacity of the mesh network by equipping mesh routers with additional wireless interfaces provide better Quality of Service (QoS) for video deliveries, but result in higher overall energy consumption for the network.This paper presents LBIS, a distributed solution which combines the benefits of both load-balancing and interface-shifting in order to enhance QoS levels for video services delivered over multi-hop WMNs, while maintaining low energy consumption levels within the network. Simulation-based results show very good performance of our proposed mechanism in terms of QoS metrics (delay, packet loss), Peak Signal-to-Noise Ratio (PSNR) and energy consumption in mesh network topologies, and with varying video traffic loads and distributions. The results demonstrate how LBIS can increase the QoS for video deliveries by more than 30% at the cost of an insignificant increase of the overall network energy consumption compared to the WMN with multiple radio interfaces without the LBIS adaptation.\"",
        "1 is \"Resource Sharing Optimization for Device-to-Device Communication Underlaying Cellular Networks\", 2 is \"A-STAR: A Mobile Ad Hoc Routing Strategy for Metropolis Vehicular Communications\"",
        "Given above information, for an author who has written the paper with the title \"Utility-Based Intelligent Network Selection In Beyond 3g Systems\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007026": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Using Bagging and Cross-Validation to improve ensembles based on penalty terms':",
        "Document: \"Adaptive boosting: dividing the learning set to increase the diversity and performance of the ensemble. As shown in the bibliography, Boosting methods are widely used to build ensembles of neural networks. This kind of methods increases the performance with respect to a single network. Since Freund and Schapire introduced Adaptive Boosting in 1996 some authors have studied and improved Adaboost. In this paper we present Cross Validated Boosting a method based on Adaboost and Cross Validation. We have applied Cross Validation to the learning set in order to get an specific training set and validation set for each network. With this procedure the diversity increases because each network uses an specific validation set to finish its learning. Finally, we have performed a comparison among Adaboost and Crossboost on eight databases from UCI, the results show that Crossboost is the best performing method.\"",
        "Document: \"Gradient descent and radial basis functions. In this paper, we present experiments comparing different training algorithms for Radial Basis Functions (RBF) neural networks. In particular we compare the classical training which consists of an unsupervised training of centers followed by a supervised training of the weights at the output, with the full supervised training by gradient descent proposed recently in same papers. We conclude that a fully supervised training performs generally better. We also compare Batch training with Online training and we conclude that Online training suppose a reduction in the number of iterations.\"",
        "Document: \"Ensembles of RBFs trained by gradient descent. Building an ensemble of classifiers is an useful way to improve the performance. In the case of neural networks the bibliography has centered on the use of Multilayer Feedforward (MF). However, there are other interesting networks like Radial Basis Functions (RBF) that can be used as elements of the ensemble. Furthermore, as pointed out recently the network RBF can also be trained by gradient descent, so all the methods of constructing the ensemble designed for MF are also applicable to RBF. In this paper we present the results of using eleven methods to construct a ensemble of RBF networks. The results show that the best method is in general the Simple Ensemble.\"",
        "Document: \"Combination methods for ensembles of RBFs. Building an ensemble of classifiers is a useful way to improve the performance. In the case of neural networks the bibliography has centered on the use of Multilayer Feedforward (MF). However, there are other interesting networks like Radial Basis Functions (RBF) that can be used as elements of the ensemble. In a previous paper we presented results of different methods to build the ensemble of RBF. The results showed that the best method is in general the Simple Ensemble. The combination method used in that research was averaging. In this paper we present results of fourteen different combination methods for a simple ensemble of RBF. The best methods are Borda Count, Weighted Average and Majority Voting.\"",
        "Document: \"First Experiments on Ensembles of Radial Basis Functions. Building an ensemble of classifiers is an useful way to improve the performance with respect to a single classifier. In the case of neural networks the bibliography has centered on the use of Multilayer Feedforward. However, there are other interesting networks like Radial Basis Functions (RBF) that can be used as elements of the ensemble. Furthermore, as pointed out recently the network RBF can also be trained by gradient descent, so all the methods of constructing the ensemble designed for Multilayer Feedforward are also applicable to RBF. In this paper we present the results of using eleven methods to construct an ensemble of RBF networks. We have trained ensembles of a reduced number of networks (3 and 9) to keep the computational cost low. The results show that the best method is in general the Simple Ensemble.\"",
        "1 is \"Error minimization in approximate range aggregates\", 2 is \"A neural-network learning theory and a polynomial time RBF algorithm\"",
        "Given above information, for an author who has written the paper with the title \"Using Bagging and Cross-Validation to improve ensembles based on penalty terms\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007060": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Drawing attention to the dangerous':",
        "Document: \"Voting Advice Applications: Missing Value Estimation Using Matrix Factorization And Collaborative Filtering. A Voting Advice Application (VAA) is a web application that recommends to a voter the party or the candidate, who replied like him/her in an online questionnaire. Every question is responding to the political positions of each party. If the voter fails to answer some questions, it is likely the VAA to offer him/her the wrong candidate. Therefore, it is necessary to inspect the missing data (not answered questions) and try to estimate them. In this paper we formulate the VAA missing value problem and investigate several different approaches of collaborative filtering to tackle it. The evaluation of the proposed approaches was done by using the data obtained from the Cypriot presidential elections of February 2013 and the parliamentary elections in Greece in May, 2012. The corresponding datasets are made freely available to other researchers working in the areas of VAA and recommender systems through the Web.\"",
        "Document: \"Exploring the Time Course of Facial Expressions with a Fuzzy System. Recognizing facial expressions is one of the important challenges of current research in Human-Computer Interaction (HCI). Previous research show the limits of recognition based on a single static image, and analyzing video sequences seems more promising. We explore here three fuzzy systems for the classification of basic facial expressions and compare their performances with a template-correlation approach. We then use those to examine the time course dynamics of facial expressions. The system's inputs are the relative variations of distances defined by salient facial points from one frame to the next. For maximum compatibility, those facial points (eyebrows, eyes, mouth) were chosen from the set of points defined in the standard MPEG-4 specifications, and so that their automatic extraction is tractable. The first results suggest that some expressions can be recognized early after the onset. For other expressions, it is in general possible to reduce significantly the number of possibilities. Forming early hypotheses regarding the expression could be necessary for a system to work in real-time, since other steps may have to follow: prediction of user's action, choice of computer's action, etc... This also has implications for the recognition of milder expressions.\"",
        "Document: \"Automatic annotation of image databases based on implicit crowdsourcing, visual concept modeling and evolution. In this paper a novel approach for automatically annotating image databases is proposed. Despite most current schemes that are just based on spatial content analysis, the proposed method properly combines several innovative modules for semantically annotating images. In particular it includes: (a) a GWAP-oriented interface for optimized collection of implicit crowdsourcing data, (b) a new unsupervised visual concept modeling algorithm for content description and (c) a hierarchical visual content display method for easy data navigation, based on graph partitioning. The proposed scheme can be easily adopted by any multimedia search engine, providing an intelligent way to even annotate completely non-annotated content or correct wrongly annotated images. The proposed approach currently provides very interesting results in limited-size both standard and generic datasets and it is expected to add significant value especially to billions of non-annotated images existing in the Web. Furthermore expert annotators can gain important knowledge relevant to user new trends, language idioms and styles of searching.\"",
        "Document: \"Semantic Gap between People: An Experimental Investigation Based on Image Annotation. Image annotation still remains the method of preference in multimedia search despite the development of many content-based multimedia retrieval platforms. Manual annotation is an extremely labour-intensive and time consuming task while the annotation expresses the view of a particular annotator at a specific context and time. Although the semantic gap has attracted large amount of research interest, the age and gender gaps in manual annotation have not been examined in detail. The aim of this study was to explore the gender and age differences in (1) the way of annotating images and, (2) the inter-annotator agreement. Our questionnaire based survey was conducted using 40 Cypriot citizens divided into two age groups who were asked to annotate an image dataset using a vocabulary of 52 keywords. Our results indicate that there are age differences in the way people annotate images, while the gender differences are smaller than our assumptions. Furthermore, there is an adequate agreement among participants for both age and gender groups.\"",
        "Document: \"Combining GAs and RBF neural networks for fuzzy rule extraction from numerical data. The idea of using RBF neural networks for fuzzy rule extraction from numerical data is not new. The structure of this kind of architectures, which supports clustering of data samples, is favorable for considering clusters as ifthen rules. However, in order for real if-then rules to be derived, proper antecedent parts for each cluster need to be constructed by selecting the appropriate subspace of input space that best matches each cluster's properties. In this paper we address the problem of antecedent part construction by (a) initializing the hidden layer of an RBF-Resource Allocating Network using an unsupervised clustering technique whose metric is based on input dimensions that best relate the data samples in a cluster, and (b) by pruning input connections to hidden nodes in a per node basis, using an innovative Genetic Algorithm optimization scheme.\"",
        "1 is \"Recommending twitter users to follow using content and collaborative filtering approaches\", 2 is \"Origins of a repetitive and co-contractive biphasic pattern of muscle activation in Parkinson's disease.\"",
        "Given above information, for an author who has written the paper with the title \"Drawing attention to the dangerous\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007108": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Multi-Resolution Multicasting Over the Grassmann and Stiefel Manifolds.':",
        "Document: \"Multiuser hybrid switched-selection diversity systems. A new multiuser scheduling scheme is proposed and analyzed in this paper. The proposed system combines features of conventional full-feedback selection-based diversity systems and reduced-feedback switch-based diversity systems. The new hybrid system provides flexibility in trading-off the channel information feedback overhead with the prospected multiuser diversity gains. The users are clustered into groups, and the users' groups are ordered into a sequence. Per-group feedback thresholds are used and optimized to maximize the system overall achievable rate. The proposed hybrid system applies switched diversity criterion to choose one of the groups, and a selection criterion to decide the user to be scheduled from the chosen group. Numerical results demonstrate that the system capacity increases as the number of users per group increases, but at the cost of more required feedback messages.\"",
        "Document: \"Performance Analysis of Dual-Hop AF Systems in Nakagami-m Fading Channels in the Presence of Interference. In this paper, we investigate the performance of a dual-hop amplify-and-forward relay system in Nakagami-emph{m} fading channels in the presence of multiple interferers. Based on the new closed-form expression for the cumulative distribution function of a new type of random variable involving a number of independent gamma random variables, we present closed-form expressions for the outage probability, general moments for the end-to-end signal to interference and noise ratio and ergodic capacity of the system. In addition, we look into the high signal to noise ratio regime and characterize the diversity order and coding gain achieved by the system. Our result shows that the diversity of the system is limited by the hop experiencing more severer fading. Moreover, the interference does not reduce the diversity order of the system, instead, it degrades the outage performance by affecting the coding gain of the system.\"",
        "Document: \"Performance Analysis of Dual-Hop AF Systems With Interference in Nakagami- m Fading Channels. In this letter, we investigate the performance of dual-hop channel state information-assisted amplify-and-forward relaying systems over Nakagami-m fading channels in the presence of multiple interferers at the relay. Assuming integer fading parameter m, we derive closed-form expressions for the exact outage probability and accurate approximation for symbol error rate of the system. Furthermore, we...\"",
        "Document: \"A Suboptimal Scheme for Multi-User Scheduling in Gaussian Broadcast Channels. This work proposes a suboptimal multi-user scheduling scheme for Gaussian broadcast channels which improves upon the classical single user selection, while considerably reducing complexity as compared to the optimal superposition coding with successful interference cancellation. The proposed scheme combines the two users with the maximum weighted instantaneous rate using superposition coding. The instantaneous rate and power allocation are derived in closed-form, while the long term rate of each user is derived in integral form for all channel distributions. Numerical results are then provided to characterize the prospected gains of the proposed scheme.\"",
        "Document: \"Outage Analysis of Spectrum Sharing Relay Systems With Multiple Secondary Destinations Under Primary User's Interference. In this paper, we investigate the impact of multiuser diversity on the performance of secondary users (SUs) in dual-hop decode-and-forward (DF) spectrum sharing systems over Nakagami- $m$ fading channels, taking into consideration the interference from primary users (PUs). In particular, we present a detailed performance comparison for two different opportunistic scheduling algorithms, i.e., the signal-to-noise ratio (SNR)-based scheduling algorithm and the signal-to-interference-plus-noise ratio (SINR)-based scheduling algorithm. For both scheduling algorithms, exact and asymptotic analytical expressions for the outage probability are derived. Our findings show that the SINR-based scheduling algorithm always outperforms the SNR-based scheduling algorithm. Nevertheless, when the number of destination is large, both scheduling algorithms attain almost the same outage performance, which suggests that, in such a scenario, the SNR-based scheduling algorithm is preferred because it requires less channel state information.\"",
        "1 is \"Fairness-Aware Energy-Efficient Resource Allocation for AF Co-Operative OFDMA Networks\", 2 is \"CSMA/CA performance under high traffic conditions: throughput and delay analysis\"",
        "Given above information, for an author who has written the paper with the title \"Multi-Resolution Multicasting Over the Grassmann and Stiefel Manifolds.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007255": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Rock Porosity Prediction Using Multilayer Perceptions':",
        "Document: \"Approximating support vector machine with artificial neural network for fast prediction. \u2022Hybrid neural network (HNN), a method to accelerate prediction speed of support vector machine (SVM) is proposed.\u2022The proposed method approximates SVM using artificial neural network (ANN).\u2022The proposed method yields much faster prediction speed without compromising prediction accuracy.\u2022The application of this method can improve practical usability of SVM.\"",
        "Document: \"Improvement of Kittler and Illingworth's minimum error thresholding. A simple modification to Kittler and Illingworth's minimum error thresholding method was made and the performance of the modified version was compared with that of the original version empirically. By correcting the biased estimates of variances of model distributions, a significant improvement in performance was found. The improvement was most outstanding among not-well-separated, but still bimodal histograms. In fact, the modification provides a more robust method. The new version is nearly computationally equivalent in complexity to the original version.\"",
        "Document: \"Evaluating the reliability level of virtual metrology results for flexible process control: a novelty detection-based approach. The purpose of virtual metrology (VM) in semiconductor manufacturing is to support process monitoring and quality control by predicting the metrological values of every wafer without an actual metrology process, based on process sensor data collected during the operation. Most VM-based quality control schemes assume that the VM predictions are always accurate, which in fact may not be true due to some unexpected variations that can occur during the process. In this paper, therefore, we propose a means of evaluating the reliability level of VM prediction results based on novelty detection techniques, which would allow flexible utilization of the VM results. Our models generate a high-reliability score for a wafer's VM prediction only when its process sensor values are found to be consistent with those of the majority of wafers that are used in model building; otherwise, a low-reliability score is returned. Thus, process engineers can selectively utilize VM results based on their reliability level. Experimental results show that our reliability generation models are effective; the VM results for wafers with a high level of reliability were found to be much more accurate than those with a low level.\"",
        "Document: \"Applying convolution filter to matrix of word-clustering based document representation. Word-clustering based document representation approaches have been suggested recently to overcome previous limitations such as high dimensionality or loss of innate interpretation; they show higher classification performance than other recent methods. Thus, we present a novel way to combine the advantages of various word-clustering based representation approaches. Instead of previous approaches, which represent documents in vector form, we represent documents in matrix form while concatenate various representation results. And we proposed another novel way to apply convolution filter to those representation while rearranging the elements by preserving the semantic distance. In order to verify the representation performance of our proposed methods, we utilized the kinds of dataset: customer-voice data from LG Electronics, public Reuter news dataset and 20 Newsgroup dataset. The results demonstrated that the proposed method outperforms all other methods and achieves a classification accuracy of 88.73%, 89.16%, and 88.06% for each dataset.\"",
        "Document: \"Learning competition and cooperation. Competitive activation mechanisms introduce competitive orinhibitory interactions between units through functional mechanismsinstead of inhibitory connections. A unit receives input fromanother unit proportional to its own activation as well as to thatof the sending unit and the connection strength between the two.This, plus the finite output from any unit, induces competitionamong units that receive activation from the same unit. Here wepresent a backpropagation learning rule for use with competitiveactivation mechanisms and show empirically how this learning rulesuccessfully trains networks to perform an exclusive-OR task and adiagnosis task. In particular, networks trained by this learningrule are found to outperform standard backpropagation networks withnovel patterns in the diagnosis problem. The ability of competitivenetworks to bring about context-sensitive competition andcooperation among a set of units proved to be crucial in diagnosingmultiple disorders.\"",
        "1 is \"A task-technology fit view of learning management system impact\", 2 is \"Workflow mining with InWoLvE\"",
        "Given above information, for an author who has written the paper with the title \"Rock Porosity Prediction Using Multilayer Perceptions\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007293": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A New Method For Automated Image Warping Based On A Variational Approach':",
        "Document: \"LAPCHAT: A Contents-Sharable Management System for Computer Supported Collaborative Learning. The purpose of this study is to explore the architecture for a collaborative learning environment, in which individual learning and collaborative learning are smoothly connected. We proposed a composition model of a collaborative workplace, and a management model for action in the collaborative learning space and the state of learning context. Based on the ides, in this paper we describe a Contents-Sharable mechanism between a private workplace and a collaborative workplace.\"",
        "Document: \"Message-Based Motivation Modeling. Social networks contain a multitude of messages that can be utilized to motivate learning. However, while some messages may increase a learner's motivation, other messages could undermine it. How can we tell which is which? Conceptual motivation models provide many answers, but how to translate these models into a concrete programmatic implementation (required by e-Learning systems) is often unclear. We approach the problem from a different angle, taking a data-driven approach by (1) assembling a corpus of over 100,000 messages, and (2) applying machine learning methods to this data to create a first-of-its-kind message motivation classifier. The constructed corpus and classifier provide for a new empirical way of studying text-based motivation, developing new models, and empirically evaluating such models on a large-scale.\"",
        "Document: \"Knowledge Management Framework for Collaborative Learning Support. The purpose of this study is to support the learning activity in the Internet learning space. In this paper, we examine the knowledge management and the knowledge representation of the learning information for the collaborative learning support. RAPSODY-EX (REX) is a distributed learning support environment organized as a learning infrastructure. REX can effectively carry out the collaborative learning support in asynchronous/synchronous learning mode. Distributed learning is a learning style where individual learning and collaborative learning are carried out on the multimedia communication network. In the distributed learning environment-, arrangement and integration of the learning information are attempted to support the decision making of learners and mediators. Various information in the educational context is referred and reused as knowledge which oneself and others can practically utilize. We aim at the construction of an increasingly growing digital portfolio database. In addition, the architecture of the learning environment that includes such a database is researched.\"",
        "Document: \"The Intelligent Discussion Supporting System under the Distributed Environment. The purpose of this study is to propose a technical and psychological framework of intelligent computer supported collaborative work/learning (Intelligent CSCW/L) which we call iDCLE (intelligent distributed collaborative learning environment). To realize this learning environment iDCLE, we constructed the real-time sharing communication system for collaborative work/learning on the computer network. In this paper, we describe the fundamental configuration of iDCLE, and discuss the mechanism of supporting discussion intelligently. The discussion support system in iDCLE can identify the state of discussion by tracing our model. The system can also diagnose the process of discussion for the purpose of making progress in the discussion using this model. If any advice is needed after diagnosing, the system gives the appropriate advice to the participants.\"",
        "Document: \"Automatic Group Formation for Informal Collaborative Learning. Ability to find appropriate collaborators and learning materials is crucial for informal collaborative learning. However, traditional group formation models are not applicable/effective in informal learning settings since little is known about learners and learning materials and teacher's assistance is not available. We propose the data-driven group formation model that automatically extracts information about learners and learning materials from multiple data sources (databases of academic publications, wikis, social networking cites, blogs, forums, etc) and automatically forms collaborative learning groups. The open source implementation of the model (a part of WebClass-RAPSODY learning management system) consists of loosely coupled modules (implementing the proposed methods for data mashup, mining and inference) integrated through the web services interface; allowing for easy adaptation, extension and customization of the model.\"",
        "1 is \"A new approach to ITS-curriculum and course authoring: the authoring environment\", 2 is \"Parallel and Distributed Computing for Cybersecurity\"",
        "Given above information, for an author who has written the paper with the title \"A New Method For Automated Image Warping Based On A Variational Approach\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007322": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A Dynamic Hybrid Scheduling Algorithm For Heterogeneous Asymmetric Environments':",
        "Document: \"Chromatin remodeling in silico: A stochastic model for SWI/SNF. Beside their contribution in DNA packaging, histone-core particles modulate the transcription machinery access to the DNA through dynamic chromatin structure. Chromatin remodeling complexes perturb such modulations through diverse mechanisms. SWI/SNF is a well-studied highly conserved chromatin remodeling complex that is ubiquitous across eukaryotes. Rigorous study of experimental observations suggests randomness in dynamics of SWI/SNF in cis chromatin remodeling process. In this work we propose a stochastic computational model that captures such fluctuations. We incorporate the physiological properties of the process through parametric microevents. Each microevent is then associated with a stochastic model that couples its random temporal and spatial dynamics with the energy landscape of the remodeling process. We further show that DNA sequence stacks and friction force have negligible effect on chromatin remodeling. Our approach shows a promising approximation to the force impinged on the DNA by the SWI/SNF complex. We validate our model predictions with several experimental data sets. The proposed model suggest that the in cis translocation rate of histone-core particle follows a Gamma distribution. By carefully analyzing the simulation results we conjecture that SWI/SNF chromatin remodeling has low energy efficiency (<0.30). We use our model to recapitulate the dynamics of the parallel remodeling processes that occur in close proximity across a typical eukaryotic genome. Our results suggest that the orchestrated chromatin remodeling makes few kilobase-pairs of the DNA accessible to the transcription machinery in a timely manner.\"",
        "Document: \"iSimBioSys: An \"In Silico\" Discrete Event Simulation Framework for Modeling Biological Systems. The genome projects have provided comprehensive information about the basic building blocks of life. The next challenge is to understand how biological functions emerge from complex interactions of these building blocks. In this work, we present a generic, extensible in silico simulation framework which allows the experimenter to test various hypotheses of an experiment \u9a74in silico\u9a74 and develop a model for subsequent wet test analysis. Undertaking a systems approach, we abstract a biological process as a set of interacting functions driven in time by a set of discrete events. We focus on three two-component gene regulatory networks, (a) PhoP/PhoQ network (b) barA/sirA network and (c) pmrB/pmrA network involved in bacterial pathogenesis in Salmonella Typhimurium and capture their interactions in various stages of infection. We report results on the expression of various gene and gene products from these pathways. We conclude that such a stochastic framework can provide insight into how collective interaction of different molecules manifests in physiology and diseases.\"",
        "Document: \"A Dynamic Hybrid Scheduling Algorithm For Heterogeneous Asymmetric Environments. The rapid growth of web services has already given birth to a set of data dissemination applications. Efficient scheduling techniques are necessary to endow such applications with advanced data processing capability. In this paper, we have developed a new hybrid scheduling scheme by effectively combining broadcasting of very popular (push) data and dissemination of less popular (pull) data to develop a new, ideal hybrid scheduling scheme. Our algorithm assumes no prior knowledge of the data access probability, i.e. which items are being pushed or pulled. Instead, data access probabilities and the cut-off-point to segregate the push and the pull sets are computed dynamically. The data items are assumed to be of variable lengths. The clients send their requests to the server, which ignores the requests for the push items but queues those for the pull items. An ideal behavior of the clients is assumed: a client, after making a request, patiently waits without taking any action. Every client is assumed to have a priority. The priority of a data item is determined by adding the priorities of all the clients requesting that item. At every instant, the item to be broadcast is selected with the help of a pure-push scheduling. On the other hand, the item to be pulled is the one stored in the pull-queue, having the optimal stretch value (maxrequest min-service time). When more than one data item has the same stretch value, the influence of the priorities of different clients on data dissemination is considered. A suitable modelling technique using the birth-and-death process is developed to analyze the performance of the system. Simulation results corroborate the average system performance and exhibit significant improvement over a pure push and existing hybrid systems in terms of average waiting time spent by a client.\"",
        "Document: \"An analytical model to estimate the time taken for cytoplasmic reactions for stochastic simulation of complex biological systems. The complexity of biological systems motivates the use of a computer or \"in silico\" stochastic event based modeling approach to better identify the dynamic interactions of different processes in the system. This requires the computation of the time taken by different events in the system based on their biological functions and corresponding environment. One such important event is the reactions between the molecules inside the cytoplasm of a cell where the reaction environment is highly chaotic. We present a mathematical formulation for the estimation of the reaction time between two molecules within a cell based on the system state assuming that the reactant molecules enter the system one at a time to initiate reactions. We derive expressions for the average and second moment of the time for reaction to be used by our stochastic event-based simulation. Unlike rate equations, the proposed model does not require the assumption of concentration stability for multiple molecule reactions. The reaction time estimate is considered to be a random variable that suits the stochastic event based simulation method.\"",
        "Document: \"A Dynamic Hybrid Scheduling Algorithm with Clients' Departure for Impatient Clients in Heterogeneous Environments. The essence of efficient scheduling and data transmission techniques lies in providing the web-applications with advanced data processing capabilities. In this paper we have efficiently combined the push and the pull scheduling to develop a new, practical, dynamic, hybrid scheduling strategy for heterogenous, asymmetric environments. The proposed algorithm dynamically computes the probabilities and the optimal cutoff-point to separate the push and the pull data sets. The data items are also assumed to be of variable lengths. While the push strategy uses the flat, roundrobin scheduling, the pull items are determined by stretch-optimal (max-request min-service time) scheduling policy. In order to make the scheduling more practical, we have considered the impact of the impatience of the clients waiting to get the service of a particular data item. The effects of this impatience can lead to departure of specific client(s) from the system. Our proposed hybrid scheduling strategy takes care of these effects to capture a real portrayal of the system dynamics. These scenarios are modelled by suitable birth and death process to analyze the overall expected delay of the system. Subsequently, simulation results corroborate the average system performance and points out significant improvement over existing hybrid systems in terms of average waiting time spent by a client.\"",
        "1 is \"A resource estimation and call admission algorithm for wireless multimedia networks using the shadow cluster concept\", 2 is \"Robust Algorithms and Price of Robustness in Shunting Problems\"",
        "Given above information, for an author who has written the paper with the title \"A Dynamic Hybrid Scheduling Algorithm For Heterogeneous Asymmetric Environments\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007338": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Automated Resolution Selection for Image Segmentation.':",
        "Document: \"Gaussian Bare-Bones Differential Evolution. Differential evolution (DE) is a well-known algorithm for global optimization over continuous search spaces. However, choosing the optimal control parameters is a challenging task because they are problem oriented. In order to minimize the effects of the control parameters, a Gaussian bare-bones DE (GBDE) and its modified version (MGBDE) are proposed which are almost parameter free. To verify the ...\"",
        "Document: \"An intuitive distance-based explanation of opposition-based sampling. The impact of the opposition concept can be observed in many areas around us. This concept has sometimes been called by different names, such as, opposite particles in physics, complement of an event in probability, absolute or relative complement in set theory, and theses and antitheses in dialectic. Recently, opposition-based learning (OBL) was proposed and has been utilized in different soft computing areas. The main idea behind OBL is the simultaneous consideration of a candidate and its corresponding opposite candidate in order to achieve a better approximation for the current solution. OBL has been employed to introduce opposition-based optimization, opposition-based reinforcement learning, and opposition-based neural networks, as some examples among others. This work proposes an Euclidean distance-to-optimal solution proof that shows intuitively why considering the opposite of a candidate solution is more beneficial than another random solution. The proposed intuitive view is generalized to N-dimensional search spaces for black-box problems.\"",
        "Document: \"Differential Evolution enhanced by neighborhood search. This paper presents a novel Differential Evolution (DE) algorithm, called DE enhanced by neighborhood search (DENS), which differs from pervious works of utilizing the neighborhood search in DE, such as DE with neighborhood search (NSDE) and self-adaptive DE with neighborhood search (SaNSDE). In DENS, we focus on searching the neighbors of individuals, while the latter two algorithms (NSDE and SaNSDE) work on the adaption of the control parameters F and CR. The proposed algorithm consists of two following main steps. First, for each individual, we create two trial individuals by local and global neighborhood search strategies. Second, we select the fittest one among the current individual and the two created trial individuals as a new current individual. Experimental studies on a comprehensive set of benchmark functions show that DENS achieves better results for a majority of test cases, when comparing with some other similar evolutionary algorithms.\"",
        "Document: \"Simulated Raindrop algorithm for global optimization. In this paper, we propose a novel single-solution based metaheuristic algorithm called Simulated Raindrop (SRD). The SRD algorithm is inspired by the principles of raindrops. When rain falls on the land, it normally flows from higher altitude to a lower due to gravity, while choosing the optimum path towards the lowest point on the landscape. We compared the performance of simulated annealing (SA) against the proposed SRD method on 8 commonly utilized benchmark functions. Experimental results confirm that SRD outperforms SA on all test problems in terms of variant performance measures, such as convergence speed, accuracy of the solution, and robustness.\"",
        "Document: \"Multi-resolution level set image segmentation using wavelets. Level set methods have been used for image segmentation. Because partial deferential equations are solved to propagate a curve, level-set image segmentation has a slow convergence speed. The objective of this paper is to propose a method that increases the convergence speed. The proposed approach exploits the benefit of multi-resolutional analysis. Wavelet transform is used to decompose the image into different resolutions. The obtained results show a great improvement in terms of speed and accuracy.\"",
        "1 is \"Optimal short-term hydro-thermal scheduling using quasi-oppositional teaching learning based optimization\", 2 is \"Thermal Face Recognition Using Local Patterns\"",
        "Given above information, for an author who has written the paper with the title \"Automated Resolution Selection for Image Segmentation.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007388": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Improved SFFS method for channel selection in motor imagery based BCI.':",
        "Document: \"Fuzzy Remote Tracking Control for Randomly Varying Local Nonlinear Models Under Fading and Missing Measurements. This paper proposes a novel remote tracking control strategy for a class of discrete-time Takagi-Sugeno fuzzy systems with randomly occurring uncertainties and randomly varying local nonlinear models. The outputs of the fuzzy system are collected through an unreliable sensor subject to missing measurements. Simultaneously, the outputs of the remote models are transmitted to the controller through ...\"",
        "Document: \"Chaotic Synchronization Using Sampled-Data Fuzzy Controller Based on Fuzzy-Model-Based Approach. This paper presents the synchronization of chaotic system using a sampled-data fuzzy controller. To carry out the system analysis, a fuzzy model is employed to represent the chaotic systems. Linear-matrix-inequality (LMI)-based system stability and performance conditions are derived using a Lyapunov-based approach. The derived LMI-based stability and performance conditions are employed to aid the ...\"",
        "Document: \"Stability analysis and stabilization of polynomial fuzzy-model-based control systems using piecewise linear membership functions. This paper presents the stability analysis of polynomial fuzzy-model-based (PFMB) control system, formed by a polynomial fuzzy model and a fuzzy controller connected in a closed loop, using sum-of-squares (SOS) approach. Unlike the published work, the PFMB control system is not required that the polynomial fuzzy controller shares the same premises membership functions as those of the polynomial fuzzy model. Piecewise linear membership functions are employed to approximate the membership functions of the polynomial fuzzy model and polynomial fuzzy controller to facilitate stability analysis and controller synthesis with consideration of approximation error. The piecewise linear membership functions offer a nice property that the grades of membership are governed by a finite number of sampled points. It is worth mentioning that the piecewise linear membership functions, which are not necessarily implemented physically, are a tool to carry out the stability analysis. The nice property of the piecewise linear membership functions allows them to be brought to the SOS-based stability conditions derived based on the Lyapunov stability theory. Consequently, the proposed SOS-based stability conditions are applied to PFMB control systems with the specified piecewise linear membership functions rather than any shapes. A simulation example is given to verify the stability analysis results and demonstrate the effectiveness of the proposed approach.\"",
        "Document: \"Reduced-Order Fault Detection Filter Design for Fuzzy Semi-Markov Jump Systems With Partly Unknown Transition Rates. This article deals with the fault detection problem for a class of Takagi\u2013Sugeno (T\u2013S) fuzzy semi-Markov jump systems (FSMJSs) with partly unknown transition rates (PUTRs) subject to output quantization by designing a reduced-order filter. First, a more general PUTRs model is constructed to describe the situation that the information of some elements is completely unknown, where this model is affected simultaneously by PU information and time-varying parameter compared with the traditional PUTRs model. Second, we take full advantage of the reduced-order filter to address the fault detection problem for FSMJSs, in which the stochastic failure phenomenon is injected into the reduced-order filter. Besides, the logarithmic quantizer is employed to tackle the limited bandwidth problem in a communication channel. Consequently, the new sufficient conditions are developed based on the Lyapunov theory to obtain the desired reduced-order filter. Simulation results with respect to the tunnel diode circuit are provided to demonstrate the usefulness and availability of the established theoretical results.\"",
        "Document: \"Output-feedback tracking control for interval type-2 polynomial fuzzy-model-based control systems. In this paper, the output tracking control issues of polynomial-fuzzy-model-based (PFMB) systems equipped with mismatched interval type-2 (IT2) membership functions are investigated. The output-feedback IT2 polynomial fuzzy controller connected with the nonlinear plant in a closed loop drives the system states of the nonlinear plant to track those of a stable reference model. The system stability is investigated based on the Lyapunov stability theory under the sum-of-squares (SOS)-based analysis approach and the SOS-based stability conditions are derived subjecting to an H performance. In addition, the fuzzy controller does not need to share the same membership functions with the plant. Moreover, the information of membership functions is included in the analysis to facilitate the analysis and relax the stability conditions. Numerical and experimental examples are presented to verify the effectiveness of the proposed tracking control approach.\"",
        "1 is \"Tensor Deflation for CANDECOMP/PARAFAC\u2014 Part I: Alternating Subspace Update Algorithm\", 2 is \"Delay-Dependent Decentralized $H_\\infty$ Filtering for Discrete-Time Nonlinear Interconnected Systems With Time-Varying Delay Based on the T\u2013S Fuzzy Model\"",
        "Given above information, for an author who has written the paper with the title \"Improved SFFS method for channel selection in motor imagery based BCI.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007458": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A new approach for processing ranked subsequence matching based on ranked union':",
        "Document: \"Efficient Storage and Querying of Horizontal Tables Using a PIVOT Operation in Commercial Relational DBMSs. In recent years, a horizontal table with a large number of attributes is widely used in OLAP or e-business applications to analyze multidimensional data efficiently. For efficient storing and querying of horizontal tables, recent works have tried to transform a horizontal table to a traditional vertical table. Existing works, however, have the drawback of not considering an optimized PIVOT operation provided (or to be provided) in recent commercial RDBMSs. In this paper we propose a formal approach that exploits the optimized PIVOT operation of commercial RDBMSs for storing and querying of horizontal tables. To achieve this goal, we first provide an overall framework that stores and queries a horizontal table using an equivalent vertical table. Under the proposed framework, we then formally define 1) a method that stores a horizontal table in an equivalent vertical table and 2) a PIVOT operation that converts a stored vertical table to an equivalent horizontal view. Next, we propose a novel method that transforms a user-specified query on horizontal tables to an equivalent PIVOT-included query on vertical tables. In particular, by providing transformation rules for all five elementary operations in relational algebra as theorems, we prove our method is theoretically applicable to commercial RDBMSs. Experimental results show that, compared with the earlier work, our method reduces storage space significantly and also improves average performance by several orders of magnitude. These results indicate that our method provides an excellent framework to maximize performance in handling horizontal tables by exploiting the optimized PIVOT operation in commercial RDBMSs.\"",
        "Document: \"Efficient distributed parallel top-down computation of ROLAP data cube using mapreduce. The computation of multidimensional OLAP(On-Line Analytical Processing) data cube takes much time, because a data cube with D dimensions consists of 2D cuboids. To build ROLAP(Relational OLAP) data cubes efficiently, existing algorithms (e.g., GBLP, PipeSort, PipeHash, BUC, etc) use several strategies sharing sort cost and input data scan, reducing data computation, and utilizing parallel processing techniques. On the other hand, MapReduce is recently emerging for the framework processing a huge volume of data like web-scale data in a distributed/parallel manner by using a large number of computers (e.g., several hundred or thousands). In the MapReduce framework, the degree of parallel processing is more important to reduce total execution time than elaborate strategies. In this paper, we propose a distributed parallel processing algorithm, called MRPipeLevel, which takes advantage of the MapReduce framework. It is based on the existing PipeSort algorithm which is one of the most efficient ones for top-down cube computation. The proposed MRPipeLevel algorithm parallelizes cube computation and reduces the number of data scan by pipelining at the same time. We implemented and evaluated the proposed algorithm under the MapReduce framework. Through the experiments, we also identify factors for performance enhancement in MapReduce to process very huge data.\"",
        "Document: \"Publishing time-series data under preservation of privacy and distance orders. In this paper we address the problem of preserving mining accuracy as well as privacy in publishing sensitive time-series data. For example, people with heart disease do not want to disclose their electrocardiogram time-series, but they still allow mining of some accurate patterns from their time-series. Based on this observation, we introduce the related assumptions and requirements.We show that only randomization methods satisfy all assumptions, but even those methods do not satisfy the requirements. Thus, we discuss the randomization-based solutions that satisfy all assumptions and requirements. For this purpose, we use the noise averaging effect of piecewise aggregate approximation (PAA), which may alleviate the problem of destroying distance orders in randomly perturbed time-series. Based on the noise averaging effect, we first propose two naive solutions that use the random data perturbation in publishing time-series while exploiting the PAA distance in computing distances. There is, however, a tradeoff between these two solutions with respect to uncertainty and distance orders. We thus propose two more advanced solutions that take advantages of both naive solutions. Experimental results show that our advanced solutions are superior to the naive solutions.\"",
        "Document: \"Efficient Level-Based Top-Down Data Cube Computation Using MapReduce. Data cube is an essential part of OLAP(On-Line Analytical Processing) to support efficiently multidimensional analysis for a large size of data. The computation of data cube takes much time, because a data cube with d dimensions consists of 2(d) (i.e., exponential order of d) cuboids. To build ROLAP (Relational OLAP) data cubes efficiently, many algorithms (e.g., GBLP, PipeSort, PipeHash, BUC, etc.) have been developed, which share sort cost and input data scan and/or reduce data computation time. Several parallel processing algorithms have been also proposed. On the other hand, MapReduce is recently emerging for the framework processing huge volume of data like web-scale data in a distributed/parallel manner by using a large number of computers (e.g., several hundred or thousands). In the MapReduce framework, the degree of parallel processing is more important to reduce total execution time than elaborate strategies like short-share and computation-reduction which existing ROLAP algorithms use. In this paper, we propose two distributed parallel processing algorithms. The first algorithm called MRLevel, which takes advantages of the MapReduce framework. The second algorithm called MRPipeLevel, which is based on the existing PipeSort algorithm which is one of the most efficient ones for top-down cube computation. (Top-down approach is more effective to handle big data, compared to others such as bottom-up and special data structures which are dependent on main-memory size.) The proposed MRLevel algorithm tries to parallelize cube computation and to reduce the number of data scan by level at the same time. The MRPipeLevel algorithm is based on the advantages of the MRLevel and to reduce the number of data scan by pipelining at the same time. We implemented and evaluated the performance of this algorithm under the MapReduce framework. Through the experiments, we also identify the factors for performance enhancement in MapReduce to process very huge data.\"",
        "Document: \"Feasibility study for simulating community based content caching on CCN network using ndnSIM simulator. In this paper we have done a feasibility study to develop a simulation model for Content Centric Network. Our goal is to form communities based on popular clustering algorithm on the simulated CCN network. Furthermore we aim to do a guided content delivery on CCN based on the community preferences. Though community formation and a guided delivery of content is not a new field. However an experimental simulation of these concepts on CCN network is novel. Basically we want to utilize CCN network and its advantages to implement a next generation CDN.\"",
        "1 is \"Providing VCR capabilities in large-scale video servers\", 2 is \"Hierarchical Density Estimates for Data Clustering, Visualization, and Outlier Detection\"",
        "Given above information, for an author who has written the paper with the title \"A new approach for processing ranked subsequence matching based on ranked union\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007460": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Online stochastic packing applied to display ad allocation':",
        "Document: \"Approximating the minimum-cost maximum flow is P-complete.  We show that it is impossible, in NC, to approximate the value of the minimumcostmaximum flow unless P = NC.Keywords: Theory of computation, P-complete, minimum-cost flow, maximum flow.1 IntroductionOnce a problem is proved to be P-complete, it is generally believed that there exists no NCor RNC algorithm to solve it exactly1. Therefore, the next important question becomes howwell can it be approximated in NC or RNC? In this note we establish an interesting contrastbetween the... \"",
        "Document: \"Models of malicious behavior in sponsored search. Search engines such as Google, Yahoo, and MSN now auction off search terms to potential advertisers. The potential advertisers place their bids on each search term of interest, as well as specifying a daily budget. Each search on this term displays an advertisement that is linked to the advertiser's website, and the advertiser pays the search engine every time the link is activated. When an advertiser's budget is reached, the search engine stops displaying their ad. This kind of advertising is extremely popular -- the combined revenue of Yahoo and Google in 2005 was estimated at over 4.5 billion dollars. We develop small models which still have the property that malicious behavior such as bid-jamming still occurs as a rational best-response strategy. Such malicious behavior occurs frequently in practice. We are able to derive bidding strategies which are the best-responses when the budget of the bidder is low relative to her competitors, as well as strategies which protect against bid-jamming.\"",
        "Document: \"An optimal online algorithm for packet scheduling with agreeable deadlines. An important issue in IP-based QoS networks is the effective management of packets at the router level. Specifically, if the arriving packets cannot all be stored in a buffer, or if the packets have deadlines by which they must be delivered, the router needs to identify the packets that should be dropped. In recent work, Kesselman et al. [6] propose a model, called buffer management with bounded delay, which can be thought of as an online scheduling problem on a single machine: packets arrive at a network switch and are stored in a buffer of size B. Each packet has a positive weight and a deadline, with the weight representing the value of transmitting the packet by its deadline. At each integer time step, exactly one packet can be transmitted, and the objective is to maximize the total weight of the transmitted packets. If B = \u221e, this is the online version of the scheduling problem 1| pj = 1, rj, dj |\u03a3 wj Uj. (We assume that rj and dj are integers.)\"",
        "Document: \"A 2-Competitive Algorithm For Online Convex Optimization With Switching Costs. We consider a natural online optimization problem set on the real line. The state of the online algorithm at each integer time is a location on the real line. At each integer time, a convex function arrives online. In response, the online algorithm picks a new location. The cost paid by the online algorithm for this response is the distance moved plus the value of the function at the final destination. The objective is then to minimize the aggregate cost over all time. The motivating application is rightsizing power-proportional data centers. We give a 2-competitive algorithm for this problem. We also give a 3-competitive memoryless algorithm, and show that this is the best competitive ratio achievable by a deterministic memoryless algorithm. Finally we show that this online problem is strictly harder than the standard ski rental problem.\"",
        "Document: \"Adding Trust to P2P Distribution of Paid Content. While peer-to-peer (P2P) file-sharing is a powerful and cost-effective content distribution model, most paid-for digital-content providers (CPs) use direct download to deliver their content. CPs are hesitant to rely on a P2P distribution model because it introduces a number of security concerns including content pollution by malicious peers, and lack of enforcement of authorized downloads. Furthermore, because users communicate directly with one another, the users can easily form illegal file-sharing clusters to exchange copyrighted content. Such exchange could hurt the content providers' profits. We present a P2P system TP2P, where we introduce a notion of trusted auditors (TAs). TAs are P2P peers that police the system by covertly monitoring and taking measures against misbehaving peers. This policing allows TP2P to enable a stronger security model making P2P a viable alternative for the distribution of paid digital content. Through analysis and simulation, we show the effectiveness of even a small number of TAs at policing the system. In a system with as many as 60% of misbehaving users, even a small number of TAs can detect 99% of illegal cluster formation. We develop a simple economic model to show that even with such a large presence of malicious nodes, TP2P can improve CP's profits (which could translate to user savings) by 62% to 122%, even while assuming conservative estimates of content and bandwidth costs. We implemented TP2P as a layer on top of BitTorrent and demonstrated experimentally using PlanetLab that our system provides trusted P2P file sharing with negligible performance overhead.\"",
        "1 is \"Minimizing communication in sparse matrix solvers\", 2 is \"Data structures for weighted matching and nearest common ancestors with linking\"",
        "Given above information, for an author who has written the paper with the title \"Online stochastic packing applied to display ad allocation\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007498": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A new modified hybrid learning algorithm for feedforward neural networks':",
        "Document: \"Dimension Reduction by Minimum Error Minimax Probability Machine. Dimension reduction is frequently adopted as a data preprocessing technique to facilitate data visualization, interpretation, and classification. Traditional dimension reduction methods such as linear discriminant analysis focus on maximizing the overall discrimination between all classes, which may be easily affected by outliers. To overcome this disadvantage, this paper proposes a novel method for multiclass dimension reduction, named dimension reduction by minimum error minimax probability machine (DR-MEMPM). It elaborately ensures that each pair of classes is well separated in the projected subspace by utilizing the separation probability between different pairwise classes. Therefore, it can put more emphasis on those less distinguishable classes, and the learned projection will not be dominated by some outlier classes which lie far away from other classes. We evaluate the proposed DR-MEMPM on a number of synthetic and real-world data sets, and show that it outperforms other state-of-the-art dimension reduction methods in terms of visual intuition and classification accuracy, especially when the distances between classes are unevenly distributed.\"",
        "Document: \"Trends in extreme learning machines: A review. Extreme learning machine (ELM) has gained increasing interest from various research fields recently. In this review, we aim to report the current state of the theoretical research and practical advances on this subject. We first give an overview of ELM from the theoretical perspective, including the interpolation theory, universal approximation capability, and generalization ability. Then we focus on the various improvements made to ELM which further improve its stability, sparsity and accuracy under general or specific conditions. Apart from classification and regression, ELM has recently been extended for clustering, feature selection, representational learning and many other learning tasks. These newly emerging algorithms greatly expand the applications of ELM. From implementation aspect, hardware implementation and parallel computation techniques have substantially sped up the training of ELM, making it feasible for big data processing and real-time reasoning. Due to its remarkable efficiency, simplicity, and impressive generalization performance, ELM have been applied in a variety of domains, such as biomedical engineering, computer vision, system identification, and control and robotics. In this review, we try to provide a comprehensive view of these advances in ELM together with its future perspectives.\"",
        "Document: \"Classification Of Mental Tasks From Eeg Signals Using Extreme Learning Machine. In this paper, a recently developed machine learning algorithm referred to as Extreme Learning Machine (ELM) is used to classify five mental tasks from different subjects using electroencephalogram (EEG) signals available from a well-known database. Performance of ELM is compared in terms of training time and classification accuracy with a Backpropagation Neural Network (BPNN) classifier and also Support Vector Machines (SVMs). For SVMs, the comparisons have been made for both 1-against-1 and I-against-all methods. Results show that ELM needs an order of magnitude less training time compared with SVMs and two orders of magnitude less compared with BPNN. The classification accuracy of ELM is similar to that of SVMs and BPNN. The study showed that smoothing of the classifiers' outputs can significantly improve their classification accuracies.\"",
        "Document: \"Face recognition based on extreme learning machine. Extreme learning machine (ELM) is an efficient learning algorithm for generalized single hidden layer feedforward networks (SLFNs), which performs well in both regression and classification applications. It has recently been shown that from the optimization point of view ELM and support vector machine (SVM) are equivalent but ELM has less stringent optimization constraints. Due to the mild optimization constraints ELM can be easy of implementation and usually obtains better generalization performance. In this paper we study the performance of the one-against-all (OAA) and one-against-one (OAO) ELM for classification in multi-label face recognition applications. The performance is verified through four benchmarking face image data sets.\"",
        "Document: \"Composite Function Wavelet Neural Networks with Differential Evolution and Extreme Learning Machine. In this paper, we introduce a new learning method for composite function wavelet neural networks (CFWNN) by combining the differential evolution (DE) algorithm with extreme learning machine (ELM), in short, as CWN-E-ELM. The recently proposed CFWNN trained with ELM (CFWNN-ELM) has several promising features. But the CFWNN-ELM may have some redundant nodes due to the number of hidden nodes assigned a priori and the input weight matrix and the hidden node parameter vector randomly generated once and never changed during the learning phase. The introduction of DE into CFWNN-ELM is to search for the optimal network parameters and to reduce the number of hidden nodes used in the network. Simulations on several artificial function approximations, real-world data regressions and a chaotic signal prediction problem show some advantages of the proposed CWN-E-ELM. Compared with CFWNN-ELM, CWN-E-ELM has a much more compact network size and Compared with several relevant methods, CWN-E-ELM is able to achieve a better generalization performance.\"",
        "1 is \"BagBoosting for tumor classification with gene expression data.\", 2 is \"Fast cross-validation algorithms for least squares support vector machine and kernel ridge regression\"",
        "Given above information, for an author who has written the paper with the title \"A new modified hybrid learning algorithm for feedforward neural networks\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007592": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Interlocking obfuscation for anti-tamper hardware':",
        "Document: \"Staged concurrent program analysis. Concurrent program verification is challenging because it involves exploring a large number of possible thread interleavings together with complex sequential reasoning. As a result, concurrent program verifiers resort to bi-modal reasoning, which alternates between reasoning over intra-thread (sequential) semantics and inter-thread (concurrent) semantics. Such reasoning often involves repeated intra-thread reasoning for exploring each interleaving (inter-thread reasoning) and leads to inefficiency. In this paper, we present a new two-stage analysis which completely separates intra- and inter-thread reasoning. The first stage uses sequential program semantics to obtain a precise summary of each thread in terms of the global accesses made by the thread. The second stage performs inter-thread reasoning by composing these thread-modular summaries using the notion of sequential consistency. Assertion violations and other concurrency errors are then checked in this composition with the help of an off-the-shelf SMT solver. We have implemented our approach in the FUSION framework for checking concurrent C programs shows that avoiding redundant bi-modal reasoning makes the analysis more scalable.\"",
        "Document: \"The compositional far side of image computation. Symbolic image computation is the most fundamental computation in BDD-based sequential system optimization and formal verification. In this paper, we explore the use of over-approximation and BDD minimization with don't cares during image computation. Our new method, based on the partitioned representation of the transition relation, consists of three phases: First, the model is treated as a set of loosely coupled components, and over-approximate images are computed to minimize the transition relation of each component. A refined overall image is then computed using the simplified transition relation. Finally, the exact image is obtained by a clipping operation that recovers all previous over-approximations. Since BDD minimization employs constraints on the next-state variables of the transition relation, instead of the customary constraints on the present-state variables, we call the resulting method far side image computation. The new method can be implemented on top of any image computation algorithm that is based on the partitioned transition relation. (For example, IWLS95, MLP, and Fine-Grain.) We demonstrate the effectiveness of our approach by experiments on models ranging from easy to hard: The new method wins significantly over the best known algorithms so far in both CPU time and memory usage, especially on the hard models.\"",
        "Document: \"Using statically computed invariants inside the predicate abstraction and refinement loop. Predicate abstraction is a powerful technique for extracting finite-state models from often complex source code. This paper reports on the usage of statically computed invariants inside the predicate abstraction and refinement loop. The main idea is to selectively strengthen (conjoin) the concrete transition relation at a given program location by efficiently computed invariants that hold at that program location. We experimentally demonstrate the usefulness of transition relation strengthening in the predicate abstraction and refinement loop. We use invariants of the form \u00b1x \u00b1y \u2264c where c is a constant and x,y are program variables. These invariants can be discovered efficiently at each program location using the octagon abstract domain. We observe that the abstract models produced by predicate abstraction of strengthened transition relation are more precise leading to fewer spurious counterexamples, thus, decreasing the total number of abstraction refinement iterations. Furthermore, the length of relevant fragments of spurious traces needing refinement shortens. This leads to an addition of fewer predicates for refinement. We found a consistent reduction in the total number of predicates, maximum number of predicates tracked at a given program location, and the overall verification time.\"",
        "Document: \"Assertion guided symbolic execution of multithreaded programs. Symbolic execution is a powerful technique for systematic testing of sequential and multithreaded programs. However, its application is limited by the high cost of covering all feasible intra-thread paths and inter-thread interleavings. We propose a new assertion guided pruning framework that identifies executions guaranteed not to lead to an error and removes them during symbolic execution. By summarizing the reasons why previously explored executions cannot reach an error and using the information to prune redundant executions in the future, we can soundly reduce the search space. We also use static concurrent program slicing and heuristic minimization of symbolic constraints to further reduce the computational overhead. We have implemented our method in the Cloud9 symbolic execution tool and evaluated it on a large set of multithreaded C/C++ programs. Our experiments show that the new method can reduce the overall computational cost significantly.\"",
        "Document: \"An SMT based method for optimizing arithmetic computations in embedded software code. We present a new method for optimizing the C/C++ code of embedded control software with the objective of minimizing implementation errors in the linear fixed-point arithmetic computations caused by overflow, underflow, and truncation. Our method relies on the use of an SMT solver to search for alternative implementations that are mathematically equivalent but require a smaller bit-width, or implementations that use the same bit-width but have a larger error-free dynamic range. Our systematic search of the bounded implementation space is based on an inductive synthesis procedure, which guarantees to find a solution as long as such solution exists. Furthermore, the synthesis procedure is applied incrementally to small code regions - one at a time - as opposed to the entire program, which is crucial for scaling the method to programs of realistic size and complexity. We have implemented our method in a software tool based on the Clang/LLVM compiler and the Yices SMT solver. Our experiments, conducted on a set of representative benchmarks from embedded control and DSP applications, show that the method is both effective and efficient in optimizing fixed-point arithmetic computations in embedded software code.\"",
        "1 is \"SAT-based sequential depth computation\", 2 is \"Design Comparison to Identify Malicious Hardware in External Intellectual Property\"",
        "Given above information, for an author who has written the paper with the title \"Interlocking obfuscation for anti-tamper hardware\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007596": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Surface modeling using multi-view range and color images':",
        "Document: \"Low-Light Image Enhancement Using Adaptive Digital Pixel Binning. This paper presents an image enhancement algorithm for low-light scenes in an environment with insufficient illumination. Simple amplification of intensity exhibits various undesired artifacts: noise amplification, intensity saturation, and loss of resolution. In order to enhance low-light images without undesired artifacts, a novel digital binning algorithm is proposed that considers brightness, context, noise level, and anti-saturation of a local region in the image. The proposed algorithm does not require any modification of the image sensor or additional frame-memory; it needs only two line-memories in the image signal processor (ISP). Since the proposed algorithm does not use an iterative computation, it can be easily embedded in an existing digital camera ISP pipeline containing a high-resolution image sensor.\"",
        "Document: \"Active shape model-based object tracking in panoramic video. Active Shape Model (ASM) paradigm is a popular method for image segmentation where a priori information about the shape of the object of interest is available. The effectiveness of the method is contingent upon a correct correspondence between model points and the features extracted from the image. Extensive application of these models soon revealed one of their limitations when, for a given model point, no obvious salient point can be found in the image. The primary cause of such limitation is due to weak edges and presence of abrupt noise which is the case with low light surveillance video images. In this paper we propose a fusion-based panoramic tracking algorithm of in low light images using multiple sensors. The proposed algorithm uses an IR and CCD sensor for image capture. The proposed tracking system consists of three steps: (i) pyramid based fusion algorithm, (ii) reconstruction of panoramic image, and (iii) active shape model (ASM)-based tracking algorithm. The experimental results show that the proposed tracking system can robustly extract and track objects on panoramic images in real-time.\"",
        "Document: \"Regularized image restoration by means of fusion for digital auto focusing. This paper proposes a novel digital auto-focusing algorithm using image fusion, which restores an image with out-of-focus objects. Instead of designing an image restoration filter for auto-focusing, we propose an image fusion-based auto-focusing algorithm by fusing multiple, restored images based on regularized iterative restoration. The proposed auto-focusing algorithm consists of (i) sum-modified-Laplacian (SML) for obtaining salient focus measure, (ii) iterative image restoration, (iii) auto focusing error metric (AFEM) for optimal restoration(iv) soft decision fusion and blending (SDFB) which enables smooth transition across region boundaries. By utilizing restored images at consecutive levels of iteration, the soft decision fusion and blending algorithm can restore images with multiple, out-of-focus objects. An auto-focusing error metric is used to provide an appropriate termination point for iterative restoration.\"",
        "Document: \"Multi-object digital auto-focusing using image fusion. This paper proposes a novel digital auto-focusing algorithm using image fusion, which restores an out-of-focus image with multiple, differently out-of-focus objects. The proposed auto-focusing algorithm consists of (i) building a prior set of point spread functions (PSFs), (ii) image restoration, and (iii) fusion of the restored images. Instead of designing an image restoration filter for multi-object auto-focusing, we propose an image fusion-based auto-focusing algorithm by fusing multiple, restored images based on prior estimated set of PSFs. The prior estimated PSFs overcome heavy computational overhead and make the algorithm suitable for real-time applications. By utilizing both redundant and complementary information provided by different images, the proposed fusion algorithm can restore images with multiple, out-of-focus objects. Experimental results show the performance of the proposed auto-focusing algorithm.\"",
        "Document: \"Color active shape models for tracking non-rigid objects. Active shape models can be applied to tracking non-rigid objects in video image sequences. Traditionally these models do not include color information in their formulation. In this paper, we present a hierarchical realization of an enhanced active shape model for color video tracking and we study the performance of both hierarchical and nonhierarchical implementations in the RGB, YUV, and HSI color spaces.\"",
        "1 is \"Facial expression recognition from line-based caricatures\", 2 is \"Efficient nonlocal means for denoising of textural patterns.\"",
        "Given above information, for an author who has written the paper with the title \"Surface modeling using multi-view range and color images\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007604": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A dynamic programming model for text segmentation based on min-max similarity':",
        "Document: \"Multi-class bootstrapping learning aspect-related terms for aspect identification. Aspect identification in entity reviews involving multiple aspects is a top priority for aspect-based opinion mining. Most of previous studies adopted machine learning techniques taking it as a multi-class text classification task. However, since building labeled training data is often expensive, some researchers put more interest in unsupervised techniques. With the subject of online restaurant reviews, this paper presents a new multi-class bootstrapping algorithm to learn Aspect-related terms to be used for aspect identification. Experimental results demonstrate that our method without requiring labeled training data achieves good performance in comparison to the state-of-the-art supervised learning techniques.\"",
        "Document: \"Divergence-based feature selection for na\u00efve Bayes text classification. A new divergence-based approach to feature selection for naive Bayes text classification is proposed in this paper. In this approach, the discrimination power of each feature is directly used for ranking various features through a criterion named overall-divergence, which is based on the divergence measures evaluated between various class density function pairs. Compared with other state-of-the-art algorithms (e.g. IG and CHI), the proposed approach shows more discrimination power for classifying confusing classes, and achieves better or comparable performance on evaluation data sets.\"",
        "Document: \"A reranking method for syntactic parsing with heterogeneous treebanks. In the field of natural language processing (NLP), there often exist multiple corpora with different annotation standards for the same task. In this paper, we take syntactic parsing as a case study and propose a reranking method which is able to make direct use of disparate tree banks simultaneously without using techniques such as tree bank conversion. The method proceeds in three steps: 1) build parsers on individual treebanks; 2) use parsers independently to generate n-best lists for each sentence in test set; 3) rerank individual n-best lists which correspond to the same sentence by using consensus information exchanged among these n-best lists. Experimental results on two open Chinese treebanks show that our method significantly outperforms the baseline system by 0.84% and 0.53% respectively. \u00a92010 IEEE.\"",
        "Document: \"Improving decoding generalization for tree-to-string translation. To address the parse error issue for tree-to-string translation, this paper proposes a similarity-based decoding generation (SDG) solution by reconstructing similar source parse trees for decoding at the decoding time instead of taking multiple source parse trees as input for decoding. Experiments on Chinese-English translation demonstrated that our approach can achieve a significant improvement over the standard method, and has little impact on decoding speed in practice. Our approach is very easy to implement, and can be applied to other paradigms such as tree-to-tree models.\"",
        "Document: \"Source Segment Encoding for Neural Machine Translation. Sequential word encoding lacks explicit representations of structural dependencies (e.g. tree, segment) over the source words in neural machine translation. Instead of using source syntax, in this paper we propose a source segment encoding (SSE) approach to modeling source segments in encoding process by two methods. One is to encode off-the-shelf n-grams of the source sentence into original source memory. The other is to jointly learn an optimal segmentation model with the translation model in an end-to-end manner without any supervision of segmentation. Experimental results show that the SSE method yields an improvement of 2.1+ BLEU points over the baselines on the Chinese-English translation task.\"",
        "1 is \"Normalization and shape recognition of three-dimensional objects by 3D moments\", 2 is \"Discriminative corpus weight estimation for machine translation\"",
        "Given above information, for an author who has written the paper with the title \"A dynamic programming model for text segmentation based on min-max similarity\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007624": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'The Opportunity of Grid Services for CSCL-Application Development':",
        "Document: \"Social Structures Representations as Aid for Effective Creation and Reuse of CSCL Scripts According to a Problem-Solving Approach to ID. Through the past two decades, education in computer and information science and technology (CISE) has moved toward a model that pervasively incorporates programming projects and other engaging student activity. Realistic activities and materials are ...\"",
        "Document: \"Identifying Design Principles for Learning Design Tools: The Case of edCrumble. Despite the existing variety of learning design tools, there is a gap in their understanding and adoption by the educators in their everyday practices. Sharing is one of the main pillars of learning design but sometimes it is not a sufficient reason to convince teachers to adopt the habit of documenting their practices so they can be shared. This study presents the design principles of edCrumble, an online learning design platform that allow teachers the creation and sharing of blended learning designs with the support of data analytics. The design principles have been learned and extracted from a participatory design process with teachers during the conceptualization and ongoing development of the tool. Several workshops including interviews were carried out as part of a design-based research iteration process. Later analysis has been done to extract and highlight those design principles aiming informing the development of learning design tools towards better learning design adoption.\"",
        "Document: \"Towards an Integrated Learning Design Environment. Learning design research focuses on how educators can act as designers of technology-supported learning activities according to their specific educational needs and objectives. To foster and sustain the adoption of Learning design, the METIS project is working towards the implementation of an Integrated Learning Design Environment (ILDE). This paper presents the vision for the ILDE and how user requirements from three educational institutions in vocational training, higher and adult education justify the need for this vision. The paper discusses the data collected in questionnaires, on-line interviews and face-to-face group work with the end-users, as a first phase in the design-based research methodology applied in the project. The results support a vision towards an ILDE that enables teachers to choose among multiple learning design authoring tools, (co-) produce, explore, share, evaluate and implement learning designs in Virtual Learning Environments. The paper also outlines a roadmap to achieve this vision.\"",
        "Document: \"LdShake: Learning design solutions sharing and co-edition. Two important challenges that teachers are currently facing are the sharing and the collaborative authoring of their learning design solutions, such as didactical units and learning materials. On the one hand, there are tools that can be used for the creation of design solutions and only some of them facilitate the co-edition. However, they do not incorporate mechanisms that support the sharing of the designs between teachers. On the other hand, there are tools that serve as repositories of educational resources but they do not enable the authoring of the designs. In this paper we present LdShake, a web tool whose novelty is focused on the combined support for the social sharing and co-edition of learning design solutions within communities of teachers. Teachers can create and share learning designs with other teachers using different access rights so that they can read, comment or co-edit the designs. Therefore, each design solution is associated to a group of teachers able to work on its definition, and another group that can only see the design. The tool is generic in that it allows the creation of designs based on any pedagogical approach. However, it can be particularized in instances providing pre-formatted designs structured according to a specific didactic method (such as Problem-Based Learning, PBL). A particularized LdShake instance has been used in the context of Human Biology studies where teams of teachers are required to work together in the design of PBL solutions. A controlled user study, that compares the use of a generic LdShake and a Moodle system, configured to enable the creation and sharing of designs, has been also carried out. The combined results of the real and controlled studies show that the social structure, and the commenting, co-edition and publishing features of LdShake provide a useful, effective and usable approach for facilitating teachers' teamwork.\"",
        "Document: \"Considering the Intrinsic Constraints for Groups Management of TAPPS and Jigsaw CLFPs. When applying a Collaborative Learning Flow Pattern (CLFP) to structure the sequence of activities in a real context, one of the tasks is to organize groups of students according to the constraints imposed by the pattern. Sometimes,unexpected events occurring on runtime force this pre-defined distribution to be changed. In such situations, a re-definition of the group structure for adjusting the activity to the actual context is needed. This paper proposes a flexible solution for supporting teachers in the group organization and adaptation profiting from the intrinsic constraints defined by a CLFPs codified in IMS Learning Design. A prototype of a web-based tool for the TAPPS and Jigsaw CLFPs and the preliminary results of a controlled user study are also presented as a first step towards flexible solutions in this context.\"",
        "1 is \"Zydeco: using mobile and web technologies to support seamless inquiry between museum and school contexts\", 2 is \"Supporting Community Awareness with Public Shared Displays\"",
        "Given above information, for an author who has written the paper with the title \"The Opportunity of Grid Services for CSCL-Application Development\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007635": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'IOPro: a parallel I/O profiling and visualization framework for high-performance storage systems':",
        "Document: \"Scaling parallel I/O performance through I/O delegate and caching system. Increasingly complex scientific applications require massive parallelism to achieve the goals of fidelity and high computational performance. Such applications periodically offload checkpointing data to file system for post-processing and program resumption. As a side effect of high degree of parallelism, I/O contention at servers doesn't allow overall performance to scale with increasing number of processors. To bridge the gap between parallel computational and I/O performance, we propose a portable MPI-IO layer where certain tasks, such as file caching, consistency control, and collective I/O optimization are delegated to a small set of compute nodes, collectively termed as I/O Delegate nodes. A collective cache design is incorporated to resolve cache coherence and hence alleviates the lock contention at I/O servers. By using popular parallel I/O benchmark and application I/O kernels, our experimental evaluation indicates considerable performance improvement with a small percentage of compute resources reserved for I/O.\"",
        "Document: \"Reducing I/O variability using dynamic I/O path characterization in petascale storage systems. In petascale systems with a million CPU cores, scalable and consistent I/O performance is becoming increasingly difficult to sustain mainly because of I/O variability. The I/O variability is caused by concurrently running processes/jobs competing for I/O or a RAID rebuild when a disk drive fails. We present a mechanism that stripes across a selected subset of I/O nodes with the lightest workload at runtime to achieve the highest I/O bandwidth available in the system. In this paper, we propose a probing mechanism to enable application-level dynamic file striping to mitigate I/O variability. We implement the proposed mechanism in the high-level I/O library that enables memory-to-file data layout transformation and allows transparent file partitioning using subfiling. Subfiling is a technique that partitions data into a set of files of smaller size and manages file access to them, making data to be treated as a single, normal file to users. We demonstrate that our bandwidth probing mechanism can successfully identify temporally slower I/O nodes without noticeable runtime overhead. Experimental results on NERSC's systems also show that our approach isolates I/O variability effectively on shared systems and improves overall collective I/O performance with less variation.\"",
        "Document: \"Design and evaluation of database layouts for MEMS-based storage systems. MEMS-based storage systems have recently generated significant interest due to their potential to be faster and more efficient than disks, while providing the non-volatility property. Designing data layouts for these devices is a challenging, important and interesting problem. In this paper, we explore various ways of placing a database on a MEMS-based storage architecture. Three novel data layouts are proposed after considering the MEMS device characteristics and the access patterns arising from queries. We then design the access methodology for each layout and evaluate these layouts based on their respective I/O service times. Overall, our results were able to identify the intricacies of placing data on a MEMS-based storage and also ascertain the large potential of MEMS-based devices for databases.\"",
        "Document: \"Materials Discovery: Understanding Polycrystals from Large-Scale Electron Patterns. This paper explores the idea of modeling a large image data collection of polycrystal electron patterns, in order to detect insights in understanding materials discovery. There is an emerging interest in applying big data processing, management and modeling methods to scientific images, which often come in a form and with patterns only interpretable to domain experts. While large-scale machine learning approaches have demonstrated certain superiority in analyzing, summarizing, and providing an understandable route to data types like natural images, speeches and texts, scientific images is still a relatively unexplored area. Deep convolutional neural networks, despite their recent triumph in natural image understanding, are still rarely seen adapted to experimental microscopic images, especially in a large scale. To the best of our knowledge, we present the first deep learning solution towards a scientific image indexing problem using a collection of over 300K microscopic images. The result obtained is 54% better than a dictionary lookup method which is state-of-theart in the materials science society\"",
        "Document: \"Data Compression for the Exascale Computing Era - Survey. AbstractWhile periodic checkpointing has been an important mechanism for tolerating faults in high performance computing HPC systems, it is cost-prohibitive as the HPC system approaches exascale. Applying compression techniques is one common way to mitigate such burdens by reducing the data size, but they are often found to be less effective for scientific datasets. Traditional lossless compression techniques that look for repeated patterns are ineffective for scientific data in which high-precision data is used and hence common patterns are rare to find. In this paper, we present a comparison of several lossless and lossy data compression algorithms and discuss their methodology under the exascale environment. As data volume increases, we discover an increasing trend of new domain-driven algorithms that exploit the inherent characteristics exhibited in many scientific dataset, such as relatively small changes in data values from one simulation iteration to the next or among neighboring data. In particular, significant data reduction has been observed in lossy compression. This paper also discusses how the errors introduced by lossy compressions are controlled and the tradeoffs with the compression ratio.\"",
        "1 is \"Generalized symbolic execution for model checking and testing\", 2 is \"A Foundational Approach to Mining Itemset Utilities from Databases\"",
        "Given above information, for an author who has written the paper with the title \"IOPro: a parallel I/O profiling and visualization framework for high-performance storage systems\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007660": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of '3D model reconstruction using neural gas accelerated on GPU':",
        "Document: \"Automatic Schaeffer's gestures recognition system. Schaeffer's sign language consists of a reduced set of gestures designed to help children with autism or cognitive learning disabilities to develop adequate communication skills. Our automatic recognition system for Schaeffer's gesture language uses the information provided by an RGB-D camera to capture body motion and recognize gestures using dynamic time warping combined with k-nearest neighbors methods. The learning process is reinforced by the interaction with the proposed system that accelerates learning itself thus helping both children and educators. To demonstrate the validity of the system, a set of qualitative experiments with children were carried out. As a result, a system which is able to recognize a subset of 11 gestures of Schaeffer's sign language online was achieved.\"",
        "Document: \"Region analysis through close contour transformation using growing neural gas. In our work we aim to explore a general framework that addresses the fundamental problem of universal unsupervised extraction of semantically meaningful visual regions. To this end this paper describes a novel region analysis technique using a self-organising map, the growing neural gas, which is adapted so as to improve modelling speed as well as to ensure a double-linkage chain around all region contours to simplify shape analysis. While the growing neural gas has been extensively applied to shape modelling, it has never explicitly been used for curvature analysis, contour description and region similarity. Once a contour network has been obtained, a transformation is applied that converts the closed contour to an open one, facilitating the use of certain angular descriptors. Discriminative descriptors derived from the properties of regions, their contours and their transformed contours are established and define a feature vector used for the representation of regions based on the appearance and contour information.\"",
        "Document: \"Natural User Interfaces in Volume Visualisation Using Microsoft Kinect. This paper presents the integration of human-machine interaction technologies within a virtual reality environment to allow for real-time manipulation of 3D objects using different gestures. We demonstrate our approach by developing a fully operational, natural user interface (NUI) system, which provides a front-end framework for back-end applications that use more traditional forms of input, such as wear cable sensors attached to the users. The implementation is a user-friendly system that has immense potential in a number of fields, especially in the medical sciences where it would be possible to increase the productivity of surgeons by providing them with easy access to relevant MRI scans.\"",
        "Document: \"Measuring GNG topology preservation in computer vision applications. Self-organizing neural networks try to preserve the topology of an input space by means of their competitive learning. This capacity has been used, among others, for the representation of objects and their motion. In addition, these applications usually have real-time constraints. In this work we have study a kind of self-organizing network, the Growing Neural Gas with different parameters, to represent different objects. In some cases, topology preservation is lost and, therefore, the quality of the representation. So, we have made a study to quantify topology preservation to establish the most suitable learning parameters, depending on the kind of objects to represent and the size of the network.\"",
        "Document: \"3D model reconstruction using neural gas accelerated on GPU. Graphical abstractDisplay Omitted HighlightsA 3D reconstruction model method based on neural gases (NG).The method can be used for reverse engineering purposes.NG reconstruction deal with noisy low cost 3D sensor acquisitions.3D models integration in design and manufacturing virtual environments.Parallelization/acceleration onto GPUs is provided. In this work, we propose the use of the neural gas (NG), a neural network that uses an unsupervised Competitive Hebbian Learning (CHL) rule, to develop a reverse engineering process. This is a simple and accurate method to reconstruct objects from point clouds obtained from multiple overlapping views using low-cost sensors. In contrast to other methods that may need several stages that include downsampling, noise filtering and many other tasks, the NG automatically obtains the 3D model of the scanned objects. To demonstrate the validity of our proposal we tested our method with several models and performed a study of the neural network parameterization computing the quality of representation and also comparing results with other neural methods like growing neural gas and Kohonen maps or classical methods like Voxel Grid. We also reconstructed models acquired by low cost sensors that can be used in virtual and augmented reality environments for redesign or manipulation purposes. Since the NG algorithm has a strong computational cost we propose its acceleration. We have redesigned and implemented the NG learning algorithm to fit it onto Graphics Processing Units using CUDA. A speed-up of 180\u00ed faster is obtained compared to the sequential CPU version.\"",
        "1 is \"Statistical shape models for 3D medical image segmentation: a review.\", 2 is \"Scene recognition with omnidirectional vision for topological map using lightweight adaptive descriptors\"",
        "Given above information, for an author who has written the paper with the title \"3D model reconstruction using neural gas accelerated on GPU\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007769": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Game theoretic analysis of multiparty access control in online social networks':",
        "Document: \"Online Strategizing Distributed Renewable Energy Resource Access in Islanded Microgrids. The smart grid, perceived as the next generation power grid, uses two-way flow of electricity and information to create a widely distributed automated energy delivery network. By grouping distributed renewable energy generations and loads, a microgrid, which is seen as one of the cornerstones of the future smart grids, can disconnect from the macrogrid and function autonomously. This intentional islanding of generations and loads has the potential to provide a higher local reliability than that provided by the power system as a whole. One of the fundamental issues for a user in an islanded microgrid is how to find the one among the distributed renewable energy resources (DRERs) in a microgrid, which can supply the power most efficiently, effectively and reliably, as its power supply source. This problem is difficult since the power pattern of renewable resources, such as wind and solar, is variable and generally speaking is not easy to accurately predict. In order to solve this problem, we first propose a distributed DRER discovery approach to discover all the available DRERs within a microgrid. Furthermore, based on the online machine learning theory, we propose two distributed algorithms according to the information the user can obtain, in order to compute a good DRER access strategy, with no assumption on what distribution the power patterns of the DRERs follow. We prove that when the time horizon is sufficiently large, on average the upper bound on the gap between the expected profit obtained at each time slot by using the global optimal strategy and that by using our algorithms is arbitrarily small.\"",
        "Document: \"Incentive Mechanisms for Crowdsensing: Crowdsourcing With Smartphones. Smartphones are programmable and equipped with a set of cheap but powerful embedded sensors, such as accelerometer, digital compass, gyroscope, GPS, microphone, and camera. These sensors can collectively monitor a diverse range of human activities and the surrounding environment. Crowdsensing is a new paradigm which takes advantage of the pervasive smartphones to sense, collect, and analyze data beyond the scale of what was previously possible. With the crowdsensing system, a crowdsourcer can recruit smartphone users to provide sensing service. Existing crowdsensing applications and systems lack good incentive mechanisms that can attract more user participation. To address this issue, we design incentive mechanisms for crowdsensing. We consider two system models: the crowdsourcer-centric model where the crowdsourcer provides a reward shared by participating users, and the user-centric model where users have more control over the payment they will receive. For the crowdsourcer-centric model, we design an incentive mechanism using a Stackelberg game, where the crowdsourcer is the leader while the users are the followers. We show how to compute the unique Stackelberg Equilibrium, at which the utility of the crowdsourcer is maximized, and none of the users can improve its utility by unilaterally deviating from its current strategy. For the user-centric model, we design an auction-based incentive mechanism, which is computationally efficient, individually rational, profitable, and truthful. Through extensive simulations, we evaluate the performance and validate the theoretical properties of our incentive mechanisms.\"",
        "Document: \"Enabling Green Mobile Crowd Sensing via Optimized Task Scheduling on Smartphones. In a mobile crowd sensing system, a smartphone undertakes many different sensing tasks that demand data from various sensors. In this paper, we consider the problem of scheduling different sensing tasks assigned to a smartphone with the objective of minimizing sensing energy consumption while ensuring Quality of SenSing (QoSS). First, we consider a simple case in which each sensing task only requests data from a single sensor. We formally define the corresponding problem as the Minimum Energy Single-sensor task Scheduling (MESS) problem and present a polynomial-time optimal algorithm to solve it. Furthermore, we address a more general case in which some sensing tasks request multiple sensors to report their measurements simultaneously. We present an Integer Linear Programming (ILP) formulation as well as an effective polynomial-time heuristic algorithm, for the corresponding Minimum Energy Multi-sensor task Scheduling (MEMS) problem. Extensive simulation results show that the proposed algorithms achieve over 79% energy savings on average compared to a widely-used baseline approach, and moreover, the proposed heuristic algorithm produces close-to-optimal solutions.\"",
        "Document: \"Truthful auction for cooperative communications. On one hand, cooperative communication has been gaining more and more popularity since it has great potential to increase the capacity of wireless networks. On the other hand, the applications of cooperative communication technology are rarely seen in reality, even in some scenarios where the demands for bandwidth-hungry applications have pushed the system designers to develop innovative network solutions. A main obstacle lying between the potential capability of channel capacity improvement and the wide adoption of cooperative communication is the lack of incentives for the participating wireless nodes to serve as relay nodes. Hence, in this paper, we design TASC, an auction scheme for the cooperative communications, where wireless node can trade relay services. TASC makes an important contribution of maintaining truthfulness while fulfilling other design objectives. We show analytically that TASC is truthful and has polynomial time complexity. Extensive experiments show that TASC can achieve multiple economic properties without significant performance degradation compared with pure relay assignment algorithms.\"",
        "Document: \"Consort: Node-Constrained Opportunistic Routing in wireless mesh networks. Opportunistic routing is proposed to improve the performance of wireless networks by exploiting the broadcast nature and spatial diversity of the wireless medium. In this paper, we study the problems of how to choose opportunistic route for each user to optimize the total utility or profit of multiple simultaneous users in a wireless mesh network (WMN) subject to node constraints. We formulate these two problems as two convex programming systems. By combining primal-dual and subgradient methods, we present a distributed iterative algorithm Consort (node-Constrained Opportunistic Routing). In each iteration, Consort updates Lagrange multipliers in a distributed manner according to the user and node behaviors obtained in the previous iteration, and then each user and each node individually adjusts its own behavior based on the updated Lagrange multipliers. We prove the convergence of this iterative algorithm, and provide bounds on the amount of feasibility violation and the gap between our solution and the optimal solution in each iteration.\"",
        "1 is \"An improved FPTAS for restricted shortest path\", 2 is \"Automated analysis of security-design models\"",
        "Given above information, for an author who has written the paper with the title \"Game theoretic analysis of multiparty access control in online social networks\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007865": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Adaptive Transmission Rate With a Fixed Threshold Decoder for Diffusion-Based Molecular Communication.':",
        "Document: \"Frame time-hopping patterns in multirate optical CDMA networks using conventional and multicode schemes. A class of generalized optical orthogonal codes (OOCs), namely, frame time-hopping (FTH) patterns with an extremely large cardinality, are studied for implementing multirate and multiservice (MR/MS) optical CDMA (OCDMA) networks. Conventional MR/MS methods, namely variable spreading rate and parallel mapping, are considered. Using FTH patterns, the problem of low OOC code cardinality in convention...\"",
        "Document: \"Convolutional Network-Coded Cooperation in Multi-Source Networks With a Multi-Antenna Relay. We propose a novel cooperative transmission scheme called Convolutional Network-Coded Cooperation (CNCC) for a network including N sources, one M-antenna relay, and one common destination. The source-relay (S-R) channels are assumed to be Nakagami-m fading, while the source-destination (S-D) and the relay-destination (R-D) channels are considered Rayleigh fading. The CNCC scheme exploits the generator matrix of a good (N + M', N,v) systematic convolutional code, with the free distance of dfree designed over GF(2), as the network coding matrix which is run by the network's nodes, such that the systematic symbols are directly transmitted from the sources, and the parity symbols are sent by the best antenna of the relay. An upper bound on the BER of the sources, and consequently, the achieved diversity orders are obtained. The numerical results indicate that the CNCC scheme outperforms the other cooperative schemes considered, in terms of the diversity order and the network throughput. The simulation results confirm the accuracy of the theoretical analysis.\"",
        "Document: \"Impact of Cognition and Cooperation on MAC Layer Performance Metrics, Part I: Maximum Stable Throughput. In this paper, we consider a broadband secondary transmitter-receiver pair which interferes with N narrowband primary users and study the effect of cognition and cooperation on the maximum stable throughput. In our study we focus on four transmission protocols as well as two channel types, i.e., flat fading and frequency selective fading. In the cooperative protocols, the broadband transmitter rel...\"",
        "Document: \"Stochastic Geometry Modeling and Analysis of Single- and Multi-Cluster Wireless Networks. This paper develops a stochastic geometry-based approach for the modeling and analysis of single- and multi-cluster wireless networks. We first define finite homogeneous Poisson point processes to model the number and locations of the transmitters in a confined region as a single-cluster wireless network. We study the coverage probability for a reference receiver for two strategies; closest-select...\"",
        "Document: \"Adaptive Transmission Rate With a Fixed Threshold Decoder for Diffusion-Based Molecular Communication. In this paper, a simple memory limited transmitter for molecular communication is proposed, in which information is encoded in the emission rate of the molecules. Taking advantage of memory, the proposed transmitter reduces the ISI problem by properly adjusting its emission rate, which can be interpreted as water-filling on the expected interference. The error probability of the proposed scheme is...\"",
        "1 is \"Cognitive Multiple Access Networks\", 2 is \"An achievable rate region for the broadcast channel\"",
        "Given above information, for an author who has written the paper with the title \"Adaptive Transmission Rate With a Fixed Threshold Decoder for Diffusion-Based Molecular Communication.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007948": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Entire colouring of plane graphs':",
        "Document: \"An improved bound on parity vertex colourings of outerplane graphs. A parity vertex colouring of a 2-connected plane graph G is a proper vertex colouring such that for each face f and colour i, either zero or an odd number of vertices incident with f are coloured i. The parity chromatic number \u03c7p(G) of G is the smallest number of colours used in a parity vertex colouring of G.\"",
        "Document: \"The L(2, 1)-labelling of trees. An L(2, 1)-labelling of a graph G is an assignment of nonnegative integers to the vertices of G such that adjacent vertices have numbers at least 2 apart, and vertices at distance 2 have distinct numbers. The L (2, 1)-labelling number \u03bb(G) of G is the minimum range of labels over all such labellings. It was shown by Griggs and Yeh [Labelling graphs with a condition at distance 2, SIAM J. Discrete Math. 5 (1992) 586-595] that every tree T has \u0394 + 1 \u2264 \u03bb(T)\u2264\u0394 + 2. This paper provides a sufficient condition for \u03bb(T) = \u0394 + 1. Namely, we prove that if a tree T contains no two vertices of maximum degree at distance either 1, 2, or 4, then \u03bb(T) = \u0394 + 1. Examples of trees T with two vertices of maximum degree at distance 4 such that \u03bb(T) = \u0394 + 2 are constructed.\"",
        "Document: \"On backbone coloring of graphs. Let G be a graph and H a subgraph of G. A backbone-k-coloring of (G,H) is a mapping f: V(G)\u9a74{1,2,\u9a74,k} such that |f(u)\u9a74f(v)|\u9a742 if uv\u9a74E(H) and |f(u)\u9a74f(v)|\u9a741 if uv\u9a74E(G)\\E(H). The backbone chromatic number of (G,H) is the smallest integer k such that (G,H) has a backbone-k-coloring. In this paper, we characterize the backbone chromatic number of Halin graphs G=T\u9a74C with respect to given spanning trees T. Also we study the backbone coloring for other special graphs such as complete graphs, wheels, graphs with small maximum average degree, graphs with maximum degree 3, etc.\"",
        "Document: \"The 2-surviving rate of planar graphs without 4-cycles. Let G be a connected graph with n=2 vertices. Suppose that a fire breaks out at a vertex v of G. A firefighter starts to protect vertices. At each time interval, the firefighter protects two vertices not yet on fire. At the end of each time interval, the fire spreads to all the unprotected vertices that have a neighbour on fire. Let sn\"2(v) denote the maximum number of vertices in G that the firefighter can save when a fire breaks out at vertex v. The surviving rate @r\"2(G) of G is defined to be @?\"v\"@?\"V\"(\"G\")sn\"2(v)/n^2, which is the average proportion of saved vertices. In this paper, we show that if G is a planar graph with n=2 vertices and without 4-cycles, then @r\"2(G)176.\"",
        "Document: \"The surviving rate of an outerplanar graph for the firefighter problem. Let G be a connected graph with n=2 vertices. Let k=1 be an integer. Suppose that a fire breaks out at a vertex v of G. A firefighter starts to protect vertices. At each time interval, the firefighter protects k-vertices not yet on fire. At the end of each time interval, the fire spreads to all the unprotected vertices that have a neighbour on fire. Let sn\"k(v) denote the maximum number of vertices in G that the firefighter can save when a fire breaks out at vertex v. The k-surviving rate @r\"k(G) of G is defined to be @?\"v\"@?\"V\"(\"G\")sn\"k(v)/n^2, which is the average proportion of saved vertices. In this paper, we investigate the surviving rate of outerplanar graphs G with n=2 vertices. The main results are as follows: (1) lim\"n\"-\"~@r\"5(G)=1; and (2) @r\"1(G)=4381-53n+3n^2 if n=8, and @r\"1(G)=13 if n=2, which improves the result in [L.Cai, W.Wang, The surviving rate of a graph for the firefighter problem, SIAM J. Discrete Math. 23 (2009) 1814-1826].\"",
        "1 is \"A survey on labeling graphs with a condition at distance two\", 2 is \"Acyclic 3-choosability of sparse graphs with girth at least 7\"",
        "Given above information, for an author who has written the paper with the title \"Entire colouring of plane graphs\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008001": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Verification of User Interface Software: The Example of Use-Related Safety Requirements and Programmable Medical Devices.':",
        "Document: \"Hierarchical formal verification using a hybrid tool. . \u00a0\u00a0We describe a hybrid formal hardware verification tool that links the HOL interactive proof system and the MDG automated hardware\n verification tool. It supports a hierarchical verification approach that mirrors the hierarchical structure of designs. We\n obtain the advantages of both verification paradigms. We illustrate its use by considering a component of a communications\n chip. Verification with the hybrid tool is significantly faster and more tractable than using either tool alone.\"",
        "Document: \"A Socio-technical Methodology for the Security and Privacy Analysis of Services. There is a widely accepted need for methodologies to verify the security of services. A typical service requires user data and then makes them available through the Internet independently from access platforms or user locations, but the layman is rarely aware of the entailed risks and seldom acts cautiously. The combined human-and-technology system is complex: it intertwines the technical protocols that establish the technical security properties, with the social protocols that regulate human attitudes to and behaviour with computers. A number of security threats are therefore inherently socio-technical. % An appropriate methodology to tackle security of web services from a socio-technical standpoint, namely when the human is in the loop, is still missing. This paper introduces one, termed the ceremony concertina traversal methodology. It advocates that technology is analysed in the presence of the human through the various structural layers that arise, from computer processes to user personas. Layers should be analysed individually then in combination, so as to transmit the guarantees that the technology is sound to its users in practical scenarios.\"",
        "Document: \"Comparing HOL and MDG: a case study on the verification of an ATM switch fabric. Interactive formal proof and automated verification based on decision graphs are two contrasting formal hardware verification techniques. In this paper, we compare these two approaches. In particular, we consider HOL and MDG. The former is an interactive theorem-proving system based on higher-order logic, while the latter is an automatic system based on Multiway Decision Graphs. As the basis for our comparison we have used both systems to independently verify a fabricated ATM communications chip, the Fairisle 4 by 4 switch fabric.\"",
        "Document: \"Detecting Multiple Classes of User Errors. Systematic user errors commonly occur in the use of interactive systems. We describe a formal reusable user model implemented in higher-order logic that can be used for machine-assisted reasoning about user errors. The core of this model is a series of non-deterministic guarded temporal rules. We consider how this approach allows errors of various specific kinds to be detected and so avoided by proving a single theorem about an interactive system. We illustrate the approach using a simple case study.\"",
        "Document: \"EMU in the Car: Evaluating Multimodal Usability of a Satellite Navigation System. The design and evaluation of multimodal systems has traditionally been a craft skill. There are some well established heuristics, guidelines and frameworks for assessing multimodal interactions, but no established methodologies that focus on the design of the interaction between user and system in context. In this paper, we present EMU, a systematic evaluation methodology for reasoning about the usability of an interactive system in terms of the modalities of interaction. We illustrate its application using an example of in-car navigation. EMU fills a niche in the repertoire of analytical evaluation approaches by focusing on the quality of interaction in terms of the modalities of interaction, how modalities are integrated, and where there may be interaction breakdowns due to modality clashes, synchronisation difficulties or distractions.\"",
        "1 is \"Analysing Cognitive Behaviour using LOTOS and Mexitl\", 2 is \"Integrating user and computer system concerns in the design of interactive systems\"",
        "Given above information, for an author who has written the paper with the title \"Verification of User Interface Software: The Example of Use-Related Safety Requirements and Programmable Medical Devices.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008039": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Reduced Complexity Wavelet-Based Predictive Coding of Hyperspectral Images for FPGA Implementation':",
        "Document: \"Exploration of pipelined FPGA interconnect structures. In this work, we parameterize and explore the interconnect structure of pipelined FPGAs. Specifically, we explore the effects of interconnect register population, length of registered routing track segments, registered IO terminals of logic units, and the flexibility of the interconnect structure on the performance of a pipelined FPGA. Our experiments with the RaPiD [4] architecture identify tradeoffs that must be made while designing the interconnect structure of a pipelined FPGA. The post-exploration architecture that we found shows a 19% improvement over RaPiD, while the area overhead incurred in placing and routing benchmarks netlists on the post-exploration architecture is 18%.\"",
        "Document: \"Dataflow-driven execution control in a coarse-grained reconfigurable array (abstract only). Coarse Grained Reconfigurable Arrays (CGRAs) are a promising class of architectures for accelerating applications using a large number of parallel execution units for high throughput. While they are typically very efficient for a single task, all functional units are required to perform in lock step; this makes some classes of applications more difficult to program and efficiently use resources. Other architectures like Massively Parallel Processor Arrays (MPPAs) are better suited for these applications and excel at executing unrelated tasks simultaneously, but the amount of resources dedicated to a single task is limited. We are developing a new architecture with the design flexibility of an MPPA and the throughput of a CGRA. A key to the flexibility of MPPAs is the ability for subtasks to execute independently instead of in lock step with all other tasks on the array. Adding this capability requires special control circuitry for architectural support in a CGRA. We decribe the modifications required and our solutions. Additionally, we also describe the CAD tool modification and application developer concerns for utilizing the resulting hybrid CGRA/MPPA architecture.\"",
        "Document: \"Runtime and quality tradeoffs in FPGA placement and routing. Many applications of FPGAs, especially logic emulation and custom computing, require the quick placement and routing of circuit designs. In these applications, the advantages FPGA-based systems have over software simulation are diminished by the long run-times of current CAD software used to map the circuit onto FPGAs. To improve the run-time advantage of FPGA systems, users may be willing to trade some mapping quality for a reduction in CAD tool runtimes. In this paper, we seek to establish how much quality degradation is necessary to achieve a given runtime improvement. For this purpose, we implemented and investigated numerous placement and routing algorithms for FPGAs. We also developed new tradeoff-oriented algorithms, where a tuning parameter can be used to control this quality vs. runtime tradeoff. We show how different algorithms can achieve different points within this tradeoff spectrum, as well as how a single algorithm can be tuned to form a curve in the spectrum. We demonstrate that the algorithms vary widely in their tradeoffs, with the fastest algorithm being 8x faster than the slowest, and the highest quality algorithm being 5x better than the least quality algorithm. Compared to the commercial Xilinx CAD tools, we can achieve a 3x speed-up by allowing 1.27x degradation in quality, and a factor of 1.6x quality improvement with 2x slowdown.\"",
        "Document: \"The triptych FPGA architecture. Field-programmable gate arrays (FPGAs) are an important implementation medium for digital logic. Unfortunately, they currently suffer from poor silicon area utilization due to routing constraints. In this paper we present Triptych, an FPGA architecture designed to achieve improved logic density with competitive performance. This is done by allowing a per-mapping tradeoff between logic and routing resources, and with a routing scheme designed to match the structure of typical circuits. We show that, using manual placement, this architecture yields a logic density improvement of up to a factor of 3.5 over commercial FPGAs, with comparable performance. We also describe Montage, the first FPGA architecture to fully support asynchronous and synchronous interface circuits.\"",
        "Document: \"Offset Pipelined Scheduling: Conditional Branching for CGRAs. Coarse Grained Reconfigurable Arrays (CGRAs) offer improved energy efficiency and performance over conventional architectures. However, modulo counter based control of these devices limits efficiency for applications with multiple execution modes. This work presents a new type of architecture that adds support for branching control flow to CGRAs. The pipelined program counter CGRA framework blends the high parallelism of traditional CGRAs with the flexibility of commodity processors. Offset Pipelined Scheduling (OPS) is the basis of an enhanced CGRA tool chain targeting these devices. OPS is shown to provide an average 1.94x speed up for benchmarks that are resource limited when modulo scheduled.\"",
        "1 is \"A case study of partially evaluated hardware circuits: Key-specific DES\", 2 is \"Optimal surface reconstruction from planar contours\"",
        "Given above information, for an author who has written the paper with the title \"Reduced Complexity Wavelet-Based Predictive Coding of Hyperspectral Images for FPGA Implementation\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008122": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'ClassView: hierarchical video shot classification, indexing, and accessing':",
        "Document: \"Mining The Relation Between Sentiment Expression And Target Using Dependency Of Words. In some applications of opinion mining in text, it is important to distinguish what an author is talking about from the subjective stance towards the topic. Therefore, it needs to find the relation between sentiment expression and target. This paper proposes a novel method based on dependency grammars to mine the relation. In this method, the process of mining the relations is turned into a procedure of searching in the dependency tree of a sentence. The result of our experiments shows that word dependency relation based methods is more flexible and effective than some previous surface patterns based methods.http://aclweb.org/anthology/Y06-1034\"",
        "Document: \"Nearest Neighbor Discriminant Analysis. Linear Discriminant Analysis (LDA) is a popular feature extraction technique in statistical pattern recognition. However, it often suffers from the small sample size problem when dealing with high-dimensional data. Moreover, while LDA is guaranteed to find the best directions when each class has a Gaussian density with a common covariance matrix, it can fail if the class densities are more general. In this paper, a novel nonparametric linear feature extraction method, nearest neighbor discriminant analysis (NNDA), is proposed from the view of the nearest neighbor classification. NNDA finds the important discriminant directions without assuming the class densities belong to any particular parametric family. It does not depend on the nonsingularity of the within-class scatter matrix either. Then we give an approximate approach to optimize NNDA and an extension to k-NN. We apply NNDA to the simulated data and real world data, the results demonstrate that NNDA outperforms the existing variant LDA methods.\"",
        "Document: \"The use of metadata, web-derived answer patterns and passage context to improve reading comprehension performance. A reading comprehension (RC) system attempts to understand a document and returns an answer sentence when posed with a question. RC resembles the ad hoc question answering (QA) task that aims to extract an answer from a collection of documents when posed with a question. However, since RC focuses only on a single document, the system needs to draw upon external knowledge sources to achieve deep analysis of passage sentences for answer sentence extraction. This paper proposes an approach towards RC that attempts to utilize external knowledge to improve performance beyond the baseline set by the bag-of-words (BOW) approach. Our approach emphasizes matching of metadata (i.e. verbs, named entities and base noun phrases) in passage context utilization and answer sentence extraction. We have also devised an automatic acquisition process for Web-derived answer patterns (AP) which utilizes question-answer pairs from TREC QA, the Google search engine and the Web. This approach gave improved RC performances for both the Remedia and ChungHwa corpora, attaining HumSent accuracies of 42% and 69% respectively. In particular, performance analysis based on Remedia shows that relative performances of 20.7% is due to metadata matching and a further 10.9% is due to the application of Web-derived answer patterns.\"",
        "Document: \"Domain adaptation for conditional random fields. Conditional Random Fields (CRFs) have received a great amount of attentions in many fields and achieved good results. However, a case frequently encountered in practice is that the test data's domain is different with the training data's. It would affect negatively the performance of CRFs. This paper presents a novel technique for maximum a posteriori (MAP) adaptation of Conditional Random Fields model. The background model, which is trained on data from a domain, could be well adapted to a new domain with a small number of labeled domain specific data. Experimental results on tasks of chunking and capitalizing show that this technique can significantly improve performance on out-of-domain data. In chunking task, the relative improvement given by the adaptation technique is 56.9%. With two in-domain sentences, it also can achieve 30.2% relative improvement.\"",
        "Document: \"Folksonomy as a Complex Network. Folksonomy is an emerging technology that works to classify the in- formation over WWW through tagging the bookmarks, photos or other web-based contents. It is understood to be organized by every user while not limited to the authors of the contents and the professional editors. This study surveyed the folksonomy as a complex network. The result indicates that the network, which is composed of the tags from the folk- sonomy, displays both properties of small world and scale free. However, the statistics only shows a local and static slice of the vast body of folk- sonomy which is still evolving.\"",
        "1 is \"Efficient Computation of the Skyline Cube\", 2 is \"Semantic Indexing of Multimedia Content Using Visual, Audio, and Text Cues\"",
        "Given above information, for an author who has written the paper with the title \"ClassView: hierarchical video shot classification, indexing, and accessing\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008186": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Continuous outlier detection in data streams: an extensible framework and state-of-the-art algorithms':",
        "Document: \"Brief announcement: on the quest of optimal service ordering in decentralized queries. This paper deals with pipelined queries over services. The execution plan of such queries defines an order in which the services are called. We present the theoretical underpinnings of a newly proposed algorithm that produces the optimal linear ordering corresponding to a query being executed in a decentralized manner, i.e., when the services communicate directly with each other. The optimality is defined in terms of query response time, which is determined by the bottleneck service in the plan. The properties discussed in this work allow a branch-and-bound approach to be very efficient.\"",
        "Document: \"Decentralized execution of linear workflows over web services. The development of workflow management systems (WfMSs) for the effective and efficient management of workflows in wide-area infrastructures has received a lot of attention in recent years. Existing WfMSs provide tools that simplify the workflow composition and enactment actions, while they support the execution of complex tasks on remote computational resources usually through calls to web services (WSs). Nowadays, an increasing number of WfMSs employ pipelining during the workflow execution. In this work, we focus on improving the performance of long-running workflows consisting of multiple pipelined calls to remote WSs when the execution takes place in a totally decentralized manner. The novelty of our algorithm lies in the fact that it considers the network heterogeneity, and although the optimization problem becomes more complex, it is capable of finding an optimal solution in a short time. Our proposal is evaluated through a real prototype deployed on PlanetLab, and the experimental results are particularly encouraging.\"",
        "Document: \"GPU processing of theta-joins. The GPGPU paradigm has recently been employed to accelerate the processing of big amounts of data through the utilization of the massive parallelism offered by modern GPUs. To date, several techniques have been proposed for the implementation of simple select, aggregate, and equality join operations on GPUs. In this paper, we study the efficient implementation of theta-join queries between two relations using the CUDA framework. Theta-joins are notoriously slow and thus can benefit from massively parallel execution. However, their GPU-based implementation significantly differs from hash- and sort-based equality joins and needs to be carefully crafted. The implementation is driven by two main objectives. The first relates to the attainment of high efficiency in the parallelization through data reuse, which relates to the minimization of accesses to the slow global memory. The second is about the most efficient exploitation of the available memory given that, in general, it cannot hold the entire input and result. We propose a methodology for processing theta-joins on a GPU, which exploits the heterogeneous nature of GPGPU, while addressing memory limitations. Furthermore, we provide a series of implementation optimizations, which yield performance improvements of an order of magnitude.\"",
        "Document: \"Practical Adaptation to Changing Resources in Grid Query Processing. Grid computational resources, as well as being heterogeneous, may also exhibit unpredictable, volatile behaviour. Therefore, query processing on the Grid needs to be adaptive in order to cope with evolving resource characteristics, such as machine load and availability. To address this challenge in a Grid environment, the non-adaptive OGSA-DQP1 system described in [1] has been enhanced with adaptive capabilities.\"",
        "Document: \"Modeling Data Flow Execution in a Parallel Environment. Although the modern data flows are executed in parallel and distributed environments, e.g. on a multi-core machine or on the cloud, current cost models, e.g., those considered by state-of-the-art data flow optimization techniques, do not accurately reflect the response time of real data flow execution in these execution environments. This is mainly due to the fact that the impact of parallelism, and more specifically, the impact of concurrent task execution on the running time is not adequately modeled. In this work, we propose a cost modeling solution that aims to accurately reflect the response time of a data flow that is executed in parallel. We focus on the single multi-core machine environment provided by modern business intelligence tools, such as Pentaho Kettle, but our approach can be extended to massively parallel and distributed settings. The distinctive features of our proposal is that we model both time overlaps and the impact of concurrency on task running times in a combined manner; the latter is appropriately quantified and its significance is exemplified.\"",
        "1 is \"Efficient algorithms for mining outliers from large data sets\", 2 is \"Execution Time Estimation for Workflow Scheduling\"",
        "Given above information, for an author who has written the paper with the title \"Continuous outlier detection in data streams: an extensible framework and state-of-the-art algorithms\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008222": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Virtual lifeline: Multimodal sensor data fusion for robust navigation in unknown environments':",
        "Document: \"Effect of TNL Flow Control Schemes for the HSDPA Network Performance. HSDPA (High Speed Downlink Packet Access) is an extension of the current UMTS (Universal Mobile Telecommunications System) technology with the objective to increase the data rate and to reduce the latency in the downlink. The main focus of this investigation is to optimise the Iub interface to provide the best end user performance at minimum cost for UMTS HSDPA network. An adaptive credit-based flow control mechanism for UTRAN transport has been developed, tested and validated in the HSDPA simulation model. In this paper, the HSDPA performance with the new adaptive credit-based flow control mechanism is compared with the generic ON/OFF flow control mechanism. The results confirm that the adaptive credit- based flow control mechanism can significantly improve the performance and therefore reduce the required bandwidth at the Iub interface meeting a specified performance: The better bandwidth utilisation and improved statistical multiplexing is achieved by reducing the burstiness of the traffic over the Iub interface. Finally, a recommendation for the required bandwidth at the Iub interface is given for bursty HSDPA traffic. Index Terms\u2014HSDPA, RNC, Node-B, Flow Control\"",
        "Document: \"Resilient data gathering and communication algorithms for\u00a0emergency scenarios. A typical application field of Wireless Sensor Networks (WSNs) is the collection of environmental data, which is sent to a\n base station. Routing protocols are needed to efficiently direct the information flows to the base station. Since sensor nodes\n have strict energy constraints, data gathering and communication schemes for WSNs need to be designed for an efficient utilization\n of the available resources. An\u00a0emergency management scenario is investigated, where a sensor network is deployed as virtual\n lifeline for fire fighters entering a building. Besides of supporting their navigation, the virtual lifeline is also used\n for two further purposes. First it enables the exchange of short voice messages between fire fighter and command post. For\n this, a fast and reliable routing protocol (EMRO) has been developed based on a broadcasting scheme. Second, measuring data,\n like temperature and gas, in the environment and informing fire fighters and command post about it, is of high importance.\n For this purpose a network coding based data gathering algorithm has been designed. The feasibility of simultaneously using\n the virtual lifeline for data gathering and communication and thus the coexistence of a classical routing protocol with a\n network coding scheme is studied in this paper by means of simulation and real experiments. The resilience to packet loss\n and node failure, as well as the transmission delay are investigated by means of short voice messages for the communication\n part and temperature readings for data gathering.\"",
        "Document: \"Network Planning for Stochastic Traffic Demands. Traffic in communication networks is not constant but fluctuates heavily, which makes the network planning task very challenging. Overestimating the traffic volume results in an expensive solution, while underestimating it leads to a poor Quality of Service (QoS) in the network. In this paper, we propose a new approach to address the network planning problem under stochastic traffic demands. We first formulate the problem as a chance-constrained programming problem, in which the capacity constraints need to be satisfied in probabilistic sense. Since we do not assume a normal distribution for the traffic demands, the problem does not have deterministic equivalent and hence cannot be solved by the well-known techniques. A heuristic approach based on genetic algorithm is therefore proposed. The experiment results show that the proposed approach can significantly reduce the network costs compared to the peak-load-based approach, while still maintaining the robustness of the solution. This approach can be applied to different network types with different QoS requirements.\"",
        "Document: \"Development of simulation environment for multi-homed devices in integrated 3GPP and non-3GPP networks. WLAN has proven itself as the most economical wireless access technology over the time. Its widespread deployment encouraged 3GPP (3rd Generation Partnership Project) to standardize the integration of such non-3GPP access technologies into the existing 3GPP access networks. This opened the door of numerous opportunities for network operators to make use of the economical bandwidth from WLAN in enhancing the end user Quality of Experience (QoE). 3GPP leaves it up to the network operators to do their own research and devise their individual algorithms for this purpose. This raises the need for a proper simulation environment supporting a heterogeneous network of access technologies inter-connected according to 3GPP specifications. The goal of this work is to develop a network simulator where 4G LTE (Long Term Evolution) and WLAN co-exist. In addition to standard 3GPP network entities and protocols the simulator also provides other features like multi-homing support, user QoE optimization algorithm as well as network bandwidth resource management techniques.\"",
        "Document: \"Impacts of air interface admission control and user mobility on UTRAN transport simulations. UMTS Terrestrial Radio Access Network (UTRAN) Transport Network Layer dimensioning has been of great interest among network researchers and planners due to the limit of Iub bandwidth. This paper investigates the potential impacts caused by Connection Admission Control (CAC) mechanism of the air interface and User Mobility (UM) on simulations carried out for the UTRAN transport network domain. Independent from the TNL CAC mechanism of the transport network which is in charge of Iub link bandwidth, the Radio CAC function deals with the highly non-linear resources needed for an attempted connection depending on its specific propagation and interference situation. The consideration of the air interface acting as bottleneck results into a less challenging amount of traffic for the transport domain. In addition, mobile user behaviors, e.g. UM, may dynamically change traffic load at the air interface, and in consequence affect Iub load, which has a very close correlation with cell load. In this work, a simulation model with Radio CAC mechanism and Handover strategies is implemented to study this correlation, and evaluate the UTRAN transport network performance given a limited radio capacity. The analysis on the impacts of the Radio CAC and UM are derived from qualitative simulations.\"",
        "1 is \"An efficient Chase decoder for turbo product codes\", 2 is \"Delay analysis of downlink IP traffic on UMTS mobile networks\"",
        "Given above information, for an author who has written the paper with the title \"Virtual lifeline: Multimodal sensor data fusion for robust navigation in unknown environments\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008241": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A low-power memory architecture with application-aware power management for motion & disparity estimation in multiview video coding':",
        "Document: \"Model Predictive Hierarchical Rate Control With Markov Decision Process for Multiview Video Coding. This paper presents a novel hierarchical rate control (HRC) for the Multiview Video Coding standard targeting improved bandwidth usage and high video quality. The HRC is designed to jointly address the rate control at both frame level and basic unit (BU) level. The proposed scheme is able to exploit the bitrate distribution correlation with neighboring frames to efficiently predict the future bitrate behavior by employing a model predictive control that defines a proper control action through quantization parameter (QP) adaptation. To provide a fine-grained tuning, the QP is further adapted within each frame by a Markov decision process implemented at BU level able to take into consideration a map of the regions of interest. A coupled frame/BU level feedback is performed in order to guarantee the system consistency. Experimental results show the superiority of our HRC compared to state-of-the-art solutions in terms of bitrate allocation accuracy and rate distortion while delivering smooth video quality at frame and BU levels.\"",
        "Document: \"Hardware Design of the H.264/AVC Variable Block Size Motion Estimation for Real-Time 1080HD Video Encoding. Amongst the video compression standards, the latest one is the H.264/AVC. This standard reaches the highest compression rates when compared to the previous standards. On the other hand, it has a high computational complexity. This high computational complexity makes it difficult the development of software applications running in a current processor when high definitions videos are considered. Thus, hardware implementations become essential. Addressing the hardware architectures, this work presents the architectural design for the variable block size motion estimation (VBSME) defined in the H.264/AVC standard. This architecture is based on full search motion estimation algorithm and SAD calculation. This architecture is able to produce the 41 motion vectors within a macroblock as specified in the standard. The implementation of this architecture was based on standard cell methodology in 0.18 mum CMOS technology. The architecture reached a throughput of 34 1080HD frames per second.\"",
        "Document: \"Architecture Of An Hdtv Intraframe Predictor For A H.264 Decoder. Multimedia applications need larger and larger bandwidth. The only way to face these demands is to provide more efficient compression algorithms, with the expense of computational complexity. The most efficient compression standard available today is the H.264/AVC. On the architectural point of view, an H.264 decoder can be seen as a system of six main modules: entropy decoder, inverse quantization, inverse transform, motion compensation, intraframe prediction and deblocking filter. These modules can be designed independently and enclosed in IN, which could be used later to build an H.264 SoC. This paper presents an architecture design of intraframe prediction module. The system was completely built in VHDL language and prototyped over a Xilinx Virtex II Pro FPGA of 27,392 logic elements. The proposed architecture attained the required performance to decode HDTV video stream in real-time, i.e. 1920x1080 pixels at 30 frames per second, and used just 20% of the chip.\"",
        "Document: \"A reduced memory bandwidth and high throughput HDTV motion compensation decoder for H.264/AVC High 4: 2:2 profile. This article presents the HP422-MoCHA: optimized Motion Compensation hardware architecture for the High 4:2:2 profile of H.264/AVC video coding standard. The proposed design focuses on real-time decoding for HDTV 1080p (1,920 \u8133 1,080 pixels) at 30 fps. It supports multiple sample bit-width (8, 9, or 10 bits) and multiple chroma sub-sampling formats (4:0:0, 4:2:0, and 4:2:2) to provide enhanced video quality experience. The architecture includes an optimized sample interpolator that processes luma and chroma samples in two parallel datapaths and features quarter sample accuracy, bi-prediction and weighted prediction. HP422-MoCHA also includes a hardwired Motion Vector Predictor, supporting temporal and spatial direct predictions. A novel memory hierarchy implemented as a 3-D Cache reduces the frame memory access, providing, on average, 62% of bandwidth and 80% of clock cycles reduction. The design was implemented in a Xilinx Virtex-II PRO FPGA, and also in an ASIC with a TSMC 0.18 \u03bcm standard cells technology. The ASIC implementation occupies 102 K equivalent gates and 56.5 KB of on-chip SRAM in a 3.8 \u8133 3.4 mm2 area. It presents a power consumption of 130 mW. Both implementations reach a maximum operation frequency of ~100 MHz, being able to motion compensate 37 bi-predictive frames or 69 predictive fps. The minimum required frequency to ensure the real-time decoding for HD1080p at 30 fps is 82 MHz. Since HP422-MoCHA is the first Motion Compensation architecture for the High 4:2:2 profile found in the literature, a Main profile MoCHA was used for comparison purposes, showing the highest throughput among all presented works. However, the HP422-MoCHA architecture also reaches the highest throughput when compared with the other published Main profile MC solutions, even considering the significantly higher complexity of the High 4:2:2 profile.\"",
        "Document: \"A High Performance H.264 Deblocking Filter. Although the H.264 Deblocking Filter process is a relatively small piece of code in a software implementation, profile results shows it cost about a third of the total CPU time in the decoder. This work presents a high performance architecture for implementing a H.264 Deblocking Filter IP that can be used either in the decoder or in the encoder as a hardware accelerator for a processor or embedded in a full-hardware codec. A developed IP using the proposed architecture support multiple high definition processing flows in real-time.\"",
        "1 is \"A 2-nW 1.1-V self-biased current reference in CMOS technology\", 2 is \"A Statistical Analysis of H.264/AVC FME Mode Reduction\"",
        "Given above information, for an author who has written the paper with the title \"A low-power memory architecture with application-aware power management for motion & disparity estimation in multiview video coding\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008297": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Fault detection of reactive ion etching using time series neural networks':",
        "Document: \"Fuzzy c-means algorithm with divergence-based kernel. A Fuzzy C-Means algorithm with a Divergence-based Kernel (FCMDK) for clustering Gaussian Probability Density Function (GPDF) data is proposed in this paper. The proposed FCMDK is based on the Fuzzy C-Means algorithm and employs a kernel method for data transformation. The kernel method adopted in the proposed FCMDK is used to transform input data into a feature space of a higher dimensionality so that the nonlinear problems residing in input space can be linearly solved in the feature space. In order to deal with GPDF data, a divergence-based kernel employing a divergence distance measure for its similarly measure is used for data transformation. The proposed FCMDK is used for clustering GPDF data in an image classification model. Experiments and results on Caltech data sets demonstrate that the proposed FCMDK is more efficient than other conventional algorithms.\"",
        "Document: \"Multiscale bilinear recurrent neural networks and their application to the long-term prediction of network traffic. A new wavelet-based neural network architecture employing the BiLinear Recurrent Neural Network (BLRNN) for time-series prediction is proposed in this paper. It is called the Multiscale BiLinear Recurrent Neural Network (M-BLRNN). The wavelet transform is employed to decompose the time-series to a multiresolution representation while the BLRNN model is used to predict a signal at each level of resolution. The proposed M-BLRNN algorithm is applied to the long-term prediction of network traffic. The performance of the proposed M-BLRNN algorithm is evaluated and compared with the traditional MultiLayer Perceptron Type Neural Network (MLPNN) and the BLRNN. The results show that the M-BLRNN gives a 20.8% to 76.5% reduction in terms of the normalized mean square error (NMSE) over the MLPNN and the BLRNN.\"",
        "Document: \"Hot Data Identification with Multiple Bloom Filters: Block-Level Decision vs I/O Request-Level Decision. Hot data identification is crucial for many applications though few investigations have examined the subject. All existing studies focus almost exclusively on frequency. However, effectively identifying hot data requires equally considering recency and frequency. Moreover, previous studies make hot data decisions at the data block level. Such a fine-grained decision fits particularly well for flash-based storage because its random access achieves performance comparable with its sequential access. However, hard disk drives (HDDs) have a significant performance disparity between sequential and random access. Therefore, unlike flash-based storage, exploiting asymmetric HDD access performance requires making a coarse-grained decision. This paper proposes a novel hot data identification scheme adopting multiple bloom filters to efficiently characterize recency as well as frequency. Consequently, it not only consumes 50% less memory and up to 58% less computational overhead, but also lowers false identification rates up to 65% compared with a state-of-the-art scheme. Moreover, we apply the scheme to a next generation HDD technology, i.e., Shingled Magnetic Recording (SMR), to verify its effectiveness. For this, we design a new hot data identification based SMR drive with a coarse-grained decision. The experiments demonstrate the importance and benefits of accurate hot data identification, thereby improving the proposed SMR drive performance by up to 42%.\"",
        "Document: \"Content-based adaptive spatio-temporal methods for MPEG repair. Block loss and propagation error due to cell loss or missing packet information during the transmission over lossy networks can cause severe degradation of block and predictive-based video coding. Herein, new fast spatial and temporal methods are presented for block loss recovery. In the spatial algorithm, missing block recovery and edge extention are performed by pixel replacement based on range constraints imposed by surrounding neighborhood edge information and structure. In the temporal algorithm, an adaptive temporal correlation method is proposed for motion vector (MV) recovery. Parameters for the temporal correlation measurement are adaptively changed in accordance to surrounding edge information of a missing macroblock (MB). The temporal technique utilizes pixels in the reference frame as well as surrounding pixels of the lost block. Spatial motion compensation is applied after MV recovery when the reference frame does not have sufficient information for lost MB restoration. Simulations demonstrate that the proposed algorithms recover image information reliably using both spatial and temporal restoration. We compare the proposed algorithm with other procedures with consistently favorable results.\"",
        "Document: \"A Shortest Path Routing Algorithm Using Hopfield Neural Network With An Improved Energy Function. A shortest path routing algorithm using the Hopfield neural network with a modified Lyapunov function is proposed. The modified version of the Lyapunov energy function for an optimal routing problem is proposed for determining routing order for a source and multiple destinations. The proposed energy function mainly prevents the solution path from having loops and partitions. Experiments are performed on 3000 networks of up to 50 nodes with randomly selected link costs. The performance of the proposed algorithm is compared with several conventional algorithms including Ali and Kamoun's, Park and Choi's, and Ahn and Ramakrishna's algorithms in terms of the route optimality and convergence rate. The results show that the proposed algorithm outperforms conventional methods in all cases of experiments. The proposed algorithm particularly shows significant improvements on the route optimality and convergence rate over conventional algorithms when the size of the network approaches 50 nodes.\"",
        "1 is \"Wireless Energy and Information Transfer Tradeoff for Limited-Feedback Multiantenna Systems With Energy Beamforming.\", 2 is \"Write off-loading: practical power management for enterprise storage\"",
        "Given above information, for an author who has written the paper with the title \"Fault detection of reactive ion etching using time series neural networks\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008405": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Optical Non-Orthogonal Multiple Access for Visible Light Communication.':",
        "Document: \"Energy-efficiency analysis of regenerative cooperative systems under spatial correlation. This work is devoted to the error rate and energy-efficiency analysis of regenerative cooperative networks in the presence of multipath fading and spatial correlation. To this end, exact analytic expressions are firstly derived for the symbol-error-rate of M-ary quadrature amplitude modulation in a dual-hop decode-and-forward relay system under spatially correlated Nakagami-m fading channels and maximum ratio combining at the destination. The derived expressions are subsequently employed in quantifying the energy consumption of the considered system, incorporating both transmit energy and the energy consumed by the transceiver circuits. The overall energy consumption is also minimized for certain quality-of-service requirements and it is shown that depending on the degree of spatial correlation, severity of fading, transmission distance, and relay location, a substantial overall energy reduction is sought when compared to conventional direct transmission.\"",
        "Document: \"Outage probability under I/Q imbalance and cascaded fading effects. Direct-conversion architectures (DCA) can offer highly integrated low-cost hardware solutions to communication transceivers. However, DCA devices are sensitive to radio frequency (RF) imperfections such as amplifier non-linearities, phase noise and in-phase/quadrature-phase imbalances (IQI), which typically lead to a severe degradation of the performance of such systems. Motivated by this, we quantify and evaluate the impact of RF IQI on wireless communications in the context of cascaded fading channels. Novel closed form expressions are derived for the corresponding outage probability for the case of ideal transmitter (TX) and receiver (RX), ideal TX and I/Q imbalanced RX, I/Q imbalanced TX and ideal RX, and joint I/Q imbalanced TX/RX. The offered analytic results have a relatively convenient algebraic representation and their validity is extensively justified through simulations. Based on these, it is shown that cascaded fading leads to considerable degradation in the system performance and that assuming ideal RF front-ends at the TX and RX induces non-negligible errors in the outage probability that can exceed 20% in several communication scenarios. We further demonstrate that the effects by cascaded multipath fading conditions are particularly severe, as they typically result to considerable performance losses of around or over an order of magnitude.\"",
        "Document: \"Sensing of Unknown Signals over Weibull Fading Conditions. Energy detection is a widely used method of spectrum sensing in cognitive radio and Radio Detection And Ranging (RADAR) systems. This paper is devoted to the analytical evaluation of the performance of an energy detector over Weibull fading channels. This is a flexible fading model that has been shown capable of providing accurate characterization of multipath fading in, e.g., typical cellular radio frequency range of 800${/}$900 MHz. A novel analytic expression for the corresponding average probability of detection is derived in a simple algebraic representation which renders it convenient to handle both analytically and numerically. As expected, the performance of the detector is highly dependent upon the severity of fading as even small variation of the fading parameters affect significantly the value of the average probability of detection. This appears to be particularly the case in severe fading conditions. The offered results are useful in evaluating the effect of multipath fading in energy detection-based cognitive radio communication systems and therefore they can be used in quantifying the associated trade-offs between sensing performance and energy efficiency in cognitive radio networks.\"",
        "Document: \"Performance analysis of energy detection over mixture gamma based fading channels with diversity reception. The present paper is devoted to the evaluation of energy detection based spectrum sensing over different multipath fading and shadowing conditions. This is realized by means of a unified and versatile approach that is based on the particularly flexible mixture gamma distribution. To this end, novel analytic expressions are firstly derived for the probability of detection over MG fading channels for the conventional single-channel communication scenario. These expressions are subsequently employed in deriving closed-form expressions for the case of square-law combining and square-law selection diversity methods. The validity of the offered expressions is verified through comparisons with results from respective computer simulations. Furthermore, they are employed in analyzing the performance of energy detection over multipath fading, shadowing and composite fading conditions, which provides useful insighs on the performance and design of future cognitive radio based communication systems.\"",
        "Document: \"Energy Efficiency Analysis of Collaborative Compressive Sensing for Cognitive Radio Networks. We investigate the energy ef&#64257;ciency of a con- ventional collaborative compressed sensing (CCCS) scheme in cognitive radio networks. In particular, we derive expressions for the throughput, energy consumption and energy ef&#64257;ciency, and analyze the trade-off between the achievable throughput and the energy consumption of the underlying CCCS scheme. Furthermore, we formulate a multiple variable non-convex optimization problem to determine the optimum compression level that maximizes the energy ef&#64257;ciency, subject to interference constraints. We propose a sub-optimal solution based on tight approximations to simplify the aforementioned optimization problem, and further demonstrate that the energy ef&#64257;ciency achieved by the CCCS scheme is higher than that of conven- tional collaborative sensing scheme, under the same prede&#64257;ned conditions. It is further shown that the increase in the energy ef&#64257;ciency of CCCS scheme is due to the considerable decrease in the energy consumption, which is particularly noticeable with a large number of sensors.\"",
        "1 is \"A Resource Allocator for the Uplink of Multi-Cell OFDMA Systems\", 2 is \"New results on selection diversity\"",
        "Given above information, for an author who has written the paper with the title \"Optical Non-Orthogonal Multiple Access for Visible Light Communication.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008427": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'The design considerations of a sensor grid for monitoring precipitation in debris-flow-prone areas':",
        "Document: \"Dynamic Sub-GOP Forward Error Correction Code for Real-Time Video Applications. Reed-Solomon erasure codes are commonly studied as a method to protect the video streams when transmitted over unreliable networks. As a block-based error correcting code, on one hand, enlarging the block size can enhance the performance of the Reed-Solomon codes; on the other hand, large block size leads to long delay which is not tolerable for real-time video applications. In this paper a novel Dynamic Sub-GOP FEC (DSGF) approach is proposed to improve the performance of Reed-Solomon codes for video applications. With the proposed approach, the Sub-GOP, which contains more than one video frame, is dynamically tuned and used as the RS coding block, yet no delay is introduced. For a fixed number of extra introduced packets, for protection, the length of the Sub-GOP and the redundancy devoted to each Sub-GOP becomes a constrained optimization problem. To solve this problem, a fast greedy algorithm is proposed. Experimental results show that the proposed approach outperforms other real-time error resilient video coding technologies.\"",
        "Document: \"Multiple Description Wavelet Based Image Coding with Classification. In this paper, a multiple description image coding scheme with classified method is presented, in which the wavelet coefficients in each class can be processed separately according to its own source distribution. All the classes will be divided into two larger classes again. One of them is quantized by MDSQ and the other is sub sampled by quad tree and quantized subsequently. The two parts are combined to form two descriptions. It is the number of class quantized by the above two ways that determines the tradeoff between side and central performance. The whole scheme is applied to natural images and simulation demonstrates that the system outperforms other related works. Most important of all, by employing the classified scheme, the central performance can get improved even when the side performance is close to its corresponding single description one.\"",
        "Document: \"Joint redundant motion vector and intra macroblock refreshment for video transmission. This paper proposes a scheme for error-resilient transmission of videos which jointly uses intra macroblock refreshment and redundant motion vector. The selection of using intra refreshment or redundant motion vector is determined by the rate-distortion optimization procedure. The end-to-end distortion is used for the rate-distortion optimization, which can be easily calculated with the recursive optimal per-pixel estimate (ROPE) method. Simulation results show that the proposed method outperforms both the intra refreshment approach and redundant motion vector approach significantly, when the two approaches are deployed separately. Specifically, for the Foreman sequence, the average PSNR of the proposed approach can be 1.12 dB higher than that of the intra refreshment approach and 5 dB higher than that of the redundant motion vector approach.\"",
        "Document: \"High-voltage-tolerant stimulator with adaptive loading consideration for electronic epilepsy prosthetic SoC in a 0.18-\u00b5m CMOS process. A novel design of high-voltage-tolerant stimulator with adaptive loading consideration for electronic epilepsy prosthetic system-on-chip (SoC) in a 0.18-\u03bcm CMOS process was proposed. This design can deliver the required stimulus current within a specific range of loading impedance. Besides, this design in 0.18-\u03bcm low-voltage CMOS process can be operated at high voltage by using only low-voltage transistors. Without using high-voltage transistors, the process step can be reduced and the fabrication yield can be improved. The proposed design can be further integrated for the electronic epilepsy prosthetic SoC applications.\"",
        "Document: \"Depth Map Coding Using Histogram-Based Segmentation and Depth Range Updating. In texture-plus-depth format, depth map compression is an important task. Different from normal texture images, depth maps have less texture information, while contain many homogeneous regions separated by sharp edges. This feature will be employed to form an efficient depth map coding scheme in this paper. Firstly, the histogram of the depth map will be analyzed to find an appropriate threshold that segments the depth map into the foreground and background regions, allowing the edge between these two kinds of regions to be obtained. Secondly, the two regions will be encoded through rate distortion optimization with a shape adaptive wavelet transform, while the edges are lossless encoded with JBIG2. Finally, a depth-updating algorithm based on the threshold and the depth range is applied to enhance the quality of the decoded depth maps. Experimental results demonstrate the effective performance on both the depth map quality and the synthesized view quality.\"",
        "1 is \"Noise Robust Speech Parameterization Using Multiresolution Feature Extraction\", 2 is \"Measuring and analyzing the characteristics of Napster and Gnutella hosts\"",
        "Given above information, for an author who has written the paper with the title \"The design considerations of a sensor grid for monitoring precipitation in debris-flow-prone areas\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008503": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Role-based cascaded delegation':",
        "Document: \"Authenticated dictionaries for fresh attribute credentials. We describe several schemes for efficiently populating an authenticated dictionary with fresh credentials. The thrust of this effort is directed at allowing for many data authors, called sources, to collectively publish information to a common repository, which is then distributed throughout a network to allow for authenticated queries on this information. Authors are assured of their contributions being added to the repository based on cryptographic receipts that the repository returns after performing the updates sent by an author. While our motivation here is the dissemination of credential status data from multiple credential issuers, applications of this technology also include time stamping of documents, document version integrity control, and multiple-CA certificate revocation management, to name just a few.\"",
        "Document: \"Failure Feedback for User Obligation Systems. In recent years, several researchers have proposed techniques for providing users with assistance in understanding and overcoming authorization denials. The incorporation of environmental factors into authorization decisions has made this particularly important and challenging. An environmental factor that has not previously been considered in this effort to provide such assistance to users arises in systems where obligations can depend on and affect authorizations. In these systems, it is desirable to ensure that users will have the authorizations they require to fulfill their obligations, and prior work has proposed denying requests to perform non-obligatory actions that would cause this property to become violated, whether the violation is a direct result of the requested action or due to obligations that would be incurred as a result of it. Because of privacy concerns, as well as the intricate interactions between actions and pending obligations, the current work focuses on helping users find means of overcoming their denials, rather than focusing on explanation of the cause for denial. We show that in general this problem is PSPACE-hard. We then develop an approach based on an AI-planning tool and evaluate its effectiveness empirically. We find that this tool can often be quite helpful in medium sized problem instances, particularly when the number of steps that must be taken to enable the desired action is relatively small.\"",
        "Document: \"Implementation and Performance Analysis of the Role-Based Trust Management System, RTC. We present representations and algorithms for the implementation of RTC, a role-based trust management language, and announce an open-source implementation available to the public. We also design and perform large-scale performance tests on policies closely modeled after possible applications of RT in the real world. These tests aim to determine the viability of RT as an authorization solution for large and potentially complex policies in a decentralized environment; the results of the tests are analyzed to identify what policy characteristics most strongly affect the performance of RT and develop strategies to achieve the rapid response times required in real-world authorization systems.\"",
        "Document: \"Assigning Responsibility for Failed Obligations. Traditional security policies largely focus on access control. Though essential, access control is only one aspect of security. In particular, the correct behavior and reliable operation of a system depends not only on what users are permitted to do, but oftentimes on what users are required to do. Such obligatory actions are integral to the security procedures of many enterprises. Unlike access control, obligations assigned to individual users are often unenforceable, that is, the system cannot ensure that each obligation will be fulfilled. Accurately determining who was at fault when obligations are not met is essential for responding appropriately, be it in terms of modified trust relationships or other recourse. In this paper, based on a formal metamodel of obligations, we propose an approach for fault. assessment through active online tracking of responsibilities and dependencies between obligations. We identify and formalize two key properties for the correct assessment of fault, and design responsibility assignment and fault assessment algorithms for a concrete yet general access control and obligation system.\"",
        "Document: \"On the modeling and analysis of obligations. Traditional security policies largely focus on access control requirements, which specify who can access what under what circumstances. Besides access control requirements, the availability of services in many applications often further imposes obligation requirements, which specify what actions have to be taken by a subject in the future as a condition of getting certain privileges at present. However, it is not clear yet what the implications of obligation policies are concerning the security goals of a system.In this paper, we propose a formal metamodel that captures the key aspects of a system that are relevant to obligation management. We formally investigate the interpretation of security policies from the perspective of obligations, and define secure system states based on the concept of accountability. We also study the complexity of checking a state's accountability under different assumptions about a system.\"",
        "1 is \"CFIMon: Detecting violation of control flow integrity using performance counters\", 2 is \"Information sharing and security in dynamic coalitions\"",
        "Given above information, for an author who has written the paper with the title \"Role-based cascaded delegation\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008529": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Sentiment Bias Detection in Support of News Credibility Judgment':",
        "Document: \"Personalized detection of fresh content and temporal annotation for improved page revisiting. Page revisiting is a popular browsing activity in the Web. In this paper we describe a method for improving page revisiting by detecting and highlighting the information on browsed Web pages that is fresh for a user. Content freshness is determined based on comparison with the previously viewed versions of pages. Any new content for the user is marked, enabling the user to quickly spot it. We also describe a mechanism for visually informing users about the degree of freshness of linked pages. By indicating the freshness level of content on linked pages, the system enables users to navigate the Web more effectively. Finally, we propose and demonstrate the concept of determining user-dependent, subjective age of page contents. Using this method, elements of Web pages are annotated with dates indicating the first time the elements were accessed by the user.\"",
        "Document: \"Analyzing Global and Pairwise Collective Spatial Attention for Geo-social Event Detection in Microblogs. Microblogging has been recently used for detecting common opinions of users at different geographic places. In this paper we propose a novel spatial visualization system for uncovering collective spatial attention and interest of users not at but rather towards different locations. In other words, we aim to answer questions of the type: what do users collectively talk about when they refer to certain geographical places? In addition, we analyze relations between geographical locations from where Twitter users issue messages and the locations they tweet about. This allows answering questions such as: what do users at a certain place commonly talk about when they refer to another geographical place? We demonstrate an online visualization system that supports the interactive analysis of collective spatial attention over time using 4 months' long collection of tweets in USA.\"",
        "Document: \"Tour recommendation system based on web information and GIS. Information recommendation and filtering techniques have been studied intensively. Traditional tour recommendation systems, which can be considered one of information recommendation systems, usually calculate the shortest path in terms of time or distance. Recently, tour recommendation systems for more general purposes have become an important research topic. In this paper we propose an efficient tourist route search system which not only recommends the path simply connecting several tourist spots, but also recommends the path with beautiful scenic sights. We focus on the visibility of scenic sights between one tourist spot and another, which is an important factor for choosing a driving route, but has not been considered in traditional tour recommendation systems. To automatically retrieve tourist spots, we propose a personalized tourist spot recommendation technique using the Web information. Although, for some regions, databases of the famous spots exist and are published, such regions are limited and usually outdated. Our method automatically extracts spots from the Web, thus our system is versatile and up-to-date for large regions. To find a route with attractive scenery, we calculate scores for paths based on the visibility of scenic sights. After generating route candidates using GIS, a 3D virtual space is constructed and the Z-Buffer method is used to decide the visibility of scenic sights for each route candidate. We implemented a prototype and tested the effectiveness of the system.\"",
        "Document: \"An E-Report Scoring Method based on Student Peer Evaluation using Groupware. Nowadays, many universities utilize groupware support for students to post and share their e-reports, and the students can browse and vote other students' reports in e-learning. Teachers then need to evaluate all students' reports, but this will require a great deal of time and effort for a fair evaluation of the reports. Therefore, we propose an e-report scoring method based on student peer evaluation by considering the relationship between voting and posting time of the e-reports, to promote the quality of the votes and prevent unfair votes. Then, the method can provide scores of reports based on a voting graph by analyzing students who vote the reports. In this paper, we perform a student peer evaluation using groupware based on voting with a \\\"Like\\\" button in a course practice, and compare the results with teachers' evaluation.\"",
        "Document: \"Page History Explorer: Visualizing And Comparing Page Histories. Due to the increased preservation efforts, large amounts of past Web data have been stored in Web archives and other archival repositories. Utilizing this data can offer certain benefits to users, for example, it can facilitate page understanding. In this paper, we propose a system for interactive exploration of page histories. We demonstrate an application called Page History Explorer (PHE) for summarizing and visualizing histories of Web pages. PHE portrays the overview of page evolution, characterizes its typical content over time and lets users observe page histories from different viewpoints. In addition, it enables flexible comparison of histories of different pages.\"",
        "1 is \"Pattern codification strategies in structured light systems\", 2 is \"Open domain event extraction from twitter\"",
        "Given above information, for an author who has written the paper with the title \"Sentiment Bias Detection in Support of News Credibility Judgment\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008659": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Computer based system for strabismus and amblyopia therapy':",
        "Document: \"Development Of Domain-Specific Solutions Within The Polish Infrastructure For Advanced Scientific Research. The Polish Grid computing infrastructure was established during the PL-Grid project (2009-2012). The main purpose of this Project was to provide the Polish scientists with an IT basic platform, allowing them to conduct interdisciplinary research on a national scale, and giving them transparent access to international grid resources via international grid infrastructures. Currently, the infrastructure is maintained and extended within a follow-up PLGrid Plus project (2011-2014). Its main objective is to increase the potential of the Polish Science by providing necessary IT services for research teams in Poland, in line with European solutions. The paper presents several examples of the domain-specific computational environments, developed within the Project. For particular environments, specialized IT solutions are prepared, i.e. dedicated software implementation and infrastructure adaptation, suited for particular researchers groups' demands.\"",
        "Document: \"Virtual whiteboard: a gesture-controlled pen-free tool emulating school whiteboard. In the paper the so-called Virtual Whiteboard is presented which may be an alternative solution for modern electronic whiteboards based on electronic pens and sensors. The presented tool enables the user to write, draw and handle whiteboard contents using his/her hands only. An additional equipment such as infrared diodes, infrared cameras or cyber gloves is not needed. The user's interaction with the Virtual Whiteboard computer application is based on dynamic hand gesture recognition. Gestures are recognized in the process of analyzing video stream obtained from a webcam coupled with a multimedia projector displaying whiteboard contents. The tracking positions of hands in the image is supported by Kalman filtering. In the paper the hardware and software of the Virtual Whiteboard is presented with a special focus on utilizing Kalman filters for prediction of consecutive hand positions. For the gestures applied to handle whiteboard contents, examined efficacy of Kalman filter supported recognition and the efficacy without using the filtering is given. In addition, the results of system efficiency tests are provided.\"",
        "Document: \"A Method of Real-Time Non-uniform Speech Stretching. Developed method of real-time non-uniform speech stretching is presented. The proposed solution is based on the well-known SOLA algorithm (Synchronous Overlap and Add). Non-uniform time-scale modification is achieved by the adjustment of time scaling factor values in accordance with the signal content. Dependently on the speech unit (vowels/consonants), instantaneous rate of speech (ROS), and speech signal presence, values of the scaling factor are selected. This provides as low as possible difference in the duration of the input and output signal and high naturalness and quality of the modified speech. In the experimental part of the paper accuracy of the proposed ROS estimator is examined. Quality of the speech stretched using the proposed method is assessed in the subjective tests.\"",
        "Document: \"Awareness Evaluation Of Patients In Vegetative State Employing Eye-Gaze Tracking System. Application of eye-gaze tracking system to awareness evaluation is demonstrated. Hitherto awareness evaluation methods are presented. The assumptions of proposed method based on analysis of visual activity of patients in vegetative state are demonstrated. The eye-gaze tracking system \"Cyber-Eye\" developed at the Multimedia Systems Department employed to conducted experiments is presented. Research described in the paper indicates that awareness level of 13 of 15 tested patients was misdiagnosed before the new method of awareness evaluation is introduced.\"",
        "Document: \"Pitch detection enhancement employing music prediction. Pitch detection methods are widely used for extracting musical data from digital signals. A review of those methods is presented in the paper. Since musical signals may contain noise and distortion, detection results can be erroneous. In this paper a new method employing music prediction to support pitch determination is introduced. This method was developed in order to override disadvantages of standard pitch detection algorithms. The new approach utilizes signal segmentation and pitch prediction based on musical knowledge extraction employing artificial neural networks. Signal segmentation allows for estimating the pitch for a single note as a whole, therefore suppressing errors in transient and decay phases. Pitch prediction helps correcting pitch estimation errors by tracking musical context of the analyzed signal. As it was shown in the experimental results, pitch estimation errors may be reduced by using both signal segmentation and music prediction techniques.\"",
        "1 is \"Finding bugs in dynamic web applications\", 2 is \"Joint Audio-Visual Speech Processing for Recognition and Enhancement\"",
        "Given above information, for an author who has written the paper with the title \"Computer based system for strabismus and amblyopia therapy\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008789": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Challenges to Evaluation of Multilingual Geographic Information Retrieval in GeoCLEF':",
        "Document: \"R2D2 at GeoCLEF 2006: a combined approach. This paper describes the participation of a combined approach in GeoCLEF-2006. We have participated in Monolingual English Task and we present joint work of the three groups or teams belonging to the project R2D2 with a new system, combining the three individual systems of these teams. We consider that research in the area of GIR is still in its very early stages, therefore, although a voting system could improve the individual results of each system, we have to further investigate different ways to achieve a better combination of these systems.\"",
        "Document: \"Subjectivity and sentiment analysis: An overview of the current state of the area and envisaged developments. In this introduction, we present an overview of the current state of research in the Natural Language Processing tasks of subjectivity and sentiment analysis, as well as their application domains and closely-related research field of emotion detection. Although many definitions exist for these tasks and the research done within their frame spans over approaches with different objectives, we consider subjectivity analysis to deal with the detection of ''private states'' (opinions, emotions, sentiments, beliefs, speculations) and sentiment analysis as the task of detecting, extracting and classifying opinions and sentiments concerning different topics, as expressed in textual input. After describing the key concepts and research directions in these tasks, we present the main achievements obtained so far and the issues that remain to be tackled. Subsequently, we introduce each of the papers in this volume and present their contribution to the research areas of subjectivity and sentiment analysis. Finally, we conclude on the present state of work in these fields and reflect on the possible future developments.\"",
        "Document: \"Challenges to Evaluation of Multilingual Geographic Information Retrieval in GeoCLEF. Abstract This is  the  third  year  of  the  evaluation  of  geographic information  retrieval  (GeoCLEF)  within the  Cross-Language  Evaluation  Forum  (CLEF). GeoCLEF 2006 presented topics and documents in  four  languages  (English,  German, Portuguese  and  Spanish).   After  two  years of  evaluation ,we  are ,beginning  to  understand the challenges to both Geographic Information Retrieval  from ,text  and ,of  evaluation of  the ,results  of  geographic  information retrieval.  This poster enumerates some,of  these ,challenges  to  evaluation  and  comments on the limitations encountered in the first two evaluations.  Keywords: Geographic Information Retrieval, Cross-Language Information\"",
        "Document: \"UMCC-DLSI: multidimensional lexical-semantic textual similarity. This paper describes the specifications and results of UMCC_DLSI system, which participated in the first Semantic Textual Similarity task (STS) of SemEval-2012. Our supervised system uses different kinds of semantic and lexical features to train classifiers and it uses a voting process to select the correct option. Related to the different features we can highlight the resource ISR-WN used to extract semantic relations among words and the use of different algorithms to establish semantic and lexical similarities. In order to establish which features are the most appropriate to improve STS results we participated with three runs using different set of features. Our best approach reached the position 18 of 89 runs, obtaining a general correlation coefficient up to 0.72.\"",
        "Document: \"A graph-based approach to WSD using relevant semantic trees and n-cliques model. In this paper we propose a new graph-based approach to solve semantic ambiguity using a semantic net based on WordNet. Our proposal uses an adaptation of the Clique Partitioning Technique to extract sets of strongly related senses. For that, an initial graph is obtained from senses of WordNet combined with the information of several semantic categories from different resources: WordNet Domains, SUMO and WordNet Affect. In order to obtain the most relevant concepts in a sentence we use the Relevant Semantic Trees method. The evaluation of the results has been conducted using the test data set of Senseval-2 obtaining promising results.\"",
        "1 is \"YAWN: A Semantically Annotated Wikipedia XML Corpus\", 2 is \"WordNet: similarity - measuring the relatedness of concepts\"",
        "Given above information, for an author who has written the paper with the title \"Challenges to Evaluation of Multilingual Geographic Information Retrieval in GeoCLEF\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008829": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Modeling Consumer Preferences and Price Sensitivities from Large-Scale Grocery Shopping Transaction Logs.':",
        "Document: \"Fusing Similarity Models with Markov Chains for Sparse Sequential Recommendation. Predicting personalized sequential behavior is a key task for recommender systems. In order to predict user actions such as the next product to purchase, movie to watch, or place to visit, it is essential to take into account both long-term user preferences and sequential patterns (i.e., short-term dynamics). Matrix Factorization and Markov Chain methods have emerged as two separate but powerful paradigms for modeling the two respectively. Combining these ideas has led to unified methods that accommodate long-and short-term dynamics simultaneously by modeling pairwise user-item and item-item interactions. In spite of the success of such methods for tackling dense data, they are challenged by sparsity issues, which are prevalent in real-world datasets. In recent years, similarity-based methods have been proposed for (sequentially-unaware) item recommendation with promising results on sparse datasets. In this paper, we propose to fuse such methods with Markov Chains to make personalized sequential recommendations. We evaluate our method, Fossil, on a variety of large, real-world datasets. We show quantitatively that Fossil outperforms alternative algorithms, especially on sparse datasets, and qualitatively that it captures personalized dynamics and is able to make meaningful recommendations.\"",
        "Document: \"Ups and Downs: Modeling the Visual Evolution of Fashion Trends with One-Class Collaborative Filtering. Building a successful recommender system depends on understanding both the dimensions of people's preferences as well as their dynamics. In certain domains, such as fashion, modeling such preferences can be incredibly difficult, due to the need to simultaneously model the visual appearance of products as well as their evolution over time. The subtle semantics and non-linear dynamics of fashion evolution raise unique challenges especially considering the sparsity and large scale of the underlying datasets. In this paper we build novel models for the One-Class Collaborative Filtering setting, where our goal is to estimate users' fashion-aware personalized ranking functions based on their past feedback. To uncover the complex and evolving visual factors that people consider when evaluating products, our method combines high-level visual features extracted from a deep convolutional neural network, users' past feedback, as well as evolving trends within the community. Experimentally we evaluate our method on two large real-world datasets from Amazon.com, where we show it to outperform state-of-the-art personalized ranking measures, and also use it to visualize the high-level fashion trends across the 11-year span of our dataset.\"",
        "Document: \"Item recommendation on monotonic behavior chains. 'Explicit' and 'implicit' feedback in recommender systems have been studied for many years, as two relatively isolated areas. However many real-world systems involve a spectrum of both implicit and explicit signals, ranging from clicks and purchases, to ratings and reviews. A natural question is whether implicit signals (which are dense but noisy) might help to predict explicit signals (which are sparse but reliable), or vice versa. Thus in this paper, we propose an item recommendation framework which jointly models this full spectrum of interactions. Our main observation is that in many settings, feedback signals exhibit monotonic dependency structures, i.e., any signal necessarily implies the presence of a weaker (or more implicit) signal (a 'review' action implies a 'purchase' action, which implies a 'click' action, etc.). We refer to these structures as 'monotonic behavior chains,' for which we develop new algorithms that exploit these dependencies. Using several new and existing datasets that exhibit a variety of feedback types, we demonstrate the quantitative performance of our approaches. We also perform qualitative analysis to uncover the relationships between different stages of implicit vs. explicit signals.\n\n\"",
        "Document: \"SPMC: Socially-Aware Personalized Markov Chains for Sparse Sequential Recommendation. Dealing with sparse, long-tailed datasets, and cold-start problems is always a challenge for recommender systems. These issues can partly be dealt with by making predictions not in isolation, but by leveraging information from related events; such information could include signals from social relationships or from the sequence of recent activities. Both types of additional information can be used to improve the performance of state-of-the-art matrix factorization-based techniques. In this paper, we propose new methods to combine both social and sequential information simultaneously, in order to further improve recommendation performance. We show these techniques to be particularly effective when dealing with sparsity and cold-start issues in several large, real-world datasets.\"",
        "Document: \"Community Detection in Networks with Node Attributes. Community detection algorithms are fundamental tools that allow us to uncover organizational principles in networks. When detecting communities, there are two possible sources of information one can use: the network structure, and the features and attributes of nodes. Even though communities form around nodes that have common edges and common attributes, typically, algorithms have only focused on one of these two data modalities: community detection algorithms traditionally focus only on the network structure, while clustering algorithms mostly consider only node attributes. In this paper, we develop Communities from Edge Structure and Node Attributes (CESNA), an accurate and scalable algorithm for detecting overlapping communities in networks with node attributes. CESNA statistically models the interaction between the network structure and the node attributes, which leads to more accurate community detection as well as improved robustness in the presence of noise in the network structure. CESNA has a linear runtime in the network size and is able to process networks an order of magnitude larger than comparable approaches. Last, CESNA also helps with the interpretation of detected communities by finding relevant node attributes for each community.\"",
        "1 is \"Adaptive web search based on user profile constructed without any effort from users\", 2 is \"Pogamut 3 Can Assist Developers in Building AI (Not Only) for Their Videogame Agents\"",
        "Given above information, for an author who has written the paper with the title \"Modeling Consumer Preferences and Price Sensitivities from Large-Scale Grocery Shopping Transaction Logs.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008868": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Scheduling Algorithms Exploiting Spare Capacity and Tasks' Laxities for Fault Detection and Location in Real-time Multiprocessor Systems':",
        "Document: \"On the DoF Region of the K-user MISO Broadcast Channel with Hybrid CSIT.   An outer bound for the degrees of freedom (DoF) region of the K-user multiple-input single-output (MISO) broadcast channel (BC) is developed under the hybrid channel state information at transmitter (CSIT) model, in which the transmitter has instantaneous CSIT of channels to a subset of the receivers and delayed CSIT of channels to the rest of the receivers. For the 3-user MISO BC, when the transmitter has instantaneous CSIT of the channel to one receiver and delayed CSIT of channels to the other two, two new communication schemes are designed, which are able to achieve the DoF tuple of $\\left(1,\\frac{1}{3},\\frac{1}{3}\\right)$, with a sum DoF of $\\frac{5}{3}$, that is greater than the sum DoF achievable only with delayed CSIT. Another communication scheme showing the benefit of the alternating CSIT model is also developed, to obtain the DoF tuple of $\\left(1,\\frac{4}{9},\\frac{4}{9}\\right)$ for the 3-user MISO BC. \"",
        "Document: \"The degrees of freedom region of the MIMO cognitive interference channel with no CSIT. The cognitive interference channel (C-IC) is defined as the interference channel that consists of two transmitter-receiver pairs and in which any one or more of the four terminals is cognitive. The degrees of freedom (dof) region of the C-IC is studied for the case in which there is perfect and no channel state information at the receivers and the transmitters, respectively. The proposed inner and outer-bounds yield the precise characterization of dof region, except for a few cases of cognition in which for certain values of the number of antennas at the four terminals, the bounds are not tight. Finally, an example on the feasibility of interference alignment without transmitter knowledge of the channel realizations is shown.\"",
        "Document: \"Scheduling Algorithms Exploiting Spare Capacity and Tasks' Laxities for Fault Detection and Location in Real-time Multiprocessor Systems. Several schemes for detecting and locating faulty processors through self-diagnosis in multiprocessor systems have been discussed in the past. These schemes attempt to start multiple copies (versions) of the tasks on available idle processors simultaneously and compare the results generated by the copies to detect or locate faulty processors. These schemes are based on FCFS scheduling strategy. But, they cannot be applied directly to real-time multiprocessor systems where tasks have timing constraints. In this paper, we present a new scheduling algorithm that not only schedules real-time tasks, but also attempts to perform self-diagnosis if the system is not heavily loaded. We define load as a function of tasks' laxities. We hate carried out extensive simulations and compared the results of our algorithm with that of the myopic algorithm, a real-time task scheduler. Simulation results show that our algorithm that exploits both tasks' laxity and spare capacity (unused processors) offers the same performance (guarantee mtio) as that of the myopic algorithm in addition to achieving fault detection and location.\"",
        "Document: \"Diversity-multiplexing tradeoff of the dynamic decode and forward protocol on a MIMO half-duplex relay channel. We compute the diversity-multiplexing tradeoff (DMT) curve for a three node multi-input-multi-output (MIMO) half-duplex (HD) relay network, operating in the dynamic decode-and-forward (DDF) [1] mode. We consider the case where the source and the destination have n antennas each and the single relay node has m antennas. Denoting such a channel as a (n, m)-relay channel, we provide an analytical characterization of the DMT curve for certain simple configurations such as (n,1), (1,m) and (2,2). We employ a numerical method to compute the DMT for more general channel configurations. Interestingly, for low multiplexing gains the achievable diversity orders of the HD-DDF protocol coincides with the diversity orders achieved by the full-duplex decode-and-forward (FDDF) protocol analyzed in [2]. In fact, the HD-DDF and FDDF protocols achieve the same diversity orders for all integer multiplexing gains. Thus, the half duplex constraint does not significantly affect the achievable DMT for the DF protocol when the source and the destination have the same number of antennas.\"",
        "Document: \"Higher genus universally decodable matrices (UDMG). We introduce the notion of Universally Decodable Matrices of Genus g (UDMG), which for g = 0 reduces to the notion of Universally Decodable Matrices (UDM) introduced in H. Fix positive K,N,L. A UDMG is a set {M-i vertical bar 1 <= i <= L} of matrices of size K x N over a finite field such that the rows of any matrix of K g columns formed from the initial segments of the M, are linearly independent. We show that UDMG can be used to build approximately universal codes. We then provide a dictionary between UDMG and linear codes under the m-metric, which quickly provides constructions of UDMG and places bounds on the size of UDMG.\"",
        "1 is \"Network Access Protocol For Hard Real Time Communication Systems\", 2 is \"Optimal Transmit Covariance For Mimo Channels With Statistical Transmitter Side Information\"",
        "Given above information, for an author who has written the paper with the title \"Scheduling Algorithms Exploiting Spare Capacity and Tasks' Laxities for Fault Detection and Location in Real-time Multiprocessor Systems\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008882": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Shape control of side effect machines for DNA classification':",
        "Document: \"Using associators to generate ensemble biclustering from multiple evolved biclusterings. Biclustering is a data mining technique that performs clustering of the rows and columns of a matrix simultaneously. An associator is a numerical measure of how closely associated two objects should be. Ensemble methods integrate information from multiple solutions to generate superior solutions. A simple evolutionary algorithm to quickly locate multiple biclusterings of synthetic test data. The good submatrices of these biclusterings are then used as associators. Associators are accumulated across many runs of the evolutionary algorithm to create a master association matrix. This matrix is then used, via simultaneous hierarchical clustering, to create a final ensemble biclustering. Results are presenting on tuning the evolutionary algorithm as well as for the overall biclustering algorithm. The algorithm correctly locates planted clusters in the data, providing proof of concept for the ensemble technique. The technique is modular with the evolutionary algorithm, fitness function, and ensemble integration technique all easily swapped for other techniques.\"",
        "Document: \"Rapid training of thermal agents with single parent genetic programming. The temperature profile across an object can be computed by iterative methods. The time spent waiting for iterative solutions to converge for multiple objects in a complex configuration is an impediment to exploratory analysis of engineering systems. A high-quality rapidly-computed initial guess can speed convergence for an iterative algorithm. A system is described and tested for creating thermal agents that supply such initial guesses. Thermal agents are specific to an object but general across different thermal boundary conditions. During an off-line training phase, genetic programming is used to locate a thermal agent by training on several sets of boundary conditions. In use, thermal agents transform boundary conditions into rapidly-converged initial values on a cellular decomposition of an object. In this study, the impact of using single parent genetic programming on thermal agents is tested. Single parent genetic programming replaces the usual sub-tree crossover in genetic programming with crossover with members of an unchanging ancestor set. The use of this ancestor set permits the incorporation of expert knowledge into the system as well as permitting the re-use of solutions derived on one object to speed training of thermal agents for another object. For three types of experiments, incorporating expert knowledge; re-using evolved solutions; and transferring knowledge between distinct configurations statistically significant improvements are obtained with single parent techniques.\"",
        "Document: \"Recentering and Restarting Genetic Algorithm variations for DNA Fragment Assembly. The Fragment Assembly Problem is a major component of the DNA sequencing process that is identified as being NP-Hard. A variety of approaches to this problem have been used, including overlap-layout-consensus, de Bruijn graphs, and greedy graph based algorithms. The overlap-layout-consensus approach is one of the more popular strategies which has been studied on a collection of heuristics and metaheuristics. In this study heuristics and Genetic Algorithm variations are combined to exploit their respective benefits. These algorithms were able to produce results that surpassed the best results obtained by a collection of state-of-the-art metaheuristics on ten of sixteen popular benchmark data sets.\"",
        "Document: \"Evaluating Distance Measures For Rna Motif Search. This paper extends an earlier study which outlined a bioinformatic pipeline for exploratory search for RNA motifs incorporating both primary and secondary structure. The pipeline is applied to three data sets, one of which is a larger version of that used in the earlier study. Instead of a single method of estimating the distance between RNA folds four distance measures were tested. The data sets are: a set of random control sequences, a set of synthetic sequences with simple designed folds, and the iron response element data set for which actual biological RNA folds are available. The pipeline demonstrates the ability to produce clusters that contain known motifs in the biological data and those designed into the synthetic data. The results for the distance measures varies substantially and one of the measures, difference in energy, is found to be too simplistic to be useful for differentiating motifs. The other three distance measures all demonstrate some degree of merit. At the heart of the pipeline is a non-linear projection algorithm that uses evolutionary computation to display the intra-RNA-fold distances so that the various distance measures can be visually compared. While the performance of this algorithm is acceptable, suggestions for improving it are made.\"",
        "Document: \"Automatic generation of game elements via evolution. This study presents a system for automatically producing puzzles for use in game design. The system incorporates an evolutionary algorithm that optimizes the puzzle to a specified level of difficulty. The fitness function uses dynamic programming to compute the minimum number of moves required to solve a puzzle. Two types of puzzle are explored, one is a maze based on chess pieces and the other uses colors as related by the color wheel to create an implicit maze. The evolutionary algorithm is able to produce a wide variety of puzzles at specified levels of difficulty. The algorithm can thus be used to provides a library for game design or for variable game content. The technique is flexible and can be generalized to puzzles of remarkable complexity by simply upgrading the dynamic programming algorithm used in the fitness function. It is found that puzzles requiring a maximum number of moves to solve are, potentially, less difficult because such puzzles present the player with few choices. This problem is addressed by modifying the algorithm to search for puzzles with a smaller minimum number of moves required, leaving more room in the puzzle for choice and its attendant confusion.\"",
        "1 is \"Large-Scale Experimental Evaluation of Cluster Representations for Multiobjective Evolutionary Clustering\", 2 is \"Summary cache: a scalable wide-area web cache sharing protocol\"",
        "Given above information, for an author who has written the paper with the title \"Shape control of side effect machines for DNA classification\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008935": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'An opportunistic resource management model to overcome resource-constraint in the Internet of Things':",
        "Document: \"A novel lifecycle model for Web-based application development in small and medium enterprises. Software engineering\u2019s lifecycle models have proven to be very important for traditional software development. However, can\n these models be applied to the development of Web-based applications as well? In recent years, Web-based applications have\n become more and more complicated and a lot of efforts have been placed on introducing new technologies such as J2EE, PhP,\n and.NET, etc., which have been universally accepted as the development technologies for Web-based applications. However, there\n is no universally accepted process model for the development of Web-based applications. Moreover, shaping the process model\n for small medium-sized enterprises (SMEs), which have limited resources, has been relatively neglected. Based on our previous\n work, this paper presents an expanded lifecycle process model for the development of Web-based applications in SMEs. It consists\n of three sets of processes, i.e., requirement processes, development processes, and evolution processes. Particularly, the\n post-delivery evolution processes are important to SMEs to develop and maintain quality web applications with limited resources\n and time.\"",
        "Document: \"A hybrid swarm intelligence algorithm for multiuser scheduling in HSDPA. Multiuser scheduling is an important aspect in the performance optimization of a wireless network since it allows multiple users to access a shared channel efficiently by exploiting multiuser diversity. To perform efficient scheduling, channel state information (CSI) for users is required, and is obtained via their respective feedback channels. In this paper, a more realistic imperfect CSI feedback, in the form of a finite set of Channel Quality Indicator (CQI) values, is assumed as specified in the HSDPA standard. A mathematical model of the problem is developed for use in the optimization process. A hybrid heuristic approach based on particle swarm optimization and simulated annealing is used to solve the problem. Simulation results indicate that the hybrid approach outperforms individual implementations of both simulated annealing and particle swarm optimization.\"",
        "Document: \"Managing Threats By The Use Of Visualisation Techniques. Identification of threats in networked systems is one of the important risk management processes that should be followed in order to be aware of all risks. In general, risk assessment guidelines for threat analysis propose to use historical organisation's data, thus, novel and unheard threats often are skipped from an analysis. In this paper, we propose a novel onion skin model (OSM) which consists of visualisation techniques, such as attack graphs, often applied for qualitative and quantitative risk assessment analyses. The model can be used to facilitate in threat identification and decision-making process by focusing on attack scenarios that illustrate vulnerable nodes, threats and shortest attack paths to the attacker's goal. The model can be used as part of risk management practices to improve security awareness through different attack scenarios and manage all system risks.\"",
        "Document: \"Geometric Desing And Space Planning Using The Marching Squares And Marching Cube Algorithms. In the paper we present a method for area and volume approximation using modifications to the Marching Cubes algorithm of Lorensen and Cline [8]. Approximations to two and three dimensional objects using marching squares and marching cubes have been covered extensively. Given an approximation to an object, an algorithm is presented which allows a simple method that can approximate the area or volume of the object. More interestingly the method can be used to estimate the area encapsulated between two points on the surface and a line or the volume encapsulated between three points on the surface and a plane. This is of use in room and space planning operations or the design of rooms and manufactured products.\"",
        "Document: \"A Graph Theoretic Framework for Trust - From Local to Global. Traditional approaches to trust, be it in agent-based societies, or within a more theoretical framework often consider trust to be a local phenomenon. Here we propose that trust should be viewed from a global perspective. Our motivation is the area of pervasive computing although we believe that our formal framework applies in many domains. Here we present our framework and formalise it in the form of Graph Theory. We present some open problems and discuss the wider application of our work.\"",
        "1 is \"Visibility-based probabilistic roadmaps for motion planning\", 2 is \"Kinematic And Dynamic Vehicle Models For Autonomous Driving Control Design\"",
        "Given above information, for an author who has written the paper with the title \"An opportunistic resource management model to overcome resource-constraint in the Internet of Things\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008953": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Checking Service Instance Protection for AMF Configurations':",
        "Document: \"Toward a UCM-Based Approach for Recovering System Availability Requirements from Execution Traces. Software maintenance accounts for a significant proportion of the cost of the software life cycle. Software engineers must spend a considerable amount of time understanding the software system functional attributes and non-functional (e. g., availability, security, etc.) aspects prior to performing a maintenance task. In this paper, we propose a dynamic analysis approach to recover availability requirements from system execution traces. Availability requirements are described and visualized using the Use Case Maps (UCM) language of the ITU-T User Requirements Notation (URN) standard, extended with availability annotations. Our UCM-based approach allows for capturing availability requirements at higher levels of abstraction from low-level execution traces. The resulting availability UCM models can then be analyzed to reveal system availability shortcomings. In order to illustrate and demonstrate the feasibility of the proposed approach, we apply it to a case study of a network implementing the HSRP (Hot Standby Router Protocol) redundancy protocol.\"",
        "Document: \"A metamodel for the compact but lossless exchange of execution traces. Understanding the behavioural aspects of a software system can be made easier if efficient tool support is provided. Lately, there has been an increase in the number of tools for analysing execution traces. These tools, however, have different formats for representing execution traces, which hinders interoperability and limits reuse and sharing of data. To allow for better synergies among trace analysis tools, it would be beneficial to develop a standard format for exchanging traces. In this paper, we present a graph-based format, called compact trace format (CTF), which we hope will lead the way towards such a standard. CTF can model traces generated from a variety of programming languages, including both object-oriented and procedural ones. CTF is built with scalability in mind to overcome the vast size of most interesting traces. Indeed, the design of CTF is based on the idea that call trees can be transformed into more compact ordered acyclic directed graphs by representing similar subtrees only once. CTF is also supported by our trace analysis tool SEAT (Software Exploration and Analysis Tool).\"",
        "Document: \"An approach based on citation analysis to support effective handling of regulatory compliance. For most global software companies with a client base that covers a large number of regulated businesses, regulatory compliance represents a significant challenge. The world of compliance has become increasingly complex due to the overwhelming number of regulations, laws, and standards that are introduced every year. These laws may vary significantly in their scope and applicability depending on the industry sector and the geographical area of the end client. In addition, many of these laws are created by different legislative bodies resulting in overlapping and sometimes conflicting provisions. To further complicate matters, laws are often created based on existing ones, forming a complex set of interdependent rules where changes made in one place can propagate to affect, sometimes in an inconsistent manner, many other laws. There is clearly a need to investigate techniques and tools that can alleviate IT solution providers from the complexity of dealing with regulatory compliance. In this paper, we present an approach and a supporting tool that aim to facilitate the analysis of multiple regulations. Our approach is based on the exploration of the citation relationship that links various laws together. The citation relationship is represented by a citation graph that can be used by an analyst to navigate through the provisions of various interrelated laws to uncover overlaps and possible conflicts or to simply understand the content of specific law documents. We also present a tool called CompDSS (Compliance Decision Support System) that supports our approach. Finally, we show the effectiveness of the presented approach by applying it to three regulations, namely, SOX, HIPAA, and GLBA.\"",
        "Document: \"Software behaviour correlation in a redundant and diverse environment using the concept of trace abstraction. Redundancy and diversity has been shown to be an effective approach for ensuring service continuity (an important requirement for autonomic systems) despite the presence of anomalies due to attacks or faults. In this paper, we focus on operating system (OS) diversity, which is useful in helping a system survive kernel-level anomalies. We propose an approach for detecting anomalies in the presence of OS diversity. We achieve this by comparing kernel-level traces generated from instances of the same application deployed on different OS. Our trace correlation process relies on the concept of trace abstraction, in which low-level system events are transformed into higher-level concepts, freeing the trace from OS-related events. We show the effectiveness of our approach through a case study, in which we selected Linux and FreeBSD as target OS. We also report on lessons learned, setting the ground for future research.\"",
        "Document: \"A survey of trace exploration tools and techniques. The analysis of large execution traces is almost impossible without efficient tool support. Lately, there has been an increase in the number of tools for analyzing traces generated from object-oriented systems. This interest has been driven by the fact that polymorphism and dynamic binding pose serious limitations to static analysis. However, most of the techniques supported by existing tools are found in the context of very specific visualization schemes, which makes them hard to reuse. It is also very common to have two different tools implement the same techniques using different terminology. This appears to result from the absence of a common framework for trace analysis approaches. This paper presents the state of the art in the area of trace analysis. We do this by analyzing the techniques that are supported by eight trace exploration tools. We also discuss their advantages and limitations and how they can be improved.\"",
        "1 is \"Priority Refinement for Dependent Tasks in Large Embedded Real-Time Software\", 2 is \"Verification and validation of declarative model-to-model transformations through invariants\"",
        "Given above information, for an author who has written the paper with the title \"Checking Service Instance Protection for AMF Configurations\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008980": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A trip-based multicasting model in wormhole-routed networks with virtual channels':",
        "Document: \"Bandwidth management providing guaranteed call dropping rates for multimedia mobile networks. One major concern for future mobile multimedia networks is how to guarantee various hand-off call dropping rates (CDRs) for different services. Previous research proposed bandwidth reservation schemes, which reduce CDR but may not guarantee CDR. Bandwidth efficiency is another important issue due to the scarcity of wireless bandwidth resources. In this paper, we address these issues by proposing schemes that combine reservation with dynamic channel allocation to improve bandwidth efficiency. Bandwidth reservations are made based on monitored network parameters. Priorities are assigned so that higher priority services may have access to more reserved channels than lower priority services. A simulation study shows that the proposed scheme guarantees predefined CDRs for different multimedia services while providing high bandwidth efficiency\"",
        "Document: \"Efficient and Scalable IEEE 802.11 Ad-Hoc-Mode Timing Synchronization Function. The IEEE 802.11 standards support the peer-to-peer mode Independent Basic Service Set (IBSS), which is an ad hoc network with all its stations within each other's transmission range. In an IBSS, it is important that all stations are synchronized to common clock. Synchronization is needed for frequency hopping and power saving. The synchronization mechanism specified in the IEEE 802.11 standards has severe scalability problem. The probability that stations may get out of synchronization is pretty high in large IBSS. New synchronization algorithm has been proposed for large-scale ad hoc network. We propose more efficient algorithm in this paper that synchronizes clock more accurately. We are able to synchronize the clock within 100 \u00b5s when the number of stations is more than 300. This is a big improvement over the current best algorithm and the 802.11 specified protocol. To our best knowledge, the current best algorithm can synchronize the clock within 550 \u00b5s for 300-station network. The 802.11 standard protocol can have clock drift over 5000 \u00b5s for the same network.\"",
        "Document: \"Efficient distributed deadlock detection and resolution using probes, tokens, and barriers. Probes and tokens are used in many deadlock detection and resolution algorithms. A deadlock is detected by propagating probes along dependency edges. When the initiator pi of a probe receives its probe back, it knows of the existence of a deadlock. pi then sends out a token to clean up those probes in the deadlock; cycle which, if not removed, may later lead to phantom deadlock detections. Only after the token returns to pi is the deadlock resolved by aborting a `victim' (usually pi). As a result, all involved transactions remain waiting and all involved resources locked until the token returns to pi, although the deadlock was already detected when the probe returned to pi. This paper proposes the idea of barriers to allow the deadlock to be resolved without waiting for the token to return to pi, thereby reducing the average deadlock persistence time considerably\"",
        "Document: \"On The Complexity Of A Family Of Generalized Matching Problems. We consider a family of generalized matching problems called k -feasible matching ( k -RM) problems, where k \u03f5 {1,2,3,\u2026} \u222a {\u221e}. We show each k -FM problem to be NP-complete even for very restricted cases. We develop a dynamic programming algorithm that solves in polynomial time the k -FM problem for graphs with width bounded by 2 k . We also show that for any subset S of {1,2,\u2026} \u222a {\u221e}, there is a set D of problem instances such that for k in S the k -FM problem is NP-complete on D , while for k not in S the k -FM problem is polynomially solvable on D .\"",
        "Document: \"Nearly on-line scheduling of multiprocessor systems with memories. We show that no multiprocessor system that contains at least one processor with memory size smaller than at least two other processors can be scheduled nearly on-line to minimize the finish time. An efficient nearly on-line algorithm to minimize Cmax is developed for multiprocessor systems that do not satisfy the preceding requirement.\"",
        "1 is \"Extending a traditional debugger to debug massively parallel applications\", 2 is \"The broadcast storm problem in a mobile ad hoc network\"",
        "Given above information, for an author who has written the paper with the title \"A trip-based multicasting model in wormhole-routed networks with virtual channels\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009033": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'LapEPI-Net: A Laplacian Pyramid EPI structure for Learning-based Dense Light Field Reconstruction.':",
        "Document: \"A parallel deblocking filter based on H.264/AVC video coding standard. The deblocking filter in H.264/AVC is one of the most time consuming part of video decoder as its high content adaptation and data dependency lead to lots of computation. In this paper, we propose a novel parallel deblocking filter design based on the H.264/AVC video coding standard, taking the advantage that the data dependency of the deblocking filter are \u201cperiodic\u201d in one dimension. Our proposed architecture successfully reduces the dependency between horizontal and vertical filters and utilizes the \u201cperiodic\u201d property to achieve pixel-level parallelism. Algorithm analysis and experiment results on JM and GPU show that the proposed deblocking filter keeps as good a coding efficiency as that in H.264/AVC, and its high parallelism is suitable and promising in multi-core/multi-thread computing.\"",
        "Document: \"Arbitrary factor image interpolation by convolution kernel constrained 2-D autoregressive modeling. Among existing interpolation methods, convolution-based methods are able to perform arbitrary factor interpolation but the results are usually blurry or jaggy, adaptive interpolation methods usually can reduce the blurry and jaggy artifacts but cannot handle arbitrary factor interpolation. In this paper we propose an arbitrary factor adaptive interpolation algorithm by combining 2-D piecewise autoregressive (PAR) modeling and convolution kernel constraint. PAR model ensures local geometries are well preserved thus the resultant image is not blurry or jaggy. Convolution kernel constraint ensures the recovered high resolution image consistent with the low resolution image, and also provides the flexibility to handle arbitrary interpolation factor. Experiment results show that our algorithm achieves state-of-the-art performance for any interpolation factor.\"",
        "Document: \"Light Field Reconstruction Using Convolutional Network on EPI and Extended Applications. In this paper, a novel convolutional neural network (CNN)-based framework is developed for light field reconstruction from a sparse set of views. We indicate that the reconstruction can be efficiently modeled as angular restoration on an epipolar plane image (EPI). The main problem in direct reconstruction on the EPI involves an information asymmetry between the spatial and angular dimensions, whe...\"",
        "Document: \"Stereo Matching with Optimal Local Adaptive Radiometric Compensation. A common assumption in stereo matching is that the corresponding pixels in stereo images have similar pixel values. Unfortunately, such an assumption may not be true due to radiometric variations in different views, leading to severely degraded matching results. In this letter, we propose a radiometrically invariant stereo matching algorithm called Optimal Local Adaptive Radiometric Compensation (LARAC). In LARAC, we approximate the spatially varying Pixel Value Correspondence Function (PVCF) between a corresponding pixel pair as a locally consistent polynomial within an optimal local adaptive window. The optimal polynomial coefficients are obtained for each candidate disparity value and are used to compute the matching cost. Meanwhile, a self-correction property is achieved by the proposed LARAC, leading to reduced matching errors for the outlier pixels. Experimental results suggest that the proposed LARAC outperforms other state-of-the-art stereo matching algorithms.\"",
        "Document: \"Novel 2-D MMSE Subpixel-Based Image Down-Sampling. Subpixel-based down-sampling is a method that can potentially improve apparent resolution of a down-scaled image on LCD by controlling individual subpixels rather than pixels. However, the increased luminance resolution comes at price of chrominance distortion. A major challenge is to suppress color fringing artifacts while maintaining sharpness. We propose a new subpixel-based down-sampling pattern called diagonal direct subpixel-based down-sampling (DDSD) for which we design a 2-D image reconstruction model. Then, we formulate subpixel-based down-sampling as a MMSE problem and derive the optimal solution called minimum mean square error for subpixel-based down-sampling (MMSE-SD). Unfortunately, straightforward implementation of MMSE-SD is computational intensive. We thus prove that the solution is equivalent to a 2-D linear filter followed by DDSD, which is much simpler. We further reduce computational complexity using a small $k\\\\times k$ filter to approximate the much larger MMSE-SD filter. To compare the performances of pixel and subpixel-based down-sampling methods, we propose two novel objective measures: normalized $l_{1}$ high frequency energy for apparent luminance sharpness and ${\\\\rm PSNR}_{\\\\rm U(V)}$ for chrominance distortion. Simulation results show that both MMSE-SD and ${\\\\rm MMSE\\\\hbox{-}SD}(k)$ can give sharper images compared with conventional down-sampling methods, with little color fringing artifacts.\"",
        "1 is \"Blind deconvolution using a normalized sparsity measure\", 2 is \"Rollover avoidance for steerable vehicles by invariance control\"",
        "Given above information, for an author who has written the paper with the title \"LapEPI-Net: A Laplacian Pyramid EPI structure for Learning-based Dense Light Field Reconstruction.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009091": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Pairing based anonymous and secure key agreement protocol for smart grid edge computing infrastructure.':",
        "Document: \"Cryptanalysis and improvement of a biometrics-based remote user authentication scheme using smart cards. Recently, Li and Hwang proposed a biometrics-based remote user authentication scheme using smart cards [Journal of Network and Computer Applications 33 (2010) 1-5]. The scheme is based on biometrics verification, smart card and one-way hash function, and it uses the nonce rather than a synchronized clock, so it is very efficient in computational cost. Unfortunately, the scheme has some security weaknesses, that is to say Li and Hwang's scheme does not provide proper authentication and it cannot resist the man-in-the-middle attacks. If an attacker controls the insecure channel, she/he can easily fabricate messages to pass the user's or server's authentication. Besides, the malicious attacker can impersonate the user to cheat the server and can impersonate the server to cheat the user without knowing any secret information. This paper proposes an improved biometrics-based remote user authentication scheme that removes the aforementioned weaknesses and supports session key agreement.\"",
        "Document: \"Hierarchical Trust Level Evaluation for Pervasive Social Networking. Pervasive social networking (PSN) is a fundamental infrastructure in social networking that has played an important role in not only the Internet but also mobile domains. A practical and accurate evaluation system is required to ensure the further development of PSN. Secure and efficient communication is also an essential issue in PSN to increase its adoption in daily life. In this paper, we discuss the establishment of a hierarchical evaluation system to support secure and trustworthy PSN with multiple and variable nodes. The proposed hierarchical evaluation system is essentially based on a special symmetric balanced incomplete block design: the (7; 3; 1)-design and the tree structure. Together, they constitute a multilevel system that supports both our hierarchical trust level (HTL) evaluation system and key agreement scheme. The former solves the problem of trust evaluation in PSN, and the latter guarantees the secure communication of trusted nodes. Note that both security and performance analyses show that the proposed HTL evaluation system can support extensive adoption of efficient and secure PSN.\"",
        "Document: \"A privacy-preserving and provable user authentication scheme for wireless sensor networks based on Internet of Things security. The notion Internet of Things (IoT) means all things in the global network can be interconnected and accessed. Wireless sensor network (WSN) is one of the most important applications of the notion and is widely used in nearly all scopes. In 2014, Hsieh et al. presented an improved authentication scheme for WSNs. But it has several weaknesses, including no session key, lack of mutual authentication and under the insider attack, the off-line guessing attack, the user forgery attack and the sensor capture attack. To avoid the weaknesses, we present a new authentication scheme which is also for WSNs. Then we employ the random oracle model to show the formal proof, and use the protocol analyzing tool Proverif to list the formal verification process. Compared with some recent schemes for WSNs via the aspects of security properties, the proposed scheme overcomes the common problems and fits for the security properties of IoT.\"",
        "Document: \"A novel smart card and dynamic ID based remote user authentication scheme for multi-server environments. With the purpose of using numerous different network services with single registration, various multi-server authentication schemes have been proposed. Furthermore, in order to protect the users from being tracked when they login to the remote server, researchers have proposed some dynamic ID based remote user authentication schemes for multi-server environments. Recently, Lee et al. have pointed out the security weaknesses of Hsiang and Shih\u2019s dynamic ID based multi-server authentication scheme, and proposed an improved dynamic ID based authentication scheme for multi-server environments. They claimed that their scheme provided user anonymity, mutual authentication, session key agreement and can resist several kinds of attacks. In this paper, however, we find that Lee et al.\u2019s scheme is still vulnerable to forgery attack and server spoofing attack. Besides, their scheme cannot provide proper authentication if the mutual authentication message is partly modified by the attacker. In order to remove these security weaknesses, we propose a novel smart card and dynamic ID based authentication scheme for multi-server environments. In order to protect the user from being tracked, the proposed scheme enables the user\u2019s identity to change dynamically when the user logs into the server. The proposed scheme is suitable for use in multi-server environments such as financial security authentication since it can ensure security while maintaining efficiency.\"",
        "Document: \"A Lightweight and Verifiable Access Control Scheme With Constant Size Ciphertext in Edge-Computing-Assisted IoT. As an extension of cloud computing, edge computing has attracted the attention of academia and industry because of its characteristics of low latency, high bandwidth, and low energy consumption. However, due to limited terminal resources and insufficient security design, the edge computing environment still faces many challenges in terms of data security and privacy protection. Among them, how to effectively control access to outsourced data is one of the main issues. In this article, we propose a lightweight and verifiable ciphertext-policy attribute-based encryption (CP-ABE)-based multiauthority access control scheme for edge computing-assisted Internet of Things (IoT), which adopts the method of outsourcing decryption to mitigate the computational cost of data users with limited resources. In addition, our scheme realizes the feature of attribute revocation, and the design of the multiauthority mechanism enables our scheme to avoid the problem of key escrow. Therefore, our proposed scheme not only ensures data confidentiality but also can resist the collusion attack. Besides, our scheme is secure against the chosen plaintext attack in the random oracle model under the decision \n<inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$q$ </tex-math></inline-formula>\n-BDHE assumption. Finally, we compared our scheme with some related work in performance, and the results demonstrate that our scheme is efficient in computation and communication. Because our scheme greatly mitigates the overhead of data users, it is very suitable for edge computing supported IoT applications with restricted computation resources.\"",
        "1 is \"Public auditing for shared data with efficient user revocation in the cloud\", 2 is \"Service-Oriented Computing and Cloud Computing: Challenges and Opportunities\"",
        "Given above information, for an author who has written the paper with the title \"Pairing based anonymous and secure key agreement protocol for smart grid edge computing infrastructure.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009100": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Array interpolation based on multivariate adaptive regression splines.':",
        "Document: \"Joint Channel Estimation for Three-Hop MIMO Relaying Systems. We propose a novel joint channel estimator for a relaying MIMO communication system. Considering a three-hop relaying protocol, our combined alternating least squares (Comb-ALS) algorithm obtains cooperative diversity by fully exploiting the tensor algebraic structures of the available cooperative MIMO links. This is achieved by coupling the tensor data for the different relay-assisted links to it...\"",
        "Document: \"Compressive Sensing Based Channel Estimation For Massive Mimo Systems With Planar Arrays. In this paper, we consider the downlink training in a massive MIMO system, where the base station (BS) is equipped with a large-scale planar array and the user equipment (UE) has a single antenna. We propose a sparsity-aware channel estimation technique to estimate the 2D angular information of the dominant channel paths, which can be subsequently used for downlink beamforming in the data transmission mode. Capitalizing on compressive sensing (CS), the idea of the proposed method is to refine the measurement matrix (beam directions) during the CSI acquisition process by exploiting the knowledge of the visibility region of a planar array. The estimation process is performed by means of a modified orthogonal matching pursuit (OMP) algorithm that takes such a refinement into account. Computer simulation results evaluate the performance of the proposed method for very short training sequences.\"",
        "Document: \"Channel estimation for MIMO multi-relay systems using a tensor approach. In this paper, we address the channel estimation problem for multiple-input multiple-output (MIMO) multi-relay systems exploiting measurements collected at the destination only. Assuming that the source, relays, and destination are multiple-antenna devices and considering a three-hop amplify-and-forward (AF)-based training scheme, new channel estimation algorithms capitalizing on a tensor modeling of the end-to-end communication channel are proposed. Our approach provides the destination with the instantaneous knowledge of all the channel matrices involved in the communication. Instead of using separate estimations for each matrix, we are interested in a joint estimation approach. Two receiver algorithms are formulated to solve the joint channel estimation problem. The first one is an iterative method based on a trilinear alternating least squares (TALS) algorithm, while the second one is a closed-form solution based on a Kronecker least squares (KRLS) factorization. A useful lower-bound on the channel training length is derived from an identifiability study. We also show the proposed tensor-based approach is applicable to two-way MIMO relaying systems. Simulation results corroborate the effectiveness of the proposed estimators and provide a comparison with existing methods in terms of channel estimation accuracy and bit error rate (BER).\"",
        "Document: \"BLAST/MIMO performance with space-time processing receivers. The use of antenna arrays at both ends of the link has attracted significant attention of the researches on space-time equalization and coding techniques for so called multiple-input-multiple-output (MIMO) channels. The presence of strong co-channel interference (CCI) in addition to inter-symbol interference (ISI) in current wireless (mobile) communication systems places a significant challenge to MIMO space-time equalizers. We assess the performance of BLAST/MIMO on frequency-selective MIMO channel model in the presence of CCI. Space-time processing is used in order to deal with the frequency selectivity and to provide more degrees of freedom to deal with cochannel interference. We consider two non-linear space-time processing-based receivers. The first one is a MIMO space-time decision feedback equalizer (MIMO ST-DFE) and the second one is a space-time delayed decision feedback sequence estimator (MIMO ST-DDFSE) with actual adaptive algorithms for channel acquisition. Noise-limited as well as interference-limited situations are evaluated.\"",
        "Document: \"Semi-Blind Receivers for Non-Regenerative Cooperative MIMO Communications Based on Nested PARAFAC Modeling. In cooperative communication systems, multiple-input multiple-output (MIMO) amplify-and-forward (AF) relaying is a promising solution for upcoming wireless standards due to its intrinsic benefits in terms of extended coverage and increased spatial diversity. However, the deployment of MIMO AF relays faces some key challenges such as the need of channel state information (CSI) for the partial chann...\"",
        "1 is \"Network selection in an integrated wireless LAN and UMTS environment using mathematical modeling and computing techniques\", 2 is \"A framework for training-based estimation in arbitrarily correlated Rician MIMO channels with Rician disturbance\"",
        "Given above information, for an author who has written the paper with the title \"Array interpolation based on multivariate adaptive regression splines.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009215": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A class of multiobjective linear programming model with fuzzy random coefficients':",
        "Document: \"Interactive bicriteria linear programming systems implemented on a personal computer. Linear programming is one of the most widely used Operations Research/Management Science/Industrial Engineering techniques. Recently, multiple criteria decision making or multiple objective linear programming has been well established as a practical approach to seeking satisfactory solutions to real-world decision problems. Also, much attention has been focused on a personal computer as an economical management tool. A bicriteria linear programming problem arises as a special case of multiple objective linear programming problems when only two objective functions are of interest. In this paper we develop a bicriteria revised simplex method using the compromise technique for interactively solving bicriteria linear programming problems on a personal computer. In the software package Micro-BLP implemented here, we can also solve single objective linear programming problems and design a conversational and user-friendly system.\"",
        "Document: \"Advances in Multiobjective Hybrid Genetic Algorithms for Intelligent Manufacturing and Logistics Systems. Recently, genetic algorithms (GA) have received considerable attention regarding their potential as a combinatorial optimization for complex problems and have been successfully applied in the area of various engineering. We will survey recent advances in hybrid genetic algorithms (HGA) with local search and tuning parameters and multiobjective HGA (MO-HGA) with fitness assignments. Applications of HGA and MO-HGA will introduced for flexible job-shop scheduling problem (FJSP), reentrant flow-shop scheduling (RFS) model, and reverse logistics design model in the manufacturing and logistics systems.\"",
        "Document: \"Large scale flexible scheduling optimization by a distributed evolutionary algorithm. \u2022A distributed cooperative evolutionary algorithm (dcEA) is proposed.\u2022dcEA is applied to Apache Spark to reduce the computational complexity.\u2022Two EAs are combined to balance the exploration and exploitation.\u2022A local search strategy is applied to improve the performance of dcEA.\u2022We generate three very large scale instances to verify the superiority dcEA.\"",
        "Document: \"A multi-objective hybrid genetic algorithm to minimize the total cost and delivery tardiness in a reverse logistics. In the recent environmental protection the reverse logistics of the used product is one of the most important research topics. The reverse logistics is the process flow of used-products that are collected to be reproduced so that they can be sold again to customers after some processing. We propose a multi-objective hybrid genetic algorithm (mo-hGA) combined with Fuzzy Logic Controller (FLC) for efficiently dealing with multi-objective reverse logistics network (mo-RLN) problem. The aim of this paper is firstly to formulate mo-RLN model, and secondly to optimize it by mo-hGA method proposed with reusable system configuration. In particular two objective functions to be minimized total costs of mo-RLN, (i.e. fixed opening cost, transportation cost and inventory cost) and also minimized delivery tardiness in all periods are considered in the model. We will clear each objective function (i.e. total costs and total delivery tardiness), computational time and number of Pareto solutions with LINGO, pri-awGA (priority-based GA with adaptive weight approach) and mo-hGA proposed with numerical examples. For demonstrating the effectiveness of the proposed model, we evaluate with the numerical examples and simulate it with a bottles distilling/sale company as a case study in Busan, Korea.\"",
        "Document: \"A new model for single machine scheduling with uncertain processing time. Uncertain single machine scheduling problem for batches of jobs is an important issue for manufacturing systems. In this paper, we use uncertainty theory to study the single machine scheduling problem with deadlines where the processing times are described by uncertain variables with known uncertainty distributions. A new model for this problem is built to maximize expected total weight of batches of jobs. Then the model is transformed into a deterministic integer programming model by using the operational law for inverse uncertainty distributions. In addition, a property of the transformed model is provided and an algorithm is designed to solve this problem. Finally, a numerical example is given to illustrate the effectiveness of the model and the proposed algorithm.\"",
        "1 is \"A survey of problems, solution techniques, and future challenges in scheduling semiconductor manufacturing operations.\", 2 is \"The self-dual core and the anti-self-dual remainder of an aggregation operator\"",
        "Given above information, for an author who has written the paper with the title \"A class of multiobjective linear programming model with fuzzy random coefficients\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009223": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'On-line recognition of cursive Korean characters using graph representation':",
        "Document: \"A Real-Time Region-Based Motion Segmentation Using Adaptive Thresholding and K-Means Clustering. This paper presents an approach for a real-time region-based motion segmentation and tracking using an adaptive thresholding and k-means clustering in a scene, with focus on a video monitoring system. In order to reduce the computational load to the motion segmentation, the presented approach is based on the variation regions application of a weighted k-means clustering algorithm, followed by a motion-based region merging procedure. To indicate motion mask regions in a scene, instead of determining the threshold value manually, we use an adaptive thresholding method to automatically choose the threshold value. To image segment, the weighted k-means clustering algorithm is applied only on the motion mask regions of the current frame. In this way we do not to process the whole image so that the computation time is reduced. The presented method is able to deal with occlusion problems. Results show the validity of the presented method.\"",
        "Document: \"Real time hand tracking based on active contour model. This paper presents active contours based method for hand tracking using color information. The main problem in active contours based approach is that results are very sensitive to location of the initial curve. Initial curve far form the object induces more heavy computational cost, low accuracy of results, as well as missing the object that has a large movement. Therefore, this paper presents a hand tracking method using a mean shift algorithm and active contours. The proposed method consists of two steps: hand localization and hand extraction. In the first step, the hand location is estimated using mean shift. And the second step, at the location, evolves the initial curve using an active contour model. To assess the effectiveness of the proposed method, it is applied to real image sequences which include moving hand.\"",
        "Document: \"Improved Method for Frequency Estimation of Sampled Sinusoidal Signals Without Iteration. This paper presents a method for estimating the signal frequency of sampled sinusoidal signals, which does not require any iteration for the frequency search. A noniterative method of frequency estimation was already developed by Zhang , where an analytical expression for the signal frequency is obtained using the differences between the neighboring input data. However, this method is very suscept...\"",
        "Document: \"An HMM based gesture recognition for perceptual user interface. This paper proposes a novel hidden Markov model (HMM)-based gesture recognition method and applies it to the HCI to control a computer game. The novelty of the proposed method is two-folds. First one, the proposed method uses a continuous sequence of human motion as an input of HMM, instead of isolated data sequences or pre-segmented sequences of the data. The other one, it performs both segmentation and recognition of the human gesture automatically. To assess the validity of the proposed method, we applied the proposed system to a real game, Quake II, and then the results demonstrate that the proposed HMM can provide very useful information to enhance the discrimination between the different classes and reduce the computational cost.\"",
        "Document: \"On-line recognition of cursive Korean characters using graph representation. The automatic recognition of cursive Korean characters is a difficult problem, not only due to the multiple possible variations involved in the shapes of characters, but also because of the interconnections of neighboring graphemes within an individual character. This paper proposes a recognition method for Korean characters using graph representation. This method uses a time-delay neural network (TDNN) and graph-algorithmic post-processor for grapheme recognition and character composition, respectively. The proposed method was evaluated using multi-writer cursive characters in a boxed input mode. For a test data set containing 26,500 hand-written cursive characters, a 92.3% recognition rate was obtained.\"",
        "1 is \"Paper windows: interaction techniques for digital paper\", 2 is \"Simulation of artificial life model in game space\"",
        "Given above information, for an author who has written the paper with the title \"On-line recognition of cursive Korean characters using graph representation\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009229": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Lifting the Smokescreen: Detecting Underlying Anomalies During a DDoS Attack':",
        "Document: \"Power napping with loud neighbors: optimal energy-constrained jamming and anti-jamming. The openness of wireless communication and the recent development of software-defined radio technology, respectively, provide a low barrier and a wide range of capabilities for misbehavior, attacks, and defenses against attacks. In this work we present finite-energy jamming games, a game model that allows a jammer and sender to choose (1) whether to transmit or sleep, (2) a power level to transmit with, and (3) what channel to transmit on. We also allow the jammer to choose on how many channels it simultaneously attacks. A major addition in finite-energy jamming games is that the jammer and sender both have a limited amount of energy which is drained according to the actions a player takes. We develop a model of our system as a zero-sum finite-horizon stochastic game with deterministic transitions. We leverage the zero-sum and finite-horizon properties of our model to design a simple polynomial-time algorithm to compute optimal randomized strategies for both players. The utility function of our game model can be decoupled into a recursive equation. Our algorithm exploits this fact to use dynamic programming to construct solutions in a bottom-up fashion. For each state of energy levels, a linear program is solved to find Nash equilibrium strategies for the subgame. With these techniques, our algorithm has only a linear dependence on the number of states, and quadratic dependence on the number of actions, allowing us to solve very large instances. By computing Nash equilibria for our game models, we explore what kind of performance guarantees can be achieved both for the sender and jammer, when playing against an optimal opponent. We also use the optimal strategies to simulate finite-energy jamming games and provide insights into robust communication among reconfigurable, yet energy-limited, radio systems. To test the performance of the optimal strategies we compare their performance with a random and adaptive strategy. Matching our intuition, the aggressiveness of an attacker is related to how much of a discount is placed on data delay. This results in the defender often choosing to sleep despite the latency implication, because the threat of jamming is high. We also present several other findings from simulations where we vary the strategies for one or both of the players.\"",
        "Document: \"JADE: Jamming-averse routing on cognitive radio mesh networks. The spectrum sensing capability of cognitive radio (CR) enables a lot of opportunities to wireless networks, but also enables intelligent attacks by malicious players. One attack in this category is reactive jamming, in which the attacker senses the wireless spectrum, decodes parts of packets, and selectively interferes with packets. In so doing, an attacker can reduce energy expenditure and increase stealth while maintaining a high impact. Of the approaches to mitigate jamming, in this work, we focus on the jamming resilient routing in CR mesh networks. To do this we use signal-to-noise-interference ratio (SINR) which reflects the jamming impact. This metric is difficult to measure with commodity radio chipsets that cannot differentiate jamming interference from the received signal. Detecting SINR becomes even harder if reactive jamming is used by an attacker. In this study, we develop a mechanism to estimate SINR under reactive jamming. The estimated SINR information of each wireless link is then used to determine the jamming-averse directivity (JAD) of packets, which improves the routing performance of the victim network. We validate the proposed mechanism with a simulation study, showing that the proposed JAD escorted (JADE) routing dramatically improves routing path discovery performance including path discovery probability, path length, elapsed time for path discovery, retransmission attempts, and path quality under reactive jamming. Among the 200 route requests at 10 different configurations in our simulation, the reactive jammer disrupts the 77.5% of total requests. However, our JADE routing decreases the route discovery failure rate to 7.5% by saving the 96.7% of failed requests.\"",
        "Document: \"PrivateDroid: Private Browsing Mode for Android. Private browsing mode is a privacy feature adopted by many modern computer browsers. With the increased use of mobile devices and escalating privacy concerns for mobile users, browser applications on mobile devices have also started incorporating private browsing mode. Even so, the use of private browsing mode is limited to the browser applications and cannot be applied directly on other third-party mobile applications. In this paper, we propose Private Droid, which provides a private browsing mode for third-party applications on the Android platform. First, we discuss three possible approaches of implementing mobile private browsing mode: code instrumentation, an extra sandbox, and a Linux container approach. Then, we implement Private Droid, which creates a new sandbox for every application in private mode and destroys the sandbox once the application is closed. After that, we evaluate usability, efficiency and security of the system with 25 popular Android applications. Our design considerations, implementation details, evaluation results, and challenges lay a foundation of private browsing mode on mobile platforms.\"",
        "Document: \"Mitigation of Control Channel Jamming under Node Capture Attacks. Availability of service in many wireless networks depends on the ability for network users to establish and maintain communication channels using control messages from base stations and other users. An adversary with knowledge of the underlying communication protocol can mount an efficient denial of service attack by jamming the communication channels used to exchange control messages. The use of spread spectrum techniques can deter an external adversary from such control channel jamming attacks. However, malicious colluding insiders or an adversary who captures or compromises system users is not deterred by spread spectrum, as they know the required spreading sequences. For the case of internal adversaries, we propose a framework for control channel access schemes using the random assignment of cryptographic keys to hide the location of control channels. We propose and evaluate metrics to quantify the probabilistic availability of service under control channel jamming by malicious or compromised users and show that the availability of service degrades gracefully as the number of colluding insiders or compromised users increases. We propose an algorithm called GUIDE for the identification of compromised users in the system based on the set of control channels that are jammed. We evaluate the estimation error using the GUIDE algorithm in terms of the false alarm and miss rates in the identification problem. We discuss various design trade-offs between robustness to control channel jamming and resource expenditure.\"",
        "Document: \"MeshJam: Intelligent Jamming Attack and Defense in IEEE 802.11s Wireless Mesh Networks. Wireless mesh networks represent an emerging network architecture which has been actively studied and standardized for the last several years. Because of their flexible network architecture, wireless mesh networks can provide alternative paths even when wireless links are broken by node failures or routing attacks. Among a variety of mesh network protocols, we focus on the recently ratified IEEE 802.11s WLAN mesh standard. With analysis of the path selection scheme in 802.11s, we show the effect of conventional jamming on 802.11s-based wireless mesh networks via simulation. We then introduce mesh jamming, which can more efficiently attack the mesh path selection process by exploiting cross-layer knowledge and more harmfully influence on the path discovery performance compared to conventional jamming. We propose a proof-of-concept defense, bi-directional path discovery to mitigate the devastating effect of mesh jamming.\"",
        "1 is \"A survey of peer-to-peer security issues\", 2 is \"The internet of things: a survey\"",
        "Given above information, for an author who has written the paper with the title \"Lifting the Smokescreen: Detecting Underlying Anomalies During a DDoS Attack\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009433": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Proving Distributed Coloring Of Forests In Dynamic Networks':",
        "Document: \"Modelling by Patterns for Correct-by-Construction Process. Patterns have greatly improved the development of programs and software by identifying practices that could be replayed and reused in different software projects. Moreover, they help to communicate new and robust solutions for software development; it is clear that design patterns are a set of recipes that are improving the production of software. When developing models of systems, we are waiting for adequate patterns for building models and later for translating models into programs or even software. In this paper, we review several patterns that we have used and identified, when teaching and when developing case studies using the Event-B modelling language. The modelling process includes the use of formal techniques and the use of refinement, a key notion for managing abstractions and complexity of proofs. We have classified patterns in classes called paradigms and we illustrate three paradigms: the inductive paradigm, the call-as-event paradigm and the service-as-event paradigm. Several case studies are given for illustrating our methodology.\"",
        "Document: \"Incremental Proof of the Producer/Consumer Property for the PCI Protocol. We present an incremental proof of the producer/consumer property for the PCI protocol. In the incremental proof, a corrected model of the multibus PCI 2.1 protocol is shown to be a refinement of the producer/consumer property. Multi-bus PCI must be corrected because the original PCI specification violates the producer/consumer property. The final model of PCI includes transaction types and reordering along with the completion mechanism for delayed PCI transactions. Verification results include multiple concurrent sessions of the producer/consumer property in a family of topologically isomorphic network configurations. The remaining configurations are identified and left for future work. In contrast to previous case studies involving this problem [13,15], the incremental proof provides structure which simplifies otherwise difficult monolithic proof attempts.\"",
        "Document: \"System-on-chip design by proof-based refinement. Systems-on-chip (SoCs) and SoC architectures provide a collection of challenging problems related to specification, modelling techniques, security issues and structuring questions. We describe a design methodology integrating the event B method and characterized by the incremental and proof-controlled construction of SoC models. The essence of the methodology is the refinement of models, starting from system requirements and producing event B models for characterizing the system under development. The refinement is a unifying concept that ensures the consistency of the different models produced and our contribution is an illustration through a case study, namely a system for measuring the parameters of audio/video quality in the digital video broadcasting (DVB) set of digital TV standards. The first part is the derivation of an architecture of parameters from the document ETSI TR 101 290 and the validation of the architecture using invariants of B models. The second part is the proposal of B models of the SystemC scheduler and an instantiation of these abstract models of the simulation semantics by parameters of the SystemC codes automatically translated from the B models of the DVB system. Finally, the third part relies upon a proof-based methodology for deriving an operational semantics of a given system that is expressed by an event B model including invariant properties.\"",
        "Document: \"Predicate Diagrams for the Verification of Reactive Systems. We define a class of diagrams that represent abstractions of--possibly infinite-state--reactive systems described by specifications written in temporal logic. Our diagrams are intended as the basis for the verification of both safety and liveness properties of such systems. Non-temporal proof obligations establish the correspondence between the original specification and the diagram, whereas model checking can be used to verify properties over finite-state abstractions. We describe the use of abstract interpretation techniques to generate proof diagrams from a given specification and user-defined predicates that represent sets of states.\"",
        "Document: \"Making explicit domain knowledge in formal system development. Modeling languages are concerned with providing techniques and tool support for the design, synthesis and analysis of the models resulting from a given modeling activity, this activity being usually part of a system development model or process. These languages quite successfully focused on the analysis of the designed system exploiting the expressed semantic power of the underlying modeling language. The semantics of these modeling languages are well understood by the system designers and/or the modeling language users i.e. implicit semantics.In general, modeling languages are not equipped with resources, concepts or entities handling explicitly domain engineering features and characteristics (domain knowledge) in which the modeled systems evolve.Indeed, the designer has to explicitly handle the knowledge issued and/or mined from an analysis of this application domain i.e. explicit semantics. Nowadays, making explicit the domain knowledge inside system design models does not obey to any methodological rule validated by the practice. The modeling languages users introduce through types, constraints, profiles, etc. these domain knowledge features.Our claim is that ontologies are good candidates for handling explicit domain knowledge. They define domain theories and provide resources for uniquely identifying domain knowledge concepts. Therefore, allowing models to make references to ontologies is a modular solution for models to explicitly handle domain knowledge.Overcoming the absence of explicit semantics expression in the modeling languages used to specify systems models will increase the robustness of the designed system models. Indeed, the axioms and theorems resulting from the ontologies, thanks to references, can be used to strengthen the properties of the designed models.The objective of this paper is to offer rigorous mechanisms for handling domain knowledge in design models. This paper also shows how these mechanisms are set up in the cases of static, dynamic and formal models.\"",
        "1 is \"How to cook a temporal proof system for your pet language\", 2 is \"Algorithms for plane representations of acyclic digraphs\"",
        "Given above information, for an author who has written the paper with the title \"Proving Distributed Coloring Of Forests In Dynamic Networks\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009477": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Effect Of Heuristics On Serendipity In Path-Based Storytelling With Linked Data':",
        "Document: \"LOD-a-lot: A Single-File Enabler for Data Science. Many data scientists make use of Linked Open Data (LOD) as a huge interconnected knowledge base represented in RDF. However, the distributed nature of the information and the lack of a scalable approach to manage and consume such Big Semantic Data makes it difficult and expensive to conduct large-scale studies. As a consequence, most scientists restrict their analyses to one or two datasets (often DBpedia) that contain at most hundreds of millions of triples. LOD-a-lot is a dataset that integrates a large portion (over 28 billion triples) of the LOD Cloud into a single ready-to-consume file that can be easily downloaded, shared and queried with a small memory footprint. This paper shows there exists a wide collection of Data Science use cases that can be performed over such a LOD-a-lot file. For these use cases LOD-a-lot significantly reduces the cost and complexity of conducting Data Science.\"",
        "Document: \"A Distance-Based Approach for Semantic Dissimilarity in Knowledge Graphs. In this paper, we introduce a distance-based approach for measuring the semantic dissimilarity between two concepts in a knowledge graph. The proposed Normalized Semantic Web Distance (NSWD) extends the idea of the Normalized Web Distance, which is utilized to determine the dissimilarity between two textural terms, and utilizes additional semantic properties of nodes in a knowledge graph. We evaluate our proposal on the knowledge graph Freebase, where the NSWD achieves a correlation of up to 0.58 with human similarity assessments on the established Miller-Charles benchmark of 30 term-pairs. These preliminary results indicate that the proposed NSWD is a promising approach for assessing semantic dissimilarity in very large knowledge graphs.\"",
        "Document: \"Social Semantic Search: A Case Study on Web 2.0 for Science. AbstractWhen researchers formulate search queries to find relevant content on the Web, those queries typically consist of keywords that can only be matched in the content or its metadata. The Web of Data extends this functionality by bringing structure and giving well-defined meaning to the content and it enables humans and machines to work together using controlled vocabularies. Due the high degree of mismatches between the structure of the content and the vocabularies in different sources, searching over multiple heterogeneous repositories of structured data is considered challenging. Therefore, the authors present a semantic search engine for researchers facilitating search in research related Linked Data. To facilitate high-precision interactive search, they annotated and interlinked structured research data with ontologies from various repositories in an effective semantic model. Furthermore, the authors' system is adaptive as researchers can synchronize using new social media accounts and efficiently explore new datasets.\"",
        "Document: \"Defining Aesthetic Principles For Automatic Media Gallery Layout For Visual And Audial Event Summarization Based On Social Networks. In this paper, we present and define aesthetic principles for the automatic generation of media galleries based on media items retrieved from social networks that-after a ranking and pruning step-can serve to authentically summarize events and their atmosphere from a visual and an audial standpoint.\"",
        "Document: \"Functional descriptions as the bridge between hypermedia APIs and the Semantic Web. The early visions for the Semantic Web, from the famous 2001 Scientific American article by Berners-Lee et al., feature intelligent agents that can autonomously perform tasks like discovering information, scheduling events, finding execution plans for complex operations, and in general, use reasoning techniques to come up with sense-making and traceable decisions. While today\u0014more than ten years later\u0014the building blocks (1) resource-oriented rest infrastructure, (2) Web APIs, and (3) Linked Data are in place, the envisioned intelligent agents have not landed yet. In this paper, we explain why capturing functionality is the connection between those three building blocks, and introduce the functional API description format RESTdesc that creates this bridge between hypermedia APIs and the Semantic Web. Rather than adding yet another component to the Semantic Web stack, RESTdesc offers instead concise descriptions that reuse existing vocabularies to guide hypermedia-driven agents. Its versatile capabilities are illustrated by a real-life agent use case for Web browsers wherein we demonstrate that RESTdesc functional descriptions are capable of fulfilling the promise of autonomous agents on the Web.\"",
        "1 is \"Customer-developer links in software development\", 2 is \"Clustering Based On Association Rule Hypergraphs\"",
        "Given above information, for an author who has written the paper with the title \"Effect Of Heuristics On Serendipity In Path-Based Storytelling With Linked Data\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009483": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Recursive code construction for reversible data hiding in DCT domain':",
        "Document: \"Humanoid On-line Pattern Generation Based on Parameters of Off-line Typical Walk Patterns. The complexity of nonlinear differential equations of dynamics makes it practically impossible to obtain the walk pattern on-line through computing the whole dynamics. This paper proposed a method of online trajectory generation based on key parameters of off-line typical walk patterns for a biped humanoid. The key parameters include hip parameters, step length, walking cycle and so on. The walking pattern can be obtained according to these parameters. In order to generate walking patterns online, first the key parameters of the on-line walking pattern are computed based on the parameters of off-line typical patterns, then stability optimization has been done on-line and on-line trajectories are derived. The effectiveness of the proposed method is confirmed by simulations and experiments with our developed humanoid robot with 33 DOF.\"",
        "Document: \"Inter-frame distortion drift analysis for reversible data hiding in encrypted H.264/AVC video bitstreams. Due to the privacy-preserving requirement for cloud data management, it is necessary to perform data hiding in the encrypted domain. This paper proposes an effective scheme for reversible data hiding in encrypted H.264/AVC video bitstreams. In the encryption phase, the codewords of intra prediction modes, motion vector differences and partial residual coefficients are encrypted without video bit rate increment for preserving the confidentiality of video content. In the data hiding phase, we first present a theoretical analysis of the picture distortion caused by data embedding and the subsequent inter-frame distortion drift. Based on the analysis, we estimate the embedding distortions caused by modifying different residual coefficients and embed data into residual coefficients with different priorities for decreasing the inter-frame distortion drift. The data embedding is implemented by the histogram shifting technique. If the receiver decrypts the encrypted video bitstream and extracts the embedded data, the original video bitstream can be perfectly recovered. In addition, if the receiver decrypts the encrypted video bitstream without extracting the embedded data, the bitstream can still be decoded to obtain the reconstructed video with good quality. Graphical abstractDisplay Omitted HighlightsVideo encryption can keep the bitstream format-compliant and the bit rate constant.A theoretical analysis of the inter-frame distortion drift is presented.The video bitstream can be losslessly recovered after video decryption and data extraction.\"",
        "Document: \"Capacity-approaching codes for reversible data hiding. By reversible data hiding, the original cover can be losslessly restored after the embedded information is extracted. Kalker and Willems established a rate-distortion model for reversible data hiding, in which they proved the capacity bound and proposed a recursive code construction. In this paper we improve the recursive construction by designing a data embedding method for all-zero covers and a more efficient compression algorithm. We prove that the proposed codes can approach the capacity bound under various distortion constraints. We also apply this coding method to RS method for spatial images, and the experimental results show that the novel codes can significantly reduce the embedding distortion.\"",
        "Document: \"On the fault-tolerant performance for a class of robust image steganography. \u2022An error model based on burst errors and STCs decoding damage is given and verified.\u2022The probability lower bound for RS-STC decoding to correctly extract embedded messages is deduced.\u2022Experiments demonstrate that the practical fault-tolerant results of robust steganography methods consists with the theoretical derivation results.\u2022A theory support for the integrity of message extraction is provided to the robust steganography based on \u201cCompression-resistant Domain Constructing + RS-STC Codes\u201d.\"",
        "Document: \"Targeted attack and security enhancement on texture synthesis based steganography. \u2022Build an effective and efficient steganography detector for patch synthesis based steganography and thus improve the steganalysis performance to a large extent.\u2022Improve the security of steganography against state-of-the-art methods and steganalytic attack we developed.\"",
        "1 is \"On the Outage Probability of Device-to-Device-Communication-Enabled Multichannel Cellular Networks: An RSS-Threshold-Based Perspective\", 2 is \"An Inpainting-Assisted Reversible Steganographic Scheme Using a Histogram Shifting Mechanism\"",
        "Given above information, for an author who has written the paper with the title \"Recursive code construction for reversible data hiding in DCT domain\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009548": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Research Frontiers in Information Retrieval: Report from the Third Strategic Workshop on Information Retrieval in Lorne (SWIRL 2018).':",
        "Document: \"Statistical comparisons of non-deterministic IR systems using two dimensional variance. We propose methods to compare non-deterministic IR systems.We show pitfalls in using standard significance tests to compare such systems.We verify the applicability of proposed methods using simulations and a case study.We show how to compare a non-deterministic IR system for equivalent effectiveness. Retrieval systems with non-deterministic output are widely used in information retrieval. Common examples include sampling, approximation algorithms, or interactive user input. The effectiveness of such systems differs not just for different topics, but also for different instances of the system. The inherent variance presents a dilemma - What is the best way to measure the effectiveness of a non-deterministic IR system? Existing approaches to IR evaluation do not consider this problem, or the potential impact on statistical significance. In this paper, we explore how such variance can affect system comparisons, and propose an evaluation framework and methodologies capable of doing this comparison.Using the context of distributed information retrieval as a case study for our investigation, we show that the approaches provide a consistent and reliable methodology to compare the effectiveness of a non-deterministic system with a deterministic or another non-deterministic system. In addition, we present a statistical best-practice that can be used to safely show how a non-deterministic IR system has equivalent effectiveness to another IR system, and how to avoid the common pitfall of misusing a lack of significance as a proof that two systems have equivalent effectiveness.\"",
        "Document: \"Fusion in Information Retrieval: SIGIR 2018 Half-Day Tutorial. Fusion is an important and central concept in Information Retrieval. The goal of fusion methods is to merge different sources of information so as to address a retrieval task. For example, in the adhoc retrieval setting, fusion methods have been applied to merge multiple document lists retrieved for a query. The lists could be retrieved using different query representations, document representations, ranking functions and corpora. The goal of this half day, intermediate-level, tutorial is to provide a methodological view of the theoretical foundations of fusion approaches, the numerous fusion methods that have been devised and a variety of applications for which fusion techniques have been applied.\n\n\"",
        "Document: \"Query Driven Algorithm Selection in Early Stage Retrieval. Large scale retrieval systems often employ cascaded ranking architectures, in which an initial set of candidate documents are iteratively refined and re-ranked by increasingly sophisticated and expensive ranking models. In this paper, we propose a unified framework for predicting a range of performance-sensitive parameters based on minimizing end-to-end effectiveness loss. The framework does not require relevance judgments for training, is amenable to predicting a wide range of parameters, allows for fine tuned efficiency-effectiveness trade-offs, and can be easily deployed in large scale search systems with minimal overhead. As a proof of concept, we show that the framework can accurately predict a number of performance parameters on a query-by-query basis, allowing efficient and effective retrieval, while simultaneously minimizing the tail latency of an early-stage candidate generation system. On the 50 million document ClueWeb09B collection, and across 25,000 queries, our hybrid system can achieve superior early-stage efficiency to fixed parameter systems without loss of effectiveness, and allows more finely-grained efficiency-effectiveness trade-offs across the multiple stages of the retrieval system.\n\n\"",
        "Document: \"Improving Search Effectiveness with Field-based Relevance Modeling. Fields are a valuable auxiliary source of information in semi-structured HTML web documents. So, it is no surprise that ranking models have been designed to leverage this information to improve search effectiveness. We present the first (initial) study of utilizing field-based information in the relevance modeling framework. Fields play two different, and integrated, roles in our models: sources of information for inducing relevance models and units on which relevance models are applied for ranking. Our preliminary results suggest that field-based relevance modeling can improve precision at top ranks; specifically, to a greater extent than the commonly used BM25F and SDM-Fields field-based models. Further analysis shows that using field-based relevance models mainly improves the effectiveness of tail queries. Our findings suggest that using field-based information together with relevance modeling is a promising area of future exploration.\"",
        "Document: \"Geo-Social Influence Spanning Maximization. Influence maximization is a recent but well-studied problem which helps identify a small set of users that are most likely to \u201cinfluence\u201d the maximum number of users in a social network. The problem has attracted a lot of attention as it provides a way to improve marketing, branding, and product adoption. However, existing studies rarely consider the physical locations of the users, but location i...\"",
        "1 is \"A Study of MatchPyramid Models on Ad-hoc Retrieval.\", 2 is \"User relevance criteria choices and the information search process\"",
        "Given above information, for an author who has written the paper with the title \"Research Frontiers in Information Retrieval: Report from the Third Strategic Workshop on Information Retrieval in Lorne (SWIRL 2018).\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009564": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Load balancing strategies for symbolic vision computations.':",
        "Document: \"Fingerprint template protection using fuzzy vault. Biometric-based authentication can provide strong security guarantee about the identity of users. However, security of biometric data is particularly important as the compromise of the data will be permanent. To protect the biometric data, we need to store it in a non-invertible transformed version. Thus, even if the transformed version is compromised, the actual biometric data remains safe. In this paper, we propose an approach to protect fingerprint templates by using the idea of the fuzzy vault. Fuzzy vault is a recently developed cryptographic construct to secure critical data with the fingerprint data in a way that only the authorized user can access the secret by providing the valid fingerprint. We modify the fuzzy vault to protect fingerprint templates and to perform fingerprint verification with the protected template at the same time. This is challenging because the fingerprint verification is performed in the domain of the protected form. Based on the experimental results, we confirm that the proposed approach can perform the fingerprint verification with the protected template.\"",
        "Document: \"Energy-Efficient Transmissions of Fingerprint Images Using Both Encryption and Fingerprinting Techniques. As user authentication by using biometric information such as fingerprint has been widely accepted, there has been a growing interest in protecting the biometric information itself against external attackers. Especially, as the mobile handheld devices equipped with fingerprint sensors are produced, it becomes important to protect the private information of an user(i.e., fingerprint image) in the remote applications. In this paper, we propose an energy-efficient technique to transmit fingerprint images from the handheld device to a server securely using both encryption and fingerprinting. In addition to the encryption to guarantee the confidentiality of the fingerprint image transmitted, we insert the logo of the application into the fingerprint image(i.e., fingerprinting) to indicate to whom the fingerprint image is transmitted(i.e., a possible privacy violator). In particular, to lower the computational burden of the handheld device, we not only offload some heavy computation to the server without compromising the security, but also apply some techniques to the handheld device such as loop transformation and data compaction. Based on experimental results, we confirm that our technique can transmit fingerprint images from the handheld device in an energy-efficient way(by a factor of 2) and detect the intentional/unintentional leakage of the transmitted fingerprint image from the server - privacy violator.\"",
        "Document: \"A fingerprint-based user authentication protocol considering both the mobility and security in the telematics environment. With the advance of the Internet and mobile communication techniques, the telematics environment where users in vehicles can use the Internet service has been realized. For the safe driving, however, we propose that user authentication for the Internet service is performed by using the driver's fingerprint, instead of typing his/her password. Since the driver's fingerprint is private information to be protected and the size of the fingerprint information is much larger than that of a typical password, we need a different user authentication protocol for the telematics environment. That is, in addition to the compliance with the standard X9.84 protocol to protect the fingerprint information transmitted, we use the watermarking technique to lessen the privacy threat, and propose a secure and efficient protocol between Access Points (APs) considering the possible hand-off during the authentication in the mobile telematics environment. Based on the experimental measurement of the proposed protocol, we confirm that the fingerprint-based user authentication can be performed in real-time in the telematics environment.\"",
        "Document: \"Automatic alignment of fingerprint features for fuzzy fingerprint vault. Biometrics-based user authentication has several advantages over traditional password-based systems for standalone authentication applications. This is also true for new authentication architectures known as crypto-biometric systems, where cryptography and biometrics are merged to achieve high security and user convenience at the same time. Recently, a cryptographic construct, called fuzzy vault, has been proposed for crypto-biometric systems. This construct aims to secure critical data(e.g., secret encryption key) with the fingerprint data in a way that only the authorized user can access the secret by providing the valid fingerprint, and some implementations results for fingerprint have been reported. However, all the previous results assumed that fingerprint features were pre-aligned, and automatic alignment in the fuzzy vault domain is a challenging issue. In this paper, we perform the automatic alignment of fingerprint features by using the geometric hashing technique which has been used for model-based object recognition applications. Based on the preliminary experimental results, we confirm that the proposed approach can align fingerprint features automatically in the domain of the fuzzy vault and can be integrated with any fuzzy fingerprint vault systems.\"",
        "Document: \"Fingerprint Template Protection Using One-Time Fuzzy Vault. The fuzzy vault scheme has emerged as a promising solution to user privacy and fingerprint template security problems. Recently, however, the fuzzy vault scheme has been shown to be susceptible to a correlation attack. This paper proposes a novel scheme for one-time templates for fingerprint authentication based on the fuzzy vault scheme. As in one-time passwords, the suggested method changes templates after each completion of authentication, and thus the compromised templates cannot be reused. Furthermore, a huge number of chaff minutiae can be added by expanding the size of the fingerprint image. Therefore, the proposed method can protect a user's fingerprint minutiae against the correlation attack. In our experiments, the proposed approach can improve the security level of a typical approach against brute-force attack by the factor of 10(34).\"",
        "1 is \"Indoor segmentation and support inference from RGBD images\", 2 is \"Object-based Surveillance Video Compression using Foreground Motion Compensation.\"",
        "Given above information, for an author who has written the paper with the title \"Load balancing strategies for symbolic vision computations.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009613": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Adaptive notch filtering in the presence of colored noise':",
        "Document: \"Detection and estimation of biochemical sources in arbitrary 2D environments. We develop algorithms for biochemical detection and estimation in arbitrary two-dimensional (2D) environments using integrated sensor arrays. The development of biochemical sensor techniques has been a subject of considerable research interest in recent years. We propose statistical algorithms for automatic monitoring of biochemical agents in realistically shaped 2D environments using multi-sensor measurements. We derive models for the concentration distribution using the diffusion equation and finite element approximations. Using these results, we develop parametric statistical models and a maximum likelihood estimation algorithm to find the parameters of the biochemical agent. To detect the presence of a source, we develop a generalized likelihood ratio test. We demonstrate the applicability of our techniques through numerical examples. Our results are potentially useful for national security, environmental engineering, food industry, oil industry, etc.\"",
        "Document: \"Adaptive notch filtering in the presence of colored noise. The authors analyze the convergence of several adaptive notch filter algorithms for sine waves in colored noise, which were recently proposed in the literature. After pointing out the previous algorithms' potential convergence problems, an algorithm is proposed for this problem that does not have convergence problems and provides accurate estimation results\"",
        "Document: \"Wideband source localization using a distributed acoustic vector-sensor array. We derive fast wideband algorithms, based on measurements of the acoustic intensity, for determining the bearings of a target using an acoustic vector sensor (AVS) situated in free space or on a reflecting boundary. We also obtain a lower bound on the mean-square angular error (MSAE) of such estimates. We then develop general closed-form weighted least-squares (WLS) and reweighted least-squares algorithms that compute the three-dimensional (3-D) location of a target whose bearing to a number of dispersed locations has been measured. We devise a scheme for adaptively choosing the weights for the WLS routine when measures of accuracy for the bearing estimates, such as the lower bound on the MSAE, are available. In addition, a measure of the potential estimation accuracy of a distributed system is developed based on a two-stage application of the Cramer-Rao bound. These 3-D results are quite independent of how bearing estimates are obtained. Naturally, the two parts of the paper are tied together by examining how well distributed arrays of AVSs located on the ground, seabed, and in free space can determine the 3-D position of a target The results are relevant to the localization of underwater and airborne sources using freely drifting, moored, or ground sensors. Numerical simulations illustrate the effectiveness of our estimators and the new potential performance measure.\"",
        "Document: \"MUSIC, maximum likelihood and Cramer-Rao bound: further results and comparisons. A number of results have been presented recently on the statistical performance of the multiple signal characterization (MUSIC) and the maximum-likelihood (ML) estimators for determining the direction of arrival of narrowband plane waves using sensor arrays and the related problem of estimating the parameters of superimposed signals from noisy measurements. It is shown that in the class of weighted MUSIC estimators, the unweighted MUSIC achieves the best performance (i.e. the minimum variance of estimation errors) in large samples. The covariance matrix of the ML estimator is derived, and detailed analytic studies of the statistical efficiency of MUSIC and ML estimators are presented. These studies include performance comparisons of MUSIC and MLE with each other as well as with the ultimate performance corresponding to the Cramer-Rao bound (CRB)\"",
        "Document: \"Multiple Rao-Blackwellized particle filtering for target tracking in urban environments. We propose a new filtering algorithm for joint tracking of multiple target states and the channel state between each pair of antennas in a radar network. The problem of tracking multiple targets in complex scenarios, such as an urban environment, poses a computational challenge as standard particle filtering (SPF) requires large number of particles to obtain an accurate estimate of the high-dimensional state vector. In this paper, we develop a hybrid filter based on the combination of multiple particle filtering (MPF) and Rao-Blackwellized particle filtering (RBPF) by exploiting the structure in the state-space model. Numerical simulations show that the proposed multiple Rao-Blackwellized particle filtering (MRBPF) performs better than the SPF and the RBPF.\"",
        "1 is \"Optimal Polarized Waveform Design For Active Target Parameter Estimation Using Electromagnetic Vector Sensors\", 2 is \"On the implementation and performance of single and double differential detection schemes\"",
        "Given above information, for an author who has written the paper with the title \"Adaptive notch filtering in the presence of colored noise\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009634": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Estimation methods for static noise margins in CMOS subthreshold logic circuits.':",
        "Document: \"Dynamic Task Mapping for MPSoCs. Multiprocessor-system-on-a-chip (MPSoC) applications can consist of a varying number of simultaneous tasks and can change even after system design, enforcing a scenario that requires the use of dynamic task mapping. This article investigates dynamic task-mapping heuristics targeting reduction of network congestion in network-on-chip (NoC)-based MPSoCs. The proposed heuristics achieve up to 31% smaller channel load and up to 22% smaller packet latency than other heuristics.\"",
        "Document: \"Early estimation of wire length for dedicated test access mechanisms in networks-on-chip based SoCs. The use of existing Networks-on-Chip (NoCs) for test data transportation has been proposed to avoid conventional dedicated Test Access Mechanism (TAM), improving the modularity of the test architecture. This paper presents a wire length estimation method used to evaluate the cost of dedicated TAMs for NoC-based SoCs early in the design flow. This wire length information (together with test time, power dissipation, among other test metrics) can help the designer to decide the best test architecture (NoC TAM or dedicated TAM) for a given chip. The experimental results demonstrate that dedicated TAMs require, on average, 26% of the global wires, enforcing quantitatively the benefits of NoC TAMs. On the other hand, results can vary depending on the SoC, from 3% to 70%, demonstrating the need of a fast wire length estimation in early stages of design.\"",
        "Document: \"A Low Area Overhead Packet-switched Network on Chip: Architecture and Prototyping. The increasing complexity of integrated circuits drives the research of new intra-chip interconnection architectures. A network-on-chip adapts concepts originated in the distributed systems and computer networks subject areas to connect IP cores in a structured and scalable way, pursuing the goal of achieving superior bandwidth to conventional intra-chip bus architectures. This paper presents the design of a switch targeted to a mesh interconnection topology. Each switch has 5 bi-directional ports, connecting 4 neighbor switches and a local IP core. They employ a XY routing algorithm, with input queue buffers. The main objective is to develop a switch with a small area, enabling its immediate practical use. The switch and a 2x2 mesh network were validated through functional simulation. Also the network has been successfully prototyped in hardware, using a million-gate FPGA.\"",
        "Document: \"Achieving QoS in NoC-based MPSoCs through Dynamic Frequency Scaling. The management of Quality-of-Service (QoS) constraints in NoC-based MPSoCs, with dozens of tasks running simultaneously, is still a challenge. Techniques applied at design or run-time to address this issue adopts different QoS metrics. Designers include in their systems monitoring techniques, adapting at run-time the QoS parameters to cope with the required constraints. In order words, MPSoC are able to self-adapt themselves, while executing a given set of applications. Self-adaptation capability is a key feature to meet applications' requirements in dynamic systems. Dynamic Voltage and Frequency Scaling (DVFS) is an adaptation technique frequently used to reduce the overall energy consumption, not coupled to QoS constraints, as throughput or latency. Another example of adaptation technique is task migration, which focus on throughput or latency optimization. The self-adaptation technique proposed in this paper adopts Dynamic Frequency Scaling (DFS) trading-off power consumption and QoS constraints. Each processor running the applications' tasks initially reaches a steady state leading each task to a frequency level that optimizes the communication with neighbor tasks. The goal of the initial state is to reach a trade-off between power consumption and communication throughput. Next, the application performance is monitored to adjust the frequency level of each task according to the QoS parameters. Results show that the proposed self-adaptability scheme can meet the required QoS constraints, by changing the frequency of the PEs running the application tasks.\"",
        "Document: \"Evaluating energy consumption of homogeneous MPSoCs using spare tiles. The yield of homogeneous network-on-chip based multi-processor chips can be improved with the addition of spare tiles. However, the impact of this reliability approach on the chip energy consumption is not documented. For instance, in a homogeneous MPSoC, application tasks can be placed onto any tile of a defect-free chip. On the other hand, a chip with defective tile needs a special task placement, where the faulty tile is avoided. This paper presents a task placement tool and the evaluation of energy consumption of homogeneous NoC-based MPSoCs with spare tiles. Results show NoC energy consumption overhead ranging from 1 to 10% when considering up to three faults randomly distributed over the tiles of a 3\u00d74 mesh network. The results also indicate that faults on the central tiles typically have more impact on energy overhead.\"",
        "1 is \"Design Challenges for a Differential-Power-Analysis Aware GALS-based AES Crypto ASIC\", 2 is \"High Throughput Multitransform And Multiparallelism Ip For H.264/Avc Video Compression Standard\"",
        "Given above information, for an author who has written the paper with the title \"Estimation methods for static noise margins in CMOS subthreshold logic circuits.\", which reference is related? Just choose 1 or 2 without further explanation."
    ]
}