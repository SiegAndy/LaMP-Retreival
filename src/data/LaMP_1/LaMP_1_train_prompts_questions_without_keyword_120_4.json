{
    "009": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Spatial-Keyword Skyline Publish/Subscribe Query Processing Over Distributed Sliding Window Streaming Data':",
        "Document: \"A VLSI-based robot dynamics learning algorithm. A computationally efficient solution to the problem of identifying the dynamic parameters of a robot manipulator is presented. The method is based on a simplified representation of the dynamics based on the Lagrange-Euler formulation. For parameter estimation, a recursive least squares technique is used. The approach has three important characteristics: (1) since they are based on the Lagrangian representation, the equations are linear in the dynamic parameters, enabling the application of linear identification techniques; (2) the dynamic parameters are easily recognized, extracted, and grouped; and (3) the equations are amenable to the implementation of parallel processing schemes. The algorithm was distributed over a network of transputers. Real-time results have been produced to demonstrate the speedup and efficiency of the proposed technique\"",
        "Document: \"A Performance Evaluation of CP List Scheduling Heuristics for Communication Intensive Task Graphs. List-based priority schedulers have long been one of the dominant classes of static scheduling algorithms. Such heuristics have been predominantly based around the \"critical path, most immediate successors first\" (CP/MISF) priority. The ability of this type of scheduler to handle increased levels of communication overhead is examined in this paper. Three of the more popular list scheduling heuristics, HLFET [1] and ISH and DSH [10], plus the Mapping Heuristic [4,6] are subjected to a performance based comparison, with results demonstrating their inadequacies in communication-intensive cases. Performance degradation in these instances is partly due to the level alteration problem, but more significantly to conservative estimation of communication costs due to the assumption of zero link contention. The significance of this component of communication is also examined in this paper.\"",
        "Document: \"Performance enhancement through hybrid replication and Genetic Algorithm co-scheduling in data grids. In data grid environments data-intensive applications require large amounts of data to execute. Data transfer is a primary cause of job execution delay. In this paper we study smart scheduling integrated with replica management optimization to improve system performance. We study the use of Genetic Algorithm (GA) for the scheduling phase of data-intensive applications. The schedulers proposed incorporate information about the datasets and their replicas needed by the jobs to be scheduled, and co-schedules the jobs and the datasets to the computation node guaranteeing minimum job execution time. We employ a data grid replica management framework for the optimization phase of the replica distribution. In this approach we try to achieve a double optimization effect from both the replica management and the scheduling phases, while integrating scheduling and data replication to improve the performance of the grid system. We evaluate and compare our Genetic Algorithm (GA) with a Tabu search (TS) and the de facto Max-Min based schedulers.\"",
        "Document: \"HPC-Based Intelligent Volt/VAr Control of Unbalanced Distribution Smart Grid in the Presence of Noise. The performance of Volt/VAr optimization has been significantly improved due to the integration of measurement data obtained from the advanced metering infrastructure of a smart grid. However, most of the existing works lack: 1) realistic unbalanced multi-phase distribution system modeling; 2) scalability of the Volt/VAr algorithm for larger test system; and 3) ability to handle gross errors and n...\"",
        "1 is \"Energy characterization of a tiled architecture processor with on-chip networks\", 2 is \"Autoencoders, Minimum Description Length and Helmholtz Free Energy\"",
        "Given above information, for an author who has written the paper with the title \"Spatial-Keyword Skyline Publish/Subscribe Query Processing Over Distributed Sliding Window Streaming Data\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "0060": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'NTIRE 2017 Challenge on Single Image Super-Resolution: Methods and Results.':",
        "Document: \"Recognition of partially occluded objects using probabilistic ARG (attributed relational graph)-based matching. In this paper, we propose a novel 2-D partial matching algorithm using the model-based probabilistic analysis of feature correspondences in the relation vector space, which is quite robust to shape variations due to noise and occlusions and invariant to 2-D geometric transformations as well. We represent an object using the attributed relational graph (ARG) model with nodes (features) of a set of the binary relation vectors. By defining relation vector space which can describe the structural information of an object centered at a specific feature, and modeling distortions due to partial occlusion or the input noise statistically in this space, lost features can be easily identified, so that the partial matching is performed efficiently. The proposed partial matching algorithm consists of two-phases. First, a finite number of candidate subgraphs are selected in an image, by using the logical constraint embedding local and structural consistency as well as the correspondence measure between model and image features. Second, the feature loss detection is done iteratively by the error detection and voting scheme through the error analysis in the relation vector space. Experimental results on real images demonstrate that the proposed algorithm has superior performance to those of the conventional relaxation algorithms, by localizing target objects robustly and correctly even in severely noisy and occluded scenes.\"",
        "Document: \"Deep Multi-scale Convolutional Neural Network for Dynamic Scene Deblurring. Non-uniform blind deblurring for general dynamic scenes is a challenging computer vision problem as blurs arise not only from multiple object motions but also from camera shake, scene depth variation. To remove these complicated motion blurs, conventional energy optimization based methods rely on simple assumptions such that blur kernel is partially uniform or locally linear. Moreover, recent machine learning based methods also depend on synthetic blur datasets generated under these assumptions. This makes conventional deblurring methods fail to remove blurs where blur kernel is difficult to approximate or parameterize (e.g. object motion boundaries). In this work, we propose a multi-scale convolutional neural network that restores sharp images in an end-to-end manner where blur is caused by various sources. Together, we present multi-scale loss function that mimics conventional coarse-to-fine approaches. Furthermore, we propose a new large-scale dataset that provides pairs of realistic blurry image and the corresponding ground truth sharp image that are obtained by a high-speed camera. With the proposed model trained on this dataset, we demonstrate empirically that our method achieves the state-of-the-art performance in dynamic scene deblurring not only qualitatively, but also quantitatively.\"",
        "Document: \"A genetic algorithm with local map for path planning in dynamic environments. In this paper, a new genetic algorithm (GA) for solving the path planning in dynamic environments is proposed. The new genetic algorithm uses local maps, therefore, does not require the knowledge of exact or estimated position of the destination point as other approaches in the literature. Consequently, the new GA could be used to solve the problem of dynamic path planning under an assumption that makes the problem more restrictive but more close to reality (in searching tasks).\"",
        "Document: \"Tracking by Sampling and IntegratingMultiple Trackers. We propose the visual tracker sampler, a novel tracking algorithm that can work robustly in challenging scenarios, where several kinds of appearance and motion changes of an object can occur simultaneously. The proposed tracking algorithm accurately tracks a target by searching for appropriate trackers in each frame. Since the real-world tracking environment varies severely over time, the trackers should be adapted or newly constructed depending on the current situation, so that each specific tracker takes charge of a certain change in the object. To do this, our method obtains several samples of not only the states of the target but also the trackers themselves during the sampling process. The trackers are efficiently sampled using the Markov Chain Monte Carlo (MCMC) method from the predefined tracker space by proposing new appearance models, motion models, state representation types, and observation types, which are the important ingredients of visual trackers. All trackers are then integrated into one compound tracker through an Interacting MCMC (IMCMC) method, in which the trackers interactively communicate with one another while running in parallel. By exchanging information with others, each tracker further improves its performance, thus increasing overall tracking performance. Experimental results show that our method tracks the object accurately and reliably in realistic videos, where appearance and motion drastically change over time, and outperforms even state-of-the-art tracking methods.\"",
        "1 is \"Markov Random Field modeling, inference & learning in computer vision & image understanding: A survey\", 2 is \"Neighborhood formation and anomaly detection in bipartite graphs\"",
        "Given above information, for an author who has written the paper with the title \"NTIRE 2017 Challenge on Single Image Super-Resolution: Methods and Results.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00188": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Third international workshop on software product management -- IWSPM'09':",
        "Document: \"Challenges and practices in aligning requirements with verification and validation: a case study of six companies. Weak alignment of requirements engineering (RE) with verification and validation (VV) may lead to problems in delivering the required products in time with the right quality. For example, weak communication of requirements changes to testers may result in lack of verification of new requirements and incorrect verification of old invalid requirements, leading to software quality problems, wasted effort and delays. However, despite the serious implications of weak alignment research and practice both tend to focus on one or the other of RE or VV rather than on the alignment of the two. We have performed a multi-unit case study to gain insight into issues around aligning RE and VV by interviewing 30 practitioners from 6 software developing companies, involving 10 researchers in a flexible research process for case studies. The results describe current industry challenges and practices in aligning RE with VV, ranging from quality of the individual RE and VV activities, through tracing and tools, to change control and sharing a common understanding at strategy, goal and design level. The study identified that human aspects are central, i.e. cooperation and communication, and that requirements engineering practices are a critical basis for alignment. Further, the size of an organisation and its motivation for applying alignment practices, e.g. external enforcement of traceability, are variation factors that play a key role in achieving alignment. Our results provide a strategic roadmap for practitioners improvement work to address alignment challenges. Furthermore, the study provides a foundation for continued research to improve the alignment of RE with VV.\"",
        "Document: \"Empirical evidence in global software engineering: a systematic review. Recognized as one of the trends of the 21st century, globalization of the world economies brought significant changes to nearly all industries, and in particular it includes software development. Many companies started global software engineering (GSE) to benefit from cheaper, faster and better development of software systems, products and services. However, empirical studies indicate that achieving these benefits is not an easy task. Here, we report our findings from investigating empirical evidence in GSE-related research literature. By conducting a systematic review we observe that the GSE field is still immature. The amount of empirical studies is relatively small. The majority of the studies represent problem-oriented reports focusing on different aspects of GSE management rather than in-depth analysis of solutions for example in terms of useful practices or techniques. Companies are still driven by cost reduction strategies, and at the same time, the most frequently discussed recommendations indicate a necessity of investments in travelling and socialization. Thus, at the same time as development goes global there is an ambition to minimize geographical, temporal and cultural separation. These are normally integral parts of cross-border collaboration. In summary, the systematic review results in several descriptive classifications of the papers on empirical studies in GSE and also reports on some best practices identified from literature.\"",
        "Document: \"Large-scale information retrieval in software engineering - an experience report from industrial application. Software Engineering activities are information intensive. Research proposes Information Retrieval (IR) techniques to support engineers in their daily tasks, such as establishing and maintaining traceability links, fault identification, and software maintenance. We describe an engineering task, test case selection, and illustrate our problem analysis and solution discovery process. The objective of the study is to gain an understanding of to what extent IR techniques (one potential solution) can be applied to test case selection and provide decision support in a large-scale, industrial setting. We analyze, in the context of the studied company, how test case selection is performed and design a series of experiments evaluating the performance of different IR techniques. Each experiment provides lessons learned from implementation, execution, and results, feeding to its successor. The three experiments led to the following observations: 1) there is a lack of research on scalable parameter optimization of IR techniques for software engineering problems; 2) scaling IR techniques to industry data is challenging, in particular for latent semantic analysis; 3) the IR context poses constraints on the empirical evaluation of IR techniques, requiring more research on developing valid statistical approaches. We believe that our experiences in conducting a series of IR experiments with industry grade data are valuable for peer researchers so that they can avoid the pitfalls that we have encountered. Furthermore, we identified challenges that need to be addressed in order to bridge the gap between laboratory IR experiments and real applications of IR in the industry.\"",
        "Document: \"The GRADE taxonomy for supporting decision-making of asset selection in software-intensive system development. Abstract   Context  The development of software-intensive systems includes many decisions involving various stakeholders with often conflicting interests and viewpoints.    Objective  Decisions are rarely systematically documented and sporadically explored. This limits the opportunity for learning and improving on important decisions made in the development of software-intensive systems.    Method  In this work, we enable support for the systematic documentation of decisions, improve their traceability and contribute to potentially improved decision-making in strategic, tactical and operational contexts.    Results  We constructed a taxonomy for documentation supporting decision-making, called GRADE. GRADE was developed in a research project that required composition of a common dedicated language to make feasible the identification of new opportunities for better decision support and evaluation of multiple decision alternatives. The use of the taxonomy has been validated through thirty three decision cases from industry.    Conclusion  This paper occupies this important yet greatly unexplored research gap by developing the GRADE taxonomy that serves as a common vocabulary to describe and classify decision-making with respect to architectural assets.\"",
        "1 is \"An approach to managing feature dependencies for product releasing in software product lines\", 2 is \"Business Models in the Service World\"",
        "Given above information, for an author who has written the paper with the title \"Third international workshop on software product management -- IWSPM'09\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00227": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Intelligent multimedia indexing and retrieval through multi-source information extraction and merging':",
        "Document: \"Large-scale, parallel automatic patent annotation. When researching new product ideas or filing new patents, inventors need to retrieve all relevant pre-existing know-how and/or to exploit and enforce patents in their technological domain. However, this process is hindered by lack of richer metadata, which if present, would allow more powerful concept-based search to complement the current keyword-based approach. This paper presents our approach to automatic patent enrichment, tested in large-scale, parallel experiments on USPTO and EPO documents. It starts by defining the metadata annotation task and examines its challenges. The text analysis tools are presented next, including details on automatic annotation of sections, references and measurements. The key challenges encountered were dealing with ambiguities and errors in the data; creation and maintenance of large, domain-independent dictionaries; and building an efficient, robust patent analysis pipeline, capable of dealing with terabytes of data. The accuracy of automatically created metadata is evaluated against a human-annotated gold standard, with results of over 90% on most annotation types.\"",
        "Document: \"Web-assisted annotation, semantic indexing and search of television and radio news. The Rich News system, that can automatically annotate radio and television news with the aid of resources retrieved from the World Wide Web, is described. Automatic speech recognition gives a temporally precise but conceptually inaccurate annotation model. Information extraction from related web news sites gives the opposite: conceptual accuracy but no temporal data. Our approach combines the two for temporally accurate conceptual semantic annotation of broadcast news. First low quality transcripts of the broadcasts are produced using speech recognition, and these are then automatically divided into sections corresponding to individual news stories. A key phrases extraction component finds key phrases for each story and uses these to search for web pages reporting the same event. The text and meta-data of the web pages is then used to create index documents for the stories in the original broadcasts, which are semantically annotated using the KIM knowledge management platform. A web interface then allows conceptual search and browsing of news stories, and playing of the parts of the media files corresponding to each news story. The use of material from the World Wide Web allows much higher quality textual descriptions and semantic annotations to be produced than would have been possible using the ASR transcript directly. The semantic annotations can form a part of the Semantic Web, and an evaluation shows that the system operates with high precision, and with a moderate level of recall.\"",
        "Document: \"NE recognition without training data on a language you don't speak. In this paper we describe an experiment to adapt a named entity recognition system from English to Cebuano as part of the TIDES surprise language program. With 4 person-days of effort, and with no previous knowledge of which language would be involved, no knowledge of the language in question once it was announced, and no training data available, we adapted the ANNIE system for Cebuano and achieved an F-measure of 77.5%.\"",
        "Document: \"Towards Controlled Natural Language for Semantic Annotation. Richly interlinked metadata constitute the foundation of the Semantic Web. Manual semantic annotation is a labor intensive task requiring training in formal ontological descriptions for the otherwise non-expert user. Although automatic annotation tools attempt to ease this knowledge acquisition barrier, their development often requires access to specialists in Natural Language Processing NLP. This challenges researchers to develop user-friendly annotation environments. Controlled Natural Languages CNLs offer an incentive to the novice user to annotate, while simultaneously authoring his/her respective documents in a user-friendly manner. CNLs have been successfully applied to ontology authoring, but little research has focused on their application to semantic annotation. This paper describes two novel approaches to semantic annotation, which permit non-expert users to simultaneously author and annotate meeting minutes using CNL. Finally, this work provides empirical evidence that for certain scenarios applying CNLs for semantic annotation can be more user friendly than a standard manual semantic annotation tool.\"",
        "1 is \"Firefixia: an accessibility web browser customization toolbar for people with dyslexia\", 2 is \"Is Word Sense Disambiguation just one more NLP task?\"",
        "Given above information, for an author who has written the paper with the title \"Intelligent multimedia indexing and retrieval through multi-source information extraction and merging\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00250": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Noninvasive BCIs: Multiway Signal-Processing Array Decompositions':",
        "Document: \"Differential learning and random walk model. This paper presents a learning algorithm for differential decorrelation, the goal of which is to find a linear transform that minimizes the concurrent change of associated output nodes. First the algorithm is derived from the minimization of the objective function which measures the differential correlation. Then we show that the differential decorrelation learning algorithm can also be derived in the framework of maximum likelihood estimation of a linear generative model with assuming a random walk model for latent variables. Algorithm derivation and local stability analysis are given with a simple numerical example.\"",
        "Document: \"Nonnegative Tucker Decomposition. Nonnegative tensor factorization (NTF) is a recent multiway (multilinear) extension of nonnegative matrix factorization (NMF), where nonnegativity constraints are imposed on the CANDECOMP/PARAFAC model. In this paper we consider the Tucker model with nonnegativity constraints and develop a new tensor factorization method, referred to as nonnegative Tucker decomposition (NTD). The main contributions of this paper include: (1) multiplicative updating algorithms for NTD; (2) an initialization method for speeding up convergence; (3) a sparseness control method in tensor factorization. Through several computer vision examples, we show the useful behavior of the NTD, over existing NTF and NMF methods.\"",
        "Document: \"Soft Geodesic Kernel K-Means. In this paper we present a kernel method for data clustering, where the soft k-means is carried out in a feature space, instead of input data space, leading to soft kernel k-means. We also incorporate a geodesic kernel into the soft kernel k-means, in order to take the data manifold structure into account. The method is referred to as soft geodesic kernel k-means. In contrast to k-means, our method is able to identify clusters that are not linearly separable. In addition, soft responsibilities as well as geodesic kernel, improve the clustering performance, compared to kernel k-means. Numerical experiments with toy data sets and real-world data sets (UCI and document clustering), confirm the useful behavior of the proposed method.\"",
        "Document: \"Principal network analysis: identification of subnetworks representing major dynamics using gene expression data. Motivation: Systems biology attempts to describe complex systems behaviors in terms of dynamic operations of biological networks. However, there is lack of tools that can effectively decode complex network dynamics over multiple conditions. Results: We present principal network analysis (PNA) that can automatically capture major dynamic activation patterns over multiple conditions and then generate protein and metabolic subnetworks for the captured patterns. We first demonstrated the utility of this method by applying it to a synthetic dataset. The results showed that PNA correctly captured the subnetworks representing dynamics in the data. We further applied PNA to two time-course gene expression profiles collected from (i) MCF7 cells after treatments of HRG at multiple doses and (ii) brain samples of four strains of mice infected with two prion strains. The resulting subnetworks and their interactions revealed network dynamics associated with HRG dose-dependent regulation of cell proliferation and differentiation and early PrPSc accumulation during prion infection.\"",
        "1 is \"Particular object retrieval with integral max-pooling of CNN activations\", 2 is \"FastMap, MetricMap, and Landmark MDS are all Nystrom Algorithms\"",
        "Given above information, for an author who has written the paper with the title \"Noninvasive BCIs: Multiway Signal-Processing Array Decompositions\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00264": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Owner-Centric Networking (OCN): Toward a Data Pollution-Free Internet':",
        "Document: \"Towards clock skew based services in wireless sensor networks. Clock skew, an inherent property of clock crystals of physical devices, is defined as the rate of deviation of a device clock from the true time. The frequency of a device's clock actually depends on its environment, such as the temperature, humidity, vibration, electromagnetic interference, as well as the type of crystal. The main contributions of this paper are twofold. First, we experimentally validate that MICAz and TelosB sensor motes have different and unique clock skews. Furthermore, the clock skew of a node can easily be monitored, even via a multi-hop Wireless Sensor Network (WSN). We argue that this feature can be used for identification of the nodes, detection of wormhole and Sybil attacks. Second, we show that the clock skew of a sensor node varies with the variation of temperature. We explain how this property can be used to detect malicious and malfunctioning nodes and to geolocalise them. We also discuss the pros and cons of utilisation of the above two properties for different services in WSNs.\"",
        "Document: \"Efficient and provably secure aggregation of encrypted data in wireless sensor networks. Wireless sensor networks (WSNs) are composed of tiny devices with limited computation and battery capacities. For such resource-constrained devices, data transmission is a very energy-consuming operation. To maximize WSN lifetime, it is essential to minimize the number of bits sent and received by each device. One natural approach is to aggregate sensor data along the path from sensors to the sink. Aggregation is especially challenging if end-to-end privacy between sensors and the sink (or aggregate integrity) is required. In this article, we propose a simple and provably secure encryption scheme that allows efficient additive aggregation of encrypted data. Only one modular addition is necessary for ciphertext aggregation. The security of the scheme is based on the indistinguishability property of a pseudorandom function (PRF), a standard cryptographic primitive. We show that aggregation based on this scheme can be used to efficiently compute statistical values, such as mean, variance, and standard deviation of sensed data, while achieving significant bandwidth savings. To protect the integrity of the aggregated data, we construct an end-to-end aggregate authentication scheme that is secure against outsider-only attacks, also based on the indistinguishability property of PRFs.\"",
        "Document: \"I have a DREAM!: differentially private smart metering. This paper presents a new privacy-preserving smart metering system. Our scheme is private under the differential privacy model and therefore provides strong and provable guarantees.With our scheme, an (electricity) supplier can periodically collect data from smart meters and derive aggregated statistics without learning anything about the activities of individual households. For example, a supplier cannot tell from a user's trace whether or when he watched TV or turned on heating. Our scheme is simple, efficient and practical. Processing cost is very limited: smart meters only have to add noise to their data and encrypt the results with an efficient stream cipher.\"",
        "Document: \"MyAdChoices: Bringing Transparency and Control to Online Advertising. The intrusiveness and the increasing invasiveness of online advertising have, in the last few years, raised serious concerns regarding user privacy and Web usability. As a reaction to these concerns, we have witnessed the emergence of a myriad of ad-blocking and antitracking tools, whose aim is to return control to users over advertising. The problem with these technologies, however, is that they are extremely limited and radical in their approach: users can only choose either to block or allow all ads. With around 200 million people regularly using these tools, the economic model of the Web\u2014in which users get content free in return for allowing advertisers to show them ads\u2014is at serious peril. In this article, we propose a smart Web technology that aims at bringing transparency to online advertising, so that users can make an informed and equitable decision regarding ad blocking. The proposed technology is implemented as a Web-browser extension and enables users to exert fine-grained control over advertising, thus providing them with certain guarantees in terms of privacy and browsing experience, while preserving the Internet economic model. Experimental results in a real environment demonstrate the suitability and feasibility of our approach, and provide preliminary findings on behavioral targeting from real user browsing profiles.\"",
        "1 is \"A new appraoch to server-aided secret computation\", 2 is \"The Kerberos Network Authentication Service (V5)\"",
        "Given above information, for an author who has written the paper with the title \"Owner-Centric Networking (OCN): Toward a Data Pollution-Free Internet\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00371": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Designing Mobile Interactions for the Ageing Populations.':",
        "Document: \"Determining the Optimal Size of Handwriting Character Entry Boxes for Pen-Based Systems. This paper presents two experiments to determine the optimal size of a box for the input of Chinese characters (called Kanji in Japanese) mixed with Japanese Kana (phonetic) handwriting characters on a tablet computer. The experiments involve an evaluation of boxes of different sizes. The results are assessed in terms of high performance factors (e.g., high character recognition rates, minimal stroke protrusions outside the character box) and subjective ratings (e.g., ease of writing and a minimum level of fatigue). The results show that the optimal size of character boxes for Kanji and Kana characters is around 1.44 x 1.44 cm. The experimental results also show that the optimal size is the same for both PDAs and Tablet PCs. The results provide important guidelines to designers who are concerned about the interplay between user comfort, input efficiency and output display efficiency. The knowledge obtained from our experiments on Tablet PCs will be added to the literature on optimal size for handwriting character entry boxes.\"",
        "Document: \"HUA: An Interactive Calligraphy and Ink-Wash Painting System. This paper presents a novel Chinese calligraphy and painting system, which servers as an expressive vehicle for interactively creating Chinese traditional calligraphy and ink-wash painting works. We developed a calligraphy-brush model which gives the user natural control of complex calligraphy-brush strokes. This model enhances the realistic sense of the user's manipulating the pen with the pressure sense. We also develop an ink-wash painting model which simulates Tuiyun - a unique Chinese ink-wash painting technique. User feedback has shown that the system is able to create vivid art works easily and smoothly.\"",
        "Document: \"Improving selection performance on pen-based systems: a study of pen-based interaction for selection tasks. Two experiments were conducted to compare pen-based selection strategies and their characteristics. Two state transition models were also formulated which provide new vocabulary that will help in investigating interactions related to target selection issues. Six strategies, which can be described by the state transition models, were used in the experiments. We determined the best strategy of the six to be the \u201cSlide Touch\u201d strategy, where the target is selected at the moment the pen-tip touches the target for the first time after landing on the screen surface. The six strategies were also classified into strategy groups according to their characteristics. We determined the best strategy group to be the \u201cIn-Out\u201d strategy group, where the target is selected by contact either inside or outside the target. Analyses show that differences between strategies are influenced by variations in target size; however, the differences between strategies are not affected by the distance to the target (i.e., pen-movement-distance) or the direction of pen movement (i.e., pen-movement-direction). We also found \u201cthe smallest maximum size\u201d of five pixels, i.e., the boundary value for the target size below which there are significant differences, and above which there are no significant differences between the strategies in error rate. Relationships between interaction states, routes, and strategy efficiency were also investigated.\"",
        "Document: \"Human Computer Integration versus Powerful Tools. In 1960, JCR Licklider forecast three phases for how humans relate to machines: human-computer interaction, human-computer symbiosis, and ultra-intelligent machines. Have we moved from interaction to symbiosis or integration, should we focus on this or on other aspects of human augmentation via powerful tools, and how will such decisions affect us as designers, researchers, and members of society? This panel will raise uneasy and disruptive HCI notions. For example, we will debate whether integration is a necessary and desirable next phase, or whether it could undermine human self-efficacy and control and lessen the predictability of machine actions.\"",
        "1 is \"Whole-home gesture recognition using wireless signals\", 2 is \"A Solid Model Based Virtual Hairy Brush\"",
        "Given above information, for an author who has written the paper with the title \"Designing Mobile Interactions for the Ageing Populations.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00387": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A Fast and Memory-Efficient Trie Structure for Name-Based Packet Forwarding':",
        "Document: \"Extending Battery System Operation via Adaptive Reconfiguration. Large-scale battery packs are commonly used in applications such as electric vehicles\u00a0(EVs) and smart grids. Traditionally, to provide stable voltage to the loads, voltage regulators are used to convert battery packs\u2019 output voltage to those of the loads\u2019 required levels, causing power loss especially when the difference between the supplied and required voltages is large or when the load is light. In this article, we address this issue via a reconfiguration framework for the battery system. By abstracting the battery system as a cell graph, we develop an adaptive reconfiguration algorithm to identify the desired system configurations based on real-time load requirements. Our design is evaluated via both prototype-based experiments, EV driving trace-based emulations, and large-scale simulations. The results demonstrate an extended system operation time of up to 5\u00d7, especially when facing severe cell imbalance.\n\n\"",
        "Document: \"Joint Energy Replenishment and Operation Scheduling in Wireless Rechargeable Sensor Networks. Wireless charging is a promising way to solve the energy constraint problem in sensor networks. While extensive efforts have been made to improve the performance of charging and communication in wireless rechargeable sensor networks (WRSNs), little has been done to address the operation scheduling problem. To fill this void, we propose a joint energy replenishment and scheduling mechanism so as to...\"",
        "Document: \"Prevention of congestion in packet-switched multistage interconnection networks. This paper proposes a simple, yet effective scheme to prevent congestion in a packet-switched multistage interconnection network (MIN) caused by hot spots. In this scheme, switches in the second and third stages of the MIN monitor their buffer occupancy to detect any notable nonuniform access behavior. When a switch detects congestion, packets generated by processors will be blocked from entering the congested switch until the congestion is cleared. Our scheme is compared with two well known schemes and shown to exhibit significantly better performance than these two\"",
        "Document: \"On building a lightweight security architecture for sensor networks. Sensor networks are characterized by their large-scale and unattended deployment that invites numerous critical attacks, thereby necessitating high-level security support for their intended applications and services. However, making sensor networks secure is challenging due mainly to the fact that sensors are battery-powered and it is usually very difficult to change or recharge their batteries. In this paper, we give a comprehensive overview of recent research results for securing such sensor networks, and then describe how to build a security framework, called a Lightweight Security Architecture (LiSA), for sensor networks, which achieves energy-aware security via closely-coupled, mutually-complementary security solutions.\"",
        "1 is \"Essential roles of exploiting internal parallelism of flash memory based solid state drives in high-speed data processing\", 2 is \"Source selectable path diversity via routing deflections\"",
        "Given above information, for an author who has written the paper with the title \"A Fast and Memory-Efficient Trie Structure for Name-Based Packet Forwarding\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00467": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Progress and prospects of the human\u2013robot collaboration':",
        "Document: \"Implementation Of The Micro-Macro Teleoperation System Without Using Slave-Side Force Sensors. This paper proposes an alternative control algorithm for scaled teleoperation system. In micro environment since force acts on the environment as well as its dimension is small, appropriate size or precision of force sensor for controlling manipulator is not always available. So, it is important to implement bilateral scaled teleoperation system without using force sensors in micro environment. The proposed algorithm utilizes mass property of the slave manipulator as it is, it could be implemented without using slave-side force sensors. The stability of the system is also guaranteed based on passivity properties of the system, for any sealing ratio between master and slave. The proposed algorithm is applied to experimental micro-macro teleoperation system and experimental results illustrate the validity of the system.\"",
        "Document: \"Coordinated motion control of dual manipulators for handling a rigid object with non-negligible deformation. In this paper, we propose a coordinated motion control algorithm of dual manipulators for handling of an almost rigid object but with non-negligible deformation. By the proposed algorithm, an almost rigid object with non-negligible deformation, such as a cardboard box, a plastic case, etc., is held and manipulated by dual manipulators stably without producing any vibrations. By defining apparent dynamics of dual manipulators for the interface force between the object and its environment and the dissipative dynamics for the internal force applied by the manipulators to the object independently, stable manipulation is achieved. Experimental results illustrate that the vibratory motion of the manipulated object caused by non-negligible deformation is eliminated by the proposed control algorithm while the apparent dynamics of the manipulated object is realized.\"",
        "Document: \"Input/output force analysis of parallel link manipulators. A method for evaluating kinematic structures of parallel link manipulators based on the relation between actuator forces/moments and output forces/moments of the mechanism is proposed. The input/output force analyses proposed previously have been based on the manipulator Jacobian, in which forces and moments are dealt with together. Manipulators are analyzed in the present work by dealing with the output forces and the output moments independently. The translatability and the rotatability of the manipulators are evaluated on the basis of the output forces and moments for given input forces and moments. The kinematic structure is then analyzed on the basis of translatability and rotatability of the manipulators. The method is natural and intuitive, in the sense that forces and moments having different dimensions are dealt with separately, and is useful for the design of the manipulator kinematic structure\"",
        "Document: \"Intelligent control of assembling robot using vision sensor. Intelligent control of an assembly robot using vision is presented. In assembling, the task is planned by the task planner, and the corresponding robot trajectory is planned as the reference trajectory. The robot should also adapt to environmental variation. To coordinate the planner and robot, a virtual internal model is used to modify the planned reference trajectory according to the environmental variation. The use of a force and vision sensor is proposed in the virtual internal model for error correction in assembly. A simple assembling task is experimentally achieved to verify the validity of the proposed method. The task considered is screwing a bolt with an open-ended wrench fixed to the wrist of the robot, where the bolt is not accurately placed at the fixed predetermined place. The experiment shows that the proposed control system works satisfactorily\"",
        "1 is \"Modeling and control of cooperative teleoperation systems\", 2 is \"A mathematical model of the adaptive control of human arm motions.\"",
        "Given above information, for an author who has written the paper with the title \"Progress and prospects of the human\u2013robot collaboration\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00521": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Maturity is also about the capability to conform the process to the right context!':",
        "Document: \"Eliciting an Enhancive Maintenance Model in Three Organisations in Ghana. To continuously meet the changing needs of the customers, organisations follow their own enhancive maintenance process models. Unfortunately, these models are not easily available to the academic world. In this paper, we study enhancive maintenance in three organisations in Ghana. Our goal is to explore information about the initial phases of the enhancive maintenance process and put them into a process model. Our model makes provision for process phases, their activities, roles, and data needed for managing enhancements. Although the primary goal of this paper is to elicit an enhancive maintenance model, we will not deny that it also provides the information on the state of enhancive maintenance practice in one developing country - Ghana.\"",
        "Document: \"Outlining a Model Integrating Risk Management and Agile Software Development. Previous studies show that the agile development models implement very few risk management practices. In this paper, we present and evaluate a model integrating the risk management and agile processes. The results show that the model provides a valid solution to address the lack of risk management, however, only in certain types of agile projects.\"",
        "Document: \"Commonalities in Risk Management and Agile Process Models. On the surface, agile and risk management process models seem to constitute two contrasting approaches. Risk management follows a heavyweight approach whereas agile process models oppose it. In this paper, we identify commonalities in these two process models. Our results show that they have much in common, and that a merge between them is possible.\"",
        "Document: \"A framework for the evolution and maintenance of Web services. In this paper, we propose SERVIAM maintenance framework - a framework for evolving and maintaining Web services. Our framework includes organisation, role, and process changes to accommodate Web service product and use characteristics.\"",
        "1 is \"Can we learn anything from hardware preventive maintenance?\", 2 is \"Sliding Spotlight Sar Processing For Terrasar-X Using A New Fon-Nulation Of The Extended Chirp Scaling Algorithm\"",
        "Given above information, for an author who has written the paper with the title \"Maturity is also about the capability to conform the process to the right context!\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00531": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Q-MAT: Computing Medial Axis Transform By Quadratic Error Minimization.':",
        "Document: \"Real-time 3D interaction with deformable model on mobile devices. Mobile-based augmented reality is an emerging technology that provides immersive experiences over wireless networks. Its applications, such as 3D streaming, have become more and more popular on mobile devices. However, most of these applications do not support real-time 3D interactions. Consequently, mobile users can only watch or browse 3D contents passively instead of actively interacting with 3D objects. In this paper, we propose a novel method that allows mobile users to change a model's shape and motions through interactions via mobile touch screen and obtain feedbacks in real-time. To accelerate computational speed and reduce communication load, we compute 3D deformations using a spectral representation. Moreover, a progressive deformation streaming technique is proposed to reduce the effect of interaction delay between the server and mobile clients. Our experimental results indicate that our method provides real-time interaction feedback, offering satisfactory user experiences.\"",
        "Document: \"Meshless thin-shell simulation based on global conformal parameterization. This paper presents a new approach to the physically-based thin-shell simulation of point-sampled geometry via explicit, global conformal point-surface parameterization and meshless dynamics. The point-based global parameterization is founded upon the rigorous mathematics of Riemann surface theory and Hodge theory. The parameterization is globally conformal everywhere except for a minimum number of zero points. Within our parameterization framework, any well-sampled point surface is functionally equivalent to a manifold, enabling popular and powerful surface-based modeling and physically-based simulation tools to be readily adapted for point geometry processing and animation. In addition, we propose a meshless surface computational paradigm in which the partial differential equations (for dynamic physical simulation) can be applied and solved directly over point samples via Moving Least Squares (MLS) shape functions defined on the global parametric domain without explicit connectivity information. The global conformal parameterization provides a common domain to facilitate accurate meshless simulation and efficient discontinuity modeling for complex branching cracks. Through our experiments on thin-shell elastic deformation and fracture simulation, we demonstrate that our integrative method is very natural, and that it has great potential to further broaden the application scope of point-sampled geometry in graphics and relevant fields.\"",
        "Document: \"Immersive multiplayer tennis with microsoft kinect and body sensor networks. We present an immersive gaming demonstration using the minimum amount of wearable sensors. The game demonstrated is two-player tennis. We combine a virtual environment with real 3D representations of physical objects like the players and the tennis racquet (if available). The main objective of the game is to provide as real an experience of tennis as possible, while also being as less intrusive as possible. The game is played across a network, and this opens the possibility of two remote players playing a game together on a single virtual tennis pitch. The Microsoft Kinect sensors are used to obtain a 3D point cloud and a skeletal map representation of the player. This 3D point cloud is mapped on to the virtual tennis pitch. We also use a wireless wearable Attitude and Heading Reference System (AHRS) mote, which is strapped onto the wrist of the players. This mote gives us precise information about the movement (swing, rotation etc.) of the playing arm. This information along with the skeletal map is used to implement the physics of the game. Using this game we demonstrate our solutions for simultaneous data acquisition, 3D point-cloud mapping in a virtual space, use of the Kinect and AHRS sensors to calibrate real and virtual objects and for interaction of virtual objects with a 3D point cloud.\"",
        "Document: \"Medial-axis-driven shape deformation with volume preservation. The medial axis is a natural skeleton for shapes. However, it is rarely used in the existing skeleton-based shape deformation techniques. In this paper, we propose a novel medial-axis-driven skin surface deformation algorithm with volume preservation property. Specifically, an as-rigid-as-possible deformation scheme is used to deform the medial axis so that its local transform is as close as possible to a rigid transform. We maintain surface features of the deformed shape based on an implicit skinning method. Our experiments show that the proposed algorithm effectively preserves the volume of deformed shape and addresses the bending and twisting problems associated with traditional skeleton-based shape deformation techniques.\"",
        "1 is \"Surface modeling with oriented particle systems\", 2 is \"Mixed Noise Removal via Laplacian Scale Mixture Modeling and Nonlocal Low-Rank Approximation.\"",
        "Given above information, for an author who has written the paper with the title \"Q-MAT: Computing Medial Axis Transform By Quadratic Error Minimization.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00544": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A Manifesto for Future Generation Cloud Computing: Research Directions for the Next Decade.':",
        "Document: \"Selective Data Encryption in Outsourced Dynamic Environments. The amount of information held by organizations' databases is increasing very quickly. A recently proposed solution to the problem of data management, which is becoming increasingly popular, is represented by database outsourcing. Several approaches have been presented to database outsourcing management, investigating the application of data encryption together with indexing information to allow the execution of queries at the third party, without the need of decrypting the data. These proposals assume access control to be under the control of the data owner, who has to filter all the access requests to data. In this paper, we put forward the idea of outsourcing also the access control enforcement at the third party. Our approach combines cryptography together with authorizations, thus enforcing access control via selective encryption. The paper describes authorizations management investigating their specification and representation as well as their enforcement in a dynamic scenario.\"",
        "Document: \"A Middleware Architecture for Integrating Privacy Preferences and Location Accuracy. Location-Based Access Control (LBAC) systems support the evaluation of conditions on locations in the enforcement of access control policies. The ability to evaluate conditions on a set of authorized locations has a number of well-known advantages, including enriching access control expressiveness. However, when locations are used in combination with personal identities, users privacy must be considered. In this paper, we describe a solution to integrate a LBAC system with privacy-enhanced techniques based on location obfuscation. Our solution is based on a privacy-aware middleware component that explicitly addresses the trade-off between users privacy and location accuracy by satisfying preferences set by users and maximizing the quality of location information released to LBAC systems.\"",
        "Document: \"A privacy-aware access control system. The protection of privacy is an increasing concern in our networked society because of the growing amount of personal information that is being collected by a number of commercial and public services. Emerging scenarios of user-service interactions in the digital world are then pushing toward the development of powerful and flexible privacy-aware models and languages. This paper aims at introducing concepts and features that should be investigated to fulfill this demand. We identify different types of privacy-aware policies: access control, release and data handling policies. The access control policies govern access/release of data/services managed by the party (as in traditional access control), and release policies govern release of personal identifiable information (PII) of the party and specify under which conditions it can be disclosed. The data handling policies allow users to specify and communicate to other parties the policy that should be enforced to deal with their data. We also discuss how data handling policies can be integrated with traditional access control systems and present a privacy control module in charge of managing, integrating, and evaluating access control, release and data handling policies.\"",
        "Document: \"Managing key hierarchies for access control enforcement: Heuristic approaches. Data outsourcing is emerging today as a successful paradigm allowing individuals and organizations to resort to external servers for storing their data, and sharing them with others. The main problem of this trend is that sensitive data are stored on a site that is not under the data owner's direct control. This scenario poses a major security problem since often the external server is relied upon for ensuring high availability of the data, but it is not authorized to read them. Data need therefore to be encrypted. In such a context, the application of an access control policy requires different data to be encrypted with different keys so to allow the external server to directly enforce access control and support selective dissemination and access. The problem therefore emerges of designing solutions for the efficient management of an encryption policy enforcing access control, with the goal of minimizing the number of keys to be maintained by the system and distributed to users. In this paper, we prove that the problem of minimizing the number of keys is NP-hard and present alternative approaches for its solution. We first formulate the minimization problem as an instance of an integer linear programming problem and then propose three different families of heuristics, which are based on a key derivation tree exploiting the relationships among user groups. Finally, we experimentally evaluate the performance of our heuristics, comparing them with previous approaches.\"",
        "1 is \"Backpressure multicast congestion control in mobile ad-hoc networks\", 2 is \"Credentials for privacy and interoperation\"",
        "Given above information, for an author who has written the paper with the title \"A Manifesto for Future Generation Cloud Computing: Research Directions for the Next Decade.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00591": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Action Recognition with Actons':",
        "Document: \"Crowd Fraud Detection in Internet Advertising. The rise of crowdsourcing brings new types of malpractices in Internet advertising. One can easily hire web workers through malicious crowdsourcing platforms to attack other advertisers. Such human generated crowd frauds are hard to detect by conventional fraud detection methods. In this paper, we carefully examine the characteristics of the group behaviors of crowd fraud and identify three persistent patterns, which are moderateness, synchronicity and dispersivity. Then we propose an effective crowd fraud detection method for search engine advertising based on these patterns, which consists of a constructing stage, a clustering stage and a filtering stage. At the constructing stage, we remove irrelevant data and reorganize the click logs into a surfer-advertiser inverted list; At the clustering stage, we define the sync-similarity between surfers' click histories and transform the coalition detection to a clustering problem, solved by a nonparametric algorithm; and finally we build a dispersity filter to remove false alarm clusters. The nonparametric nature of our method ensures that we can find an unbounded number of coalitions with nearly no human interaction. We also provide a parallel solution to make the method scalable to Web data and conduct extensive experiments. The empirical results demonstrate that our method is accurate and scalable.\"",
        "Document: \"Diversity-Promoting Bayesian Learning of Latent Variable Models. In learning latent variable models (LVMs), it is important to effectively capture infrequent patterns and shrink model size without sacrificing modeling power. Various studies have been done to \\\"diversify\\\" a LVM, which aim to learn a diverse set of latent components in LVMs. Most existing studies fall into a frequentist-style regularization framework, where the components are learned via point estimation. In this paper, we investigate how to \\\"diversify\\\" LVMs in the paradigm of Bayesian learning, which has advantages complementary to point estimation, such as alleviating overfitting via model averaging and quantifying uncertainty. We propose two approaches that have complementary advantages. One is to define diversity-promoting mutual angular priors which assign larger density to components with larger mutual angles based on Bayesian network and von Mises-Fisher distribution and use these priors to affect the posterior via Bayes rule. We develop two efficient approximate posterior inference algorithms based on variational inference and Markov chain Monte Carlo sampling. The other approach is to impose diversity-promoting regularization directly over the post-data distribution of components. These two methods are applied to the Bayesian mixture of experts model to encourage the \\\"experts\\\" to be diverse and experimental results demonstrate the effectiveness and efficiency of our methods.\"",
        "Document: \"Pose-Guided Human Parsing by an AND/OR Graph Using Pose-Context Features. Parsing human into semantic parts is crucial to human-centric analysis. In this paper, we propose a human parsing pipeline that uses pose cues, i.e., estimates of human joint locations, to provide pose-guided segment proposals for semantic parts. These segment proposals are ranked using standard appearance cues, deep-learned semantic feature, and a novel pose feature called pose-context. Then these proposals are selected and assembled using an And-Or graph to output a parse of the person. The And-Or graph is able to deal with large human appearance variability due to pose, choice of clothes, etc. We evaluate our approach on the popular Penn-Fudan pedestrian parsing dataset, showing that it significantly outperforms the state-of-the-arts, and perform diagnostics to demonstrate the effectiveness of different stages of our pipeline.\"",
        "Document: \"Online Bayesian Passive-Aggressive Learning. We present online Bayesian Passive-Aggressive (BayesPA) learning, a generic online learning framework for hierarchical Bayesian models with max-margin posterior regularization. We show that BayesPA subsumes the standard online Passive-Aggressive (PA) learning and extends naturally to incorporate latent variables for both parametric and nonparametric Bayesian inference, therefore providing great flexibility for explorative analysis. As an important example, we apply BayesPA to topic modeling and derive efficient online learning algorithms for max-margin topic models. We further develop nonparametric BayesPA topic models to infer the unknown number of topics in an online manner. Experimental results on 20newsgroups and a large Wikipedia multi-label dataset (with 1.1 millions of training documents and 0.9 million of unique terms in the vocabulary) show that our approaches significantly improve time efficiency while achieving comparable accuracy with the corresponding batch algorithms.\"",
        "1 is \"Democratic co-learning\", 2 is \"Interactively building a discriminative vocabulary of nameable attributes\"",
        "Given above information, for an author who has written the paper with the title \"Action Recognition with Actons\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00594": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Analysing the cognitive effectiveness of the UCM visual notation':",
        "Document: \"Synthesizing SDL from use case maps: an experiment. The Use Case Map (UCM) notation is part of the User Requirements Notation (URN), the most recent addition to ITU-T's family of languages. UCM models describe functional requirements and high-level designs with causal scenarios, superimposed on structures of components. It has been shown that UCMs can be transformed into more detailed MSC scenarios. However, UCMs are not executable as such. Early validation and exploration of requirements could benefit from a transformation to a formal, executable language. This paper presents the results of an experiment combining existing tool-supported techniques for the generation of MSCs from UCMs and for the synthesis of SDL from MSCs. In particular, this experiment provides useful results on the current interworking of such techniques and on requirements for future generations of tools. Through a simple case study, this paper also highlights questions and partial answers on the complementariness of these languages, on the usefulness of the resulting SDL models, and on potential improvements on the approach and on the languages themselves.\"",
        "Document: \"Evaluation of development tools for domain-specific modeling languages. Creating and maintaining tools for domain-specific modeling languages (DSML) demands time and efforts that often discourage potential developers. However, several tools are now available that promise to accelerate the development of DSML environments. In this paper, we evaluate five such tools (GME, Tau G2, RSA, XMF-Mosaic, and Eclipse with GEF and EMF) by observing how well they can be used to create graphical editors for the Goal-oriented Requirement Language (GRL), for which a simplified metamodel is provided. We discuss the evaluation criteria, results, and lessons learned during the creation of GRL editors with these technologies.\"",
        "Document: \"From event logs to goals: a systematic literature review of goal-oriented process mining. Process mining helps infer valuable insights about business processes using event logs, whereas goal modeling focuses on the representation and analysis of competing goals of stakeholders and systems. Although there are clear benefits in mining the goals of existing processes, goal-oriented approaches that consider logs during model construction are still rare. Process mining techniques, when generalizing large instance-level data into process models, can be considered as a data-driven complement to use case/scenario elicitation. Requirements engineers can exploit process mining techniques to find new system or process requirements in order to align current practices and desired ones. This paper provides a systemic literature review, based on 24 papers rigorously selected from four popular search engines in 2018, to assess the state of goal-oriented process mining. Through two research questions, the review highlights that the use of process mining in association with goals does not yet have a coherent line of research, whereas intention mining (where goal models are mined) shows a meaningful trace of research. Research about performance indicators measuring goals associated with process mining is also sparse. Although the number of publications in process mining and goal modeling is trending up, goal mining and goal-oriented process mining remain modest research areas. Yet, synergetic effects achievable by combining goals and process mining can potentially augment the precision, rationality and interpretability of mined models and eventually improve opportunities to satisfy system stakeholders.\"",
        "Document: \"Location-aware business process management for real-time monitoring of a cardiac care process. Long wait times are a global issue in the Canadian healthcare system. Patient flow management relies on flow managers to manually detect, investigate and mitigate wait time issues. However, existing data that could support this activity is usually not accurate (because of possible human errors), incomplete, late, and scattered across various information systems in a typical hospital. Yet, in the case of cardiac patients, ensuring a prompt, smooth and continuous care delivery becomes extremely important and motivates improvement of data support for patient flow management activities. This paper presents the development of a location-aware business process management system (LA-BPMS) for monitoring a cardiac care delivery process in a hospital and in real-time. The system provides a better visibility of process execution to patient flow managers who can rely on accurate and real-time information about patient process states, as well as wait time measurements to control patient flow efficiently. We show how an intelligent approach of combining location awareness and business process automation allow this to be possible. A real cardiac care process from an Ontario hospital is used as an example.\"",
        "1 is \"Business process modeling with continuous validation\", 2 is \"Considering Side Effects in Service Interactions in Home Automation - an Online Approach\"",
        "Given above information, for an author who has written the paper with the title \"Analysing the cognitive effectiveness of the UCM visual notation\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00605": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Optimal interconnect diagnosis of wiring networks':",
        "Document: \"A fast hierarchical algorithm for 3-D capacitance extraction. We presen t a new algorithm for computing the capacitance of three-dimensional perfect electrical conductors of complex structures. The new algorithm is significantly faster and uses muc h less memory than previous best algorithms, and is kernel independent.The new algorithm is based on a hierarchical algorithm for the n-body problem, and is an acceleration of the boundary-element method for solving the integral equation associated with the capacitance extraction problem. The algorithm first adaptively subdivides the conductor surfaces into panels according to an estimation of the potential coefficients and a user-supplied error band. The algorithm stores the poten tial coefficient matrix in a hierarchical data structure of size O(n), although the matrix is size n2 if expanded explicitly, wheren is the n umber of panels. The hierarchical data structure allows us to multiply the coefficient matrix with an y vector in O(n) time. Finally, w e use a generalized minimal residual algorithm to solve m linear systems each of size n \u00d7 n in O(mn) time, where m is the n umber of conductors.The new algorithm is implemented and the performance is compared with previous best algorithms. F or the k \u00d7 k bus example, our algorithm is 100 to 40 times faster than F astCap, and uses 1/100 to 1/60 of the memory used by F astCap. The results computed by the new algorithm are within 2.7% from that computed by FastCap.\"",
        "Document: \"Efficient Deterministic Algorithms for Embedding Graphs on Books.  . We derive deterministic polynomial time algorithms for bookembedding of a graph G = (V; E), jV j = n and jEj = m. In particular, wepresent the first deterministic polynomial time algorithm to embed anybipartite graph in O(pm) pages. We then use this algorithm to embed,in polynomial time, any graph G in O(pffi(G) \\Delta m) pages, where ffi(G)is the largest minimum degree over all subgraphs of G. Our algorithmsare obtained by derandomizing the probabilistic proofs.1... \"",
        "Document: \"A divide-and-conquer algorithm for 3-D capacitance extraction. We present a divide-and-conquer algorithm to improve the three-dimensional (3-D) boundary element method (BEM) for capacitance extraction. We divide large interconnect structures into small sections, set new boundary conditions using the border for each section, solve each section, and then combine the results to derive the capacitance. The target application is critical nets, clock trees, or packages where 3-D accuracy is required. Our algorithm is a significant improvement over the traditional BEMs and their enhancements, such as the \"window\" method, where conductors far away are dropped, and the \"shield\" method where conductors hidden behind other conductors are dropped. Experimental results show that our algorithm is a magnitude faster than the traditional BEM and the window+shield method, for medium to large structures. The error of the capacitance computed by the new algorithm is within 2% for self capacitance and 7% for coupling capacitance, compared with the results obtained by solving the entire system using BEM. Furthermore, our algorithms gives accurate distributed RC, where none of the previous 3-D BEM algorithms and their enhancements can.\"",
        "Document: \"A new twisted differential line structure in global bus design. Twisted differential line structure can effectively reduce crosstalk noise on global bus, which foresees a wide applicability. However, measured performance based on fabricated circuits is much worse than simulated performance based on the layout. It is suspected that the via resistance variation is the cause. In this paper, our extensive simulation confirm this. A new redundant via insertion technique is proposed to reduce via variation and signal distortion. In addition, a new buffer insertion technique is proposed to synchronize the transmitted signals, thus further improving the effectiveness of the twisted differential line. Experimental results demonstrate that the new approaches are highly effective. Under a realistic setup, a 6GHz signal can be transmitted with high fidelity using the new approaches. In contrast, only a 100MHz signal can be reliably transmitted using a single-end model with power/ground shielding. In addition, compared to conventional twisted differential line structure, our new techniques can reduce the magnitude of noise by 45%. Furthermore, compared to unbuffered twisted differential line structure, the maximum signal phase difference is reduced from 37ps to 7ps by the new buffer insertion technique.\"",
        "1 is \"An Algorithm for Convex Polytopes\", 2 is \"A Diagnosis Algorithm for Distributed Computing Systems with Dynamic Failure and Repair\"",
        "Given above information, for an author who has written the paper with the title \"Optimal interconnect diagnosis of wiring networks\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00625": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Multiple Context-Free Tree Grammars and Multi-component Tree Adjoining Grammars.':",
        "Document: \"The Substitution Vanishes. Accumulation techniques were invented to transform func- tional programs, which intensively use append functions (like inecien t list reversal), into more ecien t programs, which use accumulating pa- rameters instead (like ecien t list reversal). In this paper we present a generalized and automatic accumulation technique that also handles pro- grams operating with unary functions on arbitrary tree structures and employing substitution functions on trees which may replace dieren t designated symbols by dieren t trees. We show that this transformation does not deteriorate the eciency with respect to call-by-need reduction.\"",
        "Document: \"Unidirectional derivation semantics for synchronous tree-adjoining grammars. Synchronous tree-adjoining grammars have been given two types of semantics: one based on bimorphisms and one based on synchronous derivations, in both of which the input and output trees are constructed synchronously. We introduce a third type of semantics that is based on unidirectional derivations. It derives output trees based on a given input tree and thus marks a first step towards conditional probability distributions. We prove that the unidirectional semantics coincides with the bimorphism-based semantics with the help of a strong correspondence to linear and nondeleting extended top-down tree transducers with explicit substitution. In addition, we show that stateful synchronous tree-adjoining grammars admit a normal form in which only adjunction is used. This contrasts the situation encountered in the stateless case.\"",
        "Document: \"Minimizing weighted tree grammars using simulation. Weighted tree grammars (for short: WTG) are an extension of weighted context-free grammars that generate trees instead of strings. They can be used in natural language parsing to directly generate the parse tree of a sentence or to encode the set of all parse trees of a sentence. Two types of simulations for WTG over idempotent, commutative semirings are introduced. They generalize the existing notions of simulation and bisimulation for WTG. Both simulations can be used to reduce the size of WTG while preserving the semantics, and are thus an important tool in toolkits. Since the new notions are more general than the existing ones, they yield the best reduction rates achievable by all minimization procedures that rely on simulation or bisimulation. However, the existing notions might allow faster minimization.\"",
        "Document: \"Bisimulation Minimisation of Weighted Automata on Unranked Trees. Several models of automata are available that operate unranked trees. Two well-known examples are the stepwise unranked tree automaton (suta) and the parallel unranked tree automaton (puta). By adding a weight, taken from some semiring, to every transition we generalise these two qualitative automata models to quantitative models, thereby obtaining weighted stepwise unranked tree automata (wsuta) and weighted parallel unranked tree automata (wputa); the qualitative automata models are reobtained by choosing the BOOLEAN semiring. The weighted versions have applications in natural language processing, XML-based data management and quantitative information retrieval. We address the minimisation problem of wsuta and wputa by using (forward and backward) bisimulations and we prove the following results: (1) for every wsuta an equivalent forward (resp. backward) bisimulation minimal wsuta can be computed in time O(mn) where n is the number of states and m is the number of transitions of the given wsuta; (2) the same result is proved for wputa instead of wsuta; (3) if the semiring is additive cancellative or the BOOLEAN semiring, then the bound can be improved to O(mlog n) for both wsuta and wputa; (4) for every deterministic puta we can compute a minimal equivalent deterministic puta in time O(mlog n); (5) the automata models wsuta, wputa, and weighted unranked tree automaton have the same computational power.\"",
        "1 is \"A comparison of storage optimizatons in automatically-generated attribute evaluators\", 2 is \"The translation power of top-down tree-to-graph transducers\"",
        "Given above information, for an author who has written the paper with the title \"Multiple Context-Free Tree Grammars and Multi-component Tree Adjoining Grammars.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00847": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Spatial join selectivity using power laws':",
        "Document: \"CLAP, ACIR and SCOOP: Novel techniques for improving the performance of dynamic Metric Access Methods. \u2022Techniques for improving Metric Access Methods in similarity queries are proposed.\u2022CLAP technique reduces the number of distance calculations.\u2022ACIR and SCOOP techniques reduce the number of disk accesses.\u2022Gains of up to 63% in point queries are obtained.\u2022Gains of up to 53% in queries retrieving multiple elements are obtained.\"",
        "Document: \"A novel optimization approach to efficiently process aggregate similarity queries in metric access methods. A similarity query considers an element as the query center and searches a dataset to find either the elements far up to a bounding radius or the k nearest ones from the query center. Several algorithms have been developed to efficiently execute similarity queries. However, there are queries that require more than one center, which we call Aggregate Similarity Queries. Such queries appear when the user gives multiple desirable examples, and requests data elements that are similar to all of the examples, as in the case of applying relevance feedback. Here we give the first algorithms that can handle aggregate similarity queries on Metric Access Methods (MAM) such as the M-tree and Slim-tree. Our method, which we call Metric Aggregate Similarity Search (MASS) has the following properties: (a) it requires only the triangle inequality property; (b) it guarantees no false-dismissals, as we prove that it lower-bounds the aggregate distance scores; (c) it can work with any MAM; (d) it can handle any number of query centers, which are either scattered all over the space or concentrated on a restricted region. Experiments on both real and synthetic data show that our method scales on both the number of elements and, if the dataset is in a spatial domain, also on its dimensionality. Moreover, it achieves better results than previous related methods.\"",
        "Document: \"QMAS: Querying, Mining and Summarization of Multi-modal Databases. Given a large collection of images, very few of which have labels, how can we guess the labels of the remaining majority, and how can we spot those images that need brand new labels, different from the existing ones? Current automatic labeling techniques usually scale super linearly with the data size, and/or they fail when only a tiny amount of labeled data is provided. In this paper, we propose QMAS (Querying, Mining And Summarization of Multi-modal Databases), a fast solution to the following problems: (i) low-labor labeling (L3) \u2013 given a collection of images, very few of which are labeled with keywords, find the most suitable labels for the remaining ones, and (ii) mining and attention routing \u2013 in the same setting, find clusters, the top-NO outlier images, and the top-NR representative images. We report experiments on real satellite images, two large sets (1.5GB and 2.25GB) of proprietary images and a smaller set (17MB) of public images. We show that QMAS scales linearly with the data size, being up to 40 times faster than top competitors (GCap), obtaining better or equal accuracy. In contrast to other methods, QMAS does low-labor labeling (L3), that is, it works even with tiny initial label sets. It also solves both presented problems and spots tiles that potentially require new labels.\"",
        "Document: \"An effective cost model for similarity queries in metric spaces. This short paper presents an effective cost model to estimate the number of disk accesses (I/O cost) and the number of distance calculations (CPU cost) to process similarity range queries over data indexed by metric access methods.\"",
        "1 is \"Estimating attributes: analysis and extensions of RELIEF\", 2 is \"A Unified Approach for Indexed and Non-Indexed Spatial Joins\"",
        "Given above information, for an author who has written the paper with the title \"Spatial join selectivity using power laws\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00916": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'An International Trade Negotiation Framework for e-Government':",
        "Document: \"A multimodal adaptive dialogue manager for depressive and anxiety disorder screening: a Wizard-of-Oz experiment. In this paper, we present an Adaptive Multimodal Dialogue System for Depressive and Anxiety Disorders Screening (DADS). The system interacts with the user through verbal and non-verbal communication to elicit the information needed to make referrals and recommendations for depressive and anxiety disorders while encouraging the user and keeping them calm. We designed the problem using interconnected Markov Decision Processes using sub-goals to deal with the large state space. We present the problem formulation and the experimental procedure for the training data collection and the system training following the methodology of Wizard-of-Oz experiments.\"",
        "Document: \"Learning Rules for Large Vocabulary Word Sense Disambiguation. Word Sense Disambiguation (WSD) is the process of distinguishing between different senses of a word. In general, the disambiguation rules differ for different words. For this reason, the automatic construction of disambiguation rules is highly desirable. One way to achieve this aim is by applying machine learning techniques to training data containing the various senses of the ambiguous words. In the work presented here, the decision tree learning algorithm C4.5 is applied on a corpus of financial news articles. Instead of concentrating on a small set of ambiguous words, as done in most of the related previous work, all content words of the examined corpus are disambiguated. Furthermore, the effectiveness of word sense disambiguation for different parts of speech (nouns and verbs) is examined empirically.\"",
        "Document: \"Task-Driven Linguistic Analysis based on an Underspecified Features Representation. In this paper we explore a task-driven approach to interfacing NLP components, where language processing is guided by the end-task that each application requires. The core idea is to generalize feature values into feature value distributions, representing under-specified feature values, and to fit linguistic pipelines with a back-channel of specification requests through which subsequent components can declare to preceding ones the importance of narrowing the value distribution of particular features that are critical for the current task.\"",
        "Document: \"Investigating Metaphorical Language in Sentiment Analysis: A Sense-to-Sentiment Perspective. Intuition dictates that figurative language and especially metaphorical expressions should convey sentiment. It is the aim of this work to validate this intuition by showing that figurative language (metaphors) appearing in a sentence drive the polarity of that sentence. Towards this target, the current article proposes an approach for sentiment analysis of sentences where figurative language plays a dominant role. This approach applies Word Sense Disambiguation aiming to assign polarity to word senses rather than tokens. Sentence polarity is determined using the individual polarities for metaphorical senses as well as other contextual information. Experimental evaluation shows that the proposed method achieves high scores in comparison with other state-of-the-art approaches tested on the same corpora. Finally, experimental results provide supportive evidence that this method is also well suited for corpora consisting of literal and figurative language sentences.\"",
        "1 is \"Productivity, business profitability, and consumer surplus: three different measures of information technology value\", 2 is \"Semantic Matching: Algorithms and Implementation\"",
        "Given above information, for an author who has written the paper with the title \"An International Trade Negotiation Framework for e-Government\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00914": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A portal for access to complex distributed information about energy':",
        "Document: \"Querying Multiple Features of Groups in Relational Databases. Some aggregate and grouping queries are conceptually simple, but difficult to express in SQL. This difficulty causes both conceptual and implementation problems for the SQL-based database system. Complicated queries and views are hard to understand and maintain. Further, the code produced is sometimes unnecessarily inefficient, as we demonstrate experimentally using a commercial database system. In this paper, we examine a class of queries involving (potentially repeated) selection, grouping and aggregation over the same groups, and propose an extension of SQL syntax that allows the succinct representation of these queries. We propose a new relational algebra operation that represents several levels of aggregation over the same groups in an operand relation. We demonstrate that the extended relational operator can be evaluated using efficient algorithms. We describe. a translation from the extended SQL language into our algebraic language. We have implemented a preprocessor that evaluates our extended language on toe of a commercial database system. We demonstrate that on a variety of examples, our implementation improves performance over standard SQL representations of the same examples by orders of magnitude.\"",
        "Document: \"Inferring negative information from disjunctive databases. We propose criteria that any rule for inferring negative information from disjunctive databases should satisfy, and examine existing rules from this viewpoint. We then present a new inference rule, the \u2018disjunctive database rule\u2019 (DDR), and compare it to the existing rules with respect to the criteria. In particular, the DDR is equivalent to the CWA for definite databases, it infers no more negative information than the GCWA, and it interprets disjunction inclusively rather than exclusively. We generalize the DDR to a class of layered databases, describe an implementation of the DDR, \u2018negation as positive failure\u2019, and study its soundness and completeness properties.\"",
        "Document: \"Fast joins using join indices. Two new algorithms, \u201cJive join\u201d and \u201cSlam join,\u201d are proposed for computing the join of two relations using a join index. The algorithms are duals: Jive join range-partitions input relation tuple ids and then processes each partition, while Slam join forms ordered runs of input relation tuple ids and then merges the results. Both algorithms make a single sequential pass through each input relation, in addition to one pass through the join index and two passes through a temporary file, whose size is half that of the join index. Both algorithms require only that the number of blocks in main memory is of the order of the square root of the number of blocks in the smaller relation. By storing intermediate and final join results in a vertically partitioned fashion, our algorithms need to manipulate less data in memory at a given time than other algorithms. The algorithms are resistant to data skew and adaptive to memory fluctuations. Selection conditions can be incorporated into the algorithms. Using a detailed cost model, the algorithms are analyzed and compared with competing algorithms. For large input relations, our algorithms perform significantly better than Valduriez's algorithm, the TID join algorithm, and hash join algorithms. An experimental study is also conducted to validate the analytical results and to demonstrate the performance characteristics of each algorithm in practice.\"",
        "Document: \"Scalable aggregation on multicore processors. In data-intensive and multi-threaded programming, the performance bottleneck has shifted from I/O bandwidth to main memory bandwidth. The availability, size, and other properties of on-chip cache strongly influence performance. A key question is whether to allow different threads to work independently, or whether to coordinate the shared workload among the threads. The independent approach avoids synchronization overhead, but requires resources proportional to the number of threads and thus is not scalable. On the other hand, the shared method suffers from coordination overhead and potential contention. In this paper, we aim to provide a solution to performing in-memory parallel aggregation on the Intel Nehalem architecture. We consider several previously proposed techniques that were evaluated on other architectures, including a hybrid independent/shared method and a method that clones data items automatically when contention is detected. We also propose two algorithms: partition-and-aggregate and PLAT. The PLAT and hybrid methods perform best overall, utilizing the computational power of multiple threads without needing memory proportional to the number of threads, and avoiding much of the coordination overhead and contention apparent in the shared table method.\"",
        "1 is \"Rule ordering in bottom-up fixpoint evaluation of logic programs\", 2 is \"STBenchmark: towards a benchmark for mapping systems\"",
        "Given above information, for an author who has written the paper with the title \"A portal for access to complex distributed information about energy\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00961": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'On the relationship between fuzzy autoepistemic logic and fuzzy modal logics of belief':",
        "Document: \"Towards An Automated Deduction System For First-Order Possibilistic Logic Programming With Fuzzy Constants. In this article, we present a first-order logic programming language for fuzzy reasoning under possibilistic uncertainty and poorly known information. Formulas are represented by a pair (phi, alpha), in which p is a first-order Horn clause or a query with fuzzy constants and regular predicates, and alpha is an element of [0, 1] is a lower bound on the belief on phi in terms of necessity measures. Since fuzzy constants can occur in the logic component of formulas, the truth value of formulas is many-valued instead of Boolean. Moreover, since we have to reason about the possibilistic uncertainty of formulas with fuzzy constants, belief states are modeled by normalized possibility distributions on a set of many-valued interpretations. In this framework, (1) we define a syntax and a semantics of the underlying logic; (2) we give a sound modus ponens-style calculus by derivation based on a semantic unification pattern of fuzzy constants; (3) we develop a directional fuzzy unification algorithm based on the distinction between general and specific object constants; and (4) we describe a backward first-order proof procedure oriented to queries that is based on the calculus of the language and the computation of the unification degree between fuzzy constants in terms of a necessity measure for fuzzy events. (C) 2002 Wiley Periodicals, Inc.\"",
        "Document: \"Classifying qualitative information using centroid based methods. In this paper we consider the problem of classification of qualitative data. We present an approach to overcome several difficulties which show SAHN (sequential, agglomerative, hierarchic, nonoverlapping) clustering methods to classify qualitative data when using centroids to compute similarities between pairs of classes. The approach is based on a recently proposed class of qualitative weighted average functions.\"",
        "Document: \"A language for the execution of graded BDI agents. We are interested in the specification and deployment of multi-agent systems, and particularly we focus on the execution of agents. Along this research line, we have proposed a general model for graded BDI agents, specifying an architecture based on multi-context systems (MCSs) and able to deal with the environment uncertainty (via graded beliefs) and with graded mental proactive attitudes (via de...\"",
        "Document: \"On the Implementation of a Multiple Output Algorithm for Defeasible Argumentation. In a previous work we defined a recursive warrant semantics for Defeasible Logic Programming based on a general notion of collective conflict among arguments. The main feature of this recursive semantics is that an output of a program is a pair consisting of a set of warranted and a set of blocked formulas. A program may have multiple outputs in case of circular definitions of conflicts among arguments. In this paper we design an algorithm for computing each output and we provide an experimental evaluation of the algorithm based on two SAT encodings defined for the two main combinatorial subproblems that arise when computing warranted and blocked conclusions for each output.\"",
        "1 is \"Efficient Algorithms for Fuzzy Qualitative Temporal Reasoning\", 2 is \"A simple logic for reasoning about incomplete knowledge.\"",
        "Given above information, for an author who has written the paper with the title \"On the relationship between fuzzy autoepistemic logic and fuzzy modal logics of belief\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00973": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Revisiting the PnP Problem: A Fast, General and Optimal Solution':",
        "Document: \"Precise Simultaneous Estimation of Image Deformation Parameters. This paper presents a new method to obtain simultaneously precise N parameters of image deformation with non-iterative calculation by extending area-based matching and sub-pixel estimation. Although area-based matching and similarity interpolation for sub-pixel displacement estimation are commonly used in many areas as a fundamental procedure, they are bound to simple translation. The proposed method is based on a practical similarity model in N-dimensional parameter space. Using similarity measures obtained at discrete positions in the parameter space, our method provides a highly accurate maximum position of similarity in sub-sampling resolution; that position corresponds to image deformation parameters. Experimental results using both synthetic and real images demonstrate that our method can estimate parameters more accurately than previous methods.\"",
        "Document: \"Gradient-Domain Image Reconstruction Framework With Intensity-Range And Base-Structure Constraints. This paper presents a novel unified gradient-domain image reconstruction framework with intensity-range constraint and base-structure constraint. The existing method for manipulating base structures and detailed textures are classifiable into two major approaches: i) gradient-domain and ii) layer-decomposition. To generate detail-preserving and artifact-free output images, we combine the benefits of the two approaches into the proposed framework by introducing the intensity-range constraint and the base-structure constraint. To preserve details of the input image, the proposed method takes advantage of reconstructing the output image in the gradient domain, while the output intensity is guaranteed to lie within the specified intensity range, e.g. 0-to-255, by the intensity-range constraint. In addition, the reconstructed image lies close to the base structure by the base-structure constraint, which is effective for restraining artifacts. Experimental results show that the proposed framework is effective for various applications such as tone mapping, seamless image cloning, detail enhancement, and image restoration.\"",
        "Document: \"Single-image noise level estimation for blind denoising. Noise level is an important parameter to many image processing applications. For example, the performance of an image denoising algorithm can be much degraded due to the poor noise level estimation. Most existing denoising algorithms simply assume the noise level is known that largely prevents them from practical use. Moreover, even with the given true noise level, these denoising algorithms still cannot achieve the best performance, especially for scenes with rich texture. In this paper, we propose a patch-based noise level estimation algorithm and suggest that the noise level parameter should be tuned according to the scene complexity. Our approach includes the process of selecting low-rank patches without high frequency components from a single noisy image. The selection is based on the gradients of the patches and their statistics. Then, the noise level is estimated from the selected patches using principal component analysis. Because the true noise level does not always provide the best performance for nonblind denoising algorithms, we further tune the noise level parameter for nonblind denoising. Experiments demonstrate that both the accuracy and stability are superior to the state of the art noise level estimation algorithm for various scenes and noise levels.\"",
        "Document: \"Panoramic 3d Reconstruction Using Stereo Multi-Perspective Panorama. In this paper, we present a novel approach to imaging a panoramic (360 degrees) environment and computing its dense depth map. Our approach adopts a multi-baseline stereo strategy using a set of multi-perspective panoramas where large baseline lengths are available. We design two image acquisition rigs for capturing such multi-perspective panoramas. The first one is composed of two parallel stereo cameras. By rotating the rig about a vertical axis, we generate four multi-perspective panoramas by resampling the regular perspective images captured by the stereo cameras. Then a depth map is estimated from the four multi-perspective panoramas and an original perspective image using a multi-baseline matching technique with different types of epipolar constraints. The second one is composed of a single camera and two mirrors. By rotating the rig, we acquire a spatio-temporal volume that is made up of the sequential images captured by the camera. Then we estimate a depth map by extracting trajectories from the spatio-temporal volume by using a multi-baseline stereo technique by considering occlusions. We can consider both rotating rigs as a single rotating camera with a very large field of view (FOV), that offers a large baseline length in depth estimation. In addition, compared with a previous approach using two multi-perspective panoramas from a single rotating camera, our approach can reduce matching errors due to image noise, repeated patterns, and occlusions by multi-baseline stereo techniques. Experimental results using both synthetic and real images show that our approach produces high quality panoramic 3D reconstruction.\"",
        "1 is \"Structure from motion with wide circular field of view cameras\", 2 is \"Robust recovery of the epipolar geometry for an uncalibrated stereo rig\"",
        "Given above information, for an author who has written the paper with the title \"Revisiting the PnP Problem: A Fast, General and Optimal Solution\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00982": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Bounding the inefficiency of outcomes in generalized second price auctions.':",
        "Document: \"Sliding-Window Compression on the Hypercube (Research Note). Dictionary compression belongs to the class of lossless compression methods and is mainly used for compressing text files [1, 2, 3]. In this paper, we present a parallel algorithm for one of these coding methods, namely the LZ77 coding algorithm also known as a sliding-window coding algorithm. Although there exist PRAM algorithms [4, 5] for various dictionary compression methods, their rather irregular structure has discouraged their implementation on practical interconnection networks such as the mesh and hypercube. However in the case of LZ77 coding, we show how to exploit the specific properties of the algorithm in order to achieve an efficient implementation on the hypercube.\"",
        "Document: \"Fractional Path Coloring with Applications to WDM Networks. This paper addresses the natural relaxation of the path coloring problem, in which one needs to color directed paths on a symmetric directed graph with a minimum number of colors, in such a way that paths using the same arc of the graph have different colors. This classic combinatorial problem finds applications in the minimization of the number of wavelengths in wavelength division multiplexing (WDM) all-optical networks.\"",
        "Document: \"Sparse and limited wavelength conversion in all-optical tree networks. We study the problem of assigning a minimum number of colors to directed paths (dipaths) if a tree, so that any two dipaths that share a directed edge of the tree are not assigned the same color. The problem has applications to wavelength routing in WDM all-optical tree networks, an important engineering problem. Dipaths represent communication requests, while colors correspond to wavelengths that must be assigned to requests so that multiple users can communicate simultaneously through the same optical fiber. Recent work on wavelength routing in trees has studied a special class of algorithms which are called greedy. Although these algorithms are simple and implementable in a distributed setting, it has been proved that there are cases where a bandwidth utilization of 100% is not possible. Thus, in this work, we relax the constraints of the original engineering problem and use devices called wavelength converters that are able to convert the wavelength assigned to a segment of a communication request to another wavelength that will be assigned to some other segment of the same request. The trade-off of the use of wavelength converters is increased cost and complexity; so, our aim is to use converters that have relatively simple functionality. We study the performance of greedy deterministic algorithms in tree-shaped all-optical networks that support wavelength conversion. We study both the case of sparse conversion and limited conversion. By sparse we mean that converters have full conversion capabilities and the objective is to minimize the number of converters employed. On the other hand, in limited conversion, we assume that converters with limited conversion capabilities are placed at each non-leaf node of the tree. By limited, we mean that converters are simple according to either their wavelength degree or their size. Our results show that using converters of either low degree or small size, we can beat the known lower bounds and improve bandwidth utilization. In some cases we even achieve optimal bandwidth utilization. For the construction of the converters, we use special classes of graphs such as expanders, dispersers and depth-two superconcentrators. Explicit constructions are known for most of the graphs used in this paper. Copyright 2001 Elsevier Science B.V.\"",
        "Document: \"Fractional Path Coloring in Bounded Degree Trees with\u00a0Applications. This paper studies the natural linear programming relaxation of the path coloring problem. We prove constructively that finding an optimal fractional path coloring is Fixed Parameter Tractable (FPT), with the degree of the tree as parameter: the fractional coloring of paths in a bounded degree trees can be done in a time which is linear in the size of the tree, quadratic in the load of the set of paths, while exponential in the degree of the tree. We give an algorithm based on the generation of an efficient polynomial size linear program. Our algorithm is able to explore in polynomial time the exponential number of different fractional colorings, thanks to the notion of  of a coloring that we introduce. We further give an upper bound on the cost of such a coloring in binary trees and extend this algorithm to bounded degree graphs with bounded treewidth. Finally, we also show some relationships between the integral and fractional problems, and derive a 1+5/3\u22481.61\u2014approximation algorithm for the path coloring problem in bounded degree trees, improving on existing results. This classic combinatorial problem finds applications in the minimization of the number of wavelengths in wavelength division multiplexing () optical networks.\"",
        "1 is \"Welfare maximization and truthfulness in mechanism design with ordinal preferences\", 2 is \"A new motion compensation method for image sequence coding using hierarchical grid interpolation\"",
        "Given above information, for an author who has written the paper with the title \"Bounding the inefficiency of outcomes in generalized second price auctions.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00983": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Temporal accommodation of legal argumentation':",
        "Document: \"An OWL Ontology of Fundamental Legal Concepts. In this paper we present an OWL ontology of fundamental legal concepts developed within the ESTRELLA European project. The ontology includes the basic normative components of legal knowledge: deontic modalities, obligative rights, permissive rights, liberty rights, liability rights, different kinds of legal powers, potestative rights (rights to produce legal results) and sources of law. Besides the taxonomy the ontology comprises also the semantic relations between the concepts. We hope that it may be useful for semantic access to digital legal information and for the representation of legal knowledge.\"",
        "Document: \"Time and Defeasibility in FIPA ACL Semantics. Inferences about speech acts are often conditional, non- monotonic, and involve the issue of time. Most agent com- munication languages, however, ignore these issues, due to the difficulty to combine them in a single formalism. This paper addresses such issues in defeasible logic, and shows how to express a semantics for ACLs in order to make non- monotonic inferences on the basis of speech acts.\"",
        "Document: \"Rule-based agents, compliance, and intention reconsideration in defeasible logic. This paper shows how belief revision techniques can be used in Defeasible Logic to change rule-based theories characterizing the deliberation process of cognitive agents. We discuss intention reconsideration as a strategy to make agents compliant with the norms regulating their behavior.\"",
        "Document: \"Normative autonomy and normative co-ordination: Declarative power, representation, and mandate. In this paper we provide a formal analysis of the idea of normative co-ordination. We argue that this idea is based on the assumption that agents can achieve flexible co-ordination by conferring normative positions to other agents. These positions include duties, permissions, and powers. In particular, we explain the idea of declarative power, which consists in the capacity of the power-holder of creating normative positions, involving other agents, simply by \u00bfproclaiming\\\" such positions. In addition, we account also for the concepts of representation, namely the representative's capacity of acting in the name of his principal, and of mandate, which is the mandatee's duty to act as the mandator has requested. Finally, we show how the framework can be applied to represent the contract-net protocol. Some brief remarks on future research and applications conclude this contribution.\"",
        "1 is \"A framework for monitoring agent-based normative systems\", 2 is \"Designing compliant business processes with obligations and permissions\"",
        "Given above information, for an author who has written the paper with the title \"Temporal accommodation of legal argumentation\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001111": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'DRAIN: distributed recovery architecture for inaccessible nodes in multi-core chips':",
        "Document: \"Depth-driven verification of simultaneous interfaces. The verification of modern computing systems has grown to dominate the cost of system design, often with limited success as designs continue to be released with latent bugs. This trend is accelerated with the advent of highly integrated system-on-a-chip (SoC) designs, which feature multiple complex subcomponents connected by simultaneously active interfaces.In this paper, we introduce a closed-loop feedback technique targeting the verification of multiple components connected by parallel interfaces. We utilize an environment with hierarchical Markov models, where top-level submodels specify overarching simulation goals of the system, while lower-level submodels specify the detailed component-level input generation. Test accuracy is improved through the use of depth-driven random test generation. The approach allows users to specify correctness properties and key activity nodes in the design to be exercises. We examine three non-trivial designs, two microprocessors and a chip multiprocessor router switch, and we demonstrate that our technique finds many more bugs than constrained-random test generation technique and reduces the simulation effort in half, compared to previous Markov-model based solutions.\"",
        "Document: \"Viper: virtual pipelines for enhanced reliability. The reliability of future processors is threatened by decreasing transistor robustness. Current architectures focus on delivering high performance at low cost; lifetime device reliability is a secondary concern. As the rate of permanent hardware faults increases, robustness will become a first class constraint for even low-cost systems. Current research into reliable architectures has focused on ad-hoc solutions to improve designs without altering their centralized control logic. Unfortunately, this centralized control presents a single point of failure, which limits long-term robustness.\n\nTo address this issue, we introduce Viper, an architecture built from a redundant collection of fine-grained hardware components. Instructions are perceived as customers that require a sequence of services in order to properly execute. The hardware components vie to perform what services they can, dynamically forming virtual pipelines that avoid defective hardware. This is done using distributed control logic, which avoids a single point of failure by construction.\n\nViper can tolerate a high number of permanent faults due to its inherent redundancy. As fault counts increase, its performance degrades more gracefully than traditional centralized-logic architectures. We estimate that fault rates higher than one permanent faults per 12 million transistors, on average, cause the throughput of a classic CMP design to fall below that of a Viper design of similar size.\n\n\"",
        "Document: \"Hybrid checking for microarchitectural validation of microprocessor designs on acceleration platforms. Software-based simulation provides a convenient environment for microprocessor design validation, where a number of complex software checkers are integrated with the simulated design to identify discrepancies between design and specification. Unfortunately, the performance of software-based simulation is vastly inadequate to achieve sufficient coverage for large microprocessor designs with complex microarchitectures. Hence, acceleration and emulation platforms are heavily deployed in the industry for high-performance validation. However, software checkers cannot be directly incorporated into such platforms, forcing designers to craft ad-hoc solutions. Adapting checking solutions for software simulation to acceleration platforms presents the following constraints: i) only a limited number of signals can be monitored per cycle for checking purposes so as to retain acceptable simulation performance, and ii) the overhead of the added checking logic must be minimal.\n\nIn this work, we explore a novel solution to adapt software-based checkers for individual microarchitectural blocks to acceleration platforms, by leveraging a hybrid approach. Our solution exploits embedded logic and data tracing for post-simulation checking in a synergistic fashion to limit the associated overhead. Embedded logic can be used for synthesized local checkers as well as to compress the traced data and thus limit recording overhead. We analyze several trade-offs associated with checking accuracy and logic/recording overhead for different microarchitectural blocks of an out-of-order superscalar processor design. We strive to provide valuable insights on how to adapt such software checkers to the acceleration environment using our hybrid approach. We find that, by leveraging simple embedded checkers and data compressors (15-25% logic overhead), we can achieve excellent checking accuracy even when aggressively compressing the data for transfer (only 15-25 bits/cycle), and localize bugs up to 5,900 cycles sooner than an architectural-level checker.\n\n\"",
        "Document: \"Reap what you sow: spare cells for post-silicon metal fix. Post-silicon validation has recently become a major bottleneck in IC design. Several high profile IC designs have been taped-out with latent bugs, and forced the manufacturers to resort to additional design revisions. Such changes can be applied through metal fix; however, this is impractical without carefully pre-placed spare cells. In this work we perform the first comprehensive analysis of the issues related to spare-cell insertion, including the types of spare cells that should be used as well as their placement. In addition, we propose a new technique to measure the heterogeneity among signals and use it to determine spare-cell density. Finally, we integrate our findings into a novel multi-faceted approach that calculates regional demand for spare cells, identifies the most appropriate cell types, and places such cells into the layout. Our approach enables the use of metal fix at a much smaller delay cost, with a reduction of up to 37% compared to previous solutions\"",
        "1 is \"Fast and accurate rectilinear steiner minimal tree algorithm for VLSI design\", 2 is \"BeepBeep: a high accuracy acoustic ranging system using COTS mobile devices\"",
        "Given above information, for an author who has written the paper with the title \"DRAIN: distributed recovery architecture for inaccessible nodes in multi-core chips\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001146": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'HUGO: Hierarchical mUlti-reference Genome cOmpression for aligned reads.':",
        "Document: \"Adaptive Submodular Dictionary Selection for Sparse Representation Modeling with Application to Image Super-Resolution. This paper proposes an adaptive dictionary learning approach based on sub modular optimization. A candidate atom set is constructed based on multiple bases from the combination of analytic and trained dictionaries. With the low-frequency components by the analytic DCT atoms, high-resolution dictionaries can be inferred through online learning to make efficient approximation with rapid convergence. It is formulated as a combinatorial optimization for approximate sub modularity, which is suitable for sparse representation based on dictionaries with arbitrary structures. In single-image super-resolution, the proposed scheme has been demonstrated to improve the reconstruction performance in comparison with double sparsity dictionary in terms of both objective and subjective restoration quality.\"",
        "Document: \"Joint Coding/Routing Optimization for Distributed Video Sources in Wireless Visual Sensor Networks. This paper studies a joint coding/routing optimization between network lifetime and video distortion by applying information theory to wireless visual sensor networks for correlated sources. Arbitrary coding [distributed video coding and network coding (NC)] from both combinatorial optimization and information theory could make significant progress toward the performance limit and tractable. Also, multipath routing can spread energy utilization across nodes within the entire network to keep a potentially longer lifetime, and solve the wireless contention issues by the splitting traffic. The objective function not only keeps the total energy consumption of encoding power, transmission power, and reception power minimized, but ensures the information received by sink nodes to approximately reconstruct the visual field. Also, a generalized power consumption model for distributed video sources is developed, in which the coding complexity of Key frames and Wyner-Ziv frames is measured by translating specific coding behavior into energy consumption. On the basis of the distributed multiview video coding and NC-based multipath routing, the balance problem between lifetime (costs) and distortion (capacity) is modeled as an optimization formulation with a fully distributed solution. Through a primal decomposition, a two-level optimization is relaxed with Lagrangian dualization and solved by the gradient algorithm. The low-level optimization problem is further decomposed into a secondary master dual problem with four cross-layer subproblems: a rate control problem, a channel contention problem, a distortion control problem, and an energy conservation problem. The implementation of the distributed algorithm is discussed with regard to the communication overhead and dynamic network change. Simulation results validate the convergence and performance of the proposed algorithm.\"",
        "Document: \"Picking Neural Activations for Fine-Grained Recognition. It is a challenging task to recognize fine-grained subcategories due to the highly localized and subtle differences among them. Different from most previous methods that rely on object/part annotations, this paper proposes an automatic fine-grained recognition approach, which is free of any object/part annotation at both training and testing stages. The key idea includes two steps of picking neura...\"",
        "Document: \"Design and implementation of multiplexing rate control in broadband access network TV transmission system. This paper addresses the implementation architecture and protocol procedure of the dedicated broadband access network TV transmission system. According to application-oriented quality of service (QoS), two alternative rate control schemes for multiple DTV programs transmission are proposed collaborating with MPEG-2 system strategy and IP-base network multiplexing mechanism. As to the network bandwidth utilization, a flexible RTF-based synchronization resilience scheme for VBR transport stream (TS) transmission is realized to ensure the performance requirement for MPEG-2 standard. As to the MPEG-2 system strategy, moreover, a parallel VBR transcoding/encoding rate control algorithm for CBR TS transmission could achieve a consistent picture quality for multiple DTV programs under a constrained fixed-rate network bandwidth. For adapting the variable access network bandwidth, the system main control module can flexibly schedule the available rate control scheme for multiple slave service multiplexers. Experimental results demonstrate that the proposed schemes not only improve the traditional TV broadcast adaptation to Internet, but also achieve a more ideal program picture quality constrained by multi-program parallel multiplexing.\"",
        "1 is \"Developing a test collection for biomedical word sense disambiguation.\", 2 is \"A Neural Autoregressive Topic Model.\"",
        "Given above information, for an author who has written the paper with the title \"HUGO: Hierarchical mUlti-reference Genome cOmpression for aligned reads.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001231": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Total Variability Modeling Using Source-Specific Priors.':",
        "Document: \"Output SNR analysis of integrated active noise control and noise reduction in hearing aids under a single speech source scenario. This paper analyses the output signal-to-noise ratio for a standard noise reduction scheme based on the multichannel Wiener filter and for an integrated active noise control and noise reduction scheme based on the filtered-X multichannel Wiener filter, both applied in a hearing aid framework that includes the effects of signal leakage through an open fitting and secondary path effects. In previous work, integrating noise reduction and active noise control has been shown to allow to compensate for effects of signal leakage and secondary path effects. These experimental results are now verified theoretically. The output signal-to-noise ratios are derived under a single speech source scenario. Theoretical results are then compared to simulations for a single noise source scenario and a multiple noise sources scenario.\"",
        "Document: \"Amplitude Modulated Sinusoidal Models for Audio Modeling and Coding. In this paper a new perspective on modeling of transient phenomena in the context of sinusoidal audio modeling and coding is presented. In our approach the task of finding time-varying amplitudes for sinusoidal models is viewed as an AM demodulation problem. A general perfect reconstruction framework for amplitude modulated sinusoids is introduced and model reductions lead to a model for audio compression. Demodulation methods are considered for estimation of the time-varying amplitudes, and inherent constraints and limitations axe discussed. Finally, some applications axe considered and discussed and the concepts are demonstrated to improve sinusoidal modeling of audio and speech.\"",
        "Document: \"Filter Model of Reduced-Rank Noise Reduction. The key step in reduced-rank noise reduction algorithms is to approximate a matrix by another one with lower rank, typically by truncating a singular value decomposition (SVD). We give an explicit and closed-form derivation of the filter properties of the rank reduction operation and interpret this operation in the frequency domain by showing that the reduced-rank output signal is identical to that from a filter-bank whose analysis and synthesis filters are determined by the SVD. Our analysis includes the important general case in which pre- and dewhitening is used.\"",
        "Document: \"On Perceptual Audio Compression with Side Information at the Decoder. Due to the distributed structure of many modern audio transmission setups, it is likely to have an observation at the receiver which is correlated with the desired source at the transmitter. This observation could be used as side information to reduce the transmission rate using distributed source coding. How to integrate distributed source coding into the perceptual audio compression procedure is thus a fundamental question. In this paper, we take a completely analytical approach to this problem, in particular to the rate-distortion trade-off and the corresponding coding schemes. We then interpret the results from an audio coding perspective. The main result is that, to upgrade a regular perceptual audio coder to a distributed coder, one needs to revise the perceptual masking curve. The revised masking curve models the availability of the side information as an extra masking effect, yielding lower rates. Interestingly, this means that at least conceptually, the distributed coding scenario could be integrated into the audio coder with minor changes, and without destructing the original coder.\"",
        "1 is \"Sparse linear parametric modeling of room acoustics with Orthonormal Basis Functions\", 2 is \"Disentangling speaker and channel effects in speaker verification\"",
        "Given above information, for an author who has written the paper with the title \"Total Variability Modeling Using Source-Specific Priors.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001238": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Critical issues in interaction design':",
        "Document: \"\"Watts in it for me?\": design implications for implementing effective energy interventions in organisations. The design of technological interventions to motivate behaviour-based reductions in end-user energy consumption has recently been identified as a priority for the HCI community. Previous interventions have produced promising results, but have typically focused on domestic energy consumption. By contrast, this paper focuses on the workplace context, which presents very different opportunities and challenges. For instance, financial consequences, which have proved successful as motivations in the domestic environment, are not present in the workplace in the context of employees. We describe the outcome of a sequence of workshops that focussed on understanding employee perceptions of energy use in the workplace, with the locus of activity on energy intervention design. Using a grounded theory analysis, we produced a framework of key themes detailing user perceptions and energy intervention design considerations. Our findings provide a framework of considerations for the design of successful workplace energy interventions.\"",
        "Document: \"HCI in the press: online public reactions to mass media portrayals of HCI research. HCI researchers working in publically funded institutions are increasingly encouraged to engage the public in their research. Mass media is often seen as an effective medium with which to communicate research to large parts of the population. We present an account of three HCI projects that have used engagements with mass media in order to communicate research to the public. We describe the motivations for working with mass media and the mechanics of writing press releases. A grounded theory analysis of online public responses to the projects in the mass media leads us to identify a number of concerns about how research is portrayed by news outlets and thus interpreted by the public. Tensions about technologies and wider societal issues were revealed that might normally be hidden when using traditional user-centred methods. We critically reflect on the efficacy of using the mass media in research and provide guidance for HCI researchers wishing to engage in dialogues with the public in the future.\"",
        "Document: \"The prayer companion: openness and specificity, materiality and spirituality. In this paper we describe the Prayer Companion, a device we developed as a resource for the spiritual activity of a group of cloistered nuns. The device displays a stream of information sourced from RSS news feeds and social networking sites to suggest possible topics for prayers. The nuns have engaged with the device enthusiastically over the first ten months of an ongoing deployment, and, notwithstanding some initial irritation with the balance of content, report that it plays a significant and continuing role in their prayer life. We discuss how we balanced specificity in the design with a degree of openness for interpretation to create a resource that the nuns could both understand and appropriate, describe the importance of materiality to the device's successful adoption, consider its implications as a design for older people, and reflect on the example it provides of how computation may serve spirituality.\"",
        "Document: \"Making Problems in Design Research: The Case of Teen Shoplifters on Tumblr. ABSTRACTHCI draws on a variety of traditions but recently there have been calls to consolidate contributions around the problems researchers set out to solve. However, with this comes the assumption that problems are tractable and certain, rather than constructed and framed by researchers. We take as a case study a Tumblr community of teen shoplifters who post on how to steal from stores, discuss shoplifting as political resistance, and share jokes and stories about the practice. We construct three different \"problems\" and imagine studies that might result from applying different design approaches: Design Against Crime; Critical Design and Value Sensitive Design. Through these studies we highlight how interpretations of the same data can lead to radically different design responses. We conclude by discussing problem making as a historically and politically contingent process that allow researchers to connect data and design according to certain moral and ethical principles.\"",
        "1 is \"Spreadsheet visualisation to improve end-user understanding\", 2 is \"A long-term strategy for designing (in) the wild: lessons from the urban mediator and traffic planning in Helsinki\"",
        "Given above information, for an author who has written the paper with the title \"Critical issues in interaction design\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001275": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Cost-effective design of scalable high-performance systems using active and passive interposers.':",
        "Document: \"Design and optimization of the store vectors memory dependence predictor. Allowing loads that do not violate memory ordering to issue out of order with respect to earlier unresolved store addresses is very important for extracting parallelism in large-window superscalar processors. Previous research has proposed memory dependence prediction algorithms to prevent only loads with true memory dependencies from issuing in the presence of unresolved stores. Techniques such as load-store pair identification and store sets have been very successful in achieving performance levels close to that attained by an oracle-dependence predictor, but have relatively complex or power-hungry designs. In this article, we use the idea of dependency vectors from matrix schedulers for nonmemory instructions and adapt them to implement a new dependence prediction algorithm. We show that for conservatively sized processors, a simple PC-indexed table that tracks misordered loads is sufficient to provide most of the performance benefits achieved by more sophisticated predictors. On more aggressive processor configurations, however, our \u201cStore Vector\u201d algorithm provides better performance than the state-of-the-art store sets predictor while maintaining a simpler and more scalable design.\"",
        "Document: \"Implementing Caches in a 3D Technology for High Performance Processors. 3D integration is an emergent technology that has the potential to greatly increase device density while simultaneously providing faster on-chip communication. 3D fabrication involves stacking two or more die connected with a very high-density and low-latency interface. The die-to-die vias that comprise this interface can be treated like regular on-chip metal due to their small size (on the order of l\u03bcm) and high speed (sub-F04 die-to-die communication delay). The increased device density and the ability to place and route in the third dimension provide new opportunities for microarchitecture design. In this paper, we first present a brief overview of 3D integration technology. We then focus on the design of on-chip caches using 3D integration. In particular, we show that the dense die-to-die vias enable caches that are 3D-partitioned at the level of individual wordlines or bitlines. This results in a wire length reduction within SRAM arrays, and a reduction in the footprint of individual SRAM banks, which reduces the global routing from the edge of the cache to the banks and back. The wire length reduction provides both power and performance benefits, e.g., 21.5% latency reduction and 30.9% energy reduction for a 512KB cache. We also report that implementing only the caches in 3D, without accounting for possible benefits from implementing other components of the processor in 3D, results in a 12% IPC gain. These results demonstrate some of the potential of this new technology, and motivate further research in 3D microarchitectures.\"",
        "Document: \"Challenges of High-Capacity DRAM Stacks and Potential Directions. With rapid growth in data volumes and an increase in number of CPU/GPU cores per chip, the capacity and bandwidth of main memory can be scaled up to accommodate performance requirements of data-intensive applications. Recent 3D-stacked in-package memory devices such as high-bandwidth memory (HBM) and similar technologies can provide high amounts of memory bandwidth at low access energy. However, 3D-stacked in-package memory have limited memory capacity. In this paper, we study and present challenges of scaling the capacity of 3D-stacked memory devices by stacking more DRAM dies within a device and building taller memory stacks. We also present potential directions and mitigations to building tall HBM stacks of DRAM dies. Although taller stacks are a potentially interesting approach to increase HBM capacity, we show that more research is needed to enable high-capacity memory stacks while simultaneously scaling up their memory bandwidth. Specifically, alternative bonding and stacking technologies can be investigated as a potentially major enabler of tall HBM stacks.\n\n\"",
        "Document: \"Thermal analysis of a 3D die-stacked high-performance microprocessor. 3-dimensional integrated circuit (3D IC) technology places circuit blocks in the vertical dimension in addition to the conventional horizontal plane. Compared to conventional planar ICs, 3D ICs have shorter latencies as well as lower power consumption, due to shorter wires. The benefits of 3D ICs increase as we stack more die, due to successive reductions in wire lengths. However, as we stack more die, the power density increases due to increasing proximity of active (heat generating) devices, thus causing the temperatures to increase. Also, the topmost die on the 3D stack are located further from the heat sink and experience a longer heat dissipation path. Prior research has already identified thermal management as a critical issue in 3D technology. In this paper, we evaluate the thermal impact of building high-performance microprocessors in 3D. We estimate the temperatures of a planar IC based on the Alpha 21364 processor as well as 2-die and 4-die 3D implementations of the same. We show that, compared to the planar IC, the 2-die implementation and 4-die implementation increase the maximum temperature by 17 Kelvin and 33 Kelvin, respectively.\"",
        "1 is \"Leakage Current: Moore's Law Meets Static Power\", 2 is \"Time Complexity of genetic algorithms on exponentially scaled problems\"",
        "Given above information, for an author who has written the paper with the title \"Cost-effective design of scalable high-performance systems using active and passive interposers.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001326": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Sphere: multi-touch interactions on a spherical display':",
        "Document: \"The Design of Organic User Interfaces: Shape, Sketching and Hypercontext. With the emergence of flexible display technologies, it will be necessary for interface designers to move beyond flat interfaces and to contextualize interaction in an object&#39;s physical shape. Grounded in early explorations of organic user interfaces (OUIs), this paper examines the evolving relationship between industrial and interaction designs and examines how not only what we design is changing...\"",
        "Document: \"Demonstration of Haptic Revolver: Touch, Shear, Texture, and Shape Rendering on a VR Controller. We present Haptic Revolver, a handheld virtual reality controller that renders fingertip haptics when interacting with virtual surfaces. Haptic Revolver's core haptic element is an actuated wheel that raises and lowers underneath the finger to render contact with a virtual surface. As the user's finger moves along the surface of an object, the controller spins the wheel to render shear forces and motion under the fingertip. The wheel is interchangeable and can contain physical textures, shapes, edges, or active elements to provide different sensations to the user. Because the controller is spatially tracked, these physical features can be spatially registered with the geometry of the virtual environment and rendered on-demand.\n\n\"",
        "Document: \"Haptic Retargeting Video Showcase: Dynamic Repurposing of Passive Haptics for Enhanced Virtual Reality Experience. Manipulating a virtual object with appropriate passive haptic cues provides a satisfying sense of presence in virtual reality. However, scaling such experiences to support multiple virtual objects is a challenge as each one needs to be accompanied with a precisely-located haptic proxy object. We showcase a solution that overcomes this limitation by hacking human perception. Our framework for repurposing passive haptics, called haptic retargeting, leverages the dominance of vision when our senses conflict. With haptic retargeting, a single physical prop can provide passive haptics for multiple virtual objects. We introduce three approaches for dynamically aligning physical and virtual objects: body manipulation, world manipulation and a hybrid technique which combines both world and body warping. This video accompanies our CHI paper.\n\n\"",
        "Document: \"Enhancing input on and above the interactive surface with muscle sensing. Current interactive surfaces provide little or no information about which fingers are touching the surface, the amount of pressure exerted, or gestures that occur when not in contact with the surface. These limitations constrain the interaction vocabulary available to interactive surface systems. In our work, we extend the surface interaction space by using muscle sensing to provide complementary information about finger movement and posture. In this paper, we describe a novel system that combines muscle sensing with a multi-touch tabletop, and introduce a series of new interaction techniques enabled by this combination. We present observations from an initial system evaluation and discuss the limitations and challenges of utilizing muscle sensing for tabletop applications.\"",
        "1 is \"Audio hallway: a virtual acoustic environment for browsing\", 2 is \"VoicePen: augmenting pen input with simultaneous non-linguisitic vocalization\"",
        "Given above information, for an author who has written the paper with the title \"Sphere: multi-touch interactions on a spherical display\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001335": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Hybrid PID control for transient performance improvement of motion systems with friction':",
        "Document: \"Non-Quadratic Lyapunov Functions For Performance Analysis Of Saturated Systems. In a companion paper [14], we developed a systematic Lyapunov approach to the regional stability and performance analysis of saturated systems via quadratic Lyapunov functions. The corresponding conditions are expressed in terms of LMIs but can be too conservative in some cases. To obtain less conservative conditions, we use in this paper two types of conjugate Lyapunov functions: the convex hull quadratic function and the max quadratic function. These functions yield bilinear matrix inequalities (BMIs) as conditions for stability and guaranteed performance level. The BMI conditions cover the LMI conditions for quadratic stability as special cases and hence the reduction of conservativeness is guaranteed. A numerical example demonstrates the effectiveness of this paper's methods and the great potential of the non-quadratic Lyapunov functions.\"",
        "Document: \"Time-Varying Sampled-Data Observer With Asynchronous Measurements. In this paper, a time-varying observer for a linear continuous-time plant with asynchronous sampled measurements is proposed. The observer is contextualized in the hybrid systems framework providing an elegant setting for the proposed solution. In particular, some theoretical tools are provided, in terms of linear matrix inequalities (LMIs), certifying asymptotic stability of a certain compact set...\"",
        "Document: \"Lyapunov-based hybrid loops for stability and performance of continuous-time control systems. We construct hybrid loops that augment continuous-time control systems. We consider a continuous-time nonlinear plant in feedback with a (possibly non stabilizing) given nonlinear dynamic continuous-time state feedback controller. The arising hybrid closed loops are guaranteed to follow the underlying continuous-time closed-loop dynamics when flowing and to jump in suitable regions of the closed-loop state space to guarantee that a positive definite function V of the closed-loop state and/or a positive definite function V\"p of the plant-only state is non-increasing along the hybrid trajectories. Sufficient conditions for the construction of these hybrid loops are given for the nonlinear case and then specialized for the linear case with the use of quadratic functions. For the linear case we illustrate specific choices of the functions V and V\"p which allow for the reduction of the overshoot of a scalar output. The proposed approaches are illustrated on linear and nonlinear examples.\"",
        "Document: \"Brief A common framework for anti-windup, bumpless transfer and reliable designs. In this paper, the L\"2 anti-windup approach introduced in Teel and Kapoor (Proceedings of the Fourth ECC, Brussels, Belgium, July 1997) is generalized and applied to the problem of bumpless transfer in (saturated) multi-controller systems, and to the problem of designing highly reliable saturated control systems exploiting hardware redundancy. We first illustrate the L\"2 anti-windup technique and apply it to a simple physical example and then demonstrate the performance of its extension to the above-mentioned problems using this example and an example taken from the literature.\"",
        "1 is \"Non-Smooth 3d Modeling Of A Snake Robot With External Obstacles\", 2 is \"Convergence of Type-Symmetric and Cut-Balanced Consensus Seeking Systems\"",
        "Given above information, for an author who has written the paper with the title \"Hybrid PID control for transient performance improvement of motion systems with friction\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001346": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Efficient Thompson Sampling for Online Matrix-Factorization Recommendation.':",
        "Document: \"Learning Hierarchical Hidden Markov Models with General State Hierarchy. Abstract The  hierarchical  hidden  Markov  model  (HHMM)  is  an  ex - tension  of  the  hidden  Markov  model  to  include  a  hierarchy of the hidden states This form of hierarchical modeling has been  found useful  in applications  such as handwritten char - acter recognition,  behavior recognition,  video indexing,  and text retrieval Nevertheless, the state hierarchy in the original HHMM  is  restricted  to  a  tree  structure This  prohibits  two different states from having the same child, and thus does not allow for sharing of common substructures in the model In this  paper,  we  present  a  general  HHMM  in  which  the  state hierarchy can  be a lattice allowing arbitrary sharing  of sub - structures Furthermore, we provide a method for numerical scaling to avoid underflow, an important issue in dealing with long observation sequences We demonstrate the working of our method in a simulated environment where a hierarchical behavioral model is automatically learned and later used for recognition\"",
        "Document: \"Human Activity Learning and Segmentation using Partially Hidden Discriminative Models. Learning and understanding the typical patterns in the daily activities and routines of people from low-level sensory data is an important problem in many application domains such as building smart en- vironments, or providing intelligent assistance. Traditional approaches to this problem typically rely on supervised learning and generative models such as the hidden Markov models and its extensions. While activity data can be readily acquired from pervasive sensors, e.g. in smart environments, providing man- ual labels to support supervised training is often extremely expensive. In this paper, we propose a new approach based on semi-supervised training of partially hidden discriminative models such as the con- ditional random field (CRF) and the maximum entropy Markov model (MEMM). We show that these models allow us to incorporate both labeled and unlabeled data for learning, and at the same time, provide us with the flexibility and accuracy of the discriminative framework. Our experimental results in the video surveillance domain illustrate that these models can perform better than their generative counterpart, the partially hidden Markov model, even when a substantial amount of labels are unavailable.\"",
        "Document: \"Segmentation of Intentional Human Gestures for Sports Video Annotation. We present results on the recognition of intentional humangestures for video annotation and retrieval. We definea gesture as a particular, repeatable, human movementhaving a predefined meaning. An obvious application ofthe work is in sports video annotation where umpire gesturesindicate specific events. Our approach is to augmentvideo with data obtained from accelerometers worn as wristbands by one or more officials. We present the recognitionperformance using a Hidden Markov Model approach forgesture modeling with both isolated gestures and gesturessegmented from a stream.\"",
        "Document: \"A context-aware personal desktop assistant. We demonstrate an intelligent personal assistant agent that has been developed to aid a busy knowledge worker in managing time commitments and performing tasks. The PExA agent draws on a diverse set of AI technologies that are linked within the SPARK BDI agent framework. We focus on our agent's ability to provide assistance within the context of current user activities, based on its recognition of user workflows and their progress, and on its context-sensitive proactive suggestions. We have instrumented a common suite of desktop applications so that, endowed with a sophisticated workflow tracker, PExA has the ability to pervasively monitor the user's desktop activities. PExA follows and responds to the user's progress on shared tasks, and is highly user-centric in its support for user needs and its adaptivity to user working style and preferences.\"",
        "1 is \"Discovering significant rules\", 2 is \"Long short-term memory.\"",
        "Given above information, for an author who has written the paper with the title \"Efficient Thompson Sampling for Online Matrix-Factorization Recommendation.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001350": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A universal access control method based on host identifiers for Future Internet':",
        "Document: \"A hybrid localization and tracking system in camera sensor networks. AbstractPosition information is of vital importance in the various applications in energy-constrained wireless sensor networks. It has to design a localization mechanism considering both precision and energy consumption factors. In this paper, we propose a hybrid localization system in wireless sensor networks, which is composed of coarse-grained localization system and fine-grained localization system. The coarse-grained localization system takes the wireless signal strength as the reference for distance and gets the rough region as the unknown node. The fine-grained localization system is in charge of location refinement that takes image to localize the unknown node with camera sensor nodes. On the basis of the hybrid localization system, we furthermore design a hybrid tracking system for localizing a moving object. Finally, we build up a test bed to conduct experiments with our developed sensor network. The experiment results show that the proposed hybrid localization and tracking system can achieve high position precision and low energy consumption. Copyright \u00a9 2012 John Wiley & Sons, Ltd.\"",
        "Document: \"Dealing With Mobility-Caused Outdated Mappings in Networks With Identifier/Locator Separation. In networks with identifier/locator separation, packets from a source to a destination are generally tunneled from the source&#39;s ingress tunnel router (ITR) to the destination&#39;s egress tunnel router (ETR). In addition, the source&#39;s ITR often caches the destination&#39;s identifier-to-locator mapping for a certain duration, called cache timeout, in order to avoid frequently querying mapping servers. Whe...\"",
        "Document: \"UPnP IPv4/IPv6 bridge for home networking environment. The purposes of universal plug and play (UPnP) are to allow consumer electronics, intelligent appliances (IA) and mobile devices from different vendors to be Internet connected seamlessly and installed easily at home. Also, as the IPv6 environment becomes inevitable, IPv6 ready IA are more and more popular. However, the fact is that IPv4 UPnP control points cannot communicate with IPv6 UPnP devices or vice versa. In this paper, we proposed an UPnP IPv4/IPv6 bridge for control points and devices can interoperate between IPv4 and IPv6 protocols; that is, they can communicate with each other although some of them support only IPv4 and others support IPv6.\"",
        "Document: \"A Novel Method Defends Against The Path-Based Dos For Wireless Sensor Network. WSNs are significantly different from the traditional network architecture due to its wireless communication, energy limitation, and computation constraint and environment of the application. Because of these differences, security becomes a critical issue. The path based denial of service (PDoS) attacks harm the network maintenance and cause serious damage in the resource constrained WSNs. In a PDoS attack, an adversary can overwhelm sensor node and cluster head node to flood packets along the routing path so that intermediate node must keep active mode and exhaust the energy. In this paper, we creatively propose a novel method, which is operated at the base station to detect the malicious behavior. The proposed method is combined with triple exponential smoothing and Markov chain, so that it makes the detection results more accurate. Meanwhile, we first use the concept of black hole to defend the PDoS attack in WSNs. Simulation results are provided to evaluate the performance and illustrate the contribution of this mechanism.\"",
        "1 is \"A critical look at power law modelling of the Internet\", 2 is \"Why should i share? examining social capital and knowledge contribution in electronic networks of practice\"",
        "Given above information, for an author who has written the paper with the title \"A universal access control method based on host identifiers for Future Internet\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001404": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Transformations of Software Product Lines: A Generalizing Framework Based on Category Theory':",
        "Document: \"Co-evolving meta-models and their instance models: A formal approach based on graph transformation. Model-driven engineering focuses on models as primary artifacts of the software development process, which means programs are mainly generated by model-to-code transformations. In particular, modeling languages tailored to specific domains promise to increase the productivity of software developers and the quality of generated software. Modeling languages, however, evolve over time and therefore, existing models have to be migrated accordingly. The manual migration of models tends to be tedious and error-prone, therefore tools have been developed to (partly) automate this process. Nevertheless, the migration results may not always be well-defined.\"",
        "Document: \"Translation of Restricted OCL Constraints into Graph Constraints for Generating Meta Model Instances by Graph Grammars. The meta modeling approach to syntax definition of visual modeling techniques has gained wide acceptance, especially by using it for the definition of UML. Since meta-modeling is non-constructive, it does not provide a systematic way to generate all possible meta model instances. In our approach, an instance-generating graph grammar is automatically created from a given meta model. This graph grammar ensures correct typing and cardinality constraints, but OCL constraints for the meta model are not supported yet. To satisfy also the given OCL constraints, well-formedness checks have to be done in addition. We present a restricted form of OCL constraints that can be translated to graph constraints which can be checked during the instance generation process.\"",
        "Document: \"Generation of visual editors as eclipse plug-ins. Visual Languages (VLs) play an important role in software system development. Especially when looking at well-defined domains, a broad variety of domain specific visual languages are used for the development of new applications. These languages are typically developed specifically for a certain domain in a way that domain concepts occur as primitives in the language alphabet. Visual modeling environments are needed to support rapid development of domain-specific solutions.In this contribution we present a general approach for defining visual languages and for generating language-specific tool environments. The visual language definition is again given in a visual manner and precise enough to completely generate the visual environment. The underlying technology is Eclipse with its plug-in capabilities on the one hand, and formal graph transformation techniques on the other hand. More precisely, we present an Eclipse plug-in generating Java code for visual modeling plug-ins which can be directly executed in the Eclipse Runtime-Workbench.\"",
        "Document: \"A Variability-Based Approach to Reusable and Efficient Model Transformations. Large model transformation systems often contain transformation rules that are substantially similar to each other, causing performance bottlenecks for systems in which rules are applied nondeterministically, as long as one of them is applicable. We tackle this problem by introducing variability-based graph transformations. We formally define variability-based rules and contribute a novel match-finding algorithm for applying them. We prove correctness of our approach by showing its equivalence to the classic one of applying the rules individually, and demonstrate the achieved performance speed-up on a realistic transformation scenario.\"",
        "1 is \"On timed components and their abstraction\", 2 is \"A generic solution for syntax-driven model co-evolution\"",
        "Given above information, for an author who has written the paper with the title \"Transformations of Software Product Lines: A Generalizing Framework Based on Category Theory\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001413": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Loop acceleration exploration for ASIP architecture':",
        "Document: \"Efficient solution of the 3G network planning problem. The 3G universal mobile telecommunications system (UMTS) planning problem is combinatorially explosive and difficult to solve optimally, though solution methods exist for its three main subproblems (cell, access network, and core network planning). We previously formulated the entire problem as a single integrated mixed-integer linear program (MIP) and showed that only small instances of this global planning problem can be solved to optimality by a commercial MIP solver within a reasonable amount of time (St-Hilaire, Chamberland, & Pierre, 2006). Heuristic methods are needed for larger instances. This paper provides the first complete formulation for the heuristic sequential method (St-Hilaire, Chamberland, & Pierre, 2005) that re-partitions the global formulation into the three conventional subproblems and solves them in sequence using a MIP solver. This greatly improves the solution time, but at the expense of solution quality. We also develop a new hybrid heuristic that uses the results of the sequential method to generate constraints that provide tighter bounds for the global planning problem with the goal of reaching the provable optimum solution much more quickly. We empirically evaluate the speed and solution accuracy of four solution methods: (i) the direct MIP solution of the global planning problem, (ii) a local search heuristic applied to the global planning problem, (iii) the sequential method and (iv) the new hybrid method. The results show that the sequential method provides very good results in a fraction of the time needed for the direct MIP solution of the global problem, and that optimum results can be provided by the hybrid heuristic in greatly reduced time.\"",
        "Document: \"On the Planning of Wireless Sensor Networks: Energy-Efficient Clustering under the Joint Routing and Coverage Constraint. Minimizing energy dissipation and maximizing network lifetime are important issues in the design of applications and protocols for sensor networks. Energy-efficient sensor state planning consists in finding an optimal assignment of states to sensors in order to maximize network lifetime. For example, in area surveillance applications, only an optimal subset of sensors that fully covers the monitored area can be switched on while the other sensors are turned off. In this paper, we address the optimal planning of sensors' states in cluster-based sensor networks. Typically, any sensor can be turned on, turned off, or promoted cluster head, and a different power consumption level is associated with each of these states. We seek an energy-optimal topology that maximizes network lifetime while ensuring simultaneously full area coverage and sensor connectivity to cluster heads, which are constrained to form a spanning tree used as a routing topology. First, we formulate this problem as an Integer Linear Programming model that we prove NP-Complete. Then, we implement a Tabu search heuristic to tackle the exponentially increasing computation time of the exact resolution. Experimental results show that the proposed heuristic provides near-optimal network lifetime values within low computation times, which is, in practice, suitable for large-sized sensor networks.\"",
        "Document: \"Performance Analysis of Fast Handover for Hierarchical MIPv6 in Cellular Networks. Next-generation wireless networks present an all-IP-based architecture integrating the existing cellular networks with wireless local area networks (WLANs), wireless metropolitan area networks (WMANs), wireless ad hoc networks, wireless personal area networks (WPANs), etc. This makes mobility management an important issue for users roaming among these networks/systems. On one hand, intelligent schemes needs to be devised to benefit the IP-based technology, on the other hand, new solutions are required to take into account global roaming among various radio access technology and support of real-time multimedia applications. This paper presents a comprehensive performance analysis of fast handover for hierarchical mobile IPv6 (F-HMIPv6) using a proposed analytical model. Location update cost function, packet delivery cost function and total cost function are formulated respectively based on the fluid-flow mobility model. We investigate the impact of several wireless system factors, such as user velocity, user density, mobility domain size, session-to-mobility ratio on these costs, and present some numerical results.\"",
        "Document: \"PLAN-B: Proximity-Based Lightweight Adaptive Network Broadcasting. Broadcast is an important building block in ad hoc networks. Its challenge is to deliver a message to all nodes in the network for a reasonable cost in terms of message load and delay. Several context-aware broadcasting protocols have been proposed in order to meet this challenge, using location or proximity information in order to fine-tune retransmission decisions. However, existing protocols often target one specific setting and can reveal to be sub-optimal when settings change. Typically, optimal parameters for dense networks will differ from optimal parameters for sparse networks. To address this issue, we propose PLAN-B an adaptive proximity-based broadcast protocol that offers the possibility to define policies in order to adapt its parameters for different network settings at runtime. Our performance evaluations show that PLAN-B outperforms existing static and adaptive protocols in terms of message load in changing and unknown densities up to a factor of 2.\"",
        "1 is \"Dynamic Power Management in Wireless Sensor Networks\", 2 is \"FORMALISING COLLABORATIVE DECISION-MAKING AND PRACTICAL REASONING IN MULTI-AGENT SYSTEMS\"",
        "Given above information, for an author who has written the paper with the title \"Loop acceleration exploration for ASIP architecture\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001428": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Coherent Spatiotemporal Filtering, Upsampling and Rendering of RGBZ Videos':",
        "Document: \"Color Search and Replace. We present an interactive image enhancement technique to adjust the global color composition of an image by find- ing and replacing color gradients. We show how color gradient transformations can perform the basic operations of color editing. To recolor an image, the user designates a mapping of source color gradients to corresponding target color gradients. Each color gradient can be represented by a spherical parameterization, consisting of its midpoint color, contrast radius, as well as hue and luminance angles, in order to give the user separate and in- dependent control over color shift, contrast adjustment, and color variation. Color gradients provide not only a flexible way of selecting color features but also a powerful way of manipulating image colors, as each mapping between a source and a target color gradient defines an affine color transformation. To determine the region of influence of each color mapping, perceptual similarity between colors is evaluated by applying Shepard's law of generalization to color differences. Through a feature-based warping approach, our color warping algorithm applies a continuous, nonlinear, volumetric deformation to the color space in order to approximate the requested color mappings. By making interactive color correction easier to control, our technique may prove useful in a variety of color image enhancement tasks in digital photography, video processing, and information visualization.\"",
        "Document: \"A Psychologically-Based Simulation of Human Behaviour. We describe a system designed to simulate human behaviour in crowds in real-time, concentrating particularly on collision avoidance. The algorithms used are based heavi ly on psychology research, and the ways this has been used are explained in detail. We argue that this approac h gives better results than conventional methods, and detail further work to be done.\"",
        "Document: \"Image Sampling with Quasicrystals. We investigate the use of quasicrystals in image sampling. Quasicrystals produce space-filling, non-periodic point sets that are uniformly discrete and relatively dense, thereby ensuring the sample sites are evenly spread out throughout the sampled image. Their self-similar structure can be attractive for creating sampling patterns endowed with a decorative symmetry. We present a brief general overview of the algebraic theory of cut-and-project quasicrystals based on the geometry of the golden ratio. To assess the practical utility of quasicrystal sampling, we evaluate the visual effects of a variety of non-adaptive image sampling strategies on photorealistic image reconstruction and non-photorealistic image rendering used in multiresolution image representations. For computer visualization of point sets used in image sampling, we introduce a mosaic rendering technique.\"",
        "Document: \"Eye movements and attention for behavioural animation. This paper describes a simulation of attention behaviour aimed at computer-animated characters. Attention is the focusing of a person's perception on a particular object. This is useful for computer animation as it determines which objects the character is aware of: information that can be used in the simulation of the character's behaviour in order to automatically animate the character. The simulation of attention also determines where the character is looking and so is used to produce gaze behaviour. Copyright (C) 2002 John Wiley Sons, Ltd.\"",
        "1 is \"On the Evolution of the Skeleton\", 2 is \"Seeing Beyond Luminance: A Psychophysical Comparison of Techniques for Converting Colour Images to Greyscale.\"",
        "Given above information, for an author who has written the paper with the title \"Coherent Spatiotemporal Filtering, Upsampling and Rendering of RGBZ Videos\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001479": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A residualizing semantics for the partial evaluation of functional logic programs':",
        "Document: \"Concolic Execution and Test Case Generation in Prolog. Symbolic execution extends concrete execution by allowing symbolic input data and then exploring all feasible execution paths. It has been defined and used in the context of many different programming languages and paradigms. A symbolic execution engine is at the heart of many program analysis and transformation techniques, like partial evaluation, test case generation or model checking, to name a few. Despite its relevance, traditional symbolic execution also suffers from several drawbacks. For instance, the search space is usually huge (often infinite) even for the simplest programs. Also, symbolic execution generally computes an overapproximation of the concrete execution space, so that false positives may occur. In this paper, we propose the use of a variant of symbolic execution, called concolic execution, for test case generation in Prolog. Our technique aims at full statement coverage. We argue that this technique computes an underapproximation of the concrete execution space (thus avoiding false positives) and scales up better to medium and large Prolog applications.\"",
        "Document: \"A slicing tool for lazy functional logic programs. Program slicing is a well-known technique that has been widely used for debugging in the context of imperative programming. Debugging is a particularly difficult task within lazy declarative programming. In particular, there exist very few approaches to program slicing in this context. In this paper, we describe a slicing tool for first-order lazy functional logic languages. We also illustrate its usefulness by means of an example.\"",
        "Document: \"Measuring the Effectiveness of Partial Evaluation in Functional Logic Languages. We introduce a framework for assessing the effectiveness of partial evaluators in functional logic languages. Our framework is based on properties of the rewrite system that models a functional logic program. Consequently, our assessment is independent of any specific language implementation or computing environment. We define several criteria for measuring the cost of a computation: number of steps, number of function applications, and pattern matching effort. Most importantly, we express the cost of each criterion by means of recurrence equations over algebraic data types, which can be automatically inferred from the partial evaluation process itself. In some cases, the equations can be solved by transforming their arguments from arbitrary data types to natural numbers. In other cases, it is possible to estimate the improvement of a partial evaluation by analyzing the associated cost recurrence equations.\"",
        "Document: \"A Finite Representation of the Narrowing Space. Narrowing basically extends rewriting by allowing free variables in terms and by replacing matching with unification. As a consequence, the search space of narrowing becomes usually infinite, as in logic programming. In this paper, we introduce the use of some operators that allow one to always produce a finite data structure that still represents all the narrowing derivations. Furthermore, we extract from this data structure a novel, compact equational representation of the (possibly infinite) answers computed by narrowing for a given initial term. Both the finite data structure and the equational representation of the computed answers might be useful in a number of areas, like program comprehension, static analysis, program transformation, etc.\"",
        "1 is \"Supervising offline partial evaluation of logic programs using online techniques\", 2 is \"LiquidHaskell: experience with refinement types in the real world\"",
        "Given above information, for an author who has written the paper with the title \"A residualizing semantics for the partial evaluation of functional logic programs\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001589": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Towards verification-based development of in-vehicle safety critical software: a case study':",
        "Document: \"Hierarchical resource allocation for robust in-home video streaming. High quality video streaming puts high demands on network and processor resources. The bandwidth of the communication medium and the timely arrival of the frames necessitate a tight resource allocation. Given the dynamic environment where videos are started and stopped and electro-magnetic perturbations affect the bandwidth of the wireless medium, a framework is needed that reacts timely to the changes in network load and network operational conditions. This paper describes a hierarchical framework, which can handle the dynamic network resource allocation in a timely manner.\"",
        "Document: \"Synthesizing and Reconstructing Missing Sensory Modalities in Behavioral Context Recognition. Detection of human activities along with the associated context is of key importance for various application areas, including assisted living and well-being. To predict a user's context in the daily-life situation a system needs to learn from multimodal data that are often imbalanced, and noisy with missing values. The model is likely to encounter missing sensors in real-life conditions as well (such as a user not wearing a smartwatch) and it fails to infer the context if any of the modalities used for training are missing. In this paper, we propose a method based on an adversarial autoencoder for handling missing sensory features and synthesizing realistic samples. We empirically demonstrate the capability of our method in comparison with classical approaches for filling in missing values on a large-scale activity recognition dataset collected in-the-wild. We develop a fully-connected classification network by extending an encoder and systematically evaluate its multi-label classification performance when several modalities are missing. Furthermore, we show class-conditional artificial data generation and its visual and quantitative analysis on context classification task; representing a strong generative power of adversarial autoencoders.\"",
        "Document: \"Virtual community based secure service discovery and access for 3D video steaming applications. The Freeband I-Share project aims to define the mechanisms for trust, willingness, resource discovery and sharing mechanisms in virtual communities. To improve the secure and performance of a 3D video streaming application, which is a research vehicle of the I-Share project, we propose a virtual community based access control approach for secure service discovery and access (VICSDA) which groups services in virtual communities and only grants authenticated community members to discover and access these community services. There are two main contributions associated with this approach. First, different from most of the other access control approaches it adopts a dual access control mechanism which allows community services to define their local access control policy besides following the community membership policy. Second, behavior of these community services is monitored in order to guarantee a better QoS provision. Using this approach, the 3D video streaming application can be guaranteed with authentication and message confidentiality through the dual secure service discovery and access mechanism. Better application performance can also be achieved through the community member behavior audit.\"",
        "Document: \"Improved feasibility of fixed-priority scheduling with deferred preemption using preemption thresholds for preemption points. This paper aims at advancing the relative strength of limited-preemptive schedulers by improving the feasibility of a task set and simultaneously limiting, or even precluding, arbitrary preemptions. In particular, we present a refinement of existing limited-preemptive fixed-priority scheduling (FPS) schemes with preemption thresholds for preemption points next to preemption thresholds for sub-jobs, termed fixed-priority scheduling with varying preemption thresholds (FPVS). We derive exact schedulability analysis for FPVS and we develop algorithms to maximize the schedulability of a set of sporadic tasks for given priorities. Since FPVS generalizes existing FPS schemes, we apply our algorithms to those schemes to compare the ratio of schedulable systems. Our experiments show that FPVS can achieve the same schedulability ratio with limited-preemptive sub-jobs as with entirely non-preemptive sub-jobs.\"",
        "1 is \"Ciao: a graphical navigator for software and document repositories\", 2 is \"Ambient Sound Provides Supervision For Visual Learning\"",
        "Given above information, for an author who has written the paper with the title \"Towards verification-based development of in-vehicle safety critical software: a case study\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001606": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Admission control in IEEE 802.11e wireless LANs':",
        "Document: \"Accelerating The Distribution Estimation For The Weighted Median/Mode Filters. Various image filters for applications in the area of computer vision require the properties of the local statistics of the input image, which are always defined by the local distribution or histogram. But the huge expense of computing the distribution hampers the popularity of these filters in real-time or interactive-rate systems. In this paper, we present an efficient and practical method to estimate the local weighted distribution for the weighted median/mode filters based on the kernel density estimation with a new separable kernel defined by a weighted combinations of a series of probabilistic generative models. It reduces the large number of filtering operations in previous constant time algorithms [1,2] to a small amount, which is also adaptive to the structure of the input image. The proposed accelerated weighted median/mode filters are effective and efficient for a variety of applications, which have comparable performance against the current state-of-the-art counterparts and cost only a fraction of their execution time.\"",
        "Document: \"Video content dependent directional transform for intra frame coding. The mode-dependent directional transform (MDDT) employed Karhunen-Loe\u0300ve Transform (KLT) for compressing directional residue signal of intra prediction along its direction. The transform bases were derived from the singular value decomposition (SVD) of residue signals coming from all kinds of video sequences, which were expected to be efficient for most of video sequences. However, the advantage of KLT comes from the concept of a \u201csignal content dependent transform\u201d. MDDT and its variants failed to exploit such a concept, so they did not fully exploit the efficiency of KLT. In this paper, a video content feature is firstly defined as the histogram of the residue produced by intra prediction. Secondly, one KLT basis is computed for each feature of each mode from off-line experiments. Thus, multiple KLT bases identified by their features are provided to each mode instead of only one basis in MDDT. One of them is selected during encoding process by matching the feature of signal being processed to the predefined features. The experiments show that the average improvement of 0.17dB PSNR and 2.23% bits saving can be achieved by the proposed video content dependent directional transform (CDDT) comparing to the state-of-the-art MDDT.\"",
        "Document: \"An Efficient Intra Mode Selection Algorithm For H.264 Based On Fast Edge Classification. The H.264/AVC is the newest video coding standard recommended by ITU-T and MPEG. Compared with all existing video coding standards, H.264 can achieve superior performance by using many advanced techniques. Intra mode selection is an important feature in H.264 standard and can reduce the spatial redundancy in intra frame significantly. An efficient rate distortion optimization (RDO) technique is employed in H.264 to choose the best mode for each MB, but the computational cost increases drastically. In this paper, a fast intra mode selection algorithm is introduced. By using a fast edge detection method which is based on non-normalized Haar transform (NHT), edge for each sub-block can be extracted. Based on the local edge information, only few intra modes are chosen as mode candidates. A fast RDO algorithm is also proposed in this paper. By combing these two methods, computational load is reduced remarkably. Experimental results show that this fast intra mode selection scheme can lessen about 80% encoding time with little loss of bit-rate and visual quality.\"",
        "Document: \"Object segmentation from wide baseline video. In this paper, we propose an automatic approach to segment object from stereo videos, for which the viewpoints are widely apart. We first present a novel saliency analysis to emphasize the foreground object. The saliency map is estimated by combining the depth information recovered by feature matching and the boundary information revealed by color segmentation. The object mask is extracted initially based on the saliency map and then refined by graph-cut segmentation, where color and motion information are efficiently incorporated in both data and smoothness terms. Moreover, a background image is gradually reconstructed during video segmentation, based on which an additional constraint is imposed on the data term to further improve the video segmentation. The proposed method is tested on stereo videos with widely separated viewpoints and severe background clutters. Good experimental results demonstrate the feasibility of the proposed method.\"",
        "1 is \"Adaptive Wavelet Transform for Image Compression via Directional Quincunx Lifting\", 2 is \"A Convex Relaxation Approach For Computing Minimal Partitions\"",
        "Given above information, for an author who has written the paper with the title \"Admission control in IEEE 802.11e wireless LANs\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001634": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'GSM Based Security Analysis for Add-SS Watermarking':",
        "Document: \"Motion projection for floating object detection. Floating mines are a significant threat to the safety of ships in theatres of military or terrorist conflict. Automating mine detection is difficult, due to the unpredictable environment and high requirements for robustness and accuracy. In this paper, a floating mine detection algorithm using motion analysis methods is proposed. The algorithm aims to locate suspicious regions in the scene using contrast and motion information, specifically regions that exhibit certain predefined motion patterns. Throughput of the algorithm is improved with a parallel pipelined data flow. Moreover, this data flow enables further computational performance improvements though special hardware such as field programmable gate arrays (FPGA) or Graphics Processing Units (GPUs). Experimental results show that this algorithm is able to detect mine regions in the video with reasonable false positive and minimum false negative rates.\"",
        "Document: \"Efficient stereo vision algorithms for resource-limited systems. In most circumstances, determining an acceptable trade-off between speed and accuracy when selecting a stereo vision algorithm for implementation is dependent on the target application. This work attempts to provide a perspective on the efficiency of existing real-time stereo vision algorithms in terms of this trade-off. This work also provides an example of modifying an existing highly accurate stereo vision algorithm to increase its runtime performance while trying to limit the loss in accuracy. The modifications can be used to increase efficiency of several other local stereo vision algorithms due to sharing some common components. Such an example demonstrates the challenge of making efficient trade-offs in accuracy for runtime performance. It is shown that the modifications resulted in an 8X speedup over the original algorithm, with accuracy results comparable to existing real-time algorithms.\"",
        "Document: \"Automatic fabric defect detection with a wide-and-compact network. \u2022This paper proposes a compact convolutional neural network architecture for the detection of a few common fabric defects.\u2022The proposed network achieved superior performance in terms of detection accuracy with a much smaller model size, compared to mainstream convolutional neural network architectures.\u2022The proposed network worked well not only for fabric defects detection, but also for object recognition on a few public datasets.\"",
        "Document: \"Using Accurate Feature Matching for Unmanned Aerial Vehicle Ground Object Tracking. Tracking moving objects with a moving camera is a challenging task. For unmanned aerial vehicle applications, targets of interest such as human and vehicles often change their location from image frame to frame. This paper presents an object tracking method based on accurate feature description and matching, using the SYnthetic BAsis descriptor, to determine a homography between the previous frame and the current frame. Using this homography, the previous frame can be transformed and registered to the current frame to find the absolute difference and locate the objects. Once the objects of interest are located, the Kalman filter is then used for tracking their movement. This proposed method is evaluated with three video sequences under image deformation: illumination change, blurring and camera movement (i.e. viewpoint change). These video sequences are taken from unmanned aerial vehicles (UAVs) for tracking stationary and moving objects with a moving camera.\"",
        "1 is \"Pictorial structures revisited: People detection and articulated pose estimation\", 2 is \"Fragility analysis of adaptive quantization-based image hashing\"",
        "Given above information, for an author who has written the paper with the title \"GSM Based Security Analysis for Add-SS Watermarking\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001662": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'On the complexity of privacy-preserving complex event processing':",
        "Document: \"Interval event stream processing. Event stream processing (ESP) has become increasingly important in modern applications, ranging from supply chain management to real-time intrusion detection. Existing ESP engines have focused on detecting temporal patterns from instantaneous events, that is, events with no duration. Under such a model, an event instance can only be happening \"before\", \"after\" or \"at the same time as\" another event instance. However, such sequential patterns are inadequate to express the complex temporal relationships in domains such as medical, finance and meteorology, where the events' durations could play an important role.\"",
        "Document: \"A clustering algorithm for radial basis function neural network initialization. In this paper, we propose an Output-Constricted Clustering (OCC) algorithm for Radial Basis Function Neural Network (RBFNN) initialization. OCC first roughly partitions the output based on the required precision and then refinedly clusters data based on the input complexity within each output partition. The main contribution of the proposed clustering algorithm is that we introduce the concept of separability, which is a criterion to judge the suitability of the number of sub-clusters in each output partition. As a result, OCC is able to determine the proper number of sub-clusters with appropriate locations within each output partition by considering both input and output information. The resulting clusters from OCC are used to initialize RBFNN, with proper number and initial locations of for hidden neurons. As a result, RBFNN starting it's learning from a good point, is able to achieve better approximation performance than existing clustering methods for RBFNN initialization. This better performance is illustrated by a number of examples.\"",
        "Document: \"Recommendation Based On Frequent N-Adic Concepts. In social networks, many users tend to share items such as movies, books, songs and images by rating them with a series of discrete numbers or annotating them with a set of tags. Clearly, there are some semantic relationships among the users, items, ratings, tags and other information. Most of the past works only focused on some ternary relationships such as users-items-ratings or users-items-tags to make recommendations. But the ternary relationships which do not make good use of the given information are insufficient to provide accurate recommendations. In this paper, we propose a novel recommendation method based on frequent n-adic concepts which can mine the hidden conceptualization in the relationships. If there are tags, we model the relationships into the quadruples <users, items, ratings, tags> and if there are no tags, we also have some other information and model the relationships into the quintuples <users, items, ratings, contexts, features>. Experimental results on MovieLens dataset demonstrate that our method has shown a significant improvement over the state-of-the-art recommendation approaches in terms of precision.\"",
        "Document: \"Economic Evaluation of Limestone-Gypsum Method Based on FAHP. The limestone-gypsum flue gas desulfurization at the coal steam-electric plant applied widely, and take the currently widespread adoption of desulfurization, simple and wet method, spray fog dry method, electron beam method... etc., although electron beam method and limestone-gypsum desulfurization has a higher rate, it increase the unit price degree deeply, so the economy be not quite reasonable. Therefore, this text will take the fuzzy theories as the foundation, gather AHP analysis methods, carry on economy evaluation to the limestone-gypsum method, aims at providing a decision foundation for the power station's FGD. Thus, the evaluation result imply that the use of FAHP method to economy evaluation is reasonable and rationality.\"",
        "1 is \"Framework for evaluating clustering algorithms in duplicate detection\", 2 is \"Approximability of Sparse Integer Programs\"",
        "Given above information, for an author who has written the paper with the title \"On the complexity of privacy-preserving complex event processing\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001677": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Hierarchical lane detection for different types of roads':",
        "Document: \"Segmentation of human body parts using deformable triangulation. This paper presents a novel segmentation algorithm to segment a body posture into different body parts using the technique of deformable triangulation. To analyze each posture more accurately, they are segmented into triangular meshes, where a spanning tree can be found from the meshes using a depth-first search scheme. Then, we can decompose the tree into different subsegments, where each subsegment can be considered as a limb. Then, two hybrid methods (i.e., the skeleton-based and modeldriven methods) are proposed for segmenting the posture into different body parts according to its occlusion conditions. To analyze occlusion conditions, a novel clustering scheme is proposed to cluster the training samples into a set of key postures. Then, a model space can be used to classify and segment each posture. If the input posture belongs to the nonocclusion category, the skeleton-based method is used to divide it into different body parts that can be refined using a set of Gaussian mixture models (GMMs). For the occlusion case, we propose a model-driven technique to select a good reference model for guiding the process of body part segmentation. However, if two postures' contours are similar, there will be some ambiguity that can lead to failure during the model selection process. Thus, this paper proposes a tree structure that uses a tracking technique so that the best model can be selected not only from the current frame but also from its previous frame. Then, a suitable GMM-based segmentation scheme can be used to finely segment a body posture into the different body parts. The experimental results show that the proposed method for body part segmentation is robust, accurate, and powerful.\"",
        "Document: \"Human Face Detection Using Geometric Triangle Relationship. The proposed system consists of two main parts. The first part is to search the potential face regions that are gotten from the triangles based on the rules of \"the combination of two eyes and one mouth\". The second part of the proposed system is to perform the face verification task.The proposed face detection system can locate multiple faced embedded in complicated backgrounds. Moreover, it is able to handle different size, different light condition, varying pose and expression, noise and defocus problems, and the problem of partial occlusion of mouth and sunglasses.Experimental results demonstrate that an approximately 98% success rate is achieved and the relative false detection rate is very low.\"",
        "Document: \"Efficient matching of large-size histograms. As we know, histogram matching is a commonly-adopted technique in the applications of pattern recognition. The matching of two patterns can be accomplished by matching their corresponding histograms. In general, the number of features and the resolution of each feature will determine the size of histogram. The more the number of features and the higher the resolution of each feature, the stronger the discrimination capability of histogram will be. Unfortunately, the increase of histogram size will lead to the decrease of the efficiency of histogram matching because traditional algorithms in evaluating similarity are all relevant to the histogram size. In this paper, a novel histogram-matching algorithm is proposed whose efficiency is irrelevant to the histogram size. The proposed algorithm can be applied to commonly-adopted histogram similarity measurement functions, such as histogram intersection function, L1 norm, L2 norm, \u03c72 test and so on. By adopting our proposed algorithm, future researchers can focus more on the selection and combination of histogram features and freely adjust the resolution of each feature without worrying the decrease of retrieval efficiency.\"",
        "Document: \"Multiple classifiers for color flag and trademark image retrieval. A novel region-based multiple classifier color image retrieval system is presented. In our approach, a region-growing technique is first employed to cluster connected color pixels with the same color in an image to form color regions which are the primitive elements utilized in our proposed approach. Then, three complementary region-based classifiers that we developed are selected in the classifier selection stage, which include color classifier, shape classifier, and relational classifier. In each classifier, a virtue probability representing the probability that an image is similar to the query image is defined. Thereafter a set of virtue probabilities is calculated in each classifier. Next, the measurement dependent methods are applied to combine the virtue probabilities of classifiers in the decision combination stage. The dynamic selection scheme designed in the decision combination stage can further improve the system performance dramatically. Experimental results reveal the feasibility and validity of our proposed approach in solving the color image retrieval problem\"",
        "1 is \"Face recognition/detection by probabilistic decision-based neural network\", 2 is \"Spatio-temporal video search using the object based video representation\"",
        "Given above information, for an author who has written the paper with the title \"Hierarchical lane detection for different types of roads\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001722": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Is 2010: Curriculum Guidelines For Undergraduate Degree Programs In Information Systems':",
        "Document: \"The Behavioral Roots of Information Systems Security: Exploring Key Factors Related to Unethical IT Use. Unethical information technology (IT) use, related to activities such as hacking, software piracy, phishing, and spoofing, has become a major security concern for individuals, organizations, and society in terms of the threat to information systems (IS) security. While there is a growing body of work on this phenomenon, we notice several gaps, limitations, and inconsistencies in the literature. In order to further understand this complex phenomenon and reconcile past findings, we conduct an exploratory study to uncover the nomological network of key constructs salient to this phenomenon, and the nature of their interrelationships. Using a scenario-based study of young adult participants, and both linear and nonlinear analyses, we uncover key nuances of this phenomenon of unethical IT use. We find that unethical IT use is a complex phenomenon, often characterized by nonlinear and idiosyncratic relationships between the constructs that capture it. Overall, ethical beliefs held by the individuals, along with economic, social, and technological considerations are found to be relevant to this phenomenon. In terms of practical implications, these results suggest that multiple interventions at various levels may be required to combat this growing threat to IS security.\"",
        "Document: \"Outcomes from Conduct of Virtual Teams at Two Sites: Support for Media Synchronicity Theory. As geographic, temporal, and cost constraints move organizations toward virtual teamwork for increasingly complex tasks, research on the impact of groups attempting to solve business problems without face-to-face communication becomes more critical. In two independent studies in two different organizations, virtual groups provided feedback related to choice of communications media. The results of the studies lend support to Media Synchronicity Theory which suggests that communications media with low synchronicity (e.g. e-mail, bulletin board) may be appropriate for \"conveyance\" of information, whereas media with high synchronicity (e.g. face-to-face, video conference) may be more desirable for \"convergence\" on shared meaning. For researchers, Media Synchronicity Theory provides an alternative explanation for the variety of studies both supporting and contradicting Media Richness and Social Presence theories. For practitioners, the results suggest an approach to communications media choices that may improve the effectiveness of team problem solving.\"",
        "Document: \"Fine-Tuning the Human-Computer Interface: Verbal versus Keyboard Input in an Idea Generation Context. Voice recognition technologies are rapidly evolving to help humans interact with computers more efficiently and effectively. Despite their potential advantages, their impact on system usability has not received sufficient empirical attention. To this end, this study applies voice recognition technologies in a setting where the speed and volume of human input is critical - small group idea generation. Rather than forcing group members to input ideas via keyboard, a novel idea generation technique is introduced whereby ideas are captured directly through verbalization. The results indicate that inputting ideas verbally enhances system usability, providing a more efficient and effective mechanism for generating ideas in a computer-mediated environment. Verbalizing ideas appears to help group members focus on analytical thinking and leverage others' ideas, ultimately facilitating the creation of idea pools that are vastly superior in terms of quantity and quality. As expected, these effects are robust across nominal and small interacting groups.\"",
        "Document: \"Understanding meteor burst communications technologies. Seeking to realize the potential of a reemergent communications capability.\"",
        "1 is \"The Effects of IT-Enabled Cognitive Stimulation Tools on Creative Problem Solving: A Dual Pathway to Creativity.\", 2 is \"Web and wireless site usability: understanding differences and modeling use\"",
        "Given above information, for an author who has written the paper with the title \"Is 2010: Curriculum Guidelines For Undergraduate Degree Programs In Information Systems\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001831": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Style-sensitive 3D model retrieval through sketch-based queries.':",
        "Document: \"Ontology Based Automatic Image Annotation Using Multi-class SVM. Image annotation is usually formed as a multiclass classification problem. Traditional methods learn the co-occurrence of keywords and images while they ignore the correlation between keywords, which turned out to be one of the reasons causing poor experiment results. In this paper, we propose an automatic image annotation approach by using multiclass SVM with ontology to achieve a higher accuracy. In our paper, we choose semantic dictionary Word Net in which hierarchy defined words are derived from the text ontology to calculate the correlations between keywords. Specifically, we use Bags of Visual Words model to present the image visual feature and apply a mixed kernel in multiclass SVM. Finally, we combine the probability outputs to get the final results. Compared to other state-of-the-art multiclass classification methods, our approach tested in typical Corel dataset maintain a high level of accuracy in classification.\"",
        "Document: \"Multiple-cue saliency measurement and optimized image composition for image retargeting. We present a novel image retargeting approach for high-definition displaying based on multiple-cue salient region extraction and optimized image composition, which allows resizing an image into user-specified aspect ratios while preserving prominent visual salient image information. Firstly, we sum up six properties for measuring the salient objects and apply them to region extraction. Then, according to our optimized rules of image composition, we evaluate the aesthetic scores with different extracted sub-images. Finally, we sort out the target image with the highest aesthetic score and retarget it to fit the final scale of displaying. The experimental results show the performance and effectiveness of our approach.\"",
        "Document: \"Guiding developers to make informative commenting decisions in source code. Code commenting is a common programming practice of practical importance to help developers review and comprehend source code. However, there is a lack of thorough specifications to help developers make their commenting decisions in current practice. To reduce the effort of making commenting decisions, we propose a novel method, CommentSuggester, to guide developers regarding appropriate commenting locations in the source code. We extract context information of source code and employ machine learning techniques to identify possible commenting locations in the source code. The encouraging experimental results demonstrated the feasibility and effectiveness of our commenting suggestion method.\n\n\"",
        "Document: \"Mesh simplification algorithm based on n-edge mesh collapse. This paper presents a method for dividing the triangle mesh into n-edge mesh and puts forward a new mesh simplification algorithm based on n-edge mesh collapse. An n-edge mesh can be in the form of an edge, a triangle or a quadrangle, it depends on the value of \u2018n\u2019. The algorithm utilizes iterative collapse of n-edge mesh to simplify meshes and the surface error approximations are maintained using quadric error metrics. There are n-1 vertices and 2(n-1) faces which have to be collapsed during every simplification, so only few collapses are need when n becomes bigger. And this means, the time of the simplification process can be reduced. Our algorithm contains Garland\u2019s (n=2) [5] and Pan\u2019s (n=3) [11] cases, thus it can be regarded as the summarized algorithm of mesh simplification based on the geometry element collapse. Experimental results demonstrate the different cases, which hold different values of \u2018n\u2019 in the algorithm.\"",
        "1 is \"The Gesture Pendant: A Self-illuminating, Wearable, Infrared Computer Vision System for Home Automation Control and Medical Monitoring\", 2 is \"Progressive meshes\"",
        "Given above information, for an author who has written the paper with the title \"Style-sensitive 3D model retrieval through sketch-based queries.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001839": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Chameleon: a new multi-layer channel router':",
        "Document: \"A 475 Mv, 4.9 Ghz Enhanced Swing Differential Colpitts Vco In 130 Nm Cmos With An Fom Of 196.2 Dbc/Hz. An enhanced swing differential Colpitts VCO operates as low as 400 mV and enables oscillations to go beyond both the supply voltage and ground. Operating at 475 mV, the 4.9 GHz VCO consumes 2.7 mW. The 130 nm CMOS VCO's measured phase noise is -136.2 dBc/Hz at a 3 MHz offset frequency. The resulting FoM of 196.2 dBc/Hz makes it the highest performing integrated LC oscillator published to date.\"",
        "Document: \"Neural Network Design for Behavioral Model Generation with Shape Preserving Properties. This paper describes a novel modeling methodology for electrothermal effects in large mixed-signal circuits. It is accomplished by developing analog event-driven electrothermal and thermal models in an analog hardware description language (AHDL). The ...\"",
        "Document: \"Experimental Characterization and Analysis of an Asynchronous Approach for Reduction of Substrate Noise in Digital Circuitry. Delay insensitive asynchronous circuitry provides significant advantages with respect to substrate noise due to localized switching. The differences between the substrate noise from NULL convention logic (NCL) and traditional clocked Boolean logic (CBL) are described and analyzed based on measured results. A test chip fabricated in the TSMC 0.25 $\\mu$m process shows that a pseudo-random number generator implemented with NCL generates 23 dB less substrate noise compared to the equivalent synchronous design. In a larger scale digital circuit, the substrate noise improvement offered by an asynchronous 8051 processor over its synchronous counterpart was nearly 10 dB. The effect of this substrate noise on an analog circuit was explored with a delta-sigma modulator (DSM) example. The signal-to-noise ratio performance of a second order DSM was not affected by the substrate noise from the NCL 8051 processor while it experiences up to 15 dB degradation when the CBL 8051 processor is clocked near integer multiples of the DSM sampling frequency.\"",
        "Document: \"Parameter variation analysis for voltage controlled oscillators in phase-locked loops. Abstract\u2014 A new oscillator sensitivity analysis that predicts the impact of parameter variations of a VCO in a PLL is presented in this paper. Sensitivities of an oscillator\u2019s steady-state performance to design, process, or environmental parameter variations can be accurately and efficiently computed with the proposed analysis.\"",
        "1 is \"A 1.2-V-only 900-mW 10 gb ethernet transceiver and XAUI interface with robust VCO tuning technique\", 2 is \"Communication by sampling in time-sensitive distributed systems\"",
        "Given above information, for an author who has written the paper with the title \"Chameleon: a new multi-layer channel router\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001929": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Blind separation of two users based on user delays and optimal pulse-shape design':",
        "Document: \"Compact Cram\u00e9r-Rao Bound Expression for Independent Component Analysis. Despite of the increased interest in independent component analysis (ICA) during the past two decades, a simple closed form expression of the Cramer-Rao bound (CRB) for the demixing matrix estimation has not been established in the open literature. In the present paper we fill this gap by deriving a simple closed-form expression for the CRB of the demixing matrix directly from its definition. A si...\"",
        "Document: \"Complex ICA using generalized uncorrelating transform. An extension of the whitening transformation for complex random vectors, called the generalized uncorrelating transformation (GUT), is introduced. GUT is a generalization of the strong-uncorrelating transform [J. Eriksson, V. Koivunen, Complex-valued ICA using 2nd-order statistics, in: Proceedings of the IEEE Workshop on Machine Learning for Signal Processing (MLSP'04), Sao Luis, Brazil, 2004] based upon generalized estimators of the covariance and pseudo-covariance matrix, called the scatter matrix and spatial pseudo-scatter matrix, respectively. Depending on the selected scatter and spatial pseudo-scatter matrix, GUT estimators can have largely different statistical properties. Special emphasis is put on robust GUT estimators. We show that GUT is a separating matrix estimator for complex-valued independent component analysis (ICA) when at most one source random variable possess circularly symmetric distribution and sources do not have identical distribution. In the context of ICA, our approach is computationally attractive as it is based on straightforward matrix computations. Simulations and examples are used to confirm reliable performance of our method.\"",
        "Document: \"Procedural CAD Models from Range Data. This paper addresses the problem of constructing geometric models using data from 3-D imaging sensors. A geometric model is obtained by estimating the shape of an artifact from noisy measurements. The developed techniques can be used in Computer Aided Geometric Design as an aid for designing free-form shapes and for reverse engineering. The goal is to construct a procedural model which allows to convey structural properties of a part in addition to low, level geometric data. Optical 3-D imaging sensors produce incomplete data from each viewpoint. In, order for a complete data set to be obtained, multiple images, each from a different viewpoint, have to be merged. A technique for estimating the relative transformation among the viewpoints is presented. Multiple shape representations are used in shape estimation because at present there is no single representation that would always be appropriate. The primary geometric model construction tools are NURBS approximation, Delaunay triangulation, and superellipsoid model recovery. A surface refinement process is used to assure that the accuracy of the approximation is within a user-defined tolerance value. Experimental results merging data from multiple viewpoints and of model construction of standard geometric shapes and sculptured free-form surfaces are presented.\"",
        "Document: \"Influence Function and Asymptotic Efficiency of Scatter Matrix Based Array Processors: Case MVDR Beamformer. In this paper, we consider array processors that are scale-invariant functions of the array covariance matrix. The emphasis is on Capon's MVDR beamformer. We call such an array processor as scatter matrix based (SMB) array processor since the covariance matrix is required only up to a constant scalar and thus a scatter matrix (proportional to covariance under finite covariance assumption) provides sufficient information. In order to establish interesting statistical robustness and large sample properties, we derive a general expression for the influence function and the asymptotic covariance structure of SMB-MVDR beamformer weights. Our results apply under the class of complex elliptically symmetric distributions, which includes the commonly used complex normal distribution as a special case. We illustrate the theory by deriving the IF and asymptotic relative efficiencies of the conventional SMB-MVDR beamformer that employs the sample covariance matrix and beamformers that employ robust M -estimators of scatter. Theoretical findings are confirmed by simulations. Our findings favor beamformers based upon M-estimators of scatter, since they combine a high efficiency with appealing robustness properties.\"",
        "1 is \"Optimization flow control\u2014I: basic algorithm and convergence\", 2 is \"Decentralized inter-cell interference coordination by autonomous spectral reuse decisions\"",
        "Given above information, for an author who has written the paper with the title \"Blind separation of two users based on user delays and optimal pulse-shape design\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001985": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Guest editor's introduction special section on the Virtual Reality Conference (VR).':",
        "Document: \"An Empirical Study of Hear-Through Augmented Reality: Using Bone Conduction to Deliver Spatialized Audio. Augmented reality (AR) is the mixing of computer-generated stimuli with real-world stimuli. In this paper, we present results from a controlled, empirical study comparing three ways of delivering spatialized audio for AR applications: a speaker array, headphones, and a bone-conduction headset. Analogous to optical-see-through AR in the visual domain, hear-through AR allows users to receive computer-generated audio using the bone-conduction headset, and real-world audio using their unoccluded ears. Our results show that subjects achieved the best accuracy using a speaker array physically located around the listener when stationary sounds were played, but that there was no difference in accuracy between the speaker array and the bone-conduction device for sounds that were moving, and that both devices outperformed standard headphones for moving sounds. Subjective comments by subjects following the experiment support this performance data.\"",
        "Document: \"Handling of Virtual Contact in Immersive Virtual Environments: Beyond Visuals. This paper addresses the issue of improving the perception of contact that users make with purely virtual objects in virtual environments. Because these objects have no physical component, the user's perceptual understanding of the material properties of the object, and of the nature of the contact, is hindered, often limited solely to visual feedback. Many techniques for providing haptic feedback to compensate for the lack of touch in virtual environments have been proposed. These systems have increased our understanding of the nature of how humans perceive contact. However, providing effective, general-purpose haptic feedback solutions has proven elusive. We propose a more-holistic approach, incorporating feedback to several modalities in concert. This paper describes a prototype system we have developed for delivering vibrotactile feedback to the user. The system provides a low-cost, distributed, portable solution for incorporating vibrotactile feedback into various types of systems. We discuss different parameters that can be manipulated in order to provide different sensations, propose ways in which this feedback can be combined with feedback of other modalities to create a better understanding of virtual contact, and describe possible applications.\"",
        "Document: \"Extracting camera-control requirements and camera movement generation in a 3D virtual environment. This paper proposes a new method to generate smooth camera movement that is collision-free in a three-dimensional virtual environment. It generates a set of cells based on cell decomposition using a loose octree in order not to intersect with polygons of the environment. The method defines a camera movement space (also known as Configuration Space) which is a set of cells in the virtual environment. In order to generate collision-free camera movement, the method holds a path as a graph structure which is based on the adjacency relationship of the cells, and makes the camera move on the graph. Furthermore, by using a potential function for finding out the force that aims the camera at the subject and a penalty function for finding out the force that restrains the camera on the graph when the camera moves on the graph, we generate smooth camera movement that captures the subject while avoiding obstacles. Several results in static and dynamic environments are presented and discussed.\"",
        "Document: \"Vibrotactile letter reading using a low-resolution tactor array. Vibrotactile displays have been studied for several decades in the context of sensory substitution. Recently, a number of vibrotactile displays have been developed to extend sensory modalities in virtual reality. Some of these target the whole body as the stimulation region, but existing systems are only designed for discrete stimulation points at specific parts of the body. However, since human tactile sensation has more resolution, a higher density might be required in tactor alignment in order to realize general-purpose vibrotactile displays. One problem with this approach is that it might result in an impractically high number of required tactors. Our current focus is to explore ways of simplifying the system while maintaining an acceptable level of expressive ability. As a first step, we chose a well-studied task: tactile letter reading. We examined the possibility of distinguishing alphanumeric letters by using only a 3-by- 3 array of vibrating motors on the back of a chair. The tactors are driven sequentially in the same sequence as if someone were tracing the letter on the chair's back. The results showed 87% successful letter recognition in some cases, which was close to the results in previous research with much larger arrays.\"",
        "1 is \"Assessing continuity and compatibility in augmented reality systems\", 2 is \"Improved Techniques for Training GANs.\"",
        "Given above information, for an author who has written the paper with the title \"Guest editor's introduction special section on the Virtual Reality Conference (VR).\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002017": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Early detection of cycles with pseudo-end-station in Fasnet networks':",
        "Document: \"Learning Of Kernel Functions In Support Vector Machines. The selection and learning of kernel functions is a very important but rarely studied problem in the field of support vector learning. However, the kernel function of a support vector machine has great influence on its performance. The kernel function projects the dataset from the original data space into the feature space, and therefore the problems which can not be done in low dimensions could be done in a higher dimension through the transform of the kernel function. In this paper, we introduce the gradient descent method into the learning of kernel functions. Using the gradient descent method, we can conduct learning rules of the parameters which indicate the shape and distribution of the kernel functions. Therefore, we can obtain better kernel functions by training of their parameters with respect to the risk minimization principle. The experimental results have shown that our approach can derive better kernel functions and thus has better generalization ability than other methods.\"",
        "Document: \"Exploring Friendships In Blogosphere By Finding 1.5-Cliques. In this work, we show that finding 1.5-cliques is a good idea for exploring friendships in Blogosphere. In today's blog service environment, friend relationships are usually unidirectionally established, without the confirmation from the other party. Conventional methods find either too small or too few cliques in forming friend groups, since bidirectional links are not completely built under such circumstances. To solve this problem, the idea of finding 1.5-cliques is proposed, and is used for exploring friend groups from a social network, Wretch(www.wretch.cc), as a case study. The effectiveness of this idea is valued from the perspective of the densities of the explored groups. By investigating the data we have been continually collecting in the past year, this method is demonstrated to be pretty good.\"",
        "Document: \"An Incremental Algorithm for Discovering Fuzzy Temporal Web Usage Patterns. In this paper, we are interested in discovering from web access data temporal web usage patterns in terms of fuzzy temporal association rules. Fuzzy temporal association rules can describe periodical and seasonal web browsing behaviors of the clients and help develop effective seasonal marketing strategies and give an indication to organize web space efficiently in different time periods. We propose a mining system based on an incremental mining algorithm, called Border-Filtering, to deal with the on-line collection of usage data on the web. With the aid of the fuzzy calendric algebra, knowledge embedded in the clickstream data can be efficiently discovered By keeping knowledge of the original database in a border, the work for counting those candidates which are generated in an incoming database but also exist in the border over the old database can be saved. The minimal border can be easily derived. Moreover, the smallest set of candidate itemsets can be computed in an efficient way, and the unnecessary scans over the database with the candidate itemsets are saved. Simulation results have shown that our system runs faster than other incremental mining techniques, especially when there are a small number of new frequent itemsets in the updated database.\"",
        "Document: \"Pattern Fusion In Feature Recognition Neural Networks For Handwritten Character Recognition. Hussain and Kabuka [8] proposed a feature recognition neural network to reduce the network size of Neocognitron [6]. However, a distinct subnet is created for every training pattern. Therefore, a big network is obtained when the number of training patterns is large. Furthermore, recognition rate can be hurt due to the failure of combining features from similar training patterns. We propose an improvement by incorporating the idea of fuzzy ARTMAP [1], [2] in the feature recognition neural network. Training patterns are allowed to be merged, based on the measure of similarity among features, resulting in a subnet being shared by similar patterns. Because of the fusion of training patterns, network size is reduced and recognition rate is increased.\"",
        "1 is \"A block-based MAP segmentation for image compressions\", 2 is \"Optimized FMIPv6 Using IEEE 802.21 MIH Services in Vehicular Networks\"",
        "Given above information, for an author who has written the paper with the title \"Early detection of cycles with pseudo-end-station in Fasnet networks\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002024": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Mobile multicast source support in PMIPv6 networks.':",
        "Document: \"Performance Analysis of Time-Triggered Ether-Networks Using Off-the-Shelf-Components. The performance analysis and validation of distributed real-time systems poses significant challenges due to high accuracy requirements at the measurement tools. A fully synchronized time-scale at ultrafine granularity is not easy to generate. Even though there are several analyzer tools for standard switched Ethernet, these tools cannot be applied in time-triggered networks, since they do not meet the requirements of synchronized packet generation. This paper introduces a low cost and lightweight approach to measure end-to-end latency of time-triggered Ethernet traffic with off-the-shelf components. By using standard computer hardware and a real-time Linux Kernel, it is shown that measurement can be achieved in a resolution of microseconds. Furthermore, a validation with an Ethernet performance analyzer and a mathematical framework is presented to check the given results.\"",
        "Document: \"Evaluating requirements of high precision time synchronisation protocols using simulation. High precision time synchronisation protocols are used in distributed real-time systems such as trains, planes, cars or industrial installations. In time-triggered systems, with a coordinated time division multiple access media allocation strategy, the achievable precision of time synchronisation among sending participants determines the quality of communication and the available bandwidth. The simulation of time synchronisation protocols allows to find problems at the earliest time -- in general, during the design and configuration -- of a synchronised distributed system. In this work we show a concept for the simulation of distributed real-time synchronisation protocols that uses discrete event-based simulation. Our model for the OMNeT++ Framework is adaptable and thus allows for providing highly accurate results or fast simulations. The precise simulation of a real-time synchronisation protocol usually consumes considerable simulation time. This paper presents an approach to speed up accurate simulation, based on recordings of previous runs. We evaluate typical real-world use cases for the introduced concept by simulating the AS6802 standard for time synchronisation. Our results show that the simulation can help to reduce the effort of determining configuration parameters for clock synchronisation protocols. We further quantify the performance increase of our evolutionary approach.\"",
        "Document: \"Towards Distributed Threat Intelligence in Real-Time. In this demo, we address the problem of detecting anomalies on the Internet backbone in near real-time. Many of today's incidents may only become visible from inspecting multiple data sources and by considering multiple vantage points simultaneously. We present a setup based on the distributed forensic platform VAST that was extended to import various data streams from passive measurements and incident reporting at multiple locations, and perform an effective correlation analysis shortly after the data becomes exposed to our queries.\"",
        "Document: \"Media objects in time-a multimedia streaming system-work in progress paper v 1.5. There is now the potential for widely available networked multimedia applications embedded in a high quality Internet infrastructure, giving rise to new approaches in the areas of teleteaching and Internet presentations: the distribution of professionally styled multimedia streams is within the realm of possibility. This paper presents a prototype\u2013\u2013both model and runtime environment\u2013\u2013of a time directed media system treating any presentational contributions as reusable media object components. The plug-in free runtime system is based on a database and allows for flexible support of static media types as well as for easy extensions by streaming media servers. The prototype implementation includes a preliminary version of Web authoring platform.\"",
        "1 is \"Energy-efficient cognitive access approach to convergence communications.\", 2 is \"On sizing CCN content stores by exploiting topological information\"",
        "Given above information, for an author who has written the paper with the title \"Mobile multicast source support in PMIPv6 networks.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002136": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'QoS-adaptive bandwidth scheduling in continuous media streaming':",
        "Document: \"Topology Control For Increasing Connectivity In Cooperative Wireless Ad Hoc Networks. We propose a novel topology control scheme that reduces the transmission power of nodes and increases the network connectivity, based on the fact that Cooperative Communication (CC) technology can bridge disconnected networks Simulation results demonstrate that our scheme greatly increases the connectivity for a given transmission power, compared to other topology control schemes\"",
        "Document: \"DSML: Dual Signal Metrics for Localization in Wireless Sensor Networks. In wireless sensor networks and wireless ad-hoc networks, localization systems have used diverse signal metrics such as Received Signal Strength Indicator (RSSI) and Time Difference of Arrival (TDoA) for accurate assignment of a node position. We propose a novel scheme that applies two signal metrics, which are TDoA and RSSI exclusively, into time- based positioning scheme (TPS). For energy-efficient coverage extension, the proposed scheme uses range check technique that reduces the communication energy consumption of nodes. With two location information of neighbor nodes, the node can calculate two candidate positions through bilateration. Without an additional beacon message reception, range check is applied to find the unique position between two candidate positions. Range check also can be carried out collaboratively in general environment with the information of two-hop neighbor nodes. At the performance evaluation, we analyze and test the reduced communication cost of nodes in the extended area. Also, it is shown that the ratio of unique position assignment is increased in the general environment by range check technique.\"",
        "Document: \"Construction of directional virtual backbones with minimum routing cost in wireless networks. It is well-known that the application of directional antennas can help conserve bandwidth and energy consumption in wireless networks. Thus, to achieve efficiency in wireless networks, we study a special virtual backbone (VB) using directional antennas, requiring that from one node to any other node in the network, there exists at least one directional shortest path all of whose intermediate directions should belong to the VB, named as Minimum rOuting Cost Directional VB (MOC-DVB). In addition, VB has been well studied in Unit Disk Graph (UDG). However, radio wave based communications in wireless networks may be interrupted by obstacles (e.g., buildings and mountains). Thus, in this paper, we model a network as a general directed graph. We prove that construction of a minimum MOC-DVB is an NP-hard problem in a general directed graph and in term of the size of MOC-DVB, there exists an unreachable lower bound of the polynomial-time selected MOC-DVB. Therefore, we propose a distributed approximation algorithm for constructing MOC-DVB with approximation ratio of 1 + ln K + 2ln \u03b4D, where K is the number of antennas on each node and \u03b4D is the maximum direction degree in the network. Extensive simulations demonstrate that our constructed MOC-DVB is much more efficient in the sense of MOC-DVB size and routing cost compared to other VBs.\"",
        "Document: \"Effect of localized optimal clustering for reader anti-collision in RFID networks: fairness aspects to the readers. This paper proposes an adaptive and dynamic localized scheme unique to hierarchical clustering in RFID networks, while reducing the overlapping areas of clusters and consequently reducing collisions among RFID readers. Drew on our LLC scheme that adjusts cluster coverage to minimize energy consumption, low-energy localized clustering for RFID networks (LLCR) addresses RFID reader anti-collision problem in this paper. LLCR is a RFID reader anti-collision algorithm that minimizes collisions by minimizing overlapping areas of clusters that each RFID reader covers. LLCR takes into account each RFID reader's energy state as well as RFID reader collisions. For the energy state factor, we distinguish homogeneous RFID networks from heterogeneous ones according to computing power of each RFID reader. Therefore, we have designed efficient homo-LLCR and hetero-LLCR schemes for each case. Our simulation-based performance evaluation shows that LLCR minimizes energy consumption and overlapping areas of clusters of RFID readers.\"",
        "1 is \"NeXt generation/dynamic spectrum access/cognitive radio wireless networks: a survey\", 2 is \"Efficient and Effective Clustering Methods for Spatial Data Mining\"",
        "Given above information, for an author who has written the paper with the title \"QoS-adaptive bandwidth scheduling in continuous media streaming\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002224": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of '3d Medical Image Segmentation Approach Based On Multi-Label Front Propagation':",
        "Document: \"Video Denoising and Simplification Via Discrete Regularization on Graphs. In this paper, we present local and nonlocal algorithms for video denoising and simplification based on discrete regularization on graphs. The main difference between video and image denoising is the temporal redundancy in video sequences. Recent works in the literature showed that motion compensation is counter-productive for video denoising. Our algorithms do not require any motion estimation. In this paper, we consider a video sequence as a volume and not as a sequence of frames. Hence, we combine the contribution of temporal and spatial redundancies in order to obtain high quality results for videos. To enhance the denoising quality, we develop a robust method that benefits from local and nonlocal regularities within the video. We propose an optimized method that is faster than the nonlocal approach, while producing equally attractive results. The experimental results show the efficiency of our algorithms in terms of both Peak Signal to Noise Ratio and subjective visual quality.\"",
        "Document: \"Fast And Simple Discrete Approach For Active Contours For Biomedical Applications. In this paper, we present a fast and simple discrete approach for active contours. It is based on discrete contour evolution, which operates on the boundary of digital shape, by iterative growth processes on the boundary of the shape. We consider a curve to be the boundary of a discrete shape, We attach at each point of the boundary a cost function and deform this shape according to that cost function. The method presents some advantages. It is a discrete method, which takes an implicit representation and uses discrete algorithm with a simple data structure.\"",
        "Document: \"On the Mean Curvature Flow on Graphs with Applications in Image and Manifold Processing. In this paper, we propose an adaptation and transcription of the mean curvature level set equation on a general discrete domain (weighted graphs with arbitrary topology). We introduce the perimeters on graph using difference operators and define the curvature as the first variation of these perimeters. Our proposed approach of mean curvature unifies both local and non local notions of mean curvature on Euclidean domains. Furthermore, it allows the extension to the processing of manifolds and data which can be represented by graphs.\"",
        "Document: \"Solving Minimal Surface Problems on Surfaces and Point Clouds. Minimal surface problems play an important role not only in physics or biology but also in mathematical signal and image processing. Although the computation of respective solutions is well-investigated in the setting of discrete images, only little attention has been payed to more complicated data, e.g., surfaces represented as meshes or point clouds. In this work we introduce a novel family of discrete total variation seminorms for weighted graphs based on the upwind gradient and incorporate them into an efficient minimization algorithm to perform total variation denoising on graphs. Furthermore, we demonstrate how to utilize the latter algorithm to uniquely solve minimal surface problems on graphs. To show the universal applicability of this approach, we illustrate results from filtering and segmentation of 3D point cloud data.\"",
        "1 is \"Generalizing the hough transform to detect arbitrary shapes\", 2 is \"Linear Time Euclidean Distance Algorithms\"",
        "Given above information, for an author who has written the paper with the title \"3d Medical Image Segmentation Approach Based On Multi-Label Front Propagation\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002229": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Action Representation and Purpose: Re-evaluating the Foundations of Computational Vision':",
        "Document: \"Robot navigation using an anthropomorphic visual sensor. The use of an anthropomorphic, retinalike visual sensor for navigation tasks is investigated. The main advantage, besides the topological scaling and rotation invariance, stems from the considerable data reduction obtained with nonuniform sampling, in conjunction with high resolution in the part of the field of view corresponding to the focus of attention. Active movements are also considered to be a beneficial feature, solving the depth-from-motion problem and maintaining a 3-D representation of the viewed scene. For short range navigation, a tracking egomotion strategy is adopted which greatly simplifies the motion equations and complements the characteristics of the retinal sensor. An algorithm for the computation of depth from motion is developed for image sequences acquired with the retinal sensor, and an error analysis is carried out to determine the uncertainty of range measurements. An experiment is presented in which depth maps are computed from a sequence of images sampled with the retinalike sensor, building a volumetric representation of the scene\"",
        "Document: \"Oculo-motor stabilization reflexes: integration of inertial and visual information. STABILIZATION OF GAZE IS A FUNDAMENTAL REQUIREMENT OF AN ACTIVE VISUAL SYSTEM FOR AT LEAST TWO REASONS: (i) to increase the robustness of dynamic visual measures during observer's motion; (ii) to provide a reference with respect to the environment ([Ballard and Brown, 1992]). The aim of this paper is to address the former issue by investigating the role of integration of visuo-inertial information in gaze stabilization. The rationale comes from observations of how the stabilization problem is solved in biological systems and experimental results based on an artificial visual system equipped with space-variant visual sensors and an inertial sensor are presented. In particular the following issues are discussed: (i) the relations between eye-head geometry, fixation distance and stabilization performance; (ii) the computational requirements of the visuo-inertial stabilization approach compared to a visual stabilization approach; (iii) the evaluation of performance of the visuo-inertial strategy in a real-time monocular stabilization task. Experiments are performed to quantitatively describe the performance of the system with respect to different choices of the principal parameters. The results show that the integrated approach is indeed valuable: it makes use of visual computational resources more efficiently, extends the range of motions or external disturbances the system can effectively deal with, and reduces system complexity.\"",
        "Document: \"Analysis of object motion and camera motion in real scenes. In this paper a contour based motion estimation technique is presented applied to the measurement of the optic-flow field from image sequences. In particular two experimental situations are considered: the former, in which a fixed camera analyzes a scene with some moving objects in order to detect their trajectories; the latter, in which a sensor moves in a static environment, determining the depth map of the \"world\". For the first case, we compute for every contour the prevalent direction of movement. For the second, we solve the problem of recovering the 3D structure of the worldby computing the depth field from a sequence of images acquired during a motion of the camera, controlled in order to mantain the fixation point (i.e. the point projected over the image plane) still during the entire sequence. The algorithm has proved to be very robust in real scene analysis and an acceptable time for computation is required. Some experimental results are presented.\"",
        "Document: \"Factors Affecting the Accuracy of an Active Vision Head. In any measuring system the categorization of the error generation factors leads to simplification of complex error problems and to higher suppression of the error. In this paper we categorize, quantify and analyze the errors that affect a binocular active vision head. Simulations have been made and experimental results on a high resolution pan-tilt-vergence mechanism are also proposed. As a conclusion it can be said that the system performs optimal when it is initialized so that the two cameras are perfectly aligned and perpendicular to the baseline. Small variations in the vergence angle or small horizontal deviations of the principal point alters the measurement dramatically. On the other hand, variations in pan and tilt and vertical deviations of the principal point, affect the measurement insignificantly.\"",
        "1 is \"Novel Views of Objects from a Single Image.\", 2 is \"Torque-controlled Light Weight Arms and Articulated Hands - Do We Reach Technological Limits Now?\"",
        "Given above information, for an author who has written the paper with the title \"Action Representation and Purpose: Re-evaluating the Foundations of Computational Vision\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002422": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A Minimax Lower Bound for Empirical Quantizer Design':",
        "Document: \"The On-Line Shortest Path Problem Under Partial Monitoring. The on-line shortest path problem is considered under various models of partial monitoring. Given a weighted directed acyclic graph whose edge weights can change in an arbitrary (adversarial) way, a decision maker has to choose in each round of a game a path between two distinguished vertices such that the loss of the chosen path (defined as the sum of the weights of its composing edges) be as small as possible. In a setting generalizing the multi-armed bandit problem, after choosing a path, the decision maker learns only the weights of those edges that belong to the chosen path. For this problem, an algorithm is given whose average cumulative loss in n rounds exceeds that of the best path, matched off-line to the entire sequence of the edge weights, by a quantity that is proportional to 1/\u221an and depends only polynomially on the number of edges of the graph. The algorithm can be implemented with complexity that is linear in the number of rounds n (i.e., the average complexity per round is constant) and in the number of edges. An extension to the so-called label efficient setting is also given, in which the decision maker is informed about the weights of the edges corresponding to the chosen path at a total of m \u226a n time instances. Another extension is shown where the decision maker competes against a time-varying path, a generalization of the problem of tracking the best expert. A version of the multi-armed bandit setting for shortest path is also discussed where the decision maker learns only the total weight of the chosen path but not the weights of the individual edges on the path. Applications to routing in packet switched networks along with simulation results are also presented.\"",
        "Document: \"Efficient Tracking of Large Classes of Experts. In the framework of prediction of individual sequences, sequential prediction methods are to be constructed that perform nearly as well as the best expert from a given class. We consider prediction strategies that compete with the class of switching strategies that can segment a given sequence into several blocks, and follow the advice of a different \u201cbase\u201d expert in each block. As usual, the performance of the algorithm is measured by the regret defined as the excess loss relative to the best switching strategy selected in hindsight for the particular sequence to be predicted. In this paper, we construct prediction strategies of low computational cost for the case where the set of base experts is large. In particular, we provide a method that can transform any prediction algorithm $ {\\\\cal A}$ that is designed for the base class into a tracking algorithm. The resulting tracking algorithm can take advantage of the prediction performance and potential computational efficiency of $ {\\\\cal A}$ in the sense that it can be implemented with time and space complexity only $O(n^{\\\\gamma } \\\\ln n)$ times larger than that of $ {\\\\cal A}$, where $n$ is the time horizon and $\\\\gamma \\\\geq 0$ is a parameter of the algorithm. With $ {\\\\cal A}$ properly chosen, our algorithm achieves a regret bound of optimal order for $\\\\gamma 0$, and only $O(\\\\ln n)$ times larger than the optimal order for $\\\\gamma =0$ for all typical regret bound types we examined. For example, for predicting binary sequences with switching parameters under the logarithmic loss, our method achieves the optimal $O(\\\\ln n)$ regret rate with time complexity $O(n^{1+\\\\gamma }\\\\ln n)$ for any $\\\\gamma \\\\in (0,1)$.\"",
        "Document: \"Random-Coding Lower Bounds for the Error Exponent of Joint Quantization and Watermarking Systems. We establish random-coding lower bounds to the error exponent of discrete and Gaussian joint quantization and private watermarking systems. In the discrete system, both the covertext and the attack channel are memoryless and have finite alphabets. In the Gaussian system, the covertext is memoryless Gaussian and the attack channel has additive memoryless Gaussian noise. In both cases, our bounds on the error exponent are positive in the interior of the achievable quantization and watermarking rate region.\"",
        "Document: \"Entropy Density and Mismatch in High-Rate Scalar Quantization With R\u00e9nyi Entropy Constraint. Properties of scalar quantization with $r$th power distortion and constrained R\u00e9nyi entropy of order $\\\\alpha\\\\in (0,1)$ are investigated. For an asymptotically (high-rate) optimal sequence of quantizers, the contribution to the R\u00e9nyi entropy due to source values in a fixed interval is identified in terms of the \u201centropy density\u201d of the quantizer sequence. This extends results related to the well-known point density concept in optimal fixed-rate quantization. A dual of the entropy density result quantifies the distortion contribution of a given interval to the overall distortion. The distortion loss resulting from a mismatch of source densities in the design of an asymptotically optimal sequence of quantizers is also determined. This extends Bucklew's fixed-rate $(\\\\alpha=0)$ and Gray 's variable-rate $(\\\\alpha=1)$ mismatch results to general values of the entropy order parameter $\\\\alpha$.\"",
        "1 is \"Learning the Kernel Matrix with Semidefinite Programming\", 2 is \"On the minimum description length principle for sources with piecewise constant parameters\"",
        "Given above information, for an author who has written the paper with the title \"A Minimax Lower Bound for Empirical Quantizer Design\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002532": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Collusion-Free Multiparty Computation in the Mediated Model':",
        "Document: \"Reliable accounting in grids. Grid computing is a distributed environment in which a remote service is provided by a resource owner to a client by means of a grid infrastructure. One of the major expectations for grid computing is about the rising of a market where users pay to access the computational and storage capacity offered by a resource owner. In this scenario, all the steps of the economic transaction related to the fulfilment of a service are accomplished with the mediation of the grid infrastructure. Several economic models have been proposed for determining how to charge the services offered through a grid. In this paper, we outline one important security issue that may arise in models where services are priced according to the amount of resources they consume. Our contribution is to propose a new security model where secure grid transactions are possible even when resource owners and clients are corrupted.\"",
        "Document: \"Constant-Round Resettable Zero Knowledge with Concurrent Soundness in the Bare Public-Key Model. In the bare public-key model (BPK in short), each verifier is assumed to have deposited a public key in a file that is accessible by all users at all times. In this model, introduced by Canetti et al. [STOC 2000], constant-round black-box concurrent and resettable zero knowledge is possible as opposed to the standard model for zero knowledge. As pointed out by Micali and Reyzin [Crypto 2001], the notion of soundness in this model is more subtle and complex than in the classical model and indeed four distinct notions have been introduced (from weakest to strongest): one-time, sequential, concurrent and resettable soundness. In this paper we present the first constant-round concurrently sound resettable zero-knowledge argument system in the bare public-key model for NP. More specifically, we present a 4-round protocol, which is optimal as far as the number of rounds is concerned. Our result solves the main open problem on resettable zero knowledge in the BPK model and improves the previous works of Micali and Reyzin [EuroCrypt 2001] and Zhao et al. [EuroCrypt 2003] since they achieved concurrent soundness in stronger models.\"",
        "Document: \"An anonymous credential system and a privacy-aware PKI. In this paper we present a non-transferable anonymous credential system that is based on the concept of a chameleon certificate. A chameleon certificate is a special certificate that enjoys two interesting properties. Firstly, the owner can choose which attributes of the certificate to disclose. Moreover, a chameleon certificate is multi-show in the sense that several uses of the same chameleon certificate by the same user cannot be linked together. We adopt the framework of Brands [2] and our construction improves the results of Camenisch et al. [5] and Verheul [16] since it allows the owner of a certificate to prove general statements on the attributes encoded in the certificate and our certificates enjoy the multi-show property.\"",
        "Document: \"On constant-round concurrent non-malleable proof systems. Security under man-in-the-middle attacks is extremely important when protocols are executed on asynchronous networks, as the Internet. Focusing on interactive proof systems, one would like also to achieve unconditional soundness, so that proving a false statement is not possible even for a computationally unbounded adversarial prover. Motivated by such requirements, in this paper we address the problem of designing constant-round protocols in the plain model that enjoy simultaneously non-malleability (i.e., security against man-in-the-middle attacks) and unconditional soundness (i.e., they are proof systems). We first give a construction of a constant-round one-many (i.e., one honest prover, many honest verifiers) concurrent non-malleable zero-knowledge proof (in contrast to argument) system for every NP language in the plain model. We then give a construction of a constant-round concurrent non-malleable witness-indistinguishable proof system for every NP language. Compared with previous results, our constructions are the first constant-round proof systems that in the plain model guarantee simultaneously security against some non-trivial concurrent man-in-the-middle attacks and against unbounded malicious provers.\"",
        "1 is \"Hop-distance relationship analysis with quasi-UDG model for node localization in wireless sensor networks.\", 2 is \"A \u201cparadoxical\u201d identity-based signature scheme resulting from zero-knowledge\"",
        "Given above information, for an author who has written the paper with the title \"Collusion-Free Multiparty Computation in the Mediated Model\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002568": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Smoke Screener or Straight Shooter: Detecting Elite Sybil Attacks in User-Review Social Networks.':",
        "Document: \"Sybil Attacks and Their Defenses in the Internet of Things. The emerging Internet-of-Things (IoT) are vulnerable to Sybil attacks where attackers can manipulate fake identities or abuse pseudoidentities to compromise the effectiveness of the IoT and even disseminate spam. In this paper, we survey Sybil attacks and defense schemes in IoT. Specifically, we first define three types Sybil attacks: SA-1, SA-2, and SA-3 according to the Sybil attacker's capabilities. We then present some Sybil defense schemes, including social graph-based Sybil detection (SGSD), behavior classification-based Sybil detection (BCSD), and mobile Sybil detection with the comprehensive comparisons. Finally, we discuss the challenging research issues and future directions for Sybil defense in IoT.\"",
        "Document: \"Modelling Cumulus Cloud Shape from a Single Image. AbstractClouds are important components of the fascinating natural images. However, extracting cloud shapes from images remains a challenging task. This paper presents a calculation method for estimating the shape of a cumulus cloud from a single image suitable for flight simulations and games. The shape of the cloud is assumed to be symmetric. Based on this assumption, the intensities of pixels are correlated with the geometry of a cloud's surface via a simplified single scattering model. A propagation scheme is designed to derive the surface progressively, and mesh editing techniques are used to improve the surface. Finally, the cloud is represented by a particle system. The results show that the proposed method can generate realistic cumulus clouds that are similar to those found in the images in terms of the shape distribution.\"",
        "Document: \"Flexible editing of style, identity and content of human motion. Many prior works aim to provide style editing approaches which are intuitive and straightforward, such as [Min et al. 2010] etc. But they did not consider the content editing, like generate a running motion for an actor given his walking motion. For the purpose of style editing, most researches use linear model, such as PCA, ICA and multilinear model [Min et al. 2010]. However, the human motion is actually a highly nonlinear model [Elgammal and Lee 2004]. But the nonlinear model can not construct a direct mapping which makes the reconstruction of high-dimensional data complex. Besides, it can only manipulate data in the training database. In addition, when extracting the style of motion, most methods do not separate the content before learning the style. This confuses the content and style when doing the style learning, and therefore impairs the final result.\"",
        "Document: \"Secure and quality of service assurance scheduling scheme for WBAN with application to eHealth. Wireless Body Area Network (WBAN) is gaining popularity due to its large scale of applications in eHealth. Due to its critical and real-time nature, eHealth care system must provide security, privacy, and quality of service (QoS) support, in order to provide an efficient, valuable and fully reliable assistance to patients. This paper studies packet scheduling schemes for real-time transmission in WBAN with proper security and privacy. Real-time and non real-time traffic are classified to minimize the waiting time of the eHealth application's data traffic. An efficient secure data transmission scheme in WBAN is proposed with data integrity. The scheme is user-centric and the secure key is shared among all sensors in a WBAN to minimize any additional memory and processing power requirements. Security analysis and numerical results demonstrate that our scheme can minimize the mean waiting time of a real-time traffic in WBAN and provide proper security and privacy.\"",
        "1 is \"A run-length coding based approach to stroke extraction of Chinese characters\", 2 is \"A new approach to service provisioning in ATM networks\"",
        "Given above information, for an author who has written the paper with the title \"Smoke Screener or Straight Shooter: Detecting Elite Sybil Attacks in User-Review Social Networks.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002614": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Improved F0 modeling and generation in voice conversion':",
        "Document: \"Visualization of pronunciation diversity of world Englishes from a speaker's self-centered viewpoint. English is the only language available for global communication and is known to have a large diversity of pronunciations due to the influence of speakers' mother tongue, called accents. Our previous studies [1], [2] made an attempt to do speaker-basis clustering of those pronunciations, where every speaker was assumed to speak with his own accent. The clustering procedure required a distance matrix only in terms of pronunciation differences among speakers and [1], [2] proposed a method to predict the pronunciation distance between any pair of the speakers. A distance matrix is often visualized on a two-dimensional plane by using the Multi-Dimensional Scaling (MDS) or drawing a dendrogram. In this study, considering learners' perceptual characteristics, a new method is proposed for visualization. When a visualization result is fed back to a learner, his main interest will be in the relations from himself to the others, not those among the others. Then, by using only a part of the distance matrix and other kinds of information such as age and gender, the proposed method can visualize multiple kinds of diversity found in acoustics of English pronunciation from a speaker's self-centered viewpoint. Unlike the conventional methods, our proposal is guaranteed to cause no distortion at all in results of visualization.\"",
        "Document: \"Dialect-based speaker classification using speaker-invariant dialect features. In our previous works, a structural pronunciation representation was proposed to extract the linguistic features from dialect pronunciation and classify speakers based on their dialects. In this paper, in order to prove that the structural method can extract the purely speaker-invariant dialectal features, several new experiments are carried out. First, using the data of 19 speakers from different dialect and sub-dialect regions, a dialect-based speaker classification experiment is carried out and satisfactory result is achieved. Then, one Chinese dialectologist transcribes all the data and reads the linguistic content of each original utterance in her voice through looking at the transcript and listening to the original utterance. So a new data set with minimum speaker differences (fixed speaker identity) is created. Using the new data, similar classification experiment is carried out and the result is very similar to the result of last experiment. It means that our method can extract the purely speaker-invariant dialectal features and classify speakers based on their dialects very well. After that, for the original and mimicked data sets, data sets with maximum speaker differences are simulated using high-quality voice morphing techniques. Using the original dialect data and the simulated versions together, classification experiments are carried out based two criteria, spectral comparison and structural comparison. By comparing these results, we can find that unlike the method of spectral comparison, the structural method can purely classify speakers based on their dialects, which shows the proposed dialect structures are speaker-independent and linguistic enough features.\"",
        "Document: \"Leveraging phonetic context dependent invariant structure for continuous speech recognition. Speech acoustics intrinsically vary due to linguistic and non-linguistic factors. The invariant structure extracted from a given utterance is one of the long-span acoustic representations, where acoustic variation caused by non-linguistic factors can be removed reasonably. It expresses spectral contrasts between acoustic events in an utterance. In previous studies, the invariant structure was leveraged in continuous speech recognition for reranking the N-best candidates hypothesized by a traditional automatic speech recognition (ASR) system. Use of the invariant structure features for reranking showed good effects, however, the features were defined or labeled in a phonetic-context-independent way. In this paper, use of phonetic context to define invariant structure features is examined. The proposed method is tested in two tasks of continuous digits speech recognition and large vocabulary continuous speech recognition (LVCSR). The performances are improved relatively by 4.7% and 1.2%, respectively.\"",
        "Document: \"Pronunciation assessment based upon the phonological distortions observed in language learners' utterances. Abstract Speech representation provided by acoustic phonetics, spectro- gram, is very noisy representation in that it shows every acoustic aspect of speech. Age, gender, size, shape, microphone, room and line are completely irrelevant to speech recognition, pro- nunciation assessment, and so on. But the spectrogram is af- fected easily by these factors. This is the very essential reason why,speech systems are sometimes,unreliable and the author supposes,that the education should not endure this inevitable characteristics. The author proposed,a novel method,of acous- tic representation of speech where no dimensions of the above factors exist. The method,was derived by implementing,struc- tural phonology,on physics. This paper examines,whether the new,representation of speech can provide a good tool of pro- nunciation assessment. Results of the experiments,with good andintentionally-badpronunciationsofa,singlespeakershowed that all the students are acoustically located between,the two pronunciations, indicating that all the students are judged to be acoustically closer to the speaker than the speaker himself is. This result shows that the proposed,method,can delete the irrel- evant factors and is extremely reliable and effective in CALL.\"",
        "1 is \"KL-divergence regularized deep neural network adaptation for improved large vocabulary speech recognition\", 2 is \"Conditional Generative Adversarial Nets.\"",
        "Given above information, for an author who has written the paper with the title \"Improved F0 modeling and generation in voice conversion\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002662": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Mining unconnected patterns in workflows':",
        "Document: \"Mining hierarchies of models: from abstract views to concrete specifications. Process mining techniques have been receiving great attention in the literature for their ability to automatically support process (re)design. The output of these techniques is a concrete workflow schema that models all the possible execution scenarios registered in the logs, and that can be profitably used to support further-coming enactments. In this paper, we face process mining in a slightly different perspective. Indeed, we propose an approach to process mining that combines novel discovery strategies with abstraction methods, with the aim of producing hierarchical views of the process that satisfactorily capture its behavior at different level of details. Therefore, at the highest level of detail, the mined model can support the design of concrete workflows; at lower levels of detail, the views can be used in advanced business process platforms to support monitoring and analysis. Our approach consists of several algorithms which have been integrated into a systems architecture whose description is accounted for in the paper as well.\"",
        "Document: \"Decomposing combinatorial auctions and set packing problems. Combinatorial auctions allow bidders to bid on bundles of items rather than just on single items. The winner determination problem in combinatorial auctions is the problem of determining the allocation of items to bidders such that the sum of the accepted bid prices is maximized. This problem is equivalent to the well-known maximum-weight set packing problem. Even though these problems are NP-hard in general, they can be solved in polynomial time on instances whose associated item graphs have bounded treewidth (called structured item graphs). However, the tractability of determining whether for a given problem instance a structured item graph of fixed treewidth exists (and if so, computing one efficiently) was an open problem. In this article, we solve this problem by proving that deciding the existence of structured item graphs is computationally intractable, even for treewidth 3. Motivated by this unfavorable complexity result, we investigate other structural restrictions, and we show that the notion of hypertree decomposition, a well-studied measure of hypergraph cyclicity, turns out to be most useful here. Indeed, we show that the winner determination problem is solvable in polynomial time on instances whose dual auction hypergraphs have bounded hypertree width. Our solution method is based on encoding winner determination via a constraint satisfaction optimization problem and on exhibiting an algorithm to solve this latter problem efficiently for such structurally restricted instances. The class of tractable instances identified by our approach, while being efficiently recognizable, properly contains the class of instances having a structured item graph. Moreover, on the larger class, our method solves winner determination with the same asymptotic complexity as the best algorithm proposed in the literature for the subclass of structured item graphs. Hypertree decompositions can equally profitably be applied to the maximum-weight independent set problem, which is the dual problem of maximum-weight set packing.\"",
        "Document: \"Group Reasoning in Social Environments. While modeling group decision making scenarios, the existence of a central authority is often assumed which is in charge of amalgamating the preferences of a given set of agents with the aim of computing a socially desirable outcome, for instance, maximizing the utilitarian or the egalitarian social welfare. Departing from this classical perspective and inspired by the growing body of literature on opinion formation and diffusion, a setting for group decision making is studied where agents are selfishly interested and where each of them can adopt her own decision without a central coordination, hence possibly disagreeing with the decision taken by some of the other agents. In particular, it is assumed that agents belong to a social environment and that their preferences on the available alternatives can be influenced by the number of \\\"neighbors\\\" agreeing/disagreeing with them. The setting is formalized and studied by modeling agents' reasoning capabilities in terms of weighted propositional logics and by focusing on Nash-stable solutions as the prototypical solution concept. In particular, a thoroughly computational complexity analysis is conducted on the problem of deciding the existence of such stable outcomes. Moreover, for the classes of environments where stability is always guaranteed, the convergence of Nash dynamics consisting of sequences of best response updates is studied, too.\"",
        "Document: \"Tractable Optimization Problems through Hypergraph-Based Structural Restrictions. Several variants of the Constraint Satisfaction Problem have been proposed and investigated in the literature for modelling those scenarios where solutions are associated with some given costs. Within these frameworks computing an optimal solution is an NP-hard problem in general; yet, when restricted over classes of instances whose constraint interactions can be modelled via (nearly-)acyclic graphs, this problem is known to be solvable in polynomial time. In this paper, larger classes of tractable instances are singled out, by discussing solution approaches based on exploiting hypergraph acyclicity and, more generally, structural decomposition methods, such as (hyper)tree decompositions.\"",
        "1 is \"CloseGraph: mining closed frequent graph patterns\", 2 is \"Learning domain-independent string transformation weights for high accuracy object identification\"",
        "Given above information, for an author who has written the paper with the title \"Mining unconnected patterns in workflows\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002664": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Path-based systems to guide scientists in the maze of biological data sources.':",
        "Document: \"BiOnMap: a deductive approach for resource discovery. We present a deductive approach that supports resource discovery. The BiOnMap Web service is designed to support the selection of resources suitable to implement specific tasks. The BiOnMap service is comprised of a metadata catalog and a reasoning engine. The metadata catalog uses domain ontologies to annotate resources semantically and express domain rules that capture path equivalences at the level of the ontology graph. The BiOnMap reasoning engine is able to infer new properties of services to support service discovery, composition, and mapping. We illustrate our approach with an application case from the domain of bioinformatics.\"",
        "Document: \"Biological data integration: wrapping data and tools. Nowadays scientific data is inevitably digital and stored in a wide variety of formats in heterogeneous systems. Scientists need to access an integrated view of remote or local heterogeneous data sources with advanced data accessing, analyzing, and visualization tools. Building a digital library for scientific data requires accessing and manipulating data extracted from flat files or databases, documents retrieved from the Web as well as data generated by software. We present an approach to wrapping web data sources, databases, flat files, or data generated by tools through a database view mechanism. Generally, a wrapper has two tasks: it first sends a query to the source to retrieve data and, second builds the expected output with respect to the virtual structure. Our wrappers are composed of a retrieval component based on an intermediate object view mechanism called search views mapping the source capabilities to attributes, and an eXtensible Markup Language (XML) engine, respectively, to perform these two tasks. The originality of the approach consists of: 1) a generic view mechanism to access seamlessly data sources with limited capabilities and 2) the ability to wrap data sources as well as the useful specific tools they may provide. Our approach has been developed and demonstrated as part of the multidatabase system supporting queries via uniform object protocol model (OPM) interfaces.\"",
        "Document: \"Prune XML before you search it: XML transformations for query optimization. In this paper, we present a query optimization approach for XML document management environments loosely-coupled with the storage system Our technique is based on two main steps: first, based on the query, input documents are transformed, by pruning parts that are irrelevant with respect to the query; then the query is executed on the pruned documents An index structure is also provided to further optimize the pruning process Experimental results show that, by using our pruning strategy, query execution time can be significantly reduced.\"",
        "Document: \"A workflow for the prediction of the effects of residue substitution on protein stability. The effects of residue substitution in protein can be dramatic and predicting its impact may benefit scientists greatly. Like in many scientific domains there are various methods and tools available to address the potential impact of a mutation on the structure of a protein. The identification of these methods, their availability, the time needed to gain enough familiarity with them and their interface, and the difficulty of integrating their results in a global view where all view points can be visualized often limit their use. In this paper, we present the Structural Prediction for pRotein fOlding UTility System (SPROUTS) workflow and describe our method for designing, documenting, and maintaining the workflow. The focus of the workflow is the thermodynamic contribution to stability, which can be considered as acceptable for small proteins. It compiles the predictions from various sources calculating the \u0394\u0394G upon point mutation, together with a consensus from eight distinct algorithms, with a prediction of the mean number of interacting residues during the process of folding, and a sub domain structural analysis into fragments that may potentially be considered as autonomous folding units, i.e., with similar conformations alone and in the protein body. The workflow is implemented and available online. We illustrate its use with the analysis of the engrailed homeodomain (PDB code 1enh).\"",
        "1 is \"Structure and complexity of relational queries\", 2 is \"Using Semi-Joins to Solve Relational Queries\"",
        "Given above information, for an author who has written the paper with the title \"Path-based systems to guide scientists in the maze of biological data sources.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002672": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Performance measurement and analysis of E-mail cluster systems by using three IP load-balancing technologies':",
        "Document: \"Design of implementation of a compatible keyboard controller for keyboards and mice. By integration of the basic functions of both a standard keyboard controller and the key matrix of a scan code, we provide a compatible design for both keyboards and mice. The key matrix of a scan code consists of 16 output pins by 8 input pins, which can support up to 128 keys. Usually, we can build a matrix table based on the mapping relationship between the column and the row of a ROM/RAM. If we store the mapping relationship in the Table of an RAM chip, then we can re-download the table content at run time by means of the commands of either BIOS or the various applications. Hence, we can gain more flexibility and compatibility by means of firmware setting. We can also use the multiple sets of the matrix table pre-stored in a ROM chip. The multiple sets of the matrix table are selected by the command from BIOS. Although this selection method may require more ROM space, 128KB ROM can support a sufficient number of sets of the matrix table to thus provide a feasible solution for a design which is compatible for a keyboard controller.\"",
        "Document: \"Design and implementation of a socket with low standby power. Turned-off electric home appliances generally still require standby power when they are plugged in. In this paper we present a way to reduce the standby power of a socket. Our socket supplies the appliances with power when the user turns them on. When the user turns them off, our socket shuts the electric power off and reduces the standby power to zero. Our design, which uses a microcontroller unit (MCU), receives signals from a pyroelectric infrared (PIR) sensor which detects the user approaching the socket. A power detector provides an MCU to control the solid state relay (SSR) On/Off when used as an appliance switch for shutting off the standby power. The components we use are very inexpensive and consume only 0.2 W. The MCU monitoring program provides both automatic detection of the user by the PIR sensor and detection of power consumption.\"",
        "Document: \"An improved design of the embedded electronic measurement system. In this paper we integrate an embedded board with interface circuits in a design of an embedded electronic measurement system (EEMS) with a new human-machine touch panel interface. The EEMS includes general electronic measurement instruments such as a waveform generator, an oscilloscope and a power supply. We integrate three electronic measurement instrument interfaces into one window which works in cooperation with the touch panel. We also enhance the program by having the system fetch the touching position, so that it can do this continuously. We use and process the touching positions to improve aspects of the operation of the human-machine interface like the drawing and the shifting adjustment of the arbitrary waveform, so that the operation interface becomes more user-friendly. We also design a mean filter program which adds up and averages both the touch point and neighboring points, so that the arbitrary waveform can be outputted more smoothly.\"",
        "Document: \"Design and Implementation of Monitoring Software Modules for Optical Protection Systems. Because the optical network technique develops continuously, the nature of the test equipment used in the network will be more and more important, the remote monitor system is gaining importance for the operation of optical protection systems provide management and control of optical networks. In addition to the hardware module, the monitoring software module needed to be redesigned for automatically monitoring a group of optical modules of the hierarchy type. Our design provides a better function for the old product which can only monitor single type optical device, can't be reused, integrated and maintainable. Our design uses the network management (SNMP) interface which carries out remote supervision and use the RS232 interfaces to carry on local supervision. In addition, our design provides convenient in-time interface with the operational database functions, such as automatically the database data backup, security checking for data access and the hierarchical management scheme. Hence, our design can supervise and monitor a huge optical network system with 2500 objects, which will be continuously tested and the state of the operation situation recorded\"",
        "1 is \"Relaxation as a platform for cooperative answering\", 2 is \"Home energy management system based on power line communication\"",
        "Given above information, for an author who has written the paper with the title \"Performance measurement and analysis of E-mail cluster systems by using three IP load-balancing technologies\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002681": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Layout Analysis for Arabic Historical Document Images Using Machine Learning':",
        "Document: \"In-place 3D sketching for authoring and augmenting mechanical systems. We present a framework for authoring three-dimensional virtual scenes for Augmented Reality (AR) which is based on hand sketching. Sketches consisting of multiple components are used to construct a 3D virtual scene augmented on top of the real drawing. Model structure and properties can be modified by editing the sketch itself and printed content can be combined with hand sketches to form a single scene. Authoring by sketching opens up new forms of interaction that have not been previously explored in Augmented Reality. To demonstrate the technology, we implemented an application that constructs 3D AR scenes of mechanical systems from freehand sketches, and animates the scenes using a physics engine. We provide examples of scenes composed from trihedral solid models, forces, and springs. Finally, we describe how sketch interaction can be used to author complicated physics experiments in a natural way.\"",
        "Document: \"A carving framework for topology simplification of polygonal meshes. The topology of polygonal meshes has a large impact on the performance of various geometric processing algorithms, such as rendering and collision detection algorithms. Several approaches for simplifying topology have been discussed in the literature. These methods operate locally on models, which makes their effect on topology hard to predict and analyze. Most existing methods also tend to exhibit various disturbing artifacts, such as shrinking of the input and splitting of its components. We propose a novel top-down method for topology simplification that avoids the problems common in existing methods. The method starts with a simple, genus-zero mesh that bounds the input and gradually introduces topological features by a series of carving operations. Through this process a multiresolution stream of meshes is created with increasing topologic level of detail. Following the proposed approach, we present a practical carving algorithm that is based on the Constrained Delaunay Tetrahedralization (CDT). The algorithm pretetrahedralizes the complement of the input with respect to its convex hull and then eliminates tetrahedra in a prioritized manner. We present quality results for two families of meshes that are difficult to simplify by all existing methods known to us - topologically complex and highly clustered meshes.\"",
        "Document: \"Hierarchical On-line Arabic Handwriting Recognition. In this paper, we present a multi-level recognizer for online Arabic handwriting. In Arabic script (handwritten and printed), cursive writing \u2013 is not a style \u2013 it is an inherent part of the script. In addition, the connection between letters is done with almost no ligatures, which complicates segmenting a word into individual letters. In this work, we have adopted the holistic approach and avoided segmenting words into individual letters. To reduce the search space, we apply a series of filters in a hierarchical manner. The earlier filters perform light processing on a large number of candidates, and the later filters perform heavy processing on a small number of candidates. In the first filter, global features and delayed strokes patterns are used to reduce candidate word-part models. In the second filter, local features are used to guide a dynamic time warping (DTW) classification. The resulting k top ranked candidates are sent for shape context based classifier, which determines the recognized word-part. In this work, we have modified the classic DTW to enable different costs for the different operations and control their behavior. We have performed several experimental tests and have received encouraging results.\"",
        "Document: \"Optimized view-dependent rendering for large polygonal datasets. In this paper we are presenting a novel approach for rendering large datasets in a view-dependent manner. In a typical view-dependent rendering framework, an appropriate level of detail is selected and sent to the graphics hardware for rendering at each frame. In our approach, we have successfully managed to speed up the selection of the level of detail as well as the rendering of the selected levels. We have accelerated the selection of the appropriate level of detail by not scanning active nodes that do not contribute to the incremental update of the selected level of detail. Our idea is based on imposing a spatial subdivision over the view-dependence trees data-structure, which allows spatial tree cells to refine and merge in real-time rendering to comply with the changes in the active nodes list. The rendering of the selected level of detail is accelerated by using vertex arrays. To overcome the dynamic changes in the selected levels of detail we use multiple small vertex arrays whose sizes depend on the memory on the graphics hardware. These multiple vertex arrays are attached to the active cells of the spatial tree and represent the active nodes of these cells. These vertex arrays, which are sent to the graphics hardware at each frame, merge and split with respect to the changes in the cells of the spatial tree.\"",
        "1 is \"A Bayes-true data generator for evaluation of supervised and unsupervised learning methods\", 2 is \"Fr\u00e9chet Distance Based Approach for Searching Online Handwritten Documents\"",
        "Given above information, for an author who has written the paper with the title \"Layout Analysis for Arabic Historical Document Images Using Machine Learning\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002785": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Recent Developments in High Performance Computing for Remote Sensing: A Review':",
        "Document: \"Normal Endmember Spectral Unmixing Method for Hyperspectral Imagery. The normal compositional model (NCM) has been introduced to characterize mixed pixels in hyperspectral images, particularly when endmember variability needs to be considered in the unmixing process. Each pixel is modeled as a linear combination of endmembers, which are treated as Gaussian random variables in order to capture such spectral variability. Since the combination coefficients (i.e., abundances) and the endmembers are unknown variables at the same time in the NCM, the parameter estimation is more difficult in comparison with conventional approaches. In order to address this issue, we propose a new Bayesian method, termed normal endmember spectral unmixing (NESU), for improved parameter estimation in this context. It considers the endmembers as known variables (resulting from the extraction of endmember bundles), then performs optimal estimations of the remaining unknown parameters, i.e., the abundances, using Bayesian inference. The particle swarm optimization (PSO) technique is adopted to estimate the optimal values of abundances according to their posterior probabilities. The performance of the proposed algorithm is evaluated using both synthetic and real hyperspectral data. The obtained results demonstrate that the proposed method leads to significant improvements in terms of unmixing accuracies.\"",
        "Document: \"Dual-Mode FPGA Implementation of Target and Anomaly Detection Algorithms for Real-Time Hyperspectral Imaging. Target and anomaly detection are important techniques for remotely sensed hyperspectral data interpretation. Due to the high dimensionality of hyperspectral data and the large computational complexity associated to processing algorithms, developing fast techniques for target and anomaly detection has received considerable attention in recent years. Although several high-performance architectures have been evaluated for this purpose, field programmable gate arrays (FPGAs) offer the possibility of onboard hyperspectral data processing with low-power consumption, reconfigurability and radiation tolerance, which make FPGAs a relevant platform for hyperspectral processing. In this paper, we develop a novel FPGA-based technique for efficient target detection in hyperspectral images. The proposed method uses a streaming background statistics (SBS) approach for optimizing the constrained energy minimization (CEM) and Reed-Xiaoli (RX) algorithms, which are widely used techniques for target and anomaly detection, respectively. Specifically, these two algorithms are implemented in streaming fashion on FPGAs. Most importantly, we present a dual mode that implements a flexible datapath to decide in real time which one among these two algorithms should be used, thus allowing for the dynamic adaptation of the hardware to either target detection or anomaly detection scenarios. Our experiments, conducted with several well-known hyperspectral scenes, indicate the effectiveness of the proposed implementations.\"",
        "Document: \"Spectral Mixture Analysis of Hyperspectral Scenes Using Intelligently Selected Training Samples. In this letter, we address the use of artificial neural networks for spectral mixture analysis of hyperspectral scenes. We specifically focus on the issue of how to effectively train neural network architectures in the context of spectral mixture analysis applications. To address this issue, a multilayer perceptron neural architecture is combined with techniques for intelligent selection and labeling of training samples directly obtained from the input data, thus maximizing the information that can be obtained from those samples while reducing the need for a priori information about the scene. The proposed approach is compared to unconstrained and fully constrained linear mixture models using hyperspectral data sets acquired (in the laboratory) from artificial forest scenes, using the compact airborne spectrographic imaging system. The Spreading of Photons for Radiation INTerception (SPRINT) canopy model, which assumes detailed knowledge about object geometry, was employed to evaluate the results obtained by the different methods. Our results show that the proposed approach, when trained with both pure and mixed training samples (generated automatically without prior information) can provide similar results to those provided by SPRINT, using very few labeled training samples. An application to real airborne data using a set of hyperspectral images collected at different altitudes by the digital airborne imaging spectrometer 7915 and the reflective optics system imaging spectrometer, operating simultaneously at multiple spatial resolutions, is also presented and discussed.\"",
        "Document: \"Thin Cloud Removal Based on Signal Transmission Principles and Spectral Mixture Analysis. Cloud removal is an important goal for enhancing the utilization of optical remote sensing satellite images. Clouds dynamically affect the signal transmission due to their different shapes, heights, and distribution. In the case of thick opaque clouds, pixel replacement has been commonly adopted. For thin clouds, pixel correction techniques allow the effects of thin clouds to be removed while reta...\"",
        "1 is \"Tracking via object reflectance using a hyperspectral video camera\", 2 is \"Segmentation of microcalcifications in mammograms\"",
        "Given above information, for an author who has written the paper with the title \"Recent Developments in High Performance Computing for Remote Sensing: A Review\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002797": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Discriminative Nonnegative Spectral Clustering with Out-of-Sample Extension':",
        "Document: \"The Efficient Maintenance of Access Roles with Role Hiding. Role-based access control (RBAC) has attracted consid- erable research interest. However, the computational is- sues of RBAC models are yet to be thoroughly studied. In this paper, we study the problem of efficient maintenance of large RBAC models in a database-based multi-domain Web service environment. We propose first-order (SQL) algorithms to maintain the reachability of access roles un- der dynamic changes. The main advantages of our algo- rithms are: the support of various operations required for managing access roles with fractional information of roles; the maintenance of an update through operating a bounded number of join operations despite of the data size. To the best of our knowledge, our algorithms are the first attempt to maintain RBAC models using a first-order language.\"",
        "Document: \"The 1st International Workshop on Context-Aware Recommendation Systems with Big Data Analytics (CARS-BDA). With the explosive growth of online service platforms, increasing number of people and enterprises are doing everything online. In order for organizations, governments, and individuals to understand their users, and promote their products or services, it is necessary for them to analyse big data and recommend the media or online services in real time. Effective recommendation of items of interest to consumers has become critical for enterprises in domains such as retail, e-commerce, and online media. Driven by the business successes, academic research in this field has also been active for many years. Through many scientific breakthroughs have been achieved, there are still tremendous challenges in developing effective and scalable recommendation systems for real-world industrial applications. Existing solutions focus on recommending items based on pre-set contexts, such as time, location, weather etc. The big data sizes and complex contextual information add further challenges to the deployment of advanced recommender systems. This workshop aims to bring together researchers with wide-ranging backgrounds to identify important research questions, to exchange ideas from different research disciplines, and, more generally, to facilitate discussion and innovation in the area of context-aware recommender systems and big data analytics.\n\n\"",
        "Document: \"Enhancing text classification using synopses extraction. This paper describes a novel approach to document classification that uses decision-tree machine learning based on a succinct vector of important terms in each document. The succinct vector itself is generated by a machine-learning approach which builds parsers that can identify significant features in a document by partitioning it into regions based on low-level document characteristics. The fact that the feature vector is succinct overcomes the problem of very large term vectors, which have hindered the application of conventional machine learning to document classification. The fact that the parser can be trained to extract only important terms from documents means that small training sets can be used to achieve the same classification accuracy as with conventional approaches.\"",
        "Document: \"SoRank: incorporating social information into learning to rank models for recommendation. Most existing learning to rank based recommendation methods only use user-item preferences to rank items, while neglecting social relations among users. In this paper, we propose a novel, effective and efficient model, SoRank, by integrating social information among users into listwise ranking model to improve quality of ranked list of items. In addition, with linear complexity to the number of observed ratings, SoRank is able to scale to very large dataset. Experimental results on publicly available dataset demonstrate the effectiveness of SoRank.\"",
        "1 is \"Mining association rules between sets of items in large databases\", 2 is \"Automatic image tagging as a random walk with priors on the canonical correlation subspace\"",
        "Given above information, for an author who has written the paper with the title \"Discriminative Nonnegative Spectral Clustering with Out-of-Sample Extension\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002814": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Lifelong Learning and Collaboration of Smart Technical Systems in Open-Ended Environments -- Opportunistic Collaborative Interactive Learning':",
        "Document: \"So near and yet so far: New insight into properties of some well-known classifier paradigms. This article provides some new insight into the properties of four well-established classifier paradigms, namely support vector machines (SVM), classifiers based on mixture density models (CMM), fuzzy classifiers (FCL), and radial basis function neural networks (RBF). It will be shown that these classifiers can be formulated in a way such that they are functionally equivalent or at least highly similar. The interpretation of a specific classifier as being an SVM, CMM, FCL, or RBF then only depends on the objective function and the optimization algorithm used to adjust the parameters. The properties of these four paradigms, however, are very different: a discriminative classifier such as an SVM is expected to have optimal generalization capabilities on new data, a generative classifier such as a CMM also aims at modeling the processes from which the observed data originate, and a comprehensible classifier such as an FCL is intended to be parameterized and understood by human domain experts. We will discuss the advantages and disadvantages of these properties and show how they can be measured numerically in order to compare these classifiers. In such a way, the article aims at supporting a practitioner in assessing the properties of classifier paradigms and in selecting or combining certain paradigms for a given application problem.\"",
        "Document: \"The responsibility weighted Mahalanobis kernel for semi-supervised training of support vector machines for classification. \u2022The responsibility weighted Mahalanobis (RWM) kernel considers structure information in data with help of a parametric density model.\u2022It is perfectly suited for semi-supervised learning as the parameters of the density model can be found in an unsupervised way.\u2022For semi-supervised learning the RWM kernel outperforms some other kernel functions including the Laplacian kernel (Laplacian SVM). SVM with RWM kernels can be parameterized as easily as an SVM with standard RBF kernels, as known heuristics for the RBF kernel can be transferred to the new kernel.\u2022Standard training techniques such as SMO and standard implementations of SVM such as LIBSVM can be used with the RWM kernel without any algorithmic adjustments or extensions.\u2022Results are shown for 20 publicly available benchmark data sets.\"",
        "Document: \"Intrusion detection in computer networks with neural and fuzzy classifiers. With the rapidly increasing impact of the Internet, the development of appropriate intrusion detection systems (IDS) gains more and more importance. This article presents a performance comparison of four neural and fuzzy paradigms (multilayer perceptrons, radial basis function networks, NEFCLASS systems, and classifying fuzzy-k-means) applied to misuse detection on the basis of TCP and IP header information. As an example, four different attacks (Nmap, Portsweep, Dict, Back) will be detected utilising evaluation data provided by the Defense Advanced Research Projects Agency (DARPA). The best overall classification results (99.42%) can be achieved with radial basis function networks, which model hyperspherical clusters in the feature space.\"",
        "Document: \"Engineering and Mastering Interwoven Systems. Networked systems are becoming increasingly complex in development and operation. Due to this complexity, it is mostly impossible to follow a simple sequential designdeploy-use cycle. Instead, development and operation will become more evolutionary in nature. Additionally, one can observe that individual complex systems are coupled with each other, even though this has never been intended in the early development of these systems. As a result, we are facing interwoven systems ?? multiple open time-variant systems are coupled and interact having, e.g., different goals and objectives as well as changing system and communication structure. Based on and extending the idea of composing Systems of Systems, this article identifies challenges that are becoming increasingly apparent as the inevitable integration of systems progresses.\"",
        "1 is \"Technology as experience\", 2 is \"Learning with non-positive kernels\"",
        "Given above information, for an author who has written the paper with the title \"Lifelong Learning and Collaboration of Smart Technical Systems in Open-Ended Environments -- Opportunistic Collaborative Interactive Learning\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002819": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Using public displays to stimulate passive engagement, active engagement, and discovery in public spaces':",
        "Document: \"WMSC '16: second workshop on mobile and situated crowdsourcing. The proposed workshop seeks to build upon the success of previous workshops at UbiComp 2010 and 2011 on Ubiquitous Crowdsourcing, and UbiComp 2015 on Mobile and Situated Crowdsourcing. Increasingly, researchers and practitioners alike are turning towards crowdsourcing with ubiquitous technologies due to their affordances and potential to circumvent limitations with online crowdsourcing platforms. Hence, this workshop's main objectives are to investigate the current state of the art of mobile and situated crowdsourcing, and foster collaborations by bringing together researchers of this thriving research agenda.\"",
        "Document: \"Donating Context Data to Science: The Effects of Social Signals and Perceptions on Action-Taking. Abstract It is becoming increasingly easy for researchers to develop context-aware applications for smartphones. A perennial challenge, however, is to convince a large number of people to install them and donate contextual data for scientific purposes. Our empirical study seeks to address this challenge by investigating how people&#39;s perception and attitude affect their willingness to donate contex...\"",
        "Document: \"Kinship verification from facial images and videos: human versus machine. Automatic kinship verification from facial images is a relatively new and challenging research problem in computer vision. It consists in automatically determining whether two persons have a biological kin relation by examining their facial attributes. In this work, we compare the performance of humans and machines in kinship verification tasks. We investigate the state-of-the-art methods in automatic kinship verification from facial images, comparing their performance with the one obtained by asking humans to complete an equivalent task using a crowdsourcing system. Our results show that machines can consistently beat humans in kinship classification tasks in both images and videos. In addition, we study the limitations of currently available kinship databases and analyzing their possible impact in kinship verification experiment and this type of comparison.\"",
        "Document: \"This is not classified: everyday information seeking and encountering in smart urban spaces. We present a multipronged comparative study of citizens' self-proclaimed information needs and actual information seeking behavior in smart urban spaces. We first conducted several user studies to identify the types of information services that citizens believed to be useful in urban setting utilizing methods ranging from contextual inquiry with lo-fi prototypes to \"card sorting\" exercise with a separate set of participants, and finally to implementing selected services. We then made a sizeable constructive intervention into the urban space by deploying in a city center 12 large, interactive public displays called \"hotspots\" to offer a wide range of previously identified information services. We collected comprehensive qualitative and quantitative data on the usage of the hotspots and their services by the general public during 13\u807dmonths. Our study reveals discrepancies between a priori and a posteriori information seeking strategies extracted from the self-proclaimed information needs and the actual usage of the hotspots.\"",
        "1 is \"Second international workshop on ubiquitous crowdsourcing: towards a platform for crowd computing\", 2 is \"Lockr: social access control for web 2.0\"",
        "Given above information, for an author who has written the paper with the title \"Using public displays to stimulate passive engagement, active engagement, and discovery in public spaces\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002842": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Learning To Combine Multi-Resolution Spatially-Weighted Co-Occurrence Matrices For Image Representation':",
        "Document: \"Affective content analysis in comedy and horror videos by audio emotional event detection. We study the problem of affective content analysis. In this paper, we think of affective contents as those video/audio segments, which may cause an audience's strong reactions or special emotional experiences, such as laughing or fear. Those emotional factors are related to the users' attention, evaluation, and memories of the content. The modeling of affective effects depends on the video genres. In this work, we focus on comedy and horror films to extract the affective content by detecting a set of so-called audio emotional events (AEE) such as laughing, horror sounds, etc. Those AEE can be modeled by various audio processing techniques, and they can directly reflect an audience's emotion. We use the AEE as a clue to locate corresponding video segments. Domain knowledge is more or less employed at this stage. Our experimental dataset consists of 40-minutes comedy video and 40-minutes horror film. An average recall and precision of above 90% is achieved. It is shown that, in addition to rich visual information, an appropriate usage of special audios is an effective way to assist affective content analysis.\"",
        "Document: \"Image Retrieval Using Dominant Color Descriptor. In this paper, a new method used to calculate the similarity of Dominant Color Descriptor is discussed. Using Earth Mover's Distance (EMD), better retrieval results can be obtained compared with those obtained from the original MPEG-7 reference software (XM) [1]. In order to save the retrieval time, two different methods which can prune the images far from the query image are discussed. One is the lower bound of EMD, while the other is the M-tree index based on EMD distance. Experiments show that the lower bound is easier to implement and more efficient than the M-tree.\"",
        "Document: \"A Bayesian approach integrating regional and global features for image semantic learning. In content-based image retrieval, the \"semantic gap\" between visual image features and user semantics makes it hard to predict abstract image categories from low-level features. We present a hybrid system that integrates global features (G-features) and region features (R-features) for predicting image semantics. As an intermediary between image features and categories, we introduce the notion of mid-level concepts, which enables us to predict an image's category in three steps. First, a G-prediction system uses G-features to predict the probability of each category for an image. Simultaneously, a R-prediction system analyzes R-features to identify the probabilities of mid-level concepts in that image. Finally, our hybrid H-prediction system based on a Bayesian network reconciles the predictions from both R-prediction and G-prediction to produce the final classifications. Results of experimental validations show that this hybrid system outperforms both G-prediction and R-prediction significantly.\"",
        "Document: \"Salient region detection by jointly modeling distinctness and redundancy of image content. Salient region detection in images is a challenging task, despite its usefulness in many applications. By modeling an image as a collection of clusters, we design a unified clustering framework for salient region detection in this paper. In contrast to existing methods, this framework not only models content distinctness from the intrinsic properties of clusters, but also models content redundancy from the removed content during the retargeting process. The cluster saliency is initialized from both distinctness and redundancy and then propagated among different clusters by applying a clustering assumption between clusters and their saliency. The novel saliency propagation improves the robustness to clustering parameters as well as retargeting errors. The power of the proposed method is carefully verified on a standard dataset of 5000 real images with rectangle annotations as well as a subset with accurate contour annotations.\"",
        "1 is \"Local Fisher Discriminant Analysis for Pedestrian Re-identification\", 2 is \"Generalized search trees for database systems\"",
        "Given above information, for an author who has written the paper with the title \"Learning To Combine Multi-Resolution Spatially-Weighted Co-Occurrence Matrices For Image Representation\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002950": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'The Placing Task: A Large-Scale Geo-Estimation Challenge for Social-Media Videos and Images':",
        "Document: \"Hierarchical Filtered Motion for Action Recognition in Crowded Videos. ion recognition with cluttered and moving background is a challenging problem. One main difficulty lies in the fact that the motion field in an action region is contaminated by the background motions. We propose a hierarchical filtered motion (HFM) method to recognize actions in crowded videos by the use of motion history image (MHI) as basic representations of motion because of its robustness and efficiency. First, we detect interest points as the two-dimensional Harris corners with recent motion, e.g., locations with high intensities in the MHI. Then, a global spatial motion smoothing filter is applied to the gradients of the MHI to eliminate isolated unreliable or noisy motions. At each interest point, a local motion field filter is applied to the smoothed gradients of the MHI by computing structure proximity between any pixel in the local region and the interest point. Thus, the motion at a pixel is enhanced or weakened based on its structure proximity with the interest point. To validate its effectiveness, we characterize the spatial and temporal features by histograms of oriented gradient in the intensity image and the MHI, respectively, and use a Gaussian-mixture-model-based classifier for action recognition. The performance of the proposed approach achieves the state-of-the-art results on the KTH dataset that has clean background. More importantly, we perform cross-dataset action classification and detection experiments, where the KTH dataset is used for training, while the microsoft research (MSR) action dataset II that consists of crowded videos with people moving in the background is used for testing. Our experiments show that the proposed HFM method significantly outperforms existing techniques.\"",
        "Document: \"3D object reconstruction from a single 2D line drawing without hidden lines. The human vision system can interpret a single 2D line drawing as a 3D object without much difficulty even if the hidden lines of the object are invisible. Several reconstruction approaches have tried to emulate this ability, but they cannot recover the complete object if the hidden lines of the object are not shown. This paper proposes a novel approach for reconstructing complete 3D objects from line drawings without hidden lines. First, we develop some constraints and properties for the inference of the topology of the invisible edges and vertices of an object. Then we present a reconstruction method based on perceptual symmetry and planarity of the object. We give a number of examples to demonstrate the ability of our approach.\"",
        "Document: \"MemexQA: Visual Memex Question Answering. This paper proposes a new task, MemexQA: given a collection of photos or videos from a user, the goal is to automatically answer questions that help users recover their memory about events captured in the collection. Towards solving the task, we 1) present the MemexQA dataset, a large, realistic multimodal dataset consisting of real personal photos and crowd-sourced questions/answers, 2) propose MemexNet, a unified, end-to-end trainable network architecture for image, text and video question answering. Experimental results on the MemexQA dataset demonstrate that MemexNet outperforms strong baselines and yields the state-of-the-art on this novel and challenging task. The promising results on TextQA and VideoQA suggest MemexNetu0027s efficacy and scalability across various QA tasks.\"",
        "Document: \"Responses to the Comments on \u201cPlane-Based Optimization for 3D Object Reconstruction from Single Line Drawings\u201d. We disagree with the comments made by Varley [1] on our previous paper [2]. In this paper, we respond to his comments and show that they are not correct.\"",
        "1 is \"Mining Periodic Behavior in Dynamic Social Networks\", 2 is \"Joke-o-mat: browsing sitcoms punchline by punchline\"",
        "Given above information, for an author who has written the paper with the title \"The Placing Task: A Large-Scale Geo-Estimation Challenge for Social-Media Videos and Images\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002993": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Design of a Discrete-Time Fault-Tolerant Quantum Filter and Fault Detector':",
        "Document: \"Robust state estimation and model validation for discrete-time uncertain systems with a deterministic description of noise and uncertainty. The paper presents a new approach to robust state estimation for a class of uncertain discrete-time systems with a deterministic description of noise and uncertainty. The main result is a recursive scheme for constructing an ellipsoidal state estimation set of all states consistent with the measured output and the given noise and uncertainty description. The paper also includes a result on model validation whereby it can be determined if the assumed model is consistent with measured data.\"",
        "Document: \"Quantum filter for a non-Markovian single qubit system. In this paper, a quantum filter for estimating the states of a non-Markovian qubit system is presented in an augmented Markovian system framework including both the qubit system of interest and multi-ancillary systems for representing the internal modes of the non-Markovian environment. The colored noise generated by the multi-ancillary systems disturbs the qubit system via a direct interaction. The resulting non-Markovian dynamics of the qubit is determined by a memory kernel function arising from the dynamics of the ancillary system. In principle, colored noise with arbitrary power spectrum can be generated by a combination of Lorentzian noises. Hence, the quantum filter can be constructed for the qubit disturbed by arbitrary colored noise and the conditional state of the qubit system can be obtained by tracing out the multi-ancillary systems.\"",
        "Document: \"Decentralized coherent quantum control design for translation invariant linear quantum stochastic networks with direct coupling. This paper is concerned with coherent quantum control design for translation invariant networks of identical quantum stochastic systems subjected to external quantum noise. The network is modelled as an open quantum harmonic oscillator and is governed by a set of linear quantum stochastic differential equations. The dynamic variables of this quantum plant satisfy the canonical commutation relations. Similar large-scale systems can be found, for example, in quantum metamaterials and optical lattices. The problem under consideration is to design a stabilizing decentralized coherent quantum controller in the form of another translation invariant quantum system, directly coupled to the plant, so as to minimize a weighted mean square functional of the dynamic variables of the interconnected networks. We consider this problem in the thermodynamic limit of infinite network size and present first-order necessary conditions for optimality of the controller.\"",
        "Document: \"Robust smoothing for estimating optical phase varying as a continuous resonant process. Continuous phase estimation is known to be superior in accuracy as compared to static estimation. The estimation process is, however, desired to be made robust to uncertainties in the underlying parameters. Here, homodyne phase estimation of coherent and squeezed states of light, evolving continuously under the influence of a second-order resonant noise process, are made robust to parameter uncertainties using a robust fixed-interval smoother, designed for uncertain systems satisfying a certain integral quadratic constraint. We observe that such a robust smoother provides improved worst-case performance over the optimal smoother and also performs better than a robust filter for the uncertain system.\"",
        "1 is \"Randomized gossip algorithms\", 2 is \"An overview of subspace identification\"",
        "Given above information, for an author who has written the paper with the title \"Design of a Discrete-Time Fault-Tolerant Quantum Filter and Fault Detector\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003003": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'The role of electronic pocket dictionaries as an English learning tool among Chinese students':",
        "Document: \"Browsing of Large Video Collections on Personal Video Recorders with Remote Control Navigation Keys. Hard disk-based personal video recorders have the capacity to store large quantities of video contents. A personal video recorder serving one household may have to cater for the diverse entertainment tastes imposed by different family members. Timer event programmed video recordings have a tendency to accumulate and it is difficult to access large video collections on current generation personal video recorders. This is because the recordings often are presented linearly in a chronologically ordered list that is accessed using the navigation keys on a remote control. In this study an alternative strategy for accessing large video collections via the navigation keys of a remote control is investigated. The strategy automatically clusters the video recording using cuts that are perceptually meaningful to the viewers and that minimize the retrieval effort. However, the results suggest that navigation keys do not meet the needs of large video collection browsing.\"",
        "Document: \"Reflective Text Entry: A Simple Low Effort Predictive Input Method Based on Flexible Abbreviations. Users with reduced physical functioning such as ALS patients need more time and effort to operate computers. Most of the previous assistive technologies use prefix based predictive text input algorithms. Prefix based predictive text entry is suitable for languages such as English where the average word length is approximately 5 characters. Other languages such as Norwegian and German have longer mean word lengths as words are combined into longer compound words and prefix approaches are thus less effective. This paper proposes a new abbreviation expansion algorithm. Users mentally determine an abbreviation of the word, typically comprising significant consonants and the system proposes words that contain the matched characters. The approach is non disruptive in that it does not require the user to learn a new system or abbreviation mnemonics, and it can be used with any text input device. The system is dynamic and adapts to the users style of abbreviated input. The algorithm is easier to implement than previous approaches and no a priori system training is required. Our experimental evaluations demonstrate that the algorithm achieves real time performance with modest computer hardware.\"",
        "Document: \"An error tolerant memory aid for reduced cognitive load in number copying tasks. Number copying tasks are still common despite increased digitalization of services. Number copying tasks are cognitively and visually demanding, errors are easily introduced and the process is often perceived as laborious. This study proposes an alternative scheme based on dictionary coding that reduces the cognitive load on the user by a factor of five. The strategy has several levels of error detection and error correction characteristics and is easy to implement.\"",
        "Document: \"Fuzzy environment mapping for robot navigation based on grid computing. In order to navigate autonomously, a mobile robot needs to build an environment map where the robot is navigating. Currently, the sensors are mounted on the robot to detect if the obstacles exist and then the map immediate surrounding of the robot is built to help for navigation path planning. The map created by this method is a local map that may cause global navigation problem which a global coverage map is needed to solve such a problem. In this study, a sensor network is deployed for building global environment map. All the sensor locations are assumed known. The navigation space is divided into grids and a grid is to be detected if obstacles exist by one or a number of sensors. Fuzzy set concept is used to introduce a tool useful for sensor perception. Those sensors work as a team to explore all the space and then the global fuzzy map is constructed. The experiments show that the fuzzy map is more practical and helps the path planning problem to be solved more efficiently.\"",
        "1 is \"Ubikequitous computing: designing interactive experiences for cyclists\", 2 is \"Group cognition in computer-assisted collaborative learning\"",
        "Given above information, for an author who has written the paper with the title \"The role of electronic pocket dictionaries as an English learning tool among Chinese students\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003127": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A Neural Network Approach to Joint Modeling Social Networks and Mobile Trajectories.':",
        "Document: \"Inferring Correspondences from Multiple Sources for Microblog User Tags. Some microblog services encourage users to annotate themselves with multiple tags, indicating their attributes and interests. User tags play an important role for personalized recommendation and information retrieval. In order to better understand the semantics of user tags, we propose Tag Correspondence Model (TCM) to identify complex correspondences of tags from the rich context of microblog users. In TCM, we divide the context of a microblog user into various sources (such as short messages, user profile, and neighbors). With a collection of users with annotated tags, TCM can automatically learn the correspondences of user tags from the multiple sources. With the learned correspondences, we are able to interpret implicit semantics of tags. Moreover, for the users who have not annotated any tags, TCM can suggest tags according to users' context information. Extensive experiments on a real-world dataset demonstrate that our method can efficiently identify correspondences of tags, which may eventually represent semantic meanings of tags.\"",
        "Document: \"FewRel: A Large-Scale Supervised Few-shot Relation Classification Dataset with State-of-the-Art Evaluation. We present a Few-Shot Relation Classification Dataset (FewRel), consisting of 70, 000 sentences on 100 relations derived from Wikipedia and annotated by crowdworkers. The relation of each sentence is first recognized by distant supervision methods, and then filtered by crowdworkers. We adapt the most recent state-of-the-art few-shot learning methods for relation classification and conduct a thorough evaluation of these methods. Empirical results show that even the most competitive few-shot learning models struggle on this task, especially as compared with humans. We also show that a range of different reasoning skills are needed to solve our task. These results indicate that few-shot relation classification remains an open problem and still requires further research. Our detailed analysis points multiple directions for future research. All details and resources about the dataset and baselines are released on this http URL\"",
        "Document: \"Location-Sharing Systems With Enhanced Privacy in Mobile Online Social Networks. Location sharing is one of the critical components in mobile online social networks (mOSNs), which has attracted much attention recently. With the advent of mOSNs, more and more users' location information will be collected by the service providers in mOSN. However, the users' privacy, including location privacy and social network privacy, cannot be guaranteed in the previous work without the trust assumption on the service providers. In this paper, aiming at achieving enhanced privacy against the insider attack launched by the service providers in mOSNs, we introduce a new architecture with multiple location servers for the first time and propose a secure solution supporting location sharing among friends and strangers in location-based applications. In our construction, the user's friend set in each friends' query submitted to the location servers is divided into multiple subsets by the social network server randomly. Each location server can only get a subset of friends, instead of the whole friends' set of the user as the previous work. In addition, for the first time, we propose a location-sharing construction which provides checkability of the searching results returned from location servers in an efficient way. We also prove that the new construction is secure under the stronger security model with enhanced privacy. Finally, we provide extensive experimental results to demonstrate the efficiency of our proposed construction.\"",
        "Document: \"Text Classification Based on Transfer Learning and Self-Training. Traditional text classification methods make a basic assumption: the training and test set are homologous, while this naive assumption may not hold in the real world, especially in the Web environment. Documents on the Web change from time to time, pre-trained model may be out of date when applied to new emerging documents. However some information of training set is nonetheless useful. In this paper we proposed a novel method to discover the constant common knowledge in both training and test set by transfer learning, then a model is built based on this knowledge to fit the distribution in test set. The model is reinforced iteratively by adding most confident instances in unlabeled test set to training set until convergence, which is a self-training process, preliminary experiment shows that our method achieves an approximately 8.92% improvement as compared to the standard supervised-learning method.\"",
        "1 is \"Learning to Extract Relations from the Web using Minimal Supervision\", 2 is \"Path-following methods for linear programming\"",
        "Given above information, for an author who has written the paper with the title \"A Neural Network Approach to Joint Modeling Social Networks and Mobile Trajectories.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003137": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Design guidelines for negotiation support systems: an expert perspective using scenarios':",
        "Document: \"Effect of map sharing and confidence information in situation-map making. Motivation -- A situation map that shows the overview of a disaster situation serves as a valuable tool for disaster response teams. It helps them orientate their location and make disaster response decisions. It is, however, a quite complicated task to rapidly generate a comprehensive situation map of a disaster area. In this paper, we report on an investigation of how two persons can collaborate to make a situation map. Research approach -- We performed a controlled laboratory experiment, in which 32 participants (grouped into 16 pairs) made a situation map of incidents. The experiment was set up as a two-way repeated-measures design with the type of collaboration and the availability of confidence level information as within-subject factors. Findings/Design -- The results suggest that the collaboration type can affect the quality of the situation map. Additionally, the results also suggest that the availability of confidence information influences the discussion process during collaboration. The participants perceived the availability of confidence level information as being positive. Research limitations/Implications -- The order of using the types of collaboration might have caused a learning effect by participants. Furthermore, the lack of a practice session might have had an influence on participants' object recognition during the first session of the experiment. Originality/Value -- The study takes the position that the affected population in a disaster can actively participate in the situation-map making process. Take away message -- Situation map-making might benefit from a simple collaborative action such as sharing a map including confidence information.\"",
        "Document: \"HCI for technology enhanced learning. The involvement of technology to support and enhance learning is ever increasing; for example moving from the traditional blackboard to electronic whiteboards, from printed books to virtual reality training simulations, and from class room meetings to (a-)synchronised meeting over the web with handheld mobile devices. These technologies promise improved efficiency for traditional ways of learning or even to open up totally new ways of learning. Designing technology-enhanced learning that engages learners in successful learning strategies requires an understanding of the learning context, learners' needs, motivations, habits and desires as well as ease of use. This workshop invites researchers, designers, and educators to discuss their work in this area and explore how HCI practices and methods can be applied or should be extended.\"",
        "Document: \"Distributed collaborative situation-map making for disaster response. A situation map that shows the overview of a disaster situation serves as a valuable tool for disaster response teams. It helps them to orientate their location and to make disaster response decisions. It is, however, a complicated task to rapidly generate a complete and comprehensive situation map of a disaster area, particularly due to the centralized organization of disaster management and the ...\"",
        "Document: \"Social acceptance of negotiation support systems. We investigate people's attitudes towards the possible use of mobile negotiation support systems (NSS) in different social contexts and the consequences for their design. For that purpose we developed an online survey based on existing models of technology acceptance. In the questionnaire we showed five filmed scenarios of NSS use contexts. The data collected from 120 respondents, showed (a) that subjective norm is an important factor influencing the intention to use the system and (b) that the acceptance of NSS depends on the use context. Therefore, we argue that NSS should be designed not merely as tools being used in the actual negotiation but as social devices harnessing social networks to provide support in all negotiation phases.\"",
        "1 is \"Tribler: A Social-Based Peer-To-Peer System\", 2 is \"Effective negotiation with partial preference information\"",
        "Given above information, for an author who has written the paper with the title \"Design guidelines for negotiation support systems: an expert perspective using scenarios\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003186": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A connectionist approach for gray level image segmentation':",
        "Document: \"Multiple Exemplar-Based Facial Image Retrieval Using Independent Component Analysis. In this paper, we design a content-based image retrieval system where multiple query examples can be used to indicate the need to retrieve not only images similar to the individual examples, but also those images which actually represent a combination of the content of query images. We propose a scheme for representing content of an image as a combination of features from multiple examples. This scheme is exploited for developing a multiple example-based retrieval engine. We have explored the use of machine learning techniques for generating the most appropriate feature combination scheme for a given class of images. The combination scheme can be used for developing purposive query engines for specialized image databases. Here, we have considered facial image databases. The effectiveness of the image retrieval system is experimentally demonstrated on different databases.\"",
        "Document: \"Biometrics based Asymmetric Cryptosystem Design Using Modified Fuzzy Vault Scheme. We propose a novel biometrics cryptosystem where one can send and receive secure information using just the fingerprints. This cryptosystem is a judicious blend of the asymmetric cryptosystem like RSA and the symmetric Fuzzy Vault Scheme having the advantages of both the aforementioned cryptosystems. We have proposed a modification of the Fuzzy Vault Scheme to make it more robust against variations in the values of biometric features. Finally we propose the use of invariant features as a key to producing a hierarchical security system where the same key (fingerprint) can be used to generate encrypted messages at different levels of security.\"",
        "Document: \"Echocardiogram View Classification With Appearance And Spatial Distributions. When imaging the heart, using a 2D ultrasound probe, different views can manifest depending on the location and angulations of the probe. Some of these views have been labeled as standard views, due to the presentation and ease of assessment of key cardiac structures in them. We present an approach for automatic recognition and classification of these standard views, as a potential enabler for automated measurements or detection of noise -all without a human in the loop. We present an approach for view classification, Spatial Pyramid Histogram of Words which successfully models the appearance and shape distributions of object class. We demonstrate the effectiveness of this technique for the task of discrimination between the B-mode Parasternal Long Axis (PLAX) and the Short Axis (SAX) echocardiograms. For this task, our method shows a classification accuracy of 98.3 % on an exhaustive database of 703 ultrasound images.\"",
        "Document: \"Greedy Search for Active Learning of OCR. Active learning and crowd sourcing are becoming increasingly popular in the machine learning community for fast and cost effective generation of labels for large volumes of data. However, such labels may be noisy. So, it becomes important to ignore the noisy labels for building of a good classifier. We propose a framework for finding the best possible augmentation of a classifier for the character recognition problem using minimum number of crowd labeled samples. The approach inherently rejects the noisy data and tries to accept a subset of correctly labeled data to maximize the classifier performance.\"",
        "1 is \"Automatic Estimation of the Legibility of Binarised Historic Documents for Unsupervised Parameter Tuning\", 2 is \"Automatic extraction and description of human gait models for recognition purposes\"",
        "Given above information, for an author who has written the paper with the title \"A connectionist approach for gray level image segmentation\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003203": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Allocation and scheduling of real-time periodic tasks with relative timing constraints':",
        "Document: \"Modeling and Analysis of Code-based Call Admission Control for QoS Management in W-CDMA Systems. Efficient resource allocation for multi-class traffic with QoS guarantee is an important issue in integrated multimedia mobile communication networks. In the third generation mobile communication systems, the Orthogonal Variable Spreading Factor (OVSF) codes arc used as spreading codes. The use of OVSF codes allows the spreading factors to be changed for variable bit-rate requirements. In this paper, we presented the code- based guard channel schemes and analytical models for QoS management in W-CDMA systems. Compared with traditional channel-based schemes, the proposed schemes and analytical models exactly reflect the restrictions of OVSF code tree characteristics and arc suitable for W-CDMA systems. From the illustrations and numerical results, we find that the proposed model is easy to construct and to analyze QoS management schemes in W-CDMA systems. It call also be extended to more complicated multi-class traffic types systems.\"",
        "Document: \"Formulistic Detection of Malicious Fast-Flux Domains. Botnet creates harmful network attacks nowadays. Lawbreaker may implant malware into victim machines using botnets and, furthermore, he employs fast-flux domain technology to improve the lifetime of botnets. To circumvent the detection of command and control server, a set of bots are selected to redirect malicious communication and hides botnet communication within normal user traffic. As the dynamics of fast-flux domains, blacklist mechanism is not efficient to prevent fast-flux botnet attacks. It would be time consuming to examine the legitimacy of the domain of all the network connections. Therefore, a lightweight detection of malicious fast-flux domains is desired. Based on the time-space behavior of malicious fast-flux domains, the network behavior of domains are formulistic in this study to reduce the time complexity of feature modeling. According to the experimental results, the malicious fast-flux domains collected from real networks are identified efficiently and the proposed solution outperforms the blacklists.\"",
        "Document: \"Fast IPTV channel switching using hot-view and personalized channel preload over IEEE 802.16e. The Internet protocol television (IPTV) is emerging as one of the most promising applications over next generation networks. The recently released IEEE 802.16d/e is capable of ensuring high bandwidths and low latency, making it suitable for delivering multimedia services. In addition, it also provides wide area coverage, mobility support, and non-line-of-sight operation. In this article, we deliver IPTV streaming over 802.16 wireless systems and propose a simple but effective IPTV channel-switching algorithm to keep the channel zapping time in a tolerable range. In addition, we discuss how to allocate channels in the limited bandwidth over wireless networks, such as 802.16. The proposed algorithm is based on hot-view channel and personal favorite channel preloading to reduce the network delay and achieve the goal of fast channel switching. Finally, the experimental results show the performance of the proposed algorithm.\"",
        "Document: \"Priority-Oriented Architecture Service Management on OSGi Home-service Platform. Open Service Gateway initiative (OSGi) platforms integrate a variety of reusable applications and resources, packing them into bundles. These deployed services, which are offered by service providers, can expediently manage home appliances. However, in the user's daily life, various services or bundles access to other services directly or indirectly, creating a situation that is likely to generate problems involving non-supportable applications or insufficient hardware resources. OSGi platforms are no way to deal with these situations. Although much research has proposed methods to resolve specific issues, almost all of these methods are incomplete or are unsuitable for the limited resources in households. This paper addresses the wide range of such common home-network applications as those involving health care, home automation, and home security; and both our proposed priority-based principles of a management decision-making mechanism and our framework architecture help ensure the quality of service and or emergency responses.\"",
        "1 is \"Unified assign and schedule: a new approach to scheduling for clustered register file microarchitectures\", 2 is \"Improving Quality of VoIP Streams over WiMax\"",
        "Given above information, for an author who has written the paper with the title \"Allocation and scheduling of real-time periodic tasks with relative timing constraints\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003232": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A New Lyapunov-Krasovskii Methodology for Coupled Delay Differential Difference Equations':",
        "Document: \"Switching controller for stabilization of linear systems with switched time-varying delays. This paper considers interval time-varying delay systems with delayed estimation of the delay. This case is often encountered in the Networked Control Systems (NCS) field. Based on Lyapunov-Krasovskii functional methods and linear matrix inequality (LMI) techniques, a switching state feedback controller is designed to guarantee the exponential stability. The controller switches according to the measured time-delay which is itself delayed. The global stability of the switching closed loop is guaranteed if some dwell time conditions are satisfied.\"",
        "Document: \"An improved time-delay implementation of derivative-dependent feedback. We consider an LTI system of relative degree r\u22652 that can be stabilized using r\u22121 output derivatives. The derivatives are approximated by finite differences leading to a time-delayed feedback. We present a new method of designing and analyzing such feedback under continuous-time and sampled measurements. This method admits essentially larger time-delay/sampling period compared to the existing results and, for the first time, allows to use consecutively sampled measurements in the sampled-data case. The main idea is to present the difference between the derivative and its approximation in a convenient integral form. The kernel of this integral is hard to express explicitly but we show that it satisfies certain properties. These properties are employed to construct the Lyapunov\u2013Krasovskii functional that leads to LMI-based stability conditions. If the derivative-dependent control exponentially stabilizes the system, then its time-delayed approximation stabilizes the system with the same decay rate provided the time-delay (for continuous-time measurements) or the sampling period (for sampled measurements) are small enough.\"",
        "Document: \"Dynamic quantization of uncertain linear networked control systems. This paper is concerned with the stability analysis of networked control systems with dynamic quantization, variable sampling intervals and communication delays. A time-triggered zooming algorithm for the dynamic quantization at the sensor side is proposed that leads to an exponentially stable closed-loop system. The algorithm includes proper initialization of the zoom parameter. More precisely, given a bound on the state initial conditions and the values of the quantizer range and error, we derive conditions for finding the initial value of the zoom parameter, starting from which the exponential stability is guaranteed by using \\\"zooming-in\\\" only. Polytopic type uncertainties in the system model can be easily included in the analysis. The efficiency of the method is illustrated on an example of an uncertain cart-pendulum system.\"",
        "Document: \"Observers and initial state recovering for a class of hyperbolic systems via Lyapunov method. Recently the problem of estimating the initial state of some linear infinite-dimensional systems from measurements on a finite interval was solved by using the sequence of forward and backward observers\u00a0Ramdani, Tucsnak, and Weiss (2010). In the present paper, we introduce a direct Lyapunov approach to the problem and extend the results to the class of semilinear systems governed by wave and beam equations with boundary measurements from a finite interval. We first design forward observers and derive Linear Matrix Inequalities (LMIs) for the exponential stability of the estimation errors. Further we obtain simple finite-dimensional conditions in terms of LMIs for an upper bound T\u2217 on the minimal time, that guarantees the convergence of the sequence of forward and backward observers on [0,T\u2217] for the initial state recovering. This T\u2217 represents also an upper bound on the observability time. For observation times bigger than T\u2217, these LMIs give upper bounds on the convergence rate of the iterative algorithm in the norm defined by the Lyapunov functions. In our approach, T\u2217 is found as the minimal dwelling time for the switched exponentially stable (forward and backward estimation error) systems with the different Lyapunov functions\u00a0(Liberzon, 2003). The efficiency of the results is illustrated by numerical examples.\"",
        "1 is \"Asymptotic stabilization of nonlinear systems via sign-indefinite damping injection\", 2 is \"Sampled-Data Stabilization; A PBC Approach.\"",
        "Given above information, for an author who has written the paper with the title \"A New Lyapunov-Krasovskii Methodology for Coupled Delay Differential Difference Equations\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003247": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Closeness-based routing with temporal constraint for mobile social delay tolerant networks':",
        "Document: \"Id List Forwarding Free Confidentiality Preserving Data Aggregation For Wireless Sensor Networks. Wireless sensor networks(WSNs) are composed of sensor nodes with limited energy which is difficult to replenish. Data aggregation is considered to help reduce communication overhead with in-network processing, thus minimizing energy consumption and maximizing network lifetime. Meanwhile, it comes with challenges for data confidentiality protection. Many existing confidentiality preserving aggregation protocols have to transfer list of sensors' ID for base station to explicitly tell which sensor nodes have actually provided measurement. However, forwarding a large number of node IDs brings overwhelming extra communication overhead. In this paper, we propose provably secure aggregation scheme perturbation-based efficient confidentiality preserving protocol (PEC2P) that allows efficient aggregation of perturbed data without transferring any ID information. In general, environmental data is confined to a certain range; hence, we utilize this feature and design an algorithm to help powerful base station retrieve the ID of reporting nodes. We analyze the accuracy of PEC2P and conclude that base station can retrieve the sum of environmental data with an overwhelming probability. We also prove that PEC2P is CPA secure by security reduction. Experiment results demonstrate that PEC2P significantly reduces node congestion (especially for the root node) during aggregation process in comparison with the existing protocols.\"",
        "Document: \"An Optimal LTE-U Access Method for Throughput Maximization and Fairness Assurance. To solve the issue of scarce spectrum resources of the existing cellular network, LTE-U that expands LTE service to the unlicensed 5GHz spectrum is proposed. However, the centralized medium access control protocol of LTE largely decreases the performance of Wi-Fi networks operating in the same unlicensed spectrum. In the paper, to solve the problem, we propose a new mechanism based on the duty-cycle method. It can adaptively adjust the percentage of the airtime used by a LTE Small cell Base Station (SBS) according to the bandwidth of the licensed spectrum of the SBS and downlink data rate demands of the SBS users to maximize throughput of the SBS network on the unlicensed spectrum while ensuring fairness between the Wi-Fi and SBS network. The fairness is based on 3GPP proposed fairness coexistence criterion. To ensure the fairness, we propose a method to construct a W-Fi network offering the same level of the SBS traffic load, and throughput maximization of the SBS is formulated as a constrained non-linear optimization problem solved by an optimal algorithm. We evaluate the proposed mechanism from two aspects. The first is to prove the proposed Wi-Fi network construction method is valid. The second is to evaluate the performance of our proposed method. Simulation results show that our approach is valid and it can maximize the throughput of the SBS network and ensure the fairness criterion.\"",
        "Document: \"A Privacy-Preserving Traffic Monitoring Scheme via Vehicular Crowdsourcing. The explosive number of vehicles has given rise to a series of traffic problems, such as traffic congestion, road safety, and fuel waste. Collecting vehicles' speed information is an effective way to monitor the traffic conditions and avoid vehicles' congestion, however it may threaten vehicles' location and trajectory privacy. Motivated by the fact that traffic monitoring does not need to know each individual vehicle's speed and the average speed would be sufficient, we propose a privacy-preserving traffic monitoring (PPTM) scheme to aggregate vehicles' speeds at different locations. In PPTM, the roadside unit (RSU) collects vehicles' speed information at multiple road segments, and further cooperates with a service provider to calculate the average speed information for every road segment. To preserve vehicles' privacy, both homomorphic Paillier cryptosystem and super-increasing sequence are adopted. A comprehensive security analysis indicates that the proposed PPTM can preserve vehicles' identities, speeds, locations, and trajectories privacy from being disclosed. In addition, extensive simulations are conducted to validate the effectiveness and efficiency of the proposed PPTM scheme.\"",
        "Document: \"Fair Incentive Mechanism With Imperfect Quality in Privacy-Preserving Crowdsensing. Mobile crowdsensing (MCS) enables a platform to recruit users to collectively perform sensing tasks from requesters. In order to maximize the completion qualities of tasks, an incentive mechanism should be well designed for the platform to incentivize high-quality users\u2019 participation. The existing works largely adopt the Stackelberg game to model the strategic interactions in the incentive mechanism. However, there are practical issues that are less investigated in the context of the Stackelberg-based incentive mechanism. First, the platform has no knowledge about users\u2019 sensing qualities beforehand due to their private information. Second, the platform needs users\u2019 continuous participation in the long run, which results in fairness requirements. Third, it is also crucial to protect users\u2019 privacy due to the potential privacy leakage concerns (e.g., sensing qualities) after completing tasks. In this article, we jointly address these issues and propose the three-stage Stackelberg-based incentive mechanism for the platform to recruit participants. In detail, we leverage combinatorial volatile multiarmed bandits (CVMABs) to elicit unknown users\u2019 sensing qualities. We use the drift-plus-penalty (DPP) technique in Lyapunov optimization to handle the fairness requirements. We blur the quality feedback with tunable Laplacian noise such that the incentive mechanism protects locally differential privacy (LDP). Finally, we carry out experiments to evaluate our incentive mechanism. The numerical results show that our incentive mechanism achieves \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">sublinear</i>\n regret performance to learn unknown quality with fairness and privacy guarantee.\"",
        "1 is \"Fast and robust fixed-point algorithms for independent component analysis\", 2 is \"Internet Protocol Television (IPTV): The Killer Application for the Next-Generation Internet\"",
        "Given above information, for an author who has written the paper with the title \"Closeness-based routing with temporal constraint for mobile social delay tolerant networks\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003253": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Revisiting the Problem of Weight Initialization for Multi-Layer Perceptrons Trained with Back Propagation':",
        "Document: \"Efficient MRI Reconstruction Using a Hybrid Framework for Integrating Stepwise Bayesian Restoration and Neural Network Models in a Memory Based Priors System. The goal of this paper is to present a new image restoration methodology for extracting Magnetic Resonance Images (MRI) from reduced scans in k-space. The proposed approach considers the combined use of Support Vector Machine (SVM) models in a memory based Bayesian priors system and a novel Bayesian restoration, in the problem of MR image reconstruction from sparsely sampled k-space, following several different sampling schemes, including spiral and radial. Effective solutions to this problem are indispensable especially when dealing with MRI of dynamic phenomena since then, rapid sampling in k-space is required. The goal in such a case is to make measurement time smaller by minimizing scanning trajectories. It is suggested here that significant improvements could be achieved, concerning quality of the extracted image, applying to the k-space data a novel Bayesian restoration method based on progressively approximating the unknown image, involving a neural model based priors system with memory specifically aimed at minimizing first order image derivatives reconstruction errors. More specifically, it is demonstrated that SVM neural network techniques could construct efficient memory based Bayesian priors and introduce them in the procedure of a novel stepwise Bayesian restoration. These Priors are independent of specific image properties and probability distributions. They are based on training SVM neural filters to estimate the missing samples of complex k-space and thus, to improve k-space information capacity. Such a neural filter based Bayesian priors system with memory is integrated to the maximum likelihood procedure involved in the Bayesian reconstruction and aims at minimizing image derivatives and image reconstruction errors. It is found that the proposed methodology leads to enhanced image extraction results favorably compared to the ones obtained by the Integrated Bayesian MRI reconstruction involving simple SVM models priors as well as memory based priors minimizing image reconstruction errors instead of its derivatives errors, by the traditional Bayesian MRI reconstruction as well as by the pure Neural filter based reconstruction approach.\"",
        "Document: \"Improved Defect Detection Using Novel Wavelet Feature Extraction Involving Principal Component Analysis and Neural Network Techniques. This paper aims at investigating a novel solution to the problem of defect detection from images, that can find applications in the design of robust quality control systems for the production of furniture, textile, integrated circuits, etc. The suggested solution focuses on detecting defects from their wavelet transformation and vector quantization related properties of the associated wavelet coefficients. More specifically, a novel methodology is investigated for discriminating defects by applying a supervised neural classification technique, employing a Multilayer Perceptron (MLP) trained with the conjugate gradients algorithm, to innovative multidimensional wavelet based feature vectors. These vectors are extracted from the K-Level 2-D DWT (Discrete Wavelet Transform) transformed original image using Vector Quantization techniques and a Principal Component Analysis (PCA) applied to these wavelet domain quantization vectors. The results of the proposed methodology are illustrated in defective textile images where the defective areas are recognized with higher accuracy than the one obtained by applying two rival feature extraction methodologies. The first one of them uses all the wavelet coefficients derived from the k-Level 2-D DWT, while the second one uses only image intensities characteristics. Both rival methods involve the same classification stage as the proposed feature extraction approach. The promising results herein obtained outline the importance of judicious selection and processing of 2-D DWT wavelet coefficients for industrial pattern recognition applications.\"",
        "Document: \"On a Novel Simulation Framework and Scheduling Model Integrating Coverage Mechanisms for Sensor Networks and Handling Concurrency. Coverage is one of the fundamental metrics used to quantify the quality of service (QoS) of sensor networks. In general, we use this term to measure the ability of the network to observe and react to the phenomena taking place in the area of interest of the network. In addition, coverage is associated with connectivity and energy consumption, both important aspects in the design process of a Wireless Sensor Network (WSN). On the other hand, simulating a WSN involves taking into account different software and hardware aspects. In this paper we attempt to present a simulation framework suitable for integrating coverage mechanisms in WSN emulation using a layered architecture and a fitting scheduling model. The suggested model is derived after a critical overview and presentation of the coverage strategies as well as the simulation approaches for WSN developed so far. The main advantage of the proposed framework is its capability to handle concurrent events occurring at WSN deployment and operation through the suitable layered scheduler integrated.\"",
        "Document: \"Efficient information theoretic extraction of higher order features for improving neural network-based spam e-mail categorization. A novel approach for spam e-mail filtering is herein considered based on information theoretic extraction of higher order features and the committee machines neural network models. An extensive experimental study is organized, the most extensive so far in the literature, based on widely accepted benchmarking e-mail data sets, comparing the proposed methodology with the Naive Bayes spam filter as well as with the Boosting tree methodology, the linear models-based classification ( classification via regression) and the nonlinear models-based classification using simple neural network models, including Multilayer Perceptrons. Moreover, several feature extraction approaches based on information theory are evaluated, comparing mainly the proposed higher order feature extraction methodology with information theoretic extraction of single features. It is shown that the former outperforms the latter and, moreover, that the proposed information theoretic Boolean features present a remarkably high spam categorization performance compared to that of their analog counterparts. Finally, it is shown that the committee machines mail categorization performance compares very favorably to the other rival methods' performance, including the Bayes spam filter which is the most widely used approach in the e-mail services market.\"",
        "1 is \"The problem of bias in training data in regression problems in medical decision support\", 2 is \"DEPSO: hybrid particle swarm with differential evolution operator\"",
        "Given above information, for an author who has written the paper with the title \"Revisiting the Problem of Weight Initialization for Multi-Layer Perceptrons Trained with Back Propagation\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003275": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Interprocessor-blocking independent static task allocation for shared-bus real-time multiprocessors':",
        "Document: \"Design, Implementation, and Performance Evaluation of Flash Memory-based File System on Chip. Interoperability is an important requirement for portable storage devices that are increasingly being used to exchange and share data among diverse hosts. However, interoperability cannot be provided if different host systems use different file systems. To address this problem, we propose a storage device that contains a file system within itself, which we refer to as FSOC (File System On Chip). In this paper, we explain the design and implementation of a Flash memory-based FSOC as a proof-of-concept. We also propose a performance model for FSOC, which is derived by analyzing operations of the host and storage device. Using this model, we show that aside from qualitative benefits, there are quantitative benefits in using FSOC instead of a conventional storage device. Results from a series of experiments are given that compare the performance of a conventional storage device and the FSOC using synthetic workloads as well as real applications, which verifies the proposed model.\"",
        "Document: \"An internet protocol stack for high-speed transmission in a non-OS environment. Today's embedded systems are required to have stronger wired/wireless communication capability. Due to strict limitations on how they utilize resources such as power, address space, and processing ability, network protocols for embedded systems are designed to make the best use of constrained resources. They are developed in a way that allows for support from a range of operating systems and thus utilizes platform-independent architectures. This paper presents an Internet protocol stack design for embedded systems that operate without an operating system. Our scheme schedules the transmission and the reception of the data packets and uses cross-layer optimization. We implemented the proposed scheme in a LTE network device. The overhead of our scheme is low and it subsequently meets the constraint of transmission speed in the next generation mobile communication environments.\"",
        "Document: \"SensorMaker: A Wireless Sensor Network Simulator for Scalable and Fine-Grained Instrumentation. Nowadays, wireless sensor networks have drawn great attention as a new and important research area. These sensor networks typically consist of hundreds or even thousands of sensor nodes deployed in a geographical region to sense events. Therefore, using actual sensor networks in case of developing a new scheme or experimenting functionalities may consume too much time and cost. In this paper, we present a SensorMaker, which is a simulator for wireless sensor networks. It supports scalable and fine-grained instrumentation of the entire sensor networks. We also present the actual simulation results of the various existing routing and clustering algorithms, and network management schemes.\"",
        "Document: \"Impact on the writing granularity for incremental checkpointing. Incremental checkpointing is an cost-efficient fault tolerant technique for long running programs such as genetic algorithms. In this paper, we derive the equations for the writing granularity of incremental checkpointing and find factors associated with the time overhead and disk space for incremental checkpoint. We also verify the applicability of the derived equation and the acceptability of the factors through experiments.\"",
        "1 is \"Buffering and caching in large-scale video servers\", 2 is \"Capsule: an energy-optimized object storage system for memory-constrained sensor devices\"",
        "Given above information, for an author who has written the paper with the title \"Interprocessor-blocking independent static task allocation for shared-bus real-time multiprocessors\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003311": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Digital education in the classroom.':",
        "Document: \"Adapting the Speed of Reproduction of Audio Content and Using Text Reinforcement for Maximizing the Learning Outcome though Mobile Phones. The use of technology in learning environments should be targeted at improving the learning outcome of the process. Several technology enhanced techniques can be used for maximizing the learning gain of particular students when having access to learning resources. One of them is content adaptation. Adapting content is especially important when using limited devices such as mobile phones. Content can be adapted to restrictive network conditions, to limited terminal capabilities, to different user preferences and learning styles or to external elements in the learning and user contexts. This paper studies and analyzes the impact of modifying and therefore adapting the speed of reproduction of audio content in mobile phones on the learning gain of the user. The paper shows that the optimum speed for Spanish audio is around 206 words per minute. The impact of enhancing the audio reproduction by presenting an equivalent version in text displayed on the screen of the mobile phone is also studied. The paper concludes that this text is only important for students younger than 15. The paper analyzes data from 100 Spanish speaking users that are grouped according to different criteria, such as gender, age, or level of studies. The results are presented and discussed.\"",
        "Document: \"WCAG formalization with w3c techniques. Web accessibility consists of a set of restrictions that Web pages should follow in order to be functional for different devices and users. These restrictions, which are quite heterogeneous and rather expensive to evaluate, unless relayed to human judgement, are usually expressed within a program's code. Different solutions have recently emerged to express these restrictions in a more declarative way. We present a comparison of some of them and propose some W3C techniques for expressing these constraints. Using W3C technologies, the evaluation cost can be clearly minimized.\"",
        "Document: \"Experimenting with electromagnetism using augmented reality: Impact on flow student experience and educational effectiveness. Educational researchers have recognized Augmented Reality (AR) as a technology with great potential to impact affective and cognitive learning outcomes. However, very little work has been carried out to substantiate these claims. The purpose of this study was to assess to which extent an AR learning application affects learners' level of enjoyment and learning effectiveness. The study followed an experimental/control group design using the type of the application (AR-based, web-based) as independent variable. 64 high school students were randomly assigned to the experimental or control group to learn the basic principles of electromagnetism. The participants' knowledge acquisition was evaluated by comparing pre- and post-tests. The participants' level overall-state perception on flow was measured with the Flow State Scale and their flow states were monitored throughout the learning activity. Finally, participants' perceptions of benefits and difficulties of using the augmented reality application in this study were qualitatively identified. The results showed that the augmented reality approach was more effective in promoting students' knowledge of electromagnetic concepts and phenomena. The analysis also indicated that the augmented reality application led participants to reach higher flow experience levels than those achieved by users of the web-based application. However, not all the factors seem to have influence on learners' flow state, this study found that they were limited to: concentration, distorted sense of time, sense of control, clearer direct feedback, and autotelic experience. A deeper analysis of the flow process showed that neither of the groups reported being in flow in those tasks that were very easy or too difficult. However, for those tasks that were not perceived as difficult and included visualization clues, the experimental group showed higher levels of flow that the control group. The study suggests that augmented reality can be exploited as an effective learning environment for learning the basic principles of electromagnetism at high school provided that learning designers strike a careful balance between AR support and task difficulty.\"",
        "Document: \"Comparison of knowledge during the assembly process of learning objects. This paper describes the conceptual framework OntoGlue for the assembly of learning objects (LOs). To permit a coherent assembling process from the point of view of requirements and competencies, OntoGlue enhances the definition of LOs by including associated knowledge (i.e. requirements and competencies) in their conceptual data schema. This associated knowledge is defined in terms of classes of educational ontologies (used as taxonomies), possibly related by mappings. There are several advantages associated with the OntoGlue approach. Firstly, it provides an enhanced description of the LOs, which permits their search and reuse by considering requirements and competencies. Secondly, during the assembly process of two LOs, OntoGlue checks that the competencies of the first LO cover the requirements of the second LO, guaranteeing a coherent assembling process from the requirements and competencies' point of view. Thirdly, OntoGlue automatically calculates the meta-data of the resulting assembled LO. Finally, the definition of the associated knowledge in terms of classes of ontologies, possibly related by mappings, permits an advanced comparison of requirements and competencies during assembly and search processes.\"",
        "1 is \"An Open Abstract Framework for Modeling Interoperability of Mobile Learning Services\", 2 is \"Domain-specific languages: an annotated bibliography\"",
        "Given above information, for an author who has written the paper with the title \"Digital education in the classroom.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003316": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Cryptanalysis of an image encryption scheme based on a compound chaotic sequence':",
        "Document: \"Towards More Robust Commutative Watermarking-Encryption of Images. Histogram-based watermarking schemes are invariant against pixel permutations and can be combined with permutation-based ciphers. However, typical histogram-based watermarking schemes based on comparison of histogram bins are prone to de-synchronization attacks, where the whole histogram is shifted by a certain amount. In this paper we investigate the possibility of avoiding this kind of attacks by synchronizing the embedding and detection processes, using the mean of the histogram as a calibration point. The resulting watermarking scheme is resistant to three common types of shifts of the histogram, while the advantages of previous histogram-based schemes, especially commutativity of watermarking and permutation-based encryption, are preserved.\"",
        "Document: \"The Visualization of 3D Terrain Based on VRML. 3D terrain visualization is always a hot topic for GIS (Geographical Information System) and VR (Virtual Realiy). The emergence of WebGIS makes the integration between network and GIS closer, and makes the sharing of geographic information more convenient and faster. However, it is still an urgent problem for 3D terrain visualization based on network for a long time. This paper puts forward an effective visualized method for 3D terrain based on network, combined with VRML and its specific 3D rendering techniques. Firstly, we introduced some basic concepts of terrain visualization. Secondly, described the traits and principles of VRML. Thirdly, discussed some techniques of 3D graphic rendering. Lastly, realized the 3D terrain visualization on network based on browser plug-in. all the work provided a reference for the sharing and the visualization of 3D terrain.\"",
        "Document: \"When Human cognitive modeling meets PINs: User-independent inter-keystroke timing attacks. This paper proposes the first user-independent inter-keystroke timing attacks on PINs. Our attack method is based on an inter-keystroke timing dictionary built from a human cognitive model whose parameters can be determined by a small amount of training data on any users (not necessarily the target victims). Our attacks can thus be potentially launched on a large scale in real-world settings. We investigate inter-keystroke timing attacks in different online attack settings and evaluate their performance on PINs at different strength levels. Our experimental results show that the proposed attack performs significantly better than random guessing attacks. We further demonstrate that our attacks pose a serious threat to real-world applications and propose various ways to mitigate the threat.\"",
        "Document: \"Pseudo-random Bit Generator Based on Couple Chaotic Systems and Its Applications in Stream-Cipher Cryptography. Chaotic cryptology is widely investigated recently. This paper reviews the progress in this area and points out some existent problems in digital chaotic ciphers. As a comprehensive solution to these problems, a novel pseudo-random bit generator based on a couple of chaotic systems called CCS-PRBG is presented. Detailed theoretical analyses show that it has perfect cryptographic properties, and can be used to construct stream ciphers with higher security than other chaotic ciphers. Some experiments are made for confirmation. Finally, several examples of stream ciphers based on digital CCS-PRBG are given, and their security is discussed.\"",
        "1 is \"Consensus problems in networks of agents with switching topology and time-delays.\", 2 is \"CogTool-Explorer: a model of goal-directed user exploration that considers information layout\"",
        "Given above information, for an author who has written the paper with the title \"Cryptanalysis of an image encryption scheme based on a compound chaotic sequence\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003332": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A polynomial algorithm for a lot-sizing problem with backlogging, outsourcing and limited inventory':",
        "Document: \"A Scatter Search for the periodic capacitated arc routing problem. This paper considers the Periodic Capacitated Arc Routing Problem (PCARP), a natural extension of the well-known CARP to a multi-period horizon. Its objective is to assign a set of service days to each edge in a given network and to solve the resulting CARP for each period, in order to minimize the required fleet size and the total cost of the trips on the horizon. This new and very hard problem has various applications in periodic operations on street networks, like waste collection and sweeping. A greedy heuristic and a Scatter Search (SS) are developed and evaluated on two sets of PCARP instances derived from classical CARP benchmarks. The results show that the SS strongly improves its initial solutions and clearly outperforms the greedy heuristic. Preliminary lower bounds are also provided. As they are not sufficiently tight, the SS is also tested in the single-period case (CARP) for which tight bounds are available: in fact, it competes with one state-of-the-art metaheuristic proposed for the CARP.\"",
        "Document: \"Coordination of split deliveries in one-warehouse multi-retailer distribution systems. In this paper, we consider a distribution system where a warehouse is responsible for replenishing the inventories at multiple retailers by a fleet of vehicles of limited capacity. If a distribution policy of the system involves split deliveries, that is, the inventory of at least one retailer is replenished by using multiple vehicle routes, the coordination of the deliveries can further reduce the inventory cost of the retailer. We consider the coordination where two split deliveries are realized by direct shipping and multiple-stop shipping, respectively. To the best of our knowledge, this kind of coordination was never studied in the literature but can find its application in inventory routing problems. This paper proposes and analyses a class of coordination policies for the split deliveries which can reduce the inventory costs of the retailers without increasing transportation costs. A non-linear programming model is established for formulating the class of polices. Because the optimal coordination policy corresponding to an optimal solution of the model may be hard to find and/or implement, two simple but effective coordination policies are proposed. The inventory cost savings realized by the two policies are evaluated analytically and algorithmically. Our theoretical analysis and computational experiments show that both policies are effective. Under certain conditions, they can save 50% of the inventory costs at the retailers without increasing transportation costs.\"",
        "Document: \"An effective hybrid approach to the two-stage capacitated facility location problem. \u2022A two-stage capacitated facility location problem is studied.\u2022A new solution scheme is proposed to solve the problem to optimality.\u2022The method combines cutting plane method, local branching and kernel search.\u2022The method significantly outperforms existing methods in the literature.\"",
        "Document: \"Petri Net Modeling and Cycle-Time Analysis of Dual-Arm Cluster Tools With Wafer Revisiting. There are wafer fabrication processes in cluster tools that require wafer revisiting. If a swap strategy is applied to dual-arm cluster tools handling wafer revisiting, a three-wafer periodical process is formed with three wafers completed in each period. Such a period contains three cycles in a revisiting process and three cycles in a nonrevisiting one. Hence, analysis and scheduling of such tools become very complicated. In this paper, a Petri net (PN) model is developed to describe their operations. Based on it, it is found that, if a swap strategy is applied, such tools are always in a transient state. A systematic method is then presented to analyze their performance. With the help of the proposed PN model, this work, for the first time, derives the optimality conditions of three-wafer period scheduling. Industrial application examples are given to show the results. \u00a9 2013 IEEE.\"",
        "1 is \"An adaptive dependability model of component-based software\", 2 is \"A crane scheduling method for port container terminals\"",
        "Given above information, for an author who has written the paper with the title \"A polynomial algorithm for a lot-sizing problem with backlogging, outsourcing and limited inventory\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003384": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Dynamic Multi-Path Communication for Video Traffic':",
        "Document: \"Identifying MMORPG bots: a traffic analysis approach. MMORPGs have become extremely popular among network gamers. Despite their success, one of MMORPG's great- est challenges is the increasing use of game bots, i.e., auto- playing game clients. The use of game bots is considered unsportsmanlike and is therefore forbidden. To keep games in order, game police, played by actual human players, of- ten patrol game zones and question suspicious players. This practice, however, is labor-intensive and ineffective. To ad- dress this problem, we analyze the traffic generated by hu- man players vs. game bots and propose solutions to auto- matically identify game bots. Taking Ragnarok Online, one of the most popular MMOGs, as our subject, we study the traffic generated by mainstream game bots and human players. We find that their traffic is distinguishable by: 1) the regularity in the release time of client commands, 2) the trend and magnitude of traffic burstiness in multiple time scales, and 3) the sensitivity to network conditions. We propose four strategies and two in- tegrated schemes to identify bots. For our data sets, the conservative scheme completely avoids making false accusa- tions against bona fide players, while the progressive scheme tracks game bots down more aggressively. Finally, we show that the proposed methods are generalizable to other games and robust against counter-measures from bot developers.\"",
        "Document: \"mProducer: authoring multimedia personal experiences on mobile phones. The paper presents the mProducer tool that enables multimedia content authoring on mobile phones. It addresses three major challenges: limited storage space on mobile phones; costly and slow wireless links; limited mobile user attention. Our contribution is twofold: (1) a novel technique called storage constrained off-loading (SCO) that intelligently uploads selected portions of contents to a remote storage server so that the amount of contents a user can capture is not restricted by the size of the small mobile storage; (2) two new UI techniques, the keyframe based editing and automatically generated content navigation tree, to reduce the amount of user efforts for editing\"",
        "Document: \"Adaptive Drone Sensing with Always Return-To-Home Guaranteed. In this paper, we proposed an Adaptive Return-to-Home Sensing (ARS) algorithm for a drone sensing system deployed in an open area with missions to conduct periodic environmental sensing. ARS scheme is able to conduct as many rounds of environmental sensing without drastic oscillation between consecutive sensing attempts while reserve sufficient energy for the drone to fly back home. We evaluate the ARS scheme under environmental difficulties. The results demonstrate the proposed scheme can conduct as many rounds of environmental sensing as possible without drastic oscillation while preserve enough energy for the done to return home.\"",
        "Document: \"GETA sandals: a footstep location tracking system. This paper presents the design, implementation, and evaluation of a footstep based indoor location system. The traditional Japanese GETA sandals are equipped with force, ultrasonic, orientation, RFID sensors and an accelerometer to produce a wearable location tracking system that demand little infrastructure in the deployed environment. In its basic form, a user simply puts on GETA sandals to enable tracking of his/her locations relative to a starting point (e.g., a building entrance), making it easy for deployment everywhere. The footstep location system is based on dead-reckoning, which works by measuring and tracking displacement vectors along a trail of footsteps. Each displacement vector is formed by drawing a line between each pair of footsteps, and the position of a user can be calculated by summing up the current and all previous displacement vectors. Unlike most existing indoor location systems, the footstep based method does not suffer from problems with obstacles, multi-path effects, signal noises, signal interferences, and dead spots. There are two technical challenges in the proposed design: (1) location error accumulates over distance traveled, and (2) displacement measurements are sporadic during stair climbing. The first problem is addressed by a light RFID infrastructure, while the second problem is remedied by incorporating an accelerometer into the system. Experiments on GETA prototype are conducted to evaluate the positional accuracy of our system.\"",
        "1 is \"Multicast routing and its QoS extension: problems, algorithms, and protocols\", 2 is \"Generalized best-first search strategies and the optimality of A*\"",
        "Given above information, for an author who has written the paper with the title \"Dynamic Multi-Path Communication for Video Traffic\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003398": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Fuzzy Control System Design via Fuzzy Lyapunov Functions':",
        "Document: \"Robust H\u221e control for uncertain discrete-time systems with time-varying delays via exponential output feedback controllers. This paper considers the problem of robust H\u221e control for uncertain discrete systems with time-varying delays. The system under consideration is subject to time-varying norm-bounded parameter uncertainties in both the state and measured output matrices. Attention is focused on the design of a full-order exponential stable dynamic output feedback controller which guarantees the exponential stability of the closed-loop system and reduces the effect of the disturbance input on the controlled output to a prescribed level for all admissible uncertainties. In terms of a linear matrix inequality (LMI), a sufficient condition for the solvability of this problem is presented, which is dependent on the size of the delay. When this LMI is feasible, the explicit expression of the desired output feedback controller is also given. Finally, an example is provided to demonstrate the effectiveness of the proposed approach.\"",
        "Document: \"Delay-dependent stabilization for stochastic fuzzy systems with time delays. This paper is concerned with the delay-dependent stabilization problem for a class of time-delay stochastic fuzzy systems. The time delays are assumed to appear in both the state and the control input. The purpose is the design of a state-feedback fuzzy controller such that the resulting closed-loop system is asymptotically stable in the mean square. A delay-dependent condition for the solvability of this problem is obtained in terms of relaxed linear matrix inequalities (LMIs). By solving these LMIs, a desired controller can be obtained. Finally, a numerical example is given to demonstrate the effectiveness of the present results.\"",
        "Document: \"Global Fixed-Time Consensus Tracking of Nonlinear Uncertain Multiagent Systems With High-Order Dynamics. In this paper, we study the consensus tracking problem for high-order nonlinear uncertain multiagent systems. By using the fixed-time control technique and the modified addition of a power integrator method, a novel distributed observer-based consensus protocol is proposed. Compared with the existing results in the literature, the proposed protocol can achieve consensus tracking in a fixed time in...\"",
        "Document: \"Adaptive finite-time control for stochastic nonlinear systems subject to unknown covariance noise. This paper is devoted to the adaptive finite-time control for a class of stochastic nonlinear systems driven by the noise of covariance. The traditional growth conditions assumed on the drift and diffusion terms are removed through a technical lemma, and the negative effect generated by unknown covariance noise is compensated by combining adaptive control technique with backstepping recursive design. Then, without imposing any growth assumptions, a smooth adaptive state-feedback controller is skillfully designed and analyzed with the help of the adding a power integrator method and stochastic backstepping technique. Distinctive from the global stability in probability or asymptotic stability in probability obtained in related work, the proposed design algorithm can guarantee the solution of the closed-loop system to be finite-time stable in probability. Finally, a stochastic simple pendulum system is skillfully constructed to demonstrate the effectiveness of the proposed control scheme.\"",
        "1 is \"Brief On delay-dependent stability for linear neutral systems\", 2 is \"Exploitation of Ubiquitous Wi-Fi Devices as Building Blocks for Improvised Motion Detection Systems.\"",
        "Given above information, for an author who has written the paper with the title \"Fuzzy Control System Design via Fuzzy Lyapunov Functions\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003413": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Towards Performance Evaluation of Mobile Ad Hoc Network Protocols':",
        "Document: \"An improved replacement algorithm in fault-tolerant meshes. Since the failure of resources fatally affects processor allocation, a fault tolerant service is essential in the interconnection networks. In this paper, a new fault tolerant method is proposed and evaluated in the hybrid processor allocation scheme, which we have introduced in our previous work. Our task consists of two independent phases. First, the allocation process executes to allocate an efficient set of processors to the requested submesh. The second phase comes to work when the faulty nodes are detected in the allocated spaces. The selected processor allocation scheme allows jobs to be executed without waiting, provided that the number of processors is sufficient in the system and applicable to any size of requests. In addition, our fault tolerant algorithm adds redundancies (spare nodes and links) to the mesh network when the faulty nodes are detected. The replacement algorithm replaces faulty nodes with spare nodes by considering the location of the allocated submeshes in the system. Comparing results shows that the system performance, which has increased by applying the allocation scheme, can improve by using an efficient replacement algorithm.\"",
        "Document: \"Learning an Integrated Distance Metric for Comparing Structure of Complex Networks.   Graph comparison plays a major role in many network applications. We often need a similarity metric for comparing networks according to their structural properties. Various network features - such as degree distribution and clustering coefficient - provide measurements for comparing networks from different points of view, but a global and integrated distance metric is still missing. In this paper, we employ distance metric learning algorithms in order to construct an integrated distance metric for comparing structural properties of complex networks. According to natural witnesses of network similarities (such as network categories) the distance metric is learned by the means of a dataset of some labeled real networks. For evaluating our proposed method which is called NetDistance, we applied it as the distance metric in K-nearest-neighbors classification. Empirical results show that NetDistance outperforms previous methods, at least 20 percent, with respect to precision. \"",
        "Document: \"A probabilistic task scheduling method for grid environments. This paper presents a probabilistic task scheduling method to minimize the overall mean response time of the tasks submitted to the grid computing environments. Minimum mean response time of a given task can be obtained by finding a subset of appropriate computational resources to service the task. To achieve this, a discrete time Markov chain (DTMC) representing the task scheduling process within the grid environment is constructed. The connection probabilities between the nodes representing the grid managers and resources can be considered as transition probabilities of the obtained DTMC. Knowing the mean response times of the managers and resources, and finding fundamental matrix of the DTMC, the mean response time related to each of the absorbing DTMCs existing inside the overall DTMC can be computed. Minimizing the obtained mean response times and taking into account the probability constraints in each of the absorbing DTMCs, a nonlinear programming (NLP) problem is defined. Solving the NLP problem, the connection probabilities between the managers and resources are obtained. Finally, using the connection probabilities, the best scheduling path within the environment and the minimum mean response time of a particular task can be achieved. In a case in which there is only one optimal scheduling choice within the environment, the proposed method can deterministically find such scheduling by assigning zero or one to the connection probabilities. Results obtained from evaluating the proposed method on the hypothesis and real grid environments show the preference of the proposed method compared to the other methods in minimizing both the overall mean response time of the tasks and total makespan of the environment.\"",
        "Document: \"A Lightweight Defense Approach to Mitigate Version Number and Rank Attacks in Low-Power and Lossy Networks. The Internet of Things (IoT) presents a new paradigm of the future internet that intends to provide interactive communication between various processing object via heterogeneous networks. The routing protocol in the IoT environment is Routing Protocol for Low-Power and Lossy Networks (RPL). The current RPL specification defines primary security modes; therefore it is vulnerable to topological attacks. In this paper the RPL routing mechanism, its topological vulnerabilities and two important topological attacks namely version number attack and rank spoofing attack are analyzed. To counter the mentioned attacks, a lightweight Identity Based Offline\u2013Online Signature based scheme is proposed. Our evaluation shows that our proposed scheme is secure in the random oracle model, and in terms of computational cost and energy consumption efficiently counters with these attacks.\"",
        "1 is \"Dynamic Service Placement in Geographically Distributed Clouds\", 2 is \"Cluster-Based LTL model checking of large systems\"",
        "Given above information, for an author who has written the paper with the title \"Towards Performance Evaluation of Mobile Ad Hoc Network Protocols\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003495": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'TRINITY: Tailoring Wireless Transmission Strategies to User Profiles in Enterprise Wireless Networks':",
        "Document: \"LiTell: indoor localization using unmodified light fixtures: demo. Owing to dense deployment of light fixtures and multipath-free propagation, visible light localization technology holds potential to overcome the reliability issue of radio localization. However, existing visible light localization systems require customized light hardware, which increases deployment cost and hinders near term adoption. We present LiTell, a simple and robust localization scheme that employs unmodified fluorescent lights (FLs) as location landmarks and commodity smartphones as light sensors. LiTell builds on the key observation that each FL has an inherent characteristic frequency, which can serve as a discriminative feature. It incorporates a set of sampling, signal amplification and camera optimization mechanisms, that enable a smartphone to capture the extremely weak and high frequency (> 80 kHz) features. We have implemented LiTell as a real-time localization and navigation system on Android. In our experiments, LiTell demonstrates high reliability in discriminating different FLs, and great potential to achieve sub-meter granularity.\n\n\"",
        "Document: \"Adaptive feedback compression for MIMO networks. MIMO beamforming technology can scale wireless data rate proportionally with the number of antennas. However, the overhead induced by receivers' CSI (channel state information) feedback scales at a higher rate. In this paper, we address this fundamental tradeoff with Adaptive Feedback Compression (AFC). AFC quantizes or compresses CSI from 3 dimensions --- time, frequency and numerical values, and adapts the intensity of compression according to channel profile. This simple principle faces many practical challenges, e.g., a huge search space for adaption, estimation or prediction of the impact of compression on network throughput, and the coupling of different users in multi-user MIMO networks. AFC meets these challenges using a novel cross-layer adaptation metric, a metric extracted from 802.11 packet preambles, and uses it to guide the selection of compression intensity, so as to balance the tradeoff between overhead reduction and capacity loss (due to compression). We have implemented AFC on a software radio testbed. Our experiments show that AFC can outperform alternative approaches in a variety of radio environments.\"",
        "Document: \"Exploiting interference locality in coordinated multi-point transmission systems. Coordinated Multi-Point (CoMP) transmission is emerging as a concept that can substantially suppress interference, thus improving the capacity of multi-cell wireless networks. However, existing CoMP techniques either require sharing of data and channel state information (CSI) for all links in the network, or have limited capability of interference suppression. In this paper, we propose distributed interference alignment and cancellation (DIAC) to overcome these limitations. DIAC builds on a key intuition of interference locality - since each link is interfered with a limited number of neighboring links, it is sufficient to coordinate with those strong interferers and ignore others, in order to bound the overhead in CoMP. DIAC realizes the localized coordination by integrating interference cancellation and distributed interference alignment, and can be applied to both the uplink and downlink of multi-cell wireless networks. We validate DIAC using both model-driven and trace-based simulation where the traces are collected by implementing a MIMO-OFDM channel estimator on a software radio platform. Our experiments show that DIAC can substantially improve the degrees of freedom in multi-cell wireless networks.\"",
        "Document: \"Cooperative Carrier Signaling: Harmonizing Coexisting WPAN and WLAN Devices. The unlicensed ISM spectrum is getting crowded by wireless local area network (WLAN) and wireless personal area network (WPAN) users and devices. Spectrum sharing within the same network of devices can be arbitrated by existing MAC protocols, but the coexistence between WPAN and WLAN (e.g., ZigBee and WiFi) remains a challenging problem. The traditional MAC protocols are ineffective in dealing with the disparate transmit-power levels, asynchronous time-slots, and incompatible PHY layers of such heterogeneous networks. Recent measurement studies have shown moderate-to-high WiFi traffic to severely impair the performance of coexisting ZigBee. We propose a novel mechanism, called cooperative carrier signaling (CCS), that exploits the inherent cooperation among ZigBee nodes to harmonize their coexistence with WiFi WLANs. CCS employs a separate ZigBee node to emit a carrier signal (busy tone) concurrently with the desired ZigBee's data transmission, thereby enhancing the ZigBee's visibility to WiFi. It employs an innovative way to concurrently schedule a busy tone and a data transmission without causing interference between them. We have implemented and evaluated CCS on the TinyOS/MICAz and GNURadio/USRP platforms. Our extensive experimental evaluation has shown that CCS reduces collision between ZigBee and WiFi by 50% for most cases, and by up to 90% in the presence of a high-level interference, all at negligible WiFi performance loss.\"",
        "1 is \"A correction to the proof of a lemma in \", 2 is \"\"",
        "Given above information, for an author who has written the paper with the title \"TRINITY: Tailoring Wireless Transmission Strategies to User Profiles in Enterprise Wireless Networks\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003549": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Optimal Power Distribution and Minimum Expected Distortion in Gaussian Layered Broadcast Coding with Successive Refinement':",
        "Document: \"The Distortion-Rate Function of Sampled Wiener Processes. We consider the recovery of a continuous-time Wiener process from a quantized or a lossy compressed version of its uniform samples under limited bitrate and sampling rate. We derive a closed-form expression for the optimal tradeoff among sampling rate, bitrate, and quadratic distortion in this setting. This expression is given in terms of a reverse waterfilling formula over the asymptotic spectral...\"",
        "Document: \"Study of Gaussian Relay Channels with Correlated Noises. In this paper, we consider the full-duplex and half-duplex Gaussian relay channels where the noises at the relay and destination are arbitrarily correlated. We first derive the capacity upper bound and the achievable rates with three existing schemes: Decode-and-Forward (DF), Compress-and-Forward (CF), and Amplify-and-Forward (AF). We present two capacity results under specific noise correlation coefficients, one being achieved by DF and the other being achieved by direct link transmission (or a special case of CF). The channel for the former capacity result is equivalent to the traditional Gaussian degraded relay channel and the latter corresponds to the Gaussian reversely-degraded relay channel. For CF and AF schemes, we show that their achievable rates are strictly decreasing functions of the correlation coefficient when the correlation coefficient is negative. Moreover, when the noise correlation coefficient is positive, the CF achievable rate may also outperform the independent-noise case if the noise correlation coefficient is within a certain range. Through numerical comparisons under different channel settings, we observe that although DF completely disregards the noise correlation while the other two can potentially exploit such extra information, none of the three relay schemes always outperforms the others over different correlation coefficients. Moreover, the exploitation of noise correlation by CF and AF accrues more benefit when the source-relay link is weak. This paper also considers the optimal power allocation problem under the correlated-noise channel setting. With individual power constraints at the relay and the source, it is shown that the relay should use all its available power to maximize the achievable rates under any correlation coefficient. With a total power constraint across the source and the relay, the achievable rates are proved to be concave functions over the power allocation factor for AF and CF under full-duplex mode, where the cl- - osed-form power allocation strategy is derived.\"",
        "Document: \"Capacity theorems for the finite-state broadcast channel with feedback. We consider the discrete, time-varying broadcast channel with memory under the assumption that the channel states belong to a set of finite cardinality. We study the achievable rates in two scenarios where feedback (and cooperation) is available. One scenario is the general finite-state broadcast channel (FSBC) where both receivers send feedback to the transmitter, and in addition one receiver sends his channel outputs to the other receiver through a cooperation link. The second scenario is the degraded FSBC where only the strong receiver sends feedback to the transmitter. We find the capacity regions for both cases. In both scenarios we consider non-indecomposable as well as a class of indecomposable FSBCs.\"",
        "Document: \"Truncated power control in code-division multiple-access communications. We analyze the performance of truncated power control in a code-division multiple-access (CDMA) communication system. This power control scheme compensates for propagation gain above a certain cutoff fade depth: below the cutoff level, data transmission is suspended. We assume a channel with fast Rayleigh fading and slow lognormal shadowing plus path loss, where truncated power control is applied ...\"",
        "1 is \"Coverage enhancement through two-hop relaying in cellular radio systems\", 2 is \"The source-channel separation theorem revisited\"",
        "Given above information, for an author who has written the paper with the title \"Optimal Power Distribution and Minimum Expected Distortion in Gaussian Layered Broadcast Coding with Successive Refinement\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003614": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Finding Subsets Maximizing Minimum Structures':",
        "Document: \"Number Theory Helps Line Detection in Digital Images. This paper deals with the problem of detecting line components in a digital image. For this purpose, the Hough Transform, which is based on voting in the dual plane, is widely used. However, there have been few theoretical studies on the relationship between its computational complexity and ability of detecting straight lines. In this paper we present two completely different algorithms for detecting every maximal line component contained in a digital image. The one, which is effective in the case of a dense digital image, is based on a new transformation named L\n1-Dual Transform defined by the L\n1-distance between points and lines. Number Theory supports efficient implementation of the algorithm. It can complete the required task in least time needed to achieve the above-mentioned ability of line detection. The other, which is effective when the edge density is low, attains efficiency by using the plane sweep technique in computational geometry. Furthermore, we present an efficient approximation algorithm which can detect at least \u00d7100% of any maximal line component and show that its computational complexity depends on the value of . Choosing =0.5, for example, the time complexity of the algorithm is reduced from O(N4) to O(N\n3), where N is the length of one side of an image.\"",
        "Document: \"A rooted-forest partition with uniform vertex demand. \n A rooted-forest is a graph having self-loops such that each connected component contains exactly one loop, which is regarded\n as a root, and there exists no cycle consisting of non-loop edges. In this paper, we shall study on a partition of a graph\n into edge-disjoint rooted-forests such that each vertex is spanned by exactly d components of the partition, where d is a positive integer.\n \n \"",
        "Document: \"Use of a genetic heritage for solving the assignment problem with two objectives. The paper concerns a multiobjective heuristic to compute approximate efficient solutions for the assignment problem with two objectives. The aim here is to show that the genetic information extracted from supported solutions constitutes a useful genetic heritage to be used by crossover operators to approximate non-supported solutions. Bound sets describe one acceptable limit for applying a local search over an offspring. Results of extensive numerical experiments are reported. All exact efficient solutions are obtained using Cplex in a basic enumerative procedure. A comparison with published results shows the efficiency of this approach.\"",
        "Document: \"Polynomial-time solutions to image segmentation. No abstract available.\n\n\"",
        "1 is \"Approximation algorithms for maximum dispersion\", 2 is \"Local search guided by path relinking and heuristic bounds\"",
        "Given above information, for an author who has written the paper with the title \"Finding Subsets Maximizing Minimum Structures\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003636": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Geographic server distribution model for key revocation':",
        "Document: \"Ant Colony Optimization Based Sub-channel Allocation Algorithm for Small Cell HetNets. Two-tier heterogeneous networks (HetNets) composed of a conventional macrocellular network and small cell networks (SCNs) have been proposed in the literature with the aim to extend indoor coverage and realize efficient radio resource usage. As SCN shares the same frequency band with the underlying macrocell, the cross tier interference needs to be mitigated since the inter-SCN and cross tier interference at the SCN boundary may result in undesirable network performance degradation. In this paper, we propose an intelligent physical resource block (PRB) allocation as a solution to mitigate the downlink intra-SCN interference as well as the inter-tier interference in OFDM-based systems. The allocation of the PRBs to the network users is formulated as a graph coloring problem, and solved using an ant colony optimization (ACO)-based approach. Simulation results are provided, showing that our ACO-based algorithm outperforms the Received Power-based Allocation (RPA) and Received SINR-based Allocation (RSA) algorithms in terms of average SINR experienced by network users, outage probability, and number of required PRBs.\"",
        "Document: \"Deterministic trust management in pervasive computing. An effective trust management technique plays a vital role in evaluating relationships among devices in pervasive computing. In this paper, we propose a deterministic trust management scheme that aims at establishing trust relationships among devices using direct and indirect computation methods. Recommendations and trust updating mechanisms are used to increase the reliability of trust computations. We have carried out performance evaluations using simulation experiments. The results show that trust management with recommendation outperforms other schemes.\"",
        "Document: \"GAER: genetic algorithm-based energy-efficient routing protocol for infrastructure-less opportunistic networks. In infrastructure-less opportunistic networks (Oppnets), the routing of messages is a challenging task since nodes are not aware of the network topology and they look for an opportunity to send the message by finding or predicting a best temporary path at each hop towards the destination. As nodes perform various computations for next hop selection, a lot of battery power gets consumed, which in turn reduces the network lifetime. Thus, there is a clear demand for routing protocols for such networks which are energy-efficient and consume lesser power of nodes in forwarding a message. In this paper, a novel routing protocol named genetic algorithm-based energy-efficient routing (GAER) protocol for infrastructure-less Oppnets is proposed. This protocol uses a node's personal information, and then applies the genetic algorithm (GA) to select a better next hop among a group of neighbour nodes for the message to be routed to the destination. With the application of GA, optimal results are obtained that help in the selection of the best possible node as the next hop, which in turn, leads to prolonged battery life. Simulation results show that GAER outperforms the Epidemic, PROPHET, and Spray and Wait protocols in terms of messages delivered, overhead ratio, average residual energy, and number of dead nodes. The results obtained for average latency and average buffer time using GAER are comparable to those obtained for the aforementioned protocols.\"",
        "Document: \"An ant-swarm inspired dynamic multiresolution data dissemination protocol for wireless sensor networks. One of the main concerns of wireless sensor networks (WSNs) is to deliver useful information from data sources to users at a minimum power consumption due to constraints that sensor nodes must operate on limited power sources for extended time. In particular, achieving power-efficiency and multihop communication in WSN applications is a major issue. This paper continues on the investigation of a recently proposed Minimum-power Multiresolution Data Dissemination (MMDD) problem for WSNs (whose solution is considered here as a benchmark). We propose an ant-inspired solution to this problem. To the best of our knowledge, no attempts have been made so far in this direction. We have evaluated the performance of our proposed solution by conducting a variety of experiments and have found our solution to be promising in terms of total energy consumption in data dissemination.\"",
        "1 is \"Deriving structurally based software measures\", 2 is \"Distributed dynamic scheduling for end-to-end rate guarantees in wireless ad hoc networks\"",
        "Given above information, for an author who has written the paper with the title \"Geographic server distribution model for key revocation\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003733": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Creating Socially Adaptive Electronic Partners: Interaction, Reasoning and Ethical Challenges':",
        "Document: \"A Multi-agent Architecture for an Intelligent Website in Insurance. In this paper a multi-agent architecture for intelligent Websites is presented and applied in insurance. The architecture has been designed and implemented using the compositional development method for multi-agent systems DESIRE. The agents within this architecture are based on a generic broker agent model. It is shown how it can be exploited to design an intelligent Website for insurance, developed in co-operation with the software company Ordina Utopics and an insurance company.\"",
        "Document: \"Sharing information in teams: giving up privacy or compromising on team performance?. Human teamwork can be supported by agent technology by providing each human team member with an agent that monitors, supports and advices the human. The agent can, for example, monitor the human's workload, and share that information with (agents of) other team members so that work can be distributed effectively. However, though sharing information can lead to a higher team performance, it may violate the individual team members' privacy. This raises the question what type of and how often information should be shared between team members. This paper addresses this question by studying the trade-off between privacy loss and team performance in the train traffic control domain. We provide a conceptual domain analysis, introduce a formal model of train traffic control teams and their dynamics, and describe an agent-based simulation experiment that investigates the effects of sharing different types and amounts of information on privacy loss and team performance. The results give insight in the extent to which different information types cause privacy loss and contribute to team performance. This work enables the design of privacy-sensitive support agents for teamwork.\"",
        "Document: \"Compositional Verification of Multi-Agent Systems in Temporal Multi-Epistemic Logic. Compositional verification aims at managing the complexity of theverification process by exploiting compositionality of the systemarchitecture. In this paper we explore the use of a temporal epistemiclogic to formalize the process of verification of compositionalmulti-agent systems. The specification of a system, its properties andtheir proofs are of a compositional nature, and are formalized within acompositional temporal logic: Temporal Multi-Epistemic Logic. It isshown that compositional proofs are valid under certain conditions.Moreover, the possibility of incorporating default persistence ofinformation in a system, is explored. A completion operation on aspecific type of temporal theories, temporal completion, is introducedto be able to use classical proof techniques in verification withrespect to non-classical semantics covering default persistence.\"",
        "Document: \"Virtual reality negotiation training increases negotiation knowledge and skill. In this paper we test the hypothesis that Virtual Reality (VR) negotiation training positively influences negotiation skill and knowledge. We discuss the design of the VR training. Then, we present the results of a between subject experiment (n=42) with three experimental conditions (control, training once, repeated training) investigating learning effects on subjects' negotiation skill and knowledge. In our case negotiation skill consists of negotiation outcome (final bid utility) and conversation skill (exploratory conversational choices in VR scenario), and negotiation knowledge is the subjects' quality of reflection upon filmed behavior of two negotiating actors. Our results confirm the hypothesis. We found significant effects of training on conversation skill and negotiation knowledge. We found a marginally significant effect of training on negotation outcome. As the effect of training on negotiation outcome was marginally significant and only present when controlling for overshadowing effects of the act of reflecting, we postulate that other learning approaches (e.g., instruction) are needed for trainees to use the information gained during the joint exploration phase of a negotiation for the construction of a bid. Our results are particularly important given the sparse availability of experimental studies that show learning effects of VR negotiation training, and gives additional support to those studies that do report possitive effects such as with the BiLAT system.\"",
        "1 is \"Combining Topic Information and Structure Information in a Dynamic Language Model\", 2 is \"Organization-Based Coalition Formation\"",
        "Given above information, for an author who has written the paper with the title \"Creating Socially Adaptive Electronic Partners: Interaction, Reasoning and Ethical Challenges\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003814": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'New identity-based society oriented signature schemes from pairings on elliptic curves':",
        "Document: \"Retrieving the most similar symbolic pictures from pictorial databases. In this article, we suggest an iconic indexing mechanism for spatial similarity retrieval on iconic image databases based upon the spatial relationships among the objects in a picture. The iconic objects we deal with are some kinds of gross panorama of simple objects. We also assume that any one iconic object is not distinguished from any other object of the same kind. For our mechanism, we first transform each iconic picture into a set of ordered triples ( O i , O j , R ij ) where O i and O j are objects and R ij is the predefined spatial relationship codes between O i and O j . Then we construct a set of hashing functions for all spatial relationship codes R ij , separately, associated with all ordered pairs ( O i , O j ) extracted from the ordered triples ( O i , O j , R ij ). Thereafter, an iconic index table can be established according to the constructed hashing functions for all predefined spatial relationship codes. By applying the constructed hashing functions, the most similar pictures in the database satisfying a specified query can be fast determined. We can easily extend our mechanism for handling the case when some new spatial relationship codes are defined later for the considerations of refined spatial similarity retrieval under the maximum-likelihood measure criterion.\"",
        "Document: \"Mutual anonymity protocol with integrity protection for mobile peer-to-peer networks. A mobile peer-to-peer network (MOPNET) has drawn increasing attentions for its communication convenience and ease of resource sharing in an unfixed infrastructure. The essential requirements for applications in a MOPNET are to enforce trust of transmitted data and security of peers. However, most of mutual anonymity protocols for a MOPNET lack integrity protection, and hence, any peer might suffer from some potential security threats such as virus-infected or Trojan horse files. This paper proposes a mutual anonymity protocol with integrity protection for a MOPNET, in which the initiator and responder anonymously share data with each other, and ensure its integrity.\"",
        "Document: \"Cryptanalysis of Chang---Wu's group-oriented authentication and key exchange protocols. In 1998, Chang and Wu proposed a group-oriented authentication mechanism with key exchange [Computer Communications\u00a021 (1998) 485\u2013497]. The authors show that the Chang\u2013Wu protocols are vulnerable to the impersonation attack. They also give improvements that can not only eliminate the security flaw, but also reduce the redundant messages inherent in the original protocols.\"",
        "Document: \"Two RFID-based solutions for secure inpatient medication administration. Medication error can easily cause serious health damage to inpatients in hospital. Consequently, the whole society has to spend huge amount of extra resources for additional therapies and medication on those affected inpatients. In order to prevent medication errors, secure inpatient medication administration system is required in a hospital. Using RFID technology, such administration system provides automated medication verification for inpatient's medicine doses and generates corresponding medication evidence, which may be audited later for medical dispute. Recently, Peris-Lopez et al. (Int. J. Med. Inform., 2011) proposed an IS-RFID system to enhance inpatient medication safety. Nevertheless, IS-RFID system does not detect the denial of proof attack efficiently and the generated medication evidence cannot defend against counterfeit evidence generated from the hospital. That is, the hospital possesses enough privilege from the design of IS-RFID system to modify generated medication evidence whenever it is necessary. Hence, we design two lightweight RFID-based solutions for secure inpatient medication administration, one for online verification environment and the other for offline validation situation, to achieve system security on evidence generation and provide early detection on denial of proof attack.\"",
        "1 is \"A prototype on RFID and sensor networks for elder healthcare: progress report\", 2 is \"A practical and tightly secure signature scheme without hash function\"",
        "Given above information, for an author who has written the paper with the title \"New identity-based society oriented signature schemes from pairings on elliptic curves\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003833": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'New Insights Into the MVDR Beamformer in Room Acoustics':",
        "Document: \"A Variational EM Algorithm for the Separation of Time-Varying Convolutive Audio Mixtures. This paper addresses the problem of separating audio sources from time-varying convolutive mixtures. We propose a probabilistic framework based on the local complex-Gaussian model combined with non-negative matrix factorization. The time-varying mixing filters are modeled by a continuous temporal stochastic process. We present a variational expectation--maximization (VEM) algorithm that employs a Kalman smoother to estimate the time-varying mixing matrix, and that jointly estimate the source parameters. The sound sources are then separated by Wiener filters constructed with the estimators provided by the VEM algorithm. Extensive experiments on simulated data show that the proposed method outperforms a blockwise version of a state-of-the-art baseline method.\"",
        "Document: \"Dual-Microphone Speech Dereverberation using a Reference Signal. Speech signals recorded with a distant microphone usually contain reverberation, which degrades the fidelity and intelligibility of speech, and the recognition performance of automatic speech recognition systems. In this paper we propose a speech dereverberation system which uses two microphones. A Generalized Sidelobe Canceller (GSC) type of structure is used to enhance the desired speech signal. The GSC structure is used to create two signals. The first signal is the output of a standard delay and sum beamformer, and the second signal is a reference signal which is constructed such that the direct speech signal is blocked. We propose to utilize the reverberation which is present in the reference signal to enhance the output of the delay and sum beamformer. The power envelope of the reference signal and the power envelope of the output of the delay and sum beamformer are used to estimate the residual reverberation in the output of the delay and sum beamformer. The output of the delay and sum beamformer is then enhanced using a spectral enhancement technique. The proposed method only requires an estimate of the direction of arrival of the desired speech source. Experiments using simulated room impulse responses are presented and show significant reverberation reduction while keeping the speech distortion low.\"",
        "Document: \"Relative Transfer Function Identification Using Convolutive Transfer Function Approximation. In this paper, we present a relative transfer function (RTF) identification method for speech sources in reverberant environments. The proposed method is based on the convolutive transfer function (CTF) approximation, which enables to represent a linear convolution in the time domain as a linear convolution in the short-time Fourier transform (STFT) domain. Unlike the restrictive and commonly used multiplicative transfer function (MTF) approximation, which becomes more accurate when the length of a time frame increases relative to the length of the impulse response, the CTF approximation enables representation of long impulse responses using short time frames. We develop an unbiased RTF estimator that exploits the nonstationarity and presence probability of the speech signal and derive an analytic expression for the estimator variance. Experimental results show that the proposed method is advantageous compared to common RTF identification methods in various acoustic environments, especially when identifying long RTFs typical to real rooms.\"",
        "Document: \"A Bayesian Hierarchical Model for Speech Enhancement With Time-Varying Audio Channel. We present a fully Bayesian hierarchical approach for multichannel speech enhancement with time-varying audio channel. Our probabilistic approach relies on a Gaussian prior for the speech signal and a Gamma hyperprior for the speech precision, combined with a multichannel linear-Gaussian state-space model for the acoustic channel. Furthermore, we assume a Wishart prior for the noise precision matr...\"",
        "1 is \"A generalized subspace approach for enhancing speech corrupted by colored noise\", 2 is \"Low-complexity estimators for distributed sources\"",
        "Given above information, for an author who has written the paper with the title \"New Insights Into the MVDR Beamformer in Room Acoustics\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003912": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Visualizing simulated electrical fields from electroencephalography and transcranial electric brain stimulation: A comparative evaluation.':",
        "Document: \"Localizing the dermis/epidermis boundary in reflectance confocal microscopy images with a hybrid classification algorithm. Confocal reflectance microscopy is an emerging modality, for dermatology applications, especially for in-situ and bedside detection of skin cancers. As this technology gains acceptance, automated processing methods become increasingly important to develop. Since the dominant internal feature of the skin is the epidermis/dermis boundary, it has been chosen as the initial target for this development. This boundary is a complex corrugated 3D layer marked by optically subtle changes and features. Indeed, even trained clinicians in an attempt at validation of our early work, were unable to precisely and reliably locate the boundary within optical resolution. Thus here we propose to detect two boundaries, a lower boundary for the epidermis and an upper boundary for the dermis thus trapping the epidermis/dermis boundary. We use a novel combined segmentation/classification approach applied to z-sequences of tiles in the 3D stack. The approach employs a sequential classification on texture features, selected via on-line feature selection, to minimize the labeling required and to cope with high inter- and intra-subject variability and low optical contrast. Initial results indicate the ability of our approach to find these two boundaries successfully for most of the z-sequences from the stack.\"",
        "Document: \"A Sparse Nonnegative Demixing Algorithm With Intrinsic Regularization For Multiplexed Fluorescence Tomography. Fluorescence molecular tomography is becoming an important tool in preclinical biomedical imaging of small animals. However, the inability to perform high-throughput imaging of multiple fluorescent targets in bulk tissue remains a limitation. Recent work in our group suggests that joint measurement of spectral and temporal fluorophore data can enable robust identification (\"demixing\") and localization of at least four concurrent fluorophores. Here we present a novel demixing strategy for this data, which incorporates ideas from sparse subspace clustering and compressed sensing. It uses a suitable \"library\" of fluorophore signatures to compute a non-negative least-squares estimate of each fluorophore signal in the sample. The algorithm does not require a regularization parameter, even when the library is rank-deficient. In simulations, we simultaneously demixed four fluorophores with closely overlapping spectral and temporal profiles in a 25 mm diameter cross-sectional area with an RMS error of less than 3% per fluorophore.\"",
        "Document: \"An efficient region of interest acquisition method for dynamic magnetic resonance imaging. Motivated by work in the area of dynamic magnetic resonance imaging (MRI), we develop a new approach to the problem of reduced-order MRI acquisition. Efforts in this field have concentrated on the use of Fourier and singular value decomposition (SVD) methods to obtain low-order representations of an entire image plane. We augment this work to the case of imaging an arbitrarily-shaped region of interest (ROI) embedded within the full image. After developing a natural error metric for this problem, we show that determining the minimal order required to meet a prescribed error level is in general intractable, but can be solved under certain assumptions. We then develop an optimization approach to the related problem of minimizing the error for a given order. Finally, we demonstrate the utility of this approach and its advantages over existing Fourier and SVD methods on a number of MRI images\"",
        "Document: \"FAST REGULARIZED RECONSTRUCTION OF NON-UNIFORMLY SUBSAMPLED PARTIAL-FOURIER PARALLEL MRI DATA. ABSTRACT We present an adaptation of our previous fast, regularized parallel MRI reconstruction approach (LSQR-Hybrid) to en- compass,the reconstruction of partial-Fourier data. Recon- structions of partial-Fourier data require a constraint on the signal phase variation in the reconstructed image. Here, we employ a two-parameter Tikhonov regularization formulation to constrain the real and imaginary components,separately. A variation of the LSQR-Hybrid algorithm is used to rapidly reconstruct a set of images across the two-dimensional reg- ularization parameter search space. The \u201cbest\u201d image avail- able is chosen as the one with the least system error using the most regularization. Results are demonstrated,with subsam- pled parallel MR data acquired on a 1.5T MRI system. Index Terms\u2014Magnetic resonance imaging, Parallel\"",
        "1 is \"Feature-rich part-of-speech tagging with a cyclic dependency network\", 2 is \"Recurrent Models of Visual Attention.\"",
        "Given above information, for an author who has written the paper with the title \"Visualizing simulated electrical fields from electroencephalography and transcranial electric brain stimulation: A comparative evaluation.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003924": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'E-business intelligence via MCMP-based data mining methods':",
        "Document: \"Kimberlites Identification by Classification Methods. Kimberlites identification is a very important task for diamond mining. In traditional way, geologists draw upon past experience to do this work. Whether the bedrock should be drilled depends on their analysis of rock samples. This method has two disadvantages. First, as the database increasing, it becomes more difficult to do this work by manual inspection. Secondly, the accuracy is influenced by the expert's experience, and it reaches scarcely 80 percents averagely. So an analytical method to kimberlites identification over large geochemical datasets is demanded. This article applies two methods (SVM and decision tree) to a dataset provided by a mining company. Comparing the performances of these two methods, our results demonstrate that SVM is an effective method for this work.\"",
        "Document: \"Modelling The Mitigation Impact Of Insurance In Operational Risk Management. The paper is going to quantify the mitigation of the insurance as a risk mitigant in operational risk management for the commercial bank. Due to the uncertainties associated with the insurance policy, such as counterparty default, payment uncertainty and the liquidity risk (i.e., delayed payment), the recovery amounts are subjected to be kind of uncertainty. Thus, the efficiency of insurance as a risk mitigant may be discounted. We aim at going one step further to consider counterparty default, payment uncertainty and liquidity risk. While the counterparty default model focuses on calibration of the default time, the payment uncertainty is set as a non-increasing function depending on loss severity. The key conclusions are that counterparty default does not have significant impact on the capital calibration but still paramount in risk management, and insurance as a risk mitigant indeed improve the operational risk profile of bank and lower the capital requirement to some extent\"",
        "Document: \"Robust Unsupervised Feature Learning from Time-Series. Feature learning from unlabeled times series data is an important component in data analysis. Shaplets are discriminative sub-sequence of time series that can best predict target variable. Therefore, shaplets discovery is very important for analysis of time-series. Recently, based on optimization model, a novel approach has been proposed to learning shaplets. To make shaplets learning model more robust, in this paper, we propose a new unsupervised shaplets learning model with emphasizing joint l2,1norm minimization on both loss function and regularization, which can efficiently degenerate effect of noisy and outliers. The utilization of constraints can make the pseudo labels learned more accurate. Numeral experiments validate the effectiveness of the proposed method.\"",
        "Document: \"Supervised Object Boundary Detection Based on Structured Forests. Object boundary detection is an interesting and challenging topic in computer vision. Learning and combining the local, mid-level and high-level information play an important role in most of the recent approaches. However, few characteristics of a certain type of object are exploited. In this paper, we propose a novel supervised machine learning framework for object boundary detection, which makes use of the specific object features, such as boundary shape, directions and intensity. In the learning process, structured forest models are employed to tackle the high dimensional multi-class problem. Various experiment results show that our framework outperforms the competing models in the proposed data set, indicating that our framework is highly effective in modeling boundary for specific type of objects.\"",
        "1 is \"KBA: Kernel Boundary Alignment Considering Imbalanced Data Distribution\", 2 is \"Development of a Database Course for Bioinformatics.\"",
        "Given above information, for an author who has written the paper with the title \"E-business intelligence via MCMP-based data mining methods\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003941": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Quasi-orthogonal STBC with minimum decoding complexity: further results':",
        "Document: \"A Bayesian MMSE Turbo Receiver for Coded MIMO Systems. In conventional interference cancellation (IC)-based turbo receivers, extrinsic information (EXT) from the soft-input-soft-output (SISO) decoders is used to compute the statistical mean (SM) of the interfering signals. In this paper, we present a class of Bayesian minimum mean-square-error (MMSE) turbo receivers for coded multiple-input-multiple-output (MIMO) systems. Instead of using the EXT to estimate the prior SM, we use it in the posterior Bayesian MMSE estimation of the interfering signals. The estimation accuracy is, therefore, improved, leading to better bit-error-rate (BER) performance from our proposed Bayesian receivers. For the cases that we have studied, the proposed receiver with two iterations has a lower BER than the conventional one with five iterations, thus significantly reducing the receiver processing delay and the implementation complexity. Its superior performance is also demonstrated by the higher detector output mutual information and fewer iterations to achieve convergence, which is obtained from the extrinsic information transfer (EXIT) chart analysis.\"",
        "Document: \"Low Complexity Unitary Differential Space-Time Modulation with Spherical Code. We show a new design of unitary differential space- time modulation (DSTM) with low decoding complexity, i.e. its decoding can be performed by two parallel decoders where each of the decoder has a search space of only N for a codebook with N codewords. The design is based on Orthogonal Space-Time Block Code (O- STBC) and spherical code. We separate the symbols of an O-STBC into two groups and jointly modulate the symbols within a group using a joint constellation set constructed from spherical code. The proposed unitary DSTM scheme has a lower decoding complexity than many other DSTM schemes including those based on Group Codes and Sp(2) with better or comparable decoding performance. It also achieves a better performance than existing DSTM schemes with similar decoding comlexiy.\"",
        "Document: \"New blind frequency offset estimator for OFDM systems over frequency selective fading channels. In this paper, a blind frequency offset estimator for orthogonal frequency division multiplexing (OHAM) systems over frequency selective fading channels based on modeling the unknown channel fading gains as deterministic variables is proposed. In this estimator, the received time domain OFDM samples are first partitioned into subsets, in which neighboring samples are uncorrelated, leading to a tri-diagonal signal correlation matrix for each subset. The ML cost functions from each of the subsets are combined to perform frequency offset estimation. Simulation results show that the proposed frequency offset estimator achieves better performance than the estimators reported in [J.-J. Van De Beek, M. Sandell, P.O. Borjesson, ML estimation of time and frequency offset in OFDM systems, IEEE Trans. Signal Process. 45 (July 1997) 1800-1805] and [X. Ma, G.B. Giannakis, S. Barbarossa, Non-data-aided frequency-offset and channel estimation in OFDM: and related block transmission, IEEE ICC'01, June 2001, pp. 1866-1870]. Moreover, although the power delay profile needs to be known in deriving the proposed estimator, simulation and analytical results show that the performance of the proposed estimator is not sensitive to variation in the power delay profile.\"",
        "Document: \"Construction of quasi orthogonal STBC with minimum decoding complexity. In this paper, we formulate the algebraic structure of Quasi-Orthogonal STBC with minimum decoding complexity (MDC-QOSTBC), whose maximum likelihood (ML) decoder only requires the joint detection of two real symbols, for any numbers of transmit antennas. We also propose a systematic method to construct an MDC-QOSTBC from an Orthogonal-STBC (O- STBC). The maximum code rate of the resultant MDC-QOSTBC is shown to be the same as that of the lower-order O-STBC used to construct it. We also find the optimum constellation rotation angle for the proposed MDC-QOSTBC construction to achieve full diversity and optimum coding gain. We can show that the proposed MDC-QOSTBC has more even power distribution over the transmit antennas, better scalability in adjusting the number of transmit antennas, and more superior decoding performance than the Co-ordinate Interleaved Orthogonal Design (CIOD) and Asymmetric CIOD.\"",
        "1 is \"Binary Code-Division Multiple-Access Systems Operating in Multipath Fading, Noisy Channels\", 2 is \"The source-channel separation theorem revisited\"",
        "Given above information, for an author who has written the paper with the title \"Quasi-orthogonal STBC with minimum decoding complexity: further results\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003971": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Controllability/Observability Measures for Multiple-Valued Test Generation Based on D-Algorithm':",
        "Document: \"Fuzzy Logic Approach to 3D Magnetic Resonance Image Segmentation. This paper proposes an approach of fuzzy logic to 3D MR image segmentation. We show a fuzzy knowledge representation method to represent the knowledge needed to segment the target portions, and apply our method to 3D MR human brain image segmentation. In it we consider position knowledge, boundary surface knowledge and intensity knowledge. They are expressed by fuzzy if-then rules and compiled to a total degree as the measure of segmentation. The degree is evaluated in region growing technique and which segments the whole brain region into the left cerebral hemisphere, the right cerebral hemisphere, the cerebellum and the brain stem. The experimental result on 36 MR voxel data shows that our method extracted the portions precisely.\"",
        "Document: \"An ultrasonic thickness and wave speed determination system aided by fuzzy logic. This paper proposes an ultrasonic system to determine thickness and wave speed of an object. In it, the authors newly develop a triangle probe system composed of three probes: a central probe and two side probes. The central probe straight transmits/receives ultrasonic waves and the right probe receives ultrasonic waves transmitted from the left one. In the authors' method, the triangle probe is moved to the depth direction of an object. Then, two peak intensities are revealed at the focal points of the surface and at the bottom of the object. The authors show a method to calculate the wave speed and the thickness of the objects from two focal points. In the experiment, the authors extract the two focal points aided by fuzzy logic. Then, they determine the wave speed and thickness of the object from these focal points. The experimental results on two hard phantoms show that the method can successfully determine the wave speed and the thickness of the object with high accuracy.\"",
        "Document: \"A Rough Set-Based Clustering Method with Modification of Equivalence Relations. This paper presents a clustering method for nominal and numerical data based on rough set theory. We represent relative similarity between objects as a weighted sum of two types of distances: the Hamming distance for nominal data and the Mahalanobis distance for numerical data. On assigning initial equivalence relations to every object, modification of slightly different equivalence relations is performed to suppress excessive generation of categories. The optimal clustering result can be obtained by evaluating the cluster validity over all clusters generated with various values of similarity thresholds. After classification has been performed, features of each class are extracted based on the concept of value reduct. Experimental results on artificial data and amino acid data show that this method can deal well with both types of attributes.\"",
        "Document: \"Yuragi Synthesis For Ultrasonic Human Brain Imaging. This paper proposes YURAGI synthesis for brain imaging under the skull. The advantage of the proposed method over conventional methods is that, using YURACI synthesis, it is possible to obtain the effective results without image registration. Image registration is generally needed when more than two images are to be synthesized into one image. YURAGI synthesis does not need image registration; thus, its method is simpler than other methods that need image synthesis. The effectiveness of the proposed method was confirmed by comparing its error rate and accuracy with those of other methods. YURAGI leads the simple and energy-saving system with performing autoregulation. Autoregulation is utilized in many biological systems. In this study, YURAGI was applied to an ultrasound-based diagnostic medical imaging technique. The experimental results using YURAGI were superior to those using other methods. Thus, YURAGI is useful for visualizing the human brain.(1)\"",
        "1 is \"On implementing large binary tree architectures in VLSI and WSI\", 2 is \"Chaos in Neural Networks\"",
        "Given above information, for an author who has written the paper with the title \"Controllability/Observability Measures for Multiple-Valued Test Generation Based on D-Algorithm\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003999": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Domain-Specific Hybrid FPGA: Architecture and Floating Point Applications':",
        "Document: \"Hierarchical Benchmark Circuit Generation for FPGA Architecture Evaluation. We describe a stochastic circuit generator that can be used to automatically create benchmark circuits for use in FPGA architecture studies. The circuits consist of a hierarchy of interconnected modules, reflecting the structure of circuits designed using a system-on-chip design flow. Within each level of hierarchy, modules can be connected in a bus, star, or dataflow configuration. Our circuit generator is calibrated based on a careful study of existing system-on-chip circuits. We show that our benchmark circuits lead to more realistic architectural conclusions than circuits generated using previous generators.\"",
        "Document: \"A crosstalk-aware timing-driven router for FPGAs. As integrated circuits are migrated to more advanced technologies, it has become clear that crosstalk is an important physical phenomenon that must be taken into account. Crosstalk has primarily been a concern for ASICs, multi-chip modules, and custom chips, however, it will soon become a concern in FPGAs. In this paper, we describe the first published crosstalk-aware router that targets FPGAs. We show that, in a representative FPGA architecture implemented in a 0.18mm technology, the average routing delay in the presence of crosstalk can be reduced by 7.1% compared to a router with no knowledge of crosstalk. About half of this improvement is due to a tighter delay estimator, and half is due to an improved routing algorithm.\"",
        "Document: \"Deterministic Timing-Driven Parallel Placement by Simulated Annealing Using Half-Box Window Decomposition. As each generation of FPGAs grow in size, the run time of the associated CAD tools is rapidly increasing. Many past efforts have aimed at improving the CAD run time through parallelization of the placement algorithm. Wang and Lemieux presented an algorithm that is scalable, deterministic, timing-driven and achieves speedup over VPR [Wang and Lemieux FPGA'11]. This paper provides two significant alterations to Wang and Lemieux's algorithm, resulting in additional speedup and quality improvement. The first contribution is a new data decomposition scheme, called the half-box window technique, which achieves speedup by reducing the frequency of thread synchronization. The second contribution is the development of an improved annealing schedule, which further improves run time and slightly improves the quality of results. Together, these modifications achieve run time speedups of up to 70%. To put this in perspective, Wang and Lemieux required 25 threads to achieve best speedup, while this work requires only 16 threads. For a 10% degradation in quality, the new 16-thread algorithm achieves a 51x speedup over VPR, compared to a 35x speedup by the 25-thread original algorithm. Regarding quality, the best quality of results achieved by the new algorithm is a 5% degradation versus VPR, compared to a 8% degradation of the original Wang and Lemieux algorithm.\"",
        "Document: \"Simultaneous PVT-tolerant voltage-island formation and core placement for thousand-core platforms. In this paper, we propse a novel approach to voltage island formation and core placement for energy optimization in manycore architectures under parameter variation at pre-fabrication stage. We group the cores into irregular \"cloud-shaped\" valtage islands. The island are created by balancing the desire to limit the spatial extent of each island, to reduce PVT impact, with the communication patterns between islands. Compared to using rectangular islands, our approach leads to power improvements between 10 and 12%.\"",
        "1 is \"Fast motion vector estimation using multiresolution-spatio-temporal correlations\", 2 is \"Incremental distributed trigger insertion for efficient FPGA debug\"",
        "Given above information, for an author who has written the paper with the title \"Domain-Specific Hybrid FPGA: Architecture and Floating Point Applications\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004065": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A fuzzy approach to risk based decision making':",
        "Document: \"Trust prediction using Z-numbers and Artificial Neural Networks. Trust modeling of both the interacting parties in a virtual world, is a critical element of business intelligence. A key aspect in trust modeling is to be able to accurately predict the future trust value of an interacting party. In this paper, we propose an intelligent method for predicting the future trust value of a trusted entity. We propose the use of Z-number to represent both the trust value and its corresponding reliability. Subsequently, we apply Artificial Neural Network (ANN) to predict future trust values. We generate a large number of synthetic time series, with a view to model real-world trust values of trusted entity. We validate the working of our methodology using the generated time series.\"",
        "Document: \"Conjoint trust assessment for secure communication in cognitive radio networks. With the rapid development of wireless communication, the growth of Cognitive Radio (CR) is increasing day by day. Because CR is flexible and operates on the wireless network, there are more security threats to CR technology than to the traditional radio environment. In addition, there is no comprehensive framework for achieving security in Cognitive Radio Networks (CRNs), and the role of trust for achieving security in CRNs has not been explored previously. Security vulnerability in cognitive radio technology is unavoidable due to the intrinsic nature of the technology, so it is critical to ensure system security in CRNs. The issue of secure communication in CRNs thus becomes more important than it is in conventional wireless networks. In this paper, we propose a conjoint trust assessment approach (combining trust assessment from the Primary User Network and the Secondary User Network) in a CRN to solve the security threats brought about by untrustworthy entities, such as selfish, malicious, and faultless nodes, and to ensure secure spectrum sharing in CRNs. A numerical analysis shows the feasibility of our proposed approach.\"",
        "Document: \"Q-Contract Net: A Negotiation Protocol to Enable Quality-Based Negotiation in Digital Business Ecosystems. The Digital Business Ecosystem (DBE) is the result of the co-evolution of the Business Ecosystem and the Digital Ecosystem. There are numerous approaches and enabling technologies which are used in modeling open business marketplaces and, due to the similarities between the Digital Business environments, they can also help to enable the Digital Business Ecosystem but with some limitations. The complete lifecycle of the DBE can be decomposed into the following phases: formation, evolution and dissipation. In this work, our main focus is on the importance of negotiation in the DBE formation phase and especially on the structure of Contract Net Protocol. We will present an extension to the primitive Contract Net Protocol and name it Contract Net with Quality Protocol (CNQP or Q-Contract Net) to facilitate the negotiation process by adding the quality evaluation steps during the negotiation phase of the DBE formation.\"",
        "Document: \"Semantic client-side approach for web personalization of SaaS-based cloud services. The demand of software as a service SaaS-based services delivering computing resources as on-demand software is on the rise in the IT industry. However, one of the drawbacks of the existing SaaS services is that they offer limited or no personalization of the services provided according to the users profile. Personalization as mentioned in the literature has been a key driver in the adoption and usage of various applications and in providing better service experience to the users. However, overwhelming majority of such personalized services rely extensively on the server side, without embracing fast-developing client-side technologies. In SaaS-based cloud services, utilizing this technology is necessary considering their limited processing specifications. Approaches have been proposed in the literature that focus on cloud-based personalization using client-side technologies but none of them actually address all the different components that are required for a scalable and holistic personalization framework for SaaS-based cloud services. In this paper, we address this drawback by proposing a user-focussed personalization framework. Our proposed framework takes advantage of powerful client side browsers to reduce server overheads, ameliorate performance, establish high intelligence and enrich data processing. To validate and demonstrate the applicability of our framework, we build a prototype model and compare its performance against existing approaches using different metrics. Copyright \u00a9 2014 John Wiley & Sons, Ltd.\"",
        "1 is \"Fault-tolerant Service Level Agreement lifecycle management in clouds using actor system\", 2 is \"Team Situation Awareness Using Web-Based Fuzzy Group Decision Support Systems\"",
        "Given above information, for an author who has written the paper with the title \"A fuzzy approach to risk based decision making\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004097": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Efficient 2-D convolution algorithm with the single-data multiple kernel approach':",
        "Document: \"A de-blocking algorithm and a blockiness metric for highly compressed images. Blockiness is a typical artifact in reconstructed images that have been coded by a block-based discrete cosine transform (BDCT). In highly compressed images and video, the blocking artifacts are easily noticeable as the discontinuities between relatively homogeneous regions. Many current noniterative de-blocking algorithms attempt to remove the blocking artifacts by smoothing a few pixels around the block boundaries; however, the results are not satisfactory, especially at very low bit rates. We propose a de-blocking algorithm based on the number of connected blocks in a relatively homogeneous region, the magnitude of abrupt changes between neighboring blocks, and the quantization step size of DCT coefficients. Due to its adaptability, the proposed algorithm can smooth out the blocking artifacts while keeping the strong edges and texture areas untouched. Since this algorithm is noniterative and only identifies those block pairs that actually need de-blocking, its computation cost is low. In addition, we have developed a new metric to measure the blocking artifacts in images. Through analyzing the 2N-point (N is the block size) one-dimensional DCT coefficients of the two neighboring blocks with blocking artifacts, we show that all of the even DCT coefficients of the combined 2N points are zeros (except frequency k=0). The odd DCT coefficients are proportional to the pixel value difference between these two blocks with their magnitudes almost inversely proportional to frequency k. We selected the first DCT coefficient (frequency k=1) as an indicator for the strength of blocking artifacts in the reconstructed images. For the postprocessed images, we used a weighted summation of the squared first DCT coefficient to measure their blocking artifacts. Experimental results demonstrate that our proposed de-blocking algorithm produces better results than other methods, both visually and quantitatively, while the proposed blocking artifact metric is more consistent with subjective evaluation than the peak signal-to-noise ratio.\"",
        "Document: \"Correction of computed tomography motion artifacts using pixel-specific back-projection. Abstruct- Cardiac and respiratory motion can cause artifacts in computed tomography scans of the chest. We describe a new method for reducing these artifacts called pixel-specific back- projection (PSBP). PSBP reduces artifacts caused by in-plane motion by reconstructing each pixel in a frame of reference that moves with the in-plane motion in the volume being scanned. The motion of the frame of reference is specified by constructing maps that describe the motion of each pixel in the image at the time each projection was measured; these imaps are based on measurements of the in-plane motion. PSBP has been tested in computer simulations and with volunteer data. In computer simulations, PSBP removed the structured arti\u20acacts caused by motion. In scans of two volunteers, PSBP reduced doubling and streaking in chest scans to a level that made the images clinically useful. PSBP corrections of liver scans were lless satisfactory because the motion of the liver is predominantly superior-inferior (S-I). PSBP uses a unique set of motion parameters to describe the motion at each point in the chest as opposed to requiring that the motion be described by a single set of parameters. Therefore, PSBP may be more useful in correcting clinical scans than are other correction techniques previously described.\"",
        "Document: \"Automatic registration of high-resolution optical and SAR images based on an integrated intensity- and feature-based approach. Precise image-to-image registration is required to use multi-sensor data implementing a diversity of applications related with remote sensing. The purpose of this paper is to develop an automatic algorithm that co-registers high-resolution optical and SAR images based on an integrated intensity-and feature-based approach. As a pre-registration step, initial differences between the translation of the x and y directions between images were estimated with the Simulated Annealing optimization method using Mutual Information as an objective function. After the pre-registration, the line features were extracted to design a cost function that finds matching features based on the similarities of their locations and gradient orientations. Only one feature at each regular grid region having a minimum value of cost function was selected as a final matching point to extract the large number of well-distributed points. The final points were then used to construct a transformation combining the piecewise linear function with the affine transformation to increase the accuracy of the geometric correction.\"",
        "Document: \"MediaStation 5000: integrating video and audio. MediaStation 5000 is a highly integrated desktop multimedia system implemented on a single PC plug-in board. It performs multistandard compression, high-speed image processing, and fast 2D and 3D graphics functions. The Texas instruments Multimedia Video Processor (MVP), a single-chip multiprocessing device with a highly parallel internal architecture, provides the system's processing power and programmability.<>\"",
        "1 is \"MOGAC: a multiobjective genetic algorithm for hardware-software cosynthesis of distributed embedded systems\", 2 is \"A surface-based technique for warping three-dimensional images of the brain\"",
        "Given above information, for an author who has written the paper with the title \"Efficient 2-D convolution algorithm with the single-data multiple kernel approach\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004158": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Locating the functional and anatomical boundaries of human primary visual cortex.':",
        "Document: \"A Surface-based Analysis of Language Lateralization and Cortical Asymmetry. Among brain functions, language is one of the most lateralized. Cortical language areas are also some of the most asymmetrical in the brain. An open question is whether the asymmetry in function is linked to the asymmetry in anatomy. To address this question, we measured anatomical asymmetry in 34 participants shown with fMRI to have language dominance of the left hemisphere (LLD) and 21 participants shown to have atypical right hemisphere dominance (RLD). All participants were healthy and left-handed, and most (80%) were female. Gray matter (GM) volume asymmetry was measured using an automated surface-based technique in both ROIs and exploratory analyses. In the ROI analysis, a significant difference between LLD and RLD was found in the insula. No differences were found in planum temporale (PT), pars opercularis (POp), pars triangularis (PTr), or Heschl's gyrus (HG). The PT, POp, insula, and HG were all significantly left lateralized in both LLD and RLD participants. Both the positive and negative ROI findings replicate a previous study using manually labeled ROIs in a different cohort [Keller, S. S., Roberts, N., Garcia-Finana, M., Mohammadi, S., Ringelstein, E. B., Knecht, S., et al. Can the language-dominant hemisphere be predicted by brain anatomy? Journal of Cognitive Neuroscience, 23, 2013\u20132029, 2011]. The exploratory analysis was accomplished using a new surface-based registration that aligns cortical folding patterns across both subject and hemisphere. A small but significant cluster was found in the superior temporal gyrus that overlapped with the PT. A cluster was also found in the ventral occipitotemporal cortex corresponding to the visual word recognition area. The surface-based analysis also makes it possible to disentangle the effects of GM volume, thickness, and surface area while removing the effects of curvature. For both the ROI and exploratory analyses, the difference between LLD and RLD volume laterality was most strong- y driven by differences in surface area and not cortical thickness. Overall, there were surprisingly few differences in GM volume asymmetry between LLD and RLD indicating that gross morphometric asymmetry is only subtly related to functional language laterality.\"",
        "Document: \"Active contours under topology control genus preserving level sets. We present a novel framework to exert topology control over a level set evolution. Level set methods offer several advantages over parametric active contours, in particular automated topological changes. In some applications, where some a priori knowledge of the target topology is available, topological changes may not be desirable. This is typically the case in biomedical image segmentation, where the topology of the target shape is prescribed by anatomical knowledge. However, topologically constrained evolutions often generate topological barriers that lead to large geometric inconsistencies. We introduce a topologically controlled level set framework that greatly alleviates this problem. Unlike existing work, our method allows connected components to merge, split or vanish under some specific conditions that ensure that no topological defects are generated. We demonstrate the strength of our method on a wide range of numerical experiments and illustrate its performance on the segmentation of cortical surfaces and blood vessels.\"",
        "Document: \"Anatomical atlas-guided diffuse optical tomography of brain activation. We describe a neuroimaging protocol that utilizes an anatomical atlas of the human head to guide diffuse optical tomography of human brain activation. The protocol is demonstrated by imaging the hemodynamic response to median-nerve stimulation in three healthy subjects, and comparing the images obtained using a head atlas with the images obtained using the subject-specific head anatomy. The results indicate that using the head atlas anatomy it is possible to reconstruct the location of the brain activation to the expected gyrus of the brain, in agreement with the results obtained with the subject-specific head anatomy. The benefits of this novel method derive from eliminating the need for subject-specific head anatomy and thus obviating the need for a subject-specific MRI to improve the anatomical interpretation of diffuse optical tomography images of brain activation.\"",
        "Document: \"Improved tractography alignment using combined volumetric and surface registration. Previously we introduced an automated high-dimensional non-linear registration framework, CVS, that combines volumetric and surface-based alignment to achieve robust and accurate correspondence in both cortical and sub-cortical regions (Postelnicu et al., 2009). In this paper we show that using CVS to compute cross-subject alignment from anatomical images, then applying the previously computed alignment to diffusion weighted MRI images, outperforms state-of-the-art techniques for computing cross-subject alignment directly from the DWI data itself. Specifically, we show that CVS outperforms the alignment component of TBSS in terms of degree-of-alignment of manually labeled tract models for the uncinate fasciculus, the inferior longitudinal fasciculus and the corticospinal tract. In addition, we compare linear alignment using FLIRT based on either fractional anisotropy or anatomical volumes across-subjects, and find a comparable effect. Together these results imply a clear advantage to aligning anatomy as opposed to lower resolution DWI data even when the final goal is diffusion analysis.\"",
        "1 is \"High-resolution diffusion tensor imaging and tractography of the human optic chiasm at 9.4 T.\", 2 is \"A computational approach for corner and vertex detection\"",
        "Given above information, for an author who has written the paper with the title \"Locating the functional and anatomical boundaries of human primary visual cortex.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004166": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Manifold splines with a single extraordinary point':",
        "Document: \"GEOMETRIC COMPRESSION USING RIEMANN SURFACE STRUCTURE. This paper introduces a theoretic result that shows any surface in 3 dimensional Euclidean space can be determined by its conformal factor and mean curvature uniquely up to rigid motions. This theorem disproves the common belief that surfaces have three functional freedoms and immediately shows that one third of geometric data can be saved without loss of information. The paper develops a practical algorithm to losslessly compress geometric surfaces based on Riemann surface structures. First we compute a global conformal parameterization of the surface. The surface can be segmented by holomorphic flows, where each segment can be conformally mapped to a rectangle on the parameter plane, which is guaranteed by circle-valued Morse theory. We construct a conformal geometry image for each segment, and record conformal factor and dihedral angle for each edge. In this way, we represent the surface using only two functions with canonical connectivity. We present the proofs of the theorems and the compression examples.\"",
        "Document: \"Genus zero surface conformal mapping and its application to brain surface mapping. We developed a general method for global conformal parameterizations based on the structure of the cohomology group of holomorphic one-forms for surfaces with or without boundaries (Gu and Yau, 2002), (Gu and Yau, 2003). For genus zero surfaces, our algorithm can find a unique mapping between any two genus zero manifolds by minimizing the harmonic energy of the map. In this paper, we apply the alg...\"",
        "Document: \"Recent Advances in Computational Conformal Geometry. Computational conformal geometry focuses on developing the computational methodologies on discrete surfaces to discover conformal geometric invariants. In this work, we briefly summarize the recent developments for methods and related applications in computational conformal geometry. There are two major approaches, holomorphic differentials and curvature flow. The holomorphic differential method is a linear method, which is more efficient and robust to triangulations with lower quality. The curvature flow method is nonlinear and requires higher quality triangulations, but more flexible. The conformal geometric methods have been broadly applied in many engineering fields, such as computer graphics, vision, geometric modeling and medical imaging. The algorithms are robust for surfaces scanned from real life, general for surfaces with different topologies. The efficiency and efficacy of the algorithms are demonstrated by the experimental results.\"",
        "Document: \"Surface Classification Using Conformal Structures. 3D surface classification is a fundamental problem in computer vision and computational geometry. Surfaces can be classified by different transformation groups. Traditional classification methods mainly use topological transformation groups and Euclidean transformation groups. We introduce a novel method to classify surfaces by conformal transformation groups. Conformal equivalent class is refiner than topological equivalent class and coarser than isometric equivalent class, making it suitable for practical classification purposes. For general surfaces, the gradient fields of conformal maps form a vector space, which has a natural structure invariant under conformal transformations. We present an algorithm to compute this conformal structure, which can be represented as matrices, and use it to classify surfaces. The result is intrinsic to the geometry, invariant to triangulation and insensitive to resolution. To the best of our knowledge, this is the first paper to classify surfaces with arbitrary topologies by global conformal invariants. The method introduced here can also be used for surface matching problems.\"",
        "1 is \"Discrete surface Ricci flow: theory and applications\", 2 is \"Interactive example-based urban layout synthesis\"",
        "Given above information, for an author who has written the paper with the title \"Manifold splines with a single extraordinary point\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004173": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Performance analysis and adaptive call admission control in cellular mobile networks with time-varying traffic':",
        "Document: \"Optimal and Fair Rate Adaptation in Wireless Mesh Networks Based on Mathematical Programming and Game Theory. The authors have proposed a fair solution to optimal rate adaptation. The problem has been described in terms of the objective function, decision variables and constraints. Furthermore, using this problem definition, a rate adaptation heuristic was developed and divided into two sub-problems; part one requires finding the optimal rate allocation within the network, part two continues by finding a fair solution whilst still keeping an optimal rate allocation. The heuristic relies on cooperation in the network, and information regarding selected rates and loss due to interference, is distributed between neighbouring nodes. Furthermore, the heuristic is modelled as a repeated game with infinite horizon and it is shown how cooperation can be enforced. A Stack topology has been used for analysing and comparing performance and OMNeT++ 4.2.2 has been selected as simulation platform. The authors have shown that the heuristic effectively reduces the packet loss ratio (PLR). Thereafter, it was shown that the solution is both fair and optimal in terms of data rate.\"",
        "Document: \"Phase-type models for cellular networks supporting voice, video and data traffic. This paper presents an analytical model for cellular networks supporting voice, video and data traffic. Self-similar and bursty nature of the incoming traffic causes correlation in inter-arrival times of the incoming traffic. Therefore, arrival of calls is modeled with Markovian arrival process as it allows for the correlation. Call holding times, cell residence times and retrial times are modeled as phase-type distributions. We consider that the cells in a cellular network are statistically homogeneous, so it is enough to investigate a single cell for the performance analysis of the entire networks. With appropriate assumptions, the stochastic process that describes the state of a cell is a Quasi-birth-death (QBD) process. We derive explicit expressions for the infinitesimal generator matrix of this QBD process. Also, expressions for performance measures are obtained. Further, complexity involved in computing the steady-state probabilities is discussed. Finally, queueing examples are provided that can be obtained as particular cases of the proposed analytical model.\"",
        "Document: \"Adaptive dual-radio spectrum-sensing scheme in cognitive radio networks. In this paper, a novel spectrum-sensing scheme, called adaptive dual-radio spectrum-sensing scheme (ADRSS), is proposed for cognitive radio networks. In ADRSS, each secondary user (SU) is equipped with a dual radio. During the data transmission, with the received signal-to-noise ratio of primary user (PU) signal, the SU transmitter (SUT) and the SU receiver (SUR) are selected adaptively to sense one channel by one radio while communicating with each other by the other one. The sensing results of the SUR are sent to the SUT through feedback channels (e.g., ACK). After that, with the sensing results from the SUT or the SUR, the SUT can decide whether the channel switching should be carried out. The theoretical analysis and simulation results indicate that the normalized channel efficiency, defined as the expected ratio of time duration without interference to PUs in data transmission to the whole frame length, can be improved while satisfying the interference constraint to PUs. After that, an enhanced ADRSS is designed by integrating ADRSS with cooperative spectrum sensing, and the performance of ADRSS under imperfect feedback channel is also discussed. Copyright (c) 2011 John Wiley & Sons, Ltd.\"",
        "Document: \"An Analytical Model For Prioritized Contention Access In Ecma-368 Mac Protocol. The European Computer Manufacturers Association (ECMA) International recently defined the ECMA-368 standard, which specifies the physical and media access control (MAC) layers for Ultra Wideband (UWB) based wireless personal area networks (WPANs). The MAC protocol in ECMA-368 has a superframe structure. Each superframe is divided into three different time periods. One of them is the prioritized contention access (PCA) period which supports content ion-based access between different traffic classes. In this paper, we propose an analytical model to evaluate the performance of PCA in ECMA-368 MAC protocol. We assume that packets follow the Markovian Arrival Process (MAP) and various service times can be modeled by different phase type distributions (PHs). We apply the Matrix Geometric Method (MGM) technique and model the system as a MAP/PH/1 queueing system. We derive the probability mass function for the number of the packets in the queue. and the cumulative distribution function for the packets' waiting time. The correctness of our proposed analytical model is validated via OPNET simulations.\"",
        "1 is \"Reality mining: sensing complex social systems\", 2 is \"Ally Friendly Jamming: How to Jam Your Enemy and Maintain Your Own Wireless Connectivity at the Same Time\"",
        "Given above information, for an author who has written the paper with the title \"Performance analysis and adaptive call admission control in cellular mobile networks with time-varying traffic\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004195": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A Combinatorial Approach for Key-Distribution in Wireless Sensor Networks':",
        "Document: \"Performance evaluation of hypermeshes and meshes with wormhole routing. The hypercube, which vas widely used in early multicomputers, has fallen out of favour to be replaced by the 2-dimensional mesh or torus in recent multicomputers. This move was mainly influenced by Daily's study that has shown that for an equal implementation cost in VLSI the low-dimensional high-diameter mesh or torus has superior performance characteristics to the higher-dimensional low-diameter hypercube. Common networks such as the mesh, torus, and hypercube are graph topologies where a channel connects exactly two nodes. This paper argues that hypergraph topologies, where a channel can connect more than two nodes, represent a potential candidate for future high-performance multicomputer networks. A comparative analysis of a regular multi-dimensional hypergraph, referred to as the Distributed Crossbar Switch hypermesh (DCSH), and the mesh shows that the DCSH provides better performance for equal implementation costs in various technologies (e.g. VLSI and multiple-chip technology).\"",
        "Document: \"An Efficient Path-Based Multicast Algorithm for Mesh Networks. This paper presents a new multicast path-based algorithm, referred to here as the Qualified Groups (QG for short), which can achieve a high degree of parallelism and low communication latency over a wide range of traffic loads in the mesh. The QG algorithm relies on a new approach that divides the destinations in a way that balances the traffic load on network channels during the propagation of the multicast message. Results from extensive simulations under a variety of working conditions confirm that the QG algorithm exhibits superior performance characteristics over those of some well-known existing algorithms, such as dual-path, multiple-path, and column-path algorithms.\"",
        "Document: \"Performance modelling of pipelined circuit switching in hypercubes with hot spot traffic. Pipelined Circuit Switching (PCS) has been suggested as an efficient switching method for supporting interprocessor communication in multicomputer networks due to its ability to preserve both communication performance and fault-tolerant demands in these networks. A number of studies have demonstrated that PCS can exhibit superior performance characteristics over Wormhole Switching (WS) under uniform traffic. However, the performance properties of PCS have not yet been thoroughly investigated in the presence of non-uniform traffic. Analytical model of PCS for common networks (e.g., hypercube) under the uniform traffic pattern has been reported in the literature. A non-uniform traffic model that has attracted much attention is the hot spot model which leads to extreme network congestion resulting in serious performance degradation due to the tree saturation phenomenon in the network. An analytical model for WS with hot spot traffic has been reported in the literature. However, to the best of our knowledge, there has not been reported any analytical model for PCS augmented with virtual channels in the presence of hot spot traffic. This paper proposes a model for this switching mechanism using new methods to calculate the probability of message header blocking and hot spot rates on channels. The model makes latency predictions that are in good agreement with those obtained through simulation experiments. An extensive performance comparison using the new analytical model reveals that PCS performs the same or in some occasions worse than WS in the presence of hot spot traffic.\"",
        "Document: \"Algorithmic construction of Hamiltonians in pyramids. The hierarchical nature and rich connectivity of the pyramid network have made it a desirable topology as both a hardware architecture and software structure to solve a number of important parallel applications, for example, in image processing and machine vision. Embedding of Hamiltonian path/cycle in a host network is of great importance in network graphs and has been widely studied in the past. This paper addresses the problem of embedding Hamiltonian paths/cycles in the pyramid network.\"",
        "1 is \"Broadcasting on meshes with wormhole routing\", 2 is \"WiCTP: a token-based access control protocol for wireless networks\"",
        "Given above information, for an author who has written the paper with the title \"A Combinatorial Approach for Key-Distribution in Wireless Sensor Networks\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004305": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'FPGA Architecture of Generalized Laguerre-Volterra MIMO Model for Neural Population Spiking Activities':",
        "Document: \"A novel network for nonlinear modeling of neural systems with arbitrary point-process inputs. This paper address the issue of nonlinear model estimation for neural systems with arbitrary point-process inputs using a novel network that is composed of a pre-processing stage of a Laguerre filter bank followed by a single hidden layer with polynomial activation functions. The nonlinear modeling problem for neural systems has been attempted thus far only with Poisson point-process inputs and using cross-correlation methods to estimate low-order nonlinearities. The specific contribution of this paper is the use of the described novel network to achieve practical estimation of the requisite nonlinear model in the case of arbitrary (i.e. non-Poisson) point-process inputs and high-order nonlinearities. The success of this approach has critical implications for the study of neuronal ensembles, for which nonlinear modeling has been hindered by the requirement of Poisson process inputs and by the presence of high-order nonlinearities. The proposed methodology yields accurate models even for short input-output data records and in the presence of considerable noise. The efficacy of this approach is demonstrated with computer-simulated examples having continuous output and point-process output, and with real data from the dentate gyrus of the hippocampus.\"",
        "Document: \"The role of topography in the transformation of spatiotemporal patterns by a large-scale, biologically realistic model of the rat dentate gyrus. A large-scale, biologically realistic, computational model of the rat hippocampus is being constructed to study the input-output transformation that the hippocampus performs. In the initial implementation, the layer II entorhinal cortex neurons, which provide the major input to the hippocampus, and the granule cells of the dentate gyrus, which receive the majority of the input, are modeled. In a previous work, the topography, or the wiring diagram, connecting these two populations had been derived and implemented. This paper explores the consequences of two features of the topography, the distribution of the axons and the size of the neurons' axon terminal fields. The topography converts streams of independently generated random Poisson trains into structured spatiotemporal patterns through spatiotemporal convergence achievable by overlapping axon terminal fields. Increasing the axon terminal field lengths allowed input to converge over larger regions of space resulting in granule activation across a greater area but did not increase the total activity as a function of time as the number of targets per input remained constant. Additional simulations demonstrated that the total distribution of spikes in space depends not on the distribution of the presynaptic axons but the distribution of the postsynaptic population. Analyzing spike counts emphasizes the importance of the postsynaptic distribution, but it ignores the fact that each individual input may be carrying unique information. Therefore, a metric should be created that relates and tracks individual inputs as they are propagated and integrated through hippocampus.\"",
        "Document: \"Nonlinear dynamic modeling of spike train transformations for hippocampal-cortical prostheses. One of the fundamental principles of cortical brain regions, including the hippocampus, is that information is represented in the ensemble firing of populations of neurons, i.e., spatio-temporal patterns of electrophysiological activity. The hippocampus has long been known to be responsible for the formation of declarative, or fact-based, memories. Damage to the hippocampus disrupts the propagatio...\"",
        "Document: \"Nonlinear dynamical modeling of human hippocampal CA3-CA1 functional connectivity for memory prostheses. This paper reports the first nonlinear dynamical model of human hippocampus for building memory prostheses. In this study, spike trains are recorded from the hippocampal CA3 and CA1 regions in epileptic patients performing two memory-dependent behavioral tasks. Using CA3 and CA1 spike trains as inputs and outputs respectively, second-order generalized Laguerre-Volterra models are built to capture the nonlinear dynamics underlying the spike train transformations. These models can predict the CA1 spike trains based on the CA3 spike train and thus be used as the computational basis of the hippocampal memory prosthesis.\"",
        "1 is \"Runtime Parallel Incremental Scheduling of DAGs\", 2 is \"A Threshold Equation For Action Potential Initiation\"",
        "Given above information, for an author who has written the paper with the title \"FPGA Architecture of Generalized Laguerre-Volterra MIMO Model for Neural Population Spiking Activities\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004337": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A Shadow-Like Task Migration Model Based on Context Semantics for Mobile and Pervasive Environments.':",
        "Document: \"On Traffic-Aware Partition and Aggregation in MapReduce for Big Data Applications. The MapReduce programming model simplifies large-scale data processing on commodity cluster by exploiting parallel map tasks and reduce tasks. Although many efforts have been made to improve the performance of MapReduce jobs, they ignore the network traffic generated in the shuffle phase, which plays a critical role in performance enhancement. Traditionally, a hash function is used to partition intermediate data among reduce tasks, which, however, is not traffic-efficient because network topology and data size associated with each key are not taken into consideration. In this paper, we study to reduce network traffic cost for a MapReduce job by designing a novel intermediate data partition scheme. Furthermore, we jointly consider the aggregator placement problem, where each aggregator can reduce merged traffic from multiple map tasks. A decomposition-based distributed algorithm is proposed to deal with the large-scale optimization problem for big data application and an online algorithm is also designed to adjust data partition and aggregation in a dynamic manner. Finally, extensive simulation results demonstrate that our proposals can significantly reduce network traffic cost under both offline and online cases.\"",
        "Document: \"HAT: history-based auto-tuning MapReduce in heterogeneous environments. In MapReduce model, a job is divided into a series of map tasks and reduce tasks. The execution time of the job is prolonged by some slow tasks seriously, especially in heterogeneous environments. To finish the slow tasks as soon as possible, current MapReduce schedulers launch a backup task on other nodes for each of the slow tasks. However, traditional MapReduce schedulers cannot detect slow tasks correctly since they cannot estimate the progress of tasks accurately (Hadoop home page http://hadoop.apache.org/ , 2011; Zaharia et al. in 8th USENIX symposium on operating systems design and implementation, ACM, New York, pp. 29---42, 2008). To solve this problem, this paper proposes a History-based Auto-Tuning (HAT) MapReduce scheduler, which calculates the progress of tasks accurately and adapts to the continuously varying environment automatically. HAT tunes the weight of each phase of a map task and a reduce task according to the value of them in history tasks and uses the accurate weights of the phases to calculate the progress of current tasks. Based on the accurate-calculated progress of tasks, HAT estimates the remaining time of tasks accurately and further launches backup tasks for the tasks that have the longest remaining time. Experimental results show that HAT can significantly improve the performance of MapReduce applications up to 37% compared with Hadoop and up to 16% compared with LATE scheduler.\"",
        "Document: \"Improved Resource Allocation Algorithms For Practical Image Encoding In A Ubiquitous Computing Environment. As a case study of the ubiquitous computing system, we have implemented a prototype for the JPEG encoding application. In order to achieve this eventual development in the real world, we studied resource allocation policies that can improve the overall performance of the system. In this paper, we consider those static and dynamic allocation approaches and then propose four different allocation algorithms. In particular, we extensively studied the dynamic allocation algorithms by exploring various cache policies which include disabled cache, unrestricted cache and restricted cache. Performance of these algorithms in large scale application scenario is also evaluated based on both the improved prototype and a simulation environment. The experimental results show a significant performance improvement achieved by the new proposed algorithms in terms of load balance, execution time, waiting time and execution efficiency.\"",
        "Document: \"Architecture-based design and optimization of genetic algorithms on multi- and many-core systems. A Genetic Algorithm (GA) is a heuristic to find exact or approximate solutions to optimization and search problems within an acceptable time. We discuss GAs from an architectural perspective, offering a general analysis of performance of GAs on multi-core CPUs and on many-core GPUs. Based on the widely used Parallel GA (PGA) schemes, we propose the best one for each architecture. More specifically, the Asynchronous Island scheme, Island/Master\u2013Slave Hierarchy PGA and Island/Cellular Hierarchy PGA are the best for multi-core, multi-socket multi-core and many-core architectures, respectively. Optimization approaches and rules based on a deep understanding of multi- and many-core architectures are also analyzed and proposed. Finally, the comparison of GA performance on multi-core and many-core architectures are discussed. Three real GA problems are used as benchmarks to evaluate our analysis and findings.\"",
        "1 is \"Modeling spatially correlated data in sensor networks\", 2 is \"Collaborative TCP sequence number inference attack: how to crack sequence number under a second\"",
        "Given above information, for an author who has written the paper with the title \"A Shadow-Like Task Migration Model Based on Context Semantics for Mobile and Pervasive Environments.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004402": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'The Center for Plasma Edge Simulation Workflow Requirements':",
        "Document: \"Exploring Scientific Workflow Provenance Using Hybrid Queries over Nested Data and Lineage Graphs. Existing approaches for representing the provenance of scientific workflow runs largely ignore computation models that work over structured data, including XML. Unlike models based on transformation semantics, these computation models often employ update semantics, in which only a portion of an incoming XML stream is modified by each workflow step. Applying conventional provenance approaches to such models results in provenance information that is either too coarse (e.g., stating that one version of an XML document depends entirely on a prior version) or potentially incorrect (e.g., stating that each element of an XML document depends on every element in a prior version). We describe a generic provenance model that naturally represents workflow runs involving processes that work over nested data collections and that employ update semantics. Moreover, we extend current query approaches to support our model, enabling queries to be posed not only over data lineage relationships, but also over versions of nested data structures produced during a workflow run. We show how hybrid queries can be expressed against our model using high-level query constructs and implemented efficiently over relational provenance storage schemes.\"",
        "Document: \"Yin & Yang: Demonstrating Complementary Provenance from noWorkflow & YesWorkflow. The noWorkflow and YesWorkflow toolkits both enable researchers to capture, store, query, and visualize the provenance of results produced by scripts that process scientific data. noWorkflow captures prospective provenance representing the program structure of Python scripts, and retrospective provenance representing key events observed during script execution. YesWorkflow captures prospective provenance declared through annotations in the comments of scripts, and supports key retrospective provenance queries by observing what files were used or produced by the script. We demonstrate how combining complementary information gathered by noWorkflow and YesWorkflow enables provenance queries and data lineage visualizations neither tool can provide on its own.\"",
        "Document: \"Merging Sets of Taxonomically Organized Data Using Concept Mappings under Uncertainty. We present a method for using aligned ontologies to merge taxonomically organized data sets that have apparently compatible schemas, but potentially different semantics for corresponding domains. We restrict the relationships involved in the alignment to basic set relations and disjunctions of these relations. A merged data set combines the domains of the source data set attributes, conforms to the observations reported in both data sets, and minimizes uncertainty introduced by ontology alignments. We find that even in very simple cases, merging data sets under this scenario is non-trivial. Reducing uncertainty introduced by the ontology alignments in combination with the data set observations often results in many possible merged data sets, which are managed using a possible worlds semantics. The primary contributions of this paper are a framework for representing aligned data sets and algorithms for merging data sets that report the presence and absence of taxonomically organized entities, including an efficient algorithm for a common data set merging scenario.\"",
        "Document: \"What Makes Scientific Workflows Scientific?. A scientific workflow is the description of a process for accomplishing a scientific objective, usually expressed in terms of tasks and their dependencies [5]. While workflows have a long history in the database community as well as in business process modeling (where they are also known as business workflows ), and despite some early works on scientific workflows [3,10], the area has only recently begun to fully flourish (e.g., see [1,2,9,7,4,11]). Similar to scientific data management which has different characteristics from traditional business data management [8], scientific workflows exhibit new challenges and opportunities that distinguish them from business workflows. We present an overview of these challenges and opportunities, covering a number of issues such as different models of computation, scalable data and process management, and data provenance and lineage handling in scientific workflows.\"",
        "1 is \"The predictability of data values\", 2 is \"Graph queries through datalog optimizations\"",
        "Given above information, for an author who has written the paper with the title \"The Center for Plasma Edge Simulation Workflow Requirements\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004406": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Parallel Computational Intelligence-Based Multi-Camera Surveillance System':",
        "Document: \"Automatic Schaeffer's gestures recognition system. Schaeffer's sign language consists of a reduced set of gestures designed to help children with autism or cognitive learning disabilities to develop adequate communication skills. Our automatic recognition system for Schaeffer's gesture language uses the information provided by an RGB-D camera to capture body motion and recognize gestures using dynamic time warping combined with k-nearest neighbors methods. The learning process is reinforced by the interaction with the proposed system that accelerates learning itself thus helping both children and educators. To demonstrate the validity of the system, a set of qualitative experiments with children were carried out. As a result, a system which is able to recognize a subset of 11 gestures of Schaeffer's sign language online was achieved.\"",
        "Document: \"Where Are We After Five Editions?: Robot Vision Challenge, a Competition that Evaluates Solutions for the Visual Place Classification Problem. This article describes the Robot Vision challenge, a competition that evaluates solutions for the visual place classification problem. Since its origin, this challenge has been proposed as a common benchmark where worldwide proposals are measured using a common overall score. Each new edition of the competition introduced novelties, both for the type of input data and subobjectives of the challenge. All the techniques used by the participants have been gathered up and published to make it accessible for future developments. The legacy of the Robot Vision challenge includes data sets, benchmarking techniques, and a wide experience in the place classification research that is reflected in this article.\"",
        "Document: \"Object recognition in noisy RGB-D data using GNG. Object recognition in 3D scenes is a research field in which there is intense activity guided by the problems related to the use of 3D point clouds. Some of these problems are influenced by the presence of noise in the cloud that reduces the effectiveness of a recognition process. This work proposes a method for dealing with the noise present in point clouds by applying the growing neural gas (GNG) network filtering algorithm. This method is able to represent the input data with the desired number of neurons while preserving the topology of the input space. The GNG obtained results which were compared with a Voxel grid filter to determine the efficacy of our approach. Moreover, since a stage of the recognition process includes the detection of keypoints in a cloud, we evaluated different keypoint detectors to determine which one produces the best results in the selected pipeline. Experiments show how the GNG method yields better recognition results than other filtering algorithms when noise is present.\"",
        "Document: \"Junction detection and grouping with probabilistic edge models and Bayesian A\u2217. In this paper, we propose and integrate two Bayesian methods, one of them for junction detection, and the other one for junction grouping. Our junction detection method relies on a probabilistic edge model and a log-likelihood test. Our junction grouping method relies on finding connecting paths between pairs of junctions. Path searching is performed by applying a Bayesian A\u2217 algorithm. Such algorithm uses both an intensity and geometric model for defining the rewards of a partial path and prunes those paths with low rewards. We have extended such a pruning with an additional rule which favors the stability of longer paths against shorter ones. We have tested experimentally the efficiency and robustness of the methods in an indoor image sequence.\"",
        "1 is \"Summarising contextual activity and detecting unusual inactivity in a supportive home environment\", 2 is \"Coarse-to-Fine vision-based localization by indexing scale-invariant features.\"",
        "Given above information, for an author who has written the paper with the title \"Parallel Computational Intelligence-Based Multi-Camera Surveillance System\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004414": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'The INFOMIX system for advanced integration of incomplete and inconsistent data':",
        "Document: \"A Generic Approach for Knowledge-Based Information-Site Selection.  With the advent of the World Wide Web,a vast number of heterogenous informationsources has become available. In order toaccess and process these data, suitable toolsand methods for building an information infrastructureare necessary. One task for thispurpose is the selection of relevant informationsources in automated query answering. \"",
        "Document: \"Data repair of inconsistent DL-programs. Nonmonotonic Description Logic (DL) programs support rule-based reasoning on top of Description Logic ontologies, using a well-defined query interface. However, the interaction of the rules and the ontology may cause inconsistency such that no answer set (i.e. model) exists. We thus consider repairing DL-programs, i.e., changing formulas to obtain consistency. Viewing the data part of the ontology as the source of inconsistency, we define program repairs and repair answer sets based on changes to it. We analyze the complexity of the notion, and we extend an algorithm for evaluating DL-programs to compute repair answer sets, under optional selection of preferred repairs. The extension involves a generalized ontology repair problem, in which the entailment and non-entailment of sets of queries with updates to the ontology must be achieved. While this is intractable in general, we identify for the Description Logic DL-LiteA some tractable classes of preferred repairs that are useful in practice.\"",
        "Document: \"OMiGA: an open minded grounding on-the-fly answer set solver. We present a new solver for Answer-Set Programs whose main features include grounding on-the-fly and readiness for use in solving distributed answer-set programs. The solver is implemented in Java and uses an underlying Rete network for propagation. Initial experimental results show the benefit of using Rete for this purpose, but also exhibit the need for learning in the presence of grounding on-the-fly.\"",
        "Document: \"Equivalences in Answer-Set Programming by Countermodels in the Logic of Here-and-There. In Answer-Set Programming different notions of equivalence, such as the prominent notions of strong and uniform equivalence, have been studied and characterized by various selections of models in the logic of Here-and-There (HT). For uniform equivalence however, correct characterizations in terms of HT-models can only be obtained for finite theories, respectively programs. In this paper, we show that a selection of countermodels in HT captures uniform equivalence also for infinite theories. This result is turned into coherent characterizations of the different notions of equivalence by countermodels, as well as by a mixture of HT-models and countermodels (so-called equivalence interpretations), which are lifted to first-order theories under a very general semantics given in terms of a quantified version of HT. We show that countermodels exhibit expedient properties like a simplified treatment of extended signatures, and provide further results for non-ground logic programs. In particular, uniform equivalence coincides under open and ordinary answer-set semantics, and for finite non-ground programs under these semantics, also the usual characterization of uniform equivalence in terms of maximal and total HT-models of the grounding is correct, even for infinite domains, when corresponding ground programs are infinite.\"",
        "1 is \"A federated architecture for information management\", 2 is \"Symbolic model checking: 1020 states and beyond\"",
        "Given above information, for an author who has written the paper with the title \"The INFOMIX system for advanced integration of incomplete and inconsistent data\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004418": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Real World Video Avatar: Transmission and Presentation of Human Figure':",
        "Document: \"Dexterous object manipulation based on collision response. We propose an approach to object manipulation. In the approach, the user's hand or fingers are represented by a set of interface points, the interaction force on each interface point is computed by the collision response computation algorithm, and the motion of the object is simulated based on the interaction force. We employed a fast collision response computation method to accommodate a large number of interaction points. Also, we devised a method for stable computation of object motion. We carried out experiments on interaction with models of varying complexity and demonstrated the feasibility of our approach for dexterous manipulations.\"",
        "Document: \"Five senses theatre project: Sharing experiences through bodily ultra-reality. The Five Senses Theatre project was established for the development of a basic technology that enables the user to relive a spatial motion of other persons as if the user him/her-self experienced it in person. This technology aims to duplicate a bodily experience performed in the real space and to pass it to the other person. In other words, the user experiences another person's body that moved in a real space as if the user moved in that real space. The spatial motion may be a walking tour to the world heritage, a legend run of a top athlete, and etc. More specifically, the system creates the sensation of a self-body motion without a voluntary motion of the user by providing physical motion to the real body. The sensation of self-body motion is generated by not only a visually induced vection but proprioceptive and tactile sensations of the body passively evoked. The Five Senses Theatre provides multisensory stimuli to the user to make use of the user's body as medium to project the valuable experience from others to the user's cognition. The research issues include the following: 1) Cognitive mechanism of passive stimulation perceived as an active motion sensation (pseudo agency), and body ownership transfer (virtual body). 2) Device development of the mutisensory display system, rendering/projection algorithms, and a lifelog data system.\"",
        "Document: \"Walking experience by real-scene optic flow with synchronized vibrations on feet. We developed a walking recording and experiencing system. For the recording we captured stereo motion images from two cameras attached to a person's forehead with synchronized data of ankles' accelerations. For the experiencing we presented 3-D motion images with binocular disparity on a head-mounted display and vibrations to user's feet. The vibration was made from a sound of shoes when a person walked. We found that users subjectively reported the 3-D motion images with synchronized foot vibrations elicited stronger feelings of walking, leg motion, footsteps, and tele-existence than without vibrations in Experiment 1. In Experiment 2, participants' self-localization drifted in the direction of virtual walking after experiencing other walker's visual sight with the synchronized foot vibrations. These results suggest that our walking experiencing system gave users somewhat active walking feelings.\"",
        "Document: \"A method for transformation of 3D space into Ukiyo-e composition. In 1739, Western perspective drawing reached Japan via China. Before then, Japanese drawing, known as Yamato-e, had depicted architectural space through parallel projection. This was true for the ukiyo-e compositions that were popular among the general public during the Edo Era as well (see Figure 1.a). For some time after perspective drawing reached Japan in 1739, ukiyo-e artists created ukiyo-e compositions that incorporated perspective drawing called uki-e (see Figure 1.b). However, this movement was short lived, and after 1800, ukiyo-e artists created compositions using their own type of structure that did not conform to perspective drawing. [Kuroda 17] [Oka 92] [Kishi 94] [Yokochi 95] Works from this time by artists such as Katsushika Hokusai and Utagawa Hiroshige also became influential in the West through the Japonism movement of the 1860s. Figure 2 typifies the style of ukiyo-e composition from the 1800s.\"",
        "1 is \"TeleHuman: effects of 3d perspective on gaze and pose estimation with a life-size cylindrical telepresence pod\", 2 is \"Cellular texture generation\"",
        "Given above information, for an author who has written the paper with the title \"Real World Video Avatar: Transmission and Presentation of Human Figure\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004553": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'An agent-based framework for integrating workflows and Web services':",
        "Document: \"Formalisations of Capabilities for BDI-Agents. Intentional agent systems are increasingly being used in a wide range of complex applications. Capabilities has recently been introduced into some of these systems as a software engineering mechanism to support modularity and reusability while still allowing meta-level reasoning. This paper presents possible formalisations of capabilities within the framework of beliefs, goals and intentions and indicates how capabilities can affect agent reasoning about its intentions. We define a style of agent commitment which we refer to as a self-aware agent which allows an agent to modify its goals and intentions as its capabilities change. We also indicate which aspects of the specification of a BDI interpreter are affected by the introduction of capabilities and give some indications of additional reasoning which could be incorporated into an agent system on the basis of both the theoretical analysis and the existing implementation.\"",
        "Document: \"A unified approach for debugging is-a structure and mappings in networked taxonomies. With the increased use of ontologies and ontology mappings in semantically-enabled applications such as ontology-based search and data integration, the issue of detecting and repairing defects in ontologies and ontology mappings has become increasingly important. These defects can lead to wrong or incomplete results for the applications.We propose a unified framework for debugging the is-a structure of and mappings between taxonomies, the most used kind of ontologies. We present theory and algorithms as well as an implemented system RepOSE, that supports a domain expert in detecting and repairing missing and wrong is-a relations and mappings. We also discuss two experiments performed by domain experts: an experiment on the Anatomy ontologies from the Ontology Alignment Evaluation Initiative, and a debugging session for the Swedish National Food Agency.Semantically-enabled applications need high quality ontologies and ontology mappings. One key aspect is the detection and removal of defects in the ontologies and ontology mappings. Our system RepOSE provides an environment that supports domain experts to deal with this issue. We have shown the usefulness of the approach in two experiments by detecting and repairing circa 200 and 30 defects, respectively.\"",
        "Document: \"A tool for evaluating ontology alignment strategies. Ontologies are an important technology for the Semantic Web. In different areas ontologies have already been developed and many of these ontologies contain overlapping information. Often we would therefore want to be able to use multiple ontologies. To obtain good results, we need to find the relationships between terms in the different ontologies, i.e. we need to align them. Currently, there exist a number of systems that support users in aligning ontologies, but not many comparative evaluations have been performed and there exists little support to perform such evaluations. However, the study of the properties, the evaluation and comparison of the alignment strategies and their combinations, would give us valuable insight in how the strategies could be used in the best way. In this paper we propose the KitAMO framework for comparative evaluation of ontology alignment strategies and their combinations and present our current implementation. We evaluate the implementation with respect to performance. We also illustrate how the system can be used to evaluate and compare alignment strategies and their combinations in terms of performance and quality of the proposed alignments. Further, we show how the results can be analyzed to obtain deeper insights into the properties of the strategies.\"",
        "Document: \"A System for Aligning Taxonomies and Debugging Taxonomies and Their Alignments. With the increased use of ontologies in semantically-enabled applications, the issues of debugging and aligning ontologies have become increasingly important. The quality of the results of such applications is directly dependent on the quality of the ontologies and mappings between the ontologies they employ. A key step towards achieving high quality ontologies and mappings is discovering and resolving modeling defects, e.g., wrong or missing relations and mappings. In this demonstration paper we present a system for aligning taxonomies, the most used kind of ontologies, and debugging taxonomies and their alignments, where ontology alignment is treated as a special kind of debugging.\"",
        "1 is \"XML, bioinformatics and data integration.\", 2 is \"User modelling for live help systems: initial results\"",
        "Given above information, for an author who has written the paper with the title \"An agent-based framework for integrating workflows and Web services\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004680": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Communication-aware face detection using noc architecture':",
        "Document: \"Facial asymmetry in frequency domain: the \"phase\" connection. Facial asymmetry has now been established as a useful biometric for human identification in the presence of expression variations ([1]). The current paper investigates an alternative representation of asymmetry in the frequency domain framework, and its significance in identification tasks in terms of the phase component of the frequency spectrum of an image. The importance of the latter in face reconstruction is well-known in the engineering literature ([2]) and this establishes a firm ground for the success of asymmetry as a potentially useful biometric. We also point out some useful implications of this connection and dual representation. Moreover, the frequency domain features are shown to be more robust to intra-personal distortions than the corresponding spatial measures and yield error rates as low as 4% on a dataset with images showing extreme expression variations.\"",
        "Document: \"Emergence of Selective Invariance in Hierarchical Feed Forward Networks. Many theories have emerged which investigate how in- variance is generated in hierarchical networks through sim- ple schemes such as max and mean pooling. The restriction to max/mean pooling in theoretical and empirical studies has diverted attention away from a more general way of generating invariance to nuisance transformations. We con- jecture that hierarchically building selective invariance (i.e. carefully choosing the range of the transformation to be in- variant to at each layer of a hierarchical network) is im- portant for pattern recognition. We utilize a novel pooling layer called adaptive pooling to find linear pooling weights within networks. These networks with the learnt pooling weights have performances on object categorization tasks that are comparable to max/mean pooling networks. In- terestingly, adaptive pooling can converge to mean pooling (when initialized with random pooling weights), find more general linear pooling schemes or even decide not to pool at all. We illustrate the general notion of selective invari- ance through object categorization experiments on large- scale datasets such as SVHN and ILSVRC 2012.\"",
        "Document: \"Unconstrained iris acquisition and recognition using COTS PTZ camera. Uniqueness of iris patterns among individuals has resulted in the ubiquity of iris recognition systems in virtual and physical spaces, at high security facilities around the globe. Traditional methods of acquiring iris patterns in commercial systems scan the iris when an individual is at a predetermined location in front of the scanner. Most state-of-the-art techniques for unconstrained iris acquisition in literature use expensive customequipment and are composed of amulticamera setup, which is bulky, expensive, and requires calibration. This paper investigates a method of unconstrained iris acquisition and recognition using a single commercial off-the-shelf (COTS) pan-tilt-zoom (PTZ) camera, that is compact and that reduces the cost of the final system, compared to other proposed hierarchical multicomponent systems. We employ state-of-the-art techniques for face detection and a robust eye detection scheme using active shape models for accurate landmark localization. Additionally, our system alleviates the need for any calibration stage prior to its use. We present results using a database of iris images captured using our system, while operating in an unconstrained acquisition mode at 1.5m standoff, yielding an iris diameter in the 150-200 pixels range.\"",
        "Document: \"Gaussian Mixture Models Based on the Frequency Spectra for Human Identification and Illumination Classification. The importance of Fourier domain phase in human face identification is well-established ([7]). It therefore seems natural that identification tools based on phase features should be very efficient. In this paper we introduce a model-based approach using Gaussian mixture models (GMM) based on phase for performing human identification. Identification is performed using a MAP estimate and we show that we are able to achieve misclassification error rates as low as 2% on a database with 65 individuals with extreme illumination variations. The proposed method is easily adaptable to deal with other distortions such as expressions and poses, and hence this establishes its robustness to intra-personal variations. Finally we demonstrate that GMM based on the Fourier domain magnitude is effective for illumination normalization, so that near perfect identificationis obtained using the reconstructed illumination-free images.\"",
        "1 is \"Deep Learning Face Representation by Joint Identification-Verification.\", 2 is \"A realistic variable voltage scheduling model for real-time applications\"",
        "Given above information, for an author who has written the paper with the title \"Communication-aware face detection using noc architecture\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004724": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'FrauDetector+: An Incremental Graph-Mining Approach for Efficient Fraudulent Phone Call Detection.':",
        "Document: \"Identifying MMORPG bots: a traffic analysis approach. MMORPGs have become extremely popular among network gamers. Despite their success, one of MMORPG's great- est challenges is the increasing use of game bots, i.e., auto- playing game clients. The use of game bots is considered unsportsmanlike and is therefore forbidden. To keep games in order, game police, played by actual human players, of- ten patrol game zones and question suspicious players. This practice, however, is labor-intensive and ineffective. To ad- dress this problem, we analyze the traffic generated by hu- man players vs. game bots and propose solutions to auto- matically identify game bots. Taking Ragnarok Online, one of the most popular MMOGs, as our subject, we study the traffic generated by mainstream game bots and human players. We find that their traffic is distinguishable by: 1) the regularity in the release time of client commands, 2) the trend and magnitude of traffic burstiness in multiple time scales, and 3) the sensitivity to network conditions. We propose four strategies and two in- tegrated schemes to identify bots. For our data sets, the conservative scheme completely avoids making false accusa- tions against bona fide players, while the progressive scheme tracks game bots down more aggressively. Finally, we show that the proposed methods are generalizable to other games and robust against counter-measures from bot developers.\"",
        "Document: \"On formal models for social verification. The introduction of the ESP Game and other Games With A Purpose (GWAP) has demonstrated the potential of human computation in solving AI-hard problems. In such systems, users are normally required to input answers for questions proposed by the system, e.g., descriptions about a picture or a song. Since users may bring up irrelevant inputs intentionally or carelessly, and often the system does not have \"correct\" answers, we have to rely on the users to verify answers from others. We call this kind of mutual verification of users' answers \"social verification.\" In this paper, we propose formal models for two fundamental social verification mechanisms, simultaneous verification and sequential verification, in human computation systems. By adopting a game-theoretic approach, we perform an equilibrium analysis which explains the effect of each verification mechanism on a system's outcome. Our analysis results show that sequential verification leads to a more diverse and descriptive set of outcomes than simultaneous verification, though the latter is stronger in ensuring the correctness of verified answers. Our experiments on Amazon Mechanical Turk, which asked users to input textual terms related to a word, confirmed our analysis results. We believe that our formal models for social verification mechanisms will provide a basis for the design of future human computation systems.\"",
        "Document: \"Secure multicast in dynamic environments. A secure multicast framework should only allow authorized members of a group to decrypt received messages; usually, one ''group key'' is shared by all approved members. However, this raises the problem of ''one affects all'', whereby the actions of one member affect the whole group. Many researchers have solved the problem by dividing a group into several subgroups, but most current solutions require key distribution centers to coordinate secure data communications between subgroups. We believe this is a constraint on network scalability. In this paper, we propose a novel framework to solve key management problems in multicast networks. Our contribution is threefold: (1) We exploit the ElGamal cryptosystem and propose a technique of key composition. (2) Using key composition with proxy cryptography, the key distribution centers used in secure multicast frameworks are eliminated. (3) For key composition, the framework is designed to resist node failures and support topology reconstruction, which makes it suitable for dynamic network environments. Without reducing the security or performance of proxy cryptography, we successfully eliminate the need for key distribution centers. Our analysis shows that the proposed framework is secure, and comparison with other similar frameworks demonstrates that it is efficient in terms of time and space complexity. In addition, the costs of most protocol operations are bounded by constants, regardless of a group's size and the number of branches of transit nodes.\"",
        "Document: \"Analysis of Area Revisitation Patterns in World of Warcarft. This paper analyzes area revisitation patterns in World of Warcraft (WoW). Online-game players roam a number of in-game areas while playing the game and revisit some of them with different personal reasons. To clarify this issue, we conduct a large-scale analysis using WoW access log collected for two years consisting of more than sixty thousand characters and have discovered four main groups of area revisitation patterns. We describe also in the paper how our findings can be utilized to support both game developers and players.\"",
        "1 is \"CLR: a collaborative location recommendation framework based on co-clustering\", 2 is \"THINC: a virtual display architecture for thin-client computing\"",
        "Given above information, for an author who has written the paper with the title \"FrauDetector+: An Incremental Graph-Mining Approach for Efficient Fraudulent Phone Call Detection.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004730": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Towards an immunity-based system for detecting masqueraders':",
        "Document: \"Three Key Challenges In Arm-Coms For Entrainment Effect Acceleration In Remote Communication. Remote communication systems, which are getting popular these days, allow us to enjoy the benefit of audio/video communication over the network. However, communication based on these systems is still not identical to face-to-face meetings. For example, open issues include lack of tele-presence, lack of entrainment in communication, etc. In order to tackle these issues, this study proposes an idea of remote individuals' connection through augmented tele-presence systems called ARM-COMS: ARm-supported eMbodied COmmunication Monitor System. ARM-COMS is composed of a tablet PC as an ICT (Information and Communication Technology) device and a desktop robotic arm which manipulates the tablet. Two types of modes, or intelligent tablet mode (IT-mode) and intelligent avatar mode (IA-mode), play a key role in ARM-COMS to implement the three functions; namely, autonomous positioning (AP), autonomous entrainment movement (AEM), and autonomous entrainment positioning (AEP). This paper proposes the basic concept of ARM-COMS to accelerate the entrainment effect in remote communication.\"",
        "Document: \"Development Of A Handshake Robot System Based On A Handshake Approaching Motion Model. Humans shake hands as greetings of a first meeting to show a feeling of closeness. A handshake is an embodied interaction involving a physical contact to make communication smooth. A natural handshake between a robot and a human would presumably have the same effect. In this paper, we analyzed mutual handshaking between humans with and without voice greetings and propose a handshake approaching motion model based on the analysis. In this model, a robot can generate handshake approaching motion that is acceptable to a human using second-order lag and dead time elements from the trajectory of a human hand. Furthermore, we developed a handshake robot system by applying the proposed handshake approaching motion model and demonstrated the effectiveness of the approaching handshaking motion model and the handshake robot system through sensory evaluation.\"",
        "Document: \"Analysis of pointing motions by introducing a joint model for supporting embodied large-surface presentation. The importance of utilizing the advantages of embodiment in presentation has often been pointed out. In recent years, because of the increase in the size of screens and displays, the realization of a large-surface presentation by arranging many screens and displays on the wall of a room is expected. In this study, we precisely analyzed pointing motions by introducing a joint model and in freestyle movement. First, we introduced a joint model and proposed a new model for the calculation of a joint center. Next, we calculated the joint centers of a shoulder, an elbow, and a wrist. Then, we performed a pointing experiment in front of a large-surface and analyzed the pointing motions.\"",
        "Document: \"A Speech-Driven Embodied Communication System Based On An Eye Gaze Model In Interaction-Activated Communication. Line-of-sight such as gaze and eye-contract plays an important role to enhance the embodied interaction and communication through avatars. In addition, many gaze models and communication systems with the line-of-sight using avatars have been proposed and developed. However, the gaze behaviors by generating the above-mentioned models are not considered to enhance the embodied interaction such as activated communication, because the models stochastically generate the eyeball movements based on the human gaze behavior. Therefore, we analyzed the interaction between the human gaze behavior and the activated communication by using line-of-sight measurement devices. Then, we proposed an eye gaze model based on the above-mentioned analysis. In this study, we develop an advanced avatar-mediated communication system in which the proposed eye gaze model is applied to speech-driven embodied entrainment characters called \"InterActor.\" This system generates the avatar's eyeball movements such as gaze and looking away based on the activated communication, and provides a communication environment wherein the embodied interaction is promoted. The effectiveness of the system is demonstrated by means of sensory evaluations of 24 pairs of subjects involved in avatar-mediated communication.\"",
        "1 is \"Mutual tests using immunity-based diagnostic mobile agents in distributed intrusion detection systems\", 2 is \"One-point calibration gaze tracking method\"",
        "Given above information, for an author who has written the paper with the title \"Towards an immunity-based system for detecting masqueraders\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004739": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Autonomous decentralised systems in web computing environment':",
        "Document: \"Stochastic voting algorithms for Web services group testing. This paper proposes a stochastic voting for testing a large number of Web services (WS) under group testing. In the future, a large number of WS would be available and they need to be tested and evaluated in real time. While numerous test input generation techniques are available to generate test inputs, the oracle or the expected output of these test input is often difficult to obtain. One way to obtain the oracle in this case is to give the same input to multiple WS and to establish the oracle by a majority voting. This is based on the assumption that faulty WS often would not produce consistent results, and thus if a majority can be reached, the oracle can be established statistically. However, even correct WS may still produce slightly different outputs, and thus the majority-voting scheme must be carefully designed to distinguish correct but slightly variant output from truly incorrect output. This paper proposes a hierarchical classification based on simulated annealing and multi-dimensional Chi-square statistical techniques to analyze data to see if a majority can be reached. The algorithm is evaluated by a comprehensive simulated data as well as actual data. The data show that the proposed algorithm is effective even in a difficult situation where clusters of WS produce clusters of output.\"",
        "Document: \"A framework for contract-based collaborative verification and validation of web services. A key issue with Web Services (WS) is the verification and validation (V&V) of services to build trust between service providers and service users. This paper proposed a test-broker architecture so that all stakeholder within WS can contribute to improve the testing of the services. The test broker supports the submission, indexing, and querying of test artifacts such as test cases, defect reports and evaluations. It can also provide the services for the test generation, test coordination, and distributed testing services. The DCV&V (Decentralized, Collaborative, Verification and Validation) framework is proposed with a set of distributed and collaborated test brokers dedicated to different V&V tasks to enable scalable and flexible test collaborations. The paper explores the concept of design-by-contract and applies the principle to DCV&V. It identifies two categories of testing contracts including TSC (Testing Service Contracts) and TCC (Test Collaboration Contracts). It illustrates the application of TSC with contract-based test generation based on WS OWL-S specification. It elaborates TCC with the analysis of the test artifacts definitions.\"",
        "Document: \"QoS Enhancement for PDES Grid Based on Time Series Prediction. The combination of parallel/distributed discrete event simulation (PDES) and Grid technology is a new trend in simulation. QoS is important yet difficult in this process. With the special features of Gird architecture and PDES input, such as periodical and predictable inputs, we could enhance QoS with a PDES-specific prediction. A Grid-based framework is presented that is designed to help predict the performance of PDES. Based on this framework, a prediction algorithm using time series theory is presented in the context of large scale Grid simulations. Experiments are executed in the context of GridSim, which shows methods discussed before to help to improve QoS level.\"",
        "Document: \"Collaboration Policy Generation in Dynamic Collaborative SOA. Governance by policies is an important feature in Service-Oriented Architecture (SOA). This paper introduces a collaboration policy generation technique to govern dynamic collaboration in SOA systems. Dynamic collaboration allows two parties not knowing each other establish their collaboration protocol at runtime. Traditional SOA allows parties to collaborate but with a known collaboration protocol before execution. In dynamic collaboration, as collaboration protocols will be negotiated and determined at runtime, policies need to be generated at runtime to manage the interaction. This paper presents the concepts, architecture, and illustrative examples that demonstrate the dynamic collaboration policy generation technique.\"",
        "1 is \"Qos-Aware Object Replication In Overlay Networks\", 2 is \"A viewer for PostScript documents\"",
        "Given above information, for an author who has written the paper with the title \"Autonomous decentralised systems in web computing environment\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004765": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Mod-p Decision Diagrams: A Data Structure for Multiple-Valued Functions':",
        "Document: \"A Reconfigurable Arbiter PUF with 4 x 4 Switch Blocks. Physical Unclonable Functions (PUFs) exploit manufacturing process variation to create responses that are unique to individual integrated circuits (ICs). Typically responses of a PUF cannot be modified once the PUF is fabricated. In applications which use PUFs as a long-term secret key, it would be useful to have a simple mechanism for reconfiguring the PUF in order to update the key periodically. In this paper, we present a new type of arbiter PUFs which use 4 x 4 switch blocks instead of the conventional 2 x 2 ones. Each 4 x 4 switch block can be reconfigured in many different ways during the PUF's lifetime, making possible regular key updates.\"",
        "Document: \"Implementation of multiple-valued functions using literal-splitting technique. One of the main practical problems of implementing multiple-valued logic circuits is reduced noise margins. In this paper we show that this problem can be avoided by implementing multiple-valued functions using the following technique. First, the input domain of an m-valued n-variable function f is partitioned into m m-valued input binary-valued output characteristic functions. Then, each of these functions is treated as a function of n\u00b7m binary variables, representing the literals of the variables off. The fact that any multiple-valued function can be implemented that way is known. The main contribution of this paper is to show a technology, for which this type of implementation is natural and results in no hardware overhead.\"",
        "Document: \"A countermeasure against power analysis attacks for FSR-based stream ciphers. In this paper we analyze the power characteristics of Feedback Shift Registers (FSRs) and their e ect on FSR-based stream ciphers. We introduce a technique to isolate the switching activity of a stream cipher by equalizing the current drawn from the cipher with lower power overhead compared to previously introduced countermeasures. By re-implementing the Grain-80 and the Grain-128 ciphers with the presented approach, we lower their power consumption respectively by 20% and 25% compared to previously proposed countermeasures.\"",
        "Document: \"Double-Edge Transformation for Optimized Power Analysis Suppression Countermeasures. We introduce a power optimization technique for suppression countermeasures against Power Analysis attacks that can potentially be applied to any type of crypto-system implemented as a synchronous digital system. Since the power consumption of systems protected by suppression countermeasures is proportional to current peaks, we propose a simple transformation to move some of the switching activity of the crypto-system from the rising edge to the falling edge of the clock, so that current peaks are reduced. The transformation is easy to apply, requires only standard cell logic gates, has a low area overhead but can reduce the maximal working frequency of a system by at most a factor 2. We prove our method on an ASIC implementation of the Grain-80 stream cipher using SPICE-level simulation, obtaining 50% power savings compared to the non-optimized suppression countermeasure.\"",
        "1 is \"Abductive matchmaking using description logics\", 2 is \"Low-power sub-threshold design of secure physical unclonable functions\"",
        "Given above information, for an author who has written the paper with the title \"Mod-p Decision Diagrams: A Data Structure for Multiple-Valued Functions\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004786": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Mono-Modal Medical Image Registration With Coral Reef Optimization':",
        "Document: \"A novel framework to design fuzzy rule-based ensembles using diversity induction and evolutionary algorithms-based classifier selection and fusion. Fuzzy rule-based systems have shown a high capability of knowledge extraction and representation when modeling complex, non-linear classification problems. However, they suffer from the so-called curse of dimensionality when applied to high dimensional datasets, which consist of a large number of variables and/or examples. Multiclassification systems have shown to be a good approach to deal with this kind of problems. In this contribution, we propose an multiclassification system-based global framework allowing fuzzy rule-based systems to deal with high dimensional datasets avoiding the curse of dimensionality. Having this goal in mind, the proposed framework will incorporate several multiclassification system methodologies as well as evolutionary algorithms to design fuzzy rule-based multiclassification systems. The proposed framework follows a two-stage structure: 1) fuzzy rule-based multiclassification system design from classical and advanced multiclassification system design approaches, and 2) novel designs of evolutionary component classifier combination. By using our methodology, different fuzzy rule-based multiclassification systems can be designed dealing with several aspects such as improvement of the performance in terms of accuracy, and obtaining a good accuracy-complexity trade-off.\"",
        "Document: \"Incorporating Preferences to a Multi-objective Ant Colony Algorithm for Time and Space Assembly Line Balancing. We present an extension of a multi-objective algorithm based on Ant Colony Optimisation to solve a more realistic variant of a classical industrial problem: Time and Space Assembly Line Balancing. We study the influence of incorporating some domain knowledge by guiding the search process of the algorithm with preferences-based dominance. Our approach is compared with other techniques, and every algorithm tackles a real-world instance from a Nissan plant. We prove that the embedded expert knowledge is even more justified in a real-world problem.\"",
        "Document: \"Fuzzy Control of HVAC Systems Optimized by Genetic Algorithms. This paper presents the use of genetic algorithms to develop smartly tuned fuzzy logic controllers dedicated to the control of heating, ventilating and air conditioning systems concerning energy performance and indoor comfort requirements. This problem has some specific restrictions that make it very particular and complex because of the large time requirements existing due to the need of considering multiple criteria (which enlarges the solution search space) and to the long computation time models require to assess the accuracy of each individual.To solve these restrictions, a genetic tuning strategy considering an efficient multicriteria approach has been proposed. Several fuzzy logic controllers have been produced and tested in laboratory experiments in order to check the adequacy of such control and tuning technique. To do so, accurate models of the controlled buildings (two real test sites) have been provided by experts. Finally, simulations and real experiments were compared determining the effectiveness of the proposed strategy.\"",
        "Document: \"Craniofacial Superimposition Based on Genetic Algorithms and Fuzzy Location of Cephalometric Landmarks. Craniofacial superimposition is the second stage of a complex forensic technique that aims to identify a missing person from a photograph (or video shot) and the skull found. This specific task is devoted to find the most appropriate pose of the skull to be projected onto the photograph. The process is guided by a number of landmarks identified both in the skull (craniometric landmarks) and in the photograph (cephalometric landmarks). In this contribution we extend our previous genetic algorithm-based approach to the problem by considering the uncertainty involved in the location of the cephalometric landmarks. This proposal is tested over two real cases solved by the Physical Anthropology lab at the University of Granada (Spain).\"",
        "1 is \"Genetic fuzzy systems: taxonomy, current research trends and prospects\", 2 is \"A coevolutionary differential evolution with harmony search for reliability-redundancy optimization\"",
        "Given above information, for an author who has written the paper with the title \"Mono-Modal Medical Image Registration With Coral Reef Optimization\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004832": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Particle swarm optimizer for variable weighting in\u00a0clustering high-dimensional data':",
        "Document: \"Diagnostic Rules Discovery with Hierarchical Clustering and Focusing Mechanism Based on Rough Sets Theory. An approach is proposed to discover diagnostic rules from clinical databases. First, the diseases in the clinical database are clustered by their necessary characterization. Then focusing mechanism, which includes three processes: exclusion process, discrimination process and combining process, is exploited to derive diagnostic rules. The main characteristic feature of the approach is: 1) coverage is exploited to find necessary characterization of diseases during the exclusion process, while accuracy is exploited to find A -sufficient characterization of diseases during the discrimination process; 2) discrimination process can be executed among many diseases; 3) a series of classification information systems (CISs) derived by exclusion process from the original are considered; 4) the CISs are reducted to the simple ones; 5) crisp rules and uncertain rules can be conveniently derived. Finally, an example illustrates the approach and shows its effectiveness.\"",
        "Document: \"Human Head Modeling Based on an Improved Generic Model. With the development of computer graphics, virtual reality technology is becoming the focus of the research gradually. 3D face modeling technology, which is an important component of the virtual reality technology, has been used in many fields more and more widely. Comparing with the face modeling, the researches in the entire head modeling are less now. This paper proposed an approach to the construction of 3D human head models using an improved generic face model (Candide3) and some 2D human head images. Some simple human expressional controls are realized by adding the animation units (AUs) to the model.\"",
        "Document: \"Fuzzy cerebellar model articulation controller network optimization via self-adaptive global best harmony search algorithm. Fuzzy cerebellar model articulation controller (FCMAC) networks with excellent nonlinear appropriation ability and simple implementation are used to solve complex uncertainties problems in engineering applications. Both online and off-line learning algorithm of FCMAC networks usually applies the gradient-descent-type methods. However, such gradient-descent methods lead to the high possibility to converging into local minima. To cope with the local minimum problem, this paper alternatively proposes to apply harmony search algorithm to find optimal network parameters, so as to achieve better performances of FCMAC. The harmony search algorithm optimizes not only FCMAC network\u2019s weight variables, but also optimizes network receptive field\u2019s center position and standard deviation parameters. In order to obtain an optimal network, the weight values, center positions, and standard deviations are transformed to three data strings that can be processed by harmony search algorithm. In particular, the self-adaptive global best harmony search algorithm (SGHS) is used to search optimal parameter combinations of FCMAC within solution domains. The network\u2019s performances are verified by approximating six nonlinear formulae. In order to compare the performances of the FCMAC networks optimized by the SGHS algorithm, a back-propagation trained network and another harmony search variant optimized networks are also tested in this work. The experimental results show that the networks optimized by SGHS perform the faster convergence speed and better accuracy.\"",
        "Document: \"A CSP-Based Approach for Solving Parity Game. No matter from the theoretical or practical perspective, solving parity game plays a very important role. On one side, this problem possesses some amazing properties of computational complexity, and people are still searching for a polynomial time algorithm. On the other side, solving it and modal mu-calculus are almost the same in nature, so any efficient algorithm concerning this topic can be applied to model checking problem of modal mu-calculus. Considering the importance of modal mu-calculus in the automatic verification field, a series of model checkers will benefit from it. The main purpose of our study is to use constraints satisfaction problem (CSP), a deeply-studied and widely-accepted method, to settle parity game. The significance lies in that we can design efficient model checker through introducing various CSP algorithms, hence open a door to explore this problem of practical importance from a different viewpoint. In the paper, we propose a CSP-based algorithm and the related experimental results are presented.\"",
        "1 is \"A decision tree-based attribute weighting filter for naive Bayes\", 2 is \"TicTacToon: a paperless system for professional 2D animation\"",
        "Given above information, for an author who has written the paper with the title \"Particle swarm optimizer for variable weighting in\u00a0clustering high-dimensional data\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004840": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A facial expression recognition system using robust face features from depth videos and deep learning.':",
        "Document: \"A task-and-technique centered survey on visual analytics for deep learning model engineering. \u2022We survey recent works in visual analytics for deep learning engineering.\u2022We propose a task-and-technique taxonomy to classify such approaches.\u2022Our contribution surveys 40 papers covering multiple tasks and architectures.\u2022We aim to offer a guide to both visual analytics and machine learning experts.\u2022Finally, we outline challenges and topics for future research in the field.\"",
        "Document: \"A flexible on-chip evolution system implemented on a xilinx Virtex-II pro device. There have been introduced a number of systems with evolvable hardware on a single chip. To overcome the lack of flexibility in these systems, we propose a single-chip evolutionary system with the evolutionary algorithm implemented in software on a built-in processor. This architecture is implemented in a Xilinx Virtex-II Pro FPGA with an embedded PowerPC processor. This allows for a rapid processing of the time consuming parts in hardware and leaving other parts to more easily modifiable software. This platform will be beneficial for future work regarding both cost and compactness. Experiments have been performed on the physical device with software running in parallel with fitness computation in digital logic. The results show that the system uses only twice as much time when compared to a PC running at 10 times faster clock speed.\"",
        "Document: \"A facial expression recognition system using robust face features from depth videos and deep learning. \u2022A depth camera-based robust facial expression recognition (FER) system is proposed.\u2022Used deep learning method to extract salient features from depth faces.\u2022Eight directional strengths are obtained for each pixel in a depth image.\u2022Deep Belief Network is used to recognize facial expressions form depth images.\"",
        "Document: \"Recognizing Speed Limit Sign Numbers by Evolvable Hardware. An automatic traffic sign detection system would be important in a driver assistance system. In this paper, an approach for detecting numbers on speed limit signs is proposed. Such a system would have to provide a high recognition performance in real-time. Thus, in this paper we propose to apply evolvable hardware for the classification of the numbers extracted from images. The system is based on incremental evolution of digital logic gates. Experiments show that this is a very efficient approach.\"",
        "1 is \"Collaborative robotic instruction: A graph teaching experience\", 2 is \"Visualizing And Understanding Neural Machine Translation\"",
        "Given above information, for an author who has written the paper with the title \"A facial expression recognition system using robust face features from depth videos and deep learning.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004885": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Indexing for reuse of TV news shots':",
        "Document: \"Tag suggestion and localization in user-generated videos based on social knowledge. Nowadays, almost any web site that provides means for sharing user-generated multimedia content, like Flickr, Facebook, YouTube and Vimeo, has tagging functionalities to let users annotate the material that they want to share. The tags are then used to retrieve the uploaded content, and to ease browsing and exploration of these collections, e.g. using tag clouds. However, while tagging a single image is straightforward, and sites like Flickr and Facebook allow also to tag easily portions of the uploaded photos, tagging a video sequence is more cumbersome, so that users just tend to tag the overall content of a video. Moreover, the tagging process is completely manual, and often users tend to spend as few time as possible to annotate the material, resulting in a sparse annotation of the visual content. A semi-automatic process, that helps the users to tag a video sequence would improve the quality of annotations and thus the overall user experience. While research on image tagging has received a considerable attention in the latest years, there are still very few works that address the problem of automatically assigning tags to videos, locating them temporally within the video sequence. In this paper we present a system for video tag suggestion and temporal localization based on collective knowledge and visual similarity of frames. The algorithm suggests new tags that can be associated to a given keyframe exploiting the tags associated to videos and images uploaded to social sites like YouTube and Flickr and visual features.\"",
        "Document: \"Loki+Lire: a framework to create web-based multimedia search engines. In this paper we present Loki+Lire, a framework for the creation of web-based interfaces for search, annotation and presentation of multimedia data. The framework provides tools to ingest, transcode, present, annotate and index different types of media such as images, videos, audio files and textual documents. The front-end is compliant with the latest HTML5 standards, while the back-end allows system administrators to create processing pipelines that can be adapted for different tasks and purposes. The system has been developed in a modular way, aiming at creating loosely coupled components, letting developers to use it as a whole or to select only the parts that are needed to develop their own tools and systems.\"",
        "Document: \"Non-parametric anomaly detection exploiting space-time features. In this paper a real-time anomaly detection system for video streams is proposed. Spatio-temporal features are exploited to capture scene dynamic statistics together with appearance. Anomaly detection is performed in a non-parametric fashion, evaluating directly local descriptor statistics. A method to update scene statistics, to cope with scene changes that typically happen in real world settings, is also provided. The proposed method is tested on publicly available datasets.\"",
        "Document: \"Video Annotation with Pictorially Enriched Ontologies. Video annotation is typically performed by classifying video ele- ments according to some pre-defined ontology of the video content domain. Ontologies are defined by establishing relationships be- tween linguistic terms, that specify domain concepts at different abstraction levels. However, although linguistic terms are appro- priate to distinguish event and object categories, they are inade- quate when they must describe specific patterns of events or video entities. Instead, in these cases, pattern specifications are better ex- pressed through visual prototypes that capture the essence of the event or entity. Pictorially enriched ontologies, that include visual conceptstogether with linguistic keywords, are therefore needed to support video annotation up to the level of detail of pattern spec- ification. This paper presents pictorially enriched ontologies and provide a solution for their implementation in the soccer video do- main. The pictorially enriched ontology is used both to directly assign multimedia objects to concepts, providing a more meaning- ful definition than the linguistics terms, and to extend the initial knowledge of the domain, adding subclasses of highlights or new highlight classes that were not defined in the linguistic ontology. Automatic annotation of soccer clips up to the pattern specification level using a pictorially enriched ontology is discussed.\"",
        "1 is \"Self-Calibration of Rotating and Zooming Cameras\", 2 is \"Exploiting facial expressions for affective video summarisation\"",
        "Given above information, for an author who has written the paper with the title \"Indexing for reuse of TV news shots\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004886": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Chromatic Shadow Detection and Tracking for Moving Foreground Segmentation':",
        "Document: \"A 3d dynamic model of human actions for probabilistic image tracking. In this paper we present a method suitable to be used for human tracking as a temporal prior in a particle filtering framework such as CONDENSATION [5]. This method is for predicting feasible human postures given a reduced set of previous postures and will drastically reduce the number of particles needed to track a generic high-articulated object. Given a sequence of preceding postures, this example-driven transition model probabilistically matches the most likely postures from a database of human actions. Each action of the database is defined within a PCA-like space called UaSpace suitable to perform the probabilistic match when searching for similar sequences. So different, but feasible postures of the database become the new predicted poses.\"",
        "Document: \"Confidence Assessment On Eyelid And Eyebrow Expression Recognition. In this paper we address the recognition of subtle facial expressions by reasoning on the classification confidence. Psychological evidences have determined that eyelids and eyebrows are significant for the recognition of subtle facial expressions and the early perception of human emotions. This early perception results in a more complex problem, which requires a confidence assessment for any provided solution. Thus, traditional score-based classifiers (e.g. k-NN and NN) are not able to produce confident estimates. Instead, we first present five confidence estimators and a confidence classification assessment for Case-Based Reasoning (CBR). Second, we improve the expression retrieval from the database by learning the neighbourhood's dimensions for the expected classification confidences. Third, we reuse the previous classified expressions and the confidence assessment to improve the classification achieved by k-NN. Fourth, we improve the database for generalization with new subjects by learning thresholds to minimize misclassification with low confidence, maximize correct classifications with high confidence and re-arrange misclassification with high confidence. The proposed system represents an effective contribution for both subtle expression recognition and CBR methodology. It achieves an average recognition of 97% +/- 1% with a confidence of 96% +/- 2% for expressiveness between 20% and 100%.\"",
        "Document: \"Chalearn Looking At People Challenge 2014: Dataset And Results. This paper summarizes the ChaLearn Looking at People 2014 challenge data and the results obtained by the participants. The competition was split into three independent tracks: human pose recovery from RGB data, action and interaction recognition from RGB data sequences, and multi-modal gesture recognition from RGB-Depth sequences. For all the tracks, the goal was to perform user-independent recognition in sequences of continuous images using the overlapping Jaccard index as the evaluation measure. In this edition of the ChaLearn challenge, two large novel data sets were made publicly available and the Microsoft Codalab platform were used to manage the competition. Out-standing results were achieved in the three challenge tracks, with accuracy results of 0.20, 0.50, and 0.85 for pose recovery, action/interaction recognition, and multi-modal gesture recognition, respectively.\"",
        "Document: \"Automatic Keyframing of Human Actions for Computer Animation. This paper presents a novel human action model based on key-frames which is suitable for animation purposes. By defining an action as a sequence of time-ordered body posture configurations, we consider that the most characteristic postures (called key-frames) are enough for modeling such an action. As characteristic postures are found to correspond to low likelihood values, we build a human action eigenspace, called aSpace, which is used to estimate the likelihood value for each posture. Once the key-frames have been found automatically, they are used to build a human action model called p-action by means of interpolation between key-frames. This parameterized model represents the time evolution of the human body posture during a prototypical action, and it can be used for computer animation. As a result, realistic and smooth motion is achieved. Furthermore, realistic virtual sequences involving several actions can be automatically generated.\"",
        "1 is \"Fourier Active Appearance Models\", 2 is \"Model-Based Human Body Tracking\"",
        "Given above information, for an author who has written the paper with the title \"Chromatic Shadow Detection and Tracking for Moving Foreground Segmentation\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004918": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Optimal Power Allocation between Training and Data for MIMO Two-Way Relay Channels':",
        "Document: \"On the Effectiveness of Multiple Antennas in Distributed Detection over Fading MACs. A distributed detection problem over fading Gaussian multiple-access channels is considered. Sensors observe a phenomenon and transmit their observations to a fusion center using the amplify and forward scheme. The fusion center has multiple antennas with different channel models considered between the sensors and the fusion center, and different cases of channel state information are assumed at t...\"",
        "Document: \"Design of LDPC Codes for Two-Way Relay Systems with Physical-Layer Network Coding. This letter presents low-density parity-check (LDPC) code design for two-way relay (TWR) systems employing physical-layer network coding (PLNC). We focus on relay decoding, and propose an empirical density evolution method for estimating the decoding threshold of the LDPC code ensemble. We utilize the proposed method in conjunction with a random walk optimization procedure to obtain good LDPC code degree distributions. Numerical results demonstrate that the specifically designed LDPC codes can attain improvements of about 0.3 dB over off-the-shelf LDPC codes (designed for point-to-point additive white Gaussian noise channels), i.e., it is new code designs are essential to optimize the performance of TWR systems.\"",
        "Document: \"Estimation over fading channels with limited feedback using distributed sensing. We consider a wireless sensor network for distributed estimation over fading channels. The sensors transmit their observations over a multiple access fading channel to a fusion center (FC), where a source parameter is estimated. The sensor transmissions add incoherently over a multiple access channel, which motivates the need for channel knowledge at the sensors to improve performance. We consider the effects of different fading channel models on the performance of the system, and characterize the effect of different amounts of channel information at the sensors. We calculate the variance of the estimate for cases when both perfect, and differing amounts of partial channel information are available at the sensors. Asymptotic variance expressions for large number of sensors are derived for different channel statistics and feedback scenarios. We show that the degradation in variance due to using only channel phase information is at most a factor of 4/?? over Rayleigh fading channels. We consider continuous feedback error and evaluate the degradation in performance due to differing degrees of error. The loss in performance due to feedback quantization, and effect of error in feedback are also quantified. We also consider speed of convergence, and compare the rate of convergence under different conditions. Further, we characterize the effect of correlated channels between sensors and the FC, and provide the different values for the speed of convergence for this case. Simulation results are provided to show that only a few tens of sensors are required for the asymptotic results to hold. Numerical results corroborate our analytical results.\"",
        "Document: \"Distributed Detection Over Gaussian Multiple Access Channels With Constant Modulus Signaling. A distributed detection scheme where the sensors transmit with constant modulus signals over a Gaussian multiple access channel is considered. The deflection coefficient of the proposed scheme is shown to depend on the characteristic function of the sensing noise and the error exponent for the system is derived using large deviation theory. Optimization of the deflection coefficient and error exponent are considered with respect to a transmission phase parameter for a variety of sensing noise distributions including impulsive ones. The proposed scheme is also favorably compared with existing amplify-and-forward and detect-and-forward schemes. The effect of fading is shown to be detrimental to the detection performance through a reduction in the deflection coefficient depending on the fading statistics. Simulations corroborate that the deflection coefficient and error exponent can be effectively used to optimize the error probability for a wide variety of sensing noise distributions.\"",
        "1 is \"Preamble design for multiple-antenna OFDM-based WLANs with null subcarriers\", 2 is \"Parallel Sparse Spectral Clustering for SAR Image Segmentation\"",
        "Given above information, for an author who has written the paper with the title \"Optimal Power Allocation between Training and Data for MIMO Two-Way Relay Channels\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004920": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Mobile databases: a selection of open issues and research directions':",
        "Document: \"SWOOKI: A Peer-to-peer Semantic Wiki. In this paper, we propose to combine the advantages of se- mantic wikis and P2P wikis in order to design a peer-to-peer semantic wiki. The main challenge is how to merge wiki pages that embed se- mantic annotations. Merging algorithms used in P2P wiki systems have been designed for linear text and not for semantic data. In this paper, we evaluate two optimistic replication algorithms to build a P2P semantic wiki.\"",
        "Document: \"Collaborative practice-oriented business processes Creating a new case for business process management and CSCW synergy. In very recent times, organisations have started to shift their focus from highly standardised operational business processes (BPs) to other types of processes that cannot be easily replicated due to the knowledge, skills and creativity of people involved. Consequently the field of business process management (BPM) has gradually evolved to include four different, but equally important components: strategy, people, processes and technology. The renewed interest in process-related knowledge and collaboration has opened a new case for possible synergy of BPM and CSCW (computer supported cooperative work) fields. The paper argues that the key to this synergy is in the field of knowledge management. The paper introduces the knowledge dimension of BPs and uses it to determine how collaborative processes, in particular practice-oriented creative BPs, differ from other types of organizational processes. The paper argues that in the case of these BPs, process support needs to co-evolve with process execution itself, and therefore could be also considered as an ever evolving, ldquoorganicrdquo system, creating a new set of interesting research and practical challenges in the future.\"",
        "Document: \"CyCLaDEs: A Decentralized Cache for Triple Pattern Fragments. The Linked Data Fragment (LDF) approach promotes a new trade-off between performance and data availability for querying Linked Data. If data providers' HTTP caches plays a crucial role in LDF performances, LDF clients are also caching data during SPARQL query processing. Unfortunately, as these clients do not collaborate, they cannot take advantage of this large decentralized cache hosted by clients. In this paper, we propose CyCLaDEs an overlay network based on LDF fragments similarity. For each LDF client, CyCLaDEs builds a neighborhood of LDF clients hosting related fragments in their cache. During query processing, neighborhood cache is checked before requesting LDF server. Experimental results show that CyCLaDEs is able to handle a significant amount of LDF query processing and provide a more specialized cache on client-side.\"",
        "Document: \"COO-Transactions: Supporting Cooperative Work. Abstract. In COO, cooperation is a problem of synchronization among development activities running in parallel. We propose to encapsulate de-velopment activities within COO-transactions to ensure general proper-ties on their concurrent execution. We have developed a new correctness criterion called COO-serializability which ensures consistency property in a cooperative context. COO-serializability is purely syntactic criterion based on the log analysis of repository access. In this paper, we describe COO-serializability foundation and how to evaluate it incrementally us-ing the COO-protocol. Finally, we describe how we have implemented the COO-protocol in the COO environment. \"",
        "1 is \"Supporting cooperation in the Marvel process-centered SDE\", 2 is \"Stochastic comparisons: A methodology for the performance evaluation of fixed and mobile networks\"",
        "Given above information, for an author who has written the paper with the title \"Mobile databases: a selection of open issues and research directions\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005021": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Lightweight Distributed Execution Engine for Large-Scale Spatial Join Query Processing':",
        "Document: \"High-performance quadtree constructions on large-scale geospatial rasters using GPGPU parallel primitives. The increasingly available graphics processing units GPU hardware and the emerging general purpose computing on GPU GPGPU technologies provide an attractive solution to high-performance geospatial computing. In this study, we have proposed a parallel, primitive-based approach to quadtree construction by transforming a multidimensional geospatial computing problem into chaining a set of generic parallel primitives that are designed for one-dimensional 1D arrays. The proposed approach is largely data-independent and can be efficiently implemented on GPGPUs. Experiments on 4096*4096 and 16384*16384 raster tiles have shown that the implementation can complete the quadtree constructions in 13.33 ms and 250.75 ms, respectively, on average on an NVidia GPU device. Compared with an optimized serial CPU implementation based on the traditional recursive depth-first search DFS tree traversal schema that requires 1191.87 ms on 4096*4096 raster tiles, a significant speedup of nearly 90X has been observed. The performance of the GPU-based implementation also suggests that an indexing rate in the order of more than one billion raster cells per second can be achieved on commodity GPU devices.\"",
        "Document: \"GBD-Explorer: Extending open source java GIS for exploring ecoregion-based biodiversity data. Biodiversity and ecosystem data are both geo-referenced and \u201cspecies-referenced\u201d. Ecoregion classification systems are relevant to basic ecological research and have been increasingly used for making policy and management decisions. There are practical needs to integrate taxonomic data with ecoregion data in a GIS to visualize and explore species distribution conveniently. In this study, we represent the species distributed in an ecoregion as a taxonomic tree and extend the classic GIS data model to incorporate operations on taxonomic trees. A prototype called GBD-Explorer was developed on top of the open source JUMP GIS. We use the World Wildlife Fund (WWF) terrestrial ecoregion and WildFinder species databases as an example to demonstrate the rich capabilities implemented in the prototype.\"",
        "Document: \"Performance Evaluations of Geospatial Web Services Composition and Invocation. Geospatial data and analytical functions are essential to geospatial modeling. There are increasing interests in publishing both geospatial data and analytical functions as Web services and use them as the building blocks for domain specific geospatial modeling. While the advantages of using the Web services technologies have been well recognized and a number of prototype systems have been built to demonstrate the feasibility, very few performance evaluations have been reported in the previous studies. Compared with business data, geospatial data is rich in data types, large in data volumes and complex in semantics. On the other hand, the Web services technologies are known to have significant overheads with respects to deployment and invocation. The answers to how effective the Web services technologies can be, and, to what extent they are effective under the typical computation environments for geospatial modeling remain largely unknown. In this study we have set up an experimental system by deploying several geospatial Web services on top of popular commercial and open source spatial databases and geographical information systems (GIS). The Kepler scientific workflow system is used for geospatial Web services composition and invocation. We have conducted experiments to chain the geospatial Web services into a geospatial model under two data volume levels and two network settings. Our experiments show that the geospatial modeling using the Web services technologies remains effective in the wired LAN computation environment for data volume as large as 10000 points. However, the same data volume level incurs significant response lags under the wireless WAN computation environment. The experimental results may be used as a guideline for geospatial modeling using the Web services technologies when performances need to be taken into considerations.\"",
        "Document: \"Mobile Panoramic Vision For Assisting The Blind Via Indexing And Localization. In this paper, we propose a first-person localization and navigation system for helping blind and visually-impaired people navigate in indoor environments. The system consists of a mobile vision front end with a portable panoramic lens mounted on a smart phone, and a remote GPU-enabled server. Compact and effective omnidirectional video features are extracted and represented in the smart phone front end, and then transmitted to the server, where the features of an input image or a short video clip are used to search a database of an indoor environment via image-based indexing to find both the location and the orientation of the current view. To deal with the high computational cost in searching a large database for a realistic navigation application, data parallelism and task parallelism properties are identified in database indexing, and computation is accelerated by using multi-core CPUs and GPUs. Experiments on synthetic data and real data are carried out to demonstrate the capacity of the proposed system, with respect to real-time response and robustness.\"",
        "1 is \"Measuring social functions of city regions from large-scale taxi behaviors\", 2 is \"A Web page prediction model based on click-stream tree representation of user behavior\"",
        "Given above information, for an author who has written the paper with the title \"Lightweight Distributed Execution Engine for Large-Scale Spatial Join Query Processing\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005035": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'When 5G meets ICN: An ICN-based caching approach for mobile video in 5G networks.':",
        "Document: \"Efficient solution of the 3G network planning problem. The 3G universal mobile telecommunications system (UMTS) planning problem is combinatorially explosive and difficult to solve optimally, though solution methods exist for its three main subproblems (cell, access network, and core network planning). We previously formulated the entire problem as a single integrated mixed-integer linear program (MIP) and showed that only small instances of this global planning problem can be solved to optimality by a commercial MIP solver within a reasonable amount of time (St-Hilaire, Chamberland, & Pierre, 2006). Heuristic methods are needed for larger instances. This paper provides the first complete formulation for the heuristic sequential method (St-Hilaire, Chamberland, & Pierre, 2005) that re-partitions the global formulation into the three conventional subproblems and solves them in sequence using a MIP solver. This greatly improves the solution time, but at the expense of solution quality. We also develop a new hybrid heuristic that uses the results of the sequential method to generate constraints that provide tighter bounds for the global planning problem with the goal of reaching the provable optimum solution much more quickly. We empirically evaluate the speed and solution accuracy of four solution methods: (i) the direct MIP solution of the global planning problem, (ii) a local search heuristic applied to the global planning problem, (iii) the sequential method and (iv) the new hybrid method. The results show that the sequential method provides very good results in a fraction of the time needed for the direct MIP solution of the global problem, and that optimum results can be provided by the hybrid heuristic in greatly reduced time.\"",
        "Document: \"PRE-Fog: IoT trace based probabilistic resource estimation at Fog. Lately, pervasive and ubiquitous computing services have been under focus of not only the research community, but developers as well. Different devices generate different types of data with different frequencies. Emergency, healthcare, and latency sensitive services require real-time responses. Also, it is necessary to decide what type of data has to be uploaded to the cloud, without burdening the core network and the cloud. For this purpose, the cloud on the edge of the network, known as Fog or Micro Datacenter (MDC), plays an important role. Fog resides between the underlying Internet of Things (IoTs) and the mega datacenter cloud. Its purpose is to manage resources, perform data filtration, preprocessing, and security measures. To achieve this, Fog requires an effective and efficient resource management framework, which we propose in this paper. Fog has to deal with mobile nodes and IoTs, which involves objects and devices of different types having a fluctuating connectivity behavior. All such types of service customers have an unpredictable relinquish probability, since any object or device can stop using resources at any moment. In our proposed methodology for resource estimation and management through Fog computing, we take into account these factors and formulate resource management on the basis of fluctuating relinquish probability of the customer, service type, service price, and variance of the relinquish probability. With the intent of showing practical implications of our method, we implemented it on Crawdad real trace and Amazon EC2 pricing. Based on various services, differentiated through Amazon's price plans and historical record of Cloud Service Customers (CSCs), the model determines the amount of resources to be allocated. More loyal CSCs get better services, while for the contrary case, the provider reserves resources cautiously.\"",
        "Document: \"Uplink UMTS network design: an integrated approach. In this paper, we first propose a global approach for planning universal mobile telecommunications system (UMTS) networks in the uplink direction. Instead of partitioning the planning problem into several subproblems and solving them successively (sequential approach), we propose a mathematical programming model that addresses it as a whole. This global approach has the advantage of providing better results since, in general, optimal solutions to all subproblems do not provide an optimal solution to the global problem. In order to prove our point, we present a detailed example that compares the global and the sequential approaches. Next, we propose a local search heuristic to find \"good\" feasible solutions of the global model within a reasonable amount of time. Finally, numerical results for a set of randomly generated problems are presented. The results show that the heuristic produces solutions that are, on average, at 6.53% of the optimal solution, and in the worst case at 31.31% of the optimal solution.\"",
        "Document: \"Early Detection of DDoS Attacks Against Software Defined Network Controllers.  Software Defined Network (SDN) is a new network architecture that has an operating system. Unlike conventional production networks, SDN allows more flexibility in network management using that operating system that is called the controller. The main advantage of having a controller in the network is the separation of the forwarding and the control planes, which provides central control over the network. Although central control is the major advantage of SDN, it is also a single point of failure if it is made unreachable by a Distributed Denial of Service (DDoS) attack. In this paper, that single point of failure is addressed by utilizing the controller to detect such attacks and protect the SDN architecture of the network in its early stages. The two main objectives of this paper are to (1) make use of the controller\u2019s broad view of the network to detect DDoS attacks and (2) propose a solution that is effective and lightweight in terms of the resources that it uses. To accomplish these objectives, this paper examines the effect of DDoS attacks on the SDN controller and the way it can exhaust controller resources. The proposed solution to detect such attacks is based on the entropy variation of the destination IP address. Based on our experimental setup, the proposed method can detect DDoS within the first 250 packets of the attack traffic.\"",
        "1 is \"Handling multiple objectives with particle swarm optimization\", 2 is \"On Minimizing the Average Packet Decoding Delay in Wireless Network Coded Broadcast.\"",
        "Given above information, for an author who has written the paper with the title \"When 5G meets ICN: An ICN-based caching approach for mobile video in 5G networks.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005073": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Network-Constrained Support Vector Machine for Classification':",
        "Document: \"Robust learning-based annotation of medical radiographs. In this paper, we propose a learning-based algorithm for automatic medical image annotation based on sparse aggregation of learned local appearance cues, achieving high accuracy and robustness against severe diseases, imaging artifacts, occlusion, or missing data. The algorithm starts with a number of landmark detectors to collect local appearance cues throughout the image, which are subsequently verified by a group of learned sparse spatial configuration models. In most cases, a decision could already be made at this stage by simply aggregating the verified detections. For the remaining cases, an additional global appearance filtering step is employed to provide complementary information to make the final decision. This approach is evaluated on a large-scale chest radiograph view identification task, demonstrating an almost perfect performance of 99.98% for a posteroanterior/anteroposterior (PA-AP) and lateral view position identification task, compared with the recently reported large-scale result of only 98.2% [1]. Our approach also achieved the best accuracies for a three-class and a multi-class radiograph annotation task, when compared with other state of the art algorithms. Our algorithm has been integrated into an advanced image visualization workstation, enabling content-sensitive hanging-protocols and auto-invocation of a computer aided detection algorithm for PA-AP chest images.\"",
        "Document: \"View reconstruction from images by removing vehicles. Reconstructing views of real-world from satellite images, surveillance videos, or street view images is now a very popular problem, due to the broad usage of image data in Geographic Information Systems and Intelligent Transportation Systems. In this paper, we propose an approach that tries to replace the differences among images that are likely to be vehicles by the counterparts that are likely to be background. This method integrates the techniques for lane detection, vehicle detection, image subtraction and weighted voting, to regenerate the \"vehicle-clean\" images. The proposed approach can efficiently reveal the geographic background and preserve the privacy of vehicle owners. Experiments on surveillance images from TrafficLand.com and satellite view images have been conducted to demonstrate the effectiveness of the approach.\"",
        "Document: \"The CAM software for nonnegative blind source separation in R-Java. We describe a R-Java CAM (convex analysis of mixtures) package that provides comprehensive analytic functions and a graphic user interface (GUI) for blindly separating mixed nonnegative sources. This open-source multiplatform software implements recent and classic algorithms in the literature including Chan et al. (2008), Wang et al. (2010), Chen et al. (2011a) and Chen et al. (2011b). The CAM package offers several attractive features: (1) instead of using proprietary MATLAB, its analytic functions are written in R, which makes the codes more portable and easier to modify; (2) besides producing and plotting results in R, it also provides a Java GUI for automatic progress update and convenient visual monitoring; (3) multi-thread interactions between the R and Java modules are driven and integrated by a Java GUI, assuring that the whole CAM software runs responsively; (4) the package offers a simple mechanism to allow others to plug-in additional R-functions.\"",
        "Document: \"Information geometry of maximum partial likelihood estimation for channel equalization. Information geometry of partial likelihood is constructed and is used to derive the em-algorithm for learning parameters of a conditional distribution model through information-theoretic projections. To construct the coordinates of the information geometry, an expectation maximization (EM) framework is described for the distribution learning problem using the Gaussian mixture probability model. It is shown that the information-geometric em-algorithm is equivalent to EM to establish its convergence. The algorithm is applied to channel equalization by distribution learning and its rapid convergence characteristics are demonstrated through simulation studies.\"",
        "1 is \"Knowledge-based data analysis comes of age\", 2 is \"Differentially private network data release via structural inference\"",
        "Given above information, for an author who has written the paper with the title \"Network-Constrained Support Vector Machine for Classification\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005204": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'High-throughput Analysis of Large Microscopy Image Datasets on CPU-GPU Cluster Platforms.':",
        "Document: \"Relational Grid Monitoring Architecture (R-GMA). We describe R-GMA (Relational Grid Monitoring Architecture) which has been developed within the European DataGrid Project as a Grid Information and Monitoring System. Is is based on the GMA from GGF, which is a simple Consumer-Producer model. The special strength of this imple- mentation comes from the power of the relational model. We oer a global view of the information as if each Virtual Organisation had one large relational database. We provide a number of dierent Producer types with dierent characteristics; for example some support streaming of information. We also provide combined Consumer/Producers, which are able to combine information and repub- lish it. At the heart of the system is the mediator, which for any query is able to find and connect to the best Producers for the job. We have developed components to allow a measure of inter-working between MDS and R-GMA. We have used it both for information about the grid (primarily to find out about what services are available at any one time) and for application monitoring. R-GMA has been deployed in various testbeds; we describe some preliminary results and experiences of this deployment.\"",
        "Document: \"Understanding i/o performance using i/o skeletal applications. We address the difficulty involved in obtaining meaningful measurements of I/O performance in HPC applications, as well as the further challenge of understanding the causes of I/O bottlenecks in these applications. The need for I/O optimization is critical given the difficulty in scaling I/O to ever increasing numbers of processing cores. To address this need, we have pioneered a new approach to the analysis of I/O performance using automatic generation of I/O benchmark codes given a high-level description of an application's I/O pattern. By combining this with low-level characterization of the performance of the various components of the underlying I/O method we are able to produce a complete picture of the I/O behavior of an application. We compare the performance measurements obtained using Skel, the tool that implements our approach, with those of an instrumented version of the original application to show that our approach is accurate. We demonstrate the use of Skel to compare the performance of several I/O methods. Finally we show that the detailed breakdown of timing information produced by Skel provides better understanding of the reasons for the performance differences between the examined I/O methods. We conclude that our approach facilitates faster, more accurate and more meaningful I/O performance testing, allowing application I/O performance to be predicted, and new systems and I/O methods to be evaluated.\"",
        "Document: \"P-GRADE: A Grid Programming Environment. P-GRADE provides a high-level graphical environment to develop parallel applications transparently both for parallel systems and the Grid. P-GRADE supports the interactive execution of parallel programs as well as the creation of a Condor, Condor-G or Globus job to execute parallel programs in the Grid. In P-GRADE, the user can generate either PVM or MPI code according to the underlying Grid where the parallel application should be executed. PVM applications generated by P-GRADE can migrate between different Grid sites and as a result P-GRADE guarantees reliable, fault-tolerant parallel program execution in the Grid. The GRM/PROVE performance monitoring and visualisation toolset has been extended towards the Grid and connected to a general Grid monitor (Mercury) developed in the EU GridLab project. Using the Mercury/GRM/PROVE Grid application monitoring infrastructure any parallel application launched by P-GRADE can be remotely monitored and analysed at run time even if the application migrates among Grid sites. P-GRADE supports workflow definition and co-ordinated multi-job execution for the Grid. Such workflow management can provide parallel execution at both inter-job and intra-job level. Automatic checkpoint mechanism for parallel programs supports the migration of parallel jobs inside the workflow providing a fault-tolerant workflow execution mechanism. The paper describes all of these features of P-GRADE and their implementation concepts.\"",
        "Document: \"From computation models to models of provenance: the RWS approach. Scientific workflows often benefit from or even require advanced modeling constructs, e.g. nesting of subworkflows, cycles for executing loops, data-dependent routing, and pipelined execution. In such settings, an often overlooked aspect of provenance takes center stage: a suitable model of provenance (MoP) for scientific workflows should be based upon the underlying model of computation (MoC) used for executing the workflows. We can derive an adequate MoP from a MoC (such as Kahn's process networks) by taking into account the assumptions that a MoC entails, and by recording the observables which it affords. In this way, a MoP captures or at least better approximates \u2018real\u2019 data dependencies for workflows with advanced modeling constructs. As a specific instance, we elaborate on the Read\u2013Write\u2013ReSet model, a simple and flexible MoP suitable for a number of different MoCs. Copyright \u00a9 2007 John Wiley & Sons, Ltd.\"",
        "1 is \"Scalable Earthquake Simulation on Petascale Supercomputers\", 2 is \"Modeling of remote sensing image content using attributed relational graphs\"",
        "Given above information, for an author who has written the paper with the title \"High-throughput Analysis of Large Microscopy Image Datasets on CPU-GPU Cluster Platforms.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005208": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Data transmission and base-station placement for optimizing the lifetime of wireless sensor networks':",
        "Document: \"Scheduling Sensors for Guaranteed Sparse Coverage. Sensor networks are particularly applicable to the tracking of objects in\nmotion. For such applications, it may not necessary that the whole region be\ncovered by sensors as long as the uncovered region is not too large. This\nnotion has been formalized by Balasubramanian et.al. as the problem of\n$\\kappa$-weak coverage. This model of coverage provides guarantees about the\nregions in which the objects may move undetected. In this paper, we analyse the\ntheoretical aspects of the problem and provide guarantees about the lifetime\nachievable. We introduce a number of practical algorithms and analyse their\nsignificance. The main contribution is a novel linear programming based\nalgorithm which provides near-optimal lifetime. Through extensive\nexperimentation, we analyse the performance of these algorithms based on\nseveral parameters defined.\"",
        "Document: \"Localizing link failures in all-optical networks using monitoring tours. In this paper, we introduce the concept of monitoring tours (m-tours) to uniquely localize all possible failures up to k links in all-optical networks. We establish paths and cycles that can traverse the same link at most twice (forward and backward) and call them m-tours. An m-tour is different from other existing schemes such as m-cycle and m-trail, which traverse a link at most once. Closed (open) m-tours start and terminate at the same (distinct) monitor location(s). Each tour is constructed such that any shared risk linked group (SRLG) failure results in the failure of a unique combination of closed and open m-tours. We prove that k-edge connectivity is a sufficient condition to localize all SRLG failures with up to k-link failures when only one monitoring station is employed. We introduce an integer linear program (ILP) and a greedy scheme to find the monitoring locations to uniquely localize any SRLG failures with up to k links. We provide a heuristic scheme to compute m-tours for a given network. We demonstrate the validity of the proposed monitoring method through simulations. We show that our approach using m-tours significantly reduces the number of required monitoring locations compared to previously developed techniques.\"",
        "Document: \"Minimizing Average Path Cost in Colored Trees for Disjoint Multipath Routing. Multi-path routing (MPR) is an effective strategy to achieve robustness, load balancing, congestion reduction, and increased throughput by transmitting data over multiple paths. Disjoint multi-path routing (DMPR) requires the multiple paths to be link- or node-disjoint. Implementation of both MPR and DMPR poses significant challenges in obtaining loop-free multiple (disjoint) paths and effectively forwarding the data over the multiple paths, the latter being significant in data-gram networks. In this paper, we develop a disjoint multipath routing strategy using colored trees with an objective to minimize the total cost of the routing paths in a network. Two trees, namely red and blue, rooted at a given drain is formed. We demonstrate through extensive simulations that the developed technique is extremely effective in optimizing the average cost of the paths. In addition, we also observe that the developed approach minimizes the average minimum (minimum of the two paths) cost, which is lower than that obtained by earlier algorithms. The colored tree approach simply doubles the size of the routing table when two link- or node-disjoint paths to a specific node is needed.\"",
        "Document: \"Node clustering in wireless sensor networks: recent developments and deployment challenges. The large-scale deployment of wireless sensor networks (WSNs) and the need for data aggregation necessitate efficient organization of the network topology for the purpose of balancing the load and prolonging the network lifetime. Clustering has proven to be an effective approach for organizing the network into a connected hierarchy. In this article, we highlight the challenges in clustering a WSN, discuss the design rationale of the different clustering approaches, and classify the proposed approaches based on their objectives and design principles. We further discuss several key issues that affect the practical deployment of clustering techniques in sensor network applications.\"",
        "1 is \"Versatile low power media access for wireless sensor networks\", 2 is \"A note on relatives to the Held and Karp 1-tree problem\"",
        "Given above information, for an author who has written the paper with the title \"Data transmission and base-station placement for optimizing the lifetime of wireless sensor networks\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005219": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'How interacting pathways are regulated by miRNAs in breast cancer subtypes.':",
        "Document: \"An Assessment of Streaming Active Learning Strategies for Real-Life Credit Card Fraud Detection. Credit card fraud detection raises unique challenges due to the streaming, imbalanced, and non-stationary nature of transaction data. It additionally includes an active learning step, since the labeling (fraud or genuine) of a subset of transactions is obtained in near-real time by human investigators contacting the cardholders. These challenges and characteristics have traditionally been studied separately in the literature. In this paper, we investigate how previously proposed techniques can be combined to improve fraud detection accuracy. In particular, we highlight the existence of an exploitation/exploration tradeoff for active learning in the context of fraud detection, which has so far been overlooked in the literature. Relying on a real-world dataset of millions of transactions provided by our industrial partner Worldline, we performed an extensive experimental analysis in order to assess how traditional active learning strategies can be improved by using complementary machine learning techniques. We find that the baseline active learning strategy, denoted High Risk Querying, is a robust strategy, which can be further improved by combining it with Semi-Supervised learning.\"",
        "Document: \"Adaptive model selection for time series prediction in wireless sensor networks. In many practical applications of wireless sensor networks, the sensor nodes are required to report approximations of their readings at regular time intervals. For these applications, it has been shown that time series prediction techniques provide an effective way to reduce the communication effort while guaranteeing user-specified accuracy requirements on collected data. Achievable communication savings offered by time series prediction, however, strongly depend on the type of signal sensed, and in practice an inadequate a priori choice of a prediction model can lead to poor prediction performances. We propose in this paper the adaptive model selection algorithm, a lightweight, online algorithm that allows sensor nodes to autonomously determine a statistically good performing model among a set of candidate models. Experimental results obtained on the basis of 14 real-world sensor time series demonstrate the efficiency and versatility of the proposed framework in improving the communication savings.\"",
        "Document: \"A selecting-the-best method for budgeted model selection. The paper focuses on budgeted model selection, that is the selection between a set of alternative models when the ratio between the number of model assessments and the number of alternatives, though bigger than one, is low. We propose an approach based on the notion of probability of correct selection, a notion borrowed from the domain of Monte Carlo stochastic approximation. The idea is to estimate from data the probability that a greedy selection returns the best alternative and to define a sampling rule which maximises such quantity. Analytical results in the case of two alternatives are extended to a larger number of alternatives by using the Clark's approximation of the maximum of a set of random variables. Preliminary results on synthetic and real model selection tasks show that the technique is competititive with state-of-the-art algorithms, like the bandit UCB.\"",
        "Document: \"A review and comparison of strategies for multi-step ahead time series forecasting based on the NN5 forecasting competition. Multi-step ahead forecasting is still an open challenge in time series forecasting. Several approaches that deal with this complex problem have been proposed in the literature but an extensive comparison on a large number of tasks is still missing. This paper aims to fill this gap by reviewing existing strategies for multi-step ahead forecasting and comparing them in theoretical and practical terms. To attain such an objective, we performed a large scale comparison of these different strategies using a large experimental benchmark (namely the 111 series from the NN5 forecasting competition). In addition, we considered the effects of deseasonalization, input variable selection, and forecast combination on these strategies and on multi-step ahead forecasting at large. The following three findings appear to be consistently supported by the experimental results: Multiple-Output strategies are the best performing approaches, deseasonalization leads to uniformly improved forecast accuracy, and input selection is more effective when performed in conjunction with deseasonalization.\"",
        "1 is \"RDF: A Density-Based Outlier Detection Method using Vertical Data Representation\", 2 is \"Neutral Networks and Evolvability with Complex Genotype-Phenotype Mapping\"",
        "Given above information, for an author who has written the paper with the title \"How interacting pathways are regulated by miRNAs in breast cancer subtypes.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005353": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Guest Editorial: Analysis and Retrieval of Events/Actions and Workflows in Video Streams.':",
        "Document: \"A Cross Media Platform for Personalized Leisure & Entertainment: The POLYMNIA Approach. The POLYMNIA project aims to develop an intelligent cross-media platform for personalised leisure and entertainment in thematic parks or venues. The system allows the visitors to be the real protagonist in the venue. Towards this goal, POLYMNIA platform is equipped with innovative imaging technologies for real time detection, localisation and tracking of \u00ec human content\u00ee , i.e., the human visitor within the recoding being made in real-time by the system. No constraints are imposed on the variation of the environment. New, content-based media representation and organisation schemes will be developed to provide scalable, efficient and user-oriented description of the \u00ec human content\u00ee , enabling efficient retrieval, access, and delivery across heterogeneous media platforms. In addition, adaptive mechanisms are employed to update the system response to the current users\u00ed information needs and preferences.\"",
        "Document: \"A Novel Background Subtraction Scheme For In-Camera Acceleration In Thermal Imagery. Real-time segmentation of moving regions in image sequences is a very important task in numerous surveillance and monitoring applications. A common approach for such tasks is the \"background subtraction\" which tries to extract regions of interest from the image background for further processing or action; as a result its accuracy as well as its real-time performance is of great significance. In this work we utilize a novel scheme, designed and optimized for FPGA-based implementations, which models the intensities of each pixel as a mixture of Gaussian components; following a Bayesian approach, our method automatically estimates the number of Gaussian components as well as their parameters. Our novel system is based on an efficient and highly accurate on-line updating mechanism, which permits our system to be automatically adapted to dynamically changing operation conditions, while it avoids over/under fitting. We also present two reference implementations of our Background Subtraction Parallel System (BSPS) in Reconfigurable Hardware achieving both high performance as well as low power consumption; the presented FPGA-based systems significantly outperform a multi-core ARM and two multi-core low power Intel CPUs in terms of energy consumed per processed pixel as well as frames per second. Moreover, our low-cost, low-power devices allow for the implementation, for the first time, of a highly distributed surveillance system which will alleviate the main problems of the existing centralized approaches.\"",
        "Document: \"Unsupervised Clustering of Clickthrough Data for Automatic Annotation of Multimedia Content. Current low-level feature-based CBIR methods do not provide meaningful results on non-annotated content. On the other hand manual annotation is both time/money consuming and user-dependent. To address these problems in this paper we present an automatic annotation approach by clustering, in an unsupervised way, clickthrough data of search engines. In particular the query-log and the log of links the users clicked on are analyzed in order to extract and assign keywords to selected content. Content annotation is also accelerated by a carousel-like methodology. The proposed approach is feasible even for large sets of queries and features and theoretical results are verified in a controlled experiment, which shows that the method can effectively annotate multimedia files.\"",
        "Document: \"Precise 3D Measurements for Tracked Objects from Synchronized Stereo-Video Sequences. This paper presents a system suitable to perform precise and fast 3D measurements from synchronized stereo-video sequences and provide target's georeference in a known reference system. To this direction we combine a robust tracker with photogrammetric techniques into a fast and reliable system. For tracking objects and people, we adopt and modify a stable human tracker able to cope efficiently with the trade-off between model stability and adaptability. For achieving accurate and precise 3D measurements, camera calibration was implemented in order to recover the intrinsic parameters of the cameras of the configuration. Finally, for precise and reliable calculation of the 3D trajectory of the moving person, we apply bundle adjustment for all frames. Bundle adjustment is a very accurate algorithm and has the advantages of being tolerant of missing data while providing a true Maximum Likelihood estimate. The results have been tested and evaluated in real life conditions for proving the robustness and the accuracy of the system.\"",
        "1 is \"Decentralized M-ary detection via hierarchical binary decision fusion\", 2 is \"Geometrical and Statistical Properties of Systems of Linear Inequalities with Applications in Pattern Recognition\"",
        "Given above information, for an author who has written the paper with the title \"Guest Editorial: Analysis and Retrieval of Events/Actions and Workflows in Video Streams.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005367": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Real-Time Joint Tracking Of A Hand Manipulating An Object From Rgb-D Input':",
        "Document: \"Free-Form Gesture Authentication in the Wild. Free-form gesture passwords have been introduced as an alternative mobile authentication method. Text passwords are not very suitable for mobile interaction, and methods such as PINs and grid patterns sacrifice security over usability. However, little is known about how free-form gestures perform in the wild. We present the first field study (N=91) of mobile authentication using free-form gestures, with text passwords as a baseline. Our study leveraged Experience Sampling Methodology to increase ecological validity while maintaining control of the experiment. We found that, with gesture passwords, participants generated new passwords and authenticated faster with comparable memorability while being more willing to retry. Our analysis of the gesture password dataset indicated biases in user-chosen distribution tending towards common shapes. Our findings provide useful insights towards understanding mobile device authentication and gesture-based authentication.\n\n\"",
        "Document: \"Designing Mobile Interactions for the Ageing Populations. We are concurrently witnessing two significant shifts: mobiles are becoming the most used computing device; and older people are becoming the largest demographic group. However, despite the recent increase in related CHI publication, older adults continue to be underrepresented in HCI research as well as commercially, further widening the digital divide they face and hampering their social participation. This workshop aims to increase the momentum for such research within CHI and related fields such as gerontechnology. We plan to create a space for discussing and sharing principles and strategies to design and evaluate mobile user interfaces for the aging population. We thus welcome contributions to empirical studies, theories, design and evaluation of mobile interfaces for older adults.\"",
        "Document: \"WatchSense: On- and Above-Skin Input Sensing through a Wearable Depth Sensor. This paper contributes a novel sensing approach to support on- and above-skin finger input for interaction on the move. WatchSense uses a depth sensor embedded in a wearable device to expand the input space to neighboring areas of skin and the space above it. Our approach addresses challenging camera-based tracking conditions, such as oblique viewing angles and occlusions. It can accurately detect fingertips, their locations, and whether they are touching the skin or hovering above it. It extends previous work that supported either mid-air or multitouch input by simultaneously supporting both. We demonstrate feasibility with a compact, wearable prototype attached to a user's forearm (simulating an integrated depth sensor). Our prototype---which runs in real-time on consumer mobile devices---enables a 3D input space on the back of the hand. We evaluated the accuracy and robustness of the approach in a user study. We also show how WatchSense increases the expressiveness of input by interweaving mid-air and multitouch for several interactive applications.\"",
        "Document: \"Is motion capture-based biomechanical simulation valid for HCI studies?: study and implications. Motion-capture-based biomechanical simulation is a non-invasive analysis method that yields a rich description of posture, joint, and muscle activity in human movement. The method is presently gaining ground in sports, medicine, and industrial ergonomics, but it also bears great potential for studies in HCI where the physical ergonomics of a design is important. To make the method more broadly accessible, we study its predictive validity for movements and users typical to studies in HCI. We discuss the sources of error in biomechanical simulation and present results from two validation studies conducted with a state-of-the-art system. Study I tested aimed movements ranging from multitouch gestures to dancing, finding out that the critical limiting factor is the size of movement. Study II compared muscle activation predictions to surface-EMG recordings in a 3D pointing task. The data shows medium-to-high validity that is, however, constrained by some characteristics of the movement and the user. We draw concrete recommendations to practitioners and discuss challenges to developing the method further.\"",
        "1 is \"POSECUT: simultaneous segmentation and 3D pose estimation of humans using dynamic graph-cuts\", 2 is \"Imaginary phone: learning imaginary interfaces by transferring spatial memory from a familiar device\"",
        "Given above information, for an author who has written the paper with the title \"Real-Time Joint Tracking Of A Hand Manipulating An Object From Rgb-D Input\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005436": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'GBLinks: GNN-Based Beam Selection and Link Activation for Ultra-Dense D2D mmWave Networks':",
        "Document: \"Battery-aware routing for streaming data transmissions in wireless sensor networks. Recent technological advances have made it possible to support long lifetime and large volume streaming data transmissions in sensor networks. A major challenge is to maximize the lifetime of battery-powered sensors to support such transmissions. Battery, as the power provider of the sensors, therefore emerges as the key factor for achieving high performance in such applications. Recent study in battery technology reveals that the behavior of battery discharging is more complex than we used to think. Battery powered sensors might waste a huge amount of energy if we do not carefully schedule and budget their discharging. In this paper we study the effect of battery behavior on routing for streaming data transmissions in wireless sensor networks. We first give an on-line computable energy model to mathematically model battery discharge behavior. We show that the model can capture and describe battery behavior accurately at low computational complexity and thus is suitable for on-line battery capacity computation. Based on this battery model we then present a battery-aware routing (BAR) protocol to schedule the routing in wireless sensor networks. The routing protocol is sensitive to the battery status of routing nodes and avoids energy loss. We use the battery data from actual sensors to evaluate the performance of our protocol. The results show that the battery-aware protocol proposed in this paper performs well and can save a significant amount of energy compared to existing routing protocols for streaming data transmissions. The network lifetime is also prolonged with maximum data throughput. As far as we know; this is the first work considering battery-awareness with an accurate analytical on-line computable battery model in sensor network routing. We believe our battery model can be used to explore other energy efficient schemes for wireless networks as well.\"",
        "Document: \"Placement of Virtual Network Functions in Hybrid Data Center Networks. Hybrid data center networks (HDCNs), where each ToR switch is installed with a directional antenna, emerge as a candidate helping alleviate the over-subscription problem in traditional data centers. Meanwhile, as virtualization techniques develop rapidly, there is a trend that traditional network functions that are implemented in hardware will also be virtualized into virtual machines. However, ho...\"",
        "Document: \"Fair Caching Algorithms for Peer Data Sharing in Pervasive Edge Computing Environments. Edge devices (e.g., smartphones, tablets, connected vehicles, IoT nodes) with sensing, storage and communication resources are increasingly penetrating our environments. Many novel applications can be created when nearby peer edge devices share data. Caching can greatly improve the data availability, retrieval robustness and latency. In this paper, we study the unique issue of caching fairness in edge environment. Due to distinct ownership of peer devices, caching load balance is critical. We consider fairness metrics and formulate an integer linear programming problem, which is shown as summation of multiple Connected Facility Location (ConFL) problems. We propose an approximation algorithm leveraging an existing ConFL approximation algorithm, and prove that it preserves a 6.55 approximation ratio. We further develop a distributed algorithm where devices exchange data reachability and identify popular candidates as caching nodes. Extensive evaluation shows that compared with existing wireless network caching algorithms, our algorithms significantly improve data caching fairness, while keeping the contention induced latency similar to the best existing algorithms.\"",
        "Document: \"Relay and Power Splitting Ratio Selection for Cooperative Networks with Energy Harvesting. This paper addresses the problem of joint relay and power splitting ratio selection along with power allocation for an energy harvesting (EH) cooperative network, where the source and the relays can harvest energy from natural sources (e.g., solar) and radio frequency (RF) signals, respectively. To effectively use the harvested energy from the source, the relays employ the power splitting technique to scavenge energy from RF signals radiated by the source. We formulate this problem into a non-convex constrained optimization problem with the objective of maximizing system payoff, which is defined as the difference between system transmission benefit and system energy cost, and meanwhile minimizing system outage probability in both offline and online settings. In particular, we consider both direct transmission and relay transmission in this paper. Relay transmission is selected dynamically based on network channel conditions and available energy of EH nodes. Our simulation results reveal that considering direct transmission and selecting relay transmission and power splitting ratio dynamically can greatly improve system performance.\"",
        "1 is \"A pipeline-based approach for maximal-sized matching scheduling in input-buffered switches\", 2 is \"Reducing costs and pollution in cellular networks\"",
        "Given above information, for an author who has written the paper with the title \"GBLinks: GNN-Based Beam Selection and Link Activation for Ultra-Dense D2D mmWave Networks\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005489": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Total Energy Minimization of Real-Time Tasks in an On-Chip Multiprocessor Using Dynamic Voltage Scaling Efficiency Metric':",
        "Document: \"An Ant Colony Optimization Approach for the Preference-Based Shortest Path Search. In this paper, a modified ant colony system (ACS) algorithm is proposed to find a shortest path based on the preference of links. Most of the shortest path search algorithms aim at finding the distance or time shortest paths. However, these shortest paths are not surely an optimum path for the drivers who prefer choosing a less short, but more reliable or flexible path. For this reason, we propose the preference-based shortest path search algorithm which uses the properties of the links of the map. The properties of the links are specified by a set of data provided by the user of the car navigation system. The proposed algorithm was implemented in C and experiments were performed upon the map that includes 64 nodes with 118 links.\"",
        "Document: \"Improving Scalability of Replicated Services in Mobile Agent Systems. In this paper, we propose a strategy to improve scalability of replicated services in mobile agent systems by using an appropriate passive replication protocol for each replicated service according to whether the service is deterministic or non-deterministic. For this purpose, two passive replication protocols are introduced for nondeterministic and deterministic services respectively. They both allow visiting mobile agents to be forwarded to and execute their tasks on any node performing a service agent, not necessarily the primary agent. Additionally, in the second protocol for deterministic services, after a backup service agent has received each mobile agent request and obtained its delivery sequence number from the primary service agent, the backup is responsible for processing the request and coordinating with the other replica service agents. Therefore, our strategy using the two proposed protocols can promise better scalability of replicated services a large number of mobile agents attempt to access in mobile agent systems.\"",
        "Document: \"Broadcast Network-Based Sender Based Message Logging For Overcoming Multiple Failures. All the existing sender-based message logging (SBML) protocols share a well-known limitation that they cannot tolerate concurrent failures. In this paper, we analyze the cause for this limitation in a unicast network environment, and present an enhanced SBML protocol to overcome this shortcoming while preserving the strengths of SBML. When the processes on different nodes execute a distributed application together in a broadcast network, this new protocol replicates the log information of each message to volatile storages of other processes within the same broadcast network. It may reduce the communication overhead for the log replication by taking advantage of the broadcast nature of the network. Simulation results show our protocol performs better than the traditional one modified to tolerate concurrent failures in terms of failure-free execution time regardless of distributed application communication pattern.\"",
        "Document: \"Implementation and Performance Evaluation of Socket and RMI based Java Message Passing Systems. Recently, as speeds of computer processors and networks are rapidly increasing, a lot of researches are actively progressing to develop efficient and lightweight parallel computing platforms using heterogeneous and networked computers. According to this technical trend, this paper designs and implements a message passing library called JMPI(Java Message Passing Interface) which complies with MP], the MPI standard specification for Java language. This library provides some graphic user interface tools to enable parallel computing environments to be configured very simply by their administrators and JMPI applications to be executed very conveniently. Especially, it is implemented as two versions based on two typical distributed system communication mechanisms, Socket and RMI. According to these communication mechanisms, the performance of each message passing system is evaluated by measuring its processing speed with respect to the increasing number of computers by executing three well-known applications. Experimental results show that the most efficient processing speedup can be obtained by increasing the number of the computers in consideration of network traffics generated by applications.\"",
        "1 is \"A Transition Isolation Scan Cell Design for Low Shift and Capture Power\", 2 is \"A scalable synchronization protocol for large scale sensor networks and its applications\"",
        "Given above information, for an author who has written the paper with the title \"Total Energy Minimization of Real-Time Tasks in an On-Chip Multiprocessor Using Dynamic Voltage Scaling Efficiency Metric\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005516": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A Logic for the Statistical Model Checking of Dynamic Software Architectures.':",
        "Document: \"Towards an IoT ecosystem. In the near future, it will be possible that every object on Earth can be identifiable and addressable. Such objects will be able to be monitored and monitor their physical environments, and of executing actions on such environments in benefit of human users. Moreover, these so-called smart objects will be endowed with wireless communication capabilities. By being uniquely addressed, wireless endowed and through the use of existing protocols and standardized formats, smart things can be integrated in the Internet and accessed as any other Web resource. In this context, the Internet of Things (IoT) emerges as a paradigm in which smart things actively collaborate among them and with other physical and virtual objects available in the Web, providing value-added information and functionalities for users. The IoT paradigm has recently showed its potential of considerably impacting the daily lives of human beings mainly due to the use and interaction of physical devices in several domains, including complex systems composed of other systems. In this paper we discuss the IoT paradigm from the perspective of Systems-of-Systems and present EcoDiF, a IoT platform that integrates heterogeneous devices to provide real-time data control, visualization, processing, and storage. In EcoDiF, devices, information, users and applications are integrated to create an IoT ecosystem in which new ideas and products can be developed in an organic way.\"",
        "Document: \"A Platform for Integrating Physical Devices in the Internet of Things. The Internet of Things (IoT) has emerged as a paradigm in which smart things actively collaborate among them and with other physical and virtual objects available in the Web in order to perform high-level tasks. IoT environments are typically characterized by a high degree of heterogeneity, thus encompassing devices with different capabilities, functionalities, and network protocols. In such a scenario, it is necessary to provide abstractions for physical devices and services to applications and end-users, as well as means to manage the interoperability between such heterogeneous elements. In this context, we introduce EcoDiF (Web Ecosystem of Physical Devices), a Web-based platform for integrating heterogeneous physical devices with applications and users in order to provide services to support real-time data control, visualization, processing, and storage. In this paper, we present the main features of EcoDiF and detail its architecture and implementation, which is based on well-known Web technologies such as HTTP, REST, EEML, and EMML. Furthermore, we present a preliminary evaluation of an EcoDiF prototype through proof-of-concept applications from different domains as well as a performance analysis of the platform.\"",
        "Document: \"Software Engineering Research in Brazil: An Analysis of the Last Five Editions of SBES. The Brazilian software engineering main conference is celebrating its silver jubilee. It is time to reflect on its current status and maturity. In this paper, our goals are (i) to give an overview of the recent SBES publications and (ii) to analyze their relevance to its community and to industry. We analyzed the last five editions of SBES proceedings to identify the type of articles, the research topics, the references to previous SBES publications and the relationship with industry. The results show us that the community is very active but it is time to reflect about the identified problems, to improve the interaction with industry, and to increase the collaboration network between the researchers.\"",
        "Document: \"An approach based on the domain perspective to develop WSAN applications. As wireless sensor and actuator networks (WSANs) can be used in many different domains, WSAN applications have to be built from two viewpoints: domain and network. These different viewpoints create a gap between the abstractions handled by the application developers, namely the domain and network experts. Furthermore, there is a coupling between the application logic and the underlying sensor platform, which results in platform-dependent projects and source codes difficult to maintain, modify, and reuse. Consequently, the process of developing an application becomes cumbersome. In this paper, we propose a model-driven architecture (MDA) approach for WSAN application development. Our approach aims to facilitate the task of the developers by: (1) enabling application design through high abstraction level models; (2) providing a specific methodology for developing WSAN applications; and (3) offering an MDA infrastructure composed of PIM, PSM, and transformation programs to support this process. Our approach allows the direct contribution of domain experts in the development of WSAN applications, without requiring specific knowledge of programming WSAN platforms. In addition, it allows network experts to focus on the specific characteristics of their area of expertise without the need of knowing each specific application domain.\"",
        "1 is \"Model Checking with Multi-valued Logics\", 2 is \"On the success of empirical studies in the international conference on software engineering\"",
        "Given above information, for an author who has written the paper with the title \"A Logic for the Statistical Model Checking of Dynamic Software Architectures.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005631": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Recognition of dynamic hand gestures':",
        "Document: \"Video Denoising and Enhancement via Dynamic Video Layering. Video denoising refers to the problem of removing \u201cnoise\u201d from a video sequence. Here, the term \u201cnoise\u201d is used in a broad sense to refer to any corruption or outlier or interference that is not the quantity of interest. We develop a novel solution framework, which we call layering denoising (LD), for denoising highly noisy or otherwise corrupted videos that are well modeled as the sum of a low-ra...\"",
        "Document: \"Robust PCA with Partial Subspace Knowledge. In recent work, robust Principal Components Analysis (PCA) has been posed as a problem of recovering a low-rank matrix L and a sparse matrix S from their sum, M := L + S and a provably exact convex optimization solution called PCP has been proposed. This work studies the following problem. Suppose that we have partial knowledge about the column space of the low rank matrix L. Can we use this information to improve the PCP solution, i.e. allow recovery under weaker assumptions? We propose here a simple but useful modification of the PCP idea, called modified-PCP, that allows us to use this knowledge. We derive its correctness result which shows that, when the available subspace knowledge is accurate, modified-PCP indeed requires significantly weaker incoherence assumptions than PCP. Extensive simulations are also used to illustrate this. Comparisons with PCP and other existing work are shown for a stylized real application as well. Finally, we explain how this problem naturally occurs in many applications involving time series data, i.e. in what is called the online or recursive robust PCA problem. A corollary for this case is also given.\"",
        "Document: \"Regularized Modified BPDN for Noisy Sparse Reconstruction With Partial Erroneous Support and Signal Value Knowledge. We study the problem of sparse reconstruction from noisy undersampled measurements when the following knowledge is available. (1) We are given partial, and partly erroneous, knowledge of the signal's support, denoted by T . (2) We are also given an erroneous estimate of the signal values on T, denoted by (\u03bc\u0302)T . In practice, both of these may be available from prior knowledge. Alternatively, in recursive reconstruction applications, like real-time dynamic MRI, one can use the support estimate and the signal value estimate from the previous time instant as T and (\u03bc\u0302)T. In this paper, we introduce regularized modified basis pursuit denoising (BPDN) (reg-mod-BPDN) to solve this problem and obtain computable bounds on its reconstruction error. Reg-mod-BPDN tries to find the signal that is sparsest outside the set T, while being \u201cclose enough\u201d to (\u03bc\u0302)T on T and while satisfying the data constraint. Corresponding results for modified-BPDN and BPDN follow as direct corollaries. A second key contribution is an approach to obtain computable error bounds that hold without any sufficient conditions. This makes it easy to compare the bounds for the various approaches. Empirical reconstruction error comparisons with many existing approaches are also provided.\"",
        "Document: \"Recursive Recovery of Sparse Signal Sequences from Compressive Measurements: A Review. In this overview article, we review the literature on design and analysis of recursive algorithms for reconstructing a time sequence of sparse signals from compressive measurements. The signals are assumed to be sparse in some transform domain or in some dictionary. Their sparsity patterns can change with time, although, in many practical applications, the changes are gradual. An important class o...\"",
        "1 is \"Looking for shapes in two-dimensional cluttered point clouds.\", 2 is \"Tree clustering for layout-based document image retrieval\"",
        "Given above information, for an author who has written the paper with the title \"Recognition of dynamic hand gestures\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005764": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Global performance of traditional Chinese medicine over three decades':",
        "Document: \"A two-dimensional approach to performance evaluation for a large number of research institutions. We characterize the research performance of a large number of institutions in a two-dimensional coordinate system based on the shapes of their h-cores so that their relative performance can be conveniently observed and compared. The 2D distribution of these institutions is then utilized (1) to categorize the institutions into a number of qualitative groups revealing the nature of their performance, and (2) to determine the position of a specific institution among the set of institutions. The method is compared with some major h-type indices and tested with empirical data using clinical medicine as an illustrative case. The method is extensible to the research performance evaluation at other aggregation levels such as researchers, journals, departments, and nations.\"",
        "Document: \"International collaboration and counting inflation in the assessment of national research productivity. In this paper we describe how different accounting procedures affected the counting of scientific paper numbers at the country level and the country ranks based on paper production quantity in physics. Using 1989--2008 citation data, we also report the counting inflation ratio between different accounting procedures. We found that, in general, different accounting procedures yielded relatively similar and stable rankings. But for certain clusters of countries, the normal count procedure tended to favor the more advanced Western countries. In contrast, the newly developed countries received more credit in the adjusted and straight count procedures.\"",
        "Document: \"The influences of counting methods on university rankings based on paper count and citation count. \u2022Counting methods influence university ranking especially for the middle-ranged ones.\u2022Counting methods impacted citation counts more than paper counts.\u2022Straight counting and fractional counting are more appropriate than whole counting.\"",
        "Document: \"A study of interdisciplinarity in information science: using direct citation and co-authorship analysis. This study uses two bibliometric methods, direct citation and co-authorship, to investigate the interdisciplinary changes in information sciences during 1978\u00e2\u8059\u80702007. The disciplines of references and co-authors from five information science journals were analysed. Furthermore, Brillouin\u00e2\u8059\u8076s Index was adopted to measure the degree of interdisciplinarity. The study revealed that information science researchers have cited the publications of library and information science (LIS) most frequently. The co-authors of information science articles are also primarily from the discipline of LIS, but the percentage of references to LIS is much higher. This indicates that information science researchers mainly rely on publications in LIS, and they often produce scientific papers with researchers from LIS. The discipline rankings generated by direct citation and co-authorship show a significant consistency via Spearman\u00e2\u8059\u8076s correlation coefficient test. The interdisciplinary degree of information science has displayed growth. In particular, the degree of interdisciplinarity for co-authors has grown.\"",
        "1 is \"Transmitter optimization and optimality of beamforming for multiple antenna systems\", 2 is \"Applying Chance Discovery with Dummy Event in Technology Monitoring of Solar Cell\"",
        "Given above information, for an author who has written the paper with the title \"Global performance of traditional Chinese medicine over three decades\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005777": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Towars a Theory of Stochastic Hybrid Systems':",
        "Document: \"On piecewise quadratic control-Lyapunov functions for switched linear systems. In this paper, we prove that a discrete-time switched linear system is exponentially stabilizable if and only if there exists a stationary hybrid-control law that consists of a homogeneous switching-control law and a piecewise-linear continuous-control law under which the closed-loop system has a piecewise quadratic Lyapunov function. Such a converse control-Lyapunov function theorem justifies many of the earlier controller-synthesis methods that have adopted piecewise-quadratic Lyapunov functions and piecewise-linear continuous-control laws for convenience or heuristic reasons. Furthermore, several important properties of the proposed stabilizing control law are derived and their connections to other existing controllers studied in the literature are discussed.\"",
        "Document: \"Quadratic optimal control of switched linear stochastic systems. This paper studies a quadratic optimal control problem for discrete-time switched linear stochastic systems with nonautonomous subsystems perturbed by Gaussian random noises. The goal is to jointly design a deterministic switching sequence and a continuous feedback law to minimize the expectation of a finite-horizon quadratic cost function. Both the value function and the optimal control strategy are characterized analytically. A numerical relaxation framework is developed to efficiently compute a control strategy with a guaranteed performance upper bound. It is also proved that by choosing the relaxation parameter sufficiently small, the performance of the resulting control strategy can be made arbitrarily close to the optimal one.\"",
        "Document: \"Dynamic buffer management using optimal control of hybrid systems. This paper studies a dynamic buffer management problem with one buffer inserted between two interacting components. The component to be controlled is assumed to have multiple power modes corresponding to different data processing rates. The overall system is modeled as a hybrid system and the buffer management problem is formulated as an optimal control problem. Different from many previous studies, the objective function of the proposed problem depends on the switching cost and the size of the continuous state space, making its solution much more challenging. By exploiting some particular features of the problem, the best mode sequence and the optimal switching instants are characterized analytically using a variational approach. Simulation results based on real data shows that the proposed method can significantly reduce the energy consumption compared with another heuristic scheme in several typical situations.\"",
        "Document: \"Local Model Predictive Control for T-S Fuzzy Systems. In this paper, a new linear matrix inequality-based model predictive control (MPC) problem is studied for discrete-time nonlinear systems described as Takagi-Sugeno fuzzy systems. A recent local stability approach is applied to improve the performance of the proposed MPC scheme. At each time k, an optimal state-feedback gain that minimizes an objective function is obtained by solving a semidefinit...\"",
        "1 is \"Chronicle recognition improvement using temporal focusing and hierarchization\", 2 is \"A predictive system shutdown method for energy saving of event-driven computation\"",
        "Given above information, for an author who has written the paper with the title \"Towars a Theory of Stochastic Hybrid Systems\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005835": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Modelling Retinal Ganglion Cells Using Self-Organising Fuzzy Neural Networks':",
        "Document: \"Network on Chip Architecture for Multi-Agent Systems in FPGA. A system of interacting agents is, by definition, very demanding in terms of computational resources. Although multi-agent systems have been used to solve complex problems in many areas, it is usually very difficult to perform large-scale simulations in their targeted serial computing platforms. Reconfigurable hardware, in particular Field Programmable Gate Arrays devices, have been successfully used in High Performance Computing applications due to their inherent flexibility, data parallelism, and algorithm acceleration capabilities. Indeed, reconfigurable hardware seems to be the next logical step in the agency paradigm, but only a few attempts have been successful in implementing multi-agent systems in these platforms. This article discusses the problem of inter-agent communications in Field Programmable Gate Arrays. It proposes a Network-on-Chip in a hierarchical star topology to enable agents\u2019 transactions through message broadcasting using the Open Core Protocol as an interface between hardware modules. A customizable router microarchitecture is described and a multi-agent system is created to simulate and analyse message exchanges in a generic heavy traffic load agent-based application. Experiments have shown a throughput of 1.6Gbps per port at 100MHz without packet loss and seamless scalability characteristics.\n\n\"",
        "Document: \"An Stdp Training Algorithm For A Spiking Neural Network With Dynamic Threshold Neurons. This paper proposes a supervised training algorithm for Spiking Neural Networks (SNNs) which modifies the Spike Timing Dependent Plasticity (STDP) learning rule to support both local and network level training with multiple synaptic connections and axonal delays. The training algorithm applies the rule to two and three layer SNNs, and is benchmarked using the Iris and Wisconsin Breast Cancer (WBC) data sets. The effectiveness of hidden layer dynamic threshold neurons is also investigated and results are presented.\"",
        "Document: \"Cost Effectiveness Issues In Remote Experimentation. Constant innovation and product evolution in the area of embedded systems necessitates educational institutions and other training providers to continuingly reassess the content of engineering curricula in the context of this developing field. Increasingly web based distance education courses are on offer, augmented by the provision of remote experimentation laboratories facilitating distant access to campus based physical resources. This migration to a remote experimentation platform requires substantial financial investment and time commitments. However remote experimentation provision may lack flexibility in terms of concurrent usage, typically restricted to synchronous access by single users through resource timetabling. This paper details our recent experiences in the provision of remote experimentation facilities for embedded system training. The paper focuses oil the development costs incurred, the cost effectiveness of the approach and issues related to scalability and the delivery of asynchronous learning facilities in the context a remotely accessible embedded systems.\"",
        "Document: \"Modelling Retinal Ganglion Cells Using Self-Organising Fuzzy Neural Networks. Even though artificial vision has been in development for over half a century it still fares poorly when compared to biological vision. The processing capabilities of biological visual systems are vastly superior in terms of power, speed, and performance. Inspired by this robust performance artificial vision systems have sought to take inspiration from biology by modeling aspects of biological vision systems. Existing computational models of visual neurons can be derived by quantitatively fitting particular sets of physiological data using an input-output analysis where a known input is given to the system and its output is recorded. These models need to capture the full spatio-temporal description of neuron behaviour under natural viewing conditions. In this work we use state-of-the-art fuzzy neural network techniques to accurately model the responses of retinal ganglion cells. We illustrate how a self-organising fuzzy neural network can accurately model ganglion cell behaviour, and are a viable alternative to traditional system identification techniques.\"",
        "1 is \"Three-dimensional object recognition\", 2 is \"Multimodal learning with deep Boltzmann machines\"",
        "Given above information, for an author who has written the paper with the title \"Modelling Retinal Ganglion Cells Using Self-Organising Fuzzy Neural Networks\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005879": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Genetic algorithms for video segmentation':",
        "Document: \"Automatic word detection system for document image using mobile devices. In the current age of ubiquitous computing age that uses high bandwidth network, wearable and hand-held mobile devices with small cameras and wireless communication will be widespread in the near future. Thus, computer vision and image processing for mobile devices have recently attracted a lot of attention. Especially, many approaches to detect image texts containing useful information for automatic annotation, indexing, and structuring of image are important for a prerequisite stage of recognition in dictionary application using mobile devices equipped with a camera. To detect image texts on the mobile devices that have limited computational resources, recent works are based on two methodologies; the image texts are detected not by automatically but by manually using stylus pen to reduce the computational resources, and the server is used to detect image texts requiring many floating-computations. The main disadvantage of the manual method is that users directly select tentative text regions, and recall and precision rates are determined by the selected regions. The second method to automatically detect the image texts is difficult to perform it in real-time, due to transmission time between the mobile device and the server. Accordingly, this paper proposes a real-time automatic word detection system without support of the server. To minimize the computational time, one word in the central region of the image is considered as a target of the system. The word region is tentatively extracted by using edge density and window transition, and the tentatively extracted region is then verified by measuring uniform distribution among sub-windows of the extracted region. In the experiments, the proposed method showed high precision rates for one word in the central region of the image, and showed fast computational time on the mobile devices.\"",
        "Document: \"Flying cake: Augmented game on mobile devices. In the current age of ubiquitous computing via high bandwidth networks, wearable and hand-held mobile devices with small cameras and wireless communication will become widespread in the near future. Thus, research on augmented games for mobile devices has recently attracted a lot of attention. Most existing augmented games use a traditional \u201cbackpack\u201d system and \u201cpattern marker\u201d. However, \u2018backpack\u2019 systems are expensive, cumbersome, and inconvenient to use, while the use of a pattern marker means the game can only be played at a previously installed location. Hence, this article proposes an augmented game, called Flying Cake, where face regions are used to create virtual objects (characters) without a predefined pattern marker, plus the location of the virtual objects are measured relative to the real world on a small mobile PDA instead of using cumbersome hardware. Flying Cake is an augmented shooting game with two playing modes: (1) single player, where the player attacks a virtual character overlaid on images captured by a PDA camera in the physical world; and (2) two players, where each player attacks a virtual character in an image received via a wireless LAN from their opponent. The virtual character overlaps a face region obtained using a real-time face-detection technique. As a result, Flying Cake provides an exciting experience for players on the basis of a new game paradigm where the user interacts with both the physical world captured by a PDA camera and the virtual world.\"",
        "Document: \"Graph Cuts-Based Automatic Color Image Segmentation. A graph cuts method has recently attracted a lot of attention for image segmentation, as it can minimize an energy function composed of data term estimated in feature space and smoothness term estimated in an image domain. Although previous approaches using graph cuts have shown good performance for image segmentation, they manually obtained prior information to estimate the data term, thus automatic image segmentation is one of issues in application using the graph cuts method. To automatically estimate the data term, GMM (Gaussian mixture model) is generally used, but it is practicable only for classes with a hyper-spherical or hyper-ellipsoidal shape, as the class was represented based on the covariance matrix centered on the mean. For arbitrary-shaped classes, this paper proposes graph cuts-based image segmentation using mean shift analysis. As prior information to estimate the data term, we use the set of mean trajectories toward each mode from initial means randomly selected in L*u*v* feature space. Since the mean shift procedure requires many computational times, we transform features in continuous feature space into 3D discrete grid, and use 3D kernel based on the first moment in the grid, which are needed to move the means to modes. In the experiments, we investigated problems of normalized cuts-based and mean shift-based segmentation and graph cuts-based segmentation using GMM. As a result, the proposed method showed better performance than previous three methods on Berkeley segmentation dataset.\"",
        "Document: \"NMF-Based approach to font classification of printed english alphabets for document image understanding. This paper proposes an approach to font classification for document image understanding using non-negative matrix factorization (NMF). The basic idea of the proposed method is based on that the characteristics of each font are derived from parts of the individual characters in each font rather than holistic textures. Spatial localities, parts composing of font images, are automatically extracted using NMF. These parts are used as features representing each font. In the experimental results, the distribution of features and the appropriateness of use of the characteristics specifying each font are investigated. Add to that, the proposed method is compared with the method based on principal component analysis (PCA), in which various distance metrics are tested in the feature space. It expects that the proposed method will increase the performance of optical character recognition (OCR) systems or document indexing and retrieval systems if such systems adopt the proposed font classifier as a preprocessor.\"",
        "1 is \"Map Segmentation by Colour Cube Genetic K-Mean Clustering\", 2 is \"Comparing and combining sentiment analysis methods\"",
        "Given above information, for an author who has written the paper with the title \"Genetic algorithms for video segmentation\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005930": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Blind PRNU-Based Image Clustering for Source Identification.':",
        "Document: \"Low-complexity compression of multispectral images based on classified transform coding. Compression of remote-sensing images can be necessary in various stages of the image life, and especially on-board a satellite before transmission to the ground station. Although on-board CPU power is quite limited, it is now possible to implement sophisticated real-time compression techniques, provided that complexity constraints are taken into account at design time. In this paper we consider the class-based multispectral image coder originally proposed in [Gelli and Poggi, Compression of multispectral images by spectral classification and transform coding, IEEE Trans. Image Process. (April 1999) 476-489 [5]] and modify it to allow its use in real time with limited hardware resources. Experiments carried out on several multispectral images show that the resulting unsupervised coder has a fully acceptable complexity, and a rate-distortion performance which is superior to that of the original supervised coder, and comparable to that of the best coders known in the literature.\"",
        "Document: \"Sar Despeckling Based On Soft Classification. We propose a new approach to SAR despeckling, based on the combination of multiple alternative estimates of the same data. The many despeckling methods proposed in the literature possess different and often complementary strengths and weaknesses. Given a reliable pixel-wise classification of the image, one can take advantage of this diversity by selecting the more appropriate combination of estimators for each image region. We implement a simplified version of this approach, using soft classification and two state-of-the-art despeckling tools, with opposite properties, as basic estimators. Experiments on real-world high-resolution SAR images prove the effectiveness of the proposed technique and confirm the potential of the whole approach.\"",
        "Document: \"FaceForensics: A Large-scale Video Dataset for Forgery Detection in Human Faces. With recent advances in computer vision and graphics, it is now possible to generate videos with extremely realistic synthetic faces, even in real time. Countless applications are possible, some of which raise a legitimate alarm, calling for reliable detectors of fake videos. In fact, distinguishing between original and manipulated video can be a challenge for humans and computers alike, especially when the videos are compressed or have low resolution, as it often happens on social networks. Research on the detection of face manipulations has been seriously hampered by the lack of adequate datasets. To this end, we introduce a novel face manipulation dataset of about half a million edited images (from over 1000 videos). The manipulations have been generated with a state-of-the-art face editing approach. It exceeds all existing video manipulation datasets by at least an order of magnitude. Using our new dataset, we introduce benchmarks for classical image forensic tasks, including classification and segmentation, considering videos compressed at various quality levels. In addition, we introduce a benchmark evaluation for creating indistinguishable forgeries with known ground truth; for instance with generative refinement models.\"",
        "Document: \"A comparative analysis of forgery detection algorithms. The aim of this work is to make an objective comparison between different forgery techniques and present a tool that helps taking a more reliable decision about the integrity of a given image or part of it. The considered techniques, all recently proposed in the scientific community, follow different and complementary approaches so as to guarantee robustness with respect to tampering of different types and characteristics. Experiments have been conducted on a large set of images using an automatic copy-paste tampering generator. Early results point out significant differences about competing techniques, depending also on complexity and side information.\"",
        "1 is \"Video shot detection and characterization for video databases\", 2 is \"Digital image splicing detection based on Markov features in DCT and DWT domain\"",
        "Given above information, for an author who has written the paper with the title \"Blind PRNU-Based Image Clustering for Source Identification.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005952": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Binarization With Boosting and Oversampling for Multiclass Classification.':",
        "Document: \"Involving New Local Search in Hybrid Genetic Algorithm for Feature Selection. This paper presents a new hybrid genetic algorithm (HGA) for feature selection (FS) called as HGAFS. HGAFS incorporates a new local search operation that is devised and embedded in HGA to fine-tune the search in FS. The proposed local search operation works on basis of the distinct and informative nature of input features that is computed by their correlation information. The aim of using correlation information is to encourage the local search strategy for selecting less correlated (distinct) features. Such an encouragement reduces the redundancy of information in the generated subset of salient features. We have tested our methods on several real-world datasets and have compared the performances with the results of other existing algorithms. It is found that HGAFS produces consistently better performances.\"",
        "Document: \"An Implementation Of On-Line Traffic Information System Via Short Message Service (Sms) For Bangladesh. In this paper, a vision-based on-tine traffic information system is discussed. The system objectives are to detect levels of traffic congestion on certain roads in Dhaka City and to make this information available to the travelers. To achieve this task, multiple Web Cams will be installed on designated roads. The system will capture digital images of the passing by traffic, analyze these images, and reach a clear decision about number of car. Users will then be able to reach this data by using the short messaging service in their mobile phones. Basically, the system is divided into three independent, yet interacting modules: the image capturing module which with automate the capture of images, the digital image processing module which will process the images, and the short message service (known as SMS) server module which will receive SMSs from a user and reply back to him by an SMS.\"",
        "Document: \"Self-Organization Of Spiking Neural Network Generating Autonomous Behavior In A Miniature Mobile Robot. Purpose of this study is to develop self-organization algorithm of spiking neural network applicable to autonomous robots. We first formulated a spiking neural network model whose inputs and outputs were analog. We then implemented it into a miniature mobile robot Khepera. In order to see whether or not a solution(s) for the given task exists with the spiking neural network, the robot was evolved with the genetic algorithm (GA) in an environment. The robot acquired the obstacle-avoidance and navigation task successfully, exhibiting the presence of the solution. Then, a self-organization algorithm based on the use-dependent synaptic potentiation and depotentiation was formulated and implemented into the robot. In the environment, the robot gradually organized the network and the obstacle avoidance behavior was formed. The time needed for the training was much less than with genetic evolution, approximately one fifth (1/5).\"",
        "Document: \"A Pruning Algorithm for Training Cooperative Neural Network Ensembles. We present a training algorithm to create a neural network (NN) ensemble that performs classification tasks. It employs a competitive decay of hidden nodes in the component NNs as well as a selective deletion of NNs in ensemble, thus named a pruning algorithm for NN ensembles (PNNE). A node cooperation function of hidden nodes in each NN is introduced in order to support the decaying process. The training is based on the negative correlation learning that ensures diversity among the component NNs in ensemble. The less important networks are deleted by a criterion that indicates over-fitting. The PNNE has been tested extensively on a number of standard benchmark problems in machine learning, including the Australian credit card assessment, breast cancer, circle-in-the-square, diabetes, glass identification, ionosphere, iris identification, and soybean identification problems. The results show that classification performances of NN ensemble produced by the PNNE are better than or competitive to those by the conventional constructive and fixed architecture algorithms. Furthermore, in comparison to the constructive algorithm, NN ensemble produced by the PNNE consists of a smaller number of component NNs, and they are more diverse owing to the uniform training for all component NNs.\"",
        "1 is \"Adapting the pheromone evaporation rate in dynamic routing problems\", 2 is \"RAMOBoost: Ranked Minority Oversampling in Boosting.\"",
        "Given above information, for an author who has written the paper with the title \"Binarization With Boosting and Oversampling for Multiclass Classification.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005959": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A Query-oriented Approach for Relevance in Citation Networks.':",
        "Document: \"A hybrid approach to discover semantic hierarchical sections in scholarly documents. Scholarly documents are usually composed of sections, each of which serves a different purpose by conveying specific context. The ability to automatically identify sections would allow us to understand the semantics of what is different in different sections of documents, such as what was in the introduction, methodologies used, experimental types, trends, etc. We propose a set of hybrid algorithms to 1) automatically identify section boundaries, 2) recognize standard sections, and 3) build a hierarchy of sections. Our algorithms achieve an F-measure of 92.38% in section boundary detection, 96% accuracy (average) on standard section recognition, and 95.51% in accuracy in the section positioning task.\"",
        "Document: \"CSSeer: an expert recommendation system based on CiteseerX. We propose CSSeer, a free and publicly available keyphrase based recommendation system for expert discovery based on the CiteSeerX digital library and Wikipedia as an auxiliary resource. CSSeer generates keyphrases from the title and the abstract of each document in CiteSeerX. These keyphrases are then utilized to infer the authors' expertise. We compared CSSeer with the other two state-of-the-art expert recommenders and found that the three systems have moderately divergent recommendations on 20 benchmark queries. Thus, we recommend users to browse through several different recommenders to obtain a more complete expert list.\"",
        "Document: \"Lindex: a lattice-based index for graph databases. Subgraph querying has wide applications in various fields such as cheminformatics and bioinformatics. Given a query graph, q, a subgraph-querying algorithm retrieves all graphs, D(q), which have q as a subgraph, from a graph database, D. Subgraph querying is costly because it uses subgraph isomorphism tests, which are NP-complete. Graph indices are commonly used to improve the performance of subgraph querying in graph databases. Subgraph-querying algorithms first construct a candidate answer set by filtering out a set of false answers and then verify each candidate graph using subgraph isomorphism tests. To build graph indices, various kinds of substructure (subgraph, subtree, or path) features have been proposed with the goal of maximizing the filtering rate. Each of them works with a specifically designed index structure, for example, discriminative and frequent subgraph features work with gIndex, \u9a74-TCFG features work with FG-index, etc. We propose Lindex, a graph index, which indexes subgraphs contained in database graphs. Nodes in Lindex represent key-value pairs where the key is a subgraph in a database and the value is a list of database graphs containing the key. We propose two heuristics that are used in the construction of Lindex that allows us to determine answers to subgraph queries conducting less subgraph isomorphism tests. Consequently, Lindex improves subgraph-querying efficiency. In addition, Lindex is compatible with any choice of features. Empirically, we demonstrate that Lindex used in conjunction with subgraph indexing features proposed in previous works outperforms other specifically designed index structures. As a novel index structure, Lindex (1) is effective in filtering false graphs (2) provides fast index lookups, (3) is fast with respect to index construction and maintenance, and (4) can be constructed using any set of substructure index features. These four properties result in a fast and scalable subgraph-querying infrastructure. We substantiate the benefits of Lindex and its disk-resident variation Lindex+ theoretically and empirically.\"",
        "Document: \"Tablerank: a ranking algorithm for table search and retrieval. Tables are ubiquitous in web pages and scientific documents. With the explosive development of the web, tables have become a valuable information repository. Therefore, effectively and efficiently searching tables becomes a challenge. Existing search engines do not provide satisfactory search results largely because the current ranking schemes are inadequate for table search and automatic table understanding and extraction are rather difficult in general. In this work, we design and evaluate a novel table ranking algorithm-TableRank to improve the performance of our table search engine Table-Seer. Given a keyword based table query, TableRank facilities TableSeer to return the most relevant tables by tailoring the classic vector space model. TableRank adopts an innovative term weighting scheme by aggregating multiple weighting factors from three levels: term, table and document. The experimental results show that our table search engine out-performs existing search engines on table search. In addition, incorporating multiple weighting factors can significantly improve the ranking results.\"",
        "1 is \"A generalized maximum entropy approach to bregman co-clustering and matrix approximation\", 2 is \"Dynamic itemset counting and implication rules for market basket data\"",
        "Given above information, for an author who has written the paper with the title \"A Query-oriented Approach for Relevance in Citation Networks.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005970": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A roadmap towards sustainable self-aware service systems':",
        "Document: \"Towards dynamic monitoring of WS-BPEL processes. The intrinsic flexibility and dynamism of service-centric applications preclude their pre-release validation and demand for suitable probes to monitor their behavior at run-time. Probes must be suitably activated and deactivated according to the context in which the application is executed, but also according to the confidence we get on its quality. The paper supports the idea that significant data may come from very different sources and probes must be able to accommodate all of them. The paper presents: (1) an approach to specify monitoring directives, called monitoring rules, and weave them dynamically into the process they belong to; (2) a proxy-based solution to support the dynamic selection and execution of monitoring rules at run-time; (3) a user-oriented language to integrate data acquisition and analysis into monitoring rules.\"",
        "Document: \"Towards automated verification of layered graph transformation specifications. Graph transformation systems have recently become more and more popular as a general formal modelling language. It is a suitable formalism for modelling different systems like distributed and complex systems. However, modelling must be complemented with proper analysis capabilities to let the user understand how designed models behave and whether stated requirements are fulfilled and model checking has proven to be a viable solution for this purpose. The authors propose an efficient solution for model checking attributed typed and layered graph transformation systems. Layered graph transformation systems are a powerful formalism to formally model different systems like hierarchical systems. In our proposal, AGG layered graph transformation specifications are translated to Bandera intermediate representation (BIR) - the input language of a Bogor model checker - and then Bogor verifies the model against some interesting properties defined by combining LTL (linear temporal logic) and special graph rules. The experimental results are encouraging and show that in most cases our proposal improves existing approaches, in terms of both performance and expressiveness.\"",
        "Document: \"The disappearing boundary between development-time and run-time. Modern software systems are increasingly embedded in an open world that is constantly evolving, because of changes in the requirements, in the surrounding environment, and in the way people interact with them. The platform itself on which software runs may change over time, as we move towards cloud computing. These changes are difficult to predict and anticipate, and their occurrence is out of control of the application developers. Because of these changes, the applications themselves need to change. Often, changes in the applications cannot be handled off-line, but require the software to self-react by adapting its behavior dynamically, to continue to ensure the desired quality of service. The big challenge in front of us is how to achieve the necessary degrees of flexibility and dynamism required by software without compromising the necessary dependability. This paper advocates that future software engineering research should focus on providing intelligent support to software at run-time, breaking today's rigid boundary between development-time and run-time. Models need to continue to live at run-time and evolve as changes occur while the software is running. To ensure dependability, analysis that the updated system models continue to satisfy the goals must be performed by continuous verification. If verification fails, suitable adjustment policies, supported by model-driven re-derivation of parts of the system, must be activated to keep the system aligned with its expected requirements. The paper presents the background that motivates this research focus, the main existing research directions, and an agenda for future work.\"",
        "Document: \"Event-Based Multi-level Service Monitoring. Due to the growing pervasiveness of the service paradigm, modern systems are now often built as software as a service, and tend to exploit underlying platforms and virtualized resources also provided as services. Managing such systems requires that we be aware of the behaviors of all the different layers, and of the strong dependencies that exist between them. In this paper we present the Multi-layer Collection and Constraint Language (mlCCL). It allows us to define how to collect, aggregate, and analyze runtime data in a multi-layered system. We also present ECoWare, a framework for event correlation and aggregation that supports mlCCL, and provides a dashboard for on-line and off-line drill-down analyses of collected data. Empirical assessment shows that the impact of the approach on runtime performance is negligible.\"",
        "1 is \"Deadline-based workload management for MapReduce environments: Pieces of the performance puzzle\", 2 is \"Developing a concurrent service orchestration engine in ccr\"",
        "Given above information, for an author who has written the paper with the title \"A roadmap towards sustainable self-aware service systems\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006079": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Guest editorial: introducing perception, planning, and navigation for intelligent vehicles':",
        "Document: \"Weighted Euclidean Steiner Trees for Disaster-Aware Network Design. We consider the problem of constructing a Euclidean Steiner tree in a setting where the plane has been divided into polygonal regions, each with an associated weight. Given a set of points (terminals), the task is to construct a shortest interconnection of the points, where the cost of a line segment in a region is the Euclidean distance multiplied by the weight of the region. The problem is a natural generalization of the obstacle-avoiding Euclidean Steiner tree problem, and has obvious applications in network design. We propose an efficient heuristic strategy for the problem, and evaluate its performance on both randomly generated and near-realistic problem instances. The minimum cost Euclidean Steiner tree can be seen as an optical backbone network (a Spine) avoiding disaster prone areas, here represented as higher cost regions.\"",
        "Document: \"An outdoor guidepath navigation system for AMRs based on robust detection of magnetic markers. This paper presents an outdoor guidepath navigation system for autonomous mobile robots (AMR) that use permanent magnetic markers embedded in the ground. The odometric data provided by the wheel encoders is fused with the data from magnetic markers. The extended Kalman filter (EKF) was chosen for the fusion process. The AMR is equipped with a magnetic sensing ruler (MSR) developed at ISR-UC that is able to perform a robust detection of magnetic markers. The detection is based on a 3-D algorithm that includes longitudinal-fitting detection (LFD), and cross-fitting detection (CFD). Both, the LFD and the CFD are based on the least squares fitting (LSF) of the measurement data with the 3-D model of the vertical magnetic field. The experimental results with Robchair (intelligent wheelchair being developed at ISR-UC) primarily show that the detection system is robust, since it is able to detect true magnetic markers, and to eliminate noisy magnetic distortions and false markers. The design, and implementation of the navigation algorithm in the Robchair were carried out, and results are presented.\"",
        "Document: \"Improving the Generalization Capacity of Cascade Classifiers. The cascade classifier is a usual approach in object detection based on vision, since it successively rejects negative occurrences, e.g., background images, in a cascade structure, keeping the processing time suitable for on-the-fly applications. On the other hand, similar to other classifier ensembles, cascade classifiers are likely to have high Vapnik-Chervonenkis (VC) dimension, which may lead to overfitting the training data. Therefore, this work aims at improving the generalization capacity of the cascade classifier by controlling its complexity, which depends on the model of their classifier stages, the number of stages, and the feature space dimension of each stage, which can be controlled by integrating the parameter setting of the feature extractor (in our case an image descriptor) into the maximum-margin framework of support vector machine training, as will be shown in this paper. Moreover, to set the number of cascade stages, bounds on the false positive rate (FP) and on the true positive rate (TP) of cascade classifiers are derived based on a VC-style analysis. These bounds are applied to compose an enveloping receiver operating curve (EROC), i.e., a new curve in the TP\u2013FP space in which each point is an ordered pair of upper bound on the FP and lower bound on the TP. The optimal number of cascade stages is forecasted by comparing EROCs of cascades with different numbers of stages.\"",
        "Document: \"Trajectory planning and sliding-mode control based trajectory-tracking for cybercars. Fully automatic driving is emerging as the approach to dramatically improve efficiency (throughput per unit of space) while at the same time leading to the goal of zero accidents. This approach, based on fully automated vehicles, might improve the efficiency of road travel in terms of space and energy used, and in terms of service provided as well. For such automated operation, trajectory planning methods that produce smooth trajectories, with low level associated accelerations and jerk for providing human comfort, are required. This paper addresses this problem proposing a new approach that consists of introducing a velocity planning stage in the trajectory planner. Moreover, this paper presents the design and simulation evaluation of trajectory-tracking and path-following controllers for autonomous vehicles based on sliding mode control. A new design of sliding surface is proposed, such that lateral and angular errors are internally coupled with each other (in cartesian space) in a sliding surface leading to convergence of both variables.\"",
        "1 is \"Study on eco-route planning algorithm and environmental impact assessment\", 2 is \"A no-reference perceptual blur metric\"",
        "Given above information, for an author who has written the paper with the title \"Guest editorial: introducing perception, planning, and navigation for intelligent vehicles\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006090": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Cost-Effective lifetime prediction based routing protocol for MANET':",
        "Document: \"Gender Classification Using Local Directional Pattern (LDP). In this paper, we present a novel texture descriptor Local Directional Pattern (LDP) to represent facial image for gender classification. The face area is divided into small regions, from which LDP histograms are extracted and concatenated into a single vector to efficiently represent the face image. The classification is performed by using support vector machines (SVMs), which had been shown to be superior to traditional pattern classifiers in gender classification problem. Experimental results show the superiority of the proposed method on the images collected from FERET face database and achieved 95.05% accuracy.\"",
        "Document: \"Facial expression recognition using Local Directional Pattern (LDP). A robust face descriptor is an essential component for a good facial expression recognition system. In this paper, we analyze the performance of a new feature descriptor, Local Directional Pattern (LDP), for the representation of facial expressions. LDP features are obtained by computing the edge response values in all eight directions at each pixel position and then a code is generated according to the relative magnitude's strength. Thus each expression is represented as a distribution of LDP codes. Different machine learning techniques are compared using Cohn-Kanade facial expression database for classification. Extensive experiments explicate the superiority of the proposed LDP based descriptor over other existing well known descriptors.\"",
        "Document: \"Local Directional Pattern Variance (Ldpv): A Robust Feature Descriptor For Facial Expression Recognition. Automatic facial expression recognition is a challenging problem in computer vision, and has gained significant importance in the applications of human-computer interactions. The vital component of any successful expression recognition system is an effective facial representation from face images. In this paper, we have derived an appearance-based feature descriptor, the Local Directional Pattern Variance (LDPv), which characterizes both the texture and contrast information of facial components. The LDPv descriptor is a collection of Local Directional Pattern (LDP) codes weighted by their corresponding variances. The feature dimension is then reduced by extracting. the most discriminative elements of the representation with Principal Component Analysis (PCA). The recognition performance based on our LDPv descriptor has been evaluated using Cohn-Kanade expression database with a Support Vector Machine (SVM) classifier. The discriminative strength of LDPv representation is also assessed over a useful range of low resolution images. Experimental results with prototypic expressions show that the LDPv descriptor has achieved a higher recognition rate, as compared to other existing appearance-based feature descriptors.\"",
        "Document: \"A Dynamic Histogram Equalization for Image Contrast Enhancement. In this paper, a smart contrast enhancement technique based on conventional histogram equalization (HE) algorithm is proposed. This dynamic histogram equalization (DHE) technique takes control over the effect of traditional HE so that it performs the enhancement of an image without making any loss of details in it. DHE partitions the image histogram based on local minima and assigns specific gray level ranges for each partition before equalizing them separately. These partitions further go though a repartitioning test to ensure the absence of any dominating portions. This method outperforms other present approaches by enhancing the contrast well without introducing severe side effects, such as washed out appearance, checkerboard effects etc., or undesirable artifacts.\"",
        "1 is \"Coupled parametric active contours.\", 2 is \"A User-Guided Cognitive Agent for Network Service Selection in Pervasive Computing Environments\"",
        "Given above information, for an author who has written the paper with the title \"Cost-Effective lifetime prediction based routing protocol for MANET\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006129": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Using quantiles in ranking and selection procedures':",
        "Document: \"A multi-population genetic algorithm to solve multi-objective scheduling problems for parallel machines. In this paper we propose a two-stage multi-population genetic algorithm (MPGA) to solve parallel machine scheduling problems with multiple objectives. In the first stage, multiple objectives are combined via the multiplication of the relative measure of each objective. Solutions of the first stage are arranged into several sub-populations, which become the initial populations of the second stage. Each sub-population then evolves separately while an elitist strategy preserves the best individuals of each objective and the best individual of the combined objective. This approach is applied in parallel machine scheduling problems with two objectives: makespan and total weighted tardiness (TWT). The MPGA is compared with a benchmark method, the multi-objective genetic algorithm (MOGA), and shows better results for all of the objectives over a wide range of problems. The MPGA is extended to scheduling problems with three objectives: makespan, TWT, and total weighted completion times (TWC), and also performs better than MOGA.\"",
        "Document: \"Manufacturing supply chain applications 2: parameterization of fast and accurate simulations for complex supply networks. More efficient and effective control of supply networks is conservatively worth billions of dollars to the world economy. Adopting an approach by which the basic disciplines of Industrial Engineering, Control Engineering, System Simulation and Business Re-Engineering are integrated into one comprehensive system has been known to produce impressive results. This paper discusses a modular approach to develop a discrete event simulation model that has the appropriate level of abstraction to capture the inherent complexities that exist in a supply chain and is yet simple, fast and produces results of high fidelity. It discusses a method to parameterize each module by fine-tuning a few parameters to make it represent an entire factory, a warehouse or a transportation link.\"",
        "Document: \"Multiple orders per job formation and release strategies in large-scale wafer fabs: a simulation study. In this paper, multiple orders per job type formation and release strategies are described for semiconductor wafer fabrication facilities (wafer fabs). Different orders are grouped into one job because orders of an individual customer very often fill only a portion of a Front-Opening Unified Pod (FOUP). After job formation, an FOUP is assigned to each job and is used to move the job throughout the wafer fab. We determine an acceptable number of FOUPs given a prescribed order release rate to find appropriate values for on-time delivery performance measures, cycle time, and throughput by discrete event simulation. On the other hand, given a prescribed number of FOUPs and target values for these performance measures, we look for an appropriate order release rate. Simulation experiments with models for large-scale wafer fabs are performed to solve these two problems. We also discuss a complementary analytical method to determine an appropriate number of FOUPs in some specific situations.\"",
        "Document: \"A New Method of Exercising Pandemic Preparedness Through an Interactive Simulation and Visualization. As seen in the spring 2009 A/H1N1 influenza outbreak, influenza pandemics can have profound social, legal and economic effects. This experience has also made the importance of public health preparedness exercises more evident. Universities face unique challenges with respect to pandemic preparedness due to their dense student populations, location within the larger community and frequent student/faculty international travel. Depending on the social structure of the community, different mitigation strategies should be applied for decreasing the severity and transmissibility of the disease. To this end, Arizona State University has developed a simulation model and tabletop exercise that facilitates decision-maker interactions around emergency-response scenarios. This simulation gives policy makers the ability to see the real-time impact of their decisions. Therefore, tabletop exercises with computer simulations are developed to practice these decisions, which can possibly give opportunities for practicing better policy implementations. This paper introduces a new method of designing and performing table-top exercises for pandemic influenza via state-of-the-art technologies including system visualization and group decision making with a supporting simulation model. The presented exercise methodology can increase readiness for a pandemic through the support of computer and information technologies and the survey results that we include in this paper certainly support this result. The video scenarios and the computer simulation model make the exercise appear very compelling and real, which makes this presented method of exercising different than the other table-top exercises in the literature. Finally, designing a pandemic preparedness exercise with supporting technologies can help identifying the communication gaps between responsible authorities and advance the table-top exercising methodology.\"",
        "1 is \"A multinomial ranking and selection procedure: Simulation and applications\", 2 is \"CSCAT: the Compaq Supply Chain Analysis Tool\"",
        "Given above information, for an author who has written the paper with the title \"Using quantiles in ranking and selection procedures\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006194": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'WSDM 2017 Workshop on Mining Online Health Reports: MOHRS 2017.':",
        "Document: \"Change-point detection in time-series data by relative density-ratio estimation. The objective of change-point detection is to discover abrupt property changes lying behind time-series data. In this paper, we present a novel statistical change-point detection algorithm based on non-parametric divergence estimation between time-series samples from two retrospective segments. Our method uses the relative Pearson divergence as a divergence measure, and it is accurately and efficiently estimated by a method of direct density-ratio estimation. Through experiments on artificial and real-world datasets including human-activity sensing, speech, and Twitter messages, we demonstrate the usefulness of the proposed method.\"",
        "Document: \"Enhancing Twitter Data Analysis with Simple Semantic Filtering: Example in Tracking Influenza-Like Illnesses. Systems that exploit publicly available user generated content such as Twitter messages have been successful in tracking seasonal influenza. We developed a novel filtering method for Influenza-Like-Ilnesses (ILI)-related messages using 587 million messages from Twitter micro-blogs. We first filtered messages based on syndrome keywords from the BioCaster Ontology, an extant knowledge model of laymen's terms. We then filtered the messages according to semantic features such as negation, hashtags, emoticons, humor and geography. The data covered 36 weeks for the US 2009 influenza season from 30th August 2009 to 8th May 2010. Results showed that our system achieved the highest Pearson correlation coefficient of 98.46% (p-value<;2.2e-16), an improvement of 3.98% over the previous state-of-the-art method. The results indicate that simple NLP-based enhancements to existing approaches to mine Twitter data can increase the value of this inexpensive resource.\"",
        "Document: \"Mapping Text to Knowledge Graph Entities using Multi-Sense LSTMs. This paper addresses the problem of mapping natural language text to knowledge base entities. The mapping process is approached as a composition of a phrase or a sentence into a point in a multi-dimensional entity space obtained from a knowledge graph. The compositional model is an LSTM equipped with a dynamic disambiguation mechanism on the input word embeddings (a Multi-Sense LSTM), addressing polysemy issues. Further, the knowledge base space is prepared by collecting random walks from a graph enhanced with textual features, which act as a set of semantic bridges between text and knowledge base entities. The ideas of this work are demonstrated on large-scale text-to-entity mapping and entity classification tasks, with state of the art results.\"",
        "Document: \"PIA-Core: Semantic Annotation through Example-based Learning. This paper summarizes the aims and scope of the PIA (Portable Information Access) project's PIA-Core system for automatic annotation of documents on the Semantic Web, i.e. the next generation World Wide Web. The focus of the project is to develop a portable information extraction system that can be easily adapted to new domains. PIA has its foundations on three resources: the PIA-Core information extraction module, application modules and PIA guidelines for ensuring consistent annotation. We are currently developing PIA-Core based on advanced machines learning methods to automatically annotate documents with terminology, names, temporal and quantity expressions etc. using examples of annotated documents.\"",
        "1 is \"Automatic extraction of subcategorization from corpora\", 2 is \"Systematic topology analysis and generation using degree correlations\"",
        "Given above information, for an author who has written the paper with the title \"WSDM 2017 Workshop on Mining Online Health Reports: MOHRS 2017.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006215": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Objective characterization of human behavioural characteristics for QoE assessment: A pilot study on the use of electroencephalography features':",
        "Document: \"Multi-objective training of Generative Adversarial Networks with multiple discriminators. Recent literature has demonstrated promising results for training Generative Adversarial Networks by employing a set of discriminators, in contrast to the traditional game involving one generator against a single adversary. Such methods perform single-objective optimization on some simple consolidation of the losses, e.g. an arithmetic average. In this work, we revisit the multiple-discriminator setting by framing the simultaneous minimization of losses provided by different models as a multi-objective optimization problem. Specifically, we evaluate the performance of multiple gradient descent and the hypervolume maximization algorithm on a number of different datasets. Moreover, we argue that the previously proposed methods and hypervolume maximization can all be seen as variations of multiple gradient descent in which the update direction can be computed efficiently. Our results indicate that hypervolume maximization presents a better compromise between sample quality and computational cost than previous methods.\"",
        "Document: \"Online Ecg Quality Assessment For Context-Aware Wireless Body Area Networks. Electrocardiogram (ECG) signals are commonly used in wireless body area networks (WBAN), particularly for patient monitoring applications. ECGs, however, are sensitive to various types of noise sources, including but not limited to: powerline interference, movement, muscle and breathing artefacts. Such sensitivity is increased when burgeoning lower-cost sensors, such as textile ECG sensors, are used. Transmission of noisy ECGs can be troublesome for various reasons. For example, it consumes bandwidth, battery life, and storage space with signals that convey little cardiac information. Moreover, noisy signals may cause false alarms in automated patient monitoring systems, thus increasing the burden on medical personnel. In this paper, we describe a new ECG quality index based on the so-called modulation spectral signal representation. Two classifiers are tested to discriminate between usable and non-usable ECG segments. When applied within a quality-aware WBAN application, we show savings of up to 65% in storage space relative to a traditional scheme.\"",
        "Document: \"Improving the performance of NIRS-based brain-computer interfaces in the presence of background auditory distractions. In this paper, the effects of auditory distractions on the performance of brain-computer interfaces (BCI) based on near-infrared spectroscopy (NIRS) are investigated. Experiments show that NIRS-BCI specificity decreases by an average 19% when operated in the presence of continuous background noise (relative to operation in silence) and by 13% when operated in the presence of startle noises. To improve BCI performance in noisy environments, a simple yet effective startle noise compensation strategy is proposed. Acoustic environmental conditions are tracked in realtime and false BCI activations that occur within seconds of detected startle noises are suppressed. Experiments show NIRS-BCI systems equipped with the proposed compensation system attaining performances in noisy conditions comparable to those attained in silent conditions.\"",
        "Document: \"A sequential feature selection algorithm for GMM-based speech quality estimation. We propose a sequential feature selection algorithm for de- signing Gaussian mixture model (GMM) based estimators. Feature selection is performed progressively to minimize es- timation errors. The algorithm is applied to design estima- tors of subjective speech quality. Simulation shows that es- timators designed using the proposed algorithm outperform two benchmark algorithms by as much as 39% in correlation and 24% in root-mean-squared error. Furthermore, features selected by the proposed algorithm are suitable for diago- nal GMM estimators, which incur lower computational com- plexity.\"",
        "1 is \"Towards a multimodal brain-computer interface: Combining fNIRS and fTCD measurements to enable higher classification accuracy.\", 2 is \"Three Dimensional Scalable Video Adaptation via User-End Perceptual Quality Assessment\"",
        "Given above information, for an author who has written the paper with the title \"Objective characterization of human behavioural characteristics for QoE assessment: A pilot study on the use of electroencephalography features\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006286": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Multi-modal Mutual Topic Reinforce Modeling for Cross-media Retrieval':",
        "Document: \"Metric Learning Driven Multi-Task Structured Output Optimization For Robust Keypoint Tracking. As an important and challenging problem in computer vision and graphics, keypoint-based object tracking is typically formulated in a spatio-temporal statistical learning framework. However, most existing keypoint trackers are incapable of effectively modeling and balancing the following three aspects in a simultaneous manner: temporal model coherence across frames, spatial model consistency within frames, and discriminative feature construction. To address this issue, we propose a robust keypoint tracker based on spatio-temporal multi-task structured output optimization driven by discriminative metric learning. Consequently, temporal model coherence is characterized by multi-task structured keypoint model learning over several adjacent frames, while spatial model consistency is modeled by solving a geometric verification based structured learning problem. Discriminative feature construction is enabled by metric learning to ensure the intra-class compactness and inter-class separability. Finally, the above three modules are simultaneously optimized in a joint learning scheme. Experimental results have demonstrated the effectiveness of our tracker.\"",
        "Document: \"Incremental learning of 3D-DCT compact representations for robust visual tracking. Visual tracking usually requires an object appearance model that is robust to changing illumination, pose, and other factors encountered in video. Many recent trackers utilize appearance samples in previous frames to form the bases upon which the object appearance model is built. This approach has the following limitations: 1) The bases are data driven, so they can be easily corrupted, and 2) it is difficult to robustly update the bases in challenging situations. In this paper, we construct an appearance model using the 3D discrete cosine transform (3D-DCT). The 3D-DCT is based on a set of cosine basis functions which are determined by the dimensions of the 3D signal and thus independent of the input video data. In addition, the 3D-DCT can generate a compact energy spectrum whose high-frequency coefficients are sparse if the appearance samples are similar. By discarding these high-frequency coefficients, we simultaneously obtain a compact 3D-DCT-based object representation and a signal reconstruction-based similarity measure (reflecting the information loss from signal reconstruction). To efficiently update the object representation, we propose an incremental 3D-DCT algorithm which decomposes the 3D-DCT into successive operations of the 2D discrete cosine transform (2D-DCT) and 1D discrete cosine transform (1D-DCT) on the input video data. As a result, the incremental 3D-DCT algorithm only needs to compute the 2D-DCT for newly added frames as well as the 1D-DCT along the third dimension, which significantly reduces the computational complexity. Based on this incremental 3D-DCT algorithm, we design a discriminative criterion to evaluate the likelihood of a test sample belonging to the foreground object. We then embed the discriminative criterion into a particle filtering framework for object state inference over time. Experimental results demonstrate the effectiveness and robustness of the proposed tracker.\"",
        "Document: \"Spatially aware feature selection and weighting for object retrieval. Many recent image retrieval methods are based on the ''bag-of-words'' (BoW) model with some additional spatial consistency checking. This paper proposes a more accurate similarity measurement that takes into account spatial layout of visual words in an offline manner. The similarity measurement is embedded in the standard pipeline of the BoW model, and improves two features of the model: i) latent visual words are added to a query based on spatial co-occurrence, to improve query recall; and ii) weights of reliable visual words are increased to improve the precision. The combination of these methods leads to a more accurate measurement of image similarity. This is similar in concept to the combination of query expansion and spatial verification, but does not require query time processing, which is too expensive to apply to full list of ranked results. Experimental results demonstrate the effectiveness of our proposed method on three public datasets.\"",
        "Document: \"Deep Compositional Cross-modal Learning to Rank via Local-Global Alignment. Cross-modal retrieval is a very hot research topic that is imperative to many applications involving multi-modal data. Discovering an appropriate representation for multi-modal data and learning a ranking function are essential to boost the cross-media retrieval. Motivated by the assumption that a compositional cross-modal semantic representation (pairs of images and text) is more attractive for cross-modal ranking, this paper exploits the existing image-text databases to optimize a ranking function for cross-modal retrieval, called deep compositional cross-modal learning to rank (C2MLR). In this paper, C2MLR considers learning a multi-modal embedding from the perspective of optimizing a pairwise ranking problem while enhancing both local alignment and global alignment. In particular, the local alignment (i.e., the alignment of visual objects and textual words) and the global alignment (i.e., the image-level and sentence-level alignment) are collaboratively utilized to learn the multi-modal embedding common space in a max-margin learning to rank manner. The experiments demonstrate the superiority of our proposed C2MLR due to its nature of multi-modal compositional embedding.\"",
        "1 is \"From Point to Set: Extend the Learning of Distance Metrics\", 2 is \"Linear programming minimum sphere set covering for extreme learning machines\"",
        "Given above information, for an author who has written the paper with the title \"Multi-modal Mutual Topic Reinforce Modeling for Cross-media Retrieval\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006297": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Max-margin dictionary learning for multiclass image categorization':",
        "Document: \"Online vigilance analysis combining video and electrooculography features. In this paper, we propose a novel system to analyze vigilance level combining both video and Electrooculography (EOG) features. For one thing, the video features extracted from an infrared camera include percentage of closure (PERCLOS) and eye blinks, slow eye movement (SEM), rapid eye movement (REM) are also extracted from EOG signals. For another, other features like yawn frequency, body posture and face orientation are extracted from the video by using Active Shape Model (ASM). The results of our experiments indicate that our approach outperforms the existing approaches based on either video or EOG merely. In addition, the prediction offered by our model is in close proximity to the actual error rate of the subject. We firmly believe that this method can be widely applied to prevent accidents like fatigued driving in the future.\"",
        "Document: \"EEG-based emotion classification using deep belief networks. In recent years, there are many great successes in using deep architectures for unsupervised feature learning from data, especially for images and speech. In this paper, we introduce recent advanced deep learning models to classify two emotional categories (positive and negative) from EEG data. We train a deep belief network (DBN) with differential entropy features extracted from multichannel EEG as input. A hidden markov model (HMM) is integrated to accurately capture a more reliable emotional stage switching. We also compare the performance of the deep models to KNN, SVM and Graph regularized Extreme Learning Machine (GELM). The average accuracies of DBN-HMM, DBN, GELM, SVM, and KNN in our experiments are 87.62%, 86.91%, 85.67%, 84.08%, and 69.66%, respectively. Our experimental results show that the DBN and DBN-HMM models improve the accuracy of EEG-based emotion classification in comparison with the state-of-the-art methods.\"",
        "Document: \"Multi-View Gender Classification Using Multi-Resolution Local Binary Patterns And Support Vector Machines. In this paper, we present a novel method for multi-view gender classification considering both shape and texture information to represent facial images. The face area is divided into small regions from which local binary pattern (LBP) histograms are extracted and concatenated into a single vector efficiently representing a facial image. Following the idea of local binary pattern, we propose a new feature extraction approach called multi-resolution LBP, which can retain both fine and coarse local micro-patterns and spatial information of facial images. The classification tasks in this work are performed by support vector machines (SVMs). The experiments clearly show the superiority of the proposed method over both support gray faces and support Gabor faces on the CAS-PEAL face database. A higher correct classification rate of 96.56% and a higher cross validation average accuracy of 95.78% have been obtained. In addition, the simplicity of the proposed method leads to very fast feature extraction, and the regional histograms and fine-to-coarse description of facial images allow for multi-view gender classification.\"",
        "Document: \"Detection of Driving Fatigue Based on Grip Force on Steering Wheel with Wavelet Transformation and Support Vector Machine. This paper proposes an unobtrusive way to detect fatigue for drivers through grip forces on steering wheel. Simulated driving experiments are conducted in a refitted passenger car, during which grip forces of both hands are collected. Wavelet transformation is introduced to extract fatigue-related features from wavelet coefficients. We compare the performance of k-nearest neighbours, linear discriminant analysis, and support vector machine (SVM) on the task of discriminating drowsy and awake states. SVM with radial basis function reaches the best accuracy, 75% on average. The results show that variation in grip forces on steering wheel can be used to effectively detect drivers' fatigue. \u00a9 Springer-Verlag 2013.\"",
        "1 is \"Two methods for stabilizing MERT: NICT at IWSLT 2009.\", 2 is \"A System for new event detection\"",
        "Given above information, for an author who has written the paper with the title \"Max-margin dictionary learning for multiclass image categorization\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006298": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Range-free localization approach for M2M communication system using mobile anchor nodes.':",
        "Document: \"Analysis Of A Cell-Based Call Admission Control Scheme For Qos Support In Multimedia Wireless Networks. Next generation wireless communication systems target to provide guaranteed Quality of Service (QoS) for multimedia applications. In this paper, a multiple-threshold bandwidth reservation scheme combined with a call admission control algorithm is proposed and analyzed. The objective of our work is to be able to achieve better QoS provisioning for mobile users while achieving efficient utilization of the available limited bandwidth. The proposed scheme is modelled as M/M/C/C queueing system and the performance measures, call blocking probability and call dropping probability are computed. The performance of our scheme is compared to the Complete Sharing (CS) policy. Simulation results show that our scheme surpasses the CS policy in terms of call blocking probability, call dropping probability and bandwidth utilization.\"",
        "Document: \"An Audio/Video Crypto - Adaptive Optical Steganography Technique. In recent years a growing interest in information hiding in multimedia data as the host has been observed in the research community. This hidden information can be used for many different purposes, including source identification, copyright protection and covert data transmission. In this paper, an optical crypto technique with adaptive steganography (AS) is proposed for audio/video sequence encryption and decryption. The optical crypto technique is based on double random phase encoding algorithm to encrypt and decrypt the intended audio/video sequences. The main purpose of steganography algorithms is to hide as much information within the cover media as possible. Therefore, for steganography algorithms, the tradeoff is between the amount of covert information being embedded, called stego-data, and that the ensurance for its presence to remain undetected. While their purposes may seem different, recent advances allow more and more the use of advanced watermarking techniques to embed large amounts of covert information that is also robust against removal and detection.\"",
        "Document: \"Tramcar: A Context-Aware Cross-Layer Architecture for Next Generation Heterogeneous Wireless Networks. Major research challenges in the next generation (4G) of wireless networks include the provisioning of worldwide seamless mobility across heterogeneous wireless networks, the improvement of end-to-end Quality of Service (QoS) and enabling users to specify their personal preferences. Under this motivation, we design a novel cross-layer architecture that provides context-awareness, smart handoff and mobility control in heterogeneous wireless IP networks. We develop a Transport and Application Layer Architecture for vertical Mobility with Context-awareness (Tramcar). Tramcar is tailored for a variety of different network technologies with different characteristics and has the ability of adapting to changing environment conditions and unpredictable background traffic. Furthermore, Tramcar allows users to identify and prioritize their preferences. Simulation results demonstrate that Tramcar increases user satisfaction levels and network throughput under rough network conditions and reduces overall handoff latencies.\"",
        "Document: \"Secure timing synchronization for heterogeneous sensor network using pairing over elliptic curve. Secure time synchronization is one of the key concerns for some sophisticated sensor network applications. Most existing time synchronization protocols are affected by almost all attacks. In this paper, we consider heterogeneous sensor networks (HSNs) as a model for our proposed novel time synchronization protocol based on pairing and identity-based cryptography (IBC). This is the first approach for time synchronization protocol using pairing-based cryptography (PBC) in HSNs. The proposed protocol reduces the communication overhead of the nodes as well as prevents from all the major security attacks. Security analysis shows, it robust against reply attacks, masquerade attacks, delay attacks, and message manipulation attacks. Copyright \u00a9 2009 John Wiley & Sons, Ltd.\"",
        "1 is \"An Evolvable Operating System For Wireless Sensor Networks\", 2 is \"Toward 5G densenets: architectural advances for effective machine-type communications over femtocells\"",
        "Given above information, for an author who has written the paper with the title \"Range-free localization approach for M2M communication system using mobile anchor nodes.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006311": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Barriers to systematic model transformation testing':",
        "Document: \"A Domain Specific Modeling Language Supporting Specification, Simulation and Execution of Dynamic Adaptive Systems. Constructing and executing distributed systems that can automatically adapt to the dynamic changes of the environment are highly complex tasks. Non-trivial challenges include provisioning of efficient design time and run time representations, system validation to ensure safe adaptation of interdependent components, and scalable solutions to cope with the possible combinatorial explosions of adaptive system artifacts such as configurations, variant dependencies and adaptation rules. These are all challenges where current approaches offer only partial solutions. Furthermore, in current approaches the adaptation logic is typically specified at the code level, tightly coupled with the main system functionality, making it hard to control and maintain. This paper presents a domain specific modeling language (DSML) allowing specification of the adaptation logic at the model level, and separation of the adaptation logic from the main system functionality. It supports model-checking and design-time simulation for early validation of adaptation policies. The model level specifications are used to generate the adaptation logic. The DSML also provides indirection mechanisms to cope with combinatorial explosions of adaptive system artifacts. The proposed approach has been implemented and validated through case studies.\"",
        "Document: \"Developing a software product line for train control: a case study of CVL. This paper presents a case study of creating a software product line for the train signaling domain. The Train Control Language (TCL) is a DSL which automates the production of source code for computers controlling train stations. By applying the Common Variability Language (CVL), which is a separate and generic language to define variability on base models, we form a software product line of stations. We discuss the process and experience of using CVL to automate the production of three real train stations. A brief discussion about the verification needed for the generated products is also included.\"",
        "Document: \"A model-driven approach to develop adaptive firmwares. In a near future it is expected that most things we rely on in our everyday life will contain sensors and electronic based information, have computing power, run embedded software and connect to networks. A multitude of heterogeneous things will operate in a highly dynamic environment and will collaborate with other connected systems and things to provide users with adaptable services. Constructing and controlling such adaptive things is complex. A main challenge is to cope with the dynamicity which requires the things to autonomously adapt to various execution contexts. In this paper we present an approach to develop adaptive firmwares for devices which do not have the resources to rely on advanced operating systems, middlewares or frameworks to support runtime adaptation. The paper is illustrated with the example of an adaptive temperature sensor network running on a microcontroller platform.\"",
        "Document: \"Model-driven engineering for software migration in a large industrial context. As development techniques, paradigms and platforms evolve far more quickly than domain applications, software modernization and migration, is a constant challenge to software engineers. For more than ten years now, the Sodifrance company has been intensively using Model-Driven Engineering (MDE) for both development and migration projects. In this paper we report on the use of MDE as an efficient, flexible and reliable approach for a migration process (reverse-engineering, transformation and code generation). Moreover, we discuss how MDE is economically profitable and is cost-effective over the migration through out-sourced manual re-development. The paper is illustrated with the migration of a large-scale banking system from Mainframe to J2EE.\"",
        "1 is \"Modeling the Variability Space of Self-Adaptive Applications\", 2 is \"Decreasing the cost of mutation testing with second-order mutants\"",
        "Given above information, for an author who has written the paper with the title \"Barriers to systematic model transformation testing\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006332": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Microphone multiplexing with diffuse noise model-based principal component analysis':",
        "Document: \"Sparse Representations in Audio and Music: From Coding to Source Separation. Sparse representations have proved a powerful tool in the analysis and processing of audio signals and already lie at the heart of popular coding standards such as MP3 and Dolby AAC. In this paper we give an overview of a number of current and emerging applications of sparse representations in areas from audio coding, audio enhancement and music transcription to blind source separation solutions that can solve the ??cocktail party problem.?? In each case we will show how the prior assumption that the audio signals are approximately sparse in some time-frequency representation allows us to address the associated signal processing task.\"",
        "Document: \"Random time-frequency subdictionary design for sparse representations with greedy algorithms. Sparse signal approximation can be used to design efficient low bit-rate coding schemes. It heavily relies on the ability to design appropriate dictionaries and corresponding decomposition algorithms. The size of the dictionary, and therefore its resolution, is a key parameter that handles the tradeoff between sparsity and tractability. This work proposes the use of a non adaptive random sequence of subdictionaries in a greedy decomposition process, thus browsing a larger dictionary space in a probabilistic fashion with no additional projection cost nor parameter estimation. This technique leads to very sparse decompositions, at a controlled computational complexity. Experimental evaluation is provided as proof of concept for low bit rate compression of audio signals.\"",
        "Document: \"Audio Sparse Decompositions in Parallel. Greedy methods are often the only practical way to solve very large sparse approximation problems. Among such methods, matching pursuit (MP) is undoubtedly one of the most widely used, due to its simplicity and relatively low overhead. Since MP works sequentially, however, it is not straightforward to formulate it as a parallel algorithm, to take advantage of multicore platforms for real-time processing. In this article, we investigate how a slight modification of MP makes it possible to break down the decomposition into multiple local tasks, while avoiding blocking effects. Our simulations on audio signals indicate that this parallel local matching pursuit (PLoMP) gives results comparable to the original MP algorithm but could potentially run in a fraction of the time-on-the-fly sparse approximations of high-dimensional signals should soon become a reality.\"",
        "Document: \"Matching pursuit in adaptive dictionaries for scalable audio coding. In previous work [10], we have proposed a new signal representation for audio coding, where the signal is decomposed in a union of MDCT bases using matching pursuit. The resulting coder gave better performance than a transform coder at low bitrates but slightly worse at high bitrates. In this paper, we propose an adaptive matching pursuit algorithm that in the first iterations decomposes the signal into the redundant union of MDCT bases, and then, when the residual energy decay becomes too low, switches to an orthogonal basis (one of the MDCT bases). We investigate simple strategies to determine in which iteration switching is near-optimal in terms of rate-distortion. We present in this paper a prototype audio coder based on this algorithm, that reaches the performance of the previous approach at low bitrates and the one of transform coding at high bit rates.\"",
        "1 is \"Modeling musical instrument tones as dynamic textures\", 2 is \"Parameter estimation of superimposed signals using the EM algorithm\"",
        "Given above information, for an author who has written the paper with the title \"Microphone multiplexing with diffuse noise model-based principal component analysis\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006339": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Follow But No Track: Privacy Preserved Profile Publishing in Cyber-Physical Social Systems.':",
        "Document: \"MiSCon: a hot plugging tool for real-time motion-based system control. In this demonstration, we proposed a hot plugging tool for the real-time motion-based system control, which is more portable and application-independent than the existing commercial motion-based sensing devices such as Kinect, Wii and PlayStation Move. This tool captures and recognizes people's real-time motions through the built-in camera of PCs, mobile phones or tablets, and automatically executes the system events which have been mapped with people's customized body motion, e.g., the head and the fist. The tool relieves people from the conventional ways to play games and use applications, and enables them to customize their preferred ways to control the systems.\"",
        "Document: \"LyDAR: A LYrics Density based Approach to non-homogeneous music Resizing. In many scenarios, such as TV/radio advertising production, animation production, and presentation, music pieces are constrained in the metric of time. For example, an editor wants to use a 320s song to fit a 280s animation or to accompany a 265s radio advertisement. Current music resizing approach scales the whole piece of music in a uniform manner. However, it will degrade the effect of the compressed song and make perceptual artifacts. In this paper, a novel music resizing approach, called LyDAR (LYrics Density based Approach to non-homogeneous music Resizing), is proposed, in which the resizing operation is guided by music structural analysis. Firstly, a useful concept, lyrics density, is presented, which takes advantage of lyrics to analyze the musical structure and can be used to describe the compression-resistance for different parts of a song. Secondly, two music resizing scheduling algorithms, LDF and LDGF, are developed to schedule compression over different parts of a music piece. Finally, both subjective and objective experiments are conducted to show that LyDAR can effectively and efficiently generate compressed versions of songs with good quality.\"",
        "Document: \"Preference Join on Heterogeneous Data. There are different types of join operation dealing with different issues in database research. However, existing join operations cannot meet the increasing demands of the real world. In this paper, we define a new join operation, the preference join (p-join), which introduces the concepts of the personal preference and the satisfaction operator on various data types. We present a general join algorithm (Nested Loop) to deal with the p-join, and we also propose an advanced algorithm called MFV for p-join. To improve the MFV algorithm, two enhanced mapping methods are employed. A large number of experiments on both real-world and synthetic data sets are conducted. The experimental results demonstrate the effectiveness, efficiency and scalability of our methods, and show the advanced algorithms have advantages over the general algorithms.\"",
        "Document: \"Evaluation Models for the Effect of Sample Imbalance on Gene Selection. In this paper, we considered the problem of sample imbalance in the context of gene selection. Based on simple random sampling, two evaluation models were proposed to investigate the effect of sample imbalance on gene selection. Under the proposed evaluation models, the performances of five famous gene selection methods on the unbalanced data were compared. The experimental results indicated that the proposed evaluation models are effective and the sample imbalance has a great influence on gene selection. Our findings provide some guidelines in the design of microarray experiments and the following data analysis, and two evaluation models are suitable for selecting feasible gene selection method to identify differential expression genes\"",
        "1 is \"Multiple communication im multihop radio networks\", 2 is \"YouPivot: improving recall with contextual search\"",
        "Given above information, for an author who has written the paper with the title \"Follow But No Track: Privacy Preserved Profile Publishing in Cyber-Physical Social Systems.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006373": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Towards Transparent and Trustworthy Cloud.':",
        "Document: \"A framework for requirents monitoring of service based systems. This paper proposes a framework for monitoring the compliance of systems composed of web-services with requirements set for th. This framework assumes systems composed of web-services that are co-ordinated by a service composition process expressed in BPEL4WS and uses event calculus to specify the properties to be monitored. The monitorable properties may include behavioural properties of a syst which are automatically extracted from the specification of its composition process in BPEL4WS and/or assumptions that syst providers can specify in terms of events extracted from this specification.\"",
        "Document: \"Coverage-Based Testing for Service Level Agreements. Service level agreements (SLAs) are typically used to specify rules regarding the consumption of services that are agreed between the providers of the service-based applications (SBAs) and their consumers. An SLA includes a list of terms that contain the guarantees that must be fulfilled during the provisioning and consumption of the services. Since the violation of such guarantees may lead to the application of potential penalties, it is important to assure that the SBA behaves as expected. In this paper, we propose a proactive approach to test SLA-aware SBAs by means of identifying test requirements, which represent situations that are relevant to be tested. To address this issue, we define a four-valued logic that allows evaluating both the individual guarantee terms and their logical relationships. Grounded in this logic, we devise a test criterion based on the modified condition decision coverage (MCDC) in order to obtain a cost-effective set of test requirements from the structure of the SLA. Furthermore by analyzing the syntax and semantics of the agreement, we define specific rules to avoid non-feasible test requirements. The whole approach has been automated and applied over an eHealth case study.\"",
        "Document: \"Dynamic set-up of monitoring infrastructures for service based systems. Service based systems are intrinsically dynamic as the services deployed by them can be replaced at runtime. When this happens, the Service Level Agreements (SLAs) that regulate the provision of services may also need to change. Following such changes, the monitoring infrastructure that is used to monitor SLAs may also need to be modified to ensure the continuous provision of the necessary runtime checks. This paper presents a framework that supports the dynamic assessment of the monitorability of SLAs terms and the dynamic setup of an appropriate infrastructure for monitoring them following such changes. The monitorability checks are based on comparisons between the SLA terms for specific services and descriptions of the monitoring capabilities of these services which are expressed in languages introduced in the paper. The paper presents a prototype implementation of the framework and the results of a preliminary evaluation of it.\"",
        "Document: \"Describing and Verifying Monitoring Capabilities for SLA-Driven Service Based Systems. Monitoring the operation of Service Based Systems (SBS) to ensure compliance with a set of service level agreements (SLAs) for example cannot always rely on a pre-specified monitoring infrastructure, where all the information and components required for monitoring are a priori known and available. This because new services with unknown monitoring infrastructures and capabilities may be dynamically assembled to an SBS. To address the need for dynamic configuration of SBS monitoring infrastructures, this paper proposes a model for describing the monitoring capabilities of different services of an SBS and discusses the process for verifying the monitorability of required properties based on these capabilities.\"",
        "1 is \"Combinatorial completion by rule definition with interactive value colouring\", 2 is \"Secure Content Distribution for Digital Libraries\"",
        "Given above information, for an author who has written the paper with the title \"Towards Transparent and Trustworthy Cloud.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006445": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Mapping in heterogeneous systems with heuristic methods':",
        "Document: \"Design of parallel algorithms for the single resource allocation problem. Three new optimal parallel algorithms are presented for the single resource allocation problem. They run in the simplest networks: pipelines and rings. All of them have been implemented using PVM and MPI. Four representative platforms of the current state of parallel computing hardware were used for the experiences presented in this paper: the IBM SP2, the Cray T3E, the Silicon Origin 2000 and the Digital Alpha Server 8400. Computational results prove the good scalability of these three algorithms for practical cases.\"",
        "Document: \"From The Theory To The Tools: Parallel Dynamic Programming. Dynamic programming is an important paradigm that has been widely used to solve problems in various areas such as control theory, operation research, biology and computer science, We generalize the finite automaton formal model for dynamic programming deriving pipeline parallel algorithms. The optimality of these algorithms is established for the new class of non-decreasing finite automata, As an intermediate step for the construction of a skeleton for the automatic parallelization of dynamic programming, we have developed a tool for the implementation of pipeline algorithms. The tool maps the processes in the pipeline in the target architecture following a mix of block and cyclic policies adapted to the grain of the machine, Based on the former tool, the automatic parallelization of dynamic programming is straightforward. The use of the model and its associated tools is illustrated with the Single Resource Allocation Problem. The performance and portability of these tools is compared with specific 'hand made' code written by experienced programmers. The experimental results on distributed memory and shared distributed memory architectures prove the scalability of the proposed paradigm and its associated tools. Copyright (C) 2000 John Wiley & Sons, Ltd.\"",
        "Document: \"An Analytical Model for Pipeline Algorithms on Heterogeneous Clusters. The performance of a large class of virtual pipeline algorithms on heterogeneous networks is studied. The word heterogeneity refers here both to the processing and communication capabilities. To balance the differences among processors requires a vectorial combination of block and cyclic mappings, assigning different number of virtual processes per processor. Following a progressive approach, we derive a formula that predicts the performance for the general case. The computational results carried out on a heterogeneous cluster of PCs prove the effectiveness of the approach and the accuracy of the predictions.\"",
        "Document: \"Analytical Modeling of the Energy Consumption for the High Performance Linpack. Comparable to time performance models, it is now possible to estimate performance based upon energy consumption for HPC systems. The predictive ability of the analytical modeling is an interesting feature that motivates us to approach this methodology for the case of energy consumption. In this paper, we present an analytical model for predicting the energy consumption for the High Performance Linpack (HPL). The derived model can be used to know in advance the energy consumed by the HPL over a target architecture, and can be integrated into the schedulers of operating systems or queue managers. We established an experimental setup using a standard metered PDU that allowed us to measure the energy consumption for the HPL benchmark on our cluster. With the monitoring system in place, we can obtain the architectural and algorithmic parameters associated for both performance and energy analytical models. Also this has made possible watts and gflops-per-watt prediction when we execute Linpack executions with concrete algorithm parameters in our cluster.\"",
        "1 is \"Experiments with Scheduling Divisible Tasks in Clusters of Workstations\", 2 is \"A lossy 3D wavelet transform for high-quality compression of medical video\"",
        "Given above information, for an author who has written the paper with the title \"Mapping in heterogeneous systems with heuristic methods\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006476": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Adaptive frequency control technique for enhancing transient performance of DC-DC converters':",
        "Document: \"Fast charging technique for Li-Ion battery charger. In this paper, the compensation of internal resistance of the Li-Ion battery is proposed. The requirement of fast and steady charger becomes the most important issue for power management ICs. Refer to the characteristics of the battery, how to charge the battery with adequate current and fasten the time of charging is critical to the devisers. Due to the impedance of battery pack, the energy of charger charged to cell is consumed in it partly. As a result, the efficiency of charger is reduced inevitably. The previous design has proposed a dynamic circuit for reducing time [1]. However the technique demands the external device to compensate the defeat. Thus, this paper extends the period of the CC mode to charge the battery with a faster speed. Owing to the shifting voltage on the reference voltage, the charger can delay the time that the operation mode from CC mode to CV mode. That is a fast-charging charger can be achieved by a large constant current stored in the battery during a long constant current period. Simulation results verify the success of the fast-charging technique due to the internal resistance compensation.\"",
        "Document: \"A 0.6 V Resistance-Locked Loop Embedded Digital Low Dropout Regulator in 40 nm CMOS With 80.5% Power Supply Rejection Improvement. The proposed resistance-locked loop (RLL) can achieve high PSRR of -16 dB digital low dropout (DLDO) regulator without consuming much power which is the drawback in prior arts. Even at light loads, the RLL can be shut down for power saving. Furthermore, the duty compensator ensures DLDO stability under different duty ratio of supply voltage. The operation voltage of proposed DLDO can be down to 0.6 V and the peak current efficiency is 99.99%. The test chip was fabricated in 40 nm CMOS process with all the transistors implemented by core device for small silicon area.\"",
        "Document: \"Minimized Transient and Steady-State Cross Regulation in 55-nm CMOS Single-Inductor Dual-Output (SIDO) Step-Down DC-DC Converter. A single-inductor dual-output (SIDO) step-down DC-DC converter with continuous conduction mode (CCM) operation is proposed to achieve an area-efficient power management module. The low-voltage energy distribution controller (LV-EDC) can simultaneously guarantee good voltage regulation and low output voltage ripple. With the proposed dual-mode energy delivery methodology, cross regulation in steady-state output voltage ripple, which is rarely discussed, and cross regulation in load transient response are both effectively reduced. In addition, the energy mode transition operation helps obtain the appropriate energy operation mode using the energy delivery paths for dual outputs. Moreover, within the allowable output voltage ripple, the automatic energy bypass (AEB) mechanism can reduce the number of energy delivery paths, thereby ensuring voltage regulation and further enhancing efficiency. The test chip, fabricated in 55-nm CMOS, occupies 1.44 mm2 and achieves 91% peak efficiency, low output voltage ripple, and excellent load transient response for a high-efficiency system-on-a-chip (SoC) integration.\"",
        "Document: \"SAR-controlled adaptive off-time technique without sensing resistor for achieving high efficiency and accuracy LED lighting system. A successive approximation register (SAR) is utilized to control adaptive off-time in order to regulate accurate light-emitting diode (LED) current and improve efficiency of LED driver. The proposed SAR-controlled adaptive off-time technique without the external sensing resistor can sense the current flowing through LEDs during the turning on of the N-type power MOSFET, as well as provide adaptive off-time depending on the input voltage and the numbers of LED, to obtain an accurate LED current. Experimental results show the inductor current ripple is kept within \u00b115% of the DC current. As a result, line regulation is guaranteed in this proposed design.\"",
        "1 is \"4.6 An 85%-efficiency fully integrated 15-ratio recursive switched-capacitor DC-DC converter with 0.1-to-2.2V output voltage range\", 2 is \"Stealthy malware detection through vmm-based \"",
        "Given above information, for an author who has written the paper with the title \"Adaptive frequency control technique for enhancing transient performance of DC-DC converters\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006493": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Multi-method based algorithm for multi-objective problems under uncertainty.':",
        "Document: \"Analysing the regularity of genomes using compression and expression simplification. We propose expression simplification and tree compression as aids in understanding the evolution of regular structure in Genetic Programming individuals.We apply the analysis to two previously-published algorithms, which aimed to promote regular and repeated structure. One relies on subtree duplication operators, the other uses repeated evaluation during a developmental process. Both successfully generated solutions to difficult problems, their success being ascribed to promotion of regular structure. Our analysis modifies this ascription: the evolution of regular structure is more complex than anticipated, and the success of the techniques may have arisen from a combination of promotion of regularity, and other, so far unidentified, effects.\"",
        "Document: \"A reactive mitigation approach for managing supply disruption in a three-tier supply chain. In this paper, we develop a quantitative reactive mitigation approach for managing supply disruption for a supply chain. We consider a three-tier supply chain system with multiple raw material suppliers, a single manufacturer and multiple retailers, where the system may face sudden disruption in its raw material supply. First, we develop a mathematical model that generates a recovery plan after the occurrence of a single disruption. Here, the objective is to minimize the total cost during the recovery time window while being subject to supply, capacity, demand, and delivery constraints. We develop an efficient heuristic to solve the model for a single disruption. Second, we also consider multiple disruptions, where a new disruption may or may not affect the recovery plans of earlier disruptions. We also develop a new dynamic mathematical and heuristic approach that is capable of dealing with multiple disruptions, after the occurrence of each disruption as a series, on a real-time basis. We compare the heuristic solutions with those obtained by a standard search algorithm for a set of randomly generated disruption test problems, which shows the consistent performance of our heuristic. Finally, a simulation model is developed to analyze the effect of randomly generated disruption events that are not known in advance. The numerical results and many random experiments are presented to explain the usefulness of the developed models and methodologies.\"",
        "Document: \"Analyzing the Simple Ranking and Selection Process for Constrained Evolutionary Optimization. Many optimization problems that involve practical applications have functional constraints, and some of these constraints are active, meaning that they prevent any solution from improving the objective function value to the one that is better than any solution lying beyond the constraint limits. Therefore, the optimal solution usually lies on the boundary of the feasible region. In order to converge faster when solving such problems, a new ranking and selection scheme is introduced which exploits this feature of constrained problems. In conjunction with selection, a new crossover method is also presented based on three parents. When comparing the results of this new algorithm with six other evolutionary based methods, using 12 benchmark problems from the literature, it shows very encouraging performance. T-tests have been applied in this research to show if there is any statistically signiflcance difierences between the algorithms. A study has also been carried out in order to show the efiect of each component of the proposed\"",
        "Document: \"Task Decomposition for Optimization Problem Solving. This paper examines a new way of dividing computational tasks into smaller interacting components, in order to effectively solve constrained optimization problems. In dividing the tasks, we propose problem decomposition, and the use of GAs as the solution approach. In this paper, we consider problems with block angular structures with or without overlapping variables. We decompose not only the problem but also appropriately the chromosome for different components of the problem. We also design a communication process for exchanging information between the components. The approach can be implemented for solving large scale optimization problems using parallel machines. A number of test problems have been solved to demonstrate the use of the proposed approach. The results are very encouraging.\"",
        "1 is \"D2MOPSO: MOPSO based on decomposition and dominance with archiving using crowding distance in objective and solution spaces.\", 2 is \"An Effective Heuristic Algorithm for the Traveling-Salesman Problem\"",
        "Given above information, for an author who has written the paper with the title \"Multi-method based algorithm for multi-objective problems under uncertainty.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006632": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Discovering optimal imitation strategies':",
        "Document: \"Learning collaborative manipulation tasks by demonstration using a haptic interface. This paper presents a method by which a robot can learn through observation to perform a collaborative manipulation task, namely lifting an object. The task is first demonstrated by a user controlling the robot's hand via a haptic interface. Learning extracts statistical redundancies in the examples provided during training by using Gaussian Mixture Regression and Hidden Markov Model. Haptic communication reflects more than pure dynamic information on the task, and includes communication patterns, which result from the two users constantly adapting their hand motion to coordinate in time and space their respective motions. We show that the proposed statistical model can efficiently encapsulate typical communication patterns across different dyads of users, that are stereotypical of collaborative behaviours between humans and robots. The proposed learning approach is generative and can be used to drive the robot's retrieval of the task by ensuring a faithful reproduction of the overall dynamics of the task, namely by reproducing the force patterns for both lift the object and adapt to the human user's hand motion. This work shows the potential that teleoperation holds for transmitting both dynamic and communicative information on the task, which classical methods for programming by demonstration have traditionally overlooked.\"",
        "Document: \"Whole Body Model Predictive Control with a Memory of Motion: Experiments on a Torque-Controlled Talos. This paper presents the first successful experiment implementing whole-body model predictive control with state feedback on a torque-control humanoid robot. We demonstrate that our control scheme is able to do whole-body target tracking, control the balance in front of strong external perturbations and avoid collision with an external object. The key elements for this success are threefold. First, optimal control over a receding horizon is implemented with Crocoddyl, an optimal control library based on differential dynamics programming, providing state-feedback control in less than 10 ms. Second, a warm start strategy based on memory of motion has been implemented to overcome the sensitivity of the optimal control solver to initial conditions. Finally, the optimal trajectories are executed by a low-level torque controller, feedbacking on direct torque measurement at high frequency. This paper provides the details of the method, along with analytical benchmarks with the real humanoid robot Talos. A video of the experiment is available at https://peertube.laas.fr/videos/watch/cbc25927-337c-4635-a1bc-153b9aeb4135\"",
        "Document: \"Learning and Reproduction of Gestures by Imitation. We presented and evaluated an approach based on HMM, GMR, and dynamical systems to allow robots to acquire new skills by imitation. Using HMM allowed us to get rid of the explicit time dependency that was considered in our previous work [12], by encapsulating precedence information within the statistical representation. In the context of separated learning and reproduction processes, this novel formulation was systematically evaluated with respect to our previous approach, LWR [20], LWPR [21], and DMPs [13]. We finally presented applications on different kinds of robots to highlight the flexibility of the proposed approach in three different learning by imitation scenarios.\"",
        "Document: \"Learning basis skills by autonomous segmentation of humanoid motion trajectories. Manipulation tasks are characterized by continuous motion trajectories containing a set of key phases. In this paper, we propose a probabilistic method to autonomously segment the motion trajectories for estimating the key phases embedded in such a task. The autonomous segmentation process relies on principal component analysis to adaptively project into one of the low-dimensional subspaces, in which a Gaussian mixture model is learned based on Bayesian information criterion and expectation-maximization algorithms. The basis skills are estimated by a set of Gaussians approximating quasi-linear key phases, and those times spent calculated from the segmentation points between two consecutive Gaussians representing the local changes of dynamics and directions of the trajectories. The basis skills are then used to build novel motion trajectories with possible motion alternatives and optional parts. After sequentially reorganizing the basis skills, a Gaussian mixture regression process is used to retrieve smooth motion trajectories. Two experiments are presented to demonstrate the capability of the autonomous segmentation approach.\"",
        "1 is \"Towards A Real-Time Bayesian Imitation System For A Humanoid Robot\", 2 is \"Learning temporal, relational, force-dynamic event definitions from video\"",
        "Given above information, for an author who has written the paper with the title \"Discovering optimal imitation strategies\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006808": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Power System Extreme Event Detection: The Vulnerability Frontier':",
        "Document: \"Fast optimal load balancing algorithms for 1D partitioning. The one-dimensional decomposition of nonuniform workload arrays with optimal load balancing is investigated. The problem has been studied in the literature as the \"chains-on-chains partitioning\" problem. Despite the rich literature on exact algorithms, heuristics are still used in parallel computing community with the \"hope\" of good decompositions and the \"myth\" of exact algorithms being hard to implement and not runtime efficient. We show that exact algorithms yield significant improvements in load balance over heuristics with negligible overhead. Detailed pseudocodes of the proposed algorithms are provided for reproducibility. We start with a literature review and propose improvements and efficient implementation tips for these algorithms. We also introduce novel algorithms that are asymptotically and runtime efficient. Our experiments on sparse matrix and direct volume rendering datasets verify that balance can be significantly improved by using exact algorithms. The proposed exact algorithms are 100 times faster than a single sparse-matrix vector multiplication for 64-way decompositions on the average. We conclude that exact algorithms with proposed efficient implementations can effectively replace heuristics.\"",
        "Document: \"Communication Support for Adaptive Computation.  This memorycannot be utilized in subsequent phases, decreasing the total memory which is usablefor communication, thus potentially increasing the number of phases. Instead,another processor can temporarily move some of its data to this processor to freeup space for messages. An example is illustrated in Fig. 3. In this simple example,the top two processors want to exchange 100 units of data, but each has only oneunit of available memory. A simplistic approach will require 100 phases.... \"",
        "Document: \"Compacting sequences with invariant transition frequencies. Simulation-based power estimation is commonly used for its high accuracy despite excessive computation times. Techniques have been proposed to speed it up by compacting an input sequence while preserving its power-consumption characteristics. We propose a novel method to compact a sequence that preserves transition frequencies. We prove the problem is NP-complete, and propose a graph model to reduce it to that of finding a heaviest-weighted trail, and a heuristic utilizing this model. We also propose using multiple sequences for better accuracy with even shorter sequences. Experiments show that power dissipation can be estimated with an error of only 2.3&percnt;, while simulation times are reduced by 10. Proposed methods generate solutions that effectively preserve transition frequencies and that are very close to optimal. Experiments also show that multiple sequences grant more accurate results with even shorter sequences.\"",
        "Document: \"N-k-e Survivable Power System Design.   We consider the problem of designing (or augmenting) an electric power system such that it satisfies the N-k-e survivability criterion while minimizing total cost. The survivability criterion requires that at least (1-e) fraction of the total demand can still be met even if any k (or fewer) of the system components fail. We formulate this problem, taking into account both transmission and generation expansion planning, as a mixed-integer program. Two algorithms are designed and tested on modified instances from the IEEE-30-Bus and IEEE- 57-Bus systems. \"",
        "1 is \"Iterative algorithms for state estimation of jump Markov linear systems\", 2 is \"Maximum probability shortest path problem\"",
        "Given above information, for an author who has written the paper with the title \"Power System Extreme Event Detection: The Vulnerability Frontier\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006915": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A Surveillance Video Analysis And Storage Scheme For Scalable Synopsis Browsing':",
        "Document: \"Content-Based Rate Control For Low Bit-Rate Video Applications. In low bit-rate or very low bit-rate video applications, such as videophone and video conferencing, it is difficult for traditional methods to gain a satisfactory image quality. They often suffer from some shortcomings: images with artifacts caused by limited bandwidth and motion discontinuity caused by frame skipping when bit-streams are transmitted through a fixed rate channel in low-delay applications. To attack the two problems above, we propose a content-based rate control method for videophone and video conferencing applications. Our work is focused on three aspects: analysis of bit-streams in low bit-rate video applications, content-based bit allocation to facial areas and background, and low-delay rate control in object-layer. Experiments show that the proposed method can achieve better subjective quality compared with that of the test model TMN5 of H.263, and slightly worse performance in PSNR, but with significantly better subjective quality in the facial areas, compared with that of the TMN8 of H.263+.\"",
        "Document: \"A robust shot transition detection method based on support vector machine in compressed domain. In this paper we propose a new algorithm for shot transition detection. A multi-class support vector machine (SVM) classifier is constructed to differentiate frames of a video into three categories: abrupt change, gradual change and non-change. This approach enables us to integrate many kinds of features into a uniform structure and to eliminate arbitrary selection of thresholds. To enhance the robustness of the algorithm, we form the feature vector from all frames within a temporal windows, each frame represented by six features in compressed domain. Experimental results on TREC-2001 video data set have shown that the result of our algorithm is 8% higher than the best result of 2001 TREC evaluation in F1 comparison when cut and gradual changes are both considered.\"",
        "Document: \"Multi-camera Monitoring of Infusion Pump Use. When patients operate a home infusion pump, they maybe make some mistakes, and it will be dangerous. To detect potentially life threatening errors, we design an assistance system based on observation by multiple cameras and robust spatio-temporal algorithm. Firstly, we record the video by multiple cameras when people use the infusion pump. Secondly, we use a robust MoSIFT algorithm, which detects interest points and encodes not only their local appearance but also explicitly models local motion, to describe the action. Thirdly, we recognize each individual human operating step in the use of an infusion pump to see if the patient has correctly performed the required actions in a safe sequence. The specific infusion pump used for evaluation requires 22 operation steps from 12 action classes. From the experiments show that our best classifier can obtains an average rate of 56%, and MoSIFT algorithm is robust and stable.\"",
        "Document: \"A Detection-Aided Multi-target Tracking Algorithm. This paper addresses the problem of tracking multiple objects in monocular video sequences. This problem is difficult because one needs to identify the targets by the finite measurement data, which may be affected by variations of pose, environment clutters, etc. A particle filter based multi-target tracking framework is presented, which operates with a novel observation model. The proposed target appearance model is constructed by combining the local modules that divided according to the structure of human body. In order to depress the sample depletion during the approximation process, the observation information is used to construct the mixture proposal density by integrating the sampling manner guided by detected human faces areas with stochastic dynamic model to generate the new samples. Then the Markov chain Monte Carlo (MCMC) method was employed to recursively estimate the solution of multi-target data association problem. Experimental results show that the proposed tracker can effectively handle complex environments, irregular target motions and partial occlusions to keep the identities of the targets in real world.\"",
        "1 is \"Filterbank-based fingerprint matching\", 2 is \"Non-rigid object localization from color model using mean shift\"",
        "Given above information, for an author who has written the paper with the title \"A Surveillance Video Analysis And Storage Scheme For Scalable Synopsis Browsing\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006977": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Constructive and axiomatic approaches of fuzzy approximation operators':",
        "Document: \"Boolean Covering Approximation Space and Its Reduction. In this paper, Boolean vector algebra theory is introduced into rough set theory. A theoretical framework of Boolean covering approximation space is proposed, and based on the principle of traditional covering rough set theory, a pair of lower and upper approximation operators on a Boolean covering approximation space are defined. Properties of the lower and upper approximation operators are investigated in detail. The duality of the lower and upper approximation operators, and lower and upper definable Boolean vectors are discussed. Finally, reductions of lower and upper approximation operators are explored.\"",
        "Document: \"Rule acquisition in consistent multi-scale decision systems. With the view of granular computing, there are different granules at different levels of scale in data sets having hierarchical scale structures. In this paper, the concept of multi-scale decision systems is introduced firstly, then a formal approach to rule acquisition measured at different levels of granulations is proposed, and some algorithms for rule acquisition in consistent multi-scale decision systems are proposed with illustrative examples.\"",
        "Document: \"Dependence-space-based attribute reduction in consistent decision tables. This paper proposes a novel approach to attribute reduction in consistent decision tables within the framework of dependence spaces. For a consistent decision table $$(U,A\\cup \\{d\\}),$$ an equivalence relation r on the conditional attribute set A and a congruence relation R on the power set of A are constructed, respectively. Two closure operators, T r and T R , and two families of closed sets, $${\\mathcal C}_r$$ and $${\\mathcal C}_R,$$ are then constructed with respect to the two equivalence relations. After discussing the properties of $${\\mathcal C}_r$$ and $${\\mathcal C}_R,$$ the necessary and sufficient condition for $${\\mathcal C}_r={\\mathcal C}_R$$ is obtained and employed to formulate an approach to attribute reduction in consistent decision tables. It is also proved, under the condition $${\\mathcal C}_r={\\mathcal C}_R,$$ that a relative reduct is equivalent to a $$R$$ -reduction defined by Novotny and Pawlak (Fundam Inform 16:275\u2013287, 1992).\"",
        "Document: \"Attribute reduction based on evidence theory in incomplete decision systems. Attribute reduction is a basic issue in knowledge representation and data mining. This paper deals with attribute reduction in incomplete information systems and incomplete decision systems based on Dempster-Shafer theory of evidence. The concepts of plausibility reduct and belief reduct in incomplete information systems as well as relative plausibility reduct and relative belief reduct in incomplete decision systems are introduced. It is shown that in an incomplete information system an attribute set is a belief reduct if and only if it is a classical reduct and a plausibility consistent set must be a classical consistent set. In a consistent incomplete decision system, the concepts of relative reduct, relative plausibility reduct, and relative belief reduct are all equivalent. In an inconsistent incomplete decision system, an attribute set is a relative plausibility reduct if and only if it is a relative reduct, a plausibility consistent set must be a belief consistent set, and a belief consistent set is not a plausibility consistent set in general.\"",
        "1 is \"Fuzzy function as an approximate solution to a system of fuzzy relation equations\", 2 is \"Automated extraction of medical expert system rules from clinical databases based on rough set theory\"",
        "Given above information, for an author who has written the paper with the title \"Constructive and axiomatic approaches of fuzzy approximation operators\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006991": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Software diversity: state of the art and perspectives.':",
        "Document: \"Mobile home automation: merging mobile value added services and home automation technologies. In this paper we study mobile home automation, a field that emerges from an integration of mobile application platforms and\n home automation technologies. We motivate our research and provide a conceptual introduction, which illustrates the need for\n such applications by a two-dimensional conceptual model of mobility. As a first steps towards a solution we take the user\u2019s\n perspective and discuss different options of how he might access a mobile home automation service and the controlled devices.\n Subsequently, we suggest a general system architecture for mobile home automation services and discuss related design decision.\n This design has been implemented in a research prototype, which we named Remotile. This helps us to discuss typical components,\n such as modules that integrate various home automation devices.\"",
        "Document: \"Model-driven derivation of product architectures. Product Derivation is one of the central activities in Software Product Lines (SPL). One of the main challenges of the process of product derivation is dealing with complexity, which is caused by the large number of artifacts and dependencies between them. Another major challenge is maximizing development efficiency and reducing time-to-market, while at the same time producing high quality products. One approach to overcome these challenges is to automate the derivation process. To this end, this paper focuses on one particular activity of the derivation process; the derivation of the product-specific architecture and describes how this activity can be automated using a model-driven approach. The approach derives the product-specific architecture by selectively copying elements from the product-line architecture. The decision, which elements are included in the derived architecture, is based on a product-specific feature configuration. We present a prototype that implements the derivation as a model transformation described in the Atlas Transformation Language (ATL). We conclude with a short overview of related work and directions for future research\"",
        "Document: \"Integrating heterogeneous variability modeling approaches with invar. There have been several proposals to describe the variability of software product lines by using modeling languages. In larger organizations or projects (e.g., multi product line environments) this can lead to a situation where multiple variability modeling techniques are used simultaneously. Rather than enforcing a single modeling language, we present an integrative infrastructure that provides a unified perspective for users configuring products in such multi product line environments, regardless of the different modeling methods and tools used internally. In this tool demonstration paper, we present a prototypical implementation of our framework based on Web services. So far, the prototype has been used with a feature-based, an OVM-style and a decision-oriented variability modeling approach.\"",
        "Document: \"Aspectual Separation of Feature Dependencies for Flexible Feature Composition. Countless challenges to preserving a user\u2019s location privacy exist and have become more important than ever before with the proliferation of handheld devices and the pervasive use of Location-based Services. It is not possible to access Location-based ...\"",
        "1 is \"Proof reuse for deductive program verification\", 2 is \"Alias types for \u201cenvironment-aware\u201d computations\"",
        "Given above information, for an author who has written the paper with the title \"Software diversity: state of the art and perspectives.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007023": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Controller Design For Matrix Second-Order Nonlinear Plants Based On Requirements On Tracking Performance And Disturbance Rejection':",
        "Document: \"Human-robot interaction control for industrial robot arm through software platform for agents and knowledge management. This paper presents a new human-robot interaction control approach for industrial robot arm. With frame-based knowledge representation, a human-robot system as well as its interaction control process is modeled and defined in the software platform of agents and knowledge management (SPAK). By means of this software platform, human-robot interaction control for industrial robot arm is implemented. In this paper, an actual system comprised of human, humanoid robot (Robovie) and industrial robot arm (MELFA) is constructed and the experimental results demonstrate its effectiveness.\"",
        "Document: \"Design of Fast Multiple String Searching Based on Improved Prefix Tree. Multi-string matching is one of the most important components in data mining task. New applications in many technology fields require high performance string matching algorithms. This paper first presents a new string searching approach based on a data structure called prefix tree. The innovative algorithm eliminates the functional overlap of the table HASH and Prefix Function. Then we make a little improvement on the prefix tree and present a second algorithm that is faster and more space-saving. It is demonstrated analytically that the two algorithms inherit the optimality and are very competitive in practice. On tests of both real life and synthetic data, our algorithms are also efficient and especially effective for various string pattern and large alphabet sets.\"",
        "Document: \"Intelligent attitude control of space target based on characteristic model. With the purpose of assisting space robots to make on-orbit experiments, a space target is constructed. In order to realize good performance of attitude control for the space target, this paper proposes a characteristic modeling method. In this method, the space target, as a high-order plant, is described by a slowly time-varying difference equation. Corresponding to this method, a parameter estimation algorithm is given. Based on the characteristic model, an intelligent attitude controller for the space target is derived. The controller includes four components such as maintaining/tracking, feedback, logic integral and logic differential control law. The effectiveness of the proposed modeling and control method is verified through the experiments on a test-bed.\"",
        "Document: \"Synthetic fuzzy evaluation method of trajectory similarity in map-matching. Despite the rapid development of concepts used in current map-matching algorithms, the continuous movement of a vehicle is often ignored or just limited to 1 or several previous matches. This may result in errors in complicated situations such as at Y-junctions, and entrance or exit ramps of a highway. The trajectory-based map-matching algorithm, which determines the matching road by comparing the vehicle trajectory against candidate roads, has the potential to overcome this limitation. However, the curve-to-curve matching as a simple form of trajectory-based map-matching fails to address this. The key issue of a trajectory-based map-matching algorithm is how to evaluate the similarity between the trajectory and the possible traveling roads. This article develops a synthetic fuzzy evaluation method to address the issue. The similarities are quantified in terms of location, shape, direction, and behavior. As a new input in map-matching, vehicle behavior refers to changes in the motion of the vehicle such as turning, which is restrained by the geometry of traveling road. The article also proposes a multilevel fuzzy synthetic evaluation process to assess the similarities and hierarchically synthesize them into a final evaluation. The method is evaluated using GPS positions recorded with a vehicle traveling in the urban area of Beijing, China. The traveled road paths include complicated roads conditions such as flyovers, highway entrances and exits. The method identifies more than 98% of the road segments correctly, showing a significant improvement over existing map-matching algorithms.\"",
        "1 is \"LOCALE: Collaborative Localization Estimation for Sparse Mobile Sensor Networks\", 2 is \"Testing android apps through symbolic execution\"",
        "Given above information, for an author who has written the paper with the title \"Controller Design For Matrix Second-Order Nonlinear Plants Based On Requirements On Tracking Performance And Disturbance Rejection\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007037": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Domino of the smart grid: An empirical study of system behaviors in the interdependent network architecture':",
        "Document: \"On eavesdropping attacks and countermeasures for MU-MIMO systems. MU-IMMO beamforming offers great potential for the AP with multiple antennas to serve multiple receivers concurrently. A key factor to implement the MU-MIMO system is the CSI feedback mechanism. However., it might be exploited by malicious attackers to threaten the transmission security of benign clients. To better explore the attacks through false CSI feedback in MU-MIMO systems, this paper proposes a mathematically formulated strategy of eavesdropping attack, called polynomial attack. Based on the basic polynomial attack strategy, we investigate two kinds of advanced polynomial attacks: the passive polynomial attack and the active polynomial attack. They are discriminated by the different abilities in approaching to the eavesdropped victims. Besides proving the attack effects theoretically, we also develop defensive methods against the two attacks. Especially against the active attack, we propose a novel MU-MIMO scheme called AntiPoly, under which each client holds a pair of constant keys. One is the feedback key used for interacting with the AP in the CSI training period, and the other one is the transmission key used for decoding signals in the transmission period. With intensive experiments performed on actual measured CSI, the attack effects of polynomial attacks are illustrated, and our developed defensive methods are validated.\"",
        "Document: \"Congestion-Aware Indoor Emergency Navigation Algorithm for Wireless Sensor Networks. A typical application of wireless sensor networks is navigation for emergency evacuation whose goal is to guide people escaping from hazardous areas safely and quickly. In practical scenarios, the evacuating time depends not only on the length of the path but also on the congestion degree. However, as far as we know, the state of art fails to quantify congestion degree accurately in evacuation process. In this paper, we propose a novel congestion-aware navigation algorithm which has several key advantages: First, we use the concept of moving speed to evaluate the congestion degree to accurately estimate the evacuating time. Second, we avoid the frequent in-situ interactions between users and the navigation system by using the sensors at intersections for displaying the escape directions. Third, our algorithm can reduce the direction oscillations due to the network communication delay and adapt to the variation of hazardous regions. We evaluate our algorithm by simulations under various realistic settings. Simulation results show that our algorithm has the shorter evacuating time and fewer oscillations than state-of-the-art work.\"",
        "Document: \"Discovering Human Presence Activities With Smartphones Using Nonintrusive Wi-Fi Sniffer Sensors: The Big Data Prospective. With the explosive growth and wide-spread use of smartphones with Wi-Fi enabled, people are used to accessing the internet through Wi-Fi network interfaces of smartphones. Smartphones periodically transmit Wi-Fi messages, even when not connected to a network. In this paper, we describe the Mo-Fi system which monitors and aggregates large numbers of continuous Wi-Fi message transmissions from nearby smartphones in the area of interest using nonintrusive Wi-Fi sniffer sensors. In this paper, we propose an optimized Wi-Fi channel detection and selection method to switch the best channels automatically to aggregate the Wi-Fi messages based on channel data transmission weights and human presence activity classification method based on the features of human dwell duration sequences in order to evaluate the user engagement index. By deploying in the real-world office environment, we found that the performance of Wi-Fi messages aggregation of CAOCA and CACFA algorithms is over 3.8 times higher than the worst channel of FCA algorithms and about 76% of the best channel of FCA algorithms, and the human presence detection rate reached 87.4%.\"",
        "Document: \"Poster: Towards Fully Distributed User Authentication with Blockchain. User authentication in computer systems has been a cornerstone of computer security for decades. However, the existing user authentication schemes either require human cognitive ability to remember numerous complex id and password, or rely on a trusted third party which could fail due to technical failure or denial-of-service attacks. In this paper, we design a fully distributed user authentication framework with the blockchain technology. In our scheme, a user stores her identity in the blockchain, stores her encrypted personal information in a off-blockchain storage, and attaches a smart contract which grants different permissions to each website/application. When a user logs in a website/application, the service provider employs a challenge-response protocol to verify the identity of the user, and then retrieve the user's personal information from the off-blockchain storage.\"",
        "1 is \"Contention-aware admission control for ad hoc networks\", 2 is \"iSpy: automatic reconstruction of typed input from compromising reflections\"",
        "Given above information, for an author who has written the paper with the title \"Domino of the smart grid: An empirical study of system behaviors in the interdependent network architecture\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007048": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'An efficient algorithm for mining periodic high-utility sequential patterns.':",
        "Document: \"Target-Oriented Decision Analysis with Different Target Preferences. Decision maker's behavioral aspects play an important role in human decision making, and this was recognized by the award of the 2002 Nobel Prize in Economics to Daniel Kahneman. Target-oriented decision analysis lies in the philosophical root of bounded rationality as well as represents the S -shaped value function. In most studies on target-oriented decision making, monotonic assumptions are given in advance to simplify the problems, e.g., the attribute wealth. However, there are three types of target preferences: \"the more the better\" (corresponding to benefit target preference), \"the less the better\" (corresponding to cost target preference), and equal/range targets (too much or too little is not acceptable). Toward this end, two methods have been proposed to model the different types of target preferences: cumulative distribution function (cdf) based method and level set based method. These two methods can both induce four shaped value functions: S -shaped, inverse S -shaped, convex, and concave, which represents decision maker's psychological preference. The main difference between these two methods is that the level set based method induces a steeper value function than that by the cdf based method.\"",
        "Document: \"A Behavioral Decision Model Based On Fuzzy Targets In Decision Making Using Weather Information. Due to inevitable uncertainty in weather forecasts, many decision problems influenced by weather information have been formulated for decision making in uncertain situations. The fuzzy target-based decision making model we propose assumes that the decision maker assesses a fuzzy target expressing an aspiration, then selects the decision maximizing the possibility of attaining this target aspiration before making a decision. We then show that the decision maker's different behavior about the aspiration leads to different decisions depending on the decision maker's personal philosophy or experience. This behavioral analysis provides an interpretation for influencing psychological features of the decision maker in decision making and introduces an interesting link to attitudes towards risk by means of utility function.\"",
        "Document: \"A Fuzzy Target Based Model For Decision Making Under Uncertainty. In this paper, we aim at bringing fuzzy targets within the reach of the target-based decision model in decision analysis with uncertainty. After introduced the target-based interpretation of the expected value on which it is implicitly assumed a neutral behavior on attitude about target, we extend the target-based model for decision making under uncertainty using fuzzy targets, making use of Yager's procedure of converting possibility distributions into probability ones via a simple normalization. We also discuss a target-based framework of attitudinal decision making with uncertainty developed recently by Yager (1999).\"",
        "Document: \"Applying OWA operator to model group behaviors in uncertain QFD. It is a crucial step to derive the priority order of design requirements (DRs) from customer requirements (CRs) in quality function deployment (QFD). However, it is not straightforward to prioritize DRs due to two types of uncertainties: human subjective perception and user variability. This paper proposes an OWA based group decision-making approach to uncertain QFD with an application to a flexible manufacturing system design. The proposed model performs computations solely based on the order-based semantics of linguistic labels so as to eliminate the burden of quantifying qualitative concepts in QFD. Moreover, it incorporates the importance weights of users and the concept of fuzzy majority into aggregations of fuzzy preference relations of different DRs in order to model the group behaviors in QFD. Finally, based on a quantifier-guided net flow score procedure, the proposed model derives a priority ranking with a classification of DRs into important and unimportant ones so as to provide a better decision-support to the decision-maker.\"",
        "1 is \"A Fast Algorithm to Find Overlapping Communities in Networks\", 2 is \"Efficient mining of sequential patterns with time constraints by delimited pattern growth\"",
        "Given above information, for an author who has written the paper with the title \"An efficient algorithm for mining periodic high-utility sequential patterns.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007051": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Robust and efficient saliency modeling from image co-occurrence histograms.':",
        "Document: \"Self Learning Classification for Degraded Document Images by Sparse Representation. Document Image Binarization is a technique to segment text out from the background region of a document image, which is a challenging task due to high intensity variations of the document foreground and background. Recently, a series of document image binarization contests (DIBCOs) had been held that have drawn great research interest in this area. Several document binarization techniques have been proposed and achieve great performance on the contest datasets. However, those proposed techniques may not perform well on all kinds of degraded document images because it is difficult to design a classification method that correctly models the non-uniform degraded document background and text foreground simultaneously. In this paper, we propose a self learning classification framework that combines binary outputs of different binarization methods. The proposed framework makes used of the sparse representation to re-classify the document pixels and produces a better binary results. The experimental results on the recent DIBCO contests show the great performance and robustness of our proposed framework on different kinds of degraded document images.\"",
        "Document: \"Accurate recognition of words in scenes without character segmentation using recurrent neural network. Recognition of texts in scenes is one of the most important tasks in many computer vision applications. Though different scene text recognition techniques have been developed, scene text recognition under a generic condition is still a very open and challenging research problem. One major factor that defers the advance in this research area is character touching, where many characters in scene images are heavily touched with each other and cannot be segmented for recognition. In this paper, we proposed a novel scene text recognition technique that performs word level recognition without character segmentation. Our proposed technique has three advantages. First it converts each word image into a sequential signal for the scene text recognition. Second, it adapts the recurrent neural network (RNN) with Long Short Term Memory (LSTM), the technique that has been widely used for handwriting recognition in recent years. Third, by integrating multiple RNNs, an accurate recognition system is developed which is capable of recognizing scene texts including those heavily touched ones without character segmentation. Extensive experiments have been conducted over a number of datasets including several ICDAR Robust Reading datasets and Google Street View dataset. Experiments show that the proposed technique is capable of recognizing texts in scenes accurately.\"",
        "Document: \"Automatic document orientation detection and categorization through document vectorization. This paper presents an automatic orientation detection and categorization technique that is capable of detecting the orientation of multilingual documents with arbitrary skew and categorizing document images according to the underlying languages. We carry out orientation detection and categorization through document vectorization, which encodes document orientation and language information and converts each document image into an electronic document vector through the exploitation of the density and distribution of vertical component runs. For each language of interest, a pair of vector templates is first constructed through a training process. Orientation and category of the query image are then determined based on distances between the query document vector and the constructed vector templates. Experiments over 492 testing document images show that the average orientation detection and categorization rates reach up to 97.56% and 99.59%, respectively.\"",
        "Document: \"Object-Level Motion Detection From Moving Cameras. It is important for a moving observer to be able to identify his/her surrounding objects and determine whether these objects are moving or stationary, which is called object-level motion detection. Detecting object-level motion from moving cameras is a difficult problem to solve for collision-free navigation due to the dual motion introduced by the mixture of the camera motion and the object motio...\"",
        "1 is \"BinarizationShop: a user-assisted software suite for converting old documents to black-and-white\", 2 is \"FRank: a ranking method with fidelity loss\"",
        "Given above information, for an author who has written the paper with the title \"Robust and efficient saliency modeling from image co-occurrence histograms.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007054": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Position control of collision-tolerant passive mobile manipulator with base suspension characteristics':",
        "Document: \"A Tele-Operated Humanoid Robot Drives A Backhoe In The Open Air. This is the first successful trial in the world to remotely control a humanoid robot so as to drive an industrial vehicle (backhoe) outdoors in lieu of a human operator (see video). Furthermore, the robot's operation was controlled while! having it wear protective clothing to protect it against the rain and dust outside. This too marks a world-first success demonstrating the humanoid robot's capability of performing outdoor work even in the rain. These results were achieved thanks to the development of the following three technologies: (1) the \"remote control technology\" for instructing the humanoid robot to perform total body movements under remote control and the \"remote control system\" for executing the remote control tasks; (2) the \"protection technology\" for protecting the humanoid robot against shock and vibrations of its operating seat and against the influences of the natural environment such as rain and dust; and (3) \"full-body operation control technology\" for controlling the humanoid robot's total body work movements with autonomous control capability to prevent the robot from falling over. The humanoid robot has a promising application potential for restoration work in environments struck by catastrophes and in civil engineering and construction project sites where it can \"work\" safely and smoothly.\"",
        "Document: \"Evaluation Of A Stabilizer For Biped Walk With Toe Support Phase. We discuss a feedback controller to stabilize biped walking which has toe support phase mimicking human gait. Using a reference walking pattern proposed in our previous work [11], our stabilizer can realize reliable walking. To evaluate the quality of stabilization, we propose two indicators, the maximum floor reaction force and the root mean square of the CoM tracking error. From our walking experiments, these indicators suggest us two policies of control parameter tuning, (1) not to control ZMP at toe support, and (2) not to use the ZMP phase-lead compensation for sagittal motion. These findings were validated by simulations of linear inverted pendulum model. It is shown that the observed behavior of the controller is caused by large velocity dissipation at support exchange.\"",
        "Document: \"Resolved Momentum Control: Humanoid Motion Planning Based On The Linear And Angular Momentum. We introduce a method to generate whole body motion of a humanoid robot such that the resulted total linear/angular momenta become specified values. First, we derive a linear equation which gives the total momentum of a robot from its physical parameters, the base link speed and the joint speeds. Constraints between the legs and the environment are also considered. The whole body motion is calculated from a given momentum reference by using a pseudo-inverse of the inertia matrix. As examples, we generated the kicking and walking motions and tested on the actual humanoid robot HRP-2. This method, the Resolved Momentum Control, gives us a unified framework to generate various maneuver of humanoid robots.\"",
        "Document: \"Whole Body Teleoperation Of A Humanoid Robot Integrating Operator'S Intention And Robot'S Autonomy - An Experimental Verification. This paper presents a whole body teleoperation system for a humanoid robot integrating operator's intention and the robot's autonomy. Instead of giving command to all the joints of the body, we have been proposing a switching command based teleoperation system of which the operator selects only the necessary points of the robot's body to manipulate depending on the required tasks. We have also proposed a whole body motion generation method which satisfies both the desired movement of specific points of the robot's body and the robot's balance constraint. This paper first explains the concept of the proposed teleoperation system and the autonomous functions for maintaining balance and expanding the reachable area of a humanoid robot. Then the paper reports on the experimental results using the proposed methods to teleoperate a real humanoid robot HRP-1S to perform balanced whole body motions satisfying operator's input command (see video).\"",
        "1 is \"Design of multi-DOF jumping robot\", 2 is \"Vision-Based, Distributed Control Laws for Motion Coordination of Nonholonomic Robots\"",
        "Given above information, for an author who has written the paper with the title \"Position control of collision-tolerant passive mobile manipulator with base suspension characteristics\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007069": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Tranquility: A Low Disruptive Alternative to Quiescence for Ensuring Safe Dynamic Updates':",
        "Document: \"Visually characterizing source code changes. Revision Control Systems (e.g., SVN, Git, Mercurial) include automatic and advanced merging algorithms that help developers to merge their modifications with development repositories. While these systems can help to textually detect conflicts, they do not help to identify the semantic consequences of a change. Unfortunately, there is little support to help release masters (integrators) to take decisions about the integration of changes into the system release. Most of the time, the release master needs to read all the modified code, check the diffs to build an idea of a change, and dig for details from related unchanged code to understand the context and potential impact of some changes. As a result, such a task can be overwhelming. In this article we present a visualization tool to support integrators of object-oriented programs in comprehending changes. Our approach named Torch characterizes changes based on structural information, authors and symbolic information. It mixes text-based diff information with visual representation and metrics characterizing the changes. The current implementation of our approach analyses Smalltalk programs, and thus we describe our experiments applying it to Pharo, a large open-source system. We also report on the evaluations of our approach by release masters and developers of several open-source projects.\"",
        "Document: \"Time warp, an approach for reasoning over system histories. The version history of a software system contains a wealth of information that can assist developers in their daily implementation and maintenance tasks. By reasoning over the role of certain code entities in previous versions of the system, developers can better understand their current state, assess the required maintenance and avoid making the same mistakes over and over again. Unfortunately, current approaches do not offer a means to easily extract specific information about the source code from such a version history. In this paper we present Time Warp, a library of logic predicates that builds on the SOUL language and the FAMIX and Hismo meta-models and that allows writing queries about the history of a system. By means of a number of concrete examples, we demonstrate how our approach can be used to express interesting queries over the version history of a system.\"",
        "Document: \"KALA: Kernel Aspect language for advanced transactions. Transaction management is a known cross-cutting concern. Previous research has been conducted to express this concern as an aspect. However, this work uses general-purpose aspect languages which lack a formal foundation and are unable to express advanced models for transaction management. In contrast, we designed a domain-specific aspect language for advanced transaction management, called KALA, that is based on a formalism for advanced transactions. As a result, KALA covers the field of advanced transaction management while obtaining a much higher level of abstraction than is achieved with general-purpose aspect languages. In this paper we detail the creation process of KALA.\"",
        "Document: \"Disentangling the implementation of local-to-global transformations in a rewrite rule transformation system. Transformation rules are often used to implement compilers for domain-specific languages. In an ideal situation, each transformation rule is a modular unit transforming one input element of the source program into a new element of the output program. However, in practice, transformation rules must be written which take one input element and produce several new elements belonging to various locations in the output program, the so-called local-to-global transformations. The implementation of such transformations is very complex and tightly coupled which imposes severe constraints on maintenance and evolvability. In this paper, we propose a transformation architecture on top of rewrite rules to loosen this coupling. The resulting transformation system combines the simplicity and modularity properties of rewrite rules with a new semi-automatic composition system that enables the implementation of local-to-global transformations without hampering maintenance and future evolutions.\"",
        "1 is \"Google's MapReduce programming model \u2014 Revisited\", 2 is \"On the clustering of multidimensional pictorial data\"",
        "Given above information, for an author who has written the paper with the title \"Tranquility: A Low Disruptive Alternative to Quiescence for Ensuring Safe Dynamic Updates\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007184": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Information-Theoretic Capacity and Error Exponents of Stationary Point Processes under Random Additive Displacements':",
        "Document: \"Quantization Effects in Low-Density Parity-Check Decoders. A. class of combinatorial structures, called absorbing sets, strongly influences the performance of low-density parity- check (LDPC) decoders. In particular, the quantization scheme strongly affects which absorbing sets dominate in the error-floor region. Absorbing sets may be characterized as weak or strong. They are a characteristic of the parity check matrix of a code. Conventional quantization schemes applied to a (2209,1978) array-based LDPC code can induce low-weight weak absorbing sets and, as a result, elevate the error floor. Adaptive quantization schemes alleviate the effects of weak absorbing sets, and, as a result, only the strong ones dominate the error floor of an optimized decoder implementation. Another benefit of an adaptive quantization scheme is that it performs well even in very few iterations.\"",
        "Document: \"Non-interactive simulation of joint distributions: The Hirschfeld-Gebelein-R\u00e9nyi maximal correlation and the hypercontractivity ribbon. We consider the following problem: Alice and Bob observe sequences X-n and Y-n respectively where {(X-i; Y-i)}(i=1)(infinity) are drawn i.i.d. from P (x, y); and they output U and V respectively which is required to have a joint law that is close in total variation to a specified Q (u, v) : One important technique to establish impossibility results for this problem is the Hirschfeld-Gebelein-Renyi maximal correlation which was considered by Witsen-hausen [1]. Hypercontractivity studied by Ahlswede and Gacs [2] and reverse hypercontractivity recently studied by Mossel et al. [3] provide another approach for proving impossibility results. We consider the tightest impossibility results that can be obtained using hypercontractivity and reverse hypercontractivity and provide a necessary and sufficient condition on the source distribution P(x, y) for when this approach subsumes the maximal correlation approach. We show that the binary pair source distribution with symmetric noise satisfies this condition.\"",
        "Document: \"On Non-Interactive Simulation of Joint Distributions. We consider the following non-interactive simulation problem: Alice and Bob observe sequences Xn and Yn, respectively, where ((Xi, Yi)}i=1n are drawn independent identically distributed from P(x, y), and they output U and V, respectively, which is required to have a joint law that is close in total variation to a specified Q(u, v). It is known that the maximal correlation of U and V must necessari...\"",
        "Document: \"A Synchronization Technique for Array-based LDPC Codes in Channels With Varying Sampling Rate. We describe a method for enhancing the syn- chronization error correction properties of an array-based low density parity check (LDPC) code. The proposed method uses code expurgation: a linear subcode is retained for message encoding and additional input bits are used for protection against synchronization errors. The method is easy to implement and incurs minimal loss in rate. I. INTRODUCTION Many communication systems use a substitution-error cor- recting code to encode a binary input message x into a coded sequence c = C(x). The modulated version of this sequence, corrupted by additive noise, arrives at the receiver as a waveform r(t), r(t )= \ufffd i cih(t \u2212 iT )+ n(t),\"",
        "1 is \"Rate Region of the Quadratic Gaussian CEO Problem\", 2 is \"Lecture Notes on Network Information Theory\"",
        "Given above information, for an author who has written the paper with the title \"Information-Theoretic Capacity and Error Exponents of Stationary Point Processes under Random Additive Displacements\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007202": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Relaxing From Vocabulary: Robust Weakly-Supervised Deep Learning for Vocabulary-Free Image Tagging':",
        "Document: \"A low power 10-bit 100-MS/s SAR ADC in 65nm CMOS. A low power 1V 10-bit 100MS/s successive approximation register (SAR) analog-to-digital (ADC) converter is presented. The use of top-plate-sample switching procedure and split capacitive array dramatically reduces total capacitance and saves switching energy. As small total capacitance and split structure together make capacitive array highly sensitive to parasitic capacitance, its layout becomes the key of the ADC. High sampling rate leads to ultra-high logic control clock frequency, so a variable self-timed clock generator is implemented and logical optimization to improve conversion speed is done inside. Special cares are taken to prevent charge leakage caused by 65nm GP process from ruining the dynamic performance. The prototype was fabricated in a 65nm 1P9M GP CMOS technology. Post simulation results show a peak SNDR of 58.364dB and 1.6mW total power consumption with a figure of merit (FOM) of 24fJ/conversion-step.\"",
        "Document: \"A split-capacitor vcm-based capacitor-switching scheme for low-power SAR ADCs. A split-capacitor Vcm-based capacitor-switching scheme is proposed for successive approximation register (SAR) analog-to-digital converters (ADCs) to reduce the capacitor-switching energy. By rearranging the structure and procedure of the capacitive array, the scheme can save the capacitor-switching energy by about 92% than the conventional scheme with better monotonicity. Meanwhile, a two-segment DC offset correction scheme for the comparator is also proposed to meet the speed and accuracy requirements. These techniques are utilized in the design of a 10b 70MS/s SAR ADC in 65nm 1P9M CMOS technology. Measurement results show a peak signal-to-noise-and-distortion ratio (SNDR) of 53.2dB, while consuming 960\u03bcW from 1.2V supply. The figure of merit (FoM) is 36.8fJ/Conversion-step and the total active area is 220\u00d7220\u03bcm2.\"",
        "Document: \"Deep Bidirectional Cross-Triplet Embedding for Online Clothing Shopping. In this article, we address the cross-domain (i.e., street and shop) clothing retrieval problem and investigate its real-world applications for online clothing shopping. It is a challenging problem due to the large discrepancy between street and shop domain images. We focus on learning an effective feature-embedding model to generate robust and discriminative feature representation across domains. Existing triplet embedding models achieve promising results by finding an embedding metric in which the distance between negative pairs is larger than the distance between positive pairs plus a margin. However, existing methods do not address the challenges in the cross-domain clothing retrieval scenario sufficiently. First, the intradomain and cross-domain data relationships need to be considered simultaneously. Second, the number of matched and nonmatched cross-domain pairs are unbalanced. To address these challenges, we propose a deep cross-triplet embedding algorithm together with a cross-triplet sampling strategy. The extensive experimental evaluations demonstrate the effectiveness of the proposed algorithms well. Furthermore, we investigate two novel online shopping applications, clothing trying on and accessories recommendation, based on a unified cross-domain clothing retrieval framework.\n\n\"",
        "Document: \"Low-Rank Discriminant Embedding for Multiview Learning. This paper focuses on the specific problem of multiview learning where samples have the same feature set but different probability distributions, e.g., different viewpoints or different modalities. Since samples lying in different distributions cannot be compared directly, this paper aims to learn a latent subspace shared by multiple views assuming that the input views are generated from this late...\"",
        "1 is \"Active learning via transductive experimental design\", 2 is \"Recent advances and emerging challenges of feature selection in the context of big data\"",
        "Given above information, for an author who has written the paper with the title \"Relaxing From Vocabulary: Robust Weakly-Supervised Deep Learning for Vocabulary-Free Image Tagging\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007225": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'LMI-based neurocontroller for guaranteed cost control of uncertain time-delay systems':",
        "Document: \"LMI-based neurocontroller for guaranteed cost control of uncertain servo system. This paper considers the robust tracking control problem that is based on the guaranteed cost control approach with additive gain perturbation. Based on the linear matrix inequality (LMI) design approach, a class of a state feedback controller is established and some sufficient conditions for the existence of guaranteed cost controller are derived. A novel contribution is that the guaranteed cost control is applied to uncertain servo system for the first time. As a result, it is shown that the robust stability for uncertain servo system and the reduction of the cost performance are both attained\"",
        "Document: \"Recursive approach of optimal Kalman filtering problem for multiparameter singularly perturbed systems. In this paper, the optimal Kalman filtering problem for multiparameter singularly perturbed systems is considered. In order to obtain the filter gain, the solution of the multiparameter algebraic Riccati equations (MAREs) is needed. The main contributions in this paper are to propose a new recursive algorithm for solving the MARE and to establish sufficient conditions related to the convergence property of the proposed algorithm. Using the recursive algorithm, it is shown that the solution of the MARE converges to a positive semidefinite stabilizing solution with the rate of convergence of O(||\u00b5||i). Moreover, it is proved that the mean square error via the proposed high-order filter attain, the O(||\u00b5||2i+1) approximation compared with the optimal filter.\"",
        "Document: \"Static output feedback H2/H\u221e control of infinite horizon Markov jump linear stochastic systems with multiple decision makers. In this paper, we discuss infinite-horizon H2/H\u221e control problem for Markov jump linear stochastic system governed by Ito\u0302 differential equation with state, control and external disturbance dependent noise involving multiple decision makers. The state and static output feedback strategies are both considered. It is shown that equilibrium conditions are established by the set of cross-coupled stochastic algebraic Riccati equations (CSAREs) for the first time. Furthermore, a necessary condition for the existence of the strategy set is discussed. In particular, it is noteworthy that our new results completely involve some existing results on stochastic H2/H\u221e control for linear case in the sense that the multiple decision makers and state, control and disturbance dependent noise are considered. Finally, a numerical example to verify the efficiency of the proposed algorithms is given.\"",
        "Document: \"Multi-Objective Decision-Making Problems For Discrete-Time Stochastic Systems With State- And Disturbance-Dependent Noise. In this paper, we consider three types of infinite-horizon multi-objective decision-making problems for a class of discrete-time linear stochastic systems with state-and disturbance-dependent noise. First, the H-2/H-infinity control problem with multiple decision makers is considered. Second, in order to improve the transient response, the linear quadratic control under the Pareto solution is investigated. Finally, the soft-constrained stochastic Nash games are formulated in which robustness is attained against disturbance input. The decision strategies for the three types of problem are derived. It is found that the conditions for the existences of these strategies are related to the solutions of cross-coupled stochastic algebraic Riccati equations (CSAREs). We develop some new algorithms based on linear matrix inequality (LMI) to solve the CSAREs. Numerical example is provided to verify the efficiency of the proposed decision strategies.\"",
        "1 is \"On the closure properties of robotic grasping\", 2 is \"Performance analysis of TCP-friendly AIMD algorithms for multimedia applications\"",
        "Given above information, for an author who has written the paper with the title \"LMI-based neurocontroller for guaranteed cost control of uncertain time-delay systems\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007250": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'V-Dd-Hopping Accelerators For On-Chip Power Supply Circuit To Achieve Nano Second-Order Transient Time':",
        "Document: \"Parallel-processing VLSI architecture for mixed integer linear programming. This paper describes parallel processor architecture for a mixed integer linear programming (MILP) solver to realize motion planning and hybrid system control in robot applications. It features pipeline architecture with an MILP-specific configuration and two-port SRAM. Based on the architecture, both FPGA and VLSI implementations have been done to solve sample problems including 16 variables. The FPGA implementation can reduce the power consumption to 13 W: an 85.4% reduction compared to a 3.0-GHz processor (Pentium 4; Intel Corp.). The VLSI solver further reduces the power to 6.4 W using 0.18-mu m CMOS technology.\"",
        "Document: \"Reconfiguring Cache Associativity: Adaptive Cache Design For Wide-Range Reliable Low-Voltage Operation Using 7t/14t Sram. This paper presents an adaptive cache architecture for wide-range reliable low-voltage operations. The proposed associativity-reconfigurable cache consists of pairs of cache ways so that it can exploit the recovery feature of the novel 7T/14T SRAM cell. Each pair has two operating modes that can be selected based upon the required voltage level of current operating conditions: normal mode for high performance and dependable mode for reliable low-voltage operations. We can obtain reliable low-voltage operations by application of the dependable mode to weaker pairs that cannot operate reliably at low voltages. Meanwhile leaving stronger pairs in the normal mode, we can minimize performance losses. Our chip measurement results show that the proposed cache can trade off its associativity with the minimum operating voltage. Moreover, it can decrease the minimum operating voltage by 140 mV achieving 67.48% and 26.70% reduction of the power dissipation and energy per instruction. Processor simulation results show that designing the on-chip caches using the proposed scheme results in 2.95% maximum IPC losses, but it can be chosen various performance levels. Area estimation results show that the proposed cache adds area overhead of 1.61% and 5.49% in 32-KB and 256-KB caches, respectively.\"",
        "Document: \"V-Dd-Hopping Accelerators For On-Chip Power Supply Circuit To Achieve Nano Second-Order Transient Time. A VD D-hopping accelerator for on-chip power supply circuits is proposed and the effectiveness of the accelerator circuit is experimentally verified. The quick dropper with the linear regulator enables nanosecond-order transient time in on-chip distributed power supply systems. The measured transition time is less than 5 ns with a load circuit equivalent to 25-k logic gates in 0.18-mu m CMOS. This is to be compared with the case without the accelerator of the order of mu s and thus the acceleration by two orders of magnitude is achieved. Extensions of the basic approach are also discussed including implementation of the quick dropper for a switching DC-DC converter, the control stability improvement, automatic timing generation, and the parasitic element effects of the power lines.\"",
        "Document: \"A 14\u00b5A ECG processor with noise tolerant heart rate extractor and FeRAM for wearable healthcare systems. This report describes an electrocardiograph (ECG) processor for use with a wearable healthcare system. It comprises an analog front end, a 12-bit ADC, a robust Instantaneous Heart Rate (IHR) monitor, a 32-bit Cortex-M0 core, and 64 Kbyte Ferroelectric Random Access Memory (FeRAM). The IHR monitor uses a short-term autocorrelation (STAC) algorithm to improve the heart-rate detection accuracy despite its use in noisy conditions. The ECG processor chip consumes 13.7\u03bcA for heart rate logging application.\"",
        "1 is \"Deadlock-Free Message Routing in Multiprocessor Interconnection Networks\", 2 is \"A constrained vector quantization scheme for real-time codebook retransmission\"",
        "Given above information, for an author who has written the paper with the title \"V-Dd-Hopping Accelerators For On-Chip Power Supply Circuit To Achieve Nano Second-Order Transient Time\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007296": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A Modelling Framework to Support Internal Control':",
        "Document: \"User-Defined On-Demand Matching. We propose a user-defined on-demand matching strategy. called O-matching, in which Users can control the order of matching arguments of each operation symbol. In ordinary matching schemes it is not important to set the order of matching. however, in on-demand matching schemes, it is very important since an input term may be changed while doing the on-demand matching process. O-matching is suitable to combine with the E-strategy. which is a user-defined reduction strategy in which users can control the order of reducing arguments. We show a sufficient condition under which the E-strategy with O-matching is correct for head normal forms, that is, any reduced term is a head normal form.\"",
        "Document: \"Formal Analysis of the Bakery Protocol with Consideration of Nonatomic Reads and Writes. The bakery protocol is the first real solution of the mutual exclusion problem. It does not assume any lower mutual exclusion protocols. The bakery protocol has been often used as a benchmark to demonstrate that proposed verification methods and/or tools are powerful enough. But, the true bakery protocol has been rarely used. We have formally proved that the protocol satisfies the mutual exclusion property. The proof is mechanized with CafeOBJ, an algebraic specification language, in which state machines as well as data types can be specified. Nonatomic reads and writes to shared variables are formalized by representing an assignment to a shared variable with multiple atomic transitions. Our formal model of the protocol has states in which a shared variable is being modified. A read to the variable in such states obtains an arbitrary value, which is represented as a CafeOBJ term.\"",
        "Document: \"State Machines as Inductive Types. We describe a way to write state machines inductively. The proposed method makes it possible to use the standard techniques for proving theorems on inductive types to verify that state machines satisfy invariant properties. A mutual exclusion protocol using a queue is used to exemplify the proposed method.\"",
        "Document: \"Comparison of Maude and SAL by Conducting Case Studies Model Checking a Distributed Algorithm. SAL is a toolkit for analyzing transition systems, providing several different tools. Among the tools are a BDD-based symbolic model checker (SMC) and an SMT-based infinite bounded model checker (infBMC). The unique functionality provided by SAL is k-induction, which is supported by infBMC. Given appropriate lemmas, infBMC can prove automatically by k-induction that an infinite-state transition system satisfies invariant properties. Maude is a specification language and system based on membership equational logic and rewriting logic. Maude is equipped with an on-the-fly explicit state model checker. The unique functionality provided by the Maude model checker supports inductive data types. We make a comparison of SAL (especially SMC and infBMC) and the Maude model checker by conducting case studies in which the Suzuki-Kasami distributed mutual exclusion algorithm is analyzed. The purpose of the comparison is to clarify some of the two tools' functionalities, especially the unique ones, through the case studies.\"",
        "1 is \"Autoscaling Web Applications in Heterogeneous Cloud Infrastructures\", 2 is \"A new solution of Dijkstra's concurrent programming problem\"",
        "Given above information, for an author who has written the paper with the title \"A Modelling Framework to Support Internal Control\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007315": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'SMDP-based Downlink Packet Scheduling Scheme for Solar Energy Assisted Heterogeneous Networks.':",
        "Document: \"New p-ary sequence family with low correlation and large linear span. In this paper, for an odd prime p and positive integers n, m, and e such that n =\u00a0me, a new family $${\\mathcal{S}}$$ of p-ary sequences of period p n \u2212 1 with low correlation and large linear span is constructed. It is shown that $${\\mathcal{S}}$$ has maximum correlation $${1+p^{n+2e\\over 2}}$$, family size p n , and maximal linear span $${{(m+3)n\\over 2}}$$. When m is even, the proposed family $${\\mathcal{S}}$$ contains Tang, Udaya, and Fan\u2019s construction as a subset. Furthermore, when n is even and $${e=1, \\mathcal{S}}$$ has the same correlation and family size, but larger linear span compared with the construction by Seo, Kim, No, and Shin.\"",
        "Document: \"A new frequency-hopping sequence set based upon generalized cyclotomy. In this paper, a new set of frequency-hopping sequences is proposed, and the Hamming correlation distribution of the new set is investigated. The construction of new frequency hopping sequences is based upon generalized cyclotomy. It is shown that the proposed frequency-hopping sequence set is optimal with respect to the average Hamming correlation bound.\"",
        "Document: \"A Time-Efficient Pair-Wise Collision-Resolving Protocol for Missing Tag Identification. Radio frequency identification (RFID) technology has been employed in wide-raging application domains. In most RFID applications, time-efficient identification of missing tags is one of the most fundamental objectives, especially for asset management and anti-theft purposes. In this paper, we propose a time-efficient pair-wise collision-resolving missing tag identification (PCMTI) protocol for lar...\"",
        "Document: \"Optimal and perfect difference systems of sets from q-ary sequences with difference-balanced property. Difference systems of sets (DSS) are important for the construction of codes for synchronization. In this paper, a general construction of optimal and perfect difference systems of sets based on q-ary sequences of period n = -1 (mod q) with difference- balanced property is presented, where q is a prime power. This works for all the known q-ary sequences with ideal autocorrelation, and generalizes the earlier construction based on ternary sequences with ideal autocorrelation. In addition, we construct another class of optimal and perfect difference systems of sets, employing decimation of q-ary d-form sequences of period q (m) -1 with difference-balanced property, which generalizes the previous construction from power functions.\"",
        "1 is \"Optimal Zero-Correlation Zone Sequence Set Constructed from a Perfect Sequence\", 2 is \"PIE: Parser identification in embedded systems\"",
        "Given above information, for an author who has written the paper with the title \"SMDP-based Downlink Packet Scheduling Scheme for Solar Energy Assisted Heterogeneous Networks.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007326": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A Proxy Server-Network for Real-Time Computer Games':",
        "Document: \"A survey on real-world implementations of mobile ad-hoc networks. Simulation and emulation are valuable techniques for the evaluation of algorithms and protocols used in mobile ad-hoc networks. However, these techniques always require the simplification of real-world properties such as radio characteristics or node mobility. It has been shown that this may lead to results and conclusions which do not reflect the behavior of ad-hoc networks in the real world. Various prototype implementations demonstrate that even simple protocols such as flooding do not behave as it was predicted by earlier simulation. To overcome this problem, real-world experiments are required. In this paper, we present a survey on existing real-world implementations of mobile ad-hoc networks. We report on the technology used for the implementations as well as on key findings from experiments conducted with these implementations.\"",
        "Document: \"Peer-to-peer data structures for cooperative traffic information systems. Cooperative traffic information systems support the driver of a car in selecting a route, based on traffic information collected by other cars. We propose to use a peer-to-peer network based on Internet access via cellular networks to distribute traffic information between the participants of such a system. This approach avoids the well-known limitations of VANET-based communication. Since the data maintained in a cooperative traffic information system has a very specific structure, it is particularly profitable-in terms of bandwidth consumption and latency-to tailor the system to this specific application domain instead of re-using generic peer-to-peer approaches. This realization led us to the development of GraphTIS-a peer-to-peer network specifically designed to manage traffic information. In this paper, we derive, step-by-step, the core mechanisms of GraphTIS, starting with a standard peer-to-peer system, outlining a first solution-named PeerTIS-which is based on a modification of this standard DHT, and then presenting GraphTIS, a novel peer-to-peer system that has been specifically designed to support traffic information systems.\"",
        "Document: \"Enabling fair offline trading. Offline trading allows users to trade rights associated with digital content or any other digital asset without immediate access to a central authority. To conduct an offline trade all involved parties sign a contract describing the transaction. Later this contract may then be submitted to a central authority (e.g., a digital content repository or a bank) to conduct the actual change of ownership as specified in the contract. In this paper we show that there is a crucial problem when using offline trading: whoever signs the contract first is providing the other parties with an option to unilaterally control the contract. The other parties may sign the contract later and submit it to complete the transaction. Or they may simply discard it. If the value of the involved goods change over time, this option may have a significant value. We introduce an approach to limit the value of options granted through offline contract signing and discuss its impact, design alternatives as well as related aspects.\"",
        "Document: \"Backpressure multicast congestion control in mobile ad-hoc networks. In mobile ad-hoc networks, the multicast paradigm is of central importance. It can help to save scarce medium bandwidth if packets are to be delivered to multiple destinations. We consider the problem of congestion control for multicast traffic in wireless multihop networks. We propose to apply a congestion control concept which is tailored to the very special properties of the wireless multihop medium: implicit hop-by-hop congestion control. The idea, so far only having been considered for unicast traffic, is here generalized to multicast. We implement it in the Backpressure Multicast Congestion Control (BMCC) protocol, with a focus on how to realize it in combination with geographic multicast routing in the Scalable Position-Based Multicast (SPBM) protocol. Our evaluation points out a number of highly desirable properties of the proposed scheme. In particular, it achieves and maintains high throughput and high packet delivery ratios at low packet latencies, even in the presence of significant network load.\"",
        "1 is \"Bregman-EM-TV Methods with Application to Optical Nanoscopy\", 2 is \"Issues in Designing a Communication Architecture for Large-Scale Virtual Environments\"",
        "Given above information, for an author who has written the paper with the title \"A Proxy Server-Network for Real-Time Computer Games\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007404": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'SAR image simulation for the assessment of despeckling techniques.':",
        "Document: \"A stochastic model for very high resolution SAR amplitude images of urban areas. For the new generation of spaceborne SAR sensors a resolution even better than one meter in the spotlight acquisition mode can be obtained. However, due to the involved electromagnetic mechanisms dictating SAR image formation, direct interpretation of very high-resolution (VHR) SAR images is not straightforward. In this paper, we propose the use of the power spectral density (PSD) for the analysis and interpretation of VHR SAR images of urban areas. In particular, we introduce a theoretical model of the PSD of such images, and then we test the validity of the developed model on one COSMO/SkyMed spotlight SAR image of the area of Naples, Italy. Finally, we show how the parameters of the obtained PSD can be related to meaningful physical observables of the imaged urban scene.\"",
        "Document: \"Small Perturbation Method for Scattering From Rough Multilayers. In this paper, we present an elegant closed form solution obtained applying perturbation theory to three-dimensional layered structures with an arbitrary number of rough interfaces. A general method has been developed to treat layered structures that can be described by small changes with respect to an idealized (unperturbed) structure, whose associated problem is exactly solvable. The final expression, beyond the compactness, allows us to directly prove that our formulation satisfies the reciprocity principle.\"",
        "Document: \"Angle Independence Properties of Fractal Dimension Maps Estimated From SAR Data. The extremely remarkable properties of angle independence exhibited by an innovative SAR product, the fractal dimension map estimated from a single SAR image, are discussed. The theoretical analysis is supported by a noticeable data set of actual SAR images acquired, with look angles varying from 20\u00b0 to 45\u00b0, in the stripmap operational mode by the COSMO-SkyMed constellation. The behavior of the fr...\"",
        "Document: \"Assessment of TerraSAR-X Products with a New Feature Extraction Application: Monitoring of Cylindrical Tanks. There is no doubt that retrieving observed scene features is one of the most interesting and challenging activities in all fields of remote sensing: The successful extraction of scene parameters may not only mean the success of the adopted procedure but also the success of a prediction model, an image product, a sensor project, or even an entire mission. This paper is partly concerned with this. The mission, the sensor, and the products at issue are the TerraSAR-X; the feature retrieval approach is the deterministic model-based approach already tested on E-SAR images and now in phase of improvement and testing on high-resolution TerraSAR-X images. Together with assessing the performances of TerraSAR-X products, this paper deals with a new application which, until now, has not received enough attention even if being worth of it: monitoring of big tanks in suburban or urban areas. Detailed discussion concerning the most suitable product for this kind of application is accompanied by retrieval results carried out on recently acquired TerraSAR-X images.\"",
        "1 is \"Contact Lens Detection and Classification in Iris Images through Scale Invariant Descriptor\", 2 is \"Fusion of Support Vector Machines for Classification of Multisensor Data\"",
        "Given above information, for an author who has written the paper with the title \"SAR image simulation for the assessment of despeckling techniques.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007405": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Where Is the Hole Punch? Object Localization Capabilities on a Specific Bureau Task':",
        "Document: \"Change detection in urban areas by direct comparison of multi-view and multi-temporal ALS data. Change detection in urban areas requires the comparison of multi-temporal remote sensing data. ALS (airborne laser scanning) is one of the established techniques to deliver these data. A novelty of our approach is the consideration of multiple views that are acquired with an oblique forward-looking laser scanner. In addition to advantages in terms of data coverage, this configuration is ideally suited to support helicopter pilots during their mission, e.g., with an obstacle warning system, terrain-referenced navigation, or online change detection. In this paper, we present a framework for direct comparison of current ALS data to given reference data of an urban area. Our approach extends the concept of occupancy grids known from robot mapping, and the proposed change detection method is based on the Dempster-Shafer theory. Results are shown for an urban test site at which multi-view ALS data were acquired at an interval of one year.\"",
        "Document: \"Simultaneous Calibration of ALS Systems and Alignment of Multiview LiDAR Scans of Urban Areas. Tasks such as city modeling or urban planning require the registration, alignment, and comparison of multiview and/or multitemporal remote sensing data. Airborne laser scanning (ALS) is one of the established techniques to deliver these data. Regrettably, direct georeferencing of ALS measurements usually leads to considerable displacements that limit connectivity and/or comparability of overlappin...\"",
        "Document: \"Comparison of Two Methods for Vehicle Extraction From Airborne LiDAR Data Toward Motion Analysis. It has been revealed that single-pass airborne light detection and ranging (LiDAR) system (ALS) data could provide not only the spatial but also the dynamical information of a scanned scene due to the so-called motion artifact effect. A common strategy for extracting dynamical information from ALS data is established based on analyzing shape deformations of vehicles which have to be extracted in advance. Therefore, vehicle extraction results are directly related to the performance of motion analysis. In this letter, two vehicle extraction methods, namely, grid-cell- and 3-D point-cloud-analysis-based methods, which represent two main streams in LiDAR data processing, are to be evaluated and compared toward influences on the performance of motion analysis. Motion estimation based on the two methods is respectively applied to real ALS data sets. The results show that the 3-D data-based method can yield more accurate and robust dynamical traffic information such as motion state and velocity of vehicles, while the grid-cell-based method can provide more complete information by extracting more stationary vehicles.\"",
        "Document: \"Machine Learning Comparison between WorldView-2 and QuickBird-2-Simulated Imagery Regarding Object-Based Urban Land Cover Classification. The objective of this study is to compare WorldView-2 (WV-2) and QuickBird-2-simulated (QB-2) imagery regarding their potential for object-based urban land cover classification. Optimal segmentation parameters were automatically found for each data set and the obtained results were quantitatively compared and discussed. Four different feature selection algorithms were used in order to verify to which data set the most relevant object-based features belong to. Object-based classifications were performed with four different supervised algorithms applied to each data set and the obtained accuracies and model performances indexes were compared. Segmentation experiments carried out involving bands exclusively available in the WV-2 sensor generated segments slightly more similar to our reference segments (only about 0.23 discrepancy). Fifty seven percent of the different selected features and 53% of all the 80 selections refer to features that can only be calculated with the additional bands of the WV-2 sensor. On the other hand, 57% of the most relevant features and 63% of the second most relevant features can also be calculated considering only the QB-2 bands. In 10 out of 16 classifications, higher Kappa values were achieved when features related to the additional bands of the WV-2 sensor were also considered. In most cases, classifications carried out with the 8-band-related features generated less complex and more efficient models than those generated only with QB-2 band-related features. Our results lead to the conclusion that spectrally similar classes like ceramic tile roofs and bare soil, as well as asphalt and dark asbestos roofs can be better distinguished when the additional bands of the WV-2 sensor are used throughout the object-based classification process.\"",
        "1 is \"Estimation of Forest Structure, Ground, and Canopy Layer Characteristics From Multibaseline Polarimetric Interferometric SAR Data\", 2 is \"Visual learning and recognition of 3-D objects from appearance\"",
        "Given above information, for an author who has written the paper with the title \"Where Is the Hole Punch? Object Localization Capabilities on a Specific Bureau Task\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007420": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Fast identification of JPEG 2000 images for digital cinema profiles':",
        "Document: \"A Parameter Memorization-Free Lossless Data Hiding Method With Flexible Payload Size. This paper proposes a lossless data hiding method which accepts various payload sizes. A lossless data hiding method once distorts an image to embed data into the image. From the distorted image, the method extracts the inserted payload data and restores the original image. The proposed method does not have to memorize any parameter for data extraction and image recovery. By simple modification to the conventional method having the above mentioned features, the proposed method becomes free from fixing payload size and from iterative parameter estimation.\"",
        "Document: \"Phase scrambling for blind image matching. We propose a phase-scrambling method for blind image matching, which is a direct image matching between invisible images. The phase scrambling is motivated by visual protection of images and prevention of illegal image matching. Phase-only correlation (POC) can be directly applied to images protected by the proposed phase scrambling in order to estimate similarity and translation between images. POC with synchronized scrambling provides blind image matching, in which phase scrambling does not affect the accuracy of POC. The effect of visual protection and prevention of illegal image matching is evaluated through simulations to show the effectiveness of the proposed method.\"",
        "Document: \"Fast and Robust Identification Methods for JPEG Images with Various Compression Ratios. We propose two identification methods for JPEG-coded images. The purposes are to identify the images that are compressed from the same original image with various compression ratios in fast and robust manner. The first approach can avoid identification leakage or false negative (FN), and could result in a few false positives (FP). The second approach can avoid both FN and FP, with a slightly longer processing time. By combining the two schemes, a faster and a more perfect identification can be achieved, in which IN and FP can be avoided.\"",
        "Document: \"A Fast Image-Scramble Method Using Public-Key Encryption Allowing Backward Compatibility With Jpeg2000. A new method for partial-scrambling of JPEG 2000 images based oil public-key encryption is proposed. By using public-key encryption. the proposed method provides an easier way of managing the encryption key compared with the secret-key based method and also provides tamper resistance against attacks. Although public-key encryption is usually very time-consuming, the proposed method achieves fast encryption by controlling the number of bytes to be encrypted. An encrypted JPEG 2000 image generated by the proposed method has backward compatibility with a standard JPEG 2000 image. so that it can be decoded using a standard JPEG 2000 decoder. The proposed method also has scalability as to the degree of scrambling, on the basis of JPEG 2000 coding units. i.e., layers. DWT-levels. subbands. or code-blocks.\"",
        "1 is \"Prediction-preserving reducibility\", 2 is \"Two-dimensional integer wavelet transform with reduced influence of rounding operations.\"",
        "Given above information, for an author who has written the paper with the title \"Fast identification of JPEG 2000 images for digital cinema profiles\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007618": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'On Pinning Impulsive Control Of Complex Dynamical Networks':",
        "Document: \"A comparative study of soft-computing methodologies in identification of robotic manipulators. This paper investigates the identification of nonlinear systems by utilizing soft-computing approaches. As the identification methods, feedforward neural network architecture (FNN), radial basis function neural networks (RBFNN), Runge-Kutta neural networks (RKNN) and adaptive neuro-fuzzy inference systems (ANFIS) based identification mechanisms are studied and their performances are comparatively evaluated on a two degrees of freedom direct drive robotic manipulator. (C) 2000 Elsevier Science B.V. All rights reserved.\"",
        "Document: \"Sliding-Mode Control With Soft Computing: A Survey. Sliding-mode control (SMC) has been studied extensively for over 50 years and widely used in practical applications due to its simplicity and robustness against parameter variations and disturbances. Despite the extensive research activities carried out, the key technical problems associated with SMC remain as challenging research questions due to demands for new industrial applications and technological advances. In this respect, soft computing (SC) is a rather recent development in intelligent systems which has provided alternative means for adaptive learning and control to overcome the key SMC technical problems. Substantial efforts in integration of SMC with SC have been placed in recent years with various successes. In this paper, we provide the state of the art of recent developments in SMC systems with SC, examining key technical research issues and future perspectives.\"",
        "Document: \"Sliding Mode Control Approach For Online Learning As Applied To Type-2 Fuzzy Neural Networks And Its Experimental Evaluation. Type-2 fuzzy logic systems (FLSs) are proposed in the literature as an alternative to type-1 FLSs because of their ability to more effectively model uncertainties that may exist in the rule base. However, the parameters of the system still need to be optimized. For this purpose, the use of a sliding mode control theory-based learning algorithm is proposed in this paper. In the approach, instead of trying to minimize an error function, the parameters of the network are tuned by the proposed algorithm in such a way that the learning error is enforced to satisfy a stable equation. The update rules to achieve this are derived, and the convergence of the parameters is proved by Lyapunov stability method. The performance of the proposed algorithm is tested by simulations on a Duffing oscillator and also by real-time experiments on a laboratory servo system. The results indicate that the given type-2 fuzzy neural network with the proposed learning algorithm can handle the uncertainties in a better way as compared to its type-1 counterpart. Moreover, it is computationally easier to implement in real-time systems.\"",
        "Document: \"Optimal Selection of Parameters for Nonuniform Embedding of Chaotic Time Series Using Ant Colony Optimization. The optimal selection of parameters for time-delay embedding is crucial to the analysis and the forecasting of chaotic time series. Although various parameter selection techniques have been developed for conventional uniform embedding methods, the study of parameter selection for nonuniform embedding is progressed at a slow pace. In nonuniform embedding, which enables different dimensions to have ...\"",
        "1 is \"Indirect hierarchical FCMAC control for the ball and plate system\", 2 is \"Second-order tracking control for leader-follower multi-agent flocking in directed graphs with switching topology.\"",
        "Given above information, for an author who has written the paper with the title \"On Pinning Impulsive Control Of Complex Dynamical Networks\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007623": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Joint hardware-software leakage minimization approach for the register file of VLIW embedded architectures':",
        "Document: \"Run-Time Scheduling for Multimedia Applications on Dynamically Reconfigurable Systems. Current multimedia applications are characterized by highly dynamic and non-deterministic behavior as well as high-performance requirements. In addition, portable devices demand also low energy consumption. Potentially, Dynamically Reconfigurable Hardware resources (DRHW) present the ideal features to fulfill these requirements since they can be reconfigured at run-time to match the performance and energy consumption requirements. However, the lack of programming support for dynamic task placement as well as the large configuration overhead has prevented a broader use of DRHW resources on embedded system design. To cope with these two problems, we have adopted a DRHW model with specific support for task migration and inter-task communication. On top of it we have applied an energy-aware run-time scheduling technique capable of taking advantage of the DRHW flexibility. Finally, we have developed a set of modules that greatly reduces the reconfiguration overhead, making it affordable for current multimedia applications.\"",
        "Document: \"Design and Tool Flow of Multimedia MPSoC Platforms. This paper surveys components that are useful to build programmable, predictable, composable, and scalable multiprocessor-system-on-a-chip (MPSoC) multimedia platforms that can deliver high performance at high power-efficiency. A design-time tool flow is proposed to exploit all forms of parallelism on such platforms. As a first proof of concept, the flow is used to parallelize a relatively simple video standard on a platform consisting of off-the-shelf components. As a second proof of concept, we present the design of a high-performance platform with state-of-the-art components. This platform targets real-time H.264 high-definition video encoding at an estimated power consumption of 700 mW.\"",
        "Document: \"Hardware/Software Co-Design Of Digital Telecommunication Systems. In this paper we reflect on the nature of digital telecommunication systems. We argue that these systems require, by nature, a heterogeneous specification and an implementation with heterogeneous architectural styles. CoWare is a hardware/software co-design environment based on a data model that allows to specify, simulate, and synthesize heterogeneous hardware/software architectures from a heterogeneous specification. CoWare is based on the principle of encapsulation of existing hardware and software compilers and special attention is paid to the interactive synthesis of hardware/software and hardware/hardware interfaces. The principles of CoWare will be illustrated by the design process of a spread-spectrum receiver for a pager system.\"",
        "Document: \"Real-time high-definition stereo matching on FPGA. Although many fast stereo matching designs have been proposed in the past decades, it is still very challenging to achieve real-time speed at high definition resolution while maintaining high matching accuracy. In this paper, we propose a real-time high definition stereo matching design on FPGA. By using the Mini-Census transform and the Cross-based cost aggregation, the proposed algorithm is robust to radiometric differences and produces accurate disparity maps. The algorithm modules have been optimized for efficient hardware implementations and instantiated in an SoC environment. Implemented on a single EP3SL150 FPGA, our design achieves 60 frames per second for 1024 \u00d7 768 stereo images. Evaluated with the Middlebury stereo benchmark, the proposed design also delivers leading stereo matching accuracy among prior related work.\"",
        "1 is \"Automatic Extraction of Functional Parallelism from Ordinary Programs\", 2 is \"Optimal regulation of traffic flows in networks-on-chip\"",
        "Given above information, for an author who has written the paper with the title \"Joint hardware-software leakage minimization approach for the register file of VLIW embedded architectures\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007718": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'SPARTAN: Semantic integration of big spatio-temporal data from streaming and archival sources':",
        "Document: \"Identifying the most influential data objects with reverse top-k queries. Top-k queries are widely applied for retrieving a ranked set of the k most interesting objects based on the individual user preferences. As an example, in online marketplaces, customers (users) typically seek a ranked set of products (objects) that satisfy their needs. Reversing top-k queries leads to a query type that instead returns the set of customers that find a product appealing (it belongs to the top-k result set of their preferences). In this paper, we address the challenging problem of processing queries that identify the top-m most influential products to customers, where influence is defined as the cardinality of the reverse top-k result set. This definition of influence is useful for market analysis, since it is directly related to the number of customers that value a particular product and, consequently, to its visibility and impact in the market. Existing techniques require processing a reverse top-k query for each object in the database, which is prohibitively expensive even for databases of moderate size. In contrast, we propose two algorithms, SB and BB, for identifying the most influential objects: SB restricts the candidate set of objects that need to be examined, while BB is a branch-and-bound algorithm that retrieves the result incrementally. Furthermore, we propose meaningful variations of the query for most influential objects that are supported by our algorithms. Our experiments demonstrate the efficiency of our algorithms both for synthetic and real-life datasets.\"",
        "Document: \"Efficient execution plans for distributed skyline query processing. In this paper, we study the generation of efficient execution plans for skyline query processing in large-scale distributed environments. In such a setting, each server stores autonomously a fraction of the data, thus all servers need to process the skyline query. An execution plan defines the order in which the individual skyline queries are processed on different servers, and influences the performance of query processing. Querying servers consecutively reduces the amount of transferred data and the number of queried servers, since skyline points obtained by one server prune points in the subsequent servers, but also increases the latency of the system. To address this trade-off, we introduce a novel framework, called SkyPlan, for processing distributed skyline queries that generates execution plans aiming at optimizing the performance of query processing. Thus, we quantify the gain of querying consecutively different servers. Then, execution plans are generated that maximize the overall gain, while also taking into account additional objectives, such as bounding the maximum number of hops required for the query or balancing the load on different servers fairly. Finally, we present an algorithm for distributed processing based on the generated plan that continuously refines the execution plan during in-network processing. Our framework consistently outperforms the state-of-the-art algorithm.\"",
        "Document: \"Processing of Rank Joins in Highly Distributed Systems. In this paper, we study efficient processing of rank joins in highly distributed systems, where servers store fragments of relations in an autonomous manner. Existing rank-join algorithms exhibit poor performance in this setting due to excessive communication costs or high latency. We propose a novel distributed rank-join framework that employs data statistics, maintained as histograms, to determine the subset of each relational fragment that needs to be fetched to generate the top-k join results. At the heart of our framework lies a distributed score bound estimation algorithm that produces sufficient score bounds for each relation, that guarantee the correctness of the rank-join result set, when the histograms are accurate. Furthermore, we propose a generalization of our framework that supports approximate statistics, in the case that the exact statistical information is not available. An extensive experimental study validates the efficiency of our framework and demonstrates its advantages over existing methods.\"",
        "Document: \"Angle-based space partitioning for efficient parallel skyline computation. Recently, skyline queries have attracted much attention in the database research community. Space partitioning techniques, such as recursive division of the data space, have been used for skyline query processing in centralized, parallel and distributed settings. Unfortunately, such grid-based partitioning is not suitable in the case of a parallel skyline query, where allpartitions are examined at the same time, since many data partitions do not contribute to the overall skyline set, resulting in a lot of redundant processing. In this paper we propose a novel angle-based space partitioning scheme using the hyperspherical coordinates of the data points. We demonstrate both formally as well as through an exhaustive set of experiments that this new scheme is very suitable for skyline query processing in a parallel share-nothing architecture. The intuition of our partitioning technique is that the skyline points are equally spread to all partitions. We also show that partitioning the data according to the hyperspherical coordinates manages to increase the average pruning power of points within a partition. Our novel partitioning scheme alleviates most of the problems of traditional grid partitioning techniques, thus managing to reduce the response time and share the computational workload more fairly. As demonstrated by our experimental study, our technique outperforms grid partitioning in all cases, thus becoming an efficient and scalable solution for skyline query processing in parallel environments.\"",
        "1 is \"Analysis of the Clustering Properties of the Hilbert Space-Filling Curve\", 2 is \"Spatial join selectivity using power laws\"",
        "Given above information, for an author who has written the paper with the title \"SPARTAN: Semantic integration of big spatio-temporal data from streaming and archival sources\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007741": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'State assignment for power and area minimization':",
        "Document: \"A timing-driven soft-macro placement and resynthesis method in interaction with chip floorplanning. In this paper, we present a complete chip design method which incorporates a soft-macro placement and resynthesis method in interaction with chip floorplanning for area and timing improvements. We present a performance-driven soft-macro clustering and placement method which preserves hardware descriptive language (HDL) design hierarchy to guide the soft-macro placement process. We develop a timing-driven design flow to exploit the interaction between HDL synthesis and physical design tasks. During each design iteration, we resynthesize soft macros with either a relaxed or a tightened timing constraint which is guided by the post-layout timing information. The goal is to produce area-efficient designs while satisfying the timing constraints. Experiments on a number of industrial designs ranging from 75-K to 230-K gates demonstrate that the proposed soft-macro clustering and placement method improves critical-path delays on an average of 22%. Furthermore, the results show that by effectively relaxing the timing constraint of noncritical modules and tightening the timing constraint of critical modules, a design can achieve 11% to 30% timing improvements with little to no increase in chip area.\"",
        "Document: \"Combining technology mapping and placement for delay-minimization in FPGA designs. We combine technology mapping and placement into a single procedure, M.Map, for the design of RAM-based FPGAs. Iteratively, M.Map maps several subnetworks of a Boolean network into a number of CLBs on the layout plane simultaneously. For every output node of the unmapped portion of the Boolean network, many ways of mapping are possible. The choice of which mapping to be used depends not only on the location of the CLB into which the output node will be mapped but also on its interconnection with those already mapped CLBs. To deal with such a complicated interaction among multiple output nodes of a Boolean network, multiple ways of mappings and multiple number of CLBs, any greedy algorithm will be insufficient. Therefore, we use a bipartite weighted matching algorithm in finding a solution that takes the global information into consideration. With the availability of the partial placement information, M.Map is able to minimize the routing delay in addition to the number of CLBs. Experimental results on a set of benchmarks demonstrate that M.Map is indeed effective and efficient\"",
        "Document: \"Reference frame access optimization for ultra high resolution H.264/AVC decoding. In an ultra high resolution H.264/AVC decoder, accessing reference frame data stored in DRAM requires huge bandwidth. The access patterns of Motion Compensation (MC) and De-blocking Filter (DF) are very different. Therefore, straightforward access and arbitration may amplify access penalty and, thus, diminish DRAM efficiency. We propose three schemes, access pattern uniformization, Macro Block (MB)-column-based mapping and task-based arbitration, to minimize the DRAM access latency including both penalty and amount of transferred data. Experimental results on a pure hardwired QFHD (3840times2160) H.264/AVC decoder system shows that by employing the proposed schemes, we achieve 86% saving in DRAM access latency.\"",
        "Document: \"A 3D SoC design for H.264 application with on-chip DRAM stacking. Three-dimensional (3D) on-chip memory stacking has been proposed as a promising solution to the \u201cmemory wall\u201d challenge with the benefits of low access latency, high data bandwidth, and low power consumption. The stacked memory tiers leverage through-silicon-vias (TSVs) to communicate with logic tiers, and thus dramatically reduce the access latency and improve the data bandwidth without the constraint of I/O pin count. To demonstrate the feasibility of 3D memory stacking, this paper introduces a 3D System-on-Chip (SoC) for H.264 applications that can make use of multiple memory channels offered by 3D integration. Two logic tiers are stacked together with each having an area of 2.5\u00d75.0mm2, with a 3-layer 8-channel 3D DRAM stacked on the top. The design flow for this 3D SoC is also presented. The prototype chip has been fabricated with GlobalFoundries' 130nm low-power process and Tezzaron's 3D TSV technology. The 3D implementation shows that the 3D ICs can alleviate the pressure from I/O pin count and allow parallel memory accesses through multiple channels.\"",
        "1 is \"An overview of the Penn State design system\", 2 is \"Test Wrapper and Test Access Mechanism Co-Optimization for System-on-Chip\"",
        "Given above information, for an author who has written the paper with the title \"State assignment for power and area minimization\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007749": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Embedded systems and software challenges in electric vehicles':",
        "Document: \"A workflow for runtime adaptive task allocation on heterogeneous MPSoCs. Modern Multiprocessor Systems-on-Chips (MPSoCs) are ideal platforms for co-hosting multiple applications, which may have very distinct resource requirements (e.g. data processing intensive or communication intensive) and may start/stop execution independently at time instants unknown at design time. In such systems, the runtime task allocator, which is responsible for assigning appropriate resources to each task, is a key component to achieve high system performance. This paper presents a new task allocation strategy in which self-adaptability is introduced. By dynamically adjusting a set of key parameters at runtime, the optimization criteria of the task allocator adapts itself according to the relative scarcity of different types of resources, so that resource bottlenecks can be effectively mitigated. Compared with traditional task allocators with fixed optimization criteria, experimental results show that our adaptive task allocator achieves significant improvement both in terms of hardware efficiency and stability.\"",
        "Document: \"Conforming the runtime inputs for hard real-time embedded systems. Timing is an important concern when designing an embedded system. While lots of researches on hard real-time systems focus on design-time analysis, monitoring the corresponding runtime behaviors are seldom investigated. In this paper, we investigate the conformity problem for runtime inputs of a hard real-time system. We adopt the widely used arrival curve model which captures the worst/best-cases event arrivals in the time interval domain and propose an algorithm to on-the-fly evaluate the conformity of the system input w.r.t. given arrival curves. The developed algorithm is lightweight in terms of both computation and memory overheads, which is particularly suitable for resource-constrained embedded systems. We also provide proofs and an Fpga implementation to demonstrate the effectiveness of our approach.\"",
        "Document: \"Static scheduling of a Time-Triggered Network-on-Chip based on SMT solving. Time-Triggered Network-on-Chip (TTNoC) is a networking concept aiming at providing both predictable and high-throughput communication for modern multiprocessor systems. The message scheduling is one of the major design challenges in TTNoC-based systems. The designers not only need to allocate time slots but also have to assign communication routes for all messages. This paper tackles the TTNoC scheduling problem and presents an approach based on Satisfiability Modulo Theories (SMT) solving. We first formulate the complete problem as an SMT instance, which can always compute a feasible solution if exists. Thereafter, we propose an incremental approach that integrates SMT solving into classical heuristic algorithms. The experimental results show that the heuristic scales significantly better with only minor loss of performance.\"",
        "Document: \"GAVS: Game Arena Visualization and Synthesis. \n Reasoning on the properties of computer systems can often be reduced to deciding the winner of a game played on a finite graph.\n In this paper, we introduce GAVS, an open-source tool for the visualization of some of the most fundamental games on finite\n graphs used in theoretical computer science, including, e.g., reachability games and parity games. The main purpose of GAVS\n is educational, a fact which is emphasized by the graphical editor for both defining game graphs and also visualizing the\n computation of the winning sets. Nevertheless, the underlying solvers are implemented with scalability in mind using symbolic\n techniques where applicable.\n \n \"",
        "1 is \"Automotive user interfaces: human computer interaction in the car\", 2 is \"Bounding WCET of applications using SDRAM with Priority Based Budget Scheduling in MPSoCs\"",
        "Given above information, for an author who has written the paper with the title \"Embedded systems and software challenges in electric vehicles\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007853": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Three New Approaches to Privacy-preserving Add to Multiply Protocol and its Application':",
        "Document: \"UMPCA based feature extraction for ECG. In this paper, we propose an algorithm for 12-leads ECG signals feature extraction by Uncorrelated Multilinear Principal Component Analysis(UMPCA). However, traditional algorithms usually base on 2-leads ECG signals and do not efficiently work out for 12-leads signals. Our algorithm aims at the natural 12-leads ECG signals. We firstly do the Short Time Fourier Transformation(STFT) on the raw ECG data and obtain 3rd-order tensors in the spatial-spectral-temporal domain, then take UMPCA to find a Tensor-to-Vector Projection(TVP) for feature extraction. Finally the Support Vector Machine(SVM) classifier is applied to achieve a high accuracy with these features.\"",
        "Document: \"A Rejuvenation Strategy in Android. Compared to critical systems, the user of Android is more concerned about the application launch time. It needs to perform rejuvenation to improve the user experience when the Android suffers from software aging that would lead to an increase of the launch time. Also, rejuvenation can incur in some costs. So it is important to make an optimal rejuvenation strategy to minimize these costs and improve the user experience. At the same time, different workloads on Android have different aging phenomena, which may influence the validation of system state. Based on these, we carried out experiments to simulate user behavior on Android and find software aging phenomena in Android. Then we used active learning based on random forest and four states to build model and constructed software aging rejuvenation strategy on Android. Comparing the experiments without rejuvenation, there is an obvious decrease of the average of application launch time after using the rejuvenation strategy, and the running time of experiments with rejuvenation is longer than that without rejuvenation, which improves the user experience and reduces the probability of unexpected errors.\"",
        "Document: \"CG-Cell: an NPB benchmark implementation on cell broadband engine. The NAS Conjugate Gradient (CG) benchmark is an important scientific kernel used to evaluate machine performance and compare characteristics of different programming models. CG represents a computation and communication paradigm for sparse linear algebra, which is common in scientific fields. In this paper, we present the porting, performance optimization and evaluation of CG on Cell Broadband Engine (CBE). CBE, a heterogeneous multi-core processor with SIMD accelerators, is gaining attention and being deployed on supercomputers and high-end server architectures. We take advantages of CBE's particular architecture to optimize the performance of CG. We also quantify these optimizations and assess their impact. In addition, by exploring distributed nature of CBE, we present trade-off between parallelization and serialization, and Cell-specific data scheduling in its memory hierarchy. Our final result shows that the CG-Cell can achieve more than 4 times speedup over the performance of single comparable PowerPC Processor.\"",
        "Document: \"Enhanced Phase-Shifted Pilots Based Channel Estimation for MIMO-OFDM Systems with Virtual Subcarriers. Generalizing a conventional phase-shifted pilot (PSP) sequences and discrete Fourier transform (DFT) based channel estimator to MIMO-OFDM systems with virtual subcarriers, this paper presents two enhanced channel estimators applying Wiener interpolation and smoothing. Simulation results show that the proposed schemes can almost eliminate the mean square error floor of the conventional PSP/DFT-based channel estimator and improve the channel estimation accuracy further in some cases. Additionally, the application of a robust channel correlation matrix estimator avoids the complicated measurement of channel statistical properties and thus makes the proposed schemes more feasible in practical systems\"",
        "1 is \"On exploiting cognitive radio to mitigate interference in macro/femto heterogeneous networks\", 2 is \"Joint Channel Assignment and Routing for Throughput Optimization in Multiradio Wireless Mesh Networks\"",
        "Given above information, for an author who has written the paper with the title \"Three New Approaches to Privacy-preserving Add to Multiply Protocol and its Application\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007894": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'RactIP: fast and accurate prediction of RNA-RNA interaction using integer programming.':",
        "Document: \"Prediction of RNA secondary structure by maximizing pseudo-expected accuracy. Recent studies have revealed the importance of considering the entire distribution of possible secondary structures in RNA secondary structure predictions; therefore, a new type of estimator is proposed including the maximum expected accuracy (MEA) estimator. The MEA-based estimators have been designed to maximize the expected accuracy of the base-pairs and have achieved the highest level of accuracy. Those methods, however, do not give the single best prediction of the structure, but employ parameters to control the trade-off between the sensitivity and the positive predictive value (PPV). It is unclear what parameter value we should use, and even the well-trained default parameter value does not, in general, give the best result in popular accuracy measures to each RNA sequence.Instead of using the expected values of the popular accuracy measures for RNA secondary structure prediction, which is difficult to be calculated, the pseudo-expected accuracy, which can easily be computed from base-pairing probabilities, is introduced. It is shown that the pseudo-expected accuracy is a good approximation in terms of sensitivity, PPV, MCC, or F-score. The pseudo-expected accuracy can be approximately maximized for each RNA sequence by stochastic sampling. It is also shown that well-balanced secondary structures between sensitivity and PPV can be predicted with a small computational overhead by combining the pseudo-expected accuracy of MCC or F-score with the \u03b3-centroid estimator.This study gives not only a method for predicting the secondary structure that balances between sensitivity and PPV, but also a general method for approximately maximizing the (pseudo-)expected accuracy with respect to various evaluation measures including MCC and F-score.\"",
        "Document: \"Probabilistic alignments with quality scores: an application to short-read mapping toward accurate SNP/indel detection. Motivation: Recent studies have revealed the importance of considering quality scores of reads generated by next-generation sequence (NGS) platforms in various downstream analyses. It is also known that probabilistic alignments based on marginal probabilities (e. g. aligned-column and/or gap probabilities) provide more accurate alignment than conventional maximum score-based alignment. There exists, however, no study about probabilistic alignment that considers quality scores explicitly, although the method is expected to be useful in SNP/indel callers and bisulfite mapping, because accurate estimation of aligned columns or gaps is important in those analyses. Results: In this study, we propose methods of probabilistic alignment that consider quality scores of (one of) the sequences as well as a usual score matrix. The method is based on posterior decoding techniques in which various marginal probabilities are computed from a probabilistic model of alignments with quality scores, and can arbitrarily trade-off sensitivity and positive predictive value (PPV) of prediction (aligned columns and gaps). The method is directly applicable to read mapping (alignment) toward accurate detection of SNPs and indels. Several computational experiments indicated that probabilistic alignments can estimate aligned columns and gaps accurately, compared with other mapping algorithms e.g. SHRiMP2, Stampy, BWA and Novoalign. The study also suggested that our approach yields favorable precision for SNP/indel calling.\"",
        "Document: \"Predictions of RNA secondary structure by combining homologous sequence information. Secondary structure prediction of RNA sequences is an important problem. There have been progresses in this area, but the accuracy of prediction from an RNA sequence is still limited. In many cases, however, homologous RNA sequences are available with the target RNA sequence whose secondary structure is to be predicted.In this article, we propose a new method for secondary structure predictions of individual RNA sequences by taking the information of their homologous sequences into account without assuming the common secondary structure of the entire sequences. The proposed method is based on posterior decoding techniques, which consider all the suboptimal secondary structures of the target and homologous sequences and all the suboptimal alignments between the target sequence and each of the homologous sequences. In our computational experiments, the proposed method provides better predictions than those performed only on the basis of the formation of individual RNA sequences and those performed by using methods for predicting the common secondary structure of the homologous sequences. Remarkably, we found that the common secondary predictions sometimes give worse predictions for the secondary structure of a target sequence than the predictions from the individual target sequence, while the proposed method always gives good predictions for the secondary structure of target sequences in all tested cases.Supporting information and software are available online at: http://www.ncrna.org/software/centroidfold/ismb2009/.Supplementary data are available at Bioinformatics online.\"",
        "Document: \"Prediction of RNA secondary structure using generalized centroid estimators. Recent studies have shown that the methods for predicting secondary structures of RNAs on the basis of posterior decoding of the base-pairing probabilities has an advantage with respect to prediction accuracy over the conventionally utilized minimum free energy methods. However, there is room for improvement in the objective functions presented in previous studies, which are maximized in the posterior decoding with respect to the accuracy measures for secondary structures.We propose novel estimators which improve the accuracy of secondary structure prediction of RNAs. The proposed estimators maximize an objective function which is the weighted sum of the expected number of the true positives and that of the true negatives of the base pairs. The proposed estimators are also improved versions of the ones used in previous works, namely CONTRAfold for secondary structure prediction from a single RNA sequence and McCaskill-MEA for common secondary structure prediction from multiple alignments of RNA sequences. We clarify the relations between the proposed estimators and the estimators presented in previous works, and theoretically show that the previous estimators include additional unnecessary terms in the evaluation measures with respect to the accuracy. Furthermore, computational experiments confirm the theoretical analysis by indicating improvement in the empirical accuracy. The proposed estimators represent extensions of the centroid estimators proposed in Ding et al. and Carvalho and Lawrence, and are applicable to a wide variety of problems in bioinformatics.Supporting information and the CentroidFold software are available online at: http://www.ncrna.org/software/centroidfold/.\"",
        "1 is \"AAindex: Amino Acid Index Database.\", 2 is \"A method for identifying splice sites and translational start sites in eukaryotic mRNA\"",
        "Given above information, for an author who has written the paper with the title \"RactIP: fast and accurate prediction of RNA-RNA interaction using integer programming.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007935": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Delay Reduction in Persistent Erasure Channels for Generalized Instantly Decodable Network Coding':",
        "Document: \"O2-GIDNC: Beyond instantly decodable network coding. In this paper, we are concerned with extending the graph representation of generalized instantly decodable network coding (GIDNC) to a more general opportunistic network coding (ONC) scenario, referred to as order-2 GIDNC (O2-GIDNC). In the O2-GIDNC scheme, receivers can store non-instantly decodable packets (NIDPs) comprising two of their missing packets, and use them in a systematic way for later decodings. Once this graph representation is found, it can be used to extend the GIDNC graph-based analyses to the proposed O2-GIDNC scheme with a limited increase in complexity. In the proposed O2-GIDNC scheme, the information of the stored NIDPs at the receivers and the decoding opportunities they create can be exploited to improve the broadcast completion time and decoding delay compared to traditional GIDNC scheme. The completion time and decoding delay minimizing algorithms that can operate on the new O2-GIDNC graph are further described. The simulation results show that our proposed O2-GIDNC improves the completion time and decoding delay performance of the traditional GIDNC.\"",
        "Document: \"An adaptive network coded retransmission scheme for single-hop wireless multicast broadcast services. Network coding has recently attracted attention as a substantial improvement to packet retransmission schemes in wireless multicast broadcast services (MBS). Since the problem of finding the optimal network code maximizing the bandwidth efficiency is hard to solve and hard to approximate, two main network coding heuristic schemes, namely opportunistic and full network coding, were suggested in the literature to improve the MBS bandwidth efficiency. However, each of these two schemes usually outperforms the other in different receiver, demand, and feedback settings. The continuous and rapid change of these settings in wireless networks limits the bandwidth efficiency gains if only one scheme is always employed. In this paper, we propose an adaptive scheme that maintains the highest bandwidth efficiency obtainable by both opportunistic and full network coding schemes in wireless MBS. The proposed scheme adaptively selects, between these two schemes, the one that is expected to achieve the better bandwidth efficiency performance. The core contribution in this adaptive selection scheme lies in our derivation of performance metrics for opportunistic network coding, using random graph theory, which achieves efficient selection when compared to appropriate full network coding parameters. To compare between different complexity levels, we present three approaches to compute the performance metric for opportunistic coding using different levels of knowledge about the opportunistic coding graph. For the three considered approaches, simulation results show that our proposed scheme almost achieves the bandwidth efficiency performance that could be obtained by the optimal selection between the opportunistic and full coding schemes.\"",
        "Document: \"Adaptive network coded retransmission scheme for wireless multicast. In wireless multicast, the receivers are interested in obtaining only a subset of the packets transmitted by the access node. Consequently, it is intuitively assumed that random network coded packet retransmissions will result in a lower bandwidth efficiency compared to opportunistic network coded retransmissions as the former involves the delivery of unwanted packets. In the first part of this paper, we show, through simulations, that the random network coded retransmission (RNCR) scheme outperforms the opportunistic network coded retransmission (ONCR) scheme in terms of bandwidth efficiency in a wide range of multicast settings. Motivated by this result, we propose an adaptive algorithm that can dynamically select, from the RNCR and ONCR schemes, the one that is expected to achieve a better performance for each multicast frame. Simulation results show that the proposed algorithm almost achieves the optimal performance that can be obtained by combining these two retransmission schemes.\"",
        "Document: \"Completion Time Reduction for Partially Connected D2D-enabled Network using Binary Codes. Consider a device-to-device (D2D) enabled network wherein a set of geographically close devices each holding a subset of a frame and interested in receiving all remaining files. This paper investigates the problem of reducing the number of transmissions to complete the reception of all data by all devices using instantly decodable network coding (IDNC). While previous works assume a fully connected communication network, devices in the considered D2D configuration can target only devices in their transmission range. Hence, multiple devices are able to transmit simultaneously. The joint optimization over the transmitting devices and file combinations so as to reduce the completion time is formulated and approximated through a decoding delay control. The first part of the paper prohibits cooperation that results in collisions which allow the decoupling of the problems. The problem is reformulated as a maximum weight clique problem in the cooperation graph wherein the weight of each vertex is obtained by solving the corresponding maximum weight clique problem over the local IDNC graph. The solution is then extended to optimally solve the joint optimization problem by introducing clusters of devices that act as interference-free virtual devices. Extensive simulations reveal that the proposed solutions provide noticeable performance enhancement and outperforms previously proposed IDNC-based schemes.\"",
        "1 is \"Coordinated Scheduling For Wireless Backhaul Networks With Soft Frequency Reuse\", 2 is \"Spatial Stochastic Models and Metrics for the Structure of Base Stations in Cellular Networks.\"",
        "Given above information, for an author who has written the paper with the title \"Delay Reduction in Persistent Erasure Channels for Generalized Instantly Decodable Network Coding\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007939": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A Supermodularity-Based Differential Privacy Preserving Algorithm for Data Anonymization':",
        "Document: \"A polynomial-delay algorithm for enumerating approximate solutions to the interval constrained coloring problem. We study the interval constrained coloring problem, a combinatorial problem arising in the interpretation of data on protein structure emanating from experiments based on hydrogen/deuterium exchange and mass spectrometry. The problem captures the challenging task of increasing the spatial resolution of experimental data in order to get a better picture of the protein structure. Since solutions proposed by any algorithmic framework have to ultimately be verified by biochemists, it is important to provide not just a single solution, but a valuable set of candidate solutions. Our contribution is a polynomial-delay, polynomial-space algorithm for enumerating all exact solutions plus further approximate solutions, which are guaranteed to be within an absolute error of two of the optimum within fragments of the protein, that is, within sets of consecutive residues. Our experiments indicate that the quality of the approximate solutions is comparable to the optimal ones in terms of deviation from the underlying true solution. In addition, the experiments also confirm the effectiveness of the method in reducing the delay between two consecutive solutions considerably, compared to what it takes an integer programming solver to produce the next exact solution.\"",
        "Document: \"A QPTAS for \u03b5-envy-free profit-maximizing pricing on line graphs. We consider the problem of pricing edges of a line graph so as to maximize the profit made from selling intervals to single-minded customers. An instance is given by a set E of n edges with a limited supply for each edge, and a set of m clients, where each client j specifies one interval of E she is interested in and a budget Bj which is the maximum price she is willing to pay for that interval. An envy-free pricing is one in which every customer is allocated (possibly empty) interval maximizing her utility. Recently, Grandoni and Rothvoss (SODA 2011) gave a polynomial-time approximation scheme (PTAS) for the unlimited supply case with running time. By utilizing the known hierarchical decomposition of doubling metrics, we give a PTAS with running time. We then consider the limited supply case, and the notion of -envy-free pricing in which a customer gets an allocation maximizing her utility within an additive error of. For this case we develop an approximation scheme with running time, where $H_e=\\\\frac{B_{\\\\max}(e)}{B_{\\\\min}(e)}$ is the maximum ratio of the budgets of any two customers demanding edge e. This yields a PTAS in the uniform budget case, and a quasi-PTAS for the general case.\"",
        "Document: \"On the complexity of the highway problem. In the highway problem, we are given a path, and a set of buyers interested in buying sub-paths of this path; each buyer declares a non-negative budget, which is the maximum amount of money she is willing to pay for that sub-path. The problem is to assign non-negative prices to the edges of the path such that we maximize the profit obtained by selling the edges to the buyers who can afford to buy their sub-paths, where a buyer can afford to buy her sub-path if the sum of prices in the sub-path is at most her budget. In this paper, we show that the highway problem is strongly NP-hard; this settles the complexity of the problem in view of the existence of a polynomial-time approximation scheme, as was recently shown in Grandoni and Rothvosz (2011) [15]. We also consider the coupon model, where we allow some items to be priced below zero to improve the overall profit. We show that allowing negative prices makes the problem APX-hard. As a corollary, we show that the bipartite vertex pricing problem is APX-hard with budgets in {1,2,3}, both in the cases with negative and non-negative prices.\"",
        "Document: \"A pumping algorithm for ergodic stochastic mean payoff games with perfect information. In this paper, we consider two-person zero-sum stochastic mean payoff games with perfect information, or BWR-games, given by a digraph G=(V=VB\u222aVW\u222aVR, E), with local rewards $r: E \\to {\\mathbb R}$, and three types of vertices: black VB, white VW, and random VR. The game is played by two players, White and Black: When the play is at a white (black) vertex v, White (Black) selects an outgoing arc (v,u). When the play is at a random vertex v, a vertex u is picked with the given probability p(v,u). In all cases, Black pays White the value r(v,u). The play continues forever, and White aims to maximize (Black aims to minimize) the limiting mean (that is, average) payoff. It was recently shown in [7] that BWR-games are polynomially equivalent with the classical Gillette games, which include many well-known subclasses, such as cyclic games, simple stochastic games (SSG\u2032s), stochastic parity games, and Markov decision processes. In this paper, we give a new algorithm for solving BWR-games in the ergodic case, that is when the optimal values do not depend on the initial position. Our algorithm solves a BWR-game by reducing it, using a potential transformation, to a canonical form in which the optimal strategies of both players and the value for every initial position are obvious, since a locally optimal move in it is optimal in the whole game. We show that this algorithm is pseudo-polynomial when the number of random nodes is constant. We also provide an almost matching lower bound on its running time, and show that this bound holds for a wider class of algorithms. Let us add that the general (non-ergodic) case is at least as hard as SSG\u2032s, for which no pseudo-polynomial algorithm is known.\"",
        "1 is \"On Optimal Multiversion Access Structures\", 2 is \"A fast and simple algorithm for identifying 2-monotonic positive Boolean functions\"",
        "Given above information, for an author who has written the paper with the title \"A Supermodularity-Based Differential Privacy Preserving Algorithm for Data Anonymization\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007954": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Efficient Information Retrieval by Dispatching Mobile Agents in Parallel':",
        "Document: \"Leveraging the Power of Informative Users for Local Event Detection. Detecting local events (e.g., protests, accidents) in real-time is an important task needed by a wide spectrum of real-world applications. In recent years, with the proliferation of social media platforms, we can access massive geo-tagged social messages, which can serve as a precious resource for timely local event detection. However, existing local event detection methods either suffer from unsatisfactory performances or need intensive annotations. These limitations make existing methods impractical for large-scale applications. Through the analysis of real-world datasets, we found that the informativeness level of social media users, which is neglected by existing work, plays a highly critical role in distilling event-related information from noisy social media contexts. Motivated by this finding, we propose an unsupervised framework, named LEDetect, to estimate the informativeness level of social media users and leverage the power of highly informative users for local event detection. Experiments on a large-scale real-world dataset show that the proposed LEDetect model can improve the performance of event detection compared with the state-of-the-art unsupervised approach. Also, we use case studies to show that the events discovered by the proposed model are of high quality and the extracted highly informative users are reasonable.\n\n\"",
        "Document: \"Extracting dimensions for OLAP on multidimensional text databases. With the amount of textual information massively growing in various kinds of business systems and Internet, there are increasingly demands for analyzing both structured data and unstructured text data. Online Analysis Processing (OLAP) is effective for analyzing and mining structured data. However, while handling with unstructured data, it is powerless. After working on several information integration and data analysis applications, we have realized the defect of OLAP on text data analysis and use technical ways to handle this issue. In this paper, we propose a semi-supervised algorithm to extract dimensions and their members from textual information for the purpose of analyzing a huge set of textual data. We use straightforward measures to express analysis results. Experiment result shows that the extracting algorithm is valid and our approach has a high scalability and flexibility.\"",
        "Document: \"STREAMCUBE: Hierarchical spatio-temporal hashtag clustering for event exploration over the Twitter stream. What is happening around the world? When and where? Mining the geo-tagged Twitter stream makes it possible to answer the above questions in real-time. Although a single tweet can be short and noisy, proper aggregations of tweets can provide meaningful results. In this paper, we focus on hierarchical spatio-temporal hashtag clustering techniques. Our system has the following features: (1) Exploring events (hashtag clusters) with different space granularity. Users can zoom in and out on maps to find out what is happening in a particular area. (2) Exploring events with different time granularity. Users can choose to see what is happening today or in the past week. (3) Efficient single-pass algorithm for event identification, which provides human-readable hashtag clusters. (4) Efficient event ranking which aims to find burst events and localized events given a particular region and time frame. To support aggregation with different space and time granularity, we propose a data structure called STREAMCUBE, which is an extension of the data cube structure from the database community with spatial and temporal hierarchy. To achieve high scalability, we propose a divide-and-conquer method to construct the STREAMCUBE. To support flexible event ranking with different weights, we proposed a top-k based index. Different efficient methods are used to speed up event similarity computations. Finally, we have conducted extensive experiments on a real twitter data. Experimental results show that our framework can provide meaningful results with high scalability.\"",
        "Document: \"Assembler: Efficient Discovery of Spatial Co-evolving Patterns in Massive Geo-sensory Data. Recent years have witnessed the wide proliferation of geo-sensory applications wherein a bundle of sensors are deployed at different locations to cooperatively monitor the target condition. Given massive geo-sensory data, we study the problem of mining spatial co-evolving patterns (SCPs), i.e., groups of sensors that are spatially correlated and co-evolve frequently in their readings. SCP mining is of great importance to various real-world applications, yet it is challenging because (1) the truly interesting evolutions are often flooded by numerous trivial fluctuations in the geo-sensory time series; and (2) the pattern search space is extremely large due to the spatiotemporal combinatorial nature of SCP. In this paper, we propose a two-stage method called Assember. In the first stage, Assember filters trivial fluctuations using wavelet transform and detects frequent evolutions for individual sensors via a segment-and-group approach. In the second stage, Assember generates SCPs by assembling the frequent evolutions of individual sensors. Leveraging the spatial constraint, it conceptually organizes all the SCPs into a novel structure called the SCP search tree, which facilitates the effective pruning of the search space to generate SCPs efficiently. Our experiments on both real and synthetic data sets show that Assember is effective, efficient, and scalable.\"",
        "1 is \"A trust model based on bayesian approach\", 2 is \"BoosTexter: A Boosting-based System for Text Categorization\"",
        "Given above information, for an author who has written the paper with the title \"Efficient Information Retrieval by Dispatching Mobile Agents in Parallel\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007963": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Characterization of On-Chip Multiport Inductors for Small-Area RF Circuits':",
        "Document: \"Linear Time Calculation Of On-Chip Power Distribution Network Capacitance Considering State-Dependence. A fast calculation tool for state dependent capacitance of power distribution network is proposed The proposed method achieves linear time complexity which can be more than four orders magnitude faster than a conventional SPICE based capacitance calculation Large circuits that have been unanalyzable with the conventional method become analyzable for more comprehensive exploration of capacitance variation The capacitance obtained with the proposed method agrees SPICE based method completely (up to 5 digits) and time linearity is confirmed through numerical experiments on various circuits The maximum and minimum capacitances are also calculated using average and variance estimation Calculation times are linear time complexity too The proposed tool facilitates to build an accurate macro model of an LSI\"",
        "Document: \"A Ring-Vco-Based Injection-Locked Frequency Multiplier With Novel Pulse Generation Technique In 65 Nm Cmos. This paper proposes a low-phase-noise ring-VCO-based frequency multiplier with a new subharmonic direct injection locking technique that only uses a time-delay cell and four MOS transistors. Since the proposed technique behaves as an exclusive OR and can double the reference signal frequency, it increases phase correction points and achieves low phase noise characteristic across the wide output frequency range. The frequency multiplier was fabricated by using 65 nm Si CMOS process. Measured 1-MHz-offset phase noise at 6.34 GHz with reference signals of 528 MHz was -119 dBc/Hz.\"",
        "Document: \"An Evaluation Method of the Number of Monte Carlo STA Trials for Statistical Path Delay Analysis. We present an evaluation method for estimating the lower bound number of Monte Carlo STA trials required to obtain at least one sample which falls within top-k % of its parent population. The sample can be used to ensure that target designs are timing-error free with a predefined probability using the minimum computational cost. The lower bound number is represented as a closed-form formula which is general enough to be applied to other verifications. For validation, Monte Carlo STA was carried out on various benchmark data including ISCAS circuits. The minimum number of Monte Carlo runs determined using the proposed method successfully extracted one or more top-k % delay instances.\"",
        "Document: \"A 0.5-V 1.56-Mw 5.5-Ghz Rf Transceiver Ic Module With J-Shaped Folded Monopole Antenna. This paper presents a low-power RF-CMOS-transceiver IC-module with small antennas on a printed circuit board (PCB). Active mixer-first architecture is employed on a receiver for achieving both acceptable sensitivity and lower power. We also show strategies for lowering power consumption of the transmitter and the phase locked loop (PLL): a highgain inverter-based resonant-driver and a current-reuse voltage controlled oscillator (VCO). Power saving of the RF circuits is achieved with low supply-voltage design under 0.5V by exploiting forward body bias technique. The use of 5.5-GHz band and a J-shaped folded monopole antennas (JFMA) on the PCB enable to reduce the module footprint, and the module size of 0.78 cc is realized. The proposed transceiver circuit is fabricated in 65 nm CMOS process technology and is mounted on the PCB. 5.5-GHz RF-transceiver operation is succeeded with small power consumption of 1.56 mW under 0.5-V power supply. Output signal-power of the transmitter is -23.2 dBm, and receiver sensitivity is -61.2 dBm.\"",
        "1 is \"Configuration and Programming of Heterogeneous Multiprocessors on a Multi-FPGA System Using TMD-MPI\", 2 is \"Statistical critical path analysis considering correlations\"",
        "Given above information, for an author who has written the paper with the title \"Characterization of On-Chip Multiport Inductors for Small-Area RF Circuits\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007973": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Addressing Network Survivability Issues by Finding the K-best Paths through a Trellis Graph':",
        "Document: \"A graph-based model for malware detection and classification using system-call groups. In this paper we present a graph-based model that, utilizing relations between groups of System-calls, detects whether an unknown software sample is malicious or benign, and classifies a malicious software to one of a set of known malware families. More precisely, we utilize the System-call Dependency Graphs (or, for short, ScD-graphs), obtained by traces captured through dynamic taint analysis. We design our model to be resistant against strong mutations applying our detection and classification techniques on a weighted directed graph, namely Group Relation Graph, or Gr-graph for short, resulting from ScD-graph after grouping disjoint subsets of its vertices. For the detection process, we propose the (Delta )-similarity metric, and for the process of classification, we propose the SaMe-similarity and NP-similarity metrics consisting the SaMe-NP similarity. Finally, we evaluate our model for malware detection and classification showing its potentials against malicious software measuring its detection rates and classification accuracy.\"",
        "Document: \"A Fully Dynamic Algorithm for the Recognition of P4-Sparse Graphs. We consider the dynamic recognition problem for the class of P4-sparse graphs: the objective is to handle edge/vertex additions and deletions, to recognize if each such modification yields a P4-sparse graph, and if yes, to update a representation of the graph. Our approach relies on maintaining the modular decomposition tree of the graph, which we use for solving the recognition problem. We establish conditions for each modification to yield a P4-sparse graph and obtain a fully dynamic recognition algorithm which handles edge modifications in O(1) time and vertex modifications in O(d) time for a vertex of degree d .T hus, our algorithm implies an optimal edges-only dynamic algorithm and a new optimal incremental algorithm for P4-sparse graphs. Moreover, by maintaining the children of each node of the modular decomposition tree in a binomial heap, we can handle vertex deletions in O(logn )t ime, at the expense of needing O(logn) time for each edge modification and O(d log n) time for the addition of a vertex adjacent to d vertices.\"",
        "Document: \"An Optimal Parallel Co-Connectivity Algorithm. In this paper we consider the problem of computing the connected components of the complement of a given graph. We describe a simple sequential algorithm for this problem, which works on the input graph and not on its complement, and which for a graph on n vertices and m edges runs in optimal O(n+m) time. Moreover, unlike previous linear co-connectivity algorithms, this algorithm admits efficient parallelization, leading to an optimal O(log n)-time and O((n+m)log n)-processor algorithm on the EREW PRAM model of computation. It is worth noting that, for the related problem of computing the connected components of a graph, no optimal deterministic parallel algorithm is currently available. The co-connectivity algorithms find applications in a number of problems. In fact, we also include a parallel recognition algorithm for weakly triangulated graphs, which takes advantage of the parallel co-connectivity algorithm and achieves an O(log2 n) time complexity using O((n+m2) log n) processors on the EREW PRAM model of computation.\"",
        "Document: \"Preventing Malware Pandemics in Mobile Devices by Establishing Response-time Bounds. The spread of malicious software among computing devices nowadays poses a major threat to the systems\u2019 security. Since both the use of mobile devices and the growth of malware\u2019s propagation increase rapidly, we are interested in investigating how the time needed by a counter-measure (i.e., an antivirus or a cleaner) to detect and remove a malware from infected devices affects the malware\u2019s propagation. In this work, we study the effect of counter-measure\u2019s response-time on the propagation of a malicious software and propose a model for establishing reasonable response-time bounds for its activation in order to prevent pandemic. More precisely, given an initial infected population in a network of mobile devices and a specific city area (town\u2019s planning), our model establishes upper response-time bounds for a counter-measure which guarantee that, within a period of time, not all the susceptible devices in the city get infected and the infected ones get sanitized. To this end, we first propose a malware propagation model along with a device mobility model, and then we develop a simulator utilizing these models in order to study the spread of malware in such networks. Finally, we present experimental results for the pandemic prevention taken by our simulator for various response-time intervals and other factors that affect the spread deploying different epidemic models.\"",
        "1 is \"Optimal parallel quicksort on EREW PRAM\", 2 is \"A Layered Broadband Switching Architecture With Physical Or Virtual Path Configurations\"",
        "Given above information, for an author who has written the paper with the title \"Addressing Network Survivability Issues by Finding the K-best Paths through a Trellis Graph\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008017": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'CNB-MAC'17: The Fourth International Workshop on Computational Network Biology: Modeling, Analysis, and Control':",
        "Document: \"A Bayesian robust Kalman smoothing framework for state-space models with uncertain noise statistics. The classical Kalman smoother recursively estimates states over a finite time window using all observations in the window. In this paper, we assume that the parameters characterizing the second-order statistics of process and observation noise are unknown and propose an optimal Bayesian Kalman smoother (OBKS) to obtain smoothed estimates that are optimal relative to the posterior distribution of the unknown noise parameters. The method uses a Bayesian innovation process and a posterior-based Bayesian orthogonality principle. The optimal Bayesian Kalman smoother possesses the same forward-backward structure as that of the ordinary Kalman smoother with the ordinary noise statistics replaced by their effective counterparts. In the first step, the posterior effective noise statistics are computed. Then, using the obtained effective noise statistics, the optimal Bayesian Kalman filter is run in the forward direction over the window of observations. The Bayesian smoothed estimates are obtained in the backward step. We validate the performance of the proposed robust smoother in the target tracking and gene regulatory network inference problems.\"",
        "Document: \"Stationary and structural control in gene regulatory networks: basic concepts. A major reason for constructing gene regulatory networks is to use them as models for determining therapeutic intervention strategies by deriving ways of altering their long-run dynamics in such a way as to reduce the likelihood of entering undesirable states. In general, two paradigms have been taken for gene network intervention: (1) stationary external control is based on optimally altering the status of a control gene (or genes) over time to drive network dynamics; and (2) structural intervention involves an optimal one-time change of the network structure (wiring) to beneficially alter the long-run behaviour of the network. These intervention approaches have mainly been developed within the context of the probabilistic Boolean network model for gene regulation. This article reviews both types of intervention and applies them to reducing the metastatic competence of cells via intervention in a melanoma-related network.\"",
        "Document: \"Intrinsically Bayesian robust Karhunen-Lo\u00e8ve compression. \u2022A robust signal compression method that achieves minimum expected MSE relative to the prior distribution of unknown parameters in the covariance matrix is introduced.\u2022The compression method is based on the effective covariance matrix representing the uncertainty class of possible covariance matrices.\u2022An experimental design method for optimal uncertainty reduction in the covariance matrix using the concept of mean objective cost of uncertainty is also proposed.\u2022The proposed robust signal compression and experimental design framework has been applied to uncertain block covariance matrices and uncertain covariance matrices governed by Wishart priors.\"",
        "Document: \"Covariate-dependent negative binomial factor analysis of RNA sequencing data. Motivation: High-throughput sequencing technologies, in particular RNA sequencing (RNA-seq), have become the basic practice for genomic studies in biomedical research. In addition to studying genes individually, for example, through differential expression analysis, investigating coordinated expression variations of genes may help reveal the underlying cellular mechanisms to derive better understanding and more effective prognosis and intervention strategies. Although there exists a variety of co-expression network based methods to analyze microarray data for this purpose, instead of blindly extending these methods for microarray data that may introduce unnecessary bias, it is crucial to develop methods well adapted to RNA-seq data to identify the functional modules of genes with similar expression patterns. Results: We have developed a fully Bayesian covariate-dependent negative binomial factor analysis (dNBFA) method-dNBFA-for RNA-seq count data, to capture coordinated gene expression changes, while considering effects from covariates reflecting different influencing factors. Unlike existing co-expression network based methods, our proposed model does not require multiple ad-hoc choices on data processing, transformation, as well as co-expression measures and can be directly applied to RNA-seq data. Furthermore, being capable of incorporating covariate information, the proposed method can tackle setups with complex confounding factors in different experiment designs. Finally, the natural model parameterization removes the need for a normalization preprocessing step, as commonly adopted to compensate for the effect of sequencing-depth variations. Efficient Bayesian inference of model parameters is derived by exploiting conditional conjugacy via novel data augmentation techniques. Experimental results on several real-world RNA-seq datasets on complex diseases suggest dNBFA as a powerful tool for discovering the gene modules with significant differential expression and meaningful biological insight.\"",
        "1 is \"RAP: a new computer program for de novo identification of repeated sequences in whole genomes.\", 2 is \"M-tree: An Efficient Access Method for Similarity Search in Metric Spaces\"",
        "Given above information, for an author who has written the paper with the title \"CNB-MAC'17: The Fourth International Workshop on Computational Network Biology: Modeling, Analysis, and Control\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008019": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'On the management of unicast and multicast services in LTE networks':",
        "Document: \"A DASH-aware Performance Oriented Adaptation Agent. The proliferation of low-cost multimedia-enabled mobile devices and free educational video content is adding strain to existing limited bandwidth resource delivery networks. The newly introduced Dynamic Adaptive Streaming over HTTP (DASH) standard is a cross-platform, efficient and cost-effective solution for media streaming to a variety of IP/multimedia enabled devices. This standard supports access to content stored at multiple hosts, but does not suggest any selection mechanism. This paper describes and evaluates a DASH-aware Performance Oriented Adaptation Agent (dPOAA) which dynamically selects best performing hosts for video delivery. Preliminary evaluation shows that this solution improves the quality of delivered video in terms of maintained estimated MOS and reduced buffering and initial delay on the client side.\"",
        "Document: \"A novel quality-aware 3D video adaptive scheme. As increasing amounts of rich multimedia services, such as 3D video, are being requested by users at higher quality, delivering these services at acceptable quality levels is very challenging. A solution is for service providers to provide adaptive services in dynamic network environments. An ideal adaptive system should be able to make the best choice in terms of the delivered multimedia bit-rate in order to achieve optimum user Quality of Experience (QoE) in the given network conditions. Existing systems make adaptations based on current observations of the stream delivery, without considering the resulting effect of these adaptations on the network and the adaptive stream itself. This paper proposes a novel Quality Aware 3D video (QA3D) adaptive scheme that adapts the 3D video source according to changes in received quality due to network variations, based on the prediction of the consequences of all possible adaptation choices. With the proposed scheme, we achieved up to 52% more effective adaptation in comparison to an alternative approach, with 6.4% higher perceived quality maintained on average across the entire streaming session.\"",
        "Document: \"CMT-QA: Quality-Aware Adaptive Concurrent Multipath Data Transfer in Heterogeneous Wireless Networks. Mobile devices equipped with multiple network interfaces can increase their throughput by making use of parallel transmissions over multiple paths and bandwidth aggregation, enabled by the stream control transport protocol (SCTP). However, the different bandwidth and delay of the multiple paths will determine data to be received out of order and in the absence of related mechanisms to correct this, serious application-level performance degradations will occur. This paper proposes a novel quality-aware adaptive concurrent multipath transfer solution (CMT-QA) that utilizes SCTP for FTP-like data transmission and real-time video delivery in wireless heterogeneous networks. CMT-QA monitors and analyses regularly each path's data handling capability and makes data delivery adaptation decisions to select the qualified paths for concurrent data transfer. CMT-QA includes a series of mechanisms to distribute data chunks over multiple paths intelligently and control the data traffic rate of each path independently. CMT-QA's goal is to mitigate the out-of-order data reception by reducing the reordering delay and unnecessary fast retransmissions. CMT-QA can effectively differentiate between different types of packet loss to avoid unreasonable congestion window adjustments for retransmissions. Simulations show how CMT-QA outperforms existing solutions in terms of performance and quality of service.\"",
        "Document: \"Game Theory-Based Network Selection: Solutions and Challenges. In order to cater for the overwhelming growth in bandwidth demand from mobile Internet users operators have started to deploy different, overlapping radio access network technologies. One important challenge in such a heterogeneous wireless environment is to enable network selection mechanisms in order to keep the mobile users Always Best Connected (ABC) anywhere and anytime. Game theory techniques have been receiving growing attention in recent years as they can be adopted in order to model and understand competitive and cooperative scenarios between rational decision makers. This paper presents an overview of the network selection decision problem and challenges, a comprehensive classification of related game theoretic approaches and a discussion on the application of game theory to the network selection problem faced by the next generation of 4G wireless networks.\"",
        "1 is \"Enabling UAV cellular with millimeter-wave communication: potentials and approaches.\", 2 is \"Dynamic voltage scaling of OLED displays\"",
        "Given above information, for an author who has written the paper with the title \"On the management of unicast and multicast services in LTE networks\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008217": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A Study of Parallel Particle Tracing for Steady-State and Time-Varying Flow Fields':",
        "Document: \"MPI datatype marshalling: a case study in datatype equivalence. MPI datatypes are a convenient abstraction for manipulating complex data structures and are useful in a number of contexts. In some cases, these descriptions need to be preserved on disk or communicated between processes, such as when defining RMA windows. We propose an extension to MPI that enables marshalling and unmarshalling MPI datatypes in the spirit of MPI_Pack/MPI_Unpack. Issues in MPI datatype equivalence are discussed in detail and an implementation of the new interface outside of MPI is presented. The new marshalling interface provides a mechanism for serializing all aspects of an MPI datatype: the typemap, upper/lower bounds, name, contents/envelope information, and attributes.\"",
        "Document: \"Extending the MPI-2 generalized request interface. The MPI-2 standard added a new feature to MPI called generalized requests. Generalized requests allow users to add new nonblocking operations to MPI while still using many pieces of MPI infrastructure such as request objects and the progress notification routines (MPI_Test, MPI_Wait). The generalized request design as it stands, however, has deficiencies regarding typical use cases. These deficiencies are particularly evident in environments that do not support threads or signals, such as the leading petascale systems (IBM Blue Gene/L, Cray XT3 and XT4). This paper examines these shortcomings, proposes extensions to the interface to overcome them, and presents implementation results.\"",
        "Document: \"Lightweight Provenance Service for High-Performance Computing. Provenance describes detailed information about the history of a piece of data, containing the relationships among elements such as users, processes, jobs, and workflows that contribute to the existence of data. Provenance is key to supporting many data management functionalities that are increasingly important in operations such as identifying data sources, parameters, or assumptions behind a given result; auditing data usage; or understanding details about how inputs are transformed into outputs. Despite its importance, however, provenance support is largely underdeveloped in highly parallel architectures and systems. One major challenge is the demanding requirements of providing provenance service in situ. The need to remain lightweight and to be always on often conflicts with the need to be transparent and offer an accurate catalog of details regarding the applications and systems. To tackle this challenge, we introduce a lightweight provenance service, called LPS, for high-performance computing (HPC) systems. LPS leverages a kernel instrument mechanism to achieve transparency and introduces representative execution and flexible granularity to capture comprehensive provenance with controllable overhead. Extensive evaluations and use cases have confirmed its efficiency and usability. We believe that LPS can be integrated into current and future HPC systems to support a variety of data management needs.\"",
        "Document: \"Using Formal Grammars to Predict I/O Behaviors in HPC: The Omnisc'IO Approach. The increasing gap between the computation performance of post-petascale machines and the performance of their I/O subsystem has motivated many I/O optimizations including prefetching, caching, and scheduling. In order to further improve these techniques, modeling and predicting spatial and temporal I/O patterns of HPC applications as they run has become crucial. In this paper we present Omnisc&#39;IO...\"",
        "1 is \"Explanation-Based Learning: An Alternative View\", 2 is \"Supporting Configurable Congestion Control in Data Transport Services\"",
        "Given above information, for an author who has written the paper with the title \"A Study of Parallel Particle Tracing for Steady-State and Time-Varying Flow Fields\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008222": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Virtual lifeline: Multimodal sensor data fusion for robust navigation in unknown environments':",
        "Document: \"Effect of TNL Flow Control Schemes for the HSDPA Network Performance. HSDPA (High Speed Downlink Packet Access) is an extension of the current UMTS (Universal Mobile Telecommunications System) technology with the objective to increase the data rate and to reduce the latency in the downlink. The main focus of this investigation is to optimise the Iub interface to provide the best end user performance at minimum cost for UMTS HSDPA network. An adaptive credit-based flow control mechanism for UTRAN transport has been developed, tested and validated in the HSDPA simulation model. In this paper, the HSDPA performance with the new adaptive credit-based flow control mechanism is compared with the generic ON/OFF flow control mechanism. The results confirm that the adaptive credit- based flow control mechanism can significantly improve the performance and therefore reduce the required bandwidth at the Iub interface meeting a specified performance: The better bandwidth utilisation and improved statistical multiplexing is achieved by reducing the burstiness of the traffic over the Iub interface. Finally, a recommendation for the required bandwidth at the Iub interface is given for bursty HSDPA traffic. Index Terms\u2014HSDPA, RNC, Node-B, Flow Control\"",
        "Document: \"Resilient data gathering and communication algorithms for\u00a0emergency scenarios. A typical application field of Wireless Sensor Networks (WSNs) is the collection of environmental data, which is sent to a\n base station. Routing protocols are needed to efficiently direct the information flows to the base station. Since sensor nodes\n have strict energy constraints, data gathering and communication schemes for WSNs need to be designed for an efficient utilization\n of the available resources. An\u00a0emergency management scenario is investigated, where a sensor network is deployed as virtual\n lifeline for fire fighters entering a building. Besides of supporting their navigation, the virtual lifeline is also used\n for two further purposes. First it enables the exchange of short voice messages between fire fighter and command post. For\n this, a fast and reliable routing protocol (EMRO) has been developed based on a broadcasting scheme. Second, measuring data,\n like temperature and gas, in the environment and informing fire fighters and command post about it, is of high importance.\n For this purpose a network coding based data gathering algorithm has been designed. The feasibility of simultaneously using\n the virtual lifeline for data gathering and communication and thus the coexistence of a classical routing protocol with a\n network coding scheme is studied in this paper by means of simulation and real experiments. The resilience to packet loss\n and node failure, as well as the transmission delay are investigated by means of short voice messages for the communication\n part and temperature readings for data gathering.\"",
        "Document: \"Network Planning for Stochastic Traffic Demands. Traffic in communication networks is not constant but fluctuates heavily, which makes the network planning task very challenging. Overestimating the traffic volume results in an expensive solution, while underestimating it leads to a poor Quality of Service (QoS) in the network. In this paper, we propose a new approach to address the network planning problem under stochastic traffic demands. We first formulate the problem as a chance-constrained programming problem, in which the capacity constraints need to be satisfied in probabilistic sense. Since we do not assume a normal distribution for the traffic demands, the problem does not have deterministic equivalent and hence cannot be solved by the well-known techniques. A heuristic approach based on genetic algorithm is therefore proposed. The experiment results show that the proposed approach can significantly reduce the network costs compared to the peak-load-based approach, while still maintaining the robustness of the solution. This approach can be applied to different network types with different QoS requirements.\"",
        "Document: \"Development of simulation environment for multi-homed devices in integrated 3GPP and non-3GPP networks. WLAN has proven itself as the most economical wireless access technology over the time. Its widespread deployment encouraged 3GPP (3rd Generation Partnership Project) to standardize the integration of such non-3GPP access technologies into the existing 3GPP access networks. This opened the door of numerous opportunities for network operators to make use of the economical bandwidth from WLAN in enhancing the end user Quality of Experience (QoE). 3GPP leaves it up to the network operators to do their own research and devise their individual algorithms for this purpose. This raises the need for a proper simulation environment supporting a heterogeneous network of access technologies inter-connected according to 3GPP specifications. The goal of this work is to develop a network simulator where 4G LTE (Long Term Evolution) and WLAN co-exist. In addition to standard 3GPP network entities and protocols the simulator also provides other features like multi-homing support, user QoE optimization algorithm as well as network bandwidth resource management techniques.\"",
        "1 is \"An efficient Chase decoder for turbo product codes\", 2 is \"Delay analysis of downlink IP traffic on UMTS mobile networks\"",
        "Given above information, for an author who has written the paper with the title \"Virtual lifeline: Multimodal sensor data fusion for robust navigation in unknown environments\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008238": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Study on the exponential path tracking control of robot manipulators via direct adaptive methods':",
        "Document: \"Adaptive Fault-Tolerant Control of Wind Turbines With Guaranteed Transient Performance Considering Active Power Control of Wind Farms. As high-order nonlinear large-scale systems, wind farms composed of multiple wind turbines (WTs) need to adopt active power control (APC) to track the power set points, rather than the maximum power points. In this paper, the proportional distribution strategy is utilized to specify the power set point according to the available output power of each WT based on the ultra-short-term wind speed pred...\"",
        "Document: \"Model Reduction of Takagi\u2013Sugeno Fuzzy Stochastic Systems. This paper is concerned with the problem of H\u221e model reduction for Takagi-Sugeno (T-S) fuzzy stochastic systems. For a given mean-square stable T-S fuzzy stochastic system, our attention is focused on the construction of a reduced-order model, which not only approximates the original system well with an H\u221e performance but also translates it into a linear lower dimensional system. Then, the model reduction is converted into a convex optimization problem by using a linearization procedure, and a projection approach is also presented, which casts the model reduction into a sequential minimization problem subject to linear matrix inequality constraints by employing the cone complementary linearization algorithm. Finally, two numerical examples are provided to illustrate the effectiveness of the proposed methods.\"",
        "Document: \"Collectively Rotating Formation and Containment Deployment of Multiagent Systems: A Polar Coordinate-Based Finite Time Approach. This paper investigates the problem of achieving rotating formation and containment simultaneously via finite time control schemes for multiagent systems. It is nontrivial to maintain rotating formation where the desired formation structure is time-varying and only neighboring information is available. The underlying problem becomes even more complicated if containment is imposed yet finite time c...\"",
        "Document: \"Study on the exponential path tracking control of robot manipulators via direct adaptive methods. Exponential path tracking control represents an important issue pertaining to the transient performance of robot control systems. In this paper, the so-called Exp-transformation is applied to obtain transformed robot dynamics models which are used to derive several adaptive control algorithms that achieve exponential path tracking. In contrast to the existing composite adaptive control method; where both the tracking error and the prediction error are used and persistent excitation (p.e.) is required, the proposed strategy requires only the tracking error. This makes the control structure simpler and easier to implement. The main contribution of this paper is the development of practical control strategies for which the p.e. requirement is completely removed (as opposed to relaxing it to semi-p.e. as was done in a recent work). The fundamental idea introduced for exponential stability analysis is conceptually simple and global results are obtained.\"",
        "1 is \"SETI@home: an experiment in public-resource computing\", 2 is \"Stable Control of Vehicle Convoys for Safety and Comfort.\"",
        "Given above information, for an author who has written the paper with the title \"Study on the exponential path tracking control of robot manipulators via direct adaptive methods\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008257": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Optimal admission and preemption control in finite-source loss systems':",
        "Document: \"Optimization of Home Energy Usage by Intelligently Charging/Discharging EV/PHEV. In this paper, the problem of energy cost minimization for residential home equipped with electric drive vehicles is considered. In future residential homes, electric drive vehicles, including Hybrid electric vehicles (HEVs), plug-in hybrid electric vehicles (PHEVs), and all-electric vehicles (EVs), are expected to be common due to the growing energy cost and the need for improving efficiency and energy sustainability. In this context, a constrained optimization problem is formulated for home owners to take advantage of controlling when and how the EVs/PHEVs are charged or discharged in order to minimize their energy cost while keeping the vehicle ready for their daily commute. By taking into account the nonlinear characteristics of the battery charging and discharging, and realistic energy pricing from government agencies such as ERCOT (Electric Reliability Council of Texas), a nonlinear programming solution is derived using the sub-gradient search algorithm. Simulation results demonstrate the effectiveness of the proposed scheme.\"",
        "Document: \"Efficient Data Collection with Sampling in WSNs: Making Use of Matrix Completion Techniques. Data collection is of paramount importance in many applications of wireless sensor networks (WSNs). Especially, to accommodate ever increasing demands of signal source coding applications, the capacity of processing multi-user data query is crucial in WSNs where the efficiency is one key consideration. To that end, this paper presents EDCA: an Efficient Data Collection Approach for data query in WSNs, which exploits recent matrix completion techniques. Specifically, for the efficiency of energy consumption, we randomly select a part of nodes from the sensor network to sample at each time instance and directly forward the data to the sink. Then, to recover the data precisely, we shift the rank minimization problem, which is NP-hard, to a convex optimization one. Compared with the centralized scheme, energy consumption using EDCA is significantly reduced due to lower sampling rate and fewer packets to transmit. The experimental results demonstrate that EDCA significantly outperforms the existing naive method in terms of energy consumption and the introduced errors are quite trivial.\"",
        "Document: \"Per-node throughput performance of overlapping Cognitive Radio networks. Multiple Cognitive Radio (CR) networks may exist in the same spatial domain in many cases. In this paper we consider two uncoordinated and geographically overlapping CR networks coexisting together with a primary network. We specifically study the achievable per-node throughput performance of the CR networks. Firstly, an interference model is specified which models the situation. By using this model we derive the per-node throughput for overlapping CR networks. Furthermore, the upper bound for the probability of false alarm, which is required to achieve a certain throughput, is deduced. The results of this paper illustrate how the different CR network parameters, such as network density, transmission probability, and sensing performance, affect the achievable per-node throughput in overlapping CR networks. In addition, these results serve as guidance for the deployment of multiple CR networks.\"",
        "Document: \"Review of stochastic hybrid systems with applications in biological systems modeling and analysis. Stochastic hybrid systems (SHS) have attracted a lot of research interests in recent years. In this paper, we review some of the recent applications of SHS to biological systems modeling and analysis. Due to the nature of molecular interactions, many biological processes can be conveniently described as a mixture of continuous and discrete phenomena employing SHS models. With the advancement of SHS theory, it is expected that insights can be obtained about biological processes such as drug effects on gene regulation. Furthermore, combining with advanced experimental methods, in silico simulations using SHS modeling techniques can be carried out for massive and rapid verification or falsification of biological hypotheses. The hope is to substitute costly and time-consuming in vitro or in vivo experiments or provide guidance for those experiments and generate better hypotheses.\"",
        "1 is \"Sending a Bi-Variate Gaussian Source over a Gaussian MAC\", 2 is \"CDMA-based MAC protocol for wireless ad hoc networks\"",
        "Given above information, for an author who has written the paper with the title \"Optimal admission and preemption control in finite-source loss systems\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008261": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Cooperative Search of Multiple Unknown Transient Radio Sources Using Multiple Paired Mobile Robots':",
        "Document: \"On the Analysis of the Depth Error on the Road Plane for Monocular Vision-Based Robot Navigation. A mobile robot equipped with a single camera can take images at different locations to obtain the 3D information of the environment for navigation. The depth information perceived by the robot is critical for obstacle avoidance. Given a calibrated camera, the accuracy of depth computation largely depends on locations where images have been taken. For any given image pair, the depth error in regions close to the camera baseline can be excessively large or even infinite due to the degeneracy introduced by the triangulation in depth computation. Unfortunately, this region often overlaps with the robot's moving direction, which could lead to collisions. To deal with the issue, we analyze depth computation and propose a predictive depth error model as a function of motion parameters. We name the region where the depth error is above a given threshold as an untrusted area. Note that the robot needs to know how its motion affect depth error distribution beforehand, we propose a closed-form model predicting how the untrusted area is distributed on the road plane for given robot/camera positions. The analytical results have been successfully verified in the experiments using a mobile robot.\"",
        "Document: \"Rider trunk and bicycle pose estimation with fusion of force/inertial sensors. Estimation of human pose in physical human-machine interactions such as bicycling is challenging because of highly-dimensional human motion and lack of inexpensive, effective motion sensors. In this paper, we present a computational scheme to estimate both the rider trunk pose and the bicycle roll angle using only inertial and force sensors. The estimation scheme is built on a rider-bicycle dynami...\"",
        "Document: \"Performance Evaluation and Schedule Optimization of Multi-Cluster Tools with Process Times Uncertainty. Performance evaluation and schedule optimization of cluster tools is challenging, especially when the process times are with uncertainty. In this paper, the critical path method (CPM) is used to analyze the throughput of multi-cluster tools with fixed action sequence. Slacks of non-critical path actions are studied and integrated to find the average cycle time of the cluster tool. A thin film tool from Novellus is used as an example to explain this methodology\"",
        "Document: \"Exact algorithms for non-overlapping 2-frame problem with non-partial coverage for networked robotic cameras. We report our algorithmic development on the 2-frame problem that addresses the need of coordinating two networked robotic pan-tilt-zoom (PTZ) cameras for n, (n > 2), competing rectangular observation requests. We assume the two camera frames have no overlap on their coverage. A request is satisfied only if it is fully covered by a camera frame. The satisfaction level for a given request is quantified by comparing its desirable observation resolution with that of the camera frame which fully covers it. We propose a series of exact algorithms for the solution that maximizes the overall satisfaction. Our algorithms solve the 2-frame problem in O(n2), O(n2m) and O(n3) times for fixed, m discrete and continuous camera resolution levels, respectively. We have implemented all the algorithms and compared them with the existing work.\"",
        "1 is \"A Statistical Method for People Counting in Crowded Environments\", 2 is \"Coverage Control By Multi-Robot Networks With Limited-Range Anisotropic Sensory\"",
        "Given above information, for an author who has written the paper with the title \"Cooperative Search of Multiple Unknown Transient Radio Sources Using Multiple Paired Mobile Robots\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008319": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Arithmetic Coding of Weighted Finite Automata':",
        "Document: \"A Joint Approach for Single-Channel Speaker Identification and Speech Separation. In this paper, we present a novel system for joint speaker identification and speech separation. For speaker identification a single-channel speaker identification algorithm is proposed which provides an estimate of signal-to-signal ratio (SSR) as a by-product. For speech separation, we propose a sinusoidal model-based algorithm. The speech separation algorithm consists of a double-talk/single-talk detector followed by a minimum mean square error estimator of sinusoidal parameters for finding optimal codevectors from pre-trained speaker codebooks. In evaluating the proposed system, we start from a situation where we have prior information of codebook indices, speaker identities and SSR-level, and then, by relaxing these assumptions one by one, we demonstrate the efficiency of the proposed fully blind system. In contrast to previous studies that mostly focus on automatic speech recognition (ASR) accuracy, here, we report the objective and subjective results as well. The results show that the proposed system performs as well as the best of the state-of-the-art in terms of perceived quality while its performance in terms of speaker identification and automatic speech recognition results are generally lower. It outperforms the state-of-the-art in terms of intelligibility showing that the ASR results are not conclusive. The proposed method achieves on average, 52.3% ASR accuracy, 41.2 points in MUSHRA and 85.9% in speech intelligibility.\"",
        "Document: \"Adaptive Filtering Of Raster Map Images Using Optimal Context Selection. Filtering of raster map images or more general class of palette-indexed images is considered as a discrete denoising problem with finite color output. Statistical features of local context are used to avoid damages of some specific but frequently occurring contexts caused by conventional filters. Several context-based approaches have been developed using either fixed context templates or context tree modeling. However, these algorithms fail to reveal the local geometrical structures when the underlying contexts are also contaminated. To address this problem, we propose a novel context-based voting method to identify the possible noisy pixels, which are excluded in the context selection and optimization. Experimental results show that the proposed context based filtering outperforms all other existing filters both for impulsive and Gaussian additive noise.\"",
        "Document: \"Maximum a Posteriori Adaptation of the Centroid Model for Speaker Verification. Maximum a posteriori adapted Gaussian mixture model (GMM-MAP) is widely used in speaker verification. GMMs have three sets of parameters to be adapted: means, covariances, and weights. However, practice has shown that it is sufficient to adapt the means only. Motivated by this, we formulate maximum a posteriori vector quantization (VQ-MAP) procedure which stores and adapts the mean vectors (centro...\"",
        "Document: \"On the use of context tree for binary image compression. We consider the use of a static context tree for binary image compression. The contexts are stored in the leaves of a variable-depth binary tree. The tree structure itself is fully static and optimized off-line for a training image. The structure of the tree is similar for different images of the same type. The benefit from optimizing the tree for each input image separately is usually overweighed by the overhead required from storing the tree structure. The static approach is therefore applicable in most situations as the compression can be performed much faster and during a single pass over the image\"",
        "1 is \"Information behaviour: an interdisciplinary perspective\", 2 is \"A fast swap-based local search procedure for location problems\"",
        "Given above information, for an author who has written the paper with the title \"Arithmetic Coding of Weighted Finite Automata\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008341": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Testing Time Goal-Driven Requirements with Model Checking Techniques':",
        "Document: \"Improving Pre-Kindergarten Touch Performance. Multi-touch technology provides users with a more intuitive way of interaction. However, pre-kindergarten children, a growing group of potential users, have problems with some basic gestures according to previous studies. This is particularly the case of the double tap and long pressed gestures, which have some issues related to spurious entry events and time-constrained interactions, respectively. In this paper, we empirically test specific strategies to deal with these issues by evaluating off-the-shelf implementations of these gestures against alternative implementations that follow these guidelines. The study shows that the implementation of these design guidelines has a positive effect on success rates of these two gestures, being feasible their inclusion in future multi-touch applications targeted at pre-kindergarten children.\"",
        "Document: \"Assessing the impact of the awareness level on a co-operative game. \u2022Gamespace Awareness is evaluated by means of a controlled experiment.\u2022A co-operative multiplayer game is developed to implement Gamespace Awareness.\u2022Awareness impact is measured by creating 3 configurations of the game.\u2022Score, time, enjoyment, happiness and perceived usefulness were the metrics.\u2022Emotion recognition was used to measure player happiness.\"",
        "Document: \"Weaving a network of architectural knowledge. Recent research in software architecture has reasserted an emphasis on keeping track of design decisions and their rationales during the development process, that is, on maintaining architectural knowledge (AK). This knowledge takes the form of explicit assets, interrelated in decision networks. We argue that the relationships structuring these networks contain valuable information, specially those describing negative semantics. For this reason, we have extended an architecture-centric, model- driven development process, ATRIUM, which already provides support for AK, with new AK relationships to define AK as a network of knowledge.\"",
        "Document: \"CSRML: a goal-oriented approach to model requirements for collaborative systems. A collaborative system is software which allows several users to work together and carry out collaboration, communication and coordination tasks. To perform these tasks, the users have to be aware of other user's actions, usually by means of a set of awareness techniques. In previous works, we found by means of empirical studies that the most suitable Requirements Engineering approach to specify the requirements of this kind of systems is the Goal-Oriented one, and more precisely i* approach. In this paper, CSRML (Collaborative Systems Requirements Modelling Language) is presented, an extension of i* to deal with the specification of the requirements of these systems in which the collaboration and the awareness of other users presence / actions are crucial. In order to validate this proposal, a case study has been carried out by modelling a jigsaw activity: a cooperative-learning technique in which students individually do some research in a proposed problem and then they teach each other what they have learned by sharing each individual view of the problem.\"",
        "1 is \"AB-HCI: an interface multi-agent system to support human-centred computing\", 2 is \"Towards the theoretical foundation of choreography\"",
        "Given above information, for an author who has written the paper with the title \"Testing Time Goal-Driven Requirements with Model Checking Techniques\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008349": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Exploiting Multiple Shift Invariances In Multidimensional Harmonic Retrieval Of Damped Exponentials':",
        "Document: \"Joint Transceiver Beamforming in MIMO Cognitive Radio Network Via Second-Order Cone Programming. This paper considers the spectrum sharing multiple-input\u2013multiple-output (MIMO) cognitive radio network, in which multiple primary users (PUs) coexist with multiple secondary users (SUs). Joint transceiver cognitive beam former design is introduced to minimize the transmit power of the SU base station (SBS) while simultaneously targeting lower bounds on the received signal-to-interference-plus-noise ratio (SINR) for the SUs and imposing upper limits on the interference temperature to the PUs. With the perfect knowledge of all links, the optimal secondary transceiver beam former is achieved iteratively. Due to the limited cooperation between SBS and PUs, perfect information of primary links may not be available at SBS which could lead to severe interference to the PUs. Robust designs are developed against the uncertainties in the primary links by keeping the interference to the PU below a prespecified threshold with high probability. Simulation results are presented to validate the effectiveness of the proposed algorithms that minimizes the total transmit power and simultaneously guarantees quality-of-service (QoS) of both SUs and PUs.\"",
        "Document: \"Direction finding and array calibration based on sparse reconstruction in partly calibrated arrays. A novel convex optimization problem formulation for source localization using partly calibrated arrays composed of subarrays with unknown displacements is introduced. The proposed formulation is based on sparse reconstruction using a mixed trace- and \u21131-norm minimization and exploits joint sparsity and special structure in the signal model. The new technique is applicable to subarrays of arbitrary topologies and allows the joint estimation of the directions of arrival (DoAs) and the array calibration. As shown by simulations, our new DoA estimation technique outperforms the state of the art method RARE, especially in low number of snapshot and low signal-to-noise ratio regime.\"",
        "Document: \"Decentralized direction finding using Lanczos method. The problem of Direction of Arrival (DoA) estimation using partly calibrated arrays composed of multiple identically oriented subarrays is considered. The subarrays are assumed to possess the shift invariance property which is exploited to achieve decentralized search free DoA estimation based on the ESPRIT method. In our previous work, the decentralized power method and the Averaging Consensus (AC) algorithm were used in the subspace estimation. To reduce the communicational cost without compromising the performance, our new algorithm uses the decentralized Lanczos method in combination with the AC algorithm to estimate the signal subspace. We further address the problem of Spurious Eigenvalues (SEVs) that usually arises in the Lanczos method. We propose a scheme to avoid the occurrence of SEVs while keeping the communicational cost low. Simulation results demonstrate that the proposed scheme is able to achieve similar performance as the decentralized power method with substantially reduced communicational cost. Furthermore, our method is able to estimate more DoAs than each subarray can autonomously identify.\"",
        "Document: \"A Compact Formulation for the $\\ell_{2, 1}$ Mixed-Norm Minimization Problem. Parameter estimation from multiple measurement vectors (MMVs) is a fundamental problem in many signal processing applications, e.g., spectral analysis and direction-of-arrival estimation. Recently, this problem has been addressed using prior information in form of a jointly sparse signal structure. A prominent approach for exploiting joint sparsity considers mixed-norm minimization in which, howev...\"",
        "1 is \"Robust Adaptive Beamforming Using Multidimensional Covariance Fitting\", 2 is \"A Case for Amplify\u2013Forward Relaying in the Block-Fading Multiple-Access Channel\"",
        "Given above information, for an author who has written the paper with the title \"Exploiting Multiple Shift Invariances In Multidimensional Harmonic Retrieval Of Damped Exponentials\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008347": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Research Paper: Quantitative Assessment of Dictionary-based Protein Named Entity Tagging':",
        "Document: \"Extracting information on pneumonia in infants using natural language processing of radiology reports. Natural language processing (NLP) is critical for improvement of the healthcare process because it has the potential to encode the vast amount of clinical data in textual patient reports. Many clinical applications require coded data to function appropriately, such as decision support and quality assurance applications. However, in order to be applicable in the clinical domain, performance of the NLP systems must be adequate. A valuable clinical application is the detection of infectious diseases, such as surveillance of healthcare-associated pneumonia in newborns (e.g. neonates) because it produces significant rates of morbidity and mortality, and manual surveillance of respiratory infection in these patients is a challenge. Studies have already demonstrated that automated surveillance using NLP tools is a useful adjunct to manual clinical management, and is an effective tool for infection control practitioners. This paper presents a study aimed at evaluating the feasibility of an NLP-based electronic clinical monitoring system to identify healthcare-associated pneumonia in neonates. We estimated sensitivity, specificity, and positive predictive value by comparing the detection with clinicians' judgments and our results demonstrated that the automated method was indeed feasible. Sensitivity (recall) was 87.5%, and specificity (true negative rates) was 94.1%.\"",
        "Document: \"Word sense disambiguation via semantic type classification. Accurate concept identification is crucial to biomedical natural language processing. However,ambiguity is common during the process of mapping terms to biomedical concepts (one term can be mapped to several concepts). A cost-effective approach to disambiguation relating to training is via semantic classification of the ambiguous terms,provided that the semantic classes of the concepts are available and are all different. We propose such a semantic classification based method to disambiguate ambiguous mappings with different semantic type(s), which can be used with any program that maps terms to UMLS concepts.Classifiers for the semantic types were built using abundant features extracted from a huge corpus with terms mapped to UMLS concepts. The method achieved a precision of 0.709, with unique advantages not achievable by the other comparable methods. Our results also demonstrate a need to further investigate the complementary properties of different methods.\"",
        "Document: \"Deriving comorbidities from medical records using natural language processing. Extracting comorbidity information is crucial for phenotypic studies because of the confounding effect of comorbidities. We developed an automated method that accurately determines comorbidities from electronic medical records. Using a modified version of the Charlson comorbidity index (CCI), two physicians created a reference standard of comorbidities by manual review of 100 admission notes. We processed the notes using the MedLEE natural language processing system, and wrote queries to extract comorbidities automatically from its structured output. Interrater agreement for the reference set was very high (97.7%). Our method yielded an F1 score of 0.761 and the summed CCI score was not different from the reference standard (p=0.329, power 80.4%). In comparison, obtaining comorbidities from claims data yielded an F1 score of 0.741, due to lower sensitivity (66.1%). Because CCI has previously been validated as a predictor of mortality and readmission, our method could allow automated prediction of these outcomes.\"",
        "Document: \"A new clustering method for detecting rare senses of abbreviations in clinical notes. Abbreviations are widely used in clinical documents and they are often ambiguous. Building a list of possible senses (also called sense inventory) for each ambiguous abbreviation is the first step to automatically identify correct meanings of abbreviations in given contexts. Clustering based methods have been used to detect senses of abbreviations from a clinical corpus [1]. However, rare senses remain challenging and existing algorithms are not good enough to detect them. In this study, we developed a new two-phase clustering algorithm called Tight Clustering for Rare Senses (TCRS) and applied it to sense generation of abbreviations in clinical text. Using manually annotated sense inventories from a set of 13 ambiguous clinical abbreviations, we evaluated and compared TCRS with the existing Expectation Maximization (EM) clustering algorithm for sense generation, at two different levels of annotation cost (10 vs. 20 instances for each abbreviation). Our results showed that the TCRS-based method could detect 85% senses on average; while the EM-based method found only 75% senses, when similar annotation effort (about 20 instances) was used. Further analysis demonstrated that the improvement by the TCRS method was mainly from additionally detected rare senses, thus indicating its usefulness for building more complete sense inventories of clinical abbreviations.\"",
        "1 is \"Literature mining of protein-residue associations with graph rules learned through distant supervision.\", 2 is \"ConText: an algorithm for determining negation, experiencer, and temporal status from clinical reports.\"",
        "Given above information, for an author who has written the paper with the title \"Research Paper: Quantitative Assessment of Dictionary-based Protein Named Entity Tagging\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008360": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Fractional-order oscillator design using unity-gain voltage buffers and OTAs':",
        "Document: \"A New Electronically Tunable Voltage-Mode Active-C Phase Shifter Using Uvc And Ota. In this paper, a new voltage-mode first-order all-pass filter is presented. The proposed filter provides both of non-inverting and inverting outputs at the same configuration. The circuit is composed of single universal voltage conveyor (UVC), operational transconductance amplifier (OTA), and capacitor. Considering the electronically tunability properties of the OTA phase responses of the proposed active-C circuit can be controlled by an external bias current. The circuit is suitable for wideband applications. All the active and passive sensitivities are low. The theoretical results are verified by PSPICE simulations using BJT implementations of UVC and OTA.\"",
        "Document: \"The Conception Of Differential-Input Buffered And Transconductance Amplifier (Dbta) And Its Application. In this paper, a novel versatile active building block the differential-input buffered and transconductance amplifier (DBTA) is proposed. The application of the newly defined active function block is shown on the design of voltage-mode (VM), multi-input and single-output (MISO)-type multifunction biquad, employing single DBTA and five passive elements. Proposed VM filter structure can realize four filter functions i.e., low-(LP), band-(BP), high-pass (HP) and band-stop (BS) without changing the circuit topology and enables independent control of the quality factor Q using single passive element. Theoretical results are verified by PSPICE simulations using a BJT realization of DBTA.\"",
        "Document: \"Behavioral Model For Emulation Of Zc-Cg-Vdcc. This paper presents a proposal and an experimental analysis of behavioral emulator of so-called z-copy controlled-gain voltage differencing current conveyor (ZC-CG-VDCC) based on commercially available devices. Most of them are electronically (by DC voltage) controllable. This work represents a suitable example of preliminary stage of development of an active device before its fabrication on silicon. This approach of development provides certain and significant information about basic functionality of the active device or the whole application and proves expected theory. The particular results of tested ZC-CG-VDCC implementation in frequency domain (transfers, gains, impedances) and DC domain (dynamics, linearity) are shown.\"",
        "Document: \"Current-mode precision full-wave rectifier using two WTA cells. In this paper a fully CMOS implementation of current-mode full-wave precision rectifier is presented. The structure is generally based on the recently presented Lazzaro's winner-takes-all (WTA) circuit. The rectifier has been implemented using the 0.35 \u03bcm CMOS technology and its behavior verified by SPICE. The simulation results shown feasibility to process signals of frequencies up to 20 MHz.\"",
        "1 is \"Image encryption scheme based on chaotic neural system\", 2 is \"Explicit-Current-Output Sinusoidal Oscillators Employing Only A Single Current-Feedback Op-Amp\"",
        "Given above information, for an author who has written the paper with the title \"Fractional-order oscillator design using unity-gain voltage buffers and OTAs\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008364": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Secure localization and authentication in ultra-wideband sensor networks':",
        "Document: \"An Identity-Based Security System for User Privacy in Vehicular Ad Hoc Networks. Vehicular ad hoc network (VANET) can offer various services and benefits to users and thus deserves deployment effort. Attacking and misusing such network could cause destructive consequences. It is therefore necessary to integrate security requirements into the design of VANETs and defend VANET systems against misbehavior, in order to ensure correct and smooth operations of the network. In this paper, we propose a security system for VANETs to achieve privacy desired by vehicles and traceability required by law enforcement authorities, in addition to satisfying fundamental security requirements including authentication, nonrepudiation, message integrity, and confidentiality. Moreover, we propose a privacy-preserving defense technique for network authorities to handle misbehavior in VANET access, considering the challenge that privacy provides avenue for misbehavior. The proposed system employs an identity-based cryptosystem where certificates are not needed for authentication. We show the fulfillment and feasibility of our system with respect to the security goals and efficiency.\"",
        "Document: \"Managing Wireless Sensor Networks with Supply Chain Strategy. Wireless sensor networks (WSNs) are appealing in obtaining fine-granular observations about the physical world. Due to the fact that WSNs are composed of a large number of low-cost but energy-constrained sensor nodes, along with the notorious timer-varying and error-prone natures of wireless links, scalable, robust, and energy-efficient data disseminating techniques are requisite for the emerging WSN applications such as environment monitoring and surveillance. In this paper we examine this emerging field from a view of supply chain management and propose a hybrid data dissemination framework for WSNs. In particular, we conceptually partition a whole sensor field into several functional regions based on the supply chain management methodology, and apply different routing schemes to different regions in order to provide better performance in terms of reliability and energy usage. For this purpose, we also propose a novel zone flooding scheme, essentially a combination of geometric routing and flooding techniques. Our hybrid data dissemination framework features low overhead, high reliability, good scalability and flexibility, and preferable energy efficiency. Detailed simulation studies are carried out to validate the effectiveness and efficiency of our scheme.\"",
        "Document: \"EyeTell: Video-Assisted Touchscreen Keystroke Inference from Eye Movements. Keystroke inference attacks pose an increasing threat to ubiquitous mobile devices. This paper presents EyeTell, a novel video-assisted attack that can infer a victim's keystrokes on his touchscreen device from a video capturing his eye movements. EyeTell explores the observation that human eyes naturally focus on and follow the keys they type, so a typing sequence on a soft keyboard results in a unique gaze trace of continuous eye movements. In contrast to prior work, EyeTell requires neither the attacker to visually observe the victim's inputting process nor the victim device to be placed on a static holder. Comprehensive experiments on iOS and Android devices confirm the high efficacy of EyeTell for inferring PINs, lock patterns, and English words under various environmental conditions.\"",
        "Document: \"A secure incentive protocol for mobile ad hoc networks. The proper functioning of mobile ad hoc networks depends on the hypothesis that each individual node is ready to forward packets for others. This common assumption, however, might be undermined by the existence of selfish users who are reluctant to act as packet relays in order to save their own resources. Such non-cooperative behavior would cause the sharp degradation of network throughput. To address this problem, we propose a credit-based Secure Incentive Protocol (SIP) to stimulate cooperation among mobile nodes with individual interests. SIP can be implemented in a fully distributed way and does not require any pre-deployed infrastructure. In addition, SIP is immune to a wide range of attacks and is of low communication overhead by using a Bloom filter. Detailed simulation studies have confirmed the efficacy and efficiency of SIP.\"",
        "1 is \"Privacy-preserving data aggregation without secure channel: Multivariate polynomial evaluation\", 2 is \"Semi-supervised graph clustering: a kernel approach\"",
        "Given above information, for an author who has written the paper with the title \"Secure localization and authentication in ultra-wideband sensor networks\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008426": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Controlling the Production of Neuro-symbolic Rules':",
        "Document: \"Constructing Modular Hybrid Rule Bases for Expert Systems. Neurules are a kind of hybrid rules integrating neurocomputing and production rules. Each neurule is represented as an adaline unit. Thus, the corresponding neurule base consists of a number of autonomous adaline units (neurules). Due to this fact, a modular and natural knowledge base is constructed, in contrast to existing connectionist knowledge bases. In this paper, we present a method for generating neurules from empirical data. To overcome the difficulty of the adaline unit to classify non-separable training examples, the notion of 'closeness' between training examples is introduced. In case of a training failure, two subsets of 'close' examples are produced from the initial training set and a copy of the neurule for each subset is trained. Failure of training any copy, leads to production of further subsets as far as success is achieved.\"",
        "Document: \"Using Machine Learning Techniques to Improve the Behaviour of a Medical Decision Support System for Prostate Diseases. Prostate gland diseases, including cancer, are estimated to be of the leading causes of male deaths worldwide and their management are based on clinical practice guidelines regarding diagnosis and continuing care. HIROFILOS-II is a prototype hybrid intelligent system for diagnosis and treatment of all prostate diseases based on symptoms and test results from patient health records. It is in contrast to existing efforts that deal with only prostate cancer. The main part of HIROFILOS-II is constructed by extracting rules from patient records via machine learning techniques and then manually transforming them into fuzzy rules. The system comprises crisp as well as fuzzy rules organized in modules. Experimental results show more than satisfactory performance of the system. The machine learning component of the system, which operates off-line, can be periodically used for rule updating, given that enough new patient records have been added to the database.\"",
        "Document: \"Improving the Integration of Neuro-Symbolic Rules with Case-Based Reasoning. In this paper, we present an improved approach integrating rules, neural networks and cases, compared to a previous one. The main approach integrates neurules and cases. Neurules are a kind of integrated rules that combine a symbolic (production rules) and a connectionist (adaline unit) representation. Each neurule is represented as an adaline unit. The main characteristics of neurules are that they improve the performance of symbolic rules and, in contrast to other hybrid neuro-symbolic approaches, they retain the modularity of production rules and their naturalness in a large degree. In the improved approach, various types of indices are assigned to cases according to different roles they play in neurule-based reasoning, instead of one. Thus, an enhanced knowledge representation scheme is derived resulting in accuracy improvement. Experimental results demonstrate its effectiveness.\"",
        "Document: \"An Efficient Hybrid Rule Based Inference Engine with Explanation Capability. An inference engine for a hybrid representation scheme based on neurules is presented. Neurules are a kind of hybrid rules that combine a symbolic (production rules) and a connectionist representation (adaline unit). The inference engine uses a connectionist technique, which is based on the 'firing potential', a measurement of the firing tendency of a neurule, and symbolic pattern matching. It is proved to be more efficient and natural than pure connectionist inference engines. Explanation of 'how' type can be provided in the form of if-then symbolic rules.\"",
        "1 is \"Efficient Computation of Attributes and Saliency Maps on Tree-Based Image Representations.\", 2 is \"The Andes Physics Tutoring System: Lessons Learned\"",
        "Given above information, for an author who has written the paper with the title \"Controlling the Production of Neuro-symbolic Rules\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008481": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Granular association rules with four subtypes':",
        "Document: \"Intra-cluster similarity index based on fuzzy rough sets for fuzzy c-means algorithm. Cluster validity indices have been used to evaluate the quality of fuzzy partitions. In this paper, we propose a new index, which uses concepts of Fuzzy Rough sets to evaluate the average intra-cluster similarity of fuzzy clusters produced by the fuzzy c-means algorithm. Experimental results show that contrasted with several well-known cluster validity indices, the proposed index can yield more desirable cluster number estimation.\"",
        "Document: \"Feature selection with test cost constraint. Feature selection is an important preprocessing step in machine learning and data mining. In real-world applications, costs, including money, time and other resources, are required to acquire the features. In some cases, there is a test cost constraint due to limited resources. We shall deliberately select an informative and cheap feature subset for classification. This paper proposes the feature selection with test cost constraint problem for this issue. The new problem has a simple form while described as a constraint satisfaction problem (CSP). Backtracking is a general algorithm for CSP, and it is efficient in solving the new problem on medium-sized data. As the backtracking algorithm is not scalable to large datasets, a heuristic algorithm is also developed. Experimental results show that the heuristic algorithm can find the optimal solution in most cases. We also redefine some existing feature selection problems in rough sets, especially in decision-theoretic rough sets, from the viewpoint of CSP. These new definitions provide insight to some new research directions.\"",
        "Document: \"Cost-sensitive approximate attribute reduction with three-way decisions. In the research spectrum of rough set, the task of attribute reduction is obtaining a minimal attribute subset that preserves certain properties of the original data. Cost-sensitive attribute reduction aims at minimizing various types of costs. Approximate attribute reduction allows decision makers to leverage the advantages of knowledge discovery and their own preferences. This paper proposes the cost-sensitive approximate attribute reduction problem under both qualitative and quantitative criteria. The qualitative criterion refers to the indiscernibility, while the quantitative criterion refers to the approximate parameter \u03b5 and the cost. We present a framework based on three-way decisions and discernibility matrix to handle this new problem. First, a quality function for attribute subsets is designed with the interpretation of a hierarchical granular structure. Second, a fitness function is designed for cost performance index by investigating attribute significance. Third, three-way decision theory is applied to partition the attributes into three groups based on the fitness function and a threshold pair (\u03b1,\u03b2). Finally, deletion-based and addition-based cost-sensitive approximate reduction algorithms are designed under this framework. Experimental results indicate that our algorithms outperform the state-of-the-art methods.\"",
        "Document: \"Test-cost-sensitive attribute reduction. In many data mining and machine learning applications, there are two objectives in the task of classification; one is decreasing the test cost, the other is improving the classification accuracy. Most existing research work focuses on the latter, with attribute reduction serving as an optional pre-processing stage to remove redundant attributes. In this paper, we point out that when tests must be undertaken in parallel, attribute reduction is mandatory in dealing with the former objective. With this in mind, we posit the minimal test cost reduct problem which constitutes a new, but more general, difficulty than the classical reduct problem. We also define three metrics to evaluate the performance of reduction algorithms from a statistical viewpoint. A framework for a heuristic algorithm is proposed to deal with the new problem; specifically, an information gain-based @l-weighted reduction algorithm is designed, where weights are decided by test costs and a non-positive exponent @l, which is the only parameter set by the user. The algorithm is tested with three representative test cost distributions on four UCI (University of California - Irvine) datasets. Experimental results show that there is a trade-off while setting @l, and a competition approach can improve the quality of the result significantly. This study suggests potential application areas and new research trends concerning attribute reduction.\"",
        "1 is \"Sparseness of support vector machines\", 2 is \"Granular computing applied to ontologies\"",
        "Given above information, for an author who has written the paper with the title \"Granular association rules with four subtypes\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008484": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A Weighted Method to Update Network User Preference Profile Dynamically':",
        "Document: \"Mixed Similarity Diffusion for Recommendation on Bipartite Networks. In recommender systems, collaborative filtering technology is an important method to evaluate user preference through exploiting user feedback data, and has been widely used in industrial areas. Diffusion-based recommendation algorithms inspired by diffusion phenomenon in physical dynamics are a crucial branch of collaborative filtering technology, which use a bipartite network to represent collection behaviors between users and items. However, diffusion-based recommendation algorithms calculate the similarity between users and make recommendations by only considering implicit feedback but neglecting the benefits from explicit feedback data, which would be a significant feature in recommender systems. This paper proposes a mixed similarity diffusion model to integrate both explicit feedback and implicit feedback. First, cosine similarity between users is calculated by explicit feedback, and we integrate it with resource-allocation index calculated by implicit feedback. We further improve the performance of the mixed similarity diffusion model by considering the degrees of users and items at the same time in diffusion processes. Some sophisticated experiments are designed to evaluate our proposed method on three real-world data sets. Experimental results indicate that recommendations given by the mixed similarity diffusion perform better on both the accuracy and the diversity than that of most state-of-the-art algorithms.\"",
        "Document: \"A Dynamic Model of Reposting Information Propagation Based on Empirical Analysis and Markov Process. In this paper, based on abundant data from Sina Weibo, we perform a comprehensive and in-depth empirical analysis of repostings and draw some conclusions. First, in regards to quantity, reposting takes up a large proportion of daily microblog activity. Second, the depth of repostings follows an exponential distribution and the first three orders of repostings hold 99 percent of the total amount of reposting, which provides an important foundation for solving the question of Influence Maximization. Third, the time interval for repostings also obeys exponential distribution. Therefore, we have built a dynamic information propagation model in terms of conclusions drawn from Weibo data and the Continuous-Time Markov Process. Due to the basis of the temporal network, our proposed model can change with the time and structure of a network, thus giving it good adaptability and predictability as compared to the traditional information diffusion model. From the final simulation results, our proposed model achieves a good predictive effect.\"",
        "Document: \"An integrated model for robust multisensor data fusion. This paper presents an integrated model aimed at obtaining robust and reliable results in decision level multisensor data fusion applications. The proposed model is based on the connection of Dempster-Shafer evidence theory and an extreme learning machine. It includes three main improvement aspects: a mass constructing algorithm to build reasonable basic belief assignments (BBAs); an evidence synthesis method to get a comprehensive BBA for an information source from several mass functions or experts; and a new way to make high-precision decisions based on an extreme learning machine (ELM). Compared to some universal classification methods, the proposed one can be directly applied in multisensor data fusion applications, but not only for conventional classifications. Experimental results demonstrate that the proposed model is able to yield robust and reliable results in multisensor data fusion problems. In addition, this paper also draws some meaningful conclusions, which have significant implications for future studies.\"",
        "Document: \"A fault-tolerant group key agreement protocol exploiting dynamic setting. AbstractA fault-tolerant group key agreement is an essential infrastructure for Internet communication among all involved participants; it can establish a secure session key no matter how many malicious participants exit simultaneously in an effort to disrupt the key agreement process. Recently, Zhao et al. proposed an efficient fault-tolerant group key agreement protocol named efficient group key agreement that can resist denial-of-service attacks, reply attacks, man-in-middle attacks, and common modulus attacks; it can also preserve forward secrecy with lower computational cost than previous protocols. We show that it is still vulnerable to active attacks by malicious participants and modify the corresponding security weakness adaptively. Furthermore, we propose an efficient fault-tolerant group key agreement based on a binary tree structure and enhance it to a dynamic setting where participants can leave or join the group arbitrarily according to their preferences with instant session key refreshment. Additionally, our session key refreshment is based on secure key updating to protect forward/backward confidentiality and is resistant to active/passive attacks. The performance analysis shows that our proposed protocol has lower computational cost and little additional communication cost exploiting dynamic setting. Copyright \u00a9 2013 John Wiley & Sons, Ltd.\"",
        "1 is \"On-chip network based embedded core testing\", 2 is \"A secure audio teleconference system\"",
        "Given above information, for an author who has written the paper with the title \"A Weighted Method to Update Network User Preference Profile Dynamically\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008493": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Performance Improvement Techniques For The Dvb-Rcs2 Return Link Air Interface':",
        "Document: \"Maximum likelihood post detection integration methods for spread spectrum systems. In the context of spread spectrum systems, this paper illustrates a novel and improved method to implement post detection integration in a code acquisition scheme in the presence of an unknown frequency error. The method is developed in the AWGN channel but is applicable to fading channels, too. The novel method is obtained through the application of the maximum likelihood criterion. The result is compared with the classic noncoherent and differential post detection integration schemes, showing significant improvements. Possible applications are in cellular CDMA networks and in satellite navigation and positioning systems.\"",
        "Document: \"Coarse beamforming techniques for multi-beam satellite networks. Coarse beamforming is a space processing scheme which allows for efficient use of the available spectral resources on the feeder link of a multi-beam broadband satellite system. In this framework, spectral occupancy of the multiplexed antenna signals that must be exchanged between the satellite and the gateway is a critical issue, up to the point that a costly multiple gateway infrastructure could be required to restrain bandwidth usage. Alternatively, a hybrid on board/on ground processing architecture is desirable, where the effect of space processing allows to project feed signals on a subspace, thus reducing the required feeder link bandwidth. This foresees a fixed processing scheme on board the satellite, which we refer to as Coarse Beamforming, yielding an overall system which relies on reasonable payload complexity, together with on ground processing flexibility. We explore two Coarse Beamforming techniques for the return link of a multi beam satellite system, and we evaluate the effect of compression in terms of reconstructed signal degradation. We show how, without significant distortion, bandwidth occupancy can be considerably reduced.\"",
        "Document: \"Initial synchronization procedure in S-UMTS networks for multimedia broadcast multicast services. W-CDMA and SW-CDMA (satellite W-CDMA) air interfaces require that a user equipment in a given cell acquires slot and frame synchronization, and identifies the primary scrambling code used by the target cell before starting communications. This synchronization procedure is identified as cell search procedure in W-CDMA and beam search procedure in SW-CDMA. Notwithstanding the extensive commonalities between the two air interfaces, the two procedures are actually different. The paper aims at the evaluation of the false acquisition probability achievable by the two procedures in a scenario in which the receiver may experience both satellite and terrestrial radio propagation conditions. This is a sensible evaluation scenario since, in the delivery of multicast-broadcast multimedia services, the satellite UMTS network represents an efficient complement to terrestrial UMTS, since its coverage is extended to indoor and urban areas through, for example, the use of terrestrial intermediate repeaters. Rayleigh fading, Rice fading, and vehicular multipath propagation channels are used for the performance assessment.\"",
        "Document: \"Multiuser detection for S-UMTS and GMR-1 mobile systems. AbstractIn this paper, we dwell on the applicability of successive interference cancellation SIC and turbo spatial minimum mean-squared error joint to parallel interference cancellation turbo SMMSE-PIC to a CDMA and a TDMA mobile satellite system MSS, i.e. Satellite Universal Mobile Telecommunication System S-UMTS and Geo Mobile Radio GMR-1, which provide seamless service and coverage extension to their terrestrial counterparts: UMTS and GSM. The adoption of MUD techniques for the return link of these two systems turns out to be instrumental in achieving the required high spectral efficiency but is also very challenging due to the peculiarities of the MSS environment. The theoretical capacity is computed for the considered scenarios, and is then compared with the system performance in real conditions, considering correlated Rician fading, non-linear distortion introduced by high-power amplifiers, and residual parameter estimation errors. Physical layer results are used to perform a link budget study and assess the potential system throughput increase. Our results show that MUD techniques allow to largely increase MSS throughput: in S-UMTS the gain exceeds 50%, while full frequency reuse can be adopted in GMR-1, compared with the three or more frequency reuse patterns adopted in currently operating systems. In S-UMTS, the additional antenna gain in the beam centre is exploited to further boost the achievable capacity. Copyright \u00a9 2007 John Wiley & Sons, Ltd.\"",
        "1 is \"Framework for intelligent service adaptation to user's context in next generation networks.\", 2 is \"Optimum and suboptimum frame synchronization for pilot-symbol-assisted modulation\"",
        "Given above information, for an author who has written the paper with the title \"Performance Improvement Techniques For The Dvb-Rcs2 Return Link Air Interface\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008519": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Solving Many to many multicast QoS routing problem using DCA and proximal decomposition technique':",
        "Document: \"Exact penalty and error bounds in DC programming. In the present paper, we are concerned with conditions ensuring the exact penalty for nonconvex programming. Firstly, we consider problems with concave objective and constraints. Secondly, we establish various results on error bounds for systems of DC inequalities and exact penalty, with/without error bounds, in DC programming. They permit to recast several class of difficult nonconvex programs into suitable DC programs to be tackled by the efficient DCA.\"",
        "Document: \"Large-Scale Molecular Optimization from Distance Matrices by a D. C. Optimization Approach. A so-called DCA method based on a d.c.\\ (difference of convex functions) optimization approach (algorithm) for solving large-scale distance geometry problems is developed. Different formulations of equivalent d.c.\\ programs in the $l_{1}$-approach are stated via the Lagrangian duality without gap relative to d.c.\\ programming, and new nonstandard nonsmooth reformulations in the $l_{\\infty }$-approach (resp., the $l_{1}-l_{\\infty }$-approach) are introduced. Substantial subdifferential calculations permit us to compute sequences of iterations in the DCA quite simply. The computations actually require matrix-vector products and only one Cholesky factorization (resp., with an additional solution of a convex program) in the $l_{1}$-approach (resp., the $l_{1}-l_{\\infty }$-approach) and allow the exploitation of sparsity in the large-scale setting. Two techniques---respectively, using shortest paths between all pairs of atoms to generate the complete dissimilarity matrix and the spanning trees procedure---are investigated in order to compute a good starting point for the DCA. Finally, many numerical simulations of the molecular optimization problems with up to 12567 variables are reported, which prove the practical usefulness of the nonstandard nonsmooth reformulations, the globality of found solutions, and the robustness and efficiency of our algorithms.\"",
        "Document: \"An efficient DCA for spherical separation. The binary classification problem consists in finding a separating surface minimizing an appropriate measure of the classification error. Several mathematical programming-based approaches for this problem have been proposed. The aim of spherical seperation is to find, in the input space or in the feature space, a minimal volume sphere separating set A from set B (i.e. a sphere enclosing all points of A and no points of B). The problem can be cast into the DC programming framework. Afterwards, we propose a simple DCA scheme for solving the resulting DC program in which all computations are explicit. Computational results show the efficiency of the proposed algorithms over the two other spherical seperation methods: FC[6] and UCM[7].\"",
        "Document: \"Behavior of DCA sequences for solving the trust-region subproblem. From our results it follows that any DCA sequence for solving the trust-region subproblem (see Pham Dinh and Le Thi, in SIAM J Optim 8:476---505, 1998) is convergent provided that the basic matrix of the problem is nonsingular and it does not have multiple negative eigenvalues. Besides, under this additional assumption, there exists such an open set \u9a74 containing the global minimizers and the unique local-nonglobal minimizer (if such exists) that any DCA sequence with the initial point from \u9a74 is contained in the set and converges to a global minimizer or the local-nonglobal minimizer. Various examples are given to illustrate the limiting behavior and stability of the DCA sequences. Structure of the KKT point set of the trust-region subproblem is also analyzed.\"",
        "1 is \"Distance Geometry Optimization for Protein Structures\", 2 is \"Solving a School Timetabling Problem Using a Bee Algorithm\"",
        "Given above information, for an author who has written the paper with the title \"Solving Many to many multicast QoS routing problem using DCA and proximal decomposition technique\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008525": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Linear-time computation of local periods':",
        "Document: \"A Heuristic For Computing Repeats With A Factor Oracle: Application To Biological Sequences. We present in this article a linear time and space method for the computation of the length of a repeated suffix for each prefix of a given word p . Our method is based on the utilization of the factor oracle of p which is a new and very compact structure introduced in [1], used for representing all the factors of p . We exhibit applications where our method really speeds up the computation of repetitions in words.\"",
        "Document: \"Speeding Up Two String-Matching Algorithms. We show how to speed up two string-matching algorithms: the Boyer-Moore algorithm (BM algorithm) and its version called here the reversed-factor algorithm (the RF algorithm). The RF algorithm is based on factor graphs for the reverse of the pattern. The main feature of both algorithms is that they scan the text right-to-left from the supposed right position of the pattern, BM algorithm goes as far as the scanned segment is a suffix of the pattern, while the RF algorithm is scanning while it is a factor of the pattern. Then they make a shift of the pattern, forget the history and start again. The RF algorithm usually makes bigger shifts than BM, but is quadratic in the worst case. We show that it is enough to remember the last matched segment to speed up considerably the RF algorithm (to make linear number of comparisons with small coefficient) and to speed up BM algorithm with match-shifts (to make at most 2.n comparisons). Only a constant additional memory is needed for the search phase. We give alternative versions of an accelerated algorithm RF: the first one is based on combinatorial properties of primitive words, and two others use extensively the power of suffix trees.\"",
        "Document: \"Supervised Term Weights for Biomedical Text Classification: Improvements in Nearest Centroid Computation. Maintaining accessibility of biomedical literature databases has led to development of text classification systems to assist human indexers by recommending thematic categories to biomedical articles. These systems rely on using machine learning methods to learn the association between the document terms and predefined categories. The accuracy of a text classification method depends on the metric used in order to assign a weight to each term. Weighting metrics can be classified as supervised or unsupervised according to whether they use prior information on the number of documents belonging to each category. In this paper, we propose two supervised weighting metrics (One-way Klosgen and Loevinger) which both improve the quality of biomedical document classification. We also show that by using moment generating function centroids, an alternative to the traditional arithmetical average centroids, a nearest centroid classifier with Loevinger metric performs significantly better than SVM on a biomedical text classification task.\"",
        "Document: \"Suffix Array of Alignment: A Practical Index for Similar Data. The <em>suffix tree of alignment</em> is an index data structure for similar strings. Given an alignment of similar strings, it stores all suffixes of the alignment, called <em>alignment-suffixes</em>. An alignment-suffix represents one suffix of a string or suffixes of multiple strings starting at the same position in the alignment. The suffix tree of alignment makes good use of similarity in strings theoretically. However, suffix trees are not widely used in biological applications because of their huge space requirements, and instead suffix arrays are used in practice. In this paper we propose a space-economical version of the suffix tree of alignment, named the <em>suffix array of alignment (SAA)</em>. Given an alignment <em>\u00ef\u00be\u00bf</em> of similar strings, the SAA for <em>\u00ef\u00be\u00bf</em> is a lexicographically sorted list of all the alignment-suffixes of <em>\u00ef\u00be\u00bf</em>. The SAA supports pattern search as efficiently as the <em>generalized suffix array</em>. Our experiments show that our index uses only 14% of the space used by the generalized suffix array to index 11 human genome sequences. The space efficiency of our index increases as the number of the genome sequences increases. We also present an efficient algorithm for constructing the SAA.\"",
        "1 is \"Fast and accurate long-read alignment with Burrows-Wheeler transform.\", 2 is \"Succinct data structures for flexible text retrieval systems\"",
        "Given above information, for an author who has written the paper with the title \"Linear-time computation of local periods\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008585": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Construction of Structurally Annotated Spoken Dialogue Corpus':",
        "Document: \"Dependency parsing of Japanese spoken monologue based on clause boundaries. Spoken monologues feature greater sentence length and structural complexity than do spoken dialogues. To achieve high parsing performance for spoken monologues, it could prove effective to simplify the structure by dividing a sentence into suitable language units. This paper proposes a method for dependency parsing of Japanese monologues based on sentence segmentation. In this method, the dependency parsing is executed in two stages: at the clause level and the sentence level. First, the dependencies within a clause are identified by dividing a sentence into clauses and executing stochastic dependency parsing for each clause. Next, the dependencies over clause boundaries are identified stochastically, and the dependency structure of the entire sentence is thus completed. An experiment using a spoken monologue corpus shows this method to be effective for efficient dependency parsing of Japanese monologue sentences.\"",
        "Document: \"Text Editing for Lecture Speech Archiving on the Web. It is very significant in the knowledge society to accumulate spoken documents on the web. However, because of the high redundancy of spontaneous speech, the transcribed text in itself is not readable on an Internet browser, and therefore not suitable as a web document. This paper proposes a technique for converting spoken documents into web documents for the purpose of building a speech archiving system. The technique edits automatically transcribed texts and improves its readability on the browser. The readable text can be generated by applying technology such as paraphrasing, segmentation and structuring to the transcribed texts. An edit experiment using lecture data showed the feasibility of the technique. A prototype system of spoken document archiving was implemented to confirm its effectiveness.\"",
        "Document: \"Identifying Nonlocal Dependencies In Incremental Parsing. This paper describes a method of identifying nonlocal dependencies in incremental parsing. Our incremental parser inserts empty elements at arbitrary positions to generate partial parse trees including empty elements. To identify the correspondence between empty elements and their fillers, our method adapts a hybrid approach: slash feature annotation and heuristic rules. This decreases local ambiguity in incremental parsing and improves the accuracy of our parser.\"",
        "Document: \"A Corpus Search System Utilizing Lexical Dependency Structure. This paper presents a corpus search system utilizing lexical dependency structure. The user's query consists of a sequence of keywords. For a given query, the system automatically generates the dependency structure patterns which consist of keywords in the query, and returns the sentences whose dependency structures match the generated patterns. The dependency structure patterns are generated by using two operations: combining and interpolation, which utilize dependency structures in the searched corpus. The operations enable the system to generate only the dependency structure patterns that occur in the corpus. The system achieves simple and intuitive corpus search and it is enough linguistically sophisticated to utilize structural information. Several corpus search systems have been presented. Most systems provide keyword-based search functionality. The search is simple and intuitive, but not enough linguistically sophisticated to utilize structural information. On the other hand, (Corley et al., 2001) and (Resnik and Elkiss, 2005) have presented corpus search systems utiliz- ing syntactic structure, Gsearch and Linguist's Search En- gine (LSE), respectively. These systems can search cor- pora by using phrase structure patterns. In the Gsearch, the user gives a phrase structure pattern and a grammar to the system. The system constructs parse trees of the sen- tences in the corpus by using the given grammar, and re- turns the sentences whose parse trees match the given pat- tern. In the LSE, the user first gives an example of sen- tences which he/she needs. The system parses the example by using a statistical parser and returns the parsing result. The user edits the resulting parse tree to specify a structural query. The system finally returns the sentences whose parse trees match the structural query. The Gsearch and LSE can search corpora by utilizing syntactic information. However, they do not achieve simple search like keyword-based sys- tems. This paper presents a corpus search system which auto- matically generates structural queries from keyword-based queries. The system searches corpora based on lexical de- pendency information. The user's query is a sequence of keywords. For a given query, it generates dependency struc- ture patterns by using two operations: combining and inter- polation. The user need neither to build a grammar like the Gsearch nor to edit structural query like the LSE, because of the automatic pattern generation. The system achieves simple and intuitive corpus search and it is enough to lin- guistically sophisticated to utilize structural information. 2. Corpus Search based on Dependency Structure\"",
        "1 is \"A Stochastic Model Of Human-Machine Interaction For Learning Dialog Strategies\", 2 is \"Bayesian Based Location Estimation System Using Wireless LAN\"",
        "Given above information, for an author who has written the paper with the title \"Construction of Structurally Annotated Spoken Dialogue Corpus\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008594": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'The Virtue Of Patience When Scheduling Media In Presence Of Feedback':",
        "Document: \"In-loop atom modulus quantization for matching pursuit and its application to video coding. This paper provides a precise analytical study of the selection and modulus quantization of matching pursuit (MP) coefficients. We demonstrate that an optimal rate-distortion trade-off is achieved by selecting the atoms up to a quality-dependent threshold, and by defining the modulus quantizer in terms of that threshold. In doing so, we take into account quantization error re-injection resulting from inserting the modulus quantizer inside the MP atom computation loop. In-loop quantization not only improves coding performance, but also affects the optimal quantizer design for both uniform and nonuniform quantization. We measure the impact of our work in the context of video coding. For both uniform and nonuniform quantization, the precise understanding of the relation between atom selection and quantization results in significant improvements in terms of coding efficiency. At high bitrates, the proposed nonuniform quantization scheme results in 0.5 to 2 dB improvement over the previous method.\"",
        "Document: \"An Autonomous Framework to Produce and Distribute Personalized Team-Sport Video Summaries: A Basketball Case Study. Democratic and personalized production of multimedia content is a challenge that content providers will have to face in the near future. In this paper, we address this challenge by building on computer vision tools to automate the collection and distribution of audiovisual content. Especially, we proposed a complete production process of personalized video summaries in a typical application scenario, where the sensor network for media acquisition is composed of multiple cameras, which, for example, cover a basketball field. Distributed analysis and interpretation of the scene are exploited to decide what to show or not to show about the event, so as to produce a video composed of a valuable subset of the streams provided by each individual camera. Interestingly, the selection of the streams subsets to forward to each user depends on his/her individual preferences, making the process adaptive and personalized. The process involves numerous integrated technologies and methodologies, including but not limited to automatic scene analysis, camera viewpoint selection, adaptive streaming, and generation of summaries through automatic organization of stories. The proposed technology provides practical solutions to a wide range of applications, such as personalized access to local sport events through a web portal, cost-effective and fully automated production of content dedicated to small-audience, or even automatic log in of annotations.\"",
        "Document: \"A resource allocation framework for summarizing team sport videos. We propose a flexible summarization framework for team-sport videos, which is able to integrate both the knowledge about displayed content (e.g. level of interest, type of view, etc.), and the individual (narrative) preferences of the user. Our framework builds on the partition of the original video sequence into independent segments, and create local stories by considering multiple ways to render each segment. We discuss how to segment videos based on production principles, and design the benefit function to evaluate various local stories from a segment. Summarization by selection of local stories is regarded as a resource allocation problem, and Lagrangian relaxation is performed to find the optimum. We use a soccer video to validate our framework in our experiments.\"",
        "Document: \"Loss-resilient window-based congestion control. This paper addresses the problem of fair allocation of bandwidth resources on lossy channels in hybrid heterogeneous networks. It discusses more particularly the ability of window-based congestion control to support non-congestion related losses. We investigate methods for efficient packet loss recovery by retransmission, and build on explicit congestion control mechanisms to decouple the packet loss detection from the congestion feedback signals. For different retransmission strategies that respectively rely on conventional cumulative acknowledgments or accurate loss monitoring, we show how the principles underlying the TCP retransmission mechanisms have to be adapted in order to take advantage of an explicit congestion feedback. A novel retransmission timer is proposed in order to deal with multiple losses of data segments and, in consequence, to allow for aggressive reset of the connection recovery timer. It ensures significant benefit from temporary inflation of the send-out window, and hence the fair share of bottleneck bandwidth between loss-prone and lossy connections. Extensive simulations analyze the performance of the new loss monitoring and recovery strategies, when used with two distinct explicit congestion control mechanisms. The first one relies on a coarse binary congestion notification from the routers. The second one, introduced in [D. Katabi, M. Handley, C. Rohrs, Internet congestion control for high bandwidth-delay product environments, ACM SIGCOMM (2002) 89-102], exploits accurate and finely-tuned router feedbacks to compute a precise congestion window adjustment. For both congestion control mechanisms, we observe that retransmissions triggered based on a precise monitoring of losses lead to efficient utilization of lossy links, and provide a fair share of the bottleneck bandwidth between heterogeneous connections, even for high loss ratios and bursty loss processes. Explicit window-based congestion control, combined with appropriate error control strategies, can therefore provide a valid solution to reliable and controlled connections over lossy network infrastructures.\"",
        "1 is \"Efficient large-scale multi-view stereo for ultra high-resolution image sets\", 2 is \"On throughput efficiency of geographic opportunistic routing in multihop wireless networks\"",
        "Given above information, for an author who has written the paper with the title \"The Virtue Of Patience When Scheduling Media In Presence Of Feedback\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008608": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Capability description and discovery of Internetware entity.':",
        "Document: \"Top-Down Mining of Interesting Patterns from Very High Dimensional Data. Many real world applications deal with transactional data, characterized by a huge number of transactions (tuples) with a small number of dimensions (attributes). However, there are some other applications that involve rather high dimensional data with a small number of tuples. Examples of such applications include bioinformatics, survey-based statistical analysis, text processing, and so on. High dimensional data pose great challenges to most existing data mining algorithms. Although there are numerous algorithms dealing with transactional data sets, there are few algorithms oriented to very high dimensional data sets with a relatively small number of tuples.\"",
        "Document: \"A new approach to classification based on association rule mining. Classification is one of the key issues in the fields of decision sciences and knowledge discovery. This paper presents a new approach for constructing a classifier, based on an extended association rule mining technique in the context of classification. The characteristic of this approach is threefold: first, applying the information gain measure to the generation of candidate itemsets; second, integrating the process of frequent itemsets generation with the process of rule generation; third, incorporating strategies for avoiding rule redundancy and conflicts into the mining process. The corresponding mining algorithm proposed, namely GARC (Gain based Association Rule Classification), produces a classifier with satisfactory classification accuracy, compared with other classifiers (e.g., C4.5, CBA, SVM, NN). Moreover, in terms of association rule based classification, GARC could filter out many candidate itemsets in the generation process, resulting in a much smaller set of rules than that of CBA.\"",
        "Document: \"Exploiting User Consuming Behavior for Effective Item Tagging. Automatic tagging techniques are important for many applications such as searching and recommendation, which has attracted many researchers' attention in recent years. Existing methods mainly rely on users' tagging behavior or items' content information for tagging, yet users' consuming behavior is ignored. In this paper, we propose to leverage such information and introduce a probabilistic model called joint-tagging LDA to improve tagging accuracy. An effective algorithm based on Zero-Order Collapsed Variational Bayes is developed. Experiments conducted on a real dataset demonstrate that joint-tagging LDA outperforms existing competing methods.\n\n\"",
        "Document: \"Bayesian Network Structure Learning from Attribute Uncertain Data. In recent years there has been a growing interest in Bayesian Network learning from uncertain data. While many researchers focus on Bayesian Network learning from data with tuple uncertainty, Bayesian Network structure learning from data with attribute uncertainty gets little attention. In this paper we make a clear definition of attribute uncertain data and Bayesian Network Learning problem from such data. We propose a structure learning method named DTAU based on information theory. The algorithm assumes that the structure of a Bayesian network is a tree. It avoids enumerating all possible worlds. The dependency tree is computed with polynomial time complexity. We conduct experiments to demonstrate the effectiveness and efficiency of our method. The experiments show the clustering results on uncertain dataset by our dependency tree are acceptable. \u00a9 2012 Springer-Verlag.\"",
        "1 is \"Effective sampling for mining association rules\", 2 is \"Random Walks on Regular and Irregular Graphs\"",
        "Given above information, for an author who has written the paper with the title \"Capability description and discovery of Internetware entity.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008617": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Availability Based Multiservice Delivery for Wireless Sensor Networks in Adversarial Environments':",
        "Document: \"Group Oriented Deniable Authentication Protocol. Deniable authenticated protocol is a new cryptographic authentication protocol that enables an intended receiver to identify the source of a given message without being able to prove the identity of the sender to a third party. Therefore, it can be applied to many practical situations. Over the past years, plenty of deniable authentication protocols have been put forth. In this paper, based on the elliptic curve cryptosystem, we would like to propose a novel group oriented deniable authentication protocol. In our proposed protocol, the number of the senders reaches the qualified threshold value, and then these senders can send a deniable authentication message to an intended receiver\"",
        "Document: \"Secure and efficient encrypted keyword search for multi-user setting in cloud computing. A key challenge to design searchable encryption in multi-user setting lies in the efficient management of encryption and search keys. Existing multi-user searchable encryption schemes either extend the single-user searchable encryption framework with broadcast encryption or require search user refers to the data owner and get the search token. However, this implies the necessity that the data owner distributes a single shared secret key among the group of users or requires the data owner stay online to authorize other users to search. In this paper, we address this practical problem, which is neglected in the literature. We also study secret-key-recovery attack where a malicious user can deduce a valid secret when given a search token. We show such attack violates secret key privacy, which is important in the whole system. Inspired by asymmetric group key agreement and multilinear map technology, we provide a secure and efficient encrypted keyword search scheme for multi-user setting, in which a data owner can share data with a group users without knowing which user in the group. In the proposed scheme, (a) each user has his own secret key, (b) each user generates trapdoors without getting any help from data owner or the third party, (c) our scheme features constant communication overhead, and (d) our scheme resist the secret-key-recovery attack. Our scheme preserves the traceability inherited from the asymmetric group key agreement system. We offer rigorous security proof of our scheme, and the performance analysis demonstrates the efficiency of our scheme.\"",
        "Document: \"Efficient Id-Based Multi-Receiver Threshold Decryption. Threshold decryption allows a message encrypted under a public key to be read only when a quorum of users cooperate to decrypt the ciphertext. However, such threshold decryption scheme does not apply well in the situation where all the users have their own public/private key pairs, but not share any private key associated with a public key, such as mobile ad hoc network featured by its dynamic character. An immediate way to achieve threshold decryption in this situation is to split the message into pieces, then encrypt these pieces under the public keys of different users. However, it is not efficient. In this paper, we propose an efficient identity based multi-receiver threshold decryption scheme that could be applied efficiently in the above situation. We also define the security notions and prove the security in random oracle model. At last, we add the broadcast feature to the scheme, such that a message could be broadcast to any number of groups.\"",
        "Document: \"Efficient Privacy-Preserving Outsourced Discrete Wavelet Transform in the Encrypted Domain. Signal processing in the encrypted domain is a potential tool to protect sensitive signals against untrusted cloud servers and unauthorized users in the delegated computing setting, without affecting the accuracy of large volume of signal analyzing and processing. Most existing approaches use Paillier\u2019s public key additively homomorphic encryption to encrypt each signal in a large bundle; thus, in...\"",
        "1 is \"Resource-efficient wireless relaying protocols\", 2 is \"Personality and patterns of Facebook usage\"",
        "Given above information, for an author who has written the paper with the title \"Availability Based Multiservice Delivery for Wireless Sensor Networks in Adversarial Environments\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008675": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'An end-to-end neural network approach to story segmentation.':",
        "Document: \"Cryptanalysis of Vo-Kim Forward Secure Signature in ICISC 2005. D. L. Vo and K. Kim proposed a forward secure signature scheme from bilinear pairings in annual International Conference on Information Security and Cryptology 2005. They claimed that their scheme satisfies several merits including requiring the general security parameters only independent to the total number of time periods and performing key evolving for unlimited time periods while maintaining sizes of keys and signature fixed. They also claimed this scheme is forward secure under the assumption of computational Diffie-Hellman problem. In this paper, we analyze the security of this scheme and point out this scheme doesn't satisfy the forward security.\"",
        "Document: \"A Publicly Verifiable Dynamic Sharing Protocol for Data Secure Storage. How to protect the security of vital data is one of the most important issues of the database security. An efficient method is to divide the vital data into multiple parts that are stored among a group of servers by secret sharing technique. In this paper, we propose a publicly verifiable dynamic sharing protocol for data secure storage. In this protocol, the important data can be publicly verifiably shared among multiple servers, at the same time, the protocol can dynamically recover the bad shares in the system if some servers are attacked. Different from previous protocols, the new protocol is not only efficient but also practical in many circumstances because all operations can be verified by everyone not only shareholders.\"",
        "Document: \"Number-Theoretic Attack on Lyuu-Wu's Multi-proxy Multi-signature Scheme. Y. D. Lyuu and M. L. Wu had proposed an improved multi-proxy multi-signature scheme, which was claimed to resist the forge attack. Lately, L. Guo and G. Wang found an inside attack on the Lyuu-Wu's scheme. In this paper, we propose a new attack on Lyuu-Wu's scheme, which can factor the parameter N and Q by using efficient number-theoretic algorithms when Q is roughly larger than the square root of N. It follows that Lyuu-Wu's scheme suffers from the forge attack from the proxy signers in that case.\"",
        "Document: \"Topic embedding of sentences for story segmentation. In this paper, we propose to embed sentences into fixed-dimensional vectors that carry the topic information for story segmentation. As a sentence comprises of a sequence of words and may have different lengths, we use long short-term memory recurrent neural network (LSTM-RNN) to summarize the information of the whole sentence and only predict the topic class at the last word in the sentence. The output of the network at the last word can be used as an embedding of the sentence in the topic space. We used the obtained sentence embeddings in the HMM-based story segmentation framework and obtained promising results. On the TDT2 corpus, the F1 measure is improved to 0.789 from 0.765 which is obtained by a competitive system using DNN and bag-of-words features.\"",
        "1 is \"Mochi: visual log-analysis based tools for debugging hadoop\", 2 is \"Memory efficient subsequence DTW for Query-by-Example Spoken Term Detection\"",
        "Given above information, for an author who has written the paper with the title \"An end-to-end neural network approach to story segmentation.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008676": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Programmable multi-granular optical networks: requirements and architecture':",
        "Document: \"Programmable Multi-Granular Optical Router: Modular Architecture and Testing. Dynamic, flexible and programmable network nodes will enable the key infrastructure for the Future Internet. Re-engineering of the traditional router architectures is required in order to allow hardware-and vendor-independent routing functions and to design open and cost-effective service platforms. This paper presents a modular programmable router architecture to support multi-granular routing based on optical switching matrices, according to recent IETF standards. A possible implementation of the proposed concepts is demonstrated using a software-based optical router emulator, which integrates optical packet and circuit switching paradigms. The emulator is designed as a test platform to prove the feasibility of different subsystems and their interoperability.\"",
        "Document: \"Comparative analysis of SMP click scheduling techniques. The interest of the scientific and commercial telecommunications community for the use of software routers running in general purpose (PC) hardware, as an alternative to the traditional special purpose hardware routers, is risen quickly in the last few years. This is due to the high level of flexibility and extensibility of this solution: the support for new protocols and network architectures and services, in fact, is easily obtained by re-programming the router itself. In addition, the diffusion of multiprocessor systems due to the progress in the semiconductor technologies allows software routers to obtain high performance if supported by multiprocessor PC hardware. Of course, in order to achieve a good use of the potentiality offered by multiprocessor architectures, the distribution of the tasks among the CPUs, and the parallel execution of the different operations, requires to be performed with some care. This paper demonstrates the benefits given by the hardware technological improvements, mainly concerning the use of multiple CPUs systems with respect to single processor ones, and shows how excellent forwarding performance can be achieved by Click software routers running on high powered PC hardware. Moreover, through the comparative analysis of different CPU scheduling approaches available in SMP Click, the paper discusses how different CPU scheduling techniques, that is, different approaches in the assignment of the tasks to the different CPUs, affect the router performance.\"",
        "Document: \"MPLS over Optical Packet Switching. This paper deals with the problem of connection to wavelength assignment in an MPLS optical packet switched network with DWDM links. The need to adopt dynamic allocation of connections to wavelengths is outlined to avoid congestion and dynamic wavelength assignment algorithms are proposed. The main results show the effectiveness of dynamic assignment with respect to static one and show that the connection configuration can be exploited for performance enhancement\"",
        "Document: \"Centralized vs. distributed algorithms for resilient 5G access networks. Cloud radio access networks (C-RANs), relying on network function virtualization and software-defined networking (SDN), require a proper placement of baseband functionalities (BBUs) to reach full coverage of served areas and service continuity. In this context, network resources can be shared and orchestrated to meet the flexibility required by a dynamically evolving environment. Different methodologies, based on analytical formulation or heuristic algorithms, can be applied to achieve suitable trade-offs among cost components. This paper considers both centralized and distributed algorithms to obtain BBU hotel placement in C-RAN and compares their performance, scalability and adaptability to evolving scenarios. As expected, the results obtained with the distributed approach are sub-optimal, but very close, in most cases, to the optimal solutions obtained with a centralized algorithm based on integer linear programming. In addition to off-loading the SDN orchestrator, the distributed approach, differently from the centralized one, is shown to be able to cope with the evolution of the C-RAN topology with limited incremental changes in the original placement. The limits of the centralized approach in terms of scalability that the distributed approach is able to overcome are also evidenced.\"",
        "1 is \"QoS performance of optical burst switching in IP-over-WDM networks\", 2 is \"Power line channel characteristics and their effect on communication system design\"",
        "Given above information, for an author who has written the paper with the title \"Programmable multi-granular optical networks: requirements and architecture\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008722": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'OntoDoc: an ontology-based query system for digital libraries':",
        "Document: \"Recognition of Rotated Characters by Inexact Matching. In this note we address the recognition of rotated hand-printed characters. We define the sides and the lids of a plane figure and outline an inexact matching process using such features based on the edit distance between circular words of different lengths.\"",
        "Document: \"Lossless Image Compression By Block Matching On Practical Massively Parallel Architectures. Work-optimal O(logM log n) time implementations of lossless image compression by block matching are shown on the PRAM EREW, where n is the size of the image and M is the maximum size of the match, which can be implemented on practical architectures such as meshes of trees, pyramids and multigrids. The work-optimal implementations on pyramids and multigrids are possible under some realistic assumptions. Decompression on these architectures is also possible with the same parallel computational complexity.\"",
        "Document: \"Automatic selection of regions of interest in a video by a depth-color image matting. The automatic detection of regions of interest in a video is fundamental for a fast generation of many ground truth images. In this paper we introduce a new solution for selecting regions of interest based on an automatic image matting method. Image matting is a set of techniques designed to obtain a precise separation of background and foreground in image or video sequences. Basically all the matting approaches need a direct human interaction, there are only few total automatic solutions. To achieve this goal we combine two different video streams: the color one and the depth one. In particular, we use an automatic depth based segmentation to substitute the human input in the Soft Scissors, one of the most precise matting algorithm. The overall efficiency is achieved using the Nvidia CUDA architecture to execute the most computational intensive sections of algorithm. The result of the matting can be used as a ground truth for successive elaborations.\"",
        "Document: \"Tuning range image segmentation by genetic algorithm. Several range image segmentation algorithms have been proposed, each one to be tuned by a number of parameters in order to provide accurate results on a given class of images. Segmentation parameters are generally affected by the type of surfaces (e.g., planar versus curved) and the nature of the acquisition system (e.g., laser range finders or structured light scanners). It is impossible to answer the question, which is the best set of parameters given a range image within a class and a range segmentation algorithm? Systems proposing such a parameter optimization are often based either on careful selection or on solution space-partitioning methods. Their main drawback is that they have to limit their search to a subset of the solution space to provide an answer in acceptable time. In order to provide a different automated method to search a larger solution space, and possibly to answer more effectively the above question, we propose a tuning system based on genetic algorithms. A complete set of tests was performed over a range of different images and with different segmentation algorithms. Our system provided a particularly high degree of effectiveness in terms of segmentation quality and search time.\"",
        "1 is \"Feature Selection for High-Dimensional Data: A Fast Correlation-Based Filter Solution\", 2 is \"Publicly available topic signatures for all WordNet nominal senses\"",
        "Given above information, for an author who has written the paper with the title \"OntoDoc: an ontology-based query system for digital libraries\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008735": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Catching the boat with Strudel: experiences with a Web-site management system':",
        "Document: \"Development of Test Toolkit of Hard Review to Evaluate a Random Clinical Decision Support System for the Management of Chronic Adult Diseases. Over 30 % of Korean population suffers from chronic diseases, and the management cost of this portion of the population is estimated to be more than 7 % of the gross domestic product and is expected to continue to increase with the aging of society. The u-healthcare (ubiquitous healthcare)-based home healthcare service has been emerging as a solution to this issue. Clinical decision support system (CDSS) is software that provides professional medical knowledge to help people make medical decisions, and is required for the efficient operation of u-healthcare as a core technology. U-healthcare-based CDSS is meant to improve accessibility of medical information for the public, but it at the same time carries a substantial risk of misuse. Hence, a certain level of standards and specifications is required to guarantee the quality of u-healthcare-based CDSS to protect public health. In this context, this project aimed to develop technologies to evaluate reliabilities of u-healthcare-based CDSS dealing with diabetes mellitus and hypertension. This study created a Test Toolkit to evaluate random CDSS with a function to confirm results obtained from CDSS performance evaluation technologies. Results of CDSS were obtained based on screening test items. The design descriptions are defined as (1) creation of test scenario to test CDSS, (2) grading function using returned answers to CDSS, and (3) confirmation of test result. The functions of the Test Toolkit are divided into two types. (1) Test scenarios and the answers to the scenarios are created by reading and fabricating standard scenarios from the DB based on test items input by the evaluator. (2) Appraisal CDSS creates a file of result values using a given test scenario file, and then a CDSS evaluation report, classified by test item, is formed by comparing answers to those found in (1). It should be noted that functions (1) and (2) are completely independent. Conclusively, researchers developed a toolkit to evaluate clinical reliabilities and software stabilities of CDSS for healthcare free from misjudgments caused by human factors and to speed up overall evaluation processes tremendously and efficiently. This toolkit will ensure proper usage of CDSS by the public for health benefits.\"",
        "Document: \"Comparative meta-analysis between human and mouse cancer microarray data reveals critical pathways. Identification of deregulated biomolecular pathways in cancer may be more important than identification of individual genes through differential expression. We have analysed data from 87 microarray datasets, spanning 25 different types of cancer, and have identified several hundred pathways that are statistically significant (p < 0.01) and deregulated in cancer. We also conducted a meta-analysis of 18 mouse cancer datasets and found that a statistically significant number of ontology terms are common between human and mouse cancers and known for their role in carcinogenesis. These point to critical pathways that are disrupted in both human and mouse cancers.\"",
        "Document: \"Empirical Bayes analysis of unreplicated microarray data. Because of the high costs of microarray experiments and the availability of only limited biological materials, microarray experiments are often performed with a small number of replicates. Investigators, therefore, often have to perform their experiments with low replication or without replication. However, the heterogeneous error variability observed in microarray experiments increases the difficulty in analyzing microarray data without replication. No current analysis techniques are practically applicable to such microarray data analysis. We here introduce a statistical method, the so-called unreplicated heterogeneous error model (UHEM) for the microarray data analysis without replication. This method is possible by utilizing many adjacent-intensity genes for estimating local error variance after nonparametric elimination of differentially expressed genes between different biological conditions. We compared the performance of UHEM with three empirical Bayes prior specification methods: between-condition local pooled error, pseudo standard error, or adaptive standard error-based HEM. We found that our unreplicated HEM method is effective for the microarray data analysis when replication of an array experiment is impractical or prohibited.\"",
        "Document: \"ARCS: an aggregated related column scoring scheme for aligned sequences. Biologists frequently align multiple biological sequences to determine consensus sequences and/or search for predominant residues and conserved regions. Particularly, determining conserved regions in an alignment is one of the most important activities. Since protein sequences are often several-hundred residues or longer, it is difficult to distinguish biologically important conserved regions (motifs or domains) from others. The widely used tools, Logos, Al2co, Confind, and the entropy-based method, often fail to highlight such regions. Thus a computational tool that can highlight biologically important regions accurately will be highly desired.This paper presents a new scoring scheme ARCS (Aggregated Related Column Score) for aligned biological sequences. ARCS method considers not only the traditional character similarity measure but also column correlation. In an extensive experimental evaluation using 533 PROSITE patterns, ARCS is able to highlight the motif regions with up to 77.7% accuracy corresponding to the top three peaks.The source code is available on http://bio.informatics.indiana.edu/projects/arcs and http://goldengate.case.edu/projects/arcs\"",
        "1 is \"Automatic segmentation of text into structured records\", 2 is \"The history of histograms (abridged)\"",
        "Given above information, for an author who has written the paper with the title \"Catching the boat with Strudel: experiences with a Web-site management system\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008796": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Tracking Multiple Social Media for Stock Market Event Prediction.':",
        "Document: \"How events unfold: spatiotemporal mining in social media. There has been significant recent interest in the application of social media analytics for spatiotemporal event mining. However, no structured survey exists to capture developments in this space. This paper seeks to fill this void by reviewing recent research trends. Three branches of research are summarized here---corresponding (resp.) to modeling the past, present, and future---information tracking and backward analysis, spatiotemporal event detection, and spatiotemporal event forecasting. Each branch is illustrated with examples, challenges, and accomplishments.\n\n\"",
        "Document: \"Multi-Task Learning for Spatio-Temporal Event Forecasting. Spatial event forecasting from social media is an important problem but encounters critical challenges, such as dynamic patterns of features (keywords) and geographic heterogeneity (e.g., spatial correlations, imbalanced samples, and different populations in different locations). Most existing approaches (e.g., LASSO regression, dynamic query expansion, and burst detection) are designed to address some of these challenges, but not all of them. This paper proposes a novel multi-task learning framework which aims to concurrently address all the challenges. Specifically, given a collection of locations (e.g., cities), we propose to build forecasting models for all locations simultaneously by extracting and utilizing appropriate shared information that effectively increases the sample size for each location, thus improving the forecasting performance. We combine both static features derived from a predefined vocabulary by domain experts and dynamic features generated from dynamic query expansion in a multi-task feature learning framework; we investigate different strategies to balance homogeneity and diversity between static and dynamic terms. Efficient algorithms based on Iterative Group Hard Thresholding are developed to achieve efficient and effective model training and prediction. Extensive experimental evaluations on Twitter data from four different countries in Latin America demonstrated the effectiveness of our proposed approach.\"",
        "Document: \"Spatiotemporal Event Forecasting in Social Media. Event forecasting in Twitter is an important and challenging problem. Most existing approaches focus on forecasting temporal events (such as elections and sports) and do not consider spatial features and their underlying correlations. In this paper, we propose a generative model for spatiotemporal event forecasting in Twitter. Our model characterizes the underlying development of future events by jointly modeling the structural contexts and spatiotemporal burstiness. An effective inference algorithm is developed to train the model parameters. Utilizing the trained model, the alignment likelihood of tweet sequences is calculated by dynamic programming. Extensive experimental evaluations on two different domains demonstrated the effectiveness of our pro-\"",
        "Document: \"Non-parametric scan statistics for event detection and forecasting in heterogeneous social media graphs. Event detection in social media is an important but challenging problem. Most existing approaches are based on burst detection, topic modeling, or clustering techniques, which cannot naturally model the implicit heterogeneous network structure in social media. As a result, only limited information, such as terms and geographic locations, can be used. This paper presents Non-Parametric Heterogeneous Graph Scan (NPHGS), a new approach that considers the entire heterogeneous network for event detection: we first model the network as a \\\"sensor\\\" network, in which each node senses its \\\"neighborhood environment\\\" and reports an empirical p-value measuring its current level of anomalousness for each time interval (e.g., hour or day). Then, we efficiently maximize a nonparametric scan statistic over connected subgraphs to identify the most anomalous network clusters. Finally, the event represented by each cluster is summarized with information such as type of event, geographical locations, time, and participants. As a case study, we consider two applications using Twitter data, civil unrest event detection and rare disease outbreak detection, and present empirical evaluations illustrating the effectiveness and efficiency of our proposed approach.\"",
        "1 is \"Effective Community Search over Large Spatial Graphs.\", 2 is \"Speech recognition with deep recurrent neural networks\"",
        "Given above information, for an author who has written the paper with the title \"Tracking Multiple Social Media for Stock Market Event Prediction.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008804": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A Belief Model of Query Difficulty That Uses Subjective Logic':",
        "Document: \"User perspectives on query difficulty. The difficulty of a user query can affect the performance of Information Retrieval (IR) systems. What makes a query difficult and how one may predict this is an active research area, focusing mainly on factors relating to the retrieval algorithm, to the properties of the retrieval data, or to statistical and linguistic features of the queries that may render them difficult. This work addresses query difficulty from a different angle, namely the users' own perspectives on query difficulty. Two research questions are asked: (1) Are users aware that the query they submit to an IR system may be difficult for the system to address? (2) Are users aware of specific features in their query (e.g., domain-specificity, vagueness) that may render their query difficult for an IR system to address? A study of 420 queries from a Web search engine query log that are pre-categorised as easy, medium, hard by TREC based on system performance, reveals an interesting finding: users do not seem to reliably assess which query might be difficult; however, their assessments of which query features might render queries difficult are notably more accurate. Following this, a formal approach is presented for synthesising the user-assessed causes of query difficulty through opinion fusion into an overall assessment of query difficulty. The resulting assessments of query difficulty are found to agree notably more to the TREC categories than the direct user assessments.\"",
        "Document: \"Expanding queries with term and phrase translations in patent retrieval. Patent retrieval is a branch of Information Retrieval (IR) that aims to enable the challenging task of retrieving highly technical and often complicated patents. Typically, patent granting bodies translate patents into several major foreign languages, so that language boundaries do not hinder their accessibility. Given such multilingual patent collections, we posit that the patent translations can be exploited for facilitating patent retrieval. Specifically, we focus on the translation of patent queries from German and French, the morphology of which poses an extra challenge to retrieval. We compare two translation approaches that expand the query with (i) translated terms and (ii) translated phrases. Experimental evaluation on a standard CLEF-IP European Patent Office dataset reveals a novel finding: phrase translation may be more suited to French, and term translation may be more suited to German. We trace this finding to language morphology, and we conclude that tailoring the query translation per language can lead to improved results in patent retrieval.\"",
        "Document: \"To Phrase or Not to Phrase - Impact of User versus System Term Dependence Upon Retrieval. When submitting queries to information retrieval (IR) systems, users often have the option of specifying which, if any, of the query terms are heavily dependent on each other and should be treated as a fixed phrase, for instance by placing them between quotes. In addition to such cases where users specify term dependence, automatic ways also exist for IR systems to detect dependent terms in queries. Most IR systems use both user and algorithmic approaches. It is not however clear whether and to what extent user-defined term dependence agrees with algorithmic estimates of term dependence, nor which of the two may fetch higher performance gains. Simply put, is it better to trust users or the system to detect term dependence in queries? To answer this question, we experiment with 101 crowdsourced search engine users and 334 queries (52 train and 282 test TREC queries) and we record 10 assessments per query. We find that (i) user assessments of term dependence differ significantly from algorithmic assessments of term dependence (their overlap is approximately 30%); (ii) there is little agreement among users about term dependence in queries, and this disagreement increases as queries become longer; (iii) the potential retrieval gain that can be fetched by treating term dependence (both user- and system-defined) over a bag of words baseline is reserved to a small subset (approxi-mately 8%) of the queries, and is much higher for low-depth than deep preci-sion measures. Points (ii) and (iii) constitute novel insights into term dependence.\"",
        "Document: \"Dependencies: Formalising Semantic Catenae for Information Retrieval. Building machines that can understand text like humans is an AI-complete problem. A great deal of research has already gone into this, with astounding results, allowing everyday people to discuss with their telephones, or have their reading materials analysed and classified by computers. A prerequisite for processing text semantics, common to the above examples, is having some computational representation of text as an abstract object. Operations on this representation practically correspond to making semantic inferences, and by extension simulating understanding text.  complexity and granularity of semantic processing that can be realised is constrained by the mathematical and computational robustness, expressiveness, and rigour of the tools used.  dissertation contributes a series of such tools, diverse in their mathematical formulation, but common in their application to model semantic inferences when machines process text. These tools are principally expressed in nine distinct models that capture aspects of semantic dependence in highly interpretable and non-complex ways. This dissertation further reflects on present and future problems with the current research paradigm in this area, and makes recommendations on how to overcome them. The amalgamation of the body of work presented in this dissertation advances the complexity and granularity of semantic inferences that can be made automatically by machines.\"",
        "1 is \"A sequential factorization method for recovering shape and motion from image streams\", 2 is \"Re-ranking approach to classification in large-scale power-law distributed category systems\"",
        "Given above information, for an author who has written the paper with the title \"A Belief Model of Query Difficulty That Uses Subjective Logic\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008824": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Integrating BDD-Based and SAT-Based Symbolic Model Checking':",
        "Document: \"On Optimization Modulo Theories, MaxSMT and Sorting Networks. Optimization Modulo Theories $$\\\\text {OMT}$$ is an extension of SMT which allows for finding models that optimize given objectives. Partial weighted MaxSMT---or equivalently $$\\\\text {OMT}$$ with Pseudo-Boolean objective functions, $$\\\\text {OMT+PB}$$ --- is a very-relevant strict subcase of $$\\\\text {OMT}$$. We classify existing approaches for MaxSMT or $$\\\\text {OMT+PB}$$ in two groups: MaxSAT-based approaches exploit the efficiency of state-of-the-art MaxSAT solvers, but they are specific-purpose and not always applicable; OMT-based approaches are general-purpose, but they suffer from intrinsic inefficiencies on MaxSMT/$$\\\\text {OMT+PB}$$ problems. We identify a major source of such inefficiencies, and we address it by enhancing $$\\\\text {OMT}$$ by means of bidirectional sorting networks. We implemented this idea on top of the OptiMathSAT $$\\\\text {OMT}$$ solver. We run an extensive empirical evaluation on a variety of problems, comparing MaxSAT-based and $$\\\\text {OMT}$$-based techniques, with and without sorting networks, implemented on top of OptiMathSAT and [InlineEquation not available: see fulltext.]. The results support the effectiveness of this idea, and provide interesting insights about the different approaches.\"",
        "Document: \"MathSAT: Tight Integration of SAT and Mathematical Decision Procedures. Recent improvements in propositional satisfiability techniques (SAT) made it possible to tackle successfully some hard real-world problems (e.g., model-checking, circuit testing, propositional planning) by encoding into SAT. However, a purely Boolean representation is not expressive enough for many other real-world applications, including the verification of timed and hybrid systems, of proof obligations in software, and of circuit design at RTL level. These problems can be naturally modeled as satisfiability in linear arithmetic logic (LAL), that is, the Boolean combination of propositional variables and linear constraints over numerical variables. In this paper we present MathSAT, a new, SAT-based decision procedure for LAL, based on the (known approach) of integrating a state-of-the-art SAT solver with a dedicated mathematical solver for LAL. We improve MathSAT in two different directions. First, the top\u9a74level line procedure is enhanced and now features a tighter integration between the Boolean search and the mathematical solver. In particular, we allow for theory-driven backjumping and learning, and theory-driven deduction; we use static learning in order to reduce the number of Boolean models that are mathematically inconsistent; we exploit problem clustering in order to partition mathematical reasoning; and we define a stack-based interface that allows us to implement mathematical reasoning in an incremental and backtrackable way. Second, the mathematical solver is based on layering; that is, the consistency of (partial) assignments is checked in theories of increasing strength (equality and uninterpreted functions, linear arithmetic over the reals, linear arithmetic over the integers). For each of these layers, a dedicated (sub)solver is used. Cheaper solvers are called first, and detection of inconsistency makes call of the subsequent solvers superfluous. We provide a through experimental evaluation of our approach, by taking into account a large set of previously proposed benchmarks. We first investigate the relative benefits and drawbacks of each proposed technique by comparison with respect to a reference option setting. We then demonstrate the global effectiveness of our approach by a comparison with several state-of-the-art decision procedures. We show that the behavior of MathSAT is often superior to its competitors, both on LAL and in the subclass of difference logic.\"",
        "Document: \"Optimization in SMT with LA(Q) cost functions. In the contexts of automated reasoning and formal verification, important decision problems are effectively encoded into Satisfiability Modulo Theories (SMT). In the last decade efficient SMT solvers have been developed for several theories of practical interest (e.g., linear arithmetic, arrays, bit-vectors). Surprisingly, very little work has been done to extend SMT to deal with optimization problems; in particular, we are not aware of any work on SMT solvers able to produce solutions which minimize cost functions over arithmetical variables. This is unfortunate, since some problems of interest require this functionality. In this paper we start filling this gap. We present and discuss two general procedures for leveraging SMT to handle the minimization of ${\\mathcal LA}$ (\u211a) cost functions, combining SMT with standard minimization techniques. We have implemented the procedures within the MathSAT SMT solver. Due to the absence of competitors in AR and SMT domains, we have experimentally evaluated our implementation against state-of-the-art tools for the domain of linear generalized disjunctive programming (LGDP), which is closest in spirit to our domain, on sets of problems which have been previously proposed as benchmarks for the latter tools. The results show that our tool is very competitive with, and often outperforms, these tools on these problems, clearly demonstrating the potential of the approach.\"",
        "Document: \"Optimathsat: A Tool For Optimization Modulo Theories. Many SMT problems of interest may require the capability of finding models that are optimal wrt. some objective functions. These problems are grouped under the umbrella term of Optimization Modulo Theories - OMT. In this paper we present OPTIMATHSAT, an OMT tool extending the MATHSAT5 SMT solver. OPTIMATHSAT allows for solving a list of optimization problems on SMT formulas with linear objective functions - on the Boolean, the rational and the integer domains, and on their combination thereof- including MaxSMT. Multiple objective functions can be combined together and handled either independently, or lexicographically, or in a min-max/max-min fashion.OPTIMATHSAT ships with an extended SMT-LIBV2 input syntax and C API bindings, and it preserves the incremental attitude of its underlying SMT solver.\"",
        "1 is \"Jena: a semantic Web toolkit\", 2 is \"Verifiying Safety Properties of a Power PC Microprocessor Using Symbolic Model Checking without BDDs\"",
        "Given above information, for an author who has written the paper with the title \"Integrating BDD-Based and SAT-Based Symbolic Model Checking\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008828": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A Survey of Trust and Reputation Management Systems in Wireless Communications.':",
        "Document: \"Equalization for 4BOK DS-UWB Systems. Direct-sequence spreading ultra-wideband (DS-UWB) is a strong contender for the standardization of the physical layer of wireless personal area networks (WPANs) by the IEEE 802.15.3a committee. Since long delay spreads in UWB channels cause significant intersymbol interference, equalization is required at the receiver of DS-UWB systems. In this paper, we investigate equalization for DS-UWB with 4-ary bi-orthogonal keying (4BOK), which is one of the two proposed modulation formats. We first derive the corresponding matched-filter bound (MFB), which is the theoretical performance limit. Considering a new equivalent multiple-input multiple-output (MIMO) description of 4BOK DS-UWB, we then devise linear and nonlinear MIMO equalizers. Furthermore, we propose the application of widely linear (WL) processing to these equalizers. Simulation and semi-analytical results show that the proposed MIMO equalizers allow for power-efficient 4BOK DS-UWB transmission close to the theoretical limits with moderate computational complexity.\"",
        "Document: \"An agent-based game for the predictive diagnosis of parkinson's disease. Existing Parkinson's Disease (PD) diagnosis relies heavily on doctors' observations combined with neurological exams. Such a technique is often inconvenient, infrequent, and subjective, which leads to a high misdiagnosis rate. As several cardinal symptoms of PD require long term observations, a technology platform which allows potential PD patients to exhibit related behaviors in a natural setting over long period of time is needed. In this paper, we describe an agent-based game for the predictive diagnosis of PD. Agents in this tablet based game provide companionship, encouragement, and analysis capabilities to help retain users' interest, and analyze their risk of developing PD symptoms based on their in-game behavior. The game has been launched together with a world renowned PD research centre for clinical trial. It can potentially provide a new dimension of longitudinal interactive behavior data to assist early and more accurate PD diagnosis in the future.\"",
        "Document: \"Credibility: How Agents Can Handle Unfair Third-Party Testimonies in Computational Trust Models. Usually, agents within multiagent systems represent different stakeholders that have their own distinct and sometimes conflicting interests and objectives. They would behave in such a way so as to achieve their own objectives, even at the cost of others. Therefore, there are risks in interacting with other agents. A number of computational trust models have been proposed to manage such risk. However, the performance of most computational trust models that rely on third-party recommendations as part of the mechanism to derive trust is easily deteriorated by the presence of unfair testimonies. There have been several attempts to combat the influence of unfair testimonies. Nevertheless, they are either not readily applicable since they require additional information which is not available in realistic settings, or ad hoc as they are tightly coupled with specific trust models. Against this background, a general credibility model is proposed in this paper. Empirical studies have shown that the proposed credibility model is more effective than related work in mitigating the adverse influence of unfair testimonies.\"",
        "Document: \"A Particle Swarm Optimization Algorithm for Multiuser Scheduling in HSDPA. This paper briefs the problem of optimal multiuser scheduling in HSDPA. The modulation and coding schemes (MCSs), numbers of multicodes and power levels for all users are jointly optimized at each scheduling period, given that only limited Channel Quality Indicator (CQI) information, as specified in the HSDPA standard [1], is fed back to the BS. An integer programming model is proposed in order to provide a globally optimal solution to the multiuser scheduling problem. Due to the complexity of the globally optimal method, a swarm intelligence approach, namely particle swarm optimization (PSO), is subsequently proposed. The experimentations suggest that it potentially provides a near-optimum performance with significantly reduced complexity.\"",
        "1 is \"Downlink scheduling for multiclass traffic in LTE\", 2 is \"Research on the negotiation mechanism of multi-agent system based on game theory\"",
        "Given above information, for an author who has written the paper with the title \"A Survey of Trust and Reputation Management Systems in Wireless Communications.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008956": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Schedulability-driven frame packing for multi-cluster distributed embedded systems':",
        "Document: \"Schedulability analysis and optimisation for the synthesis of multi-cluster distributed embedded systems. An approach to schedulability analysis for the synthesis of multi-cluster distributed embedded systems consisting of time-triggered and event-triggered clusters, interconnected via gateways, is presented. A buffer size and worst case queuing delay analysis for the gateways, responsible for routing inter-cluster traffic, is also proposed. Optimisation heuristics for the priority assignment and synthesis of bus access parameters aimed at producing a schedulable system with minimal buffer needs have been proposed. Extensive experiments and a real-life example show the efficiency of the approaches\"",
        "Document: \"Task Mapping and Partition Allocation for Mixed-Criticality Real-Time Systems. In this paper we address the mapping of mixed-criticality hard real-time applications on distributed embedded architectures. We assume that the architecture provides both spatial and temporal partitioning, thus enforcing enough separation between applications. With temporal partitioning, each application runs in a separate partition, and each partition is allocated several time slots on the processors where the application is mapped. The sequence of time slots for all the applications on a processor are grouped within a Major Frame, which is repeated periodically. We assume that the applications are scheduled using static-cyclic scheduling. We are interested to determine the task mapping to processors, and the sequence and size of the time slots within the Major Frame on each processor, such that the applications are schedulable. We have proposed a Tabu Search-based approach to solve this optimization problem. The proposed algorithm has been evaluated using several synthetic and real-life benchmarks.\"",
        "Document: \"Timing analysis of the FlexRay communication protocol. FlexRay is a communication protocol heavily promoted on the market by a large group of car manufacturers and automotive electronics suppliers. However, before it can be successfully used for safety-critical applications that require predictability, timing analysis techniques are necessary for providing bounds for the message communication times. In this paper, we propose techniques for determining the timing properties of messages transmitted in both the static and the dynamic segments of a FlexRay communication cycle. The analysis techniques for messages are integrated in the context of a holistic schedulability analysis that computes the worst-case response times of all the tasks and messages in the system. We have evaluated the proposed analysis techniques using extensive experiments. We also present and evaluate three optimisation algorithms that can be used to improve the schedulability of a system that uses FlexRay.\"",
        "Document: \"Schedulability analysis for systems with data and control dependencies. Presents an approach to schedulability analysis for hard real-time systems with control and data dependencies. We consider distributed architectures consisting of multiple programmable processors, and the scheduling policy is based on a static priority pre-emptive strategy. Our model of the system captures both data and control dependencies, and the schedulability approach is able to reduce the pessimism of the analysis by using the knowledge about control and data dependencies. Extensive experiments as well as a real-life example demonstrate the efficiency of our approach\"",
        "1 is \"Cross-Contamination Avoidance for Droplet Routing in Digital Microfluidic Biochips\", 2 is \"Analyzing refinements of state based specifications: the case of TB nets\"",
        "Given above information, for an author who has written the paper with the title \"Schedulability-driven frame packing for multi-cluster distributed embedded systems\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008975": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Accelerating Face Detection on Programmable SoC Using C-Based Synthesis.':",
        "Document: \"High-Level Synthesis for FPGAs: From Prototyping to Deployment. Escalating system-on-chip design complexity is pushing the design community to raise the level of abstraction beyond register transfer level. Despite the unsuccessful adoptions of early generations of commercial high-level synthesis (HLS) systems, we believe that the tipping point for transitioning to HLS msystem-on-chip design complexityethodology is happening now, especially for field-programmable gate array (FPGA) designs. The latest generation of HLS tools has made significant progress in providing wide language coverage and robust compilation technology, platform-based modeling, advancement in core HLS algorithms, and a domain-specific approach. In this paper, we use AutoESL's AutoPilot HLS tool coupled with domain-specific system-level implementation platforms developed by Xilinx as an example to demonstrate the effectiveness of state-of-art C-to-FPGA synthesis solutions targeting multiple application domains. Complex industrial designs targeting Xilinx FPGAs are also presented as case studies, including comparison of HLS solutions versus optimized manual designs. In particular, the experiment on a sphere decoder shows that the HLS solution can achieve an 11-31% reduction in FPGA resource usage with improved design productivity compared to hand-coded design.\"",
        "Document: \"Area-efficient pipelining for FPGA-targeted high-level synthesis. Traditional techniques for pipeline scheduling in high-level synthesis for FPGAs assume an additive delay model where each operation incurs a pre-characterized delay. While a good approximation for some operation types, this fails to consider technology mapping, where a group of logic operations can be mapped to a single look-up table (LUT) and together incur one LUT worth of delay. We propose an exact formulation of the throughput-constrained, mapping-aware pipeline scheduling problem for FPGA-targeted high-level synthesis with area minimization being a primary objective. By taking this cross-layered approach, our technique is able to mitigate the pessimism inherent in static delay estimates and reduce the usage of LUTs and pipeline registers. Experimental results using our method demonstrate improved resource utilization for a number of logic-intensive, real-life benchmarks compared to a state-of-the-art commercial HLS tool for Xilinx FPGAs.\"",
        "Document: \"Gradual Relaxation Techniques with Applications to Behavioral Synthesis. Heuristics are widely used for solving computational intractablesynthesis problems. However, until now, there has been limitedeffort to systematically develop heuristics that can be applied to avariety of synthesis tasks. We focus on development of generaloptimization principles so that they can be applied to a wide rangeof synthesis problems. In particular, we propose a new way torealize the most constraining principle where at each step wegradually relax the constraints on the most constrained elementsof the solution. This basic optimization mechanism is augmentedwith several new heuristic principles: minimal freedom reduction,negative thinking, calibration, simultaneous step consideration,and probabilistic modeling.We have successfully applied these optimization principles to anumber of common behavioral synthesis tasks. Specifically, wedemonstrate a systematic way to develop optimization algorithmsfor maximum independent set, time-constrained scheduling, andsoft real-time system scheduling. The effectiveness of theapproach and algorithms is validated on extensive real-lifebenchmarks.\"",
        "Document: \"PRIMAL: Power Inference using Machine Learning. This paper introduces PRIMAL, a novel learning-based framework that enables fast and accurate power estimation for ASIC designs. PRIMAL trains machine learning (ML) models with design verification testbenches for characterizing the power of reusable circuit building blocks. The trained models can then be used to generate detailed power profiles of the same blocks under different workloads. We evaluate the performance of several established ML models on this task, including ridge regression, gradient tree boosting, multi-layer perceptron, and convolutional neural network (CNN). For average power estimation, ML-based techniques can achieve an average error of less than 1% across a diverse set of realistic benchmarks, outperforming a commercial RTL power estimation tool in both accuracy and speed (15x faster). For cycle-by-cycle power estimation, PRIMAL is on average 50x faster than a commercial gate-level power analysis tool, with an average error less than 5%. In particular, our CNN-based method achieves a 35x speed-up and an error of 5.2% for cycle-by-cycle power estimation of a RISC-V processor core. Furthermore, our case study on a NoC router shows that PRIMAL can achieve a small estimation error of 4.5% using cycle-approximate traces from SystemC simulation.\n\n\"",
        "1 is \"FINN: A Framework for Fast, Scalable Binarized Neural Network Inference.\", 2 is \"Defect tolerance in VLSI circuits: techniques and yield analysis\"",
        "Given above information, for an author who has written the paper with the title \"Accelerating Face Detection on Programmable SoC Using C-Based Synthesis.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008978": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Context analysis to support development of virtual reality applications':",
        "Document: \"Stochastic modeling of temporal variability of HIV-1 population. A multivariate stochastic model for describing the dynamics of complex non-numerical ensembles, such as observed in Human Immunodeficiency Virus (HIV) genome, is developed. This model is based on principle component analyses for numberized variables. The model coefficients are presented in the terms of deterministic trends with correlated lags. The results indicate that we may use this model in short-term forecast of HIV evolution, for evaluation of HIV drug resistance and for testing and validation of diagnostic expert rules. The model also reproduces the specific shape of the bi-modal distribution for the mutations number.\"",
        "Document: \"Performance Modeling Of 2d Cellular Automata On Fpga. FPGA-based computation engines have been used as cellular automata (CA) accelerators within the scientific community for some time now. In this paper, we present a methodology to predict the performance of running such applications on a specific FPGA hardware technology before engineering the design in reality. This will help to determine the optimal values for the various parameters that control the application and the given FPGA hardware specifications. The model will be validated for a specific two-dimensional CA.\"",
        "Document: \"Simulated Annealing for N-body Systems. In this paper we discuss the mapping of the physical problem of 2D crystallization with spherical boundary conditions onto a Simulated Annealing model, and the mapping of this model onto a parallel computer. We discuss some aspects of the finetuning of the simulation code and the overall behaviour, stability and scalability, of our parallel implementation.\"",
        "Document: \"GEOPROVE: Geometric Probes for Virtual Environments.  . We present a software architecture that can be used to instrumentinteractive virtual environments with virtual probes to obtainquantitative information from geometric presentations. This architectureprovides tools by which measurements can be obtained from multiple levelsof data presentations, ranging from graphically displayed geometryto the underlying raw data sets.1 IntroductionIn many scientific computing problems, the level of complexity in the generateddata is too vast to ... \"",
        "1 is \"Evaluating collaborative filtering recommender systems\", 2 is \"Performance Evaluation on Grids: Directions, Issues and Open Problems\"",
        "Given above information, for an author who has written the paper with the title \"Context analysis to support development of virtual reality applications\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008985": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A SKOS-based framework for Subject Ontologies to improve learning experiences.':",
        "Document: \"A SKOS-based framework for Subject Ontologies to improve learning experiences. Subject Ontologies represent conceptualizations of disciplinary domains in which concepts symbolize topics that are relevant for the considered domain and are associated each other by means of specific relations. Usually, these kind of lightweight ontologies are adopted in knowledge-based educational environments to enable semantic organization and search of resources and, in other cases, to support personalization and adaptation features for learning and teaching experiences. For this reason, applying effective management methodologies for Subject Ontologies is a crucial aspect in engineering the environments. In particular, this paper proposes an approach to use SKOS (a Semantic Web-based vocabulary providing a standard way to represent knowledge organization systems) for modelling subject ontologies. Moreover, the paper underlines the main benefits of SKOS. It focuses on alternative strategies for storing and accessing ontologies in order to support the knowledge sharing, knowledge reusing, planning, assessment, customization and adaptation processes related to learning scenarios. The results of an early experimentation allowed the authors defining a framework able to support, from both methodological and technological viewpoints, the use of Subject Ontologies in the context of a Semantic Web-based Educational System. The defined framework has high performances in terms of response and this may really improve the user experience.\"",
        "Document: \"Towards Perception-Oriented Situation Awareness Systems. This paper proposes a new approach for identifying situations from sensor data by using a perception-based mechanism that has been borrowed from humans: sensation, perception and cognition. The proposed approach is based on two phases: low-level perception and high-level perception. The first one is realized by means of semantic technologies and allows to generate more abstract information from raw sensor data by also considering knowledge about the environment. The second one is realized by means of Fuzzy Formal Concept Analysis and allows to organize and classify abstract information, coming from the first phase, by generating a knowledge representation structure, namely lattice, that can be traversed to obtain information about occurring situation and augment human perception. The work proposes also a sample scenario executed in the context of an early experimentation.\"",
        "Document: \"Distributed online Temporal Fuzzy Concept Analysis for stream processing in smart cities. Nowadays, one of the main challenges in the smart cities is mining high-level semantics from low-level activities. In this context, real-time data streams are continuously produced and analysed by efficient and effective algorithms, which are able to handle complexities related to big data, in order to enable the core functions of Decision Support Systems in the smart city. These algorithms should receive input data coming from different city domains (or pillars) and process, aggregate and reason over them in a way that it is possible to find hidden correlations among different and heterogeneous elements (e.g., traffic, weather, cultural events) along space and time dimensions. This paper proposes the online implementation and deployment of Temporal Fuzzy Concept Analysis on a distributed real-time computation system, based on Apache Storm, to face with big data stream analysis in the smart city context. Such online distributed algorithm is able to incrementally generate the timed fuzzy lattice that organizes the knowledge on several and cross-domain aspects of the city. Temporal patterns, of how situations evolve in the city, can be elicited by both exploring the lattice and observing its growth in order to obtain actionable knowledge to support smart city decision-making processes.\"",
        "Document: \"An approach based on semantic stream reasoning to support decision processes in smart cities. \u2022An approach for processing data streams in Smart City supporting decision making.\u2022A semantic-driven architecture implementing the overall approach.\u2022Stream reasoning techniques to process semantically-enriched data streams.\u2022Incremental reasoning techniques for supporting cross-domain decision making.\u2022Performance evaluation by means of a case study related to the city of Aarhus.\"",
        "1 is \"Projection-based spatially adaptive reconstruction of block-transform compressed images.\", 2 is \"Synchronization of a line of identical processors at a given time\"",
        "Given above information, for an author who has written the paper with the title \"A SKOS-based framework for Subject Ontologies to improve learning experiences.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009001": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A detection system for human abnormal behavior':",
        "Document: \"H\u221e filtering for discrete-time piecewise homogeneous Markov jump Lur'e systems with application to economic systems. This paper addresses the robust H\u221e filtering problem for a class of Markov jump Lur'e systems with time-varying transition probabilities in discrete-time domain. The time-varying character of transition probabilities is considered to be finite piecewise homogeneous. A full-order filter is designed such that the resulting closed-loop systems are stochastically stable and have a guaranteed H\u221e performance index in terms of linear matrix inequalities. The effectiveness and potential of the developed results are verified through an example about a class of economic systems.\"",
        "Document: \"Dimensionality reduction of data sequences for human activity recognition. Although current human activity recognition can achieve high accuracy rates, data sequences with high-dimensionality are required for a reliable decision to recognize the entire activity. Traditional dimensionality reduction methods do not exploit the local geometry of classification information. In this paper, we introduce the framework of manifold elastic net that encodes the local geometry to find an aligned coordinate system for data representation. The introduced method is efficient because classification error minimization criterion is utilized to directly link the classification error with the selected subspace. In the experimental section, a dataset on human activity recognition is studied from wearable, object, and ambient sensors.\"",
        "Document: \"Towards culturally aware robot navigation. When we look towards the world of humans and robots harmonically working together in a social environment, the robots should behave in cultural norms. A culturally aware robot navigation is highly expected to enable mobile service robots to politely and respectfully navigate among humans in human-robot shared workspaces. In this paper, we present a foundation of culturally aware robot navigation for mobile service robots in a social environment. The culturally aware robot navigation system is developed by integrating extended personal spaces representing individual states and social interaction spaces representing human-robot interactions and human groups. The culturally aware robot navigation plays the role of human-aware decision making upon the conventional robot navigation system to ensure that a mobile service robot is capable of detecting and identifying social contexts and situations to culturally navigate in human appearances. Simulation results illustrate our methodological approach.\"",
        "Document: \"Improving 3D indoor mapping with motion data. Using both RGB and depth information obtained from low-cost RGB-D cameras, 3D models of indoor environment can be reconstructed, which provide extensive knowledge for mobile robots to accomplish tasks such as localization, mapping, interaction with human, etc. Due to the limited views of RGB-D cameras, additional information about the camera pose is needed. In this paper, an enhanced 3D mapping algorithm is proposed to overcome the limitations. The motion of the RGB-D camera is estimated by a motion capture system after a calibration process. Based on the estimated pose, a multi-level ICP (Iterative Closest Point) algorithm is used to improve the alignment. The result shows that the 3D map can be generated in real-time. We compare our results with other approaches to show the robustness of our algorithm.\"",
        "1 is \"Kinematics Modeling Of A Wheel-Based Pole Climbing Robot (Ut-Pcr)\", 2 is \"Maritime abnormality detection using Gaussian processes.\"",
        "Given above information, for an author who has written the paper with the title \"A detection system for human abnormal behavior\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009060": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'TERAFLUX: Harnessing dataflow in next generation teradevices.':",
        "Document: \"Flexible Page-level Memory Access Monitoring Based on Virtualization Hardware. Page protection is often used to achieve memory access monitoring in many applications, dealing with program-analysis, checkpoint-based failure recovery, and garbage collection in managed runtime systems. Typically, low overhead access monitoring is limited by the relatively large page-level granularity of memory management unit hardware support for virtual memory protection. In this paper, we improve upon traditional page-level mechanisms by additionally using hardware support for virtualization in order to achieve fine and flexible granularities that can be smaller than a page. We first introduce a memory allocator based on page protection that can achieve fine-grained monitoring. Second, we explain how virtualization hardware support can be used to achieve dynamic adjustment of the monitoring granularity. In all, we propose a process-level virtual machine to achieve dynamic and fine-grained monitoring. Any application can run on our process-level virtual machine without modification. Experimental results for an incremental checkpoint tool provide a use-case to demonstrate our work. Comparing with traditional page-based checkpoint, our work can effectively reduce the amount of checkpoint data and improve performance.\"",
        "Document: \"Online non-stationary boosting. Oza's Online Boosting algorithm provides a version of AdaBoost which can be trained in an online way for stationary problems. One perspective is that this enables the power of the boosting framework to be applied to datasets which are too large to fit into memory. The online boosting algorithm assumes the data distribution to be independent and identically distributed (i.i.d.) and therefore has no provision for concept drift. We present an algorithm called Online Non-Stationary Boosting (ONSBoost) that, like Online Boosting, uses a static ensemble size without generating new members each time new examples are presented, and also adapts to a changing data distribution. We evaluate the new algorithm against Online Boosting, using the STAGGER dataset and three challenging datasets derived from a learning problem inside a parallelising virtual machine. We find that the new algorithm provides equivalent performance on the STAGGER dataset and an improvement of up to 3% on the parallelisation datasets.\"",
        "Document: \"SpiNNaker: Enhanced multicast routing. \u2022We implemented 4 multicast routing algorithms for SpiNNaker.\u2022The two most complex algorithms allow for different implementation details.\u2022We explored the effects of these implementation details.\u2022The exploration allowed for more effective implementation of the algorithms.\u2022The enhanced implementations are better suited to be used in production.\"",
        "Document: \"Improving performance by reducing aborts in hardware transactional memory. The optimistic nature of Transactional Memory (TM) systems can lead to the concurrent execution of transactions that are later found to conflict. Conflicts degrade scalability, and may lead to aborts that increase wasted work, and degrade performance. A promising approach to reducing conflicts at runtime is dynamically, and transparently, reordering the execution of transactions upon discovery of conflicts. This approach has been explored in Software TMs (STMs), but not in Hardware TMs (HTMs). Furthermore, STM implementations of this approach cannot be ported to HTMs easily. This paper investigates the feasibility of such reordering in HTMs, and presents two designs that are scalable, independent of the on-chip interconnect, require only minor modifications to each core, and add no execution overhead if no conflicts occur. The evaluation takes LogTM-SE as a base line and considers benchmarks with different levels of contention (transactional conflicts). The results show that the preferred design increases HTM performance by up to 17% when contention is low, 57% when contention is high, and never degrades performance. Finally, the designs are orthogonal to LogTM-SE; they require no modification to cache structures, and continue to support transaction virtualization, open and closed unbounded nesting, paging, thread suspension, and thread migration.\"",
        "1 is \"An approach to resource-aware co-scheduling for CMPs\", 2 is \"Clustering Data Retrieved from Java Source Code to Support Software Maintenance: A Case Study\"",
        "Given above information, for an author who has written the paper with the title \"TERAFLUX: Harnessing dataflow in next generation teradevices.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009095": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Compressive sensing-based wind speed estimation for low-altitude wind-shear with airborne phased array radar':",
        "Document: \"An Overview of In-memory Processing with Emerging Non-volatile Memory for Data-intensive Applications. The conventional von Neumann architecture has been revealed as a major performance and energy bottleneck for rising data-intensive applications. The decade-old idea of leveraging in-memory processing to eliminate substantial data movements has returned and led extensive research activities. The effectiveness of in-memory processing heavily relies on memory scalability, which cannot be satisfied by traditional memory technologies. Emerging non-volatile memories (eNVMs) that pose appealing qualities such as excellent scaling and low energy consumption, on the other hand, have been heavily investigated and explored for realizing in-memory processing architecture. In this paper, we summarize the recent research progress in eNVM-based in-memory processing from various aspects, including the adopted memory technologies, locations of the in-memory processing in the system, supported arithmetics, as well as applied applications.\n\n\"",
        "Document: \"Read Performance: The Newest Barrier in Scaled STT-RAM. Spin-torque transfer RAM (STT-RAM), a promising alternative to static RAM (SRAM) for reducing leakage power consumption, has been widely studied to mitigate the impact of its asymmetrically long write latency. However, physical effects of technology scaling down to 45 nm and below, in particular, process variation, introduce the previously unreported and alarming trends in read performance and reliability due to reduced sensing margins and increasing error rates. In this brief, we study the scaling trends of STT-RAM from 65 down to 22 nm as they pertain to read performance, including a 50% increase in sensing versus peripheral circuit delay ratio and a more than 80% increase in uncorrectable read error rates. Through differential sensing, we show how 22 nm can return to sense delay ratio levels at 65 nm and uncorrectable read errors can be reduced by an order of magnitude. Through a case study of a multilevel STT-RAM cache, we show how a reconfigurable cache cell can create an extreme access mode (X-mode) based on differential sensing improve to outperform the state-of-the-art STT-RAM caching techniques in both raw performance and performance per watt by more than 10% while still reducing energy consumption over SRAM caches by more than 1/3.\"",
        "Document: \"Compressive sensing-based wind speed estimation for low-altitude wind-shear with airborne phased array radar. An important issue in low-altitude wind-shear detection is to estimate the wind speed of wind field. In this paper, a novel method for wind speed estimation with airborne phased array radar is proposed by combining space time adaptive processing and compressive sensing. The proposed method is able to achieve accurate wind speed estimate in the condition of limited number of sampling pulses, as demonstrated by numerical examples.\"",
        "Document: \"An efficient STT-RAM-based register file in GPU architectures. Modern GPGPUs employ a large register file (RF) to efficiently process heavily parallel threads in single instruction multiple thread (SIMT) fashion. The up-scaling of RF capacity, however, is greatly constrained by large cell area and high leakage power consumption of SRAM implementation. In this work, we propose a novel GPU RF design based on the emerging multi-level cell (MLC) spin-transfer torque RAM (STT-RAM) technology. Compared to SRAM, MLC STT-RAM (or MLC-STT) has much smaller cell area and almost zero standby power due to its non-volatility. Moreover, by leveraging the asymmetric performance of the soft and the hard bits of a MLC-STT cell, we propose a remapping strategy to perform a flexible tradeoff between the access time and the capacity of the RF based on run-time access patterns. A novel rescheduling scheme is also developed to minimize the waiting time of the issued warps to access register banks. Experimental results over ISPASS2009 and CUDA benchmarks show that on average, our proposed MLC-STT RF can achieve 3.28% performance improvement, 9.48% energy reduction, and 38.9% energy efficiency enhancement compared to conventional SRAM-based design.\"",
        "1 is \"Inverse Function Analysis Method for Fringe Pattern Profilometry\", 2 is \"LDPC-in-SSD: making advanced error correction codes work effectively in solid state drives\"",
        "Given above information, for an author who has written the paper with the title \"Compressive sensing-based wind speed estimation for low-altitude wind-shear with airborne phased array radar\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009117": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Outage probability analysis for superposition coded symmetric relaying':",
        "Document: \"Probabilistic-constrained robust secure transmission for energy harvesting over MISO channels. In this paper, we consider a system supporting simultaneous wireless information and power transfer (SWIPT), where the transmitter delivers private message to a destination receiver (DR) and powers to multiple energy receivers (ERs) with multiple single-antenna external eavesdroppers (Eves). We study secure robust beamformer and power splitting (PS) design under imperfect channel state information (CSI). The artificial noise (AN) scheme is further utilized at the transmitter to provide strong wireless security. We aim at maximizing the energy harvested by ERs subject to the transmission power constraint, a range of outage constraints concerning the signal-to-interference-plus-noise ratio (SINR) recorded at the DR and the Eves, as well as concerning the energy harvested at the DR. The energy harvesting maximization (EHM) problem is challenging to directly solve, we resort to Bernstein-type inequality restriction technique to reformulate the original problem as a tractable approximated version. Numerical results show that our robust beamforming scheme outperforms the beamforming scheme relying on the worst-case design philosophy.\"",
        "Document: \"Secure Communications for Dual-Polarized MIMO Systems. To enhance secure communications, we deploy the dual-polarized antenna arrays at communication nodes of the multi-input multioutput (MIMO) system, where the base station communicates with multiple legitimate users in the presence of an eavesdropper. We also adopt the dual-structured precoding in which a preprocessing matrix based on the polarized array spatial correlation and a linear precoding ba...\"",
        "Document: \"Performance analysis for uplink massive MIMO systems with a large and random number of UEs. In this paper, we analyze the ergodic achievable rate of an uplink massive multi-user multipleinput multiple-output (MIMO) system with a large and Poisson distributed number of users. In the considered scenario, multiple user equipments (UEs) transmit their information to a base station equipped with a very large number of antennas. New asymptotic expressions for the ergodic achievable rate for large and deterministic number of users are derived for both maximum ratio combining (MRC) detector and zero-forcing (ZF) detector, as well as the ergodic achievable rate for large and random number of users. Simulation results assess the accuracy of these analytical expressions. It is shown that compared with the MRC detector, ZF detector can achieve much higher spectrum efficiency. Also, the results provide a meaningful fact that with different settings the randomness of the number of users will result in different extents of impact on the performance of massive MIMO.\"",
        "Document: \"Terminal-based Dynamic Clustering Algorithm in Multi-Cell Cellular System. A terminal-based dynamic clustering algorithm is proposed in a multi-cell scenario, where the user could select the cooperative BSs from the predetermined static base stations (BSs) set based on dynamic channel condition. First, the user transmission rate is derived based on linear precoding and per-cell feedback scheme. Then, the dynamic clustering algorithm can be implemented based on two criteria: (a) the transmission rate should meet the user requirement for quality of service (QoS); (b) the rate increment exceeds the predetermined constant threshold. By adopting random vector quantization (RVQ), the optimized number of cooperative BSs and the corresponding channel conditions are presented respectively. Numerical results are given and show that the performance of the proposed method can improve the system resources utilization effectively.\"",
        "1 is \"Stochastic Geometry Modeling of Cellular Networks: Analysis, Simulation and Experimental Validation\", 2 is \"Spectral efficiency of multi-cell multi-user DAS with pilot contamination.\"",
        "Given above information, for an author who has written the paper with the title \"Outage probability analysis for superposition coded symmetric relaying\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009123": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A temporal blackboard for a multi-agent environment':",
        "Document: \"An Ada Binding to the IEEE 1003.1q (POSIX Tracing) Standard. Run-time monitoring tools are specially helpful in developing real-time applications. The recently approved standard POSIX 1003.1q defines a common application interface (API) for trace management. No Ada interface currently exists for the POSIX Tracing services. In this paper an Ada binding specification to the amendment 1003.1q (POSIX Tracing) is proposed, in order to be able to use POSIX Tracing services in Ada applications. This specification has been implemented in the real-time kernel MaRTE OS.\"",
        "Document: \"\u03bcDDS: A Middleware for Real-time Wireless Embedded Systems. Abstract A Real-Time Wireless Distributed Embedded System (RTWDES) is formed by a large quantity of small devices with certain computing power, wireless communication and sensing/actuators capabilities. These types of networks have become popular as they have been developed for applications which can carry out a vast quantity of tasks, including home and building monitoring, object tracking, precision agriculture, military applications, disaster recovery, industry applications, among others. For this type of applications a middleware is used in software systems to bridge the gap between the application and the underlying operating system and networks. As a result, a middleware system can facilitate the development of applications and is designed to provide common services to the applications. The development of a middleware for sensor networks presents several challenges due to the limited computational resources and energy of the different nodes. This work is related with the design, implementation and test of a micro middleware for RTWDES; the proposal incorporates characteristics of a message oriented middleware thus allowing the applications to communicate by employing the publish/subscribe model. Experimental evaluation shows that the proposed middleware provides a stable and timely service to support different Quality of Service (QoS) levels.\"",
        "Document: \"Implementation of a constant-time dynamic storage allocator. This paper describes the design criteria and implementation details of a dynamic storage allocator for real-time systems. The main requirements that have to be considered when designing a new allocator are concerned with temporal and spatial constraints. The proposed algorithm, called TLSF (two-level segregated fit), has an asymptotic constant cost, O(1), maintaining a fast response time (less than 200 processor instructions on a x86 processor) and a low level of memory usage (low fragmentation). TLSF uses two levels of segregated lists to arrange free memory blocks and an incomplete search policy. This policy is implemented with word-size bitmaps and logical processor instructions. Therefore, TLSF can be categorized as a good-fit allocator. The incomplete search policy is shown also to be a good policy in terms of fragmentation. The fragmentation caused by TLSF is slightly smaller (better) than that caused by best fit (which is one of the best allocators regarding memory fragmentation). In order to evaluate the proposed allocator, three analyses are presented in this paper. The first one is based on worst-case scenarios. The second one provides a detailed consideration of the execution cost of the internal operations of the allocator and its fragmentation. The third analysis is a comparison with other well-known allocators from the temporal (number of cycles and processor instructions) and spatial (fragmentation) points of view. In order to compare them, a task model has been presented. Copyright \u00a9 2007 John Wiley & Sons, Ltd.\"",
        "Document: \"Real time planning in N-dim state space. This paper presents the actual work in real-time planning as search [1] [2]. Based in this work we tried to solve the path planning in numerical state space. We found that precision, performance, and time were very linked. In real-time problem solving, the agent can fall in traps made of forbidden zones and to go out it, have to spend too much computing time. To solve this problem we propose a multilayer inference based in subgoals computation. An architecture based in two agents, one for low level task with the maximum precision and other for subgoals computation is proposed here.\"",
        "1 is \"A Multi-Parameter Complexity Analysis of Cost-Optimal and Net-Benefit Planning.\", 2 is \"Middleware to support sensor network applications\"",
        "Given above information, for an author who has written the paper with the title \"A temporal blackboard for a multi-agent environment\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009131": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Open information gateway for disaster management':",
        "Document: \"Estimating link capacity in high speed networks. Knowledge of bottleneck capacity of an Internet path is critical for efficient network design, management, and usage. With emerging high speed Internet links, most traditional estimation techniques are limited in providing fast and accurate capacity estimations. In this paper, we propose a new technique, called PBProbe, to estimate high speed links. PBProbe is based on CapProbe; however, instead of solely relying on packet pairs, PBProbe employs a \u201cpacket bulk\u201d technique and adapts the bulk length in order to overcome the well known problem with packet pair based approaches, namely the lack of accurate timer resolution. As a result, PBProbe not only preserves the simplicity and speed of CapProbe, but it also correctly estimates link capacities within a much larger range. Using analysis, we evaluate PBProbe with various bulk lengths and network configurations. We then perform emulation and Internet experiments to verify the accuracy and speed of PBProbe on high speed links. The results show that PBProbe is consistently fast and accurate in the great majority of test cases.\"",
        "Document: \"MetroNet: a disruption-tolerant approach for mobile downloads on metro systems. AbstractIn this study, we propose a disruption-tolerant network based system called MetroNet, which exploits the precise schedules of metro systems to pre-fetch data and facilitate mobile Internet downloads, even if an end-to-end path between the source i.e. content sources and the destination i.e. users does not exist contemporaneously. Using a comprehensive set of simulations, as well as real-world scenarios of the Taipei and Atlanta metro systems, we demonstrate that MetroNet can handle the intermittent Internet connectivity caused by mobility and therefore provide a better mobile download service. The simulation results demonstrate that the proposed call admission control algorithm utilises the network resource effectively and provides better quality of service for mobile users than the first-come-first-serve and least-first algorithms. Copyright \u00a9 2013 John Wiley & Sons, Ltd.\"",
        "Document: \"XD: A Cross-Layer Designed Data Collection Mechanism for Mission-Critical WSNs in Urban Buildings. As the R&D experience accumulates, there is a rising interest of wireless sensor network (WSN) deployment in the urban environment. For mission critical applications such as healthcare and workplace safety, in particular, it is essential that the data dissemination mechanisms satisfy two important quality of service (QoS) requirements: (1) high delivery rate and (2) low transmission delay. Proposed in this work is a cross-layer designed data dissemination mechanism, referred to as Cross-Layer Diffusion (XD), in which notions in the path discovery (routing) component are exploited by data forwarding (MAC) component to improve the delivery rate and transmission delay. Using traces collected from a prototype WSN deployed in urban environment, we compare XD to the state-of-the-art mechanisms and find that XD is not only more efficient but also more practical.\"",
        "Document: \"An analytical study of GWAP-based geospatial tagging systems. Geospatial tagging (geotagging) is an emerging and very promising application that can help users find a wide variety of location-specific information, and facilitate the development of future location-based services. Conventional geotagging systems share some limitations, such as the use of a two-phase operating model and the tendency to tag popular objects with simple contexts. To address these problems, geotagging systems based on the concept of `Games with a Purpose' (GWAP) have been developed recently. In this study, we use analysis to investigate these new systems. Based on our analysis results, we design three metrics to evaluate the system performance, and develop five task assignment algorithms for a GWAP-based system. Using a comprehensive set of simulations under both synthetic and realistic mobility scenarios, we find that the Least-Throughput-First Assignment algorithm (LTFA) is the most effective approach because it can achieve competitive system utility, while its computational complexity remains moderate. We also find that, to improve the system utility, it is better to assign as many tasks as possible in each round. However, because players may feel annoyed if too many tasks are assigned at the same time, it is recommended that multiple tasks be assigned one by one in each round in order to achieve higher system utility.\"",
        "1 is \"StarPU: a unified platform for task scheduling on heterogeneous multicore architectures\", 2 is \"Pushing Web Pages into Personal Digital Assistants: Need, Tools and Solutions\"",
        "Given above information, for an author who has written the paper with the title \"Open information gateway for disaster management\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009163": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Data-driven online variational filtering in wireless sensor networks':",
        "Document: \"Reweighted nonnegative least-mean-square algorithm. Statistical inference subject to nonnegativity constraints is a frequently occurring problem in learning problems. The nonnegative least-mean-square (NNLMS) algorithm was derived to address such problems in an online way. This algorithm builds on a fixed-point iteration strategy driven by the Karush-Kuhn-Tucker conditions. It was shown to provide low variance estimates, but it however suffers from unbalanced convergence rates of these estimates. In this paper, we address this problem by introducing a variant of the NNLMS algorithm. We provide a theoretical analysis of its behavior in terms of transient learning curve, steady-state and tracking performance. We also introduce an extension of the algorithm for online sparse system identification. Monte-Carlo simulations are conducted to illustrate the performance of the algorithm and to validate the theoretical results. HighlightsWe proposed a variant of NN-LMS algorithm with balanced weight convergence rates.Accurate performance analysis is performed for a general nonstationarity model.The sparse system identification problem can be solved via the derived algorithm.\"",
        "Document: \"Distributed image reconstruction for very large arrays in radio astronomy. Current and future radio interferometric arrays such as LOFAR and SKA are characterized by a paradox. Their large number of receptors (up to millions) allow theoretically unprecedented high imaging resolution. In the same time, the ultra massive amounts of samples makes the data transfer and computational loads (correlation and calibration) order of magnitudes too high to allow any currently existing image reconstruction algorithm to achieve, or even approach, the theoretical resolution. We investigate here decentralized and distributed image reconstruction strategies which select, transfer and process only a fraction of the total data. The loss in MSE incurred by the proposed approach is evaluated theoretically and numerically on simple test cases.\"",
        "Document: \"Regularized kernel-based Wiener filtering. Application to magnetoencephalographic signals denoising. In this paper we proceeded to take up a new approach of nonlinear Wiener filtering. This approach is based on the theory of reproducing kernel Hilbert spaces (RKHS). By means of the well-known \"kernel trick\", the arithmetic operations are carried out in the initial space. We show that the solution is given by solving a linear system which may be ill-conditioned. To find a solution for such problem, we resorted to kernel principal component analysis (KPCA) method to perform dimensionality reduction in RKHS. A new reduced-rank Wiener filter based on KPCA is thus elaborated. It is applied on magnetoencephalographic (MEG) data for cardiac artifacts extraction.\"",
        "Document: \"A simple scheme for unmixing hyperspectral data based on the geometry of the N-dimensional simplex. In this paper, we study the problem of decomposing spectra in hyperspectral data into the sum of pure spectra, or endmembers. We propose to jointly extract the endmembers and estimate the corresponding fractions, or abundances. For this purpose, we show that these abundances can be easily computed using volume of simplices, from the same information used in the classical N-Findr algorithm. This results into a simple scheme for unmixing hyperspectral data, with low computational complexity. Experimental results show the efficiency of the proposed method.\"",
        "1 is \"A new statistical approach for the automatic segmentation of continuous speech signals\", 2 is \"Sparse Distributed Learning Based on Diffusion Adaptation\"",
        "Given above information, for an author who has written the paper with the title \"Data-driven online variational filtering in wireless sensor networks\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009235": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Refining video annotation by exploiting pairwise concurrent relation':",
        "Document: \"Depth map super-resolution using stereo-vision-assisted model. In this paper, we propose a novel Stereo-Vision-Assisted (SVA) model for depth map super-resolution. Given a low-resolution depth map as input, we recover a high-resolution depth map using the registered high-resolution color stereo image pair. First, based on the mutual benefits between raw depth map and features of high resolution color image, we model the relationship with two constraint terms of local and non-local priors and sufficiently explore their complementary nature. Moreover, by considering reliable disparity pixels calculated from stereo matching algorithm, we formulate a stereo disparity regularization term to further reinforce the preservation of fine depth detail. Hence, our SVA objective function includes a non-local prior constraint, a local prior constraint and a stereo disparity prior constraint. In addition, we employ an efficient algorithm to optimize the objective function. Experimental results demonstrate that our approach can obtain superior performance in comparison with other methods.\"",
        "Document: \"Pattern Recognition Based on Stability of Discrete Time Cellular Neural Networks. In this paper, some sufficient conditions are obtained to guarantee that discrete time cellular neural networks (DTCNNs) can have some stable memory patterns. These conditions can be directly derived from the structure of the neural networks. Moreover, the method of how to estimate of the attracting domain of such stable memory patterns is also described in this paper. In addition, a new design algorithm for DTCNNs is developed based on stability theory (not based on the well-known perceptron training algorithm), and the convergence of the design algorithm can be guaranteed by some stability theorems. Finally, the simulating results demonstrate the validity and feasibility of our proposed approach.\"",
        "Document: \"A novel elliptical basis function neural networks optimized by particle swarm optimization. In this paper, a novel model of elliptical basis function neural networks (EBFNN) is proposed. Firstly, a geometry analytic algorithm is applied to construct the hyper-ellipsoid units of hidden layer of the EBFNN, i.e., an initial structure of the EBFNN, which is further pruned by the particle swarm optimization (PSO) algorithm. Finally, the experimental results demonstrated the proposed hybrid optimization algorithm for the EBFNN model is feasible and efficient, and the EBFNN is not only parsimonious but also has better generalization performance than the RBFNN.\"",
        "Document: \"Creating and simulating a realistic physiological tongue model for speech production. Simulation of the tongue has important applications in biomechanics, medical science, linguistics, and graphics. The accuracy of the geometry, intrinsic structure and dynamic simulation of tongue are crucial for these applications. In this paper, we build a 3D anatomically and biomechanically accurate tongue model. For ensuring anatomical accuracy, the tongue mesh model is constructed based on accurate medical data and an interactive muscle marking method for specifying the muscle geometry and fiber arrangement. For ensuring biomechanical accuracy, a nonlinear, quasi-incompressible, isotropic, hyperelastic constitutive model is applied for describing the tongue tissues. Particularly, tongue muscles are additionally endowed with an anisotropic constitutive model, which reflects the active and passive mechanical behavior of muscle fibers. The dynamic simulation results of tongue movements subjected to certain muscle activations are presented and validated with experimental data, indicating the suitability for visual speech synthesis.\"",
        "1 is \"Multiplicative updates for non-negative projections\", 2 is \"Theory and Use of the EM Algorithm\"",
        "Given above information, for an author who has written the paper with the title \"Refining video annotation by exploiting pairwise concurrent relation\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009241": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Guest Editorial Energy-Efficiency in Optical Networks':",
        "Document: \"Fault Localization in All-Optical Networks with User and Supervisory Lightpaths. Transparent all-optical networks introduce a challenging problem of achieving efficient and accurate full-coverage fault-localization. In this paper we present a novel and efficient monitoring approach that exploits the benefits of provisioned user lightpaths for in-band monitoring and achieves complete fault localization coverage at the minimum resource cost through the use of complementary supervisory lightpaths. We formulate the problem and present an Integer Linear Program (ILP) solution. An efficient heuristic for provisioning complementary supervisory paths and cycles is also introduced and the solution quality is compared with ILP for different sets of random user lightpaths and different parameters. Extensive numerical comparisons with several recently proposed monitoring algorithms demonstrate the advantages of the proposed approach.\"",
        "Document: \"Distributed Hierarchical Monitoring And Alarm Management In Transparent Optical Networks. Rapid fault detection and localization in transparent optical networks is crucial due to the high data rates at which they operate and optical transparency. Furthermore, large all-optical networks require distributed fault-management in order to achieve scalable, accurate, and rapid fault localization. In this work we present an efficient scheme for hierarchically distributed monitoring and fault-localization in transparent all-optical networks. We prove that the proposed scheme yields identical fault-localization capability as the centralized and flat fault-localization scheme, while allowing for distributed optimization of alarm-vector size and fault-localization. Simulation results provide supporting comparison of achievable fault localization capability and effective reduction of alarm-vector lengths and fault localization complexity for flat and hierarchically distributed monitoring approaches.\"",
        "Document: \"Optimizing job reliability via contention-free, distributed scheduling of vm checkpointing. Checkpointing a virtual machine (VM) is a proven technique to improve the reliability in modern datacenters. Inspired by the CSMA protocol in wireless congestion control, we propose a novel framework for distributed and contention-free scheduling of VM checkpointing to offer reliability as a transparent, elastic service in datacenters. In this work, we quantify the reliability in closed form by studying system stationary behaviors, and maximize the job reliability through utility optimization. We implement a proof-of-concept prototype based on our design. Evaluation results show that the proposed checkpoint scheduling can significantly reduce the performance interference from checkpointing and improve reliability by as much as one order of magnitude over contention-oblivious scheme.\"",
        "Document: \"Adaptive QoS routing in dynamic wavelength-routed optical networks. In transparent DWDM (Dense Wavelength Division Multiplexing) networks, high blocking rate of lightpaths due to unsatisfactory BERs (Bit Error Rates) can become a major traffic bottleneck. Most of the existing routing schemes have focused on reducing the call rejection rate that is caused by wavelength exhaustion on the assigned routes, without considering the inferior network BER performance that these schemes often produce. In this paper, we propose a dynamic AQoS (Adaptive Quality of Service) routing algorithm, which assigns routes based on real-time Q factor measurements collected from devices. Not only is it BER-friendly, the incorporated CLC (Constrained Least Congested) approach in route selection makes our algorithm wavelength-efficient as well. AQoS routing can work seamlessly with variable data rates and heterogeneous device characteristics. More importantly, it handles non-uniform networks and state changes in a much more flexible way than traditional schemes. Simulation results show that our routing algorithm gives the best overall performance in all tested scenarios.\"",
        "1 is \"Security vulnerabilities in DNS and DNSSEC\", 2 is \"Dynamic Heterogeneity-Aware Resource Provisioning in the Cloud\"",
        "Given above information, for an author who has written the paper with the title \"Guest Editorial Energy-Efficiency in Optical Networks\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009314": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Generating exact- and ranked partially-matched answers to questions in advertisements':",
        "Document: \"SimPaD: A word-similarity sentence-based plagiarism detection tool on Web documents. Plagiarism is a serious problem that infringes copyrighted documents/materials, which is an unethical practice and decreases the economic incentive received by their legal owners. Unfortunately, plagiarism is getting worse due to the increasing number of on-line publications and easy access on the Web, which facilitates locating and paraphrasing information. In solving this problem, we propose a novel plagiarism-detection method, called SimPaD, which (i) establishes the degree of resemblance between any two documents D1 and D2 based on their sentence-to-sentence similarity computed by using pre-defined word-correlation factors, and (ii) generates a graphical view of sentences that are similar (or the same) in D1 and D2. Experimental results verify that SimPaD is highly accurate in detecting (non-)plagiarized documents and outperforms existing plagiarism-detection approaches.\"",
        "Document: \"Personalized recommendations on books for K-12 readers. Learning to read efficiently and effectively is emphasized at the elementary and high school levels. Finding books that children/youth are interested in reading, however, is a non-trivial task due to the diversity of topics and different readability levels covered in the huge volume of books available these days. Ideally, K-12 students can turn to book recommenders which suggest books that match their interests. However, since the preferences and reading levels of these students vary from one grade to another, books suggested by existing recommenders, which ignore the literacy skills and the personal interests of their users, may be unsuitable for the targeted audience. In this paper, we present additional design issues that should be applied in developing a book recommender based on BReK12, our previously-proposed book recommender for K-12 users, to further enhance the quality of its recommendations. BReK12, which performs content and readability analysis to identify books potentially appealing to its users, is extended to incorporate (i) a multi-criteria analysis that studies its users' complex and diverse interests and (ii) an enhanced readability-detection tool that determines precisely the readability levels of books which match the literary skills of its users.\"",
        "Document: \"A query-based multi-document sentiment summarizer. Review websites, such as Epinions.com, which offer users a platform to share their opinions on diverse products and services, provide a valuable source of opinion-rich information. Browsing through archived reviews to locate different opinions on a product or service, however, is a time-consuming and tedious task, and in most cases, the large amount of available information is difficult for users to absorb. To facilitate the process of synthesizing opinions expressed in reviews on a product or service P specified in a user query/question Q, we introduce QMSS, a query-based multi-document sentiment summarizer. QMSS creates a summary for Q, which either reflects the general opinions on P or is tailored to specific facets (i.e., features) and/or sentiment of P as specified in Q. QMSS (i) identifies the facets addressed in reviews retrieved for Q, (ii) employs a sentence-based, sentiment classifier to determine the polarity of each sentence in each review, and (iii) clusters sentences in reviews according to the facets captured in the sentences, which are identified using a keyword-label extraction algorithm. This process dictates which sentences in the reviews should be included in the summary for Q. Empirical studies have verified that QMSS is highly effective in generating summaries that satisfy users' information needs and ranks on top among the state-of-the-art query-based multi-document sentiment summarizers\"",
        "Document: \"Generating exact- and ranked partially-matched answers to questions in advertisements. Taking advantage of the Web, many advertisements (ads for short) websites, which aspire to increase client's transactions and thus profits, offer searching tools which allow users to (i) post keyword queries to capture their information needs or (ii) invoke form-based interfaces to create queries by selecting search options, such as a price range, filled-in entries, check boxes, or drop-down menus. These search mechanisms, however, are inadequate, since they cannot be used to specify a natural-language query with rich syntactic and semantic content, which can only be handled by a question answering (QA) system. Furthermore, existing ads websites are incapable of evaluating arbitrary Boolean queries or retrieving partially-matched answers that might be of interest to the user whenever a user's search yields only a few or no results at all. In solving these problems, we present a QA system for ads, called CQAds, which (i) allows users to post a natural-language question Q for retrieving relevant ads, if they exist, (ii) identifies ads as answers that partially-match the requested information expressed in Q, if insufficient or no answers to Q can be retrieved, which are ordered using a similarity-ranking approach, and (iii) analyzes incomplete or ambiguous questions to perform the \"best guess\" in retrieving answers that \"best match\" the selection criteria specified in Q. CQAds is also equipped with a Boolean model to evaluate Boolean operators that are either explicitly or implicitly specified in Q, i.e., with or without Boolean operators specified by the users, respectively. CQAds is easy to use, scalable to all ads domains, and more powerful than search tools provided by existing ads websites, since its query-processing strategy retrieves relevant ads of higher quality and quantity. We have verified the accuracy of CQAds in retrieving ads on eight ads domains and compared its ranking strategy with other well-known ranking approaches.\"",
        "1 is \"The MovieLens Datasets: History and Context\", 2 is \"Query-based sampling of text databases\"",
        "Given above information, for an author who has written the paper with the title \"Generating exact- and ranked partially-matched answers to questions in advertisements\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009380": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Millimeter Wave Communications for Future Mobile Networks.':",
        "Document: \"Narrowband LTE-M System for M2M Communication. The Internet of Things will bring billions of devices that are inter-connected using cellular networks. LTE cellular systems are being deployed worldwide and will remain in place for the foreseeable future. However, LTE was designed for high data-rate broadband services. Even with M2M features being added in LTE Rel-12, it is not optimized for low data-rate and wide area M2M services such as smart meters, remote sensors, and consumer devices. In this paper, we present a design of a new narrowband M2M system built from existing LTE functionalities for Low Power Wide Area (LPWA) systems. Salient features of this new system include low-cost devices, high coverage, long device battery life, and high capacity. It can be deployed using minimum of one GSM channel and can also share spectrum with existing broadband LTE systems. Cost analysis shows significant reduction compared to LTE UEs. In addition, coverage analysis and capacity results are presented. Finally, coexistence with GSM is analyzed.\"",
        "Document: \"LTE-advanced: next-generation wireless broadband technology [Invited Paper. LTE Release 8 is one of the primary broadband technologies based on OFDM, which is currently being commercialized. LTE Release 8, which is mainly deployed in a macro/microcell layout, provides improved system capacity and coverage, high peak data rates, low latency, reduced operating costs, multi-antenna support, flexible bandwidth operation and seamless integration with existing systems. LTE-Adva...\"",
        "Document: \"Multi-Antenna Systems for LTE eNodeB. Long-term evolution (LTE) of the UMTS network provides improved system capacity and coverage, high peak data rates, low latency, reduced operating costs, multi-antenna support, flexible bandwidth operations and seamless integration with existing systems. To achieve these enhancements, a new design for the air interface including state-of-art multi-antenna technology was developed. This article describes multi-antenna schemes for UMTS LTE and provides system performance of different multi-antenna schemes under various scenarios.\"",
        "Document: \"Model-based architecture analysis for wireless healthcare. The primary challenges in deploying a wireless healthcare solution stem from real-time, distributed resource constraints, as well as stringent clinical requirements of reliability, safety, device interoperability, QoS guarantee, and privacy/security. Although optimized solutions exist for each individual element of the system, the complex, distributed, and concurrent interactions among multiple subsystems make system integration a costly bottleneck. We show that end-to-end modeling and analysis using the formalisms of architecture description languages like AADL can alleviate the hurdles of system integration and provide an effective way of addressing these challenges, thereby, making deployments possible.\"",
        "1 is \"How much training is needed in multiple-antenna wireless links?\", 2 is \"Shannon-theoretic approach to a Gaussian cellular multiple-access channel\"",
        "Given above information, for an author who has written the paper with the title \"Millimeter Wave Communications for Future Mobile Networks.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009470": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Protein complex identification through Markov clustering with firefly algorithm on dynamic protein-protein interaction networks.':",
        "Document: \"Sparse kernel logistic regression for \u03b2-turns prediction. A \u03b2-turn is a secondary protein structure type that plays a significant role in protein folding, stability, and molecular recognition. On average 25% of amino acids in protein structures are located in \u03b2-turns. Development of accurate and efficient method for \u03b2-turns prediction is very important. Most of the current successful \u03b2-turns prediction methods use support vector machines (SVMs) or Neural Networks (NNs), however a method that can yield probabilistic outcome, and has a well-defined extension to the multi-class case will be more valuable in \u03b2-turns prediction. Although kernel logistic regression (KLR) is a powerful classification technique that has been applied successfully in many classification problems, however it is often not found in \u03b2-turns classification, mainly because it is computationally expensive. In this paper we used KLR to obtain sparse \u03b2-turns prediction in short evolution time after speeding it using Nystrom approximation method. Secondary structure information and position specific scoring matrices (PSSMs) are utilized as input features. We achieved Qtotal of 80.4% and MCC of 50% on BT426 dataset. These results show that KLR method with the right algorithm can yield performance equivalent or even better than NNs and SVMs in \u03b2-turns prediction. In addition KLR yields probabilistic outcome and has a well-defined extension to multi-class case.\"",
        "Document: \"Drug repositioning based on comprehensive similarity measures and Bi-Random Walk algorithm. Motivation: Drug repositioning, which aims to identify new indications for existing drugs, offers a promising alternative to reduce the total time and cost of traditional drug development. Many computational strategies for drug repositioning have been proposed, which are based on similarities among drugs and diseases. Current studies typically use either only drug-related properties (e.g. chemical structures) or only disease-related properties (e.g. phenotypes) to calculate drug or disease similarity, respectively, while not taking into account the influence of known drug-disease association information on the similarity measures. Results: In this article, based on the assumption that similar drugs are normally associated with similar diseases and vice versa, we propose a novel computational method named MBiRW, which utilizes some comprehensive similarity measures and Bi-Random walk (BiRW) algorithm to identify potential novel indications for a given drug. By integrating drug or disease features information with known drug-disease associations, the comprehensive similarity measures are firstly developed to calculate similarity for drugs and diseases. Then drug similarity network and disease similarity network are constructed, and they are incorporated into a heterogeneous network with known drug-disease interactions. Based on the drug-disease heterogeneous network, BiRW algorithm is adopted to predict novel potential drug-disease associations. Computational experiment results from various datasets demonstrate that the proposed approach has reliable prediction performance and outperforms several recent computational drug repositioning approaches. Moreover, case studies of five selected drugs further confirm the superior performance of our method to discover potential indications for drugs practically.\"",
        "Document: \"Feature Selection for Tandem Mass Spectrum Quality Assessment. In the literature, hundreds of features have been proposed to assess the quality of tandem mass spectra. However, some features may be nearly irrelevant, and thus the inclusion of these nearly irrelevant features may degenerate the performance of quality assessment. This paper introduces a two-stage support vector machine recursive feature elimination (SVM-RFE) method to select the most relevant features from those found in the literature. To verify the relevance of the selected features, the classifiers with the selected features are trained and their performances are evaluated. The out performances of classifiers with the selected features illustrate that the set of selected features is more relevant to the quality of spectra than any set of features used in the literature.\"",
        "Document: \"Guest editorial: Special focus on bioinformatics and systems biology. With advances in biotechnologies, large-scale biological data has been and will continue to be produced. These large-scale biological data contain insightful information for understanding the mechanism of biological systems and have proven useful in the diagnosis, treatment, and drug design for genomic-alerted diseases. In this special section, five papers in their significantly extended versions were selected from the papers presented at the IEEE Conference on Bioinformatics and Biomedicine (BIBM), 2009. These papers present recent research in bioinformatics and systems biology to make sense from large-scale biological data.\"",
        "1 is \"Learning Kernels for Unsupervised Domain Adaptation with Applications to Visual Object Recognition\", 2 is \"The use of edge-betweenness clustering to investigate biological function in protein interaction networks.\"",
        "Given above information, for an author who has written the paper with the title \"Protein complex identification through Markov clustering with firefly algorithm on dynamic protein-protein interaction networks.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009659": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Privacy-Preserving Collaborative Web Services QoS Prediction via Yao's Garbled Circuits and Homomorphic Encryption.':",
        "Document: \"Incremental Graph Pattern Based Node Matching. Graph Pattern based Node Matching (GPNM) is to find all the matches of the nodes in a data graph GD based on a given pattern graph GP. GPNM has become increasingly important in many applications, e.g., group finding and expert recommendation. In real scenarios, both GP and GD are updated frequently. However, the existing GPNM methods need to perform a new GPNM procedure from scratch to deliver the node matching results based on the updated GP and updated GD, which consumes much time. Therefore, there is a pressing need for a novel method to efficiently deliver the node matching results. In this paper, we propose a novel INCremental GPNM method called INC-GPNM, where we first build up indices to incrementally maintain the shortest path length range between different label types in GD, and then identify the affected parts of GD in GPNM including nodes and edges w.r.t. the updates of GP and GD. Moreover, based on the index structure and our novel search strategies, INC-GPNM can efficiently deliver node matching results taking the updates of GP and GD as input, and can greatly save the query processing time with improved time complexity. The extensive experiments on five real-world social graphs demonstrate that our method greatly outperforms the state-of-the-art GPNM method in efficiency.\"",
        "Document: \"Efficient Query Processing with Mutual Privacy Protection for Location-Based Services. Data privacy in location-based services involves two aspects. The location of a user is a kind of private data as many sensitive information can be inferred from it given some background knowledge. On the other hand, the POI database is a great asset to the LBS provider as its construction requires many resources and efforts. In this paper, we propose a method of protecting mutual privacy i.e., the location of the user issuing a query and the POI database of the LBS provider for location-based query processing. Our approach consists of two steps: data preparation and query processing. Data preparation is conducted by LBS itself and is totally an offline computation, while query processing involves some online computation and multiple rounds of communication between LBS and the user. We implement the query processing by two rounds of oblivious transfer extension OT-Extension on two small key sets, resulting an immediate response even on some big POI databases. We also theoretically prove the security and analyze the complexity of our approach. Compared with two state-of-the-art methods, our approach has several orders of magnitude improvement in response time, at the expense of little and acceptable communication cost.\"",
        "Document: \"Misclassification Minimization Based on Multiple Criteria Linear Programming. Misclassification minimization is an important and interesting topic in classification problem. Obviously, exploring the solution for this topic will benefit to many real life problems, such as credit card clients classification. This paper focuses on misclassification minimization based on multiple criteria linear programming (MCLP), proposing two different schemes to minimize the number of misclassified points in original MCLP. Especially, the complementarity is used to construct the first scheme and linear approximation technique is applied to solve it. Furthermore, successive linearization algorithm (SLA) is employed to achieve minimization the second scheme. Finally, numerical experiment tests the effect of this idea.\"",
        "Document: \"A new resource selection approach based on reputation driven Min-min algorithm in the grid economy. As Grid is a dynamic environment, the function of reputation is more and more obvious. Reputation systems have been a hot topic in the Peer-to-Peer community for several years. However the researches of it in the Grid are still not perfect, especially in the emerging Grid economy field. In this paper, we propose a reputation calculation model that is suitable in the economic-based Grid computing environment. We also propose a new module, Reputation Control Module (RCM), for the Grid Architecture for Computational Economy (GRACE), and give a detailed introduction of the internal structure of RCM and workflow of different module in GRACE architecture. Additionally, a reputation driven Min-min algorithm was designed based on reputation system in the Grid economy for resource selection. At last we show the performance of the system by simulating experiments.\"",
        "1 is \"Online mobile Micro-Task Allocation in spatial crowdsourcing\", 2 is \"Differential Privacy: An Economic Method for Choosing Epsilon\"",
        "Given above information, for an author who has written the paper with the title \"Privacy-Preserving Collaborative Web Services QoS Prediction via Yao's Garbled Circuits and Homomorphic Encryption.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009680": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A multiresolution motion estimation technique with indexing':",
        "Document: \"Orthogonal space time code based partial rank affine projection adaptive filtering algorithm. A space time code based partial rank affine projection (PRAP) algorithm is proposed. The proposed algorithm uses an input signal where the input signal matrix Xk becomes an orthogonal matrix. For this input signal, matrix (XkTXk) becomes a diagonal matrix whose inverse can be easily computed. Thus, the proposed algorithm saves a significant amount of computations. Due to this feature the proposed PRAP algorithm is shown to offer a faster convergence speed and a smaller computational burden per iteration than the NLMS algorithm does.\"",
        "Document: \"An efficient multidimensional decimation-in-frequency FHT algorithm based on the radix-2/4 approach. An efficient multidimensional (MD) split vector-radix (SVR) decimation-in-frequency (DIF) fast Hartley transform (FHT) algorithm is proposed by introducing a two-step decomposition strategy coupled with an efficient index mapping and the Kronecker product. Compared to existing MD FHT algorithms, the proposed one significantly reduces the complexity. In addition, the butterfly of the proposed algorithm is characterized by simple closed-form expressions allowing easy software or hardware implementation of the algorithm.\"",
        "Document: \"Design of a multidimensional split vector-radix decimation-in-frequency FFT algorithm. In this paper, the existing one-dimensional (1-D) radix-2/4 decimation-in-frequency (DIF) fast Fourier transform (FFT) algorithm is generalized to the case of an arbitrary dimension by introducing a mixture of radix-(2 times 2 times ... times 2) and radix-(4 times 4 times ... times 4) index maps. The introduction of these index maps coupled with an appropriate use of the Kronecker product enable us to design an efficient multi-dimensional (M-D) split vector-radix DIF FFT algorithm and characterize its butterfly by simple closed-form expressions allowing easy software or hardware implementation of the algorithm for any dimension. It is shown that the proposed algorithm substantially reduces the complexity compared to the existing M-D FFT algorithms\"",
        "Document: \"On sparsity issues in compressive sensing based speech enhancement. Signal sparsity is the fundamental requirement of compressive sensing (CS) techniques. In our previous work, a CS-based speech enhancement algorithm has been proposed. However, several issues concerning speech sparsity have not yet been thoroughly studied. In this paper, we focus on studying the following issues: (1) the sparsity of clean speech and audio signals; (2) the sparsity of various noise signals; (3) analysis of the capacity of two sparse transforms i.e., wavelet and discrete cosine transform (DCT), to explore speech sparsity. In this respect, several measures are proposed to analytically compare the wavelet transform with DCT. We found that (1) signal compressibility is an important factor for the CS-based method. (2) DCT explores the best compressibility for noisy signals and achieves the best enhancement performance; (2) The CS-based speech enhancement methods are more efficient in reducing the noise with worse compressibility.\"",
        "1 is \"A scalable and programmable architecture for 2-D DWT decoding\", 2 is \"DCT Implementation with Distributed Arithmetic\"",
        "Given above information, for an author who has written the paper with the title \"A multiresolution motion estimation technique with indexing\", which reference is related? Just choose 1 or 2 without further explanation."
    ]
}