{
    "0018": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Computational design of mechanical characters':",
        "Document: \"Efficient simulation of secondary motion in rig-space. We present an efficient method for augmenting keyframed character animations with physically-simulated secondary motion. Our method achieves a performance improvement of one to two orders of magnitude over previous work without compromising on quality. This performance is based on a linearized formulation of rig-space dynamics that uses only rig parameters as degrees of freedom, a physics-based volumetric skinning method that allows our method to predict the motion of internal vertices solely from deformations of the surface, as well as a deferred Jacobian update scheme that drastically reduces the number of required rig evaluations. We demonstrate the performance of our method by comparing it to previous work and showcase its potential on a production-quality character rig.\"",
        "Document: \"Computational design of mechanical characters. We present an interactive design system that allows non-expert users to create animated mechanical characters. Given an articulated character as input, the user iteratively creates an animation by sketching motion curves indicating how different parts of the character should move. For each motion curve, our framework creates an optimized mechanism that reproduces it as closely as possible. The resulting mechanisms are attached to the character and then connected to each other using gear trains, which are created in a semi-automated fashion. The mechanical assemblies generated with our system can be driven with a single input driver, such as a hand-operated crank or an electric motor, and they can be fabricated using rapid prototyping devices. We demonstrate the versatility of our approach by designing a wide range of mechanical characters, several of which we manufactured using 3D printing. While our pipeline is designed for characters driven by planar mechanisms, significant parts of it extend directly to non-planar mechanisms, allowing us to create characters with compelling 3D motions.\"",
        "1 is \"Super-helices for predicting the dynamics of natural hair\", 2 is \"Face transfer with multilinear models\"",
        "Given above information, for an author who has written the paper with the title \"Computational design of mechanical characters\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "0079": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Fusing Geometric Features for Skeleton-Based Action Recognition Using Multilayer LSTM Networks.':",
        "Document: \"On developing and enhancing plant-level disease rating systems in real fields. Cercospora leaf spot (CLS) is one of the most serious diseases of sugar beet worldwide, and if uncontrolled, causes nearly complete defoliation and loss of revenue for beet growers. The beet sugar industry continuously seeks CLS-resistant sugar beet cultivars as one strategy to combat this disease. Normally human experts manually observe and rate the resistance of a large variety of sugar beet plants over a period of a few months. Unfortunately, this procedure is laborious and the labels vary from one expert to another resulting in disagreements on the level of resistance. Therefore, we propose a novel computer vision system, CLS Rater, to automatically and accurately rate plant images in the real field to the \u201cUSDA scale\u201d of 0\u201310. Given a set of plant images captured by a tractor-mounted camera, CLS Rater extracts multi-scale superpixels, where in each scale a novel Histogram of Importances feature encodes both the within-superpixel local and across-superpixel global appearance variations. These features at different superpixel scales are then fused for learning a regressor that estimates the rating for each plant image. We further address the issue of the noisy labels by experts in the field, and propose a method to enhance the performance of the CLS Rater by automatically calibrating the experts ratings to ensure consistency. We test our system on the field data collected from two years over a two-month period for each year, under different lighting and weather conditions. Experimental results show that both the CLS Rater and the enhanced CLS Rater to be highly consistent with the rating errors of 0.65 and 0.59 respectively, which demonstrates a higher consistency than the rating standard deviation of 1.31 by human experts.\"",
        "Document: \"Joint Multi-Leaf Segmentation, Alignment and Tracking from Fluorescence Plant Videos. This paper proposes a novel framework for fluorescence plant video processing. The plant research community is interested in the leaf-level photosynthetic analysis within a plant. A prerequisite for such analysis is to segment all leaves, estimate their structures, and track them overtime. We identify this as a joint multi-leaf segmentation, alignment, and tracking problem. First, leaf segmentatio...\"",
        "1 is \"Indoor Scene Segmentation Using A Structured Light Sensor\", 2 is \"Panoptic Feature Pyramid Networks\"",
        "Given above information, for an author who has written the paper with the title \"Fusing Geometric Features for Skeleton-Based Action Recognition Using Multilayer LSTM Networks.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00137": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A Roadmap for Web Mining: From Web to Semantic Web':",
        "Document: \"Semantic Network Analysis of Ontologies. A key argument for modeling knowledge in ontologies is the easy re-use and re-engineering of the knowledge. However, beside consistency checking, current ontology engineering tools provide only basic functionalities for analyzing ontolo- gies. Since ontologies can be considered as (labeled, directed) graphs, graph analysis techniques are a suitable answer for this need. Graph analysis has been performed by sociologists for over 60 years, and resulted in the vivid research area of Social Network Analysis (SNA). While social network structures in general currently re- ceive high attention in the Semantic Web community, there are only very few SNA applications up to now, and virtually none for analyzing the structure of ontologies. We illustrate in this paper the benefits of applying SNA to ontologies and the Seman- tic Web, and discuss which research topics arise on the edge between the two areas. In particular, we discuss how different notions of centrality describe the core content and structure of an ontology. From the rather simple notion of degree centrality over betweenness centrality to the more complex eigenvector centrality based on Hermi- tian matrices, we illustrate the insights these measures provide on two ontologies, which are different in purpose, scope, and size.\"",
        "Document: \"On Publication Usage in a Social Bookmarking System. Scholarly success is traditionally measured in terms of citations to publications. With the advent of publication management and digital libraries on the web, scholarly usage data has become a target of investigation and new impact metrics computed on such usage data have been proposed -- so called altmetrics. In scholarly social bookmarking systems, scientists collect and manage publication meta data and thus reveal their interest in these publications. In this work, we investigate connections between usage metrics and citations, and find posts, exports, and page views of publications to be correlated to citations.\"",
        "1 is \"Generic pattern trees for exhaustive exceptional model mining\", 2 is \"An incremental data stream clustering algorithm based on dense units detection\"",
        "Given above information, for an author who has written the paper with the title \"A Roadmap for Web Mining: From Web to Semantic Web\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00169": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Automatic modeling of personality states in small group interactions':",
        "Document: \"Recurrent Convolutional Shape Regression. The mainstream direction in face alignment is now dominated by cascaded regression methods. These methods start from an image with an initial shape and build a set of shape increments based on features with respect to the current estimated shape. These shape increments move the initial shape to the desired location. Despite the advantages of the cascaded methods, they all share two major limitatio...\"",
        "Document: \"PET: An eye-tracking dataset for animal-centric Pascal object classes. We present PET- the Pascal animal classes Eye Tracking database. Our database comprises eye movement recordings compiled from forty users for the bird, cat, cow, dog, horse and sheep trainval sets from the VOC 2012 image set. Different from recent eye-tracking databases such as [1, 2], a salient aspect of PET is that it contains eye movements recorded for both the free-viewing and visual search task conditions. While some differences in terms of overall gaze behavior and scanning patterns are observed between the two conditions, a very similar number of fixations are observed on target objects for both conditions. As a utility application, we show how feature pooling around fixated locations enables enhanced (animal) object classification accuracy.\"",
        "1 is \"Fast and automatic video object segmentation and tracking for content-based applications\", 2 is \"An energy-saving support system for office environments\"",
        "Given above information, for an author who has written the paper with the title \"Automatic modeling of personality states in small group interactions\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00228": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'KAON - Towards a Large Scale Semantic Web':",
        "Document: \"Enhanced navigation and focus on TileBars with barycenter heuristic-based reordering. The classic TileBars paradigm has been used to show distribution information of query terms in full-text documents. However, when used to show the distribution of a large number of entities of interest to users within a document, it hinders users' quick comprehension due to the inherent visual complexity problem. In this paper, we present a novel approach to improve the visual presentation of TileBars, in which barycenter heuristic for bigraph crossing minimization is used to reorder TileBars' elements. The reordered TileBars enables users to quickly and easily identify which entities appear in the beginning, end, or throughout a document. A user study has shown that the reordered TileBars can provide users with better focus and navigation while exploring text documents.\"",
        "Document: \"Reordered tilebars for visual text exploration. The classic TileBars paradigm has been used to show distribution information of query terms in full-text documents. However, when the number of query terms becomes large, it is not an easy task for users to comprehend their distribution within certain parts of a document. In this paper, we present a novel approach to improve the visual presentation of TileBars, in which barycenter heuristic for bigraph crossing minimization is used to reorder TileBars elements. The reordered TileBars can be demonstrated to provide users with better focus and navigation while exploring text documents.\"",
        "1 is \"Skip n-grams and ranking functions for predicting script events\", 2 is \"Ontology-mediated integration of intranet Web services\"",
        "Given above information, for an author who has written the paper with the title \"KAON - Towards a Large Scale Semantic Web\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00244": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Nonlinear black-box modeling in system identification: a unified overview':",
        "Document: \"Sparse control using sum-of-norms regularized model predictive control. Some control applications require the use of piecewise constant or impulse-type control signals, with as few changes as possible. So as to achieve this type of control, we consider the use of regularized model predictive control (MPC), which allows us to impose this structure through the use of regularization. It is then possible to regulate the trade-off between control performance and control signal characteristics by tuning the so-called regularization parameter. However, since the mentioned trade-off is only indirectly affected by this parameter, its tuning is often unintuitive and time-consuming. In this paper, we propose an equivalent reformulation of the regularized MPC, which enables us to configure the desired trade-off in a more intuitive and computationally efficient manner. This reformulation is inspired by the so-called \u03b5-constraint formulation of multi-objective optimization problems and enables us to quantify the trade-off, by explicitly assigning bounds over the control performance.\"",
        "Document: \"Asymptotic Properties of Hyperparameter Estimators by Using Cross-Validations for Regularized System Identification. This paper studies the asymptotic properties of the hyperparameter estimators including the leave- k-out cross validation (LKOCV) and r-fold cross validation (RFCV) and discloses their relation with the Stein's unbiased risk estimators (SURE) as well as the mean squared error (MSE). It is shown that as the number of data goes to infinity, the LKOCV shares the same asymptotic best hyperparameter minimizing the MSE estimator as the SURE does if the input is bounded and the ratio between the training data and the whole data tends to zero. We illustrate the efficacy of the theoretical result by Monte Carlo simulations.\"",
        "1 is \"Orthogonal least squares learning algorithm for radial basis function networks\", 2 is \"On correct and complete strong merging of partial behaviour models\"",
        "Given above information, for an author who has written the paper with the title \"Nonlinear black-box modeling in system identification: a unified overview\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00252": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'OntoNotes: The 90% Solution':",
        "Document: \"Getting the most out of transition-based dependency parsing. This paper suggests two ways of improving transition-based, non-projective dependency parsing. First, we add a transition to an existing non-projective parsing algorithm, so it can perform either projective or non-projective parsing as needed. Second, we present a bootstrapping technique that narrows down discrepancies between gold-standard and automatic parses used as features. The new addition to the algorithm shows a clear advantage in parsing speed. The bootstrapping technique gives a significant improvement to parsing accuracy, showing near state-of-the-art performance with respect to other parsing approaches evaluated on the same data set.\"",
        "Document: \"Propbank Frameset Annotation Guidelines Using a Dedicated Editor, Cornerstone. This paper gives guidelines of how to create and update Propbank frameset files using a dedicated editor, Cornerstone. Propbank is a corpus in which the arguments of each verb predicate are annotated with their semantic roles in relation to the predicate. Propbank annotation also requires the choice of a sense ID for each predicate. Thus, for each predicate in Propbank, there exists a corresponding frameset file showing the expected predicate argument structure of each sense related to the predicate. Since most Propbank annotations are based on the predicate argument structure defined in the frameset files, it is important to keep the files consistent, simple to read as well as easy to update. The frameset files are written in XML, which can be difficult to edit when using a simple text editor. Therefore, it is helpful to develop a user-friendly editor such as Cornerstone, specifically customized to create and edit frameset files. Cornerstone runs platform independently, is light enough to run as an X11 application and supports multiple languages such as Arabic, Chinese, English, Hindi and Korean.\"",
        "1 is \"Generation of single-sentence paraphrases from predicate/argument structure using lexico-grammatical resources\", 2 is \"Joint learning improves semantic role labeling\"",
        "Given above information, for an author who has written the paper with the title \"OntoNotes: The 90% Solution\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00254": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Combining Nonmonotonic Reasoning and Belief Revision: A Practical Approach':",
        "Document: \"Computing Horn Strong Backdoor Sets Thanks to Local Search. In this paper a new approach for computing Strong Backdoor sets of boolean formula in conjunctive normal form (CNF) is proposed. It makes an original use of local search techniques for finding an assignment leading to a largest renamable Horn sub-formula of a given CNF. More precisely, at each step, preference is given to variables such that when assigned to the opposite value lead to the smallest number of remaining non- Horn clauses. Consequently, if no positive or non Horn clauses remain in the formula, our approach answer the satisfiability of the original formula; otherwise, a smallest non-Horn sub-formula is used to extract the set of variables (Strong Backdoor) such that when assigned leads to a tractable sub-formula. Branching on the variables of the Strong Backdoor set leads to significant improvements of Zchaff SAT solver with respect to many real worlds SAT instances.\"",
        "Document: \"Prime Implicates Based Inconsistency Characterization. Measuring inconsistency is recognized as an important issue for handling inconsistencies [5, 6]. Based on prime implicates canonical representation, we first characterize the conflicting variables allowing us to refine an existing inconsistency measure. Secondly, we propose a new measure, to circumscribe the internal conflicts in a knowledge base. This measure is proved to satisfy a new but weaker form of dominance.\"",
        "1 is \"An empirical study of greedy local search for satisfiability testing\", 2 is \"A Reasoning Model Based on the Production of Acceptable Arguments\"",
        "Given above information, for an author who has written the paper with the title \"Combining Nonmonotonic Reasoning and Belief Revision: A Practical Approach\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00297": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Neural networks for discrimination and modelization of speakers':",
        "Document: \"Topological multi-view clustering for collaborative filtering. Collaborative filtering is a well-known technique for recommender systems. Collaborative filtering models use the available preferences of a group of users to make recommendations or predictions of the unknown preferences for other users. Collaborative filtering suffers from the data sparsity problem when users only rate a small set of items which makes the computation of users similarity imprecise and reduce consequently the accuracy of the recommended items. Clustering techniques include multiplex network clustering can be used to deal with this problem. In this paper, we propose a collaborative filtering system based on clustering multiplex network that predict the rate value that a user would give to an item. This approach looks, in a first step, for users having the same behavior or sharing the same characteristics. Then, use the ratings from those similar users found in the first step to predict other ratings. The proposed approach has been validated on MovieLens dataset and the obtained results have shown very promising performances.\"",
        "Document: \"Mining Customers' Spatio-Temporal Behavior Data Using Topographic Unsupervised Learning. Radio Frequency IDentification (RFID) is an advanced tracking technology that can be used to study the spatio-temporal behavior of customers in a supermarket. The aim of this work is to build a new RFID-based autonomous system to follow individuals' spatio-temporal activity, a tool not currently available, and to develop new methods for automatic data mining. Here, we study how to transform these data to investigate the customers' behaviors. We propose a new unsupervised data mining method to deal with this complex and very noisy data. This method is fast, efficient and allows some useful analysis to understand how the customers behave during shopping.\"",
        "1 is \"Growing Grid \u2014 a self-organizing network with constant neighborhood range and adaptation strength\", 2 is \"Reinforcement Learning as Classification: Leveraging Modern Classifiers\"",
        "Given above information, for an author who has written the paper with the title \"Neural networks for discrimination and modelization of speakers\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00453": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Reasoning of abstract motion of a target object through task order with natural language - Pre-knowledge of object-handling-task programming for a service robot':",
        "Document: \"Receding horizon path planning for 3D exploration and surface inspection. Within this paper a new path planning algorithm for autonomous robotic exploration and inspection is presented. The proposed method plans online in a receding horizon fashion by sampling possible future configurations in a geometric random tree. The choice of the objective function enables the planning for either the exploration of unknown volume or inspection of a given surface manifold in both known and unknown volume. Application to rotorcraft Micro Aerial Vehicles is presented, although planning for other types of robotic platforms is possible, even in the absence of a boundary value solver and subject to nonholonomic constraints. Furthermore, the method allows the integration of a wide variety of sensor models. The presented analysis of computational complexity and thorough simulations-based evaluation indicate good scaling properties with respect to the scenario complexity. Feasibility and practical applicability are demonstrated in real-life experimental test cases with full on-board computation.\"",
        "Document: \"Comparing ICP variants on real-world data sets - Open-source library and experimental protocol. Many modern sensors used for mapping produce 3D point clouds, which are typically registered together using the iterative closest point (ICP) algorithm. Because ICP has many variants whose performances depend on the environment and the sensor, hundreds of variations have been published. However, no comparison frameworks are available, leading to an arduous selection of an appropriate variant for particular experimental conditions. The first contribution of this paper consists of a protocol that allows for a comparison between ICP variants, taking into account a broad range of inputs. The second contribution is an open-source ICP library, which is fast enough to be usable in multiple real-world applications, while being modular enough to ease comparison of multiple solutions. This paper presents two examples of these field applications. The last contribution is the comparison of two baseline ICP variants using data sets that cover a rich variety of environments. Besides demonstrating the need for improved ICP methods for natural, unstructured and information-deprived environments, these baseline variants also provide a solid basis to which novel solutions could be compared. The combination of our protocol, software, and baseline results demonstrate convincingly how open-source software can push forward the research in mapping and navigation.\"",
        "1 is \"Omnidirectional Vision Based Topological Navigation\", 2 is \"Dynamic adaptation strategies for long-term and short-term user profile to personalize search\"",
        "Given above information, for an author who has written the paper with the title \"Reasoning of abstract motion of a target object through task order with natural language - Pre-knowledge of object-handling-task programming for a service robot\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00502": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Improving Multi-Modal Representations Using Image Dispersion: Why Less Is Sometimes More':",
        "Document: \"On the Robustness of Entropy-Based Similarity Measures in Evaluation of Subcategorization Acquisition Systems. Some statistical learning systems are evaluated using measures of distributional similarity. To deal with the problem of zero events in the distributions under comparison, smoothing is frequently performed before similarity measures are applied. Smoothing alters the information in the original distribution, and may add noise to the results. Here, we investigate the sensitivity of entropy-based similarity measures to noise from uninformative smoothing. Our experiments with two subcategorization acquisition systems show that similarity measures vary in their robustness. While some are led astray by noise from smoothing, others are more resilient.\"",
        "Document: \"Statistical metaphor processing. Metaphor is highly frequent in language, which makes its computational processing indispensable for real-world NLP applications addressing semantic tasks. Previous approaches to metaphor modeling rely on task-specific hand-coded knowledge and operate on a limited domain or a subset of phenomena. We present the first integrated open-domain statistical model of metaphor processing in unrestricted text. Our method first identifies metaphorical expressions in running text and then paraphrases them with their literal paraphrases. Such a text-to-text model of metaphor interpretation is compatible with other NLP applications that can benefit from metaphor resolution. Our approach is minimally supervised, relies on the state-of-the-art parsing and lexical acquisition technologies distributional clustering and selectional preference induction, and operates with a high accuracy.\"",
        "1 is \"Tagging English text with a probabilistic model\", 2 is \"Development of the Japanese WordNet\"",
        "Given above information, for an author who has written the paper with the title \"Improving Multi-Modal Representations Using Image Dispersion: Why Less Is Sometimes More\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00574": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Exploiting GPU peak-power and performance tradeoffs through reduced effective pipeline latency':",
        "Document: \"A Combined Interval and Floating Point Multiplier. Interval arithmetic provides an efficient method for monitoring and controlling errors in numerical calculations. However, existing software packages for interval arithmetic are often too slow for numerically intensive computations. This paper presents the design of a multiplier that performs either interval or floating point multiplication. This multiplier requires only slightly more area and delay than a conventional floating point multiplier, and is one to two orders of magnitude faster than software implementations of interval multiplication.\"",
        "Document: \"Combined Multiplication And Sum-Of-Squares Units. Multiplication and squaring are important operations in digital signal processing and multimedia applications. This paper presents designs for units that implement either multiplication, A x B, or sum-of-squares computations, A(2) + B-2, based on an input control signal. Compared to conventional parallel multipliers, these units have a modest increase in area and delay, but allow either multiplication or sum-of-squares computations to be performed. Combined multiplication and sum-of-squares units for unsigned and two's complement operands are presented, along with integrated designs that can operate on either unsigned or two's complement operands. The designs can also be extended to work with a third accumulator operand to compute either Z + A x B or Z + A(2) + B-2. Synthesis results indicate that a combined multiplication and sum-of-squares unit for 32-bit two's complement operands can be implemented with roughly 15% more area and nearly the same worst case delay as a conventional 32-bit two's complement multiplier.\"",
        "1 is \"Efficient online computation of core speeds to maximize the throughput of thermally constrained multi-core processors\", 2 is \"A static task partitioning approach for heterogeneous systems using OpenCL\"",
        "Given above information, for an author who has written the paper with the title \"Exploiting GPU peak-power and performance tradeoffs through reduced effective pipeline latency\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00592": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'PD Notify: Investigating Personal Content on Public Displays.':",
        "Document: \"Wearable computing for older adults: initial insights into head-mounted display usage. With recent interest in industry, wearable computers with head-mounted displays are about to become mainstream. As it is typical for novel technologies, development is directed towards early adopters. This typically excludes special target groups such as older adults with age related special needs. However, it is necessary to consider their requirements when the technology matures, as they can benefit from wearable computing. In this paper we present an explorative, qualitative study with three older adults that used a wearable computer with a head mounted display during everyday activities. We derive requirements from the usage of existing applications, describe emerging usage patterns, highlight promising applications, and the reaction of the public.\"",
        "Document: \"Investigation Of An Ambient And Pervasive Smart Wall Calendar With Event Suggestions. People have been using calendars for thousands of years to schedule appointments and to keep track of their daily lives. Today, calendars have a variety of form factors, including wall, desk, and digital calendars that all have specific advantages and limitations. In previous work, we envisioned Caloo a smart wall calendar. In addition to displaying users' schedule, Caloo suggests nearby events. Caloo aims to increase the awareness regarding appointments and to support to be active through event suggestions. In this paper, we present the implementation and insights of the developed smart calendar. We deployed Caloo for four weeks in participants' homes. Our results show that all participants are eager to use the developed system. Our analysis further indicates that the usage of Caloo makes users more open to attending local events. Results also suggest that it is important to provide fine-grained control over event suggestions, enable users to define when events should be suggested as well as to prioritize events.\"",
        "1 is \"Probing communities: study of a village photo display\", 2 is \"Keystroke dynamics authentication for mobile phones\"",
        "Given above information, for an author who has written the paper with the title \"PD Notify: Investigating Personal Content on Public Displays.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00623": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Multiple description coding of free viewpoint video for multi-path network streaming':",
        "Document: \"Distributed Streaming Via Packet Partitioning. We propose a system for adaptive streaming from multiple servers to a single receiver over separate network paths. Based on incoming packets, the receiver estimates the available bandwidth on every path and returns this information to the servers. An optimization algorithm is designed that enables the servers to independently partition the media packets among them according to the bandwidth information and such that the resulting video quality at the receiver is maximized. To this end, the algorithm takes advantage of a source pruning technique that preprocesses the media stream ahead of time. Simulation results demonstrate that the proposed streaming framework provides superior performance over a conventional transmission scheme that performs proportional packet scheduling based only on the available network bandwidth. Due to its low-complexity aspect, the framework is suitable for practical implementations of adaptive and efficient distributed streaming systems.\"",
        "Document: \"Joint optimization of flow allocation and data center placement in multi-service networks. We present a framework for jointly optimizing the data center placement and network flow allocation in multi-service networks. The optimization is derived within a cost-performance formulation that aims at minimizing the overall operating cost of the network over which the services are provided, given a set of end-user performance requirements. The problem formulation represents a linear programming that is efficiently solved via a convex optimization technique that we design to this end. The generality of the formulation allows its application to other important problems, such as green networking and transportation of hazardous materials. We introduce the concept of cost performance versus service requirements lower-convex hull that can be employed to study the operational efficiency of various data center location and network flow allocation configuration points. We investigate the performance of the optimization framework via simulation experiments and compare its efficiency against two heuristic reference schemes. We demonstrate substantial operational cost savings or conversely service performance advances, for the same overall operating cost.\"",
        "1 is \"Software Structure Metrics Based on Information Flow\", 2 is \"Trinary Partition Black-Burst based Broadcast Protocol for Emergency Message dissemination in VANET\"",
        "Given above information, for an author who has written the paper with the title \"Multiple description coding of free viewpoint video for multi-path network streaming\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00720": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Strategies for mapping dataflow blocks to distributed hardware':",
        "Document: \"Reconsidering Custom Memory Allocation. Programmers hoping to achieve performance improvements often use custom memory allocators. This in-depth study examines eight applications that use custom allocators. Surprisingly, for six of these applications, a state-of-the-art general-purpose allocator (the Lea allocator) performs as well as or better than the custom allocators. The two exceptions use regions, which deliver higher performance (improvements of up to 44%). Regions also reduce programmer burden and eliminate a source of memory leaks. However, we show that the inability of programmers to free individual objects within regions can lead to a substantial increase in memory consumption. Worse, this limitation precludes the use of regions for common programming idioms, reducing their usefulness.We present a generalization of general-purpose and region-based allocators that we call reaps. Reaps are a combination of regions and heaps, providing a full range of region semantics with the addition of individual object deletion. We show that our implementation of reaps provides high performance, outperforming other allocators with region-like semantics. We then use a case study to demonstrate the space advantages and software engineering benefits of reaps in practice. Our results indicate that programmers needing fast regions should use reaps, and that most programmers considering custom allocators should instead use the Lea allocator.\n\n\"",
        "Document: \"Merging Head and Tail Duplication for Convergent Hyperblock Formation. VLIW and EDGE (Explicit Data Graph Execution) ar- chitectures rely on compilers to form high-quality hyper- blocks for good performance. These compilers typically perform hyperblock formation, loop unrolling, and scalar optimizations in a fixed order. This approach limits the compiler's ability to exploit or correct interactions among these phases. EDGE architectures exacerbate this problem by imposing structural constraints on hyperblocks, such as instruction count and instruction composition. This paper presents convergent hyperblock formation, which iteratively applies if-conversion, peeling, unrolling, and scalar optimizations until converging on hyperblocks that are as close as possible to the structural constraints. To perform peeling and unrolling, convergent hyperblock formation generalizes tail duplication, which removes side entrances to acyclic traces, to remove back edges into cyclic traces using head duplication. Simulation results for an EDGE architecture show that convergent hyperblock formation improves code quality over discrete-phase ap- proaches with heuristics for VLIW and EDGE. This algo- rithm offers a solution to hyperblock phase ordering prob- lems and can be configured to implement a wide range of policies.\"",
        "1 is \"CHIMAERA: a high-performance architecture with a tightly-coupled reconfigurable functional unit\", 2 is \"Bytecode compression via profiled grammar rewriting\"",
        "Given above information, for an author who has written the paper with the title \"Strategies for mapping dataflow blocks to distributed hardware\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00805": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Towards multi-level and modular conceptual schema specifications':",
        "Document: \"Event relations in plan-based plot composition. The process of plot composition in the context of interactive storytelling is considered from a fourfold perspective, that is, from the syntagmatic, paradigmatic, antithetic, and meronymic relations between the constituent events. These relations are shown to be associated with the four major tropes of semiotic research. A conceptual model and set of facilities for interactive plot composition and adaptation dealing with the four relations is described. To accommodate antithetic relations, corresponding to the irony trope, our plan-based approach leaves room for the unplanned. A simple storyboarding prototype tool has been implemented to conduct experiments.\"",
        "Document: \"Storytelling Variants: The Case of Little Red Riding Hood. A small number of variants of a widely disseminated folktale is surveyed, and then analyzed in an attempt to determine how such variants can emerge while staying within the conventions of the genre. The study follows the classification of types and motifs contained in the Index of Antti Aarne and Stith Thompson. The paper's main contribution is the characterization of four kinds of type interactions in terms of semiotic relations. Our objective is to provide the conceptual basis for the development of semi-automatic methods to help users compose their own narrative plots.\"",
        "1 is \"Measurement-based admission control with aggregate traffic envelopes\", 2 is \"Constraint acquisition for Entity-Relationship models\"",
        "Given above information, for an author who has written the paper with the title \"Towards multi-level and modular conceptual schema specifications\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00809": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Pointwise Motion Image (PMI): A Novel Motion Representation and Its Applications to Abnormality Detection and Behavior Recognition':",
        "Document: \"PnP Problem Revisited. Perspective-n-Point camera pose determination, or the PnP problem, has attracted much attention in the literature. This paper gives a systematic investigation on the PnP problem from both geometric and algebraic standpoints, and has the following contributions: Firstly, we rigorously prove that the PnP problem under distance-based definition is equivalent to the PnP problem under orthogonal-transformation-based definition when n 3, and equivalent to the PnP problem under rotation-transformation-based definition when n = 3. Secondly, we obtain the upper bounds of the number of solutions for the PnP problem under different definitions. In particular, we show that for any three non-collinear control points, we can always find out a location of optical center such that the P3P problem formed by these three control points and the optical center can have 4 solutions, its upper bound. Additionally a geometric way is provided to construct these 4 solutions. Thirdly, we introduce a depth-ratio based approach to represent the solutions of the whole PnP problem. This approach is shown to be advantageous over the traditional elimination techniques. Lastly, degenerated cases for coplanar or collinear control points are also discussed. Surprisingly enough, it is shown that if all the control points are collinear, the PnP problem under distance-based definition has a unique solution, but the PnP problem under transformation-based definition is only determined up to one free parameter.\"",
        "Document: \"A robust method to recognize critical configuration for camera calibration. When space points and camera optical center lie on a twisted cubic, no matter how many pairs there are used from the space points to their image points, camera parameters cannot be determined uniquely. This configuration is critical for camera calibration. We set up invariant relationship between six space points and their image points for the critical configuration. Then based on the relationship, an algorithm to recognize the critical configuration of at least six pairs of space and image points is proposed by using a constructed criterion function, where no any explicit computation on camera projective matrix or optical center is needed. Experiments show the efficiency of the proposed method.\"",
        "1 is \"Calibration of mirror position and extrinsic parameters in axial non-central catadioptric systems.\", 2 is \"Dynamic and scalable large scale image reconstruction\"",
        "Given above information, for an author who has written the paper with the title \"Pointwise Motion Image (PMI): A Novel Motion Representation and Its Applications to Abnormality Detection and Behavior Recognition\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00810": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Learning Depth from Single Images with Deep Neural Network Embedding Focal Length.':",
        "Document: \"PnP Problem Revisited. Perspective-n-Point camera pose determination, or the PnP problem, has attracted much attention in the literature. This paper gives a systematic investigation on the PnP problem from both geometric and algebraic standpoints, and has the following contributions: Firstly, we rigorously prove that the PnP problem under distance-based definition is equivalent to the PnP problem under orthogonal-transformation-based definition when n 3, and equivalent to the PnP problem under rotation-transformation-based definition when n = 3. Secondly, we obtain the upper bounds of the number of solutions for the PnP problem under different definitions. In particular, we show that for any three non-collinear control points, we can always find out a location of optical center such that the P3P problem formed by these three control points and the optical center can have 4 solutions, its upper bound. Additionally a geometric way is provided to construct these 4 solutions. Thirdly, we introduce a depth-ratio based approach to represent the solutions of the whole PnP problem. This approach is shown to be advantageous over the traditional elimination techniques. Lastly, degenerated cases for coplanar or collinear control points are also discussed. Surprisingly enough, it is shown that if all the control points are collinear, the PnP problem under distance-based definition has a unique solution, but the PnP problem under transformation-based definition is only determined up to one free parameter.\"",
        "Document: \"Radial distortion invariants and lens evaluation under a single-optical-axis omnidirectional camera. \u2022We establish invariants for space points and radially distorted image points.\u2022We construct the invariant criterion functions that are very stable to noise.\u2022A feature vector is constructed from the functions and its infinity norm is computed.\u2022The norm is applied to evaluate camera lens alignment or tangent distortion.\u2022The evaluation is flexible and useful in applications such as 3D reconstruction.\"",
        "1 is \"Robust Sliding Mode-Based Learning Control for Steer-by-Wire Systems in Modern Vehicles.\", 2 is \"Efficient exact inference for 3d indoor scene understanding\"",
        "Given above information, for an author who has written the paper with the title \"Learning Depth from Single Images with Deep Neural Network Embedding Focal Length.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00882": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Focused Hierarchical RNNs for Conditional Sequence Processing.':",
        "Document: \"Exploiting Determinism To Scale Relational Inference. One key challenge in statistical relational learning (SRL) is scalable inference. Unfortunately, most real-world problems in SRL have expressive models that translate into large grounded networks, representing a bottleneck for any inference method and weakening its scalability. In this paper we introduce Preference Relaxation (PR), a two-stage strategy that uses the determinism present in the underlying model to improve the scalability of relational inference. The basic idea of PR is that if the underlying model involves mandatory (i.e. hard) constraints as well as preferences (i.e. soft constraints) then it is potentially wasteful to allocate memory for all constraints in advance when performing inference. To avoid this, PR starts by relaxing preferences and performing inference with hard constraints only. It then removes variables that violate hard constraints, thereby avoiding irrelevant computations involving preferences. In addition it uses the removed variables to enlarge the evidence database. This reduces the effective size of the grounded network. Our approach is general and can be applied to various inference methods in relational domains. Experiments on real-world applications show how PR substantially scales relational inference with a minor impact on accuracy.\"",
        "Document: \"Combining Generative and Discriminative Methods for Pixel Classification with Multi-Conditional Learning. It is possible to broadly characterize two approaches to probabilistic modeling in terms of generative and discriminative methods. Provided with sufficient training data the discriminative approach is expected to yield superior accuracy as compared to the analogous generative model since no modeling power is expended on the marginal distribution of the features. Conversely, if the model is accurate the generative approach can perform better with less data. In general it is less vulnerable to overfitting and allows one to more easily specify meaningful priors on the model parameters. We investigate multi-conditional learning - a method combining the merits of both approaches. Through specifying a joint distribution over classes and features we derive a family of models with analogous parameters. Parameter estimates are found by optimizing an objective function consisting of a weighted combination of conditional log-likelihoods. Systematic experiments in the context of foreground/background pixel classification with the Microsoft-Berkeley segmentation database using mixtures of factor analyzers illustrate tradeoffs between classifier complexity, the amount of training data and generalization accuracy. We show experimentally that this approach can lead to models with better generalization performance than purely generative or discriminative approaches\"",
        "1 is \"Learning from streaming data with concept drift and imbalance: an overview.\", 2 is \"It makes sense: a wide-coverage word sense disambiguation system for free text\"",
        "Given above information, for an author who has written the paper with the title \"Focused Hierarchical RNNs for Conditional Sequence Processing.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00901": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Performance of polar codes for quantum and private classical communication':",
        "Document: \"Quantum data hiding in the presence of noise. When classical or quantum information is broadcast to separate receivers, there exist codes that encrypt the encoded data, such that the receivers cannot recover it when performing local operations and classical communication, but they can reliably decode if they bring their systems together and perform a collective measurement. This phenomenon is known as quantum data hiding and hitherto has been...\"",
        "Document: \"Quantum interference channels. The discrete memoryless interference channel is modelled as a conditional\nprobability distribution with two outputs conditional on two inputs and has\nwidespread applications in practical communication scenarios. In this paper, we\nintroduce and study the quantum interference channel, a generalization of a\ntwo-input, two-output memoryless channel to the setting of quantum Shannon\ntheory. Our work uses three different coding strategies for quantum multiple\naccess channels to build codes giving achievable rate regions for quantum\ninterference channels. Our proof of the quantum Han-Kobayashi coding strategy\nis conditional on a conjecture regarding the existence of a quantum\nsimultaneous decoder for multiple access channels. This conjecture holds for\nthe class of quasi-classical multiple access channels, for which the outputs of\nthese channels commute.\"",
        "1 is \"Financial incentives and the \", 2 is \"\"",
        "Given above information, for an author who has written the paper with the title \"Performance of polar codes for quantum and private classical communication\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00911": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A note on problem difficulty measures in black-box optimization: classification, realizations and predictability.':",
        "Document: \"Medium step sizes are harmful for the compact genetic algorithm. We study the intricate dynamics of the Compact Genetic Algorithm (cGA) on OneMax, and how its performance depends on the step size 1/K, that determines how quickly decisions about promising bit values are fixed in the probabilistic model. It is known that cGA and UMDA, a related algorithm, run in expected time O(n log n) when the step size is just small enough [EQUATION] to avoid wrong decisions being fixed. UMDA also shows the same performance in a very different regime (equivalent to K = \u0398(log n) in the cGA) with much larger steps sizes, but for very different reasons: many wrong decisions are fixed initially, but then reverted efficiently.\n\nWe show that step sizes in between these two optimal regimes are harmful as they yield larger runtimes: we prove a lower bound \u03a9(K1/3n+n log n) for the cGA on OneMax for [EQUATION]. For K = \u03a9(log3n) the runtime increases with growing K before dropping again to [EQUATION] for [EQUATION]. This suggests that the expected runtime for cGA is a bimodal function in K with two very different optimal regions and worse performance in between.\n\n\"",
        "Document: \"A few ants are enough: ACO with iteration-best update. Ant colony optimization (ACO) has found many applications in different problem domains. We carry out a first rigorous runtime analysis of ACO with iteration-best update, where the best solution in the each iteration is reinforced. This is similar to comma selection in evolutionary algorithms. We compare ACO to evolutionary algorithms for which it is well known that an offspring size of \u03a9(log n), n the problem dimension, is necessary to optimize even simple functions like ONEMAX. In sharp contrast, ACO is efficient on ONEMAX even for the smallest possible number of two ants. Remarkably, this only holds if the pheromone evaporation rate is small enough; the collective memory of many ants stored in the pheromones makes up for the small number of ants. We further prove an exponential lower bound for ACO with iteration-best update that depends on a trade-off between the number of ants and the evaporation rate.\"",
        "1 is \"Incremental local search in ant colony optimization: why it fails for the quadratic assignment problem\", 2 is \"Multi-objective competitive coevolution for efficient GP classifier problem decomposition\"",
        "Given above information, for an author who has written the paper with the title \"A note on problem difficulty measures in black-box optimization: classification, realizations and predictability.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00970": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'RoboCup-2003 new scientific and technical advances':",
        "Document: \"Maintaining coherent perceptual information using anchoring. The purpose of this paper is to address the problem of maintaining coherent perceptual information in a mobile robotic system working over extended periods of time, interacting with a user and using multiple sensing modalities to gather information about the environment and specific objects. We present a system which is able to use spatial and olfactory sensors to patrol a corridor and execute user requested tasks. To cope with perceptual maintenance we present an extension of the anchoring framework capable of maintaining the correspondence between sensor data and the symbolic descriptions referring to objects. It is also capable of tracking and acquiring information from observations derived from sensor-data as well as information from a priori symbolic concepts. The general system is described and an experimental validation on a mobile robot is presented.\"",
        "Document: \"Network robot systems. This article introduces the definition of Network Robot Systems (NRS) as is understood in Europe, USA and Japan. Moreover, it describes some of the NRS projects in Europe and Japan and presents a summary of the papers of this Special Issue.\"",
        "1 is \"Multirobot systems: a classification focused on coordination\", 2 is \"A Constraint-Based Method for Project Scheduling with Time Windows\"",
        "Given above information, for an author who has written the paper with the title \"RoboCup-2003 new scientific and technical advances\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00985": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Delay Minimal Policies In Energy Harvesting Broadcast Channels':",
        "Document: \"Buffer-Aware Network Coding for Wireless Networks. Network coding, which can combine various traffic flows or packets via algebraic operations, has the potential of achieving substantial throughput and power efficiency gains in wireless networks. As such, it is considered as a powerful solution to meet the stringent demands and requirements of next-generation wireless systems. However, because of the random and asynchronous packet arrivals, network coding may result in severe delay and packet loss because packets need to wait to be network-coded with each others. To overcome this and guarantee quality of service (QoS), we present a novel cross-layer approach, which we shall refer to as Buffer-Aware Network Coding, or BANC, which allows transmission of some packets without network coding to reduce the packet delay. We shall derive the average delay and power consumption of BANC by presenting a random mapping description of BANC and Markov models of buffer states. A cross-layer optimization problem that minimizes the average delay under a given power constraint is then proposed and analyzed. Its solution will not only demonstrate the fundamental performance limits of BANC in terms of the achievable delay region and delay-power tradeoff, but also obtains the delay-optimal BANC schemes. Simulation results will show that the proposed approach can strike the optimal tradeoff between power efficiency and QoS.\"",
        "Document: \"Utilization of LTE-a uplink resource for cognitive radio network via matching and quantizing. With the development of next generation mobile communications, the underlay coexistence problem of the OFDMA based Secondary System(SS) with LTE-A systems becomes more and more important, which yet has not been studied in a systematic way. In contrast to other Primary Systems(PS), the LTE-A system puts high demands on the low complexity of the coexistence strategies. This paper focuses on the resource allocation and interference mitigation issues in the aforementioned scenario, whose objective is to protect the spectrum utilization priority of PS as well as utilize secondary resource efficiently. The difficulty lies in the fact that even the subproblem, or power allocation with interference, is NP-Hard. Therefore, this paper will propose a two-phase resource allocation algorithm using maximum weighted Matching in the subcarrier allocation phase and interference Quantizing in the power allocation phase, referred to as the MQ algorithm. As presented in this paper, the MQ algorithm enjoys the advantage of polynomial complexity of O(KJ3 + LKJ), where K, J and L denote the number of SSs, subcarriers and quantizing steps, respectively. The simulation results will show that the proposed MQ algorithm is capable of achieving near optimal system and user throughputs, which are close to the exhaustive searching algorithm.\"",
        "1 is \"A conditional entropy bound for a pair of discrete random variables\", 2 is \"End-to-End Learning of Geometry and Context for Deep Stereo Regression\"",
        "Given above information, for an author who has written the paper with the title \"Delay Minimal Policies In Energy Harvesting Broadcast Channels\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00997": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Comparing probability measures using possibility theory: A notion of relative peakedness':",
        "Document: \"Decision tree and instance-based learning for label ranking. The label ranking problem consists of learning a model that maps instances to total orders over a finite set of predefined labels. This paper introduces new methods for label ranking that complement and improve upon existing approaches. More specifically, we propose extensions of two methods that have been used extensively for classification and regression so far, namely instance-based learning and decision tree induction. The unifying element of the two methods is a procedure for locally estimating predictive probability models for label rankings.\"",
        "Document: \"Comparing Fuzzy Partitions: A Generalization of the Rand Index and Related Measures. In this paper, we introduce a fuzzy extension of a class of measures to compare clustering structures, namely, measures that are based on the number of concordant and the number of discordant pairs of data points. This class includes the well-known Rand index but also commonly used alternatives, such as the Jaccard measure. In contrast with previous proposals, our extension exhibits desirable metrical properties. Apart from elaborating on formal properties of this kind, we present an experimental study in which we compare different fuzzy extensions of the Rand index and the Jaccard measure.\"",
        "1 is \"Solving Fuzzy PERT Using Gradual Real Numbers\", 2 is \"The human &quot;magnesome&quot;: detecting magnesium binding sites on human proteins.\"",
        "Given above information, for an author who has written the paper with the title \"Comparing probability measures using possibility theory: A notion of relative peakedness\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001004": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Asymptotic Performance of a Censoring Sensor Network':",
        "Document: \"Robust Management of Motion Uncertainty in Intensity-Modulated Radiation Therapy. Radiation therapy is subject to uncertainties that need to be accounted for when determining a suitable treatment plan for a cancer patient. For lung and liver tumors, the presence of breathing motion during treatment is a challenge to the effective and reliable delivery of the radiation. In this paper, we build a model of motion uncertainty using probability density functions that describe breathing motion, and provide a robust formulation of the problem of optimizing intensity-modulated radiation therapy. We populate our model with real patient data and measure the robustness of the resulting solutions on a clinical lung example. Our robust framework generalizes current mathematical programming formulations that account for motion, and gives insight into the trade-off between sparing the healthy tissues and ensuring that the tumor receives sufficient dose. For comparison, we also compute solutions to a nominal (no uncertainty) and margin (worst-case) formulation. In our experiments, we found that the nominal solution typically underdosed the tumor in the unacceptable range of 6% to 11%, whereas the robust solution underdosed by only 1% to 2% in the worst case. In addition, the robust solution reduced the total dose delivered to the main organ-at-risk (the left lung) by roughly 11% on average, as compared to the margin solution.\"",
        "Document: \"Feature-based methods for large scale dynamic programming. We develop a methodological framework and present a few different ways in which dynamic programming and compact representations\n can be combined to solve large scale stochastic control problems. In particular, we develop algorithms that employ two types\n of feature-based compact representations; that is, representations that involve feature extraction and a relatively simple\n approximation architecture. We prove the convergence of these algorithms and provide bounds on the approximation error. As\n an example, one of these algorithms is used to generate a strategy for the game of Tetris. Furthermore, we provide a counter-example\n illustrating the difficulties of integrating compact representations with dynamic programming, which exemplifies the shortcomings\n of certain simple approaches.\n \"",
        "1 is \"Robust Power Allocation for Amplify-and-Forward Relay Networks\", 2 is \"Replica management should be a game\"",
        "Given above information, for an author who has written the paper with the title \"Asymptotic Performance of a Censoring Sensor Network\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001083": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Polynomial-time learning of elementary formal systems':",
        "Document: \"An efficient grid layout algorithm for biological networks utilizing various biological attributes. Clearly visualized biopathways provide a great help in understanding biological systems. However, manual drawing of large-scale biopathways is time consuming. We proposed a grid layout algorithm that can handle gene-regulatory networks and signal transduction pathways by considering edge-edge crossing, node-edge crossing, distance measure between nodes, and subcellular localization information from Gene Ontology. Consequently, the layout algorithm succeeded in drastically reducing these crossings in the apoptosis model. However, for larger-scale networks, we encountered three problems: (i) the initial layout is often very far from any local optimum because nodes are initially placed at random, (ii) from a biological viewpoint, human layouts still exceed automatic layouts in understanding because except subcellular localization, it does not fully utilize biological information of pathways, and (iii) it employs a local search strategy in which the neighborhood is obtained by moving one node at each step, and automatic layouts suggest that simultaneous movements of multiple nodes are necessary for better layouts, while such extension may face worsening the time complexity.We propose a new grid layout algorithm. To address problem (i), we devised a new force-directed algorithm whose output is suitable as the initial layout. For (ii), we considered that an appropriate alignment of nodes having the same biological attribute is one of the most important factors of the comprehension, and we defined a new score function that gives an advantage to such configurations. For solving problem (iii), we developed a search strategy that considers swapping nodes as well as moving a node, while keeping the order of the time complexity. Though a na\u00efve implementation increases by one order, the time complexity, we solved this difficulty by devising a method that caches differences between scores of a layout and its possible updates.Layouts of the new grid layout algorithm are compared with that of the previous algorithm and human layout in an endothelial cell model, three times as large as the apoptosis model. The total cost of the result from the new grid layout algorithm is similar to that of the human layout. In addition, its convergence time is drastically reduced (40% reduction).\"",
        "Document: \"Integrating biopathway databases for large-scale modeling and simulation. Biopathway databases have been developed, such as KEGG and EcoCyc, that compile interaction structures of biopathways together with biological annotations. However, these biopathways are not directly editable and simulatable. Thus, we have made an application, the Biopathway Executer (BPE), that reconstructs these two major biopathway databases to XML formats of modeling and simulation platforms. BPE is developed with JAVA and has a database of executable biopathways that integrates some parts of biopathway information, KEGG and BioCyc, and other databases, e.g. MIPS and BRENDA. Currently, BPE employs the XML format (GONML) of a Hybrid Functional Petri net (HFPN) for the output. The features of HFPN are: (i) biopathways that contain discrete and continuous processes can be modeled, (ii) all biopathways that are modeled with ordinary differential equations (ODEs) can be remodeled, (iii) biopathways can be modeled while keeping readability by human. Other XML formats of biopathways, SBML and CellML are subsets of GONML. Thus, BPE can bridge major biopathway databases and major modeling and simulating softwares. To demonstrate the effectiveness/usability of BPE, four examples are created and simulated on Genomic Object Net which is based on the HFPN architecture; (i) executable KEGG maps while keeping the features of original maps, (ii) executable BioCyc maps while keeping the features of original maps, (iii) large-scale editable and simulatable KEGG metabolic pathways, (iv) a metabolic pathway with gene regulatory networks. These examples show that BPE is a useful tool for integrating biopathway databases for largescale modeling and simulation.\"",
        "1 is \"Pattern-Matching for Strings with Short Descriptions\", 2 is \"CLEVER: clique-enumerating variant finder.\"",
        "Given above information, for an author who has written the paper with the title \"Polynomial-time learning of elementary formal systems\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001122": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'NetClass: A network-based relational model for document classification.':",
        "Document: \"A Component-Based Digital Library Service for Finding Missing Documents. Providing access to the full text of cataloged articles is a highly de- sirable feature for a digital library. However, in many such systems, not all metadata records have (a direct pointer to) a corresponding full-text document. In this paper, we present a new service for finding missing documents from dig- ital library catalogs. This service is implemented as a software component in order to be readily deployable to existing systems. It works as a parameterized meta-search engine and allows digital library administrators to easily set up a search strategy, i.e., a list of existing search engines to be queried for the miss- ing document, as well as the filtering and ranking policies to be applied to the results retrieved by each engine. We present a performance evaluation of our service with respect to its efficiency and the effectiveness of a suggested search strategy for collections in the computer science and in the biomedical and life sciences fields.\"",
        "Document: \"BLOSS: Effective meta-blocking with almost no effort. \u2022We present a new solution for configuring meta-blocking with reduced user effort.\u2022A sampling is combined with an active learning approach to select informative pairs.\u2022A strategy is proposed to remove outliers, improving the recall.\u2022The experiments show a reducing in labeling effort and also a precision improvement.\"",
        "1 is \"Growth of the flickr social network\", 2 is \"Analysis of topological characteristics of huge online social networking services\"",
        "Given above information, for an author who has written the paper with the title \"NetClass: A network-based relational model for document classification.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001157": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Cluster formation over adaptive networks with selfish agents':",
        "Document: \"A Distributed Broadcasting Time-Synchronization Scheme For Wireless Sensor Networks. This paper proposes a globally distributed synchronization algorithm for wireless sensor networks. In the proposed scheme, there is a master node that controls the network synchronization and the other sensors periodically broadcast synchronization pulses; they also monitor different frequency channels in an effort to overcome the effect of channel fading.\"",
        "Document: \"An invariant matrix structure in multiantenna communications. This letter shows that the matrix structure with 2/spl times/2 Alamouti sub-blocks remains invariant under several nontrivial matrix operations, including matrix inversion, Schur complementation, Riccati recursion, triangular factorization, and QR factorization.\"",
        "1 is \"Optimal Task Scheduling by Removing Inter-Core Communication Overhead for Streaming Applications on MPSoC\", 2 is \"Detection of coded modulation signals on linear, severely distorted channels using decision-feedback noise prediction with interleaving\"",
        "Given above information, for an author who has written the paper with the title \"Cluster formation over adaptive networks with selfish agents\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001191": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Optimization of a Three Degrees of Freedom DELTA Manipulator for Well-Conditioned Workspace with a Floating Point Genetic Algorithm':",
        "Document: \"Energy Efficient Coverage Path Planning for Autonomous Mobile Robots on 3D Terrain. Coverage Path Planning (CPP) is an essential problem in many applications of robotics, including but not limited to autonomous demining and farming. Most works on CPP address time efficiency or coverage completeness in a bi-dimensional and flat environment, not taking the terrain relief into account. In this paper we use a Genetic Algorithm to optimize the solution to the CPP problem in terms of energy consumption, taking into account the constraints of natural terrains: obstacles and relief. Simulation results show that our approach is effective in reducing energy consumption of a mobile robot performing CPP.\"",
        "Document: \"Autonomous mapping for inspection of 3D structures. This paper proposes a method based on cooperation between climbing and ground robots in order to address the mapping problem for autonomous inspection of 3D structures. In the proposed method ground robots act as a mobile observer with a wide coverage for a climbing robot, to detect and estimate the size of the structure being climbed. We will present a case study in which multiple terrain robots provide the model of a structure which should be explored by a pole climbing robot. Each terrain robot is equipped with a low cost wide angle VGA camera, and some markers are fixed on the climbing robot. At each navigation step, the climbing robot and terrain robots cooperate to model a part of the structure which should be explored by the climbing robot.\"",
        "1 is \"The OWL API: A Java API for OWL ontologies\", 2 is \"Measurement And Correction Of Systematic Odometry Errors In Mobile Robots\"",
        "Given above information, for an author who has written the paper with the title \"Optimization of a Three Degrees of Freedom DELTA Manipulator for Well-Conditioned Workspace with a Floating Point Genetic Algorithm\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001314": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Annotating answer-set programs in lana*':",
        "Document: \"An Ordered Choice Logic Programming Front-End for Answer Set Solvers. Ordered Choice Logic Programming (OCLP) allows for preference- based decision-making with multiple alternatives without the burden of any form of negation. This complete absence of negation does not weaken the language as both forms (classical and as-failure) can be intuitively simulated in the language. The semantics of the language is based on the preference between alternatives, yielding both a skeptical and a credulous approach. In this paper we demon- strate how OCLPs can be translated to semi-negative logic programs such that, depending on the transformation, the answer sets of the latter correspond with the skeptical or credulous answer sets of the former. By providing such a mapping, we have a mechanism for implementing OCLP using an answer set solver like smodels or dlv.\"",
        "Document: \"LAIMA: a multi-agent platform using ordered choice logic programming. Multi-agent systems (MAS) can take many forms depending on the characteristics of the agents populating them. Amongst the more demanding properties with respect to the design and implementation of multi-agent system is how these agents may individually reason and communicate about their knowledge and beliefs, with a view to cooperation and collaboration. In this paper, we present a deductive reasoning multi-agent platform using an extension of answer set programming (ASP). We show that it is capable of dealing with the specification and implementation of the system's architecture, communication and the individual agent's reasoning capacities. Agents are represented as Ordered Choice Logic Programs (OCLP) as a way of modelling their knowledge and reasoning capacities, with communication between the agents regulated by uni-directional channels transporting information based on their answer sets. In the implementation of our system we combine the extensibility of the JADE framework with the flexibility of the OCT front-end to the Smodels answer set solver. The power of this approach is demonstrated by a multi-agent system reasoning about equilibria of extensive games with perfect information.\"",
        "1 is \"Fully abstract compositional semantics for logic programs\", 2 is \"What Is Answer Set Programming?\"",
        "Given above information, for an author who has written the paper with the title \"Annotating answer-set programs in lana*\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001338": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'On the Weighted Mean of a Pair of Strings':",
        "Document: \"Fitting of 3D circles and ellipses using a parameter decomposition approach. Many optimization processes encounter a problem in efficiently reaching a global minimum or a near global minimum. Traditional methods such as Levenberg-Marquardt algorithm and trust-region method face the problems of dropping into local minima as well. On the other hand, some algorithms such as simulated annealing and genetic algorithm try to find a global minimum but they are mostly time-consuming. Without a good initialization, many optimization methods are unable to guarantee a global minimum result. We address a novel method in 3D circle and ellipse fitting, which alleviates the optimization problem. It can not only increase the probability of getting in global minima but also reduce the computation time. Based on our previous work, we decompose the parameters into two parts: one part of parameters can be solved by an analytic or a direct method and another part has to be solved by an iterative procedure. Via this scheme, the topography of optimization space is simplified and therefore, we reduce the number of local minima and the computation time. We experimentally compare our method with the traditional ones and show superior performance.\"",
        "Document: \"Histogram-based optical flow for functional imaging in echocardiography. Echocardiographic imaging provides various challenges for medical image analysis due to the impact of physical effects in the process of data acquisition. The most significant difference to other medical data is its high level of speckle noise that makes the use of conventional algorithms difficult. Motion analysis on ultrasound (US) data is often referred to as 'Speckle Tracking' which plays an important role in diagnosis and monitoring of cardiovascular diseases and the identification of abnormal cardiac motion. In this paper we address the problem of speckle noise within US images for estimating optical flow. We demonstrate that methods which directly use image intensities are inferior to methods using local features within the US images. Based on this observation we propose an optical flow method which uses histograms as a local feature of US images and show that this approach is more robust under the presence of speckle noise than classical optical flow methods.\"",
        "1 is \"Integration Testing Using Interface Mutation\", 2 is \"A symbol spotting approach in graphical documents by hashing serialized graphs\"",
        "Given above information, for an author who has written the paper with the title \"On the Weighted Mean of a Pair of Strings\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001398": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Evaluating the trustworthiness of advice about seller agents in e-marketplaces: A personalized approach':",
        "Document: \"Analyzing the structure of argumentative discourse. Consider a discourse situation where the speaker tries to convince the hearer of a particular point of view. The first task for the hearer is to understand what it is the speaker wants him to believe--to analyze the structure of the argument being presented, before judging credibility and eventually responding.This paper describes a model for the analysis of arguments that includes:\u2022 a theory of expected coherent structure which is used to limit analysis to the reconstruction of particular transmission forms;\u2022 a theory of linguistic clues which assigns a functional interpretation to special words and phrases used by the speaker to indicate the structure of the argument;\u2022 a theory of evidence relationships which includes the demand for pragmatic analysis to accommodate beliefs not currently held.The implications of this particular design for dialogue analysis in general are thus:\u2022 structure is an important feature to extract in a representation to control the processing;\u2022 linguistic constructions can be assigned useful interpretations;\u2022 pragmatic analysis is crucial in cases where the participants differ in beliefs.\"",
        "Document: \"Resolving Plan Ambiguity for Response Generation. Recognizing the plan underlying a query aids in the generation of an appropriate response. In this paper, we address the problem of how to generate coopera- tive responses when the user's plan is ambiguous. We show that it is not always necessary to resolve the ambiguity, and provide a procedure that estimates whether the ambiguity matters to the task of formu- lating a response. If the ambiguity does matter, we propose to resolve the ambiguity by entering into a clarification dialogue with the user and provide a pro- cedure that performs this task. Together, these pro- cedures allow a question-answering system to take advantage of the interactive and collaborative nature of dialogue in recognizing plans and resolving ambi- guity.\"",
        "1 is \"Strategic Interactions In A Supply Chain Game\", 2 is \"Point-based value iteration: an anytime algorithm for POMDPs\"",
        "Given above information, for an author who has written the paper with the title \"Evaluating the trustworthiness of advice about seller agents in e-marketplaces: A personalized approach\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001418": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Topic 3: Scheduling and Load Balancing':",
        "Document: \"Implementation of a reliable remote memory pager. Traditional operating systems use magnetic disks as paging devices, even though the cost of a disk transfer measured in processor cycles continues to increase. In this paper we explore the use of remote main memory for paging. We describe the design, implementation and evaluation of a pager that uses main memory of remote workstations as a faster-than-disk paging device and provides reliability in case of single workstation failures. Our pager has been implemented as a block device driver linked to the DEC OSF/1 operating system, without any modifications to the kernel code. Using several test applications we measure the performance of remote memory paging over an Ethernet interconnection network and find it to be faster than traditional disk paging. We evaluate the performance of various reliability policies and prove their feasibility even over low bandwidth networks, like Ethernet. We conclude that the benefits of reliable remote memory paging in workstation clusters are significant today and will probably increase in the near future.\"",
        "Document: \"ITA: Innocuous Topology Awareness for Unstructured P2P Networks. One of the most appealing characteristics of unstructured P2P overlays is their enhanced self-* properties, which results from their loose, random structure. In addition, most of the algorithms which make searching in unstructured P2P systems scalable, such as dynamic querying and 1-hop replication, rely on the random nature of the overlay to function efficiently. The underlying communications network (i.e., the Internet), however, is not as randomly constructed. This leads to a mismatch between the distance of two peers on the overlay and the hosts they reside on at the IP layer, which in turn leads to its misuse. The crux of the problem arises from the fact that any effort to provide a better match between the overlay and the IP layer will inevitably lead to a reduction in the random structure of the P2P overlay, with many adverse results. With this in mind, we propose ITA, an algorithm which creates a random overlay of randomly connected neighborhoods providing topology awareness to P2P systems, while at the same time has no negative effect on the self-* properties or the operation of the other P2P algorithms. Using extensive simulations, both at the IP router level and autonomous system level, we show that ITA reduces communication latencies by as much as 50 percent. Furthermore, it not only reduces by 20 percent the number of IP network messages which is critical for ISPs carrying the burden of transporting P2P traffic, but also distributes the traffic load more evenly on the routers of the IP network layer.\"",
        "1 is \"Fast and scalable pattern matching for content filtering\", 2 is \"Load balancing for term-distributed parallel retrieval\"",
        "Given above information, for an author who has written the paper with the title \"Topic 3: Scheduling and Load Balancing\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001420": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'High throughput filtering using FPGA-acceleration':",
        "Document: \"Low-Power Split-Radix FFT Processors Using Radix-2 Butterfly Units. Split-radix fast Fourier transform (SRFFT) is an ideal candidate for the implementation of a low-power FFT processor, because it has the lowest number of arithmetic operations among all the FFT algorithms. In the design of such processors, an efficient addressing scheme for FFT data as well as twiddle factors is required. The signal flow graph of SRFFT is the same as radix-2 FFT, and therefore, th...\"",
        "Document: \"Implementing the Blue Midnight Wish Hash Function on Xilinx Virtex-5 FPGA Platform. This paper presents the design and analysis of an area efficient implementation of the SHA-3 candidate Blue Midnight Wish (BMW-256) hash function with digest size of 256 bits on an FPGA platform. Our architecture is based on a 32 bit data-path. The core functionality with finalization implementation without padding stage of BMW on Xilinx Virtex-5 FPGA requires 84 slices and two blocks of memory: one memory block to store the intermediate values and hash constants and the other memory block to store the instruction controls. The proposed implementation achieves a throughput of 56 Mpbs.\"",
        "1 is \"Modulo 3 residue checker: new results on performance and cost\", 2 is \"A Parallel Task-Based Approach to Linear Algebra\"",
        "Given above information, for an author who has written the paper with the title \"High throughput filtering using FPGA-acceleration\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001491": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Adding Unlabeled Samples to Categories by Learned Attributes':",
        "Document: \"Stating the Obvious: Extracting Visual Common Sense Knowledge. Obtaining common sense knowledge using current information extraction techniques is extremely challenging. In this work, we instead propose to derive simple common sense statements from fully annotated object detection corpora such as the Microsoft Common Objects in Context dataset. We show that many thousands of common sense facts can be extracted from such corpora at high quality. Furthermore, using WordNet and a novel submodular k-coverage formulation, we are able to generalize our initial set of common sense assertions to unseen objects and uncover over 400k potentially useful facts.\"",
        "Document: \"Target-driven Visual Navigation in Indoor Scenes using Deep Reinforcement Learning. Two less addressed issues of deep reinforcement learning are (1) lack of generalization capability to new goals, and (2) data inefficiency, i.e., the model requires several (and often costly) episodes of trial and error to converge, which makes it impractical to be applied to real-world scenarios. In this paper, we address these two issues and apply our model to target-driven visual navigation. To address the first issue, we propose an actor-critic model whose policy is a function of the goal as well as the current state, which allows better generalization. To address the second issue, we propose the AI2-THOR framework, which provides an environment with high-quality 3D scenes and a physics engine. Our framework enables agents to take actions and interact with objects. Hence, we can collect a huge number of training samples efficiently. We show that our proposed method (1) converges faster than the state-of-the-art deep reinforcement learning methods, (2) generalizes across targets and scenes, (3) generalizes to a real robot scenario with a small amount of fine-tuning (although the model is trained in simulation), (4) is end-to-end trainable and does not need feature engineering, feature matching between frames or 3D reconstruction of the environment.\"",
        "1 is \"The quality of training sample estimates of the Bhattacharyya coefficient\", 2 is \"Unbiased look at dataset bias\"",
        "Given above information, for an author who has written the paper with the title \"Adding Unlabeled Samples to Categories by Learned Attributes\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001571": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'On subrecursiveness in weak combinatory logic':",
        "Document: \"Improving reachability analysis of infinite state systems by specialization. We consider infinite state reactive systems specified by using linear constraints over the integers, and we address the problem of verifying safety properties of these systems by applying reachability analysis techniques. We propose a method based on program specialization, which improves the effectiveness of the backward and forward reachability analyses. For backward reachability our method consists in: (i) specializing the reactive system with respect to the initial states, and then (ii) applying to the specialized system a reachability analysis that works backwards from the unsafe states. For forward reachability our method works as for backward reachability, except that the role of the initial states and the unsafe states are interchanged. We have implemented our method using the MAP transformation system and the ALV verification system. Through various experiments performed on several infinite state systems, we have shown that our specialization-based verification technique considerably increases the number of successful verifications without significantly degrading the time performance.\"",
        "Document: \"A Folding Rule for Eliminating Existential Variables from Constraint Logic Programs. The existential variables of a clause in a constraint logic program are the variables which occur in the body of the clause and not in its head. The elimination of these variables is a transformation technique which is often used for improving program efficiency and verifying program properties. We consider a folding transformation rule which ensures the elimination of existential variables and we propose an algorithm for applying this rule in the case where the constraints are linear inequations over rational or real numbers. The algorithm combines techniques for matching terms modulo equational theories and techniques for solving systems of linear inequations. Through some examples we show that an implementation of our folding algorithm has a good performance in practice.\"",
        "1 is \"A Hypergraph-based Framework for Visual Interaction with Databases\", 2 is \"Verification of java bytecode using analysis and transformation of logic programs\"",
        "Given above information, for an author who has written the paper with the title \"On subrecursiveness in weak combinatory logic\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001580": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A Structure Preserving Database Encryption Scheme':",
        "Document: \"A Structure Preserving Database Encryption Scheme. A new simple and efficient database encryption scheme is presented. The new scheme enables encrypting the entire content of the database without changing its structure. In addition, the scheme suggests how to convert the conventional database index to a secure index on the encrypted database so that the time complexity of all queries is maintained. No one with access to the encrypted database can learn anything about its content without having the encryption key.\"",
        "Document: \"Schemes for Privately Computing Trust and Reputation. Trust and Reputation systems in distributed environments attain widespread interest as online communities are becoming an inherent part of the daily routine of Internet users. Several models for Trust and Reputation have been suggested recently, among them the Knots model [8]. The Knots model provides a member of a community with a method to compute the reputation of other community members. Reputation in this model is subjective and tailored to the taste and choices of the computing member and those members that have similar views, i.e. the computing member's Trust-Set. A discussion on privately computing trust in the Knots model appears in [16]. The present paper extends and improves [16] by presenting three efficient and private protocols to compute trust in trust based reputation systems that use any trust-sets based model. The protocols in the paper are rigorously proved to be private against a semi-honest adversary given standard assumptions on the existence of an homomorphic, semantically secure, public key encryption system. The protocols are analyzed and compared in terms of their privacy characteristics and communication complexity.\"",
        "1 is \"Frequent Subtree Mining - An Overview\", 2 is \"Sampling attack against active learning in adversarial environment\"",
        "Given above information, for an author who has written the paper with the title \"A Structure Preserving Database Encryption Scheme\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001622": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Evaluating Direct Manipulation Operations For Constraint-Based Layout':",
        "Document: \"Climbing the ladder: capability maturity model integration level 3. This article details the attempt to form a complete workflow model for an information and communication technologies (ICT) company in order to achieve a capability maturity model integration (CMMI) maturity rating of 3. During this project, business processes across the company's core and auxiliary sectors were documented and extended using modern enterprise modelling tools and a The Open Group Architectural Framework (TOGAF) methodology. Different challenges were encountered with regard to process customisation and tool support for enterprise modelling. In particular, there were problems with the reuse of process models, the integration of different project management methodologies and the integration of the Rational Unified Process development process framework that had to be solved. We report on these challenges and the perceived effects of the project on the company. Finally, we point out research directions that could help to improve the situation in the future.\"",
        "Document: \"Domain Specific High-Level Constraints for User Interface Layout. We present the Auckland Layout Model (ALM), a constraint-based technique for specifying 2D layout as it is used for arranging the controls in a graphical user interface (GUI). Most GUI frameworks offer layout managers that are basically adjustable tables; often adjacent table cells can be merged. In the ALM, the focus switches from the table cells to vertical and horizontal tabulators between the cells. On the lowest level of abstraction, the model applies linear constraints, and an optimal layout is calculated using linear programming. However, bare linear programming makes layout specification cumbersome and unintuitive, especially for GUI domain experts who are often not used to such mathematical formalisms. In order to improve the usability of the model, ALM offers several other layers of abstraction that make it possible to define common GUI layout more easily. In the domain of user interfaces it is important that specifications are not over-constrained, therefore ALM introduces soft constraints, which are automatically translated to appropriate hard linear constraints and terms in the objective function. GUIs are usually composed of rectangular areas containing controls, therefore ALM offers an abstraction for such areas. Dynamic resizing behavior is very important for GUIs, hence areas have domain-specific parameters specifying their minimum, maximum and preferred sizes. From such definitions, hard and soft constraints are automatically derived. A third level of abstraction allows designers to arrange GUIs in a tabular fashion, using abstractions for columns and rows, which offer additional parameters for ordering and alignment. Row and column definitions are used to automatically generate definitions from lower levels of abstraction, such as hard and soft constraints and areas. Specifications from all levels of abstraction can be consistently combined, offering GUI developers a rich set of tools that is much closer to their needs than pure linear constraints. Incremental computation of solutions makes constraint solving fast enough for near real-time use.\"",
        "1 is \"Left-over Windows Cause Window Clutter... But What Causes Left-over Windows?\", 2 is \"Reaching for objects in VR displays: lag and frame rate\"",
        "Given above information, for an author who has written the paper with the title \"Evaluating Direct Manipulation Operations For Constraint-Based Layout\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001651": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'M-AID: An adaptive middleware built upon anomaly detectors for intrusion detection and rational response':",
        "Document: \"An Eight-Approximation Algorithm for Computing Rooted Three-Vertex Connected Minimum Steiner Networks. For a given undirected (edge) weighted graph G = (V, E), a terminal set S \u2286 V and a root r \u2208 S, the rooted k-vertex connected minimum Steiner network (kVSMNr) problem requires to construct a minimum-cost subgraph of G such that each terminal in S {R} is k-vertex connected to \u03c4. As an important problem in survivable network design, the kVSMN\u03c4 problem is known to be NP-hard even when k 1/4 1 [14]. For k 1/4 3 this paper presents a simple combinatorial eight-approximation algorithm, improving the known best ratio 14 of Nutov [20]. Our algorithm constructs an approximate 3VSMN\u03c4 through augmenting a two-vertex connected counterpart with additional edges of bounded cost to the optimal. We prove that the total cost of the added edges is at most six times of the optimal by showing that the edges in a 3VSMN\u03c4 compose a subgraph containing our solution in such a way that each edge appears in the subgraph at most six times.\"",
        "Document: \"Encoder Rate Control for Pixel-Domain Distributed Video Coding without Feedback Channel. Distributed Video Coding (DVC) is a new video coding technique which has low encoder complexity and good error resilience. Rate control is one of the most important problems in DVC. In order to allocate an optimal number of parity bits for the current frame,most distributed video coders use a feedback channel(FC) to perform rate control at the decoder. However,using feedback channel can increase the decoding complexity and result in transmission delay. To solve these problems, we propose a novel encoder rate control (ERC) algorithm for pixel-domain distributed video (PDDV) coder without feedback channel at the cost of slightly increased encoder complexity. The experiment results show that the performance obtained using our ERC algorithm is comparable to that obtained using the decoder rate control (DRC)algorithm.\"",
        "1 is \"A Hierarchical Projection Pursuit Clustering Algorithm\", 2 is \"Privacy-preserving set operations\"",
        "Given above information, for an author who has written the paper with the title \"M-AID: An adaptive middleware built upon anomaly detectors for intrusion detection and rational response\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001714": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Mapping Partially Observable Features from Multiple Uncertain Vantage Points':",
        "Document: \"Exactly Sparse Extended Information Filters for Feature-based SLAM. Recent research concerning the Gaussian canonical form for Simultaneous Localization and Mapping (SLAM) has given rise to a handful of algorithms that attempt to solve the SLAM scalability problem for arbitrarily large environments. One such estimator that has received due attention is the Sparse Extended Information Filter (SEIF) proposed by Thrun et al., which is reported to be nearly constant time, irrespective of the size of the map. The key to the SEIF's scalability is to prune weak links in what is a dense information (inverse covariance) matrix to achieve a sparse approximation that allows for efficient, scalable SLAM. We demonstrate that the SEIF sparsification strategy yields error estimates that are overconfident when expressed in the global reference frame, while empirical results show that relative map consistency is maintained. In this paper, we propose an alternative scalable estimator based on an information form that maintains sparsity while preserving consistency. The paper describes a method for controlling the population of the information matrix, whereby we track a modified version of the SLAM posterior, essentially by ignoring a small fraction of temporal measurements. In this manner, the Exactly Sparse Extended Information Filter (ESEIF) performs inference over a model that is conservative relative to the standard Gaussian distribution. We compare our algorithm to the SEIF and standard EKF both in simulation as well as on two nonlinear datasets. The results convincingly show that our method yields conservative estimates for the robot pose and map that are nearly identical to those of the EKF.\"",
        "Document: \"Towards consistent visual-inertial navigation. Visual-inertial navigation systems (VINS) have prevailed in various applications, in part because of the complementary sensing capabilities and decreasing costs as well as sizes. While many of the current VINS algorithms undergo inconsistent estimation, in this paper we introduce a new extended Kalman filter (EKF)-based approach towards consistent estimates. To this end, we impose both state-transition and obervability constraints in computing EKF Jacobians so that the resulting linearized system can best approximate the underlying nonlinear system. Specifically, we enforce the propagation Jacobian to obey the semigroup property, thus being an appropriate state-transition matrix. This is achieved by parametrizing the orientation error state in the global, instead of local, frame of reference, and then evaluating the Jacobian at the propagated, instead of the updated, state estimates. Moreover, the EKF linearized system ensures correct observability by projecting the most-accurate measurement Jacobian onto the observable subspace so that no spurious information is gained. The proposed algorithm is validated by both Monte-Carlo simulation and real-world experimental tests.\"",
        "1 is \"Deriving and matching image fingerprint sequences for mobile robot localization\", 2 is \"Structure from Motion Causally Integrated Over Time\"",
        "Given above information, for an author who has written the paper with the title \"Mapping Partially Observable Features from Multiple Uncertain Vantage Points\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001718": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'An orthogonal local search genetic algorithm for the design and optimization of power electronic circuits':",
        "Document: \"Parallel Particle Swarm Optimization with Adaptive Asynchronous Migration Strategy. This paper proposes a parallel particle swarm optimization (PPSO) by dividing the search space into sub-spaces and using different swarms to optimize different parts of the space. In the PPSO framework, the search space is regarded as a solution vector and is divided into two sub-vectors. Two cooperative swarms work in parallel and each swarm only optimizes one of the sub-vectors. An adaptive asynchronous migration strategy (AAMS) is designed for the swarms to communicate with each other. The PPSO benefits from the following two aspects. First, the PPSO divides the search space and each swarm can focus on optimizing a smaller scale problem. This reduces the problem complexity and makes the algorithm promising in dealing with large scale problems. Second, the AAMS makes the migration adapt to the search environment and results in a very timing and efficient communication fashion. Experiments based on benchmark functions have demonstrated the good performance of the PPSO with AAMS on both solution accuracy and convergence speed when compared with the traditional serial PSO (SPSO) and the PPSO with fixed migration frequency.\"",
        "Document: \"Dichotomy Guided Based Parameter Adaptation for Differential Evolution. Differential evolution (DE) is an efficient and powerful population-based stochastic evolutionary algorithm, which evolves according to the differential between individuals. The success of DE in obtaining the optima of a specific problem depends greatly on the choice of mutation strategies and control parameter values. Good parameters lead the individuals towards optima successfully. The increasing of the success rate (the ratio of entering the next generation successfully) of population can speed up the searching. Adaptive DE incorporates success-history or population-state based parameter adaptation. However, sometimes poor parameters may improve individual with small probability and are regarded as successful parameters. The poor parameters may mislead the parameter control. So, in this paper, we propose a novel approach to distinguish between good and poor parameters in successful parameters. In order to speed up the convergence of algorithm and find more \"good\" parameters, we propose a dichotomy adaptive DE (DADE), in which the successful parameters are divided into two parts and only the part with higher success rate is used for parameter adaptation control. Simulation results show that DADE is competitive to other classic or adaptive DE algorithms on a set of benchmark problem and IEEE CEC 2014 test suite.\n\n\"",
        "1 is \"Continuous Gaussian Estimation of Distribution Algorithm.\", 2 is \"The Kernel-Adatron Algorithm: A Fast and Simple Learning Procedure for Support Vector Machines\"",
        "Given above information, for an author who has written the paper with the title \"An orthogonal local search genetic algorithm for the design and optimization of power electronic circuits\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001757": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Memory Assignment for Multiprocessor Caches through Grey Coloring':",
        "Document: \"A Multi-Mode Power Gating Structure for Low-Voltage Deep-Submicron CMOS ICs. Most existing power gating structures provide only one power-saving mode. We propose a novel power gating structure that supports both a cutoff mode and an intermediate power-saving and data-retaining mode. Experiments with test structures fabricated in 0.13-mum CMOS bulk technology show that our power gating structure yields an expanded design space with more power-performance tradeoff alternativ...\"",
        "Document: \"Analytical macromodeling for high-level power estimation. 1 Abstract In this paper, we present a new analytical macro-modeling technique for high-level power estimation. Our technique is based on a parameterizable analytical model that relies exclusively on statistical information of the circuit' s pri- mary inputs. During estimation, the statistics of the re- quired metrics are extracted from the input stream and a power estimate is obtained by evaluating a model function that has been characterized in advance. Thus, our model yields power estimates within seconds, since it does not rely on the statistics of the circuit' s primary outputs and, consequently, does not perform any simulation during es- timation. Moreover, our model achieves significantly bet- ter accuracy than previous macro-modeling approaches, because it takes into account both spatial and temporal correlations in the input stream. In experiments with ISCAS-85 combinational circuits, the average absolute relative error of our power macro- modeling technique was at most 3%. For all but one cir- cuit in our test suite, the worst-case error was at most 19.5%. In comparison with power estimates for a ripple- carry adder family that were obtained using Spice, the av- erage absolute and worst-case errors of our model' s esti- mates were 5.1% and 19.8%, respectively. In addition to power dissipation, our macro-modeling technique can be used to estimate the statistics of a cir- cuit' s primary outputs. Our experiments with ISCAS-85 circuits yield very low average errors. Thus, our tech- nique is suitable for fast and accurate power estimation in core-based system with pre-characterized blocks. Once the metrics of the primary inputs are known, the power dissipation of the entire system can be estimated by sim- ply propagating this information through the blocks using their corresponding model functions.\"",
        "1 is \"Fast Multilevel Implementation Of Recursive Spectral Bisection For Partitioning Unstructured Problems\", 2 is \"Dense image registration through MRFs and efficient linear programming.\"",
        "Given above information, for an author who has written the paper with the title \"Memory Assignment for Multiprocessor Caches through Grey Coloring\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001855": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Perspectives on cognitive computing and applications: Summary of plenary Panel I of IEEE ICCI'10.':",
        "Document: \"Automatic image annotation via local sparse coding. Sparse coding is an active research topic in machine learning and signal processing community. In this paper, we propose a novel local sparse model for multi-label image annotation. Existing feature descriptors and extraction algorithms pay less attention to semantic information and extracted feature dimension usually is high, which leads to heavy computation. Noise and redundant information often reduce the performance of sparse model. To address these issues, we combine label and visual information for feature selection while most previous work only utilizes labels and ignores visual information itself. First of all, we make use of label sets to seek images neighbor relations and generate the Gaussian kernel matrix over these neighbor images, then use LLP(Local Learning Projection) algorithm to get minimal local estimation error. After that, for each query image, we find its K nearest neighbors in the transformed space and use these neighbors to reconstruct it via sparse coding. Moreover, during coding, we penalize the corresponding reconstruction coefficients to implicitly reflect the neighbor relations. Finally, propagating tags from training data to test data. Image annotation experiments on the Corel5k dataset show the performance of our approach is comparable to several state-of-the-art algorithms.\"",
        "Document: \"Unstructured P2p-Enabled Service Discovery In The Cloud Environment. As the Cloud computing appears to be part of the mainstream computing in a few years, the number of the services it provides, its users, and the requests of these services will be on the rise accordingly. To deliver satisfactory experience to Cloud users, it is essential that highly scalable techniques for service discovery should be available. We embarked on a preliminary study, in Cloud environments, on service discovery by adopting an unstructured P2P technique. In the context of Cloud computing, we start by examining proposed and deployed solutions to service discovery, discuss the methodology of developping efficient mechanisms for service description, description indexing, and query routing, and finally identify open issues.\"",
        "1 is \"Semi-supervised and unsupervised extreme learning machines.\", 2 is \"Formal Description of the Cognitive Process of Decision Making\"",
        "Given above information, for an author who has written the paper with the title \"Perspectives on cognitive computing and applications: Summary of plenary Panel I of IEEE ICCI'10.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001872": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Energy-Efficient Routing in the Broadcast Communication Model':",
        "Document: \"Common data security network (CDSN). We present the idea of using a separate network that processes and enforces security in a data network. We briefly discuss various components of such a network, called common data security network (CDSN). We use the example of the IEEE 802.11i to determine one of the link level metrics of the proposed network, the fractional overhead for IEEE 802.1X and temporal key integrity protocol (TKIP).\"",
        "Document: \"Energy-Efficient Routing in the Broadcast Communication Model. The Broadcast Communication Model (BCM, for short) is a distributed system with no central arbiter populated by p stations denoted by S(1), S(2), ..., S(p) that communicate by transmitting messages on a communication channel. The stations are assumed to have the computing power of a laptop computer and to be synchronous\u9a74in particular, they all run the same program, albeit on different data. We assume that a station is expending power while transmitting or receiving messages. As it turns out, one of the most effective energy-saving strategies is to mandate individual stations to power their transceiver off (i.e., go to sleep) whenever they are not transmitting or receiving messages. Suppose that the p stations of the BCM store collectively n items such that station S(i), (1\"",
        "1 is \"Distributed clustering for ad hoc networks\", 2 is \"A fast bit-vector algorithm for approximate string matching based on dynamic programming\"",
        "Given above information, for an author who has written the paper with the title \"Energy-Efficient Routing in the Broadcast Communication Model\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001884": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Collaboration through computation: incorporating trust model into service-based software systems':",
        "Document: \"The Level of Decomposition Impact on Component Fault Tolerance. In fault tolerant software systems, the Level of Decomposition (LoD) where design diversity is applied has a major impact on software system reliability. By disregarding this impact, current fault tolerance techniques are prone to reliability decrease due to the inappropriate application level of design diversity. In this paper, we quantify the effect of the LoD on system reliability during software recomposition when the functionalities of the system are redistributed across its components. We discuss the LoD in fault tolerant software architectures according to three component failure transitions: component failure occurrence, component failure propagation, and component failure impact. We illustrate the component aspects that relate the LoD to each of these failure transitions. Finally, we quantify the effect of the LoD on system reliability according to a series of decomposition and/or merge operations that may occur during software recomposition.\"",
        "Document: \"Droid Mood Swing (DMS): Automatic Security Modes Based on Contexts. Smartphones are becoming ubiquitous and we use them for different types of tasks. One problem of using the same device for multiple tasks is that each task requires a different security model. To address this problem, we introduce Droid Mood Swing (DMS), an operating system component that applies different security policies to detected security modes automatically. DMS uses a context manager that tracks the context of the phone from the available sensors. DMS then determines the security mode from the contexts and can impose a number of security measures, namely fine-grained permissions, an intent firewall, a context-aware SD card filesystem, and a permission verification system. The permission verification system uses machine learning techniques to detect suspicious apps and anomalous permission requests. DMS also provides an API that enables third-party developers to make their apps behave differently in different modes. DMS is designed especially for end users and does not compromise the usability of the phone. Device vendors will be able to control configurations (a switching logic and security policies) of the modes through DMS. We implement DMS using the Android Open Source Project (AOSP) and evaluate it in terms of portability, functionality, security, and operational overheads. The evaluation results show that DMS offers a more secure smartphone operating system without incurring any noticeable overhead.\"",
        "1 is \"A buddy model of security for mobile agent communities operating in pervasive scenarios\", 2 is \"Early Detection of Security Misconfiguration Vulnerabilities in Web Applications\"",
        "Given above information, for an author who has written the paper with the title \"Collaboration through computation: incorporating trust model into service-based software systems\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001919": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Evaluation of intra-group optimistic data replication in P2P groupware systems':",
        "Document: \"Effects of Selection Operators for Mesh Router Placement in Wireless Mesh Networks Considering Weibull Distribution of Mesh Clients. In this paper, we evaluate the performance of WMN-GA system for node placement problem in WMNs. For evaluation, we consider Weibull Distribution of mesh clients and different selection operators. The population size is considered 64 and the number of generation 200. For evaluation, we consider the giant component and the number of covered users metrics. The simulation results show that for small and medium grid sizes and big density of mesh clients WMN-GA performs better for Linear Ranking. For big grid sizes and low density of mesh clients WMN-GA performs better for Exponential Ranking.\"",
        "Document: \"Mesh Router Node Placement in Wireless Mesh Networks Considering Different Initial Router Placement Methods. In this paper, we deal with connectivity and coverage problems of Wireless Mesh Networks (WMNs). Because these problems are known to be NP-Hard, we propose and implement a system based on GA. We call the proposed system: WMN-GA. We evaluate the performance of WMN-GA considering Giant Component and Covered Mesh Clients metrics. We evaluate and compare the performance of different initial router node placement methods in a wireless mesh network considering different distributions of mesh clients. Our WMN-GA System have a good behavior and can find the best initial router node placement methods for every distribution of mesh clients.\"",
        "1 is \"Wide Area Service Discovery and Adaptation for Mobile Clients in Networks with Ad Hoc Behaviour\", 2 is \"Communication and Coordination in Wireless Sensor and Actor Networks\"",
        "Given above information, for an author who has written the paper with the title \"Evaluation of intra-group optimistic data replication in P2P groupware systems\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001944": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'An Efficient Modeling and Execution Framework for Complex Systems Development':",
        "Document: \"Model driven resource usage simulation for critical embedded systems. Facing a growing complexity, embedded systems design relies on model-based approaches to ease the exploration of a design space. A key aspect of such exploration is performance evaluation, mainly depending on usage of the hardware resources. In model-driven engineering, hardware resources usage is often approximated by static properties. In this paper, we propose an extensible modeling framework, to describe with different levels of detail the hardware resource usage. Our method relies on the AADL to describe the whole system, and SystemC to refine the execution platform description. In this paper we expose how we generate and compose SystemC models from the execution platform model described in AADL. We also present promising experimental results obtained on an avionics use-case.\"",
        "Document: \"An Efficient Modeling and Execution Framework for Complex Systems Development. In this paper, we present different modeling and execution frameworks that allow us to efficiently analyze, design and verify complex systems, mainly to cope with the specific concerns of the Real-time and embedded systems (RTE) domain. First we depict a UML /MARTE based methodology for executable RTE systems modeling with a framework and its underlying model transformations required to execute UML models conforming to the MARTE standard. The advantages of adopting a more generic action language with formal features are highlighted, in order to raise the level of abstraction with formal features. Then, we investigate how MARTE, with its Time Model facilities, can be made to represent faithfully AADL periodic/aperiodic tasks communicating through event or data ports, in an approach to end-to-end flow latency analysis. An analytical framework allows us to optimize port-based communication by generating a run time executive that utilizes shared data areas where appropriate, while ensuring the timing semantic assumed by the control application. An analysis of the AADL mode change protocol is also provided, exposing a translation process that takes as an input an AADL model and produces as an output a time Petri net. We show how an AADL model transformation provides a formal model for model checking activities and we suggest that model transformation provides useful support to improve the integration of formal verification in an industrial engineering process. As a case study we use an implementation of a UDP /IP protocol stack.\"",
        "1 is \"Towards a Cost-Effective Estimation of Uncaught Exceptions in SML Programs\", 2 is \"Preemptive Job-Shop Scheduling Using Stopwatch Automata\"",
        "Given above information, for an author who has written the paper with the title \"An Efficient Modeling and Execution Framework for Complex Systems Development\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002021": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Unified Stochastic Geometry Modeling and Analysis of Cellular Networks in LOS/NLOS and Shadowed Fading.':",
        "Document: \"Distributed Collaborative Beamforming Design for Maximized Throughput in Interfered and Scattered Environments. In this paper, we consider a dual-hop communication from a source surrounded by MI interferences to a receiver, through a wireless network comprised of K independent terminals. In the first time slot, all sources send their signals to the network while, in the second time slot, the terminals multiply the received signal by their respective beamforming weights and forward the resulting signals to the receiver. We design these weights so as to minimize the interferences plus noises\u2019 powers while maintaining the received power from the source to a constant level. We show, however, that they are intractable in closed-form due to the complexity of the polychromatic channels arising from the presence of scattering. By resorting to a two-ray channel approximation proved valid at relatively low angular spread (AS) values, we are able to derive the new optimum weights and prove that they could be locally computed at each terminal, thereby complying with the distributed feature of the network of interest. The so-obtained bichromatic distributed collaborative beamforming (B-DCB) is then analyzed and compared in performance to the monochromatic CB (MCB), whose design does not account for scattering, and the optimal CSI-based CB (OCB). Comparisons are made under both ideal and real-world conditions where we account for implementation errors and the overhead incurred by each CB solution. They reveal that the proposed BDCB always outperforms MCB in practice; and that it approaches OCB in lightly- to moderately-scattered environments under ideal conditions and outperforms it under real-world conditions even in highly-scattered environments. In such conditions, indeed, the BDCB operational regions in terms of AS values over which it is favored against OCB could reach until 50 degrees and, hence, cover about the entire span of AS values.\"",
        "Document: \"Accurate Range-Free Node Localization In Mobile Ad Hoc Networks. In this paper, we propose a novel range-free localization algorithm tailored for mobile ad hoc networks (MANET)s. In contrast to most existing range-free techniques, we take into account the nodes mobility and hence enable the nodes to estimate their positions using solely their locally-available information, thereby avoiding any unnecessary costs in overhead and power that would have been incurred if information exchange between nodes were required. Furthermore, we show that the proposed algorithm outperforms in accuracy the best representative range-free algorithms. In contrast to the latter, it is able to compensate the nodes mobility effects when the nodes' speeds are moderate.\"",
        "1 is \"Multichannel Eigenspace Beamforming in a Reverberant Noisy Environment With Multiple Interfering Speech Signals\", 2 is \"Power Control for D2D Underlaid Cellular Networks: Modeling, Algorithms, and Analysis\"",
        "Given above information, for an author who has written the paper with the title \"Unified Stochastic Geometry Modeling and Analysis of Cellular Networks in LOS/NLOS and Shadowed Fading.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002128": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Analyses of Splittable Amplifier Technique and Cancellation of Memory Effect for Opamp Sharing.':",
        "Document: \"Mismatch-Aware Common-Centroid Placement for Arbitrary-Ratio Capacitor Arrays Considering Dummy Capacitors. Switched capacitors are commonly used in analog circuits to increase the accuracy of analog signal processing and lower power consumption. To take full advantage of switched capacitors, it is very important to achieve accurate capacitance ratios in the layout of the capacitor arrays, which are affected by systematic and random mismatches. A good capacitor placement should have a common-centroid structure with the highest possible degree of dispersion to mitigate mismatches. Several dummy units should be inserted to make the placement shape more square and compact. This paper proposes a simulated-annealing-based approach for mismatch-aware common-centroid placement under the above constraints. A pair-sequence representation is used to record a placement, and a couple of associated operations are developed to find better solutions. The experimental results show that the proposed placements achieve smaller oxide-gradient-induced mismatch and larger overall correlation coefficients (i.e., higher degree of dispersion) than those of previous works.\"",
        "Document: \"A Power-Efficient Sizing Methodology Of Sar Adcs. Analog-to-digital converter (ADC) is a vital component for modern electronic systems, but designing an ADC usually takes much time and effort. Though several synthesis methods have been presented for analog circuits, there exists limited works focusing on ADC design automation. In this paper, we propose a systematic sizing methodology to minimize the power consumption for successive approximation register (SAR) ADCs in transistor level. This method manipulates the characteristics of SAR ADC to develop an efficient searching algorithm for shortening the sizing time. The time complexity of our method is O (2 log(2) \\S\\), where \\S\\ is the number of candidates in the searching space. According to the proposed sizing flow, we develop a sizing tool which is independent of manufacturing process and is able to minimize power consumption for SAR ADCs. By using the developed sizing tool, a proof-of-concept prototype was carried out within only 15 minutes and fabricated in a 1P4M 0.11 mu m process. The measurement results show the prototype demonstrates a high competitiveness compared to other state-of-the-art works on performance and power efficiency.\"",
        "1 is \"New fast binary pyramid motion estimation for MPEG2 and HDTV encoding\", 2 is \"An Over-60 dB True Rail-to-Rail Performance Using Correlated Level Shifting and an Opamp With Only 30 dB Loop Gain\"",
        "Given above information, for an author who has written the paper with the title \"Analyses of Splittable Amplifier Technique and Cancellation of Memory Effect for Opamp Sharing.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002136": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'QoS-adaptive bandwidth scheduling in continuous media streaming':",
        "Document: \"Topology Control For Increasing Connectivity In Cooperative Wireless Ad Hoc Networks. We propose a novel topology control scheme that reduces the transmission power of nodes and increases the network connectivity, based on the fact that Cooperative Communication (CC) technology can bridge disconnected networks Simulation results demonstrate that our scheme greatly increases the connectivity for a given transmission power, compared to other topology control schemes\"",
        "Document: \"DSML: Dual Signal Metrics for Localization in Wireless Sensor Networks. In wireless sensor networks and wireless ad-hoc networks, localization systems have used diverse signal metrics such as Received Signal Strength Indicator (RSSI) and Time Difference of Arrival (TDoA) for accurate assignment of a node position. We propose a novel scheme that applies two signal metrics, which are TDoA and RSSI exclusively, into time- based positioning scheme (TPS). For energy-efficient coverage extension, the proposed scheme uses range check technique that reduces the communication energy consumption of nodes. With two location information of neighbor nodes, the node can calculate two candidate positions through bilateration. Without an additional beacon message reception, range check is applied to find the unique position between two candidate positions. Range check also can be carried out collaboratively in general environment with the information of two-hop neighbor nodes. At the performance evaluation, we analyze and test the reduced communication cost of nodes in the extended area. Also, it is shown that the ratio of unique position assignment is increased in the general environment by range check technique.\"",
        "1 is \"NeXt generation/dynamic spectrum access/cognitive radio wireless networks: a survey\", 2 is \"Efficient and Effective Clustering Methods for Spatial Data Mining\"",
        "Given above information, for an author who has written the paper with the title \"QoS-adaptive bandwidth scheduling in continuous media streaming\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002159": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'C4PS - helping facebookers manage their privacy settings':",
        "Document: \"Secrecy forever? Analysis of Anonymity in Internet-based Voting Protocols. Internet-based elections have become more and more popular in the last five years. In future a lot of people are probably going to cast their ballot over the Internet. This could be a great benefit. But at the same time new possibilities to manipulate the election will arise: Besides other network specific attacks, sniffing of the network traffic becomes interesting. This paper identifies the problems with respect to temporal unlimited election secrecy against sniffing on the network. The problem is the voter's IP address and the fact that in practice, there is no anonymous communication channel. Thus the voter's anonymity can be broken in future. However, the attacker will not be able to proof his knowledge about the voter's decision. We will figure out how this vulnerability could be prevented with state of the art technologies.\"",
        "Document: \"An investigation into the usability of electronic voting systems for complex elections. Many studies on electronic voting evaluate their usability in the context of simple elections. Complex elections, which take place in many European countries, also merit attention. The complexity of the voting process, as well as that of the tallying and verification of the ballots, makes usability even more crucial in this context. Complex elections, both paper-based and electronic, challenge voters and electoral officials to an unusual extent. In this work, we present two studies of an electronic voting system that is tailored to the needs of complex elections. In the first study, we evaluate the effectiveness of the ballot design with respect to motivating voters to verify their ballot. Furthermore, we identify factors that motivate voters to verify, or not to verify, their ballot. The second study also addresses the effectiveness of the ballot design in terms of verification, but this time from the electoral officials\u2019 perspective. Last, but not least, we evaluate the usability of the implemented EasyVote prototype from both the voter and electoral official perspectives. In both studies, we were able to improve effectiveness, without impacting efficiency and satisfaction. Despite these usability improvements, it became clear that voters who trusted the electronic system were unlikely to verify their ballots. Moreover, these voters failed to detect the \u201cfraudulent\u201d manipulations. It is clear that well-formulated interventions are required in order to encourage verification and to improve the detection of errors or fraudulent attempts.\"",
        "1 is \"Secrecy Outage in MISO Systems With Partial Channel Information\", 2 is \"Verifiability, Privacy, and Coercion-Resistance: New Insights from a Case Study\"",
        "Given above information, for an author who has written the paper with the title \"C4PS - helping facebookers manage their privacy settings\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002202": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Non-blind deblurring of structured images with geometric deformation':",
        "Document: \"Non-negative low rank and sparse graph for semi-supervised learning. Constructing a good graph to represent data structures is critical for many important machine learning tasks such as clustering and classification. This paper proposes a novel non-negative low-rank and sparse (NNLRS) graph for semi-supervised learning. The weights of edges in the graph are obtained by seeking a nonnegative low-rank and sparse matrix that represents each data sample as a linear combination of others. The so-obtained NNLRS-graph can capture both the global mixture of subspaces structure (by the low rankness) and the locally linear structure (by the sparseness) of the data, hence is both generative and discriminative. We demonstrate the effectiveness of NNLRS-graph in semi-supervised classification and discriminative analysis. Extensive experiments testify to the significant advantages of NNLRS-graph over graphs obtained through conventional means.\"",
        "Document: \"Fast Low-Rank Subspace Segmentation. Subspace segmentation is the problem of segmenting (or grouping) a set of \\($\\) data points into a number of clusters, with each cluster being a (linear) subspace. The recently established algorithms such as Sparse Subspace Clustering (SSC), Low-Rank Representation (LRR) and Low-Rank Subspace Segmentation (LRSS) are effective in terms of segmentation accuracy, but computationally inefficient as they possess a complexity of \\($O(n^{3}\\), which is too high to afford for the case where \\($\\) is very large. In this paper we devise a fast subspace segmentation algorithm with complexity of \\($O(n\\log (n)\\). This is achieved by firstly using partial Singular Value Decomposition (SVD) to approximate the solution of LRSS, secondly utilizing Locality Sensitive Hashing (LSH) to build a sparse affinity graph that encodes the subspace memberships, and finally adopting a fast Normalized Cut (NCut) algorithm to produce the final segmentation results. Besides of high efficiency, our algorithm also has comparable effectiveness as the original LRSS method.\"",
        "1 is \"Variational Inference for Bayesian Mixture of Factor Analysers\", 2 is \"Existence of solutions to a class of nonlinear convergent chattering-free sliding mode control systems\"",
        "Given above information, for an author who has written the paper with the title \"Non-blind deblurring of structured images with geometric deformation\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002369": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Towards fault tolerance pervasive computing':",
        "Document: \"Incremental DNA sequence analysis in the cloud. In this paper, we propose to demonstrate a \"stream-as-you-go\" approach that minimizes the data transfer time of data- and compute-intensive scientific applications deployed in the cloud, by making them incrementally processable. We describe a system that implements this approach based on the IBM InfoSphere Streams computing platform deployed over Amazon EC2. The functionality, performance, and usability of the system will be demonstrated through two DNA sequence analysis applications.\"",
        "Document: \"Introduction to special section on formal methods in pervasive computing. Ubiquitous and pervasive applications may present critical requirements from the point of view of functional correctness, reliability, availability, security, and safety. Unlike traditional safety-critical applications, the behavior of ubiquitous and pervasive applications is affected by the movements and location of users and resources. In this article, we first present emerging formal methods for the description of both entities and their behavior in pervasive computing environments; then, we introduce this special issue. Despite many previous works that have focused on modeling the entities, relatively few have concentrated on modeling or verifying behaviors; and almost none has dealt with combining techniques proposed in these two aspects. The articles accepted in this special issue cover some of the topics aforementioned and constitute a representative sample of the latest development of formal methods in pervasive computing environments.\"",
        "1 is \"Using handhelds and PCs together\", 2 is \"No one (cluster) size fits all: automatic cluster sizing for data-intensive analytics\"",
        "Given above information, for an author who has written the paper with the title \"Towards fault tolerance pervasive computing\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002379": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Accelerating Urban Science by Crowdsensing with Civil Officers.':",
        "Document: \"Single-Epoch Supernova Classification with Deep Convolutional Neural Networks. Supernovae Type-Ia (SNeIa) play a significant role in exploring the history of the expansion of the Universe, since they are the best-known standard candles with which we can accurately measure the distance to the objects. Finding large samples of SNeIa and investigating their detailed characteristics have become an important issue in cosmology and astronomy. The current photometric supernova surveys produce vastly more candidates than can be followed up spectroscopically, highlighting the need for effective classification methods. Existing methods relied on a photometric approach that first measures the luminance of supernova candidates precisely and then fits the results to a parametric function of temporal changes in luminance. However, it inevitably requires multi-epoch observations and complex luminance measurements. In this work, we present a novel method for classifying SNeIa simply from single-epoch observation images without any complex measurements, by effectively integrating the state-of-the-art computer vision methodology into the standard photometric approach. Our method first builds a convolutional neural network for estimating the luminance of supernovae from telescope images, and then constructs another neural network for the classification, where the estimated luminances and observation dates are used as features for classification. Both of the neural networks are integrated into a single deep neural network to classify SNeIa directly from observation images. Experimental results show the effectiveness of the proposed method and reveal classification performance comparable to existing photometric methods with multi-epoch observations.\"",
        "Document: \"A New Multi-task Learning Method for Personalized Activity Recognition. Personalized activity recognition usually faces the problem of data sparseness. We aim at improving accuracy of personalized activity recognition by incorporating the information from other persons. We propose a new online multi-task learning method for personalized activity recognition. The proposed online multi-task learning method automatically learns the ``transfer-factors\" (similarities) among different tasks (i.e., among different persons in our case). Experiments demonstrate that the proposed method significantly outperforms existing methods. The novelty of this paper is twofold: (1) A new multi-task learning framework, which can naturally learn similarities among tasks, (2) To our knowledge, this is the first study of large-scale personalized activity recognition.\"",
        "1 is \"Field-aware Factorization Machines for CTR Prediction.\", 2 is \"Ask me better questions: active learning queries based on rule induction\"",
        "Given above information, for an author who has written the paper with the title \"Accelerating Urban Science by Crowdsensing with Civil Officers.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002385": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'An Envisioned Approach for Modeling and Supporting User-Centric Query Activities on Data Warehouses':",
        "Document: \"A framework for enriching Data Warehouse analysis with Question Answering systems. Business Intelligence (BI) applications allow their users to query, understand, and analyze existing data within their organizations in order to acquire useful knowledge, thus making better strategic decisions. The core of BI applications is a Data Warehouse (DW), which integrates several heterogeneous structured data sources in a common repository of data. However, there is a common agreement in that the next generation of BI applications should consider data not only from their internal data sources, but also data from different external sources (e.g. Big Data, blogs, social networks, etc.), where relevant update information from competitors may provide crucial information in order to take the right decisions. This external data is usually obtained through traditional Web search engines, with a significant effort from users in analyzing the returned information and in incorporating this information into the BI application. In this paper, we propose to integrate the DW internal structured data, with the external unstructured data obtained with Question Answering (QA) techniques. The integration is achieved seamlessly through the presentation of the data returned by the DW and the QA systems into dashboards that allow the user to handle both types of data. Moreover, the QA results are stored in a persistent way through a new DW repository in order to facilitate comparison of the obtained results with different questions or even the same question with different dates.\"",
        "Document: \"Contextual ontology module learning from web snippets and past user queries. In this paper, we focus on modularization aspects for query reformulation in ontology-based question answering on the Web. The main objective is to automatically learn ontology modules that cover search terms of the user. Indeed, the main problem is that current approaches of ontology modularization consider only the input existant ontologies, instead of underlying semantics found in texts. This work proposes an approach of contextual ontology module learning covering particular search terms by analyzing past user queries and snippets provided by search engines. The obtained contextual modules will be used for query reformulation. The proposal has been evaluated on the ground of semantic cotopy measure of discovered ontology modules, relevance of search results.\"",
        "1 is \"Staying FIT: efficient load shedding techniques for distributed stream processing\", 2 is \"A general process mining framework for correlating, predicting and clustering dynamic behavior based on event logs\"",
        "Given above information, for an author who has written the paper with the title \"An Envisioned Approach for Modeling and Supporting User-Centric Query Activities on Data Warehouses\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002393": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Twitter part-of-speech tagging using pre-classification Hidden Markov model':",
        "Document: \"Topic Detection with Hypergraph Partition Algorithm. An algorithm named SMHP (Similarity Matrix based Hypergraph Partition) algorithm is proposed, which aims at improving the efficiency of Topic Detection. In SMHP, a T-MI-TFIDF model is designed by introducing Mutual Information (MI) and enhancing the weight of terms in the title. Then Vector Space Model (VSM) is constructed according to terms' weight, and the dimension is reduced by combining H-TOPN and Principle Component Analysis (PCA). Then topics are grouped based on SMHP. Experiment results show the proposed methods are more suitable for clustering topics. SMHP with novel approaches can effectively solve the relationship of multiple stories problem and improve the accuracy of cluster results. \u00a9 2011 Academy Publisher.\"",
        "Document: \"Enhancing the accuracy of knowledge discovery: a supervised learning method. The amount of biomedical literature available is growing at an explosive speed, but a large amount of useful information remains undiscovered in it. Researchers can make informed biomedical hypotheses through mining this literature. Unfortunately, popular mining methods based on co-occurrence produce too many target concepts, leading to the declining relevance ranking of the potential target concepts.This paper presents a new method for selecting linking concepts which exploits statistical and textual features to represent each linking concept, and then classifies them as relevant or irrelevant to the starting concepts. Relevant linking concepts are then used to discover target concepts.Through an evaluation it is observed textual features improve the results obtained with only statistical features. We successfully replicate Swanson's two classic discoveries and find the rankings of potentially relevant target concepts are relatively high.The number of target concepts is greatly reduced and potentially relevant target concepts gain higher ranking by adopting only relevant linking concepts. Thus, the proposed method has the potential to help biomedical experts find the most useful and valuable target concepts effectively.\"",
        "1 is \"Program plagiarism revisited: current issues and approaches\", 2 is \"A core-attachment based method to detect protein complexes in PPI networks\"",
        "Given above information, for an author who has written the paper with the title \"Twitter part-of-speech tagging using pre-classification Hidden Markov model\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002436": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'DyLink2Vec: Effective Feature Representation for Link Prediction in Dynamic Networks.':",
        "Document: \"Towards Safe Cities: A Mobile and Social Networking Approach. Population density and natural and man-made disasters make public safety a concern of growing importance. In this paper we aim to enable the vision of smart and safe cities by exploiting mobile and social networking technologies to securely and privately extract, model and embed real-time public safety information into quotidian user experiences. We first propose novel approaches to define location- and user-based safety metrics. We evaluate the ability of existing forecasting techniques to predict future safety values. We introduce iSafe, a privacy-preserving algorithm for computing safety snapshots of co-located mobile devices as well as geosocial network users. We present implementation details of iSafe as both an Android application and a browser plugin that visualizes safety levels of visited locations and browsed geosocial venues. We evaluate iSafe using crime and census data from the Miami-Dade (FL) county as well as data we collected from Yelp, a popular geosocial network.\"",
        "Document: \"Liveness verifications for citizen journalism videos. Citizen journalism videos increasingly complement or even replace the professional news coverage through direct reporting by event witnesses. This raises questions of the integrity and credibility of such videos. We introduce Vamos, the first user transparent video \\\"liveness\\\" verification solution based on video motion, that can be integrated into any mobile video capture application without requiring special user training. Vamos' algorithm not only accommodates the full range of camera movements, but also supports videos of arbitrary length. We develop strong attacks both by utilizing fully automated attackers and by employing trained human experts for creating fraudulent videos to thwart mobile video verification systems. We introduce the concept of video motion categories to annotate the camera and user motion characteristics of arbitrary videos. We share motion annotations of YouTube citizen journalism videos and of free-form video samples that we collected through a user study. We observe that the performance of Vamos differs across video motion categories. We report the expected performance of Vamos on the real citizen journalism video chunks, by projecting on the distribution of categories. Even though Vamos is based on motion, we observe a surprising and seemingly counter-intuitive resilience against attacks performed on relatively \\\"static\\\" video chunks, which turn out to contain hard-to-imitate involuntary movements. We show that the accuracy of Vamos on the task of verifying whole length videos exceeds 93% against the new attacks.\"",
        "1 is \"Efficient sampling algorithm for estimating subgraph concentrations and detecting network motifs.\", 2 is \"Assessing the accuracy of prediction algorithms for classification: an overview.\"",
        "Given above information, for an author who has written the paper with the title \"DyLink2Vec: Effective Feature Representation for Link Prediction in Dynamic Networks.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002452": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Reliability and vulnerability analyses of critical infrastructures: Comparing two approaches in the context of power systems.':",
        "Document: \"Risk assessment and risk-cost optimization of distributed power generation systems considering extreme weather conditions. Security and reliability are major concerns for future power systems with distributed generation. A comprehensive evaluation of the risk associated with these systems must consider contingencies under normal environmental conditions and also extreme ones. Environmental conditions can strongly influence the operation and performance of distributed generation systems, not only due to the growing shares of renewable-energy generators installed but also for the environment-related contingencies that can damage or deeply degrade the components of the power grid. In this context, the main novelty of this paper is the development of probabilistic risk assessment and risk-cost optimization framework for distributed power generation systems, that take the effects of extreme weather conditions into account. A Monte Carlo non-sequential algorithm is used for generating both normal and severe weather. The probabilistic risk assessment is embedded within a risk-based, bi-objective optimization to find the optimal size of generators distributed on the power grid that minimize both risks and cost associated with severe weather. An application is shown on a case study adapted from the IEEE 13 nodes test system. By comparing the results considering normal environmental conditions and the results considering the effects of extreme weather, the relevance of the latter clearly emerges.\"",
        "Document: \"A neuro-fuzzy technique for fault diagnosis and its application to rotating machinery. Malfunctions in machinery are often sources of reduced productivity and increased maintenance costs in various industrial applications. For this reason, machine condition monitoring is being pursued to recognise incipient faults. In this paper, the fault diagnostic problem is tackled within a neuro-fuzzy approach to pattern classification. Besides the primary purpose of a high rate of correct classification, the proposed neuro-fuzzy approach also aims at obtaining an easily interpretable classification model. The efficiency of the approach is verified with respect to a literature problem and then applied to a case of motor bearing fault classification.\"",
        "1 is \"A neuro-fuzzy inference system through integration of fuzzy logic and extreme learning machines.\", 2 is \"The role of natural language in a multimodal interface\"",
        "Given above information, for an author who has written the paper with the title \"Reliability and vulnerability analyses of critical infrastructures: Comparing two approaches in the context of power systems.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002569": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Reject me: peer review and SIGCHI':",
        "Document: \"PaperTonnetz: supporting music composition with interactive paper. A Tonnetz, or \"tone-network\" in German, is a two-dimensional representation of the relationships among musical pitches. In this paper, we present PaperTonnetz, a tool that lets musicians explore and compose music with Tonnetz representations by making gestures on interactive paper. In addition to triggering musical notes with the pen as a button based-interface, the drawn gestures become interactive paths that can be used as chords or melodies to support composition.\"",
        "Document: \"Generative walkthroughs: to support creative redesign. Generative Walkthoughs support the redesign phase of an iterative design process, helping designers generate new design alternatives informed by social science principles. Designers first analyze their own scenarios or storyboards with respect to concrete examples drawn from five socio-technical principles: situated action, rhythms & routines, co-adaptive systems, peripheral awareness and distributed cognition. They then walk through the scenario and brainstorm new design alternatives that reflect the design principle in question. This combination of structured walkthroughs with focused brainstorming helps designers, particularly those with little social science background, to generate concrete, actionable ideas that reflect key findings from the social science literature. We taught Generative Walkthroughs in ten courses with over 220 students and found that technically-trained students not only learned these socio-technical principles, but were able to apply them in innovative ways in a variety of design settings.\"",
        "1 is \"Virtual Network Computing\", 2 is \"Adaptive and Incremental Query Expansion for Cluster-based Browsing\"",
        "Given above information, for an author who has written the paper with the title \"Reject me: peer review and SIGCHI\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002626": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Variational delay metrics for interconnect timing analysis':",
        "Document: \"Process Variation and Temperature-Aware Full Chip Oxide Breakdown Reliability Analysis. Gate oxide breakdown (OBD) is a key factor limiting the useful lifetime of an integrated circuit. Unfortunately, the conventional approach for full chip OBD reliability analysis assumes a uniform oxide thickness and worst-case temperature for all devices. In practice, however, gate oxide thickness varies from die-to-die and within-die and hence may cause different reliability for different devices even chips. Moreover, due to the increased across-die temperature variation, such difference may be exacerbated. Thus, as the precision of variation control worsens, an alternative reliability analysis approach is needed. In this paper, we propose a statistical framework for chip-level gate OBD reliability analysis while considering both die-to-die and within-die components of thickness variations as well as the across-die temperature variation. The thickness of each device is modeled as a distinct random variable and thus the full chip reliability estimation problem is defined on a huge sample space of several million devices. We observe that the chip-level OBD reliability function is independent of the relative location of the individual devices. This enables us to transform the problem such that the resulting representation can be expressed in terms of much fewer random variables. Using this transformation, we present a computationally efficient and accurate approach for estimating the full chip reliability while considering spatial correlations of gate oxide thickness as well as temperature variation. We show that, compared to Monte Carlo simulation, the proposed method incurs an error of only around 1% while improving the runtime by more than three orders of magnitude.\"",
        "Document: \"11.2 A 1Mb embedded NOR flash memory with 39\u00b5W program power for mm-scale high-temperature sensor nodes. Miniature sensor nodes are ideal for monitoring environmental conditions in emerging applications such as oil exploration. One key requirement for sensor nodes is embedded non-volatile memory for compact and retentive data storage in the event that the sensor power source is exhausted. Non-volatile memory also allows for near-zero standby power modes, which are particularly challenging to achieve at high temperatures when using SRAM in standby due to the exponential rise in leakage with temperature, which rapidly degrades battery life (Fig. 11.2.1). However, traditional NOR flash requires mW-level program and erase power, which cannot be sustained by mm-scale batteries with internal resistances >10k\u03a9 To address this issue, we propose an ultra-low power NOR flash design and demonstrate its integration into a complete sensor system that is specifically designed for environmental monitoring under high temperature conditions: such as when injected into geothermal or oil wells.\"",
        "1 is \"Stochastic analysis of interconnect performance in the presence of process variations\", 2 is \"A 1.4-V Supply CMOS Fractional Bandgap Reference\"",
        "Given above information, for an author who has written the paper with the title \"Variational delay metrics for interconnect timing analysis\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002688": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Examining the Challenges of Scientific Workflows':",
        "Document: \"Parallel and distributed systems: UML based modeling of performance oriented parallel and distributed applications. In this paper we introduce a novel approach for modeling performance oriented distributed and parallel applications based on the Unified Modeling Language (UML). We utilize the UML extension mechanisms to customize UML for the domain of performance oriented distributed and parallel computing. A set of UML building blocks is described that model some of the most important constructs of message passing and shared memory parallel paradigms which can be used to develop models for large and complex parallel and distributed applications. We illustrate our approach by modeling a parallel many-body physics application that combines message passing and shared memory parallelism.\"",
        "Document: \"Towards a Light-weight Workflow Engine in the Asklon Grid Environment. Workflow scheduling and execution belong to the most difficult problems in the Grid computing research area. Instead of using a full-ahead planning to schedule workflows, which requires precise predictions of task execution time, file transfer time and Grid site status during the workflow execution, we propose a lightweight workflow engine for the ASKALON Grid environment, which uses just in-time scheduling based on automatically generated performance predictions and task prioritization. An extensive survey of the related work is discussed. The architecture of the proposed workflow engine and some preliminary results are presented.\"",
        "1 is \"Dependency-based distributed intrusion detection\", 2 is \"jMetal: A Java framework for multi-objective optimization\"",
        "Given above information, for an author who has written the paper with the title \"Examining the Challenges of Scientific Workflows\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002697": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Joint Active Learning with Feature Selection via CUR Matrix Decomposition.':",
        "Document: \"Semantic knowledge extraction and annotation for web images. Nowadays, images have become widely available on the World Wide Web (WWW). It's essential to develop effective ways for managing and retrieving such abundant images. Advantageously, compared to the traditional images where very little information is provided, the web images contain plentiful context data. This paper introduces a system that can automatically acquire semantic knowledge for web image annotation. By using a page layout analysis method that can precisely assign context to web images, we developed efficient algorithms to extract semantic knowledge for web images, such as description, people, temporal and geographic information. To validate the practicality and efficiency of this system, we applied it to about 6,500 images crawled from Web. Experiments demonstrated that our approach could achieve satisfactory results.\"",
        "Document: \"A framework for providing adaptive sports video to mobile devices. In this paper, we present a novel framework that serves adaptive sports video to mobile users. Our framework combines content-based sports highlights extraction and quality-domain video compression technologies in video server, capable of reducing wireless bandwidth consumption more effectively. We develop a robust replay-based highlights extraction method, and propose a content-based video streaming coding scheme to handle the problems of bandwidth and capacity of computation. To validate the practicality and effectiveness of our system, we conduct the experiments on several real soccer videos. The experimental results demonstrate the robustness of our highlights extraction method and more than 77.5% of the bandwidth consumption could be reduced.\"",
        "1 is \"Adjustment Learning and Relevant Component Analysis\", 2 is \"Smoothly varying affine stitching\"",
        "Given above information, for an author who has written the paper with the title \"Joint Active Learning with Feature Selection via CUR Matrix Decomposition.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002879": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A quality model for mashups':",
        "Document: \"Workshop PC Chairs' Message. According to recent studies, an estimated 90% of Web sites and applications suffer from usability and/or accessibility problems.\n As user satisfaction has increased in importance, the need for usable and accessible Web applications has become more critical.\n To achieve usability for a Web product (e.g., a service, a model, a running application, a portal), the attributes of Web\n artefacts must be clearly defined. Otherwise, assessment of usability is left to the intuition or to the responsibility of\n people who are in charge of the process. In this sense, usability models (describing all the usability sub-characteristics,\n attributes and their relationships) should be built, and Usability Evaluation Methods (UEMs) should be used during the requirements,\n design and implementation stages based on these models. Similarly, identifying the set of characteristics that make the Web\n more accessible for everybody, including those with disabilities is necessary to systematize the way practitioners face accessibility\n issues.\n \"",
        "Document: \"PEUDOM: a mashup platform for the end user development of common information spaces. This paper presents a Web platform for the user-driven, service-based creation of Common Information Spaces (CISs). Two composition environments, characterized by intuitive visual notations, enable i) the integration of services to create UI-rich components and ii) the synchronization of components into interactive workspaces. Collaborative features allow multiple users to collaborate, synchronously and asynchronously, to share and co-create CISs.\"",
        "1 is \"Taverna: A Tool For Building And Running Workflows Of Services\", 2 is \"The design and applications of a context service\"",
        "Given above information, for an author who has written the paper with the title \"A quality model for mashups\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002917": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'An Integrated Development Environment to Support the Multi-core Embedded Systems Development':",
        "Document: \"A Xen-based paravirtualization system toward efficient high performance computing environments. A virtual machine provides platforms to install an OS within another OS which provides resources. It can be accomplished to construct a computational cluster system on a single machine. The real cluster with machines provides full utilization of its resource for users while a virtual machine assigns the resources of the host to residing OSs. Xen is such kind of virtual machine to construct the virtualization system. It is chosen to be our system's virtual machine monitor because it provides better efficiency, supports different operating system work simultaneously, and gives each operating system an independent system environment. The performance of the virtualization system is examined by comparing with a non-virtualization system which is a real cluster system. The experiments show less power consumption and better computing efficiency by executing programs such as matrix multiplication, LINPACK, lower-upper triangular and Primes test sets. The results show better choices of constructing a large-scaled computing system using a virtual machine.\"",
        "Document: \"Performance-based workload distribution on grid environments. Imbalanced workload-distribution can significantly degrade performance of grid computing environments. In the past, the theory of divisible load has been widely investigated in static heterogeneous systems. However, it has not been widely applied to grid environments, which are characterized by heterogeneous resources and dynamic environments. In this paper, we propose a performance-based approach to workload distribution for master-slave types of applications on grids. Furthermore, applications with irregular workloads are addressed. We implemented three kinds of applications and conducted experimentations on our grid test-beds. Experimental results show that this approach performs more efficiently than conventional schemes. Consequently, we claim that dynamic workload distribution can benefit applications on grid environments.\"",
        "1 is \"Variant Bayesian Networks\", 2 is \"Efficient and user-friendly verification\"",
        "Given above information, for an author who has written the paper with the title \"An Integrated Development Environment to Support the Multi-core Embedded Systems Development\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002982": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Development of advanced image processing technology and its application to computer assisted diagnosis and surgery':",
        "Document: \"Mediastinal atlas creation from 3-D chest computed tomography images: application to automated detection and station mapping of lymph nodes. One important aspect of lung cancer staging is the assessment of mediastinal lymph nodes in 3-D chest computed tomography (CT) images. In the current clinical routine this is done manually by analyzing the 3-D CT image slice by slice to find nodes, evaluate them quantitatively, and assign labels to them for describing the clinical and pathologic extent of metastases. In this paper we present a method to automate the process of lymph node detection and labeling by creation of a mediastinal average image and a novel lymph node atlas containing probability maps for mediastinal, aortic, and N1 nodes. Utilizing a fast deformable registration approach to match the atlas with CT images of new patients, our method can maintain an acceptable runtime. In comparison to previously published methods for mediastinal lymph node detection and labeling it also shows a good sensitivity and positive predictive value.\"",
        "Document: \"Visual SLAM for bronchoscope tracking and bronchus reconstruction in bronchoscopic navigation. We present a new scheme for bronchoscopic navigation by exploiting visual SLAM for bronchoscope tracking. Bronchoscopic navigation system is used to guide physicians by providing 3D space information about the bronchoscope during bronchoscopic examination. Existing bronchoscopic navigation systems mainly used CT-video or sensor for bronchoscope tracking. CT-video based tracking estimates the bronchoscope pose by registration of real bronchoscope images and virtual images generated from computed tomography (CT) images, which requires lots of time. Sensor based tracking calculates the bronchoscope pose based on information from sensor, which is easily influenced by examination tools. We improve the bronchoscope tracking by using visual simultaneous localization and mapping (VSLAM), which can overcome the aforementioned shortcomings. VSLAM is an approach to estimate the camera pose and reconstruct surrounding structure around a camera (called map). We use the adjacent frames to increase the points used for tracking, and use VSLAM for bronchoscope tracking. Tracking performance of VSLAM were evaluated with phantom and in-vivo videos. Reconstruction performance of VSLAM was evaluated by root mean square (RMS) value, which is calculated using aligned reconstructed points and segmented bronchus from pre-operative CT volumes. Experimental results showed that the successfully tracked frames in the proposed method increased more than 700 frames compared with the original ORB-SLAM for six cases. The average RMS in phantom case between estimated bronchus from SLAM and bronchus shape from segmented bronchus was 2.55 mm.\"",
        "1 is \"Agent-Based Annotation of Interactive 3D Visualizations\", 2 is \"A Recognition Method of Matrices by Using Variable Block Pattern Elements Generating Rectangular Area\"",
        "Given above information, for an author who has written the paper with the title \"Development of advanced image processing technology and its application to computer assisted diagnosis and surgery\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002992": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'When content-based video retrieval and human computation unite: Towards effective collaborative video search':",
        "Document: \"One-handed mobile video browsing. Various situations exist in which people want to use just one hand when interacting with their mobile phones or PDAs. However, most of the few advanced video browsing approaches for such devices only support two handed operation. In this paper, we present an interface design for one-handed mobile video browsing. Using a PDA with touch screen, users can interactively scroll through a list of thumbnails representing the video's content. Thumbnail sizes can be manipulated in order to provide easier access to relevant information. Two evaluations are presented which motivate the design and verify the usability of our solution.\"",
        "Document: \"Interactive audio-visual video browsing. We present the AV-ZoomSlider interface for video browsing. It complements existing approaches, such as storyboards and video skims by enabling users to interactively navigate along the time line of a video file. Our solution smoothly integrates position- and speed-based navigation concepts and provides synchronized audio-visual feedback during scrolling when applicable.\"",
        "1 is \"A distance metric for multidimensional histograms\", 2 is \"iTunes University and the classroom: Can podcasts replace Professors?\"",
        "Given above information, for an author who has written the paper with the title \"When content-based video retrieval and human computation unite: Towards effective collaborative video search\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003064": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Programming the FlexRAM parallel intelligent memory system':",
        "Document: \"Writing productive stencil codes with overlapped tiling. Stencil computations constitute the kernel of many scientificapplications. Tiling is often used to improve the performance ofstencil codes for data locality and parallelism. However, tiledstencil codes typically require shadow regions, whose managementbecomes a burden to programmers. In fact, it is often the case thatthe code required to manage these regions, and in particular theirupdates, is much longer than the computational kernel of thestencil. As a result, shadow regions usually impact programmers'productivity negatively. In this paper, we describe overlappedtiling, a construct that supports shadow regions in aconvenient, flexible and efficient manner in the context of thehierarchically tiled array (HTA) data type. The HTA is a classdesigned to express algorithms with a high degree of parallelismand-or locality as naturally as possible in terms of tiles. Wediscuss the syntax and implementation of overlapped HTAs as well asour experience in rewriting parallel and sequential codes usingthem. The results have been satisfactory in terms of bothproductivity and performance. For example, overlapped HTAs reducedthe number of communication statements in non-trivial codes by 78%on average while speeding them up. We also examine differentimplementation options and compare overlapped HTAs with previousapproaches. Copyright \u00a9 2008 John Wiley & Sons, Ltd.Portions of this paper were previously published in 'Programmingwith Tiles' by Guo J, Bikshandi G, Fraguela BB, Garzar\u00e1n MJ,Padua D. Proceedings of the 13th ACM SIGPLAN Symposium onPrinciples and Practice of Parallel Programming (PPoPP),February 2008.\"",
        "Document: \"Adaptive line placement with the set balancing cache. Efficient memory hierarchy design is critical due to the increasing gap between the speed of the processors and the memory. One of the sources of inefficiency in current caches is the non-uniform distribution of the memory accesses on the cache sets. Its consequence is that while some cache sets may have working sets that are far from fitting in them, other sets may be underutilized because their working set has fewer lines than the set. In this paper we present a technique that aims to balance the pressure on the cache sets by detecting when it may be beneficial to associate sets, displacing lines from stressed sets to underutilized ones. This new technique, called Set Balancing Cache or SBC, achieved an average reduction of 13% in the miss rate of ten benchmarks from the SPEC CPU2006 suite, resulting in an average IPC improvement of 5%.\"",
        "1 is \"Virtualizing Transactional Memory\", 2 is \"Variable-size mosaics: A process-variation aware technique to increase the performance of tile-based, massive multi-core processors\"",
        "Given above information, for an author who has written the paper with the title \"Programming the FlexRAM parallel intelligent memory system\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003120": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Saddlepoint approximation to the outage capacity of MIMO channels':",
        "Document: \"Interleaving and decoding scheme for a product code for a mobile data communication. We propose a new ordered decoding scheme for a product code for mobile data communications. The ordered decoding scheme determines the order of decoding for both row and column component codewords according to the probability of decoding the component codeword correctly. Component codewords are decoded independently. To randomize burst errors in both row and column codewords, a diagonal interleaving scheme is used for code symbols in the codeword. It is shown that the ordered decoding scheme combined with diagonal interleaving improves the performance of a product code with reasonably long code length for mobile data communications\"",
        "Document: \"Resource Allocation for CoMP With Multiuser MIMO-OFDMA. In this paper, we propose new resource allocation schemes with (or without) adaptive modulation for coordinated multipoint (CoMP) with multiuser multiple-input multiple-output orthogonal frequency-division multiple-access (MIMO-OFDMA). In the proposed schemes, a practical linear pre- and postprocessing technique is used to cancel interuser interference and decompose a single-user MIMO channel into parallel noninterfering spatial layers. The transmit power is allocated to spatial layers with (or without) adaptive modulation under the total base station (BS) power constraint and the per-BS power constraint. The set of users for each subcarrier is determined by the exhaustive search user allocation, the sum rate-based user allocation, and the random user allocation. Simulation results show that the proposed schemes enhance the sum rate.\"",
        "1 is \"Adaptive bit-interleaved coded modulation\", 2 is \"Dual-hop systems with noisy relay and interference-limited destination\"",
        "Given above information, for an author who has written the paper with the title \"Saddlepoint approximation to the outage capacity of MIMO channels\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003123": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Interview Review: An Empirical Study On Detecting Ambiguities In Requirements Elicitation Interviews':",
        "Document: \"Specifying variability in service contracts. In Service Oriented Computing (SOC) contracts characterise the behavioural conformance of a composition of services and guarantee that the composition does not lead to spurious results. Variability features can enable services to adapt to customer requirements and to changes in the context in which they execute. We extend a recently introduced formal model of service contracts to specify variability mechanisms in a composition of services. Necessary and permitted service requests can be defined and triggered to increase adaptability. The compositional rules of the original formalism are enriched to fulfil all necessary requirements and the maximal number of permitted ones.\"",
        "Document: \"Assisting the design of a groupware system. Product Data Management (PDM) systems support the product/document management of design processes such as those typically used in the manufacturing industry. They allow enterprises to capture, organise, automate and share engineering information in an efficient way. The efficient handling of queries on product information and the uploading and downloading of families of related files for modification by designers are essential aspects of such systems. The efficiency of the system as perceived by its clients depends on its correct functioning, but also for a significant part on its performance aspects. In this article, we apply both qualitative and stochastic model-checking techniques to evaluate various usability and performance aspects of the thinkteam PDM system, and of several proposed extensions, thereby assisting the design phase of an industrial groupware system.\"",
        "1 is \"A Formal Model for Multi SPLs.\", 2 is \"Report on a development project use of an issue-based information system\"",
        "Given above information, for an author who has written the paper with the title \"Interview Review: An Empirical Study On Detecting Ambiguities In Requirements Elicitation Interviews\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003124": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Model-based testing of service infrastructure components':",
        "Document: \"An Open Visualization Framework for Metamodel-Based Modeling Languages. In the paper, we propose an automated, SVG-based visualization framework for modeling languages defined by metamodeling techniques. Our framework combines XML standards with existing graph transformation and graph drawing technologies in order to provide an open, tool-independent architecture.\"",
        "Document: \"Incremental backward change propagation of view models by logic solvers. View models are key concepts of domain-specific modeling to provide task-specific focus (e.g., power or communication architecture of a system) to the designers by highlighting only the relevant aspects of the system. View models can be specified by unidirectional forward transformations (frequently captured by graph queries), and automatically maintained upon changes of the underlying source model using incremental transformation techniques. However, tracing back complex changes from one or more abstract view to the underlying source model is a challenging task, which, in general, requires the simultaneous analysis of transformation specifications and well-formedness constraints to create valid changes in the source model. In this paper we introduce a novel delta-based backward transformation technique using SAT solvers to synthetize valid and consistent change candidates in the source model, where only forward transformation rules are specified for the view models.\"",
        "1 is \"Agent-oriented modeling with graph transformation\", 2 is \"Model Refactorings as Rule-Based Update Transformations\"",
        "Given above information, for an author who has written the paper with the title \"Model-based testing of service infrastructure components\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003142": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Spatial-Keyword Skyline Publish/Subscribe Query Processing Over Distributed Sliding Window Streaming Data':",
        "Document: \"Dynamic Packet Length Control in Wireless Sensor Networks. Previous packet length optimizations for sensor networks often employ a fixed optimal length scheme, while in this study we present DPLC, a Dynamic Packet Length Control scheme. To make DPLC more efficient in terms of channel utilization, we incorporate a lightweight and accurate link estimation method. We further provide two easy-to-use services, i.e., small message aggregation and large message fragmentation, to facilitate upper-layer application programming. The implementation of DPLC based on TinyOS 2.1 is lightweight, with respect to computation, memory, and header overhead. Our experiments using a real indoor testbed running CTP show that DPLC achieves the best performance compared with previous works.\"",
        "Document: \"Walking down the STAIRS: Efficient collision resolution for wireless sensor networks. Collision resolution is a crucial issue in wireless sensor networks. The existing approaches of collision resolution have drawbacks with respect to energy efficiency and processing latency. In this paper, we propose ST AIRS, a time and energy efficient collision resolution mechanism for wireless sensor networks. STAIRS incorporates the constructive interference technique in its design and explicitly forms superimposed colliding signals. Through extensive observations and theoretical analysis, we show that the RSSI of the superimposed signals exhibit stairs-like phenomenon with different number of contenders. That principle offers an attractive feature to efficiently distinguish multiple contenders and in turn makes collision-free schedules for channel access. In the design and implementation of STAIRS, we address practical challenges such as contenders alignment, online detection of RSSI change points, and fast channel assignment. The experiments on real testbed show that STARIS realizes fast and effective collision resolution, which significantly improves the network performance in terms of both latency and throughput.\"",
        "1 is \"Low-power amdahl-balanced blades for data intensive computing\", 2 is \"Multicast Scaling Properties in Massively Dense Ad Hoc Networks\"",
        "Given above information, for an author who has written the paper with the title \"Spatial-Keyword Skyline Publish/Subscribe Query Processing Over Distributed Sliding Window Streaming Data\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003174": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Feature Selection for Neural Network-Based Interval Forecasting of Electricity Demand Data':",
        "Document: \"Experimental Comparison Study on Joint and Cartesian Space Control Schemes for a Teleoperation System Under Time-Varying Delay. In this paper, teleoperation control of a UR5 robotic manipulator under time-varying delay is addressed. Two control methods of joint and Cartesian space are investigated for the teleoperation system. The singularity situations of the UR5 robot are considered for the case of different degrees of freedom (DoF), and also different kinematic properties for the master and slave systems. We first focus on a Cartesian space control of a teleoperation system which is common in the literature. It is shown that this control methodology is highly prone to singular situations. To overcome this problem, a Jacobian-based control strategy in joint space is presented. An extensive simulation study demonstrates the superiority and reliability of the later control methodology.\"",
        "Document: \"A novel fuzzy multi-objective framework to construct optimal prediction intervals for wind power forecast. The forecasting behavior of the high volatile and unpredictable wind power energy has always been a challenging issue in the power engineering area. In this regard, this paper proposes a new multi-objective framework based on fuzzy idea to construct optimal prediction intervals (Pis) to forecast wind power generation more sufficiently. The proposed method makes it possible to satisfy both the PI coverage probability (PICP) and PI normalized average width (PINAW), simultaneously. In order to model the stochastic and nonlinear behavior of the wind power samples, the idea of lower upper bound estimation (LUBE) method is used here. Regarding the optimization tool, an improved version of particle swam optimization (PSO) is proposed. In order to see the feasibility and satisfying performance of the proposed method, the practical data of a wind farm in Australia is used as the case study.\"",
        "1 is \"A Genetic Algorithm For Video Segmentation And Summarization\", 2 is \"CLAST: CUDA implemented large-scale alignment search tool.\"",
        "Given above information, for an author who has written the paper with the title \"Feature Selection for Neural Network-Based Interval Forecasting of Electricity Demand Data\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003182": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Statistical physics of signal estimation in Gaussian noise: theory and examples of phase transitions':",
        "Document: \"On information rates for mismatched decoders. Reliable transmission over a discrete-time memoryless channel with a decoding metric that is not necessarily matched to the channel (mismatched decoding) is considered. It is assumed that the encoder knows both the true channel and the decoding metric. The lower bound on the highest achievable rate found by Csiszar and Korner (1981) and by Hui (1983) for DMC's, hereafter denoted CLM, is shown to bear some interesting information-theoretic meanings. The bound CLM turns out to be the highest achievable rate in the random coding sense, namely, the random coding capacity for mismatched decoding. It is also demonstrated that the \u03b5-capacity associated with mismatched decoding cannot exceed CLM. New bounds and some properties of CLM are established and used to find relations to the generalized mutual information and to the generalized cutoff rate. The expression for CLM is extended to a certain class of memoryless channels with continuous input and output alphabets, and is used to calculate CLM explicitly for several examples of theoretical and practical interest. Finally, it is demonstrated that in contrast to the classical matched decoding case, here, under the mismatched decoding regime, the highest achievable rate depends on whether the performance criterion is the bit error rate or the message error probability and whether the coding strategy is deterministic or randomized\"",
        "Document: \"Identification in the presence of side information with application to watermarking. Watermarking codes are analyzed from an information-theoretic viewpoint as identification codes with side information that is available at the transmitter only or at both ends. While the information hider embeds a secret message (watermark) in a covertext message (typically, text, image, sound, or video stream) within a certain distortion level, the attacker, modeled here as a memoryless channel, processes the resulting watermarked message (within limited additional distortion) in attempt to invalidate the watermark. In most applications of watermarking codes, the decoder need not carry out full decoding, as in ordinary coded communication systems, but only to test whether a watermark at all exists and if so, whether it matches a particular hypothesized pattern. This fact motivates us to view the watermarking problem as an identification problem, where the original covertext source serves as side information. In most applications, this side information is available to the encoder only, but sometimes it can be available to the decoder as well. For the case where the side information is available at both encoder and decoder, we derive a formula for the identification capacity and also provide a characterization of achievable error exponents. For the case where side information is available at the encoder only, we derive upper and lower bounds on the identification capacity. All characterizations are obtained as single-letter expressions\"",
        "1 is \"Universal codes for a class of composite sources (Corresp.)\", 2 is \"Impulse radio: how it works\"",
        "Given above information, for an author who has written the paper with the title \"Statistical physics of signal estimation in Gaussian noise: theory and examples of phase transitions\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003248": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'On the properties of combination set operations':",
        "Document: \"A speech understanding framework that uses multiple language models and multiple understanding models. The optimal combination of language model (LM) and language understanding model (LUM) varies depending on available training data and utterances to be handled. Usually, a lot of effort and time are needed to find the optimal combination. Instead, we have designed and developed a new framework that uses multiple LMs and LUMs to improve speech understanding accuracy under various situations. As one implementation of the framework, we have developed a method for selecting the most appropriate speech understanding result from several candidates. We use two LMs and three LUMs, and thus obtain six combinations of them. We empirically show that our method improves speech understanding accuracy. The performance of the oracle selection suggests further potential improvements in our system.\"",
        "Document: \"A Corpus-Based Analysis of Coreferential Recency Effect in Japanese Discourse for Tracking Dynamic Topic. This paper describes an analysis of the recency effect of terms appearing in a corpus of meeting transcripts in Japanese. The aim of the study is to improve a method for tracking dynamic topics that have been developed for visualizing a long meeting transcript. The recency effect observed from the appearance of terms in discourse context, the coreferential recency effect, is an important factor to deal with dynamic transition of topic. Statistical analysis using a corpus of meeting transcripts quantitatively clarifies the relationship between linguistic features and the coreferential recency effect.\"",
        "1 is \"The String-to-String Correction Problem\", 2 is \"Content-based audio classification and retrieval using a fuzzy logic system: towards multimedia search engines\"",
        "Given above information, for an author who has written the paper with the title \"On the properties of combination set operations\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003318": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Automatic Inference of Upper Bounds for Recurrence Relations in Cost Analysis':",
        "Document: \"Object-sensitive cost analysis for concurrent objects. AbstractThis article presents a novel cost analysis framework for concurrent objects. Concurrent objects form a well-established model for distributed concurrent systems. In this model, objects are the concurrency units that communicate among them via asynchronous method calls. Cost analysis aims at automatically approximating the resource consumption of executing a program in terms of its input parameters. While cost analysis for sequential programming languages has received considerable attention, concurrency and distribution have been notably less studied. The main challenges of cost analysis in a concurrent setting are as follows. First, inferring precise size abstractions for data in the program in the presence of shared memory. This information is essential for bounding the number of iterations of loops. Second, distribution suggests that analysis must infer the cost of the diverse distributed components separately. We handle this by means of a novel form of object-sensitive recurrence equations that use cost centres in order to keep the resource usage assigned to the different components separate. We have implemented our analysis and evaluated it on several small applications that are classical examples of concurrent and distributed programming. Copyright \u00a9 2015John Wiley & Sons, Ltd.\"",
        "Document: \"Comparing cost functions in resource analysis. Cost functions provide information about the amount of resources required to execute a program in terms of the sizes of input arguments. They can provide an upper-bound, a lower-bound, or the average-case cost. Motivated by the existence of a number of automatic cost analyzers which produce cost functions, we propose an approach for automatically proving that a cost function is smaller than another one. In all applications of resource analysis, such as resource-usage verification, program synthesis and optimization, etc., it is essential to compare cost functions. This allows choosing an implementation with smaller cost or guaranteeing that the given resource-usage bounds are preserved. Unfortunately, automatically generated cost functions for realistic programs tend to be rather intricate, defined by multiple cases, involving non-linear subexpressions (e.g., exponential, polynomial and logarithmic) and they can contain multiple variables, possibly related by means of constraints. Thus, comparing cost functions is far from trivial. Our approach first syntactically transforms functions into simpler forms and then applies a number of sufficient conditions which guarantee that a set of expressions is smaller than another expression. Our preliminary implementation in the COSTA system indicates that the approach can be useful in practice.\"",
        "1 is \"Discovering properties about arrays in simple programs\", 2 is \"Tabled evaluation with delaying for general logic programs\"",
        "Given above information, for an author who has written the paper with the title \"Automatic Inference of Upper Bounds for Recurrence Relations in Cost Analysis\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003381": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Retinal vessel centerline extraction using multiscale matched filter and sparse representation-based classifier':",
        "Document: \"Non-Dominated Sorting Evolution Strategy-Based K-Means Clustering Algorithm For Accent Classification. In this paper a new method is proposed based on the side information and Non-dominated Sorting Evolution Strategy (NSES)-based K-means clustering algorithm. In a distance metric learning approach, data points are transformed to a new space where the Euclidean distances between similar and dissimilar points are at their minimum and maximum, respectively. However the NSES-based K-means clustering yields globally optimized Gaussian components for an accent classification system. This hybrid clustering and classification approach enhances the performance of natural langauge call-routing systems. Accent classification performs the task of acoustic model switching based on the confidence measure for the caller's query.\"",
        "Document: \"A robust adaptive fuzzy position/force control scheme for cooperative manipulators. We examine the complex problem of simultaneous position and internal force control in multiple cooperative manipulator systems. This is done in the presence of unwanted parametric and modeling uncertainties as well as external disturbances. A decentralized adaptive fuzzy controller scheme is proposed here. The controller makes use of a multi-input-multi-output fuzzy logic engine and a systematic online adaptation mechanism. Unlike conventional adaptive controllers, the proposed algorithm requires neither a precise mathematical model of the system's dynamics nor a linear parameterization of the system's uncertain physical parameters. Using a Lyapunov stability approach, the controller is proven to be robust in the face of varying intensity levels of the aforementioned uncertainties. The payload position/orientation error and that of the internal forces are also shown to asymptotically converge to zero under such conditions. The performance of the controller proposed is then compared with that of a well-known conventional adaptive controller.\"",
        "1 is \"A Complete Methodology for Generating Multi-Robot Task Solutions using ASyMTRe-D and Market-Based Task Allocation\", 2 is \"On Spectral Clustering: Analysis and an algorithm.\"",
        "Given above information, for an author who has written the paper with the title \"Retinal vessel centerline extraction using multiscale matched filter and sparse representation-based classifier\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003387": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Using Domain Models for Context-Rich User Logging':",
        "Document: \"On the Interplay of Roles and Power. In this paper a formal analysis of multi-agent systems based on organizational and normative concepts is presented. In particular, social agents and multi-agent systems have structure which is underpinned by roles. Power is legitimized within a social context and it stems from a role, in other words it is based on authority relations between agents. How roles are intertwined with commitments, obligations, power, rights and authority relations is discussed.\"",
        "Document: \"Automatically structuring domain knowledge from text: An overview of current research. This paper presents an overview of automatic methods for building domain knowledge structures (domain models) from text collections. Applications of domain models have a long history within knowledge engineering and artificial intelligence. In the last couple of decades they have surfaced noticeably as a useful tool within natural language processing, information retrieval and semantic web technology. Inspired by the ubiquitous propagation of domain model structures that are emerging in several research disciplines, we give an overview of the current research landscape and some techniques and approaches. We will also discuss trade-offs between different approaches and point to some recent trends.\"",
        "1 is \"Logic Programs for Intelligent Web Search\", 2 is \"Evaluating collaborative filtering recommender systems\"",
        "Given above information, for an author who has written the paper with the title \"Using Domain Models for Context-Rich User Logging\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003449": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Deep Learning for Mobile Multimedia: A Survey':",
        "Document: \"Counter-forensics of median filtering. Median filtering is a well-known non linear denoising filter often used as an harmless post-processing, sometimes also employed to affect the reliability of some forensic techniques. In this work, we present a novel counter-forensic method able to conceal the characteristic traces left by median filtering. By exploiting the knowledge of features used in existing median filtering detectors, we are able to remove the characteristic footprints via suitable random pixel modification, while keeping the quality of the counter-attacked image high. Experimental results show that the proposed method is very effective, computationally efficient and competitive with other state-of-the-art techniques.\"",
        "Document: \"Ensemble of Deep Models for Event Recognition. In this article, we address the problem of recognizing an event from a single related picture. Given the large number of event classes and the limited information contained in a single shot, the problem is known to be particularly hard. To achieve a reliable detection, we propose a combination of multiple classifiers, and we compare three alternative strategies to fuse the results of each classifier, namely: (i) induced order weighted averaging operators, (ii) genetic algorithms, and (iii) particle swarm optimization. Each method is aimed at determining the optimal weights to be assigned to the decision scores yielded by different deep models, according to the relevant optimization strategy. Experimental tests have been performed on three event recognition datasets, evaluating the performance of various deep models, both alone and selectively combined. Experimental results demonstrate that the proposed approach outperforms traditional multiple classifier solutions based on uniform weighting, and outperforms recent state-of-the-art approaches.\n\n\"",
        "1 is \"Adaptive Coding of Monochrome and Color Images\", 2 is \"The internet of things: a survey\"",
        "Given above information, for an author who has written the paper with the title \"Deep Learning for Mobile Multimedia: A Survey\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003463": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'PassPoints: design and longitudinal evaluation of a graphical password system':",
        "Document: \"Novice comprehension of small programs written in the procedural and object-oriented styles. This research studied the comprehension of small procedural and object-oriented programs by novice programmers. The objective was to find out what kinds of information novice programmers extract fr...\"",
        "Document: \"Gender HCI: What About the Software?. Studies building upon theories and research from several domains investigate how gender differences interact with software.\"",
        "1 is \"Layered explanations of software: a methodology for program comprehension\", 2 is \"Mining for proposal reviewers: lessons learned at the national science foundation\"",
        "Given above information, for an author who has written the paper with the title \"PassPoints: design and longitudinal evaluation of a graphical password system\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003518": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Kiln: closing the performance gap between systems with and without persistence support':",
        "Document: \"CACTI: an enhanced cache access and cycle time model. This paper describes an analytical model for the access and cycle times of on-chip direct-mapped and set-associative caches. The inputs to the model are the cache size, block size, and associativity, as well as array organization and process parameters. The model gives estimates that are within 6% of Hspice results for the circuits we have chosen. This model extends previous models and fixes many ...\"",
        "Document: \"Integration and packaging plateaus of processor performance. Integration and packaging performance limits are refined in the context of computer systems. In particular, limits of computer performance under various packaging, architectural, organizational, and design techniques (e.g. gate-array versus custom) are explored. It is concluded that fully integrated processors can have modest electrical signal I/O requirements because the frequency of signals crossing their pins can be several times less than that of the on-chip clock frequency. In order to exploit emerging technologies without high levels of integration, advanced volumetric packaging techniques will still be very important\"",
        "1 is \"Power-aware scheduling of conditional task graphs in real-time multiprocessor systems\", 2 is \"Window-Extent Tradeoffs in Inverse Dithering.\"",
        "Given above information, for an author who has written the paper with the title \"Kiln: closing the performance gap between systems with and without persistence support\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003513": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Differentiable sparse unmixing based on Bregman divergence for hyperspectral remote sensing imagery.':",
        "Document: \"Robust unsupervised feature selection via dual self-representation and manifold regularization. Unsupervised feature selection has become an important and challenging pre-processing step in machine learning and data mining since large amount of unlabelled high dimensional data are often required to be processed. In this paper, we propose an efficient method for robust unsupervised feature selection via dual self-representation and manifold regularization, referred to as DSRMR briefly. On the one hand, a feature self-representation term is used to learn the feature representation coefficient matrix to measure the importance of different feature dimensions. On the other hand, a sample self-representation term is used to automatically learn the sample similarity graph to preserve the local geometrical structure of data which has been verified critical in unsupervised feature selection. By using l2,1-norm to regularize the feature representation residual matrix and representation coefficient matrix, our method is robustness to outliers, and the row sparsity of the feature coefficient matrix induced by l2,1-norm can effectively select representative features. During the optimization process, the feature coefficient matrix and sample similarity graph constrain each other to obtain optimal solution. Experimental results on ten real-world data sets demonstrate that the proposed method can effectively identify important features, outperforming many state-of-the-art unsupervised feature selection methods in terms of clustering accuracy (ACC) and normalized mutual information (NMI).\"",
        "Document: \"Thermal aware workload scheduling with backfilling for green data centers. Data centers now play an important role in modern IT infrastructures. Related research has shown that the energy consumption for data center cooling systems has recently increased significantly. There is also strong evidence to show that high temperatures with in a data center will lead to higher hardware failure rates and thus an increase in maintenance costs. This paper devotes itself in the field of thermal aware resource management for data centers. This paper proposes an analytical model, which describes data center resources with heat transfer properties and workloads with thermal features. Then a thermal aware task scheduling algorithm with backfilling is presented which aims to reduce power consumption and temperatures in a data center. A simulation study is carried out to evaluate the performance of the algorithm. Simulation results show that our algorithm can significantly reduce temperatures in data centers by introducing endurable decline in performance.\"",
        "1 is \"Investigation of the random forest framework for classification of hyperspectral data\", 2 is \"Mathematical justification of a heuristic for statistical correlation of real-life time series\"",
        "Given above information, for an author who has written the paper with the title \"Differentiable sparse unmixing based on Bregman divergence for hyperspectral remote sensing imagery.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003562": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Event-Triggered Privacy-Preserving Average Consensus for Multiagent Networks With Time Delay: An Output Mask Approach':",
        "Document: \"Active Power Oscillation Property Classification of Electric Power Systems Based on SVM. Nowadays, low frequency oscillation has become a major problem threatening the security of large-scale interconnected power systems. According to generation mechanism, active power oscillation of electric power systems can be classified into two categories: free oscillation and forced oscillation. The former results from poor or negative damping ratio of power system and external periodic disturbance may lead to the latter. Thus control strategies to suppress the oscillations are totally different. Distinction from each other of those two different kinds of power oscillations becomes a precondition for suppressing the oscillations with proper measures. This paper proposes a practical approach for power oscillation classification by identifying real-time power oscillation curves. Hilbert transform is employed to obtain envelope curves of the power oscillation curves. Twenty sampling points of the envelope curve are selected as the feature matrices to train and test the supporting vector machine (SVM). The tests on the 16-machine 68-bus benchmark power system and a real power system in China indicate that the proposed oscillation classification method is of high precision.\"",
        "Document: \"Comparisons of ADABOOST, KNN, SVM and Logistic Regression in Classification of Imbalanced Dataset. Data mining classification techniques are affected by the presence of imbalances between classes of a response variable. The difficulty in handling the imbalanced data issue has led to an influx of methods, either resolving the imbalance issue at data or algorithmic level. The R programming language is one of the many tools available for data mining. This paper compares some classification algorithms in R for an imbalanced medical data set. The classifiers ADABOOST, KNN, SVM-RBF and logistic regression were applied to the original, random oversampling and undersampling data sets. Results show that ADABOOST, KNN and SVM-RBF exhibits over-fitting when applied to the original dataset. No overfitting occurs for the random oversampling dataset where by SVM-RBF has the highest accuracy (Training: 91.5%, Testing: 90.6%), sensitivity (Training : 91.0%, Testing: 91.0%), specificity (Training: 92.0%, Testing: 90.2%) and precision (Training: 91.9%, Testing 90.5%) for training and testing data set. For random undersampling, no overfitting occurs only for ADABOOST and logistic regression. Logistic regression is the most stable classifier exhibiting consistent training an testing results.\"",
        "1 is \"Exponential stability and periodic oscillatory solution in BAM networks with delays.\", 2 is \"Multi-class protein fold recognition using support vector machines and neural networks\"",
        "Given above information, for an author who has written the paper with the title \"Event-Triggered Privacy-Preserving Average Consensus for Multiagent Networks With Time Delay: An Output Mask Approach\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003606": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Support of software framework for embedded multi-core systems with Android environments':",
        "Document: \"Compilation for compact power-gating controls. Power leakage constitutes an increasing fraction of the total power consumption in modern semiconductor technologies due to the continuing size reductions and increasing speeds of transistors. Recent studies have attempted to reduce leakage power using integrated architecture and compiler power-gating mechanisms. This approach involves compilers inserting instructions into programs to shut down and wake up components, as appropriate. While early studies showed this approach to be effective, there are concerns about the large amount of power-control instructions being added to programs due to the increasing amount of components equipped with power-gating controls in SoC design platforms. In this article we present a sink-n-hoist framework for a compiler to generate balanced scheduling of power-gating instructions. Our solution attempts to merge several power-gating instructions into a single compound instruction, thereby reducing the amount of power-gating instructions issued. We performed experiments by incorporating our compiler analysis and scheduling policies into SUIF compiler tools and by simulating the energy consumption using Wattch toolkits. The experimental results demonstrate that our mechanisms are effective in reducing the amount of power-gating instructions while further reducing leakage power compared to previous methods.\"",
        "Document: \"Compilers for leakage power reduction. Power leakage constitutes an increasing fraction of the total power consumption in modern semiconductor technologies. Recent research efforts indicate that architectures, compilers, and software can be optimized so as to reduce the switching power (also known as dynamic power) in microprocessors. This has lead to interest in using architecture and compiler optimization to reduce leakage power (also known as static power) in microprocessors. In this article, we investigate compiler-analysis techniques that are related to reducing leakage power. The architecture model in our design is a system with an instruction set to support the control of power gating at the component level. Our compiler provides an analysis framework for utilizing instructions to reduce the leakage power. We present a framework for analyzing data flow for estimating the component activities at fixed points of programs whilst considering pipeline architectures. We also provide equations that can be used by the compiler to determine whether employing power-gating instructions in given program blocks will reduce the total energy requirements. As the duration of power gating on components when executing given program routines is related to the number and complexity of program branches, we propose a set of scheduling policies and evaluate their effectiveness. We performed experiments by incorporating our compiler analysis and scheduling policies into SUIF compiler tools and by simulating the energy consumptions on Wattch toolkits. The experimental results demonstrate that our mechanisms are effective in reducing leakage power in microprocessors.\"",
        "1 is \"An HMM-based approach to humming transcription\", 2 is \"MCUDA: An Efficient Implementation of CUDA Kernels for Multi-core CPUs\"",
        "Given above information, for an author who has written the paper with the title \"Support of software framework for embedded multi-core systems with Android environments\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003657": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Good Vibrations: Artificial Ambience-Based Relay Attack Detection':",
        "Document: \"Secure Application Execution in Mobile Devices. Smart phones have rapidly become hand-held mobile devices capable of sustaining multiple applications. Some of these applications allow access to services including healthcare, financial, online social networks and are becoming common in the smart phone environment. From a security and privacy point of view, this seismic shift is creating new challenges, as the smart phone environment is becoming a suitable platform for security- and privacy-sensitive applications. The need for a strong security architecture for this environment is becoming paramount, especially from the point of view of Secure Application Execution SAE. In this chapter, we explore SAE for applications on smart phone platforms, to ensure application execution is as expected by the application provider. Most of the proposed SAE proposals are based on having a secure and trusted embedded chip on the smart phone. Examples include the GlobalPlatform Trusted Execution Environment, M-Shield and Mobile Trusted Module. These additional hardware components, referred to as secure and trusted devices, provide a secure environment in which the applications can execute security-critical code and/or store data. These secure and trusted devices can become the target of malicious entities; therefore, they require a strong framework that will validate and guarantee the secure application execution. This chapter discusses how we can provide an assurance that applications executing on such devices are secure by validating the secure and trusted hardware.\"",
        "Document: \"An Exploratory Analysis of the Security Risks of the Internet of Things in Finance. The Internet of Things (IoT) is projected to significantly impact consumer finance, through greater customer personalisation, more frictionless payments, and novel pricing schemes. The lack of deployed applications, however, renders it difficult to evaluate potential security risks, which is further complicated by the presence of novel, IoT-specific risks absent in conventional systems. In this work, we present two-part study that uses scenario planning to evaluate emerging risks of IoT in a variety of financial products and services, using ISO/IEC 20005: 2008 to assess those risks from related work. Over 1,400 risks were evaluated from a risk assessment with 7 security professionals within the financial industry, which was contrasted with an external survey of 40 professionals within academia and industry. From this, we draw a range of insights to advise future IoT research and decision-making regarding potentially under-appreciated risks. To our knowledge, we provide the first empirical investigation for which threats, vulnerabilities, asset classes and, ultimately, risks may take precedence in this domain.\"",
        "1 is \"Classification trees with bivariate splits\", 2 is \"Issues in designing middleware for wireless sensor networks\"",
        "Given above information, for an author who has written the paper with the title \"Good Vibrations: Artificial Ambience-Based Relay Attack Detection\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003729": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A method of holistic 3D visualization of arbitrarily large datasets':",
        "Document: \"Clustering of Leaf-Labelled Trees on Free Leafset. This paper focuses on the clustering of leaf-labelled trees on free leafset. It extends the previously proposed algorithms, designed for trees on the same leafset. The term z-equality is proposed and all the necessary consensus and distance notions are redefined with respect to z-equality. The clustering algorithms that focus on maximizing the quality measure for two representative trees are described, together with the measure itself. Finally, the promising results of experiments on tandem duplication trees are presented.\"",
        "Document: \"Managing Multimedia Educational Contents in Databases. Methods of storing, managing and presenting educational multimedia data are proposed. Application of these methods to interactive synchronous and asynchronous Distance Learning systems is discussed. An example system based on the proposed solution is presented in details. The system uses clean organization of educational material and enables storage, management, and presentation of various types of multimedia contents. An important feature of the system is a flexible assigning of attributes to educational objects at different levels of abstraction. A powerful Web interface enables convenient access to data for local and remote users.\"",
        "1 is \"Modeling Directed Obligations and Permissions in Trade Contracts\", 2 is \"Cat-a-Cone: an interactive interface for specifying searches and viewing retrieval results using a large category hierarchy\"",
        "Given above information, for an author who has written the paper with the title \"A method of holistic 3D visualization of arbitrarily large datasets\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003735": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A modified brain storm optimization':",
        "Document: \"An Adaptive Multiobjective Differential Evolution Algorithm. The mutation strategy and control parameter have a significant influence on the performance of differential evolution. An local and global mutation operator based subregion and external set strategies are proposed in this paper. They use the idea of direct simplex method of mathematical programming. It is advantageous to search the better solutions in the search space. The local mutation operator is applied to improve the local search performance of the algorithm and accelerate convergence speed. The global mutation operator is used to exploit a wider area and jump out of the local optima. An adaptive strategy for assigning mutation strategies and control parameters is proposed in this paper. The more successful is a mutation strategy and control parameter setting in the previous search, the more chance it will be used in the further search. Moreover, a novel crossover operator based subregion and external set strategy also is introduced. In order to demonstrate the performance of the proposed algorithm, it is compared with the MOEA/D-DE and the hybrid-NSGA-II-DE. The result indicates that the proposed algorithm is efficient.\"",
        "Document: \"A Monte-Carlo ant colony system for scheduling multi-mode projects with uncertainties to optimize cash flows. Project scheduling under uncertainty is a challenging field of research that has attracted an increasing attention in recent years. While most existing studies only considered the classical single-mode project scheduling problem with makespan criterion under uncertainty, this paper aims to deal with a more realistic and complicated model called the stochastic multi-mode resource constrained project scheduling problem with discounted cash flows (S-MRCPSPDCF). In the model, uncertainty is sourced from activity durations and costs, which are given by random variables. The objective is to find an optimal baseline schedule so that the project's expected net present value (NPV) of cash flows is maximized. In order to solve this intractable problem, an ant colony system (ACS) algorithm is designed. The algorithm dispatches a group of ants to build baseline schedules iteratively based on pheromones and an expected discounted cost (EDC) heuristic. In addition, because it is impossible to evaluate the expected NPVs of baseline schedules directly due to the presence of random variables, the algorithm adopts Monte Carlo (MC) simulations to evaluate the performance of baseline schedules. Experimental results on 33 instances demonstrate the effectiveness of the proposed scheduling model and the ACS approach.\"",
        "1 is \"A Grid-Based Evolutionary Algorithm for Many-Objective Optimization\", 2 is \"Bridging the Gap: Many-Objective Optimization and Informed Decision-Making.\"",
        "Given above information, for an author who has written the paper with the title \"A modified brain storm optimization\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003873": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Multi-AUV Target Search Based on Bioinspired Neurodynamics Model in 3-D Underwater Environments.':",
        "Document: \"Analyzing livestock farm odour using a neuro-fuzzy approach. An adaptive neuro-fuzzy based method for analyzing odour generation factors to the perception of livestock farm odour was proposed. In this approach, the parameters associated with a given membership function could be tuned so as to tailor the membership functions to the input/output data in order to account for these types of variations in the data values. A multi-factor livestock farm odour model was developed, and both numeric factors and linguistic factors were considered. The proposed method was tested with a livestock farm odour database. The results demonstrated the effectiveness of the proposed approach in comparison to a typical neural network.\"",
        "Document: \"Particle Swarm Optimization Approach To Thruster Fault-Tolerant Control Of Unmanned Underwater Vehicles. A control reconfiguration approach based on particle swarm optimization (PSO) for unmanned underwater vehicles (UUV) thruster fault-tolerant control is presented in this paper. The underwater vehicle named URIS has four horizontal thrusters. When faults in a thruster occur, the corresponding weight matrix is updated to compensate for the restricted usage of the faulty thruster. To complete an appropriate control law reconfiguration, a control energy cost function is used as the optimization criterion. The PSO algorithm is used to find the solution to the control reallocation problem. Compared to the weighted pseudo-inverse method, the PSO algorithm does not need truncation or scaling to ensure the feasibility of the solution because its particles search for the solution in the feasible space. Both the magnitude error and direction error of the obtained control input vector using the PSO algorithm are equal to zero in keeping with its original state.\"",
        "1 is \"Dynamic Output Feedback Control of Switched Linear Systems\", 2 is \"High precision touch screen interaction\"",
        "Given above information, for an author who has written the paper with the title \"Multi-AUV Target Search Based on Bioinspired Neurodynamics Model in 3-D Underwater Environments.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003889": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Towards an immunity-based system for detecting masqueraders':",
        "Document: \"Framework of an immunity-based anomaly detection system for user behavior. This paper focuses on anomaly detection in user behavior. We present a review of our immunity-based anomaly detection system, and propose a framework of the immunity-based anomaly detection system with a new mechanism of diversity generation. In the framework, each computer on a LAN generates diverse agents, and the agents generated on each computer are shared with all other computers on the LAN. The sharing of agents contributes to their diversity. In addition, we propose an evaluation framework of immunity-based anomaly detection, which is capable of evaluating the differences in detection accuracy between internal and external malicious users.\"",
        "Document: \"Active noise control by an immune algorithm: adaptation in immune system as an evolution. A new information processing architecture is extracted from immune systems. By focusing on informational features of the immune system (i.e. specificity, diversity, tolerance, and memory), an immune algorithm is proposed. The algorithm proceeds in three steps: diversity generation, establishment of self-tolerance, and memorizing non-self. An agent-based architecture based on the local memory hypothesis and a network-based architecture based on the network hypothesis are discussed. An agent-based architecture is elaborated with an application to control systems where the knowledge about disturbances is not available. An adaptive disturbance neutralizer is formalized and simulated for a simple plant\"",
        "1 is \"Neurotypical and autistic children aged 6 to 7 years in a speaker-listener situation with a human or a minimalist InterActor robot\", 2 is \"Implementing and testing a virus throttle\"",
        "Given above information, for an author who has written the paper with the title \"Towards an immunity-based system for detecting masqueraders\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003924": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'E-business intelligence via MCMP-based data mining methods':",
        "Document: \"Kimberlites Identification by Classification Methods. Kimberlites identification is a very important task for diamond mining. In traditional way, geologists draw upon past experience to do this work. Whether the bedrock should be drilled depends on their analysis of rock samples. This method has two disadvantages. First, as the database increasing, it becomes more difficult to do this work by manual inspection. Secondly, the accuracy is influenced by the expert's experience, and it reaches scarcely 80 percents averagely. So an analytical method to kimberlites identification over large geochemical datasets is demanded. This article applies two methods (SVM and decision tree) to a dataset provided by a mining company. Comparing the performances of these two methods, our results demonstrate that SVM is an effective method for this work.\"",
        "Document: \"Modelling The Mitigation Impact Of Insurance In Operational Risk Management. The paper is going to quantify the mitigation of the insurance as a risk mitigant in operational risk management for the commercial bank. Due to the uncertainties associated with the insurance policy, such as counterparty default, payment uncertainty and the liquidity risk (i.e., delayed payment), the recovery amounts are subjected to be kind of uncertainty. Thus, the efficiency of insurance as a risk mitigant may be discounted. We aim at going one step further to consider counterparty default, payment uncertainty and liquidity risk. While the counterparty default model focuses on calibration of the default time, the payment uncertainty is set as a non-increasing function depending on loss severity. The key conclusions are that counterparty default does not have significant impact on the capital calibration but still paramount in risk management, and insurance as a risk mitigant indeed improve the operational risk profile of bank and lower the capital requirement to some extent\"",
        "1 is \"KBA: Kernel Boundary Alignment Considering Imbalanced Data Distribution\", 2 is \"Development of a Database Course for Bioinformatics.\"",
        "Given above information, for an author who has written the paper with the title \"E-business intelligence via MCMP-based data mining methods\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003937": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A (fault-tolerant)2 scheduler for real-time HW tasks':",
        "Document: \"A Highly Parameterized and Efficient FPGA-Based Skeleton for Pairwise Biological Sequence Alignment. This paper presents the design and implementation of the most parameterisable field-programmable gate array (FPGA)-based skeleton for pairwise biological sequence alignment reported in the literature. The skeleton is parameterised in terms of the sequence symbol type, i.e., DNA, RNA, or protein sequences, the sequence lengths, the match score, i.e., the score attributed to a symbol match, mismatch or gap, and the matching task, i.e., the algorithm used to match sequences, which includes global alignment, local alignment, and overlapped matching. Instances of the skeleton implement the Smith-Waterman and the Needleman-Wunsch algorithms. The skeleton has the advantage of being captured in the Handel-C language, which makes it FPGA platform-independent. Hence, the same code could be ported across a variety of FPGA families. It implements the sequence alignment algorithm in hand using a pipeline of basic processing elements, which are tailored to the algorithm parameters. This paper presents a number of optimizations built into the skeleton and applied at compile-time depending on the user-supplied parameters. These result in high performance FPGA implementations tailored to the algorithm in hand. For instance, actual hardware implementations of the Smith-Waterman algorithm for Protein sequence alignment achieve speedups of two orders of magnitude compared to equivalent standard desktop software implementations.\"",
        "Document: \"A Prolog-Based Hardware Development Environment. This paper presents a Hardware Development Environment based on the logic programming language Prolog. Central to this environment are a hardware description notation called HIDE, and a high level generator, which takes an application specific, high level algorithm description, and translates it into a HIDE description. The latter describes scaleable and parameterised architectures using a small set of Prolog constructors. EDIF netlists can be automatically generated from HIDE descriptions. The high-level algorithm descriptions are based on a library of reusable Hardware Skeletons. A hardware skeleton is a parameterised description of a task-specific architecture, to which the user can supply parameters such as values, functions or even other skeletons. A skeleton contains built-in rules, written in Prolog that will apply optimisations specific to the target hardware at the implementation phase. This is the key towards the satisfaction of the dual requirement of high-level abstract hardware design and hardware efficiency.\"",
        "1 is \"Parallel Architectures and Algorithms for Image Component Labeling\", 2 is \"Evolvable Systems in Hardware Design: Taxonomy, Survey and Applications\"",
        "Given above information, for an author who has written the paper with the title \"A (fault-tolerant)2 scheduler for real-time HW tasks\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003989": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'An iterative learning control method with application to robot manipulators':",
        "Document: \"Feature subset selection using separability index matrix. Effective Feature Subset Selection (FSS) is an important step when designing engineering systems that classify complex data in real time. The electromyographic (EMG) signal-based walking assistance system is a typical system that requires an efficient computational architecture for classification. The performance of such a system depends largely on a criterion function that assesses the quality of selected feature subsets. However, many well-known conventional criterion functions use less relevant features for classification or they have a high computational cost. Here, we propose a new criterion function that provides more effective FSS. The proposed criterion function, known as a separability index matrix (SIM), provides features pertinent to the classification task and a very low computational cost. This new function produces to a simple feature selection algorithm when combined with the forward search paradigm. We performed extensive experimental comparisons in terms of classification accuracy and computational costs to confirm that the proposed algorithm outperformed other filter-type feature selection methods that are based on various distance measures, including inter-intra, Euclidean, Mahalanobis, and Bhattacharyya distances. We then applied the proposed method to a gait phase recognition problem in our EMG signal-based walking assistance system. We demonstrated that the proposed method performed competitively when compared with other wrapper-type feature selection methods in terms of class-separability and recognition rate.\"",
        "Document: \"Design of \u201cPersonalized\u201d Classifier Using Soft Computing Techniques for \u201cPersonalized\u201d Facial Expression Recognition. We propose a design method of personalized classifier with soft computing techniques for automatic facial expression recognition. Motivated by the fact that even though human facial expressions of emotion are often ambiguous and inconsistent, humans are, in general, very good at classifying such complex images. In consideration of individual characteristics, we adopt a similar strategy of building a personalized classifier to enhance the recognition performance. For realization, we use a soft computing technique of neurofuzzy approach. Specifically, two core steps-ldquomodel building/modificationrdquo and ldquofeature selectionrdquo-are applied to build a ldquopersonalizedrdquo classification structure. The proposed scheme of classifier construction achieves a higher classification rate, minimal network parameters, easy-to-extend structure, and faster computation time, among others. Four sets of facial expression data are chosen and image features are extracted from each of them to show effectiveness of the proposed method, which confirms considerable enhancement of the whole performance.\"",
        "1 is \"Performance comparison of several feature selection methods based on node pruning in handwritten character recognition\", 2 is \"Real-time surgery simulation with haptic feedback using finite elements\"",
        "Given above information, for an author who has written the paper with the title \"An iterative learning control method with application to robot manipulators\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003997": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Equilibrium strategies for an OFDM network that might be under a jamming attack':",
        "Document: \"Exploiting the physical layer for enhanced security. While conventional cryptographic security mechanisms are essential to the overall problem of securing wireless networks, they do not directly leverage the unique properties of the wireless domain to address security threats. The wireless medium is a powerful source of domain-specific information that can complement and enhance traditional security mechanisms. In this article we argue that new security paradigms which exploit physical layer properties of the wireless medium, such as the rapid spatial, spectral, and temporal decorrelation properties of the radio channel, can enhance confidentiality and authentication services. We outline some basic constructions for these services, and then provide a case study for how such strategies can be integrated into a broader security framework for a wireless network.\"",
        "Document: \"An Eavesdropping Game with SINR as an Objective Function. We examine eavesdropping over wireless channels, where secret communication in the presence of an eavesdropper is formulated as a zero-sum game. In our problem, the legitimate receiver does not have complete knowledge about the environment, i.e. does not know the exact values of the channels gains, but instead knows just their distribution. To communicate secretly, the user must decide how to transmit its information across subchannels under a worst-case condition and thus, the legal user faces a max-min optimization problem. To formulate the optimization problem, we pose the environment as a secondary player in a zero-sum game whose objective is to hamper communication by the user. Thus, nature faces a min-max optimization problem. In our formulation, we consider signal-to-interference ratio (SINR) as a payoff function. We then study two specific scenarios: (i) the user does not know the channels gains; and (ii) the user does not know how the noise is distributed among the main channels. We show that in model (i) in his optimal behavior the user transmits signal energy uniformly across a subset of selected channels. In model (ii), if the user does not know the eavesdropper's channel gains he/she also employs a strategy involving uniformly distributing energy across a subset of channels. However, if the user acquires extra knowledge about environment, e.g. the eavesdropper's channel gains, the user may better tune his/her power allocation among the channels. We provide criteria for selecting which channels the user should transmit on by deriving closed-form expressions for optimal strategies for both players.\"",
        "1 is \"On Limits of Wireless Communications in a Fading Environment when Using Multiple Antennas\", 2 is \"Conditions for target recovery in spatial compressive sensing for MIMO radar\"",
        "Given above information, for an author who has written the paper with the title \"Equilibrium strategies for an OFDM network that might be under a jamming attack\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003999": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Domain-Specific Hybrid FPGA: Architecture and Floating Point Applications':",
        "Document: \"Hierarchical Benchmark Circuit Generation for FPGA Architecture Evaluation. We describe a stochastic circuit generator that can be used to automatically create benchmark circuits for use in FPGA architecture studies. The circuits consist of a hierarchy of interconnected modules, reflecting the structure of circuits designed using a system-on-chip design flow. Within each level of hierarchy, modules can be connected in a bus, star, or dataflow configuration. Our circuit generator is calibrated based on a careful study of existing system-on-chip circuits. We show that our benchmark circuits lead to more realistic architectural conclusions than circuits generated using previous generators.\"",
        "Document: \"A crosstalk-aware timing-driven router for FPGAs. As integrated circuits are migrated to more advanced technologies, it has become clear that crosstalk is an important physical phenomenon that must be taken into account. Crosstalk has primarily been a concern for ASICs, multi-chip modules, and custom chips, however, it will soon become a concern in FPGAs. In this paper, we describe the first published crosstalk-aware router that targets FPGAs. We show that, in a representative FPGA architecture implemented in a 0.18mm technology, the average routing delay in the presence of crosstalk can be reduced by 7.1% compared to a router with no knowledge of crosstalk. About half of this improvement is due to a tighter delay estimator, and half is due to an improved routing algorithm.\"",
        "1 is \"Fast motion vector estimation using multiresolution-spatio-temporal correlations\", 2 is \"Incremental distributed trigger insertion for efficient FPGA debug\"",
        "Given above information, for an author who has written the paper with the title \"Domain-Specific Hybrid FPGA: Architecture and Floating Point Applications\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004022": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Spatial Modulation for Molecular Communication.':",
        "Document: \"Error Probability and Capacity Analysis of Generalised Pre-Coding Aided Spatial Modulation. The recently proposed multiple input multiple output (MIMO) transmission scheme termed as generalized pre-coding aided spatial modulation (GPSM) is analyzed, where the key idea is that a particular subset of receive antennas is activated and the specific activation pattern itself conveys useful implicit information. We provide the upper bound of both the symbol error ratio (SER) and bit error ratio (BER) expression of the GPSM scheme of a low-complexity decoupled detector. Furthermore, the corresponding discrete-input continuous-output memoryless channel (DCMC) capacity as well as the achievable rate is quantified. Our analytical SER and BER upper bound expressions are confirmed to be tight by our numerical results. We also show that our GPSM scheme constitutes a flexible MIMO arrangement and there is always a beneficial configuration for our GPSM scheme that offers the same bandwidth efficiency as that of its conventional MIMO counterpart at a lower signal to noise ratio (SNR) per bit.\"",
        "Document: \"Differential acquisition of m-sequences using recursive soft sequential estimation. A novel sequential estimation method is proposed for the acquisition of m-sequences. This sequential estimation method exploits the principle of iterative soft-in-soft-out (SISO) decoding for enhancing the acquisition performance, and that of differential preprocessing for the sake of achieving an enhanced acquisition performance, when communicating over various communication environments. Hence, the advocated acquisition arrangement is referred to as the differential recursive soft sequential estimation (DRSSE) acquisition scheme. The DRSSE acquisition scheme exhibits a low complexity, which is similar to that of an m-sequence generator, while achieving an acquisition time that is linearly dependent on the number of stages in the m-sequence generator. A low acquisition time is achieved with the advent of the property that the proposed DRSSE scheme is capable of determining the real-time reliabilities associated with the decision concerning a set of, say, S consecutive chips. This set of consecutive chips constitutes the sufficient initial condition for enabling the local m-sequence generator to produce a synchronized local despreading m-sequence replica. Owing to these attractive characteristics, the DRSSE acquisition scheme constitutes a promising initial synchronization scheme for acquisition of long m-sequences, when communicating over various propagation environments.\"",
        "1 is \"Dynamic Spectrum Management (DSM) Algorithms for Multi-User xDSL.\", 2 is \"Simplified Spatial Correlation Models for Clustered MIMO Channels With Different Array Configurations\"",
        "Given above information, for an author who has written the paper with the title \"Spatial Modulation for Molecular Communication.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004067": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'MULTIPLE HUMAN TRACKING IN NON-SPECIFIC COVERAGE WITH WEARABLE CAMERAS':",
        "Document: \"Shape Correspondence Through Landmark Sliding. Motivated by improving statistical shape analysis, this paper presents a novel landmark-based method for accurate shape correspondence, where the general goal is to align multiple shape instances by corresponding a set of given landmark points along those shapes. Different from previous methods, we consider both global shape deformation and local geometric features in defining the shape-correspondence cost function to achieve a consistency between the landmark correspondence and the underlying shape correspondence. According to this cost function, we develop a novel landmark-sliding algorithm to achieve optimal landmark-based shape correspondence with preserved shape topology. The proposed method can be applied to correspond various 2D shapes in the forms of single closed curves, single open curves, self-crossing curves, and multiple curves. We also discuss the practical issue of landmark initialization. The proposed method has been tested on various biological shapes arising from medical image analysis and validated in constructing statistical shape models.\"",
        "Document: \"2D nonrigid partial shape matching using MCMC and contour subdivision. Shape matching has many applications in computer vision, such as shape classification, object recognition, object detection, and localization. In 2D cases, shape instances are 2D closed contours and matching two shape contours can usually be formulated as finding a one-to-one dense point correspondence between them. However, in practice, many shape contours are extracted from real images and may contain partial occlusions. This leads to the challenging partial shape matching problem, where we need to identify and match a subset of segments of the two shape contours. In this paper, we propose a new MCMC (Markov chain Monte Carlo) based algorithm to handle partial shape matching with mildly non-rigid deformations. Specifically, we represent each shape contour by a set of ordered landmark points. The selection of a subset of these landmark points into the shape matching is evaluated and updated by a posterior distribution, which is composed of both a matching likelihood and a prior distribution. This prior distribution favors the inclusion of more and consecutive landmark points into the matching. To better describe the matching likelihood, we develop a contour-subdivision technique to highlight the contour segment with highest matching cost from the selected subsequences of the points. In our experiments, we construct 1,600 test shape instances by introducing partial occlusions to the 40 shapes chosen from different categories in MPEG-7 dataset. We evaluate the performance of the proposed algorithm by comparing with three well-known partial shape matching methods.\"",
        "1 is \"3d Vertebrae Segmentation Using Graph Cuts With Shape Prior Constraints\", 2 is \"Global Optimization through Rotation Space Search\"",
        "Given above information, for an author who has written the paper with the title \"MULTIPLE HUMAN TRACKING IN NON-SPECIFIC COVERAGE WITH WEARABLE CAMERAS\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004138": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Dynamic Test Compaction for Transition Faults in Broadside Scan Testing Based on an Influence Cone Measure':",
        "Document: \"Low-capture-power at-speed testing using partial launch-on-capture test scheme. Most previous DFT-based techniques for low-capture-power broadside testing can only reduce test power in one of the two capture cycles, launch cycle and capture cycle. Even if some methods can reduce both of them, they may make some testable faults in standard broadside testing untestable. In this paper, a new test application scheme called partial launch-on-capture (PLOC) is proposed to solve the two problems. It allows only a part of scan flip-flops to be active in the launch cycle and capture cycle. In order to guarantee that all testable faults in the standard broadside testing can be detected in the new test scheme, extra efforts are required to check the overlapping part. In addition, calculation of the overlapping part is different from previous techniques for the stuck-at fault testing because broadside testing requires two consecutive capture cycles. Therefore, a new scan flip-flop partition algorithm is proposed to minimize the overlapping part. Sufficient experimental results are presented to demonstrate the efficiency of the proposed method.\"",
        "Document: \"A global algorithm for the partial scan design problem using circuit state information. A global partial scan design algorithm based on circuit state information is proposed. Valid states obtained via logic simulation are used to evaluate testability of the circuit. A testability measure based on the density of encoding is used to select scan flip flops, and the problem is formulated into an optimization problem. An algorithm is presented to obtain an initial partial scan design solution, and a variety of techniques are used to subsequently derive an optimal solution. The most significant technique used in the global algorithm is that a dynamic testability measure is adopted, which can greatly reduce the size of the search space and enhance the effectiveness of the search problem. The partial scan design method can greatly reduce potential backtracks during test generation. Experimental results demonstrate 100% test efficiency can be obtained for most circuits selecting fewer scan flip flops than other methods\"",
        "1 is \"Adaptive learning of an accurate skin-color model\", 2 is \"Adaptive channel queue routing on k-ary n-cubes\"",
        "Given above information, for an author who has written the paper with the title \"Dynamic Test Compaction for Transition Faults in Broadside Scan Testing Based on an Influence Cone Measure\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004211": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A novel adaptive wide-angle SAR imaging algorithm based on Boltzmann machine model':",
        "Document: \"Sparse Signal Recovery With Omp Algorithm Using Sensing Measurement Matrix. Orthogonal matching pursuit (OMP) algorithm with random measurement matrix (RMM), often selects an incorrect variable due to the induced coherent interference between the columns of RMM. In this paper, we propose a sensing measurement matrix (SMM)-OMP which mitigates the coherent interference and thus improves the successful recovery probability of signal. It is shown that the SMM-OMP selects all the significant variables of the sparse signal before selecting the incorrect ones. We present a mutual incoherent property (MIP) based theoretical analysis to verify that the proposed method has a better performance than RMM-OMP. Various simulation results confirm our proposed method efficiency.\"",
        "Document: \"Support Vector Regression for Basis Selection in Laplacian Noise Environment. We demonstrate that the objective function of a basis selection problem in Laplacian noise environment falls into the framework of support vector regression (SVR), and, by iteratively solving a convex quadratic programming (QP) problem that guarantees a globally optimal solution, the sparse solution to the inverse problem can be found. The effectiveness of the proposed algorithm is verified via th...\"",
        "1 is \"Asynchronous time-of-arrival-based source localization\", 2 is \"Image quality assessment: from error visibility to structural similarity.\"",
        "Given above information, for an author who has written the paper with the title \"A novel adaptive wide-angle SAR imaging algorithm based on Boltzmann machine model\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004258": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Multi-objective shape segmentation and labeling':",
        "Document: \"Compact and efficient generation of radiance transfer for dynamically articulated characters. We present a data-driven technique for generating the precomputed radiance transfer vectors of an animated character as a function of its joint angles. We learn a linear model for generating real-time lighting effects on articulated characters while capturing soft self-shadows caused by dynamic distant lighting. Indirect illumination can also be reproduced using our framework. Previous data-driven techniques have either restricted the type of lighting response (generating only ambient occlusion), the type of animated sequences (response functions to external forces) or have complicated runtime algorithms and incur non-trivial memory costs. We provide insights into the dimensionality reduction of the pose and coefficient spaces. Our model can be fit quickly as a preprocess, is very compact (~1 MB) and runtime transfer vectors are generated using a simple algorithm in real-time ( 100 Hz using a CPU-only implementation.) We can reproduce lighting effects on hundreds of trained poses using less memory than required to store a single mesh's PRT coefficients. Moreover, our model extrapolates to produce smooth, believable lighting results on novel poses and our method can be easily integrated into existing interactive content pipelines.\"",
        "Document: \"LACES: live authoring through compositing and editing of streaming video. Video authoring activity typically consists of three phases: planning (pre-production), capture (production) and processing (post-production). The status quo is that these phases occur separately, and the latter two have a significant amount of \\\"slack time\\\", where the camera operator is watching the scene unfold during capture, and the editor is re-watching and navigating through recorded footage during post-production. While this process is well suited to creating polished or professional video, video clips produced by casual video makers as seen in online forums could benefit from some editing without the overhead of current authoring tools. We introduce LACES, a tablet-based system enabling simple video manipulations in the midst of filming. Seamless in-situ integration of video capture and manipulation forms a novel workflow, allowing greater spontaneity and exploration of video creation.\"",
        "1 is \"ConMan: A visual programming language for interactive graphics\", 2 is \"Structured importance sampling of environment maps\"",
        "Given above information, for an author who has written the paper with the title \"Multi-objective shape segmentation and labeling\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004291": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A Blind Time-Reversal Detector in the Presence of Channel Correlation':",
        "Document: \"Distributed graph coloring for self-organization in LTE networks. Primary Component Carrier Selection and Physical Cell ID Assignment are two important self-configuration problems pertinent to LTE-Advanced. In this work, we investigate the possibility to solve these problems in a distributive manner using a graph coloring approach. Algorithms based on real-valued interference pricing of conflicts converge rapidly to a local optimum, whereas algorithms with binary interference pricing have a chance to find a global optimum. We apply both local search algorithms and complete algorithms such as Asynchronous Weak-Commitment Search. For system level performance evaluation, a picocellular scenario is considered, with indoor base stations in office houses placed in a Manhattan grid. We investigate a growing network, where neighbor cell lists are generated using practical measurement and reporting models. Distributed selection of conflict-free primary component carriers is shown to converge with 5 or more component carriers, while distributed assignment of confusionfree physical cell IDs is shown to converge with less than 15 IDs. The results reveal that the use of binary pricing of interference with an attempt to find a global optimum outperforms real-valued pricing.\"",
        "Document: \"Self-Organizing Algorithms for Interference Coordination in Small Cell Networks. This paper discusses novel joint (intracell and intercell) resource allocation algorithms for self-organized interference coordination in multicarrier multiple-input multiple-output (MIMO) small cell networks. The proposed algorithms enable interference coordination autonomously, over multiple degrees of freedom, such as base station transmit powers, transmit precoders, and user scheduling weights...\"",
        "1 is \"Optimized signaling for MIMO interference systems with feedback\", 2 is \"A Novel Distributed Scheduling Algorithm for Downlink Relay Networks\"",
        "Given above information, for an author who has written the paper with the title \"A Blind Time-Reversal Detector in the Presence of Channel Correlation\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004363": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Allocation of layer bandwidths and FECs for video multicast over wired and wireless networks':",
        "Document: \"TCP and UDP performance for Internet over optical packet-switched networks. Optical packet switched (OPS) network is a strong candidate for the future optical Internet. In this paper, we study packet aggregation and deflection routing a employed in OPS networks on the performance of upper layer Internet protocols represented by TCP and UDP. Regarding packet aggregation schemes, we study no aggregation, mixed-flow aggregation, and per-flow aggregation. Our results show that for both TCP and UDP, mixed-flow aggregation significantly decreases the fairness at large aggregation intervals. Both aggregation schemes increase UDP delay jitter. Regarding deflection routing, we show that deflection routing significantly improves TCP throughput in spit of the out-of-order packet delivery. However, the congestion of the deflection path significantly affects the improvement that can be achieved. UDP throughput does not suffer from out-of-order packet delivery, and hence its throughput improvement by deflection routing can be even more prominent. The larger the deflection cost (delay difference between the deflection path and the shortest path) is, the larger UDP delay jitter results. Deflection cost, however, does not affect the throughput and fairness very much for both TCP and UDP.\"",
        "Document: \"WIANI: wireless infrastructure and ad-hoc network integration. Wireless networks have been widely deployed in recent years to provide high-speed Internet access to mobile users. In traditional IEEE 802.11 wireless LANs, all users directly connect to an access point (AP) and all packets are forwarded by the AP. As a result, the coverage and capacity of the network is limited. If ad hoc mode is adopted in both the AP and mobile nodes, the one hop connections from AP can be extended to multiple hops. Such an architecture, termed WIANI (wireless infrastructure and ad-hoc network integration), is able to extend the network coverage beyond the coverage of APs. Furthermore, users may take advantage of the ad hoc connections to forward local data and hence alleviate the traffic load through the AP and increase the network capacity. We propose a dynamic load-balancing protocol for WIANI in which all APs and nodes operate in ad-hoc mode. Our protocol consists of two parts, a load-balancing zone forming algorithm and a weighted x-hop routing algorithm. Using simulation, we show that our protocol improves system throughput and reduces packet delivery delay.\"",
        "1 is \"The expectation-maximization algorithm\", 2 is \"Sensor-assisted wi-fi indoor location system for adapting to environmental dynamics\"",
        "Given above information, for an author who has written the paper with the title \"Allocation of layer bandwidths and FECs for video multicast over wired and wireless networks\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004390": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Long-Range Prediction For Real.-Time Mpeg Video Traffic: An H-Infinity Filter Approach':",
        "Document: \"Parasitic Effect Removal for Analog Measurement in P1149.4 Environment. An intrinsic response extraction algorithm is derived and implemented to remove the parasitic effects in P1149.4 analog measurement. The methodology is tested on and verified by SPICE simulation results and real measurement data.\"",
        "Document: \"BIST for Measuring Clock Jitter of Charge-Pump Phase-Locked Loops. This paper presents a built-in self-test (BIST) circuit that measures the clock jitter of the charge-pump phase-locked loops (PLLs). The jitter-measurement structure is based on a novel time-to-digital converter (TDC) which has a high resolution. A small area overhead is also achieved using the voltage-controlled oscillator and the loop filter of the PLL under test as parts of the TDC. The experiment result shows that the resolution is about 1 ps and that the measurement error is smaller than 20%.\"",
        "1 is \"A Crosstalk Sensor Implementation for Measuring Interferences in Digital CMOS VLSI Circuits\", 2 is \"Rectilinear block placement using B*-trees\"",
        "Given above information, for an author who has written the paper with the title \"Long-Range Prediction For Real.-Time Mpeg Video Traffic: An H-Infinity Filter Approach\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004407": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'An optimized hardware architecture for the montgomery multiplication algorithm':",
        "Document: \"Performance bounds of partial run-time reconfiguration in high-performance reconfigurable computing. High-Performance Reconfigurable Computing (HPRC) systems have always been characterized by their high performance and flexibility. Flexibility has been traditionally exploited through the Run-Time Reconfiguration (RTR) provided by most of the available platforms. However, the RTR feature comes with the cost of high configuration overhead which might negatively impact the overall performance. Currently, modern FPGAs have more advanced mechanisms for reducing the configuration overheads, particularly Partial Run-Time Reconfiguration (PRTR). It has been perceived that PRTR on HPRC systems can be the trend for improving the performance. In this work, we will investigate the potential of PRTR on HPRC by formally analyzing the execution model and experimentally verifying our analytical findings by enabling PRTR for the first time, to the best of our knowledge, on one of the state-of-the-art HPRC systems, Cray XD1. Our approach is general and can be applied to any of the available HPRC systems. The paper will conclude with recommendations and conditions, based on our conceptual and experimental work, for the optimal utilization of PRTR as well as possible future usage in HPRC.\"",
        "Document: \"An Adaptive Hybrid OLAP Architecture with optimized memory access patterns. OLAP (On-Line Analytical Processing) is an approach to efficiently evaluate multidimensional data for business intelligence applications. OLAP contributes to business decision-making by identifying, extracting, and analyzing multidimensional data. The fundamental structure of OLAP is a data cube that enables users to interactively explore the distinct data dimensions. Processing depends on the complexity of queries, dimensionality, and growing size of the data cube. As data volumes keep on increasing and the demands by business users also increase, higher processing speed than ever is needed, as faster processing means faster decisions and more profit to industry.In this paper, we are proposing an Adaptive Hybrid OLAP Architecture that takes advantage of heterogeneous systems with GPUs and CPUs and leverages their different memory subsystems characteristics to minimize response time. Thus, our approach (a)\u807dexploits both types of hardware rather than using the CPU only as a frontend for GPU; (b)\u807duses two different data formats (multidimensional cube and relational cube) to match the GPU and CPU memory access patterns and diverts queries adaptively to the best resource for solving the problem at hand; (c)\u807dexploits data locality of multidimensional OLAP on NUMA multicore systems through intelligent thread placement; and (d)\u807dguides its adaptation and choices by an architectural model that captures the memory access patterns and the underlying data characteristics.Results show an increase in performance by roughly four folds over the best known related approach. There is also the important economical factor. The proposed hybrid system costs only 10\u807d% more than same system without GPU. With this small extra cost, the added GPU increases query processing by almost 2\u807dtimes.\"",
        "1 is \"CMOS SRAM scaling limits under optimum stability constraints\", 2 is \"Switch MSHR: a technique to reduce remote read memory access time in CC-NUMA multiprocessors\"",
        "Given above information, for an author who has written the paper with the title \"An optimized hardware architecture for the montgomery multiplication algorithm\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004456": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Dynamical Behaviors of a Large Class of General Delayed Neural Networks':",
        "Document: \"Stability of Analytic Neural Networks With Event-Triggered Synaptic Feedbacks. In this paper, we investigate stability of a class of analytic neural networks with the synaptic feedback via event-triggered rules. This model is general and include Hopfield neural network as a special case. These event-trigger rules can efficiently reduces loads of computation and information transmission at synapses of the neurons. The synaptic feedback of each neuron keeps a constant value ba...\"",
        "Document: \"Finite-time and fixed-time anti-synchronization of neural networks with time-varying delays. In this paper, the finite-time and fixed-time anti-synchronization of master-slave dynamical systems with time-varying delays are investigated. The feedback controller is designed only depending on the system state at present time t, but independent of the delayed states, which would be much easier to be verified and realized in practice. Rigorous analysis is developed in two cases with respect to the different ranges of initial state of error systems. Then the absolute value of each error state component can be considered as that, flowing from the initial value to 1 first, then from 1 to 0, and the time it needs in this whole process is finite. As special cases, several neural network models with unbounded time delays are addressed to illustrate the effectiveness and efficiency of our obtained results.\"",
        "1 is \"Dynamics of periodic Cohen\u2013Grossberg neural networks with varying delays\", 2 is \"An Optimal Fuzzy Pid Controller\"",
        "Given above information, for an author who has written the paper with the title \"Dynamical Behaviors of a Large Class of General Delayed Neural Networks\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004486": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Context-aware Point-of-Interest Recommendation Using Tensor Factorization with Social Regularization':",
        "Document: \"System Identification Under Regular, Binary, and Quantized Observations: Moderate Deviations Error Bounds. This technical note presents new results on probabilistic characterization of identification errors in their relationships to data sizes and accuracy requirements. Employing the moderate deviations principle, this technical note shows that if the identification accuracy progressively increases with a suitable rate, the probability of an estimate going outside the precision bounds decays exponentially with the data size. The precise rate of the decaying probability is obtained. System identification under regular, binary, and quantized observations are considered. Impact of unmodeled dynamics is also investigated.\"",
        "Document: \"Exploiting hybrid contexts for Tweet segmentation. Twitter has attracted hundred millions of users to share and disseminate most up-to-date information. However, the noisy and short nature of tweets makes many applications in information retrieval (IR) and natural language processing (NLP) challenging. Recently, segment-based tweet representation has demonstrated effectiveness in named entity recognition (NER) and event detection from tweet streams. To split tweets into meaningful phrases or segments, the previous work is purely based on external knowledge bases, which ignores the rich local context information embedded in the tweets. In this paper, we propose a novel framework for tweet segmentation in a batch mode, called HybridSeg. HybridSeg incorporates local context knowledge with global knowledge bases for better tweet segmentation. HybridSeg consists of two steps: learning from off-the-shelf weak NERs and learning from pseudo feedback. In the first step, the existing NER tools are applied to a batch of tweets. The named entities recognized by these NERs are then employed to guide the tweet segmentation process. In the second step, HybridSeg adjusts the tweet segmentation results iteratively by exploiting all segments in the batch of tweets in a collective manner. Experiments on two tweet datasets show that HybridSeg significantly improves tweet segmentation quality compared with the state-of-the-art algorithm. We also conduct a case study by using tweet segments for the task of named entity recognition from tweets. The experimental results demonstrate that HybridSeg significantly benefits the downstream applications.\"",
        "1 is \"Synthesizing high utility suggestions for rare web search queries\", 2 is \"Session-based item recommendation in e-commerce: on short-term intents, reminders, trends and discounts.\"",
        "Given above information, for an author who has written the paper with the title \"Context-aware Point-of-Interest Recommendation Using Tensor Factorization with Social Regularization\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004496": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'WSCC: A Weight-Similarity-Based Client Clustering Approach for Non-IID Federated Learning':",
        "Document: \"Optimizing the location deployment of dynamic mobile base stations. In this paper, we address the issue of optimizing the location deployment of dynamic mobile base stations. Particularly, we propose a low-cost, dynamic deployment protocol to locate mobile base stations, which is capable of meeting the Quality of Service (QoS) requirements necessary for applications. Unlike other deployment techniques, our proposed protocol can find an optimal schedule to relocate mobile base stations based on fluctuating population and traffic densities in an incident area. Through simulations, we demonstrate that our proposed technique can increase network throughput and reduce end-to-end delay, jitter, and packet loss rate.\"",
        "Document: \"On cascading failures and countermeasures based on energy storage in the smart grid. Recently, there have been growing concerns about electric power grid security and resilience. The performance of the power grid may suffer from component failures or targeted attacks. A sophisticated adversary may target critical components in the grid, leading to cascading failures and large blackouts. To this end, this paper begins with identifying the most critical components that lead to cascading failures in the grid and then presents a defensive mechanism using energy storage to defend against cascading failures. Based on the optimal power flow control on the standard IEEE power system test cases, we systematically assess component significance, simulate attacks against power grid components, and evaluate the consequences. We also conduct extensive simulations to investigate the effectiveness of deploying Energy Storage Systems (ESSs), in terms of storage capacity and deployment locations, to mitigate cascading failures. Through extensive simulations, our data shows that integrating energy storage systems into the smart grid can efficiently mitigate cascading failures.\"",
        "1 is \"Image fusion based on pyramidal multiband multiresolution markovian analysis\", 2 is \"Geo-indistinguishability: differential privacy for location-based systems\"",
        "Given above information, for an author who has written the paper with the title \"WSCC: A Weight-Similarity-Based Client Clustering Approach for Non-IID Federated Learning\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004539": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Multi-Sensor-Based Aperiodic Least-Squares Estimation for Networked Systems With Transmission Constraints':",
        "Document: \"Convergence and Mean Square Stability of Suboptimal Estimator for Systems With Measurement Packet Dropping. We consider remote state estimation over a packet-dropping network. A new suboptimal filter is derived by minimizing the mean squared estimation error. The estimator is designed by solving one deterministic Riccati equation. Convergence of the estimation error covariance and mean square stability of the estimator are proved under standard assumptions. It is shown that the new estimator has smaller error covariance and has wider applications when compared with the linear minimum mean squared error estimator. One of the key techniques adopted in this technical note is the introduction of the innovation sequence for the multiplicative noise systems.\"",
        "Document: \"Infinite-horizon sensor scheduling for estimation over lossy networks. In this paper, we consider the problem of infinite-horizon sensor scheduling for estimation of a discrete-time linear Gaussian system, where the sensor information may be dropped during communication over a memoryless erasure channel, modeled as an independent Bernoulli process. We assume that due to energy or bandwidth constraints, the sensor can only maintain an average communication rate of r. The optimal communication schedule is thus chosen by the sensor to minimize the average estimation error of the receiver. We prove necessary and sufficient conditions for the feasibility of the optimization problem for two classes of protocols, one where deterministic acknowledgments are transmitted from the central location back to the sensor (ACK) and the other where the acknowledgement structure is absent (No-ACK). We prove that for the No-ACK case, the optimal schedule can be approximated by a periodic schedule with arbitrarily close performance. For the ACK case, we provide the explicit optimal sensor schedule. A numerical example is provided to illustrate the effectiveness of the proposed strategy.\"",
        "1 is \"On active disturbance rejection control for nonlinear systems using time-varying gain\", 2 is \"Robust fault detection for networked systems with communication delay and data missing\"",
        "Given above information, for an author who has written the paper with the title \"Multi-Sensor-Based Aperiodic Least-Squares Estimation for Networked Systems With Transmission Constraints\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004592": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Stochastic Performance Analysis of a Wireless Finite-State Markov Channel':",
        "Document: \"Heterogeneous Vehicular Networking: A Survey on Architecture, Challenges and Solutions. With the rapid development of the Intelligent Transportation System (ITS), vehicular communication networks have been widely studied in recent years. Dedicated Short Range Communication (DSRC) can provide efficient real-time information exchange among vehicles without the need of pervasive roadside communication infrastructure. Although mobile cellular networks are capable of providing wide coverage for vehicular users, the requirements of services that require stringent realtime safety cannot always be guaranteed by cellular networks. Therefore, the Heterogeneous Vehicular NETwork (HetVNET), which integrates cellular networks with DSRC, is a potential solution for meeting the communication requirements of the ITS. Although there are a plethora of reported studies on either DSRC or cellular networks, joint research of these two areas is still at its infancy. This paper provides a comprehensive survey on recent wireless networks techniques applied to HetVNETs. Firstly, the requirements and use cases of safety and non-safety services are summarized and compared. Consequently, a Het- VNET framework that utilizes a variety of wireless networking techniques is presented, followed by the descriptions of various applications for some typical scenarios. Building such HetVNETs requires a deep understanding of heterogeneity and its associated challenges. Thus, major challenges and solutions that are related to both the Medium Access Control (MAC) and network layers in HetVNETs are studied and discussed in detail. Finally, we outline open issues that help to identify new research directions in HetVNETs\"",
        "Document: \"Proportional fair-based joint subcarrier and power allocation in relay-enhanced orthogonal frequency division multiplexing systems. There has emerged lots of interests in the resource allocation of the relay-enhanced orthogonal frequency division multiplexing (OFDM) system. Most of the existing research works are with the aim to maximise the transmit rate of the system. However, few of them have taken into account the fairness requirements. In order to achieve the trade-off between the system transmit rate and user fairness, this study originally studies the optimality of proportional fairness (PF) in a downlink relay-enhanced OFDM system, where one source communicates with multiple destinations by the aid of one or many relays. We investigate the joint subcarrier and power allocation problem with PF constraint. Firstly, the resource allocation problem is formulated. Then, the problem is studied in the framework of concave maximisation. By using mathematical decomposition techniques, the optimisation problem is decomposed into multiple-layer subproblems which can be resolved efficiently. With this method, a cross-layer algorithm is derived to achieve the near optimal solution. Finally, numerical results are illustrated. Compared to the existing schemes, our proposed algorithm can both maximise the sum of logarithmic per user average transmit rate and enhance the system transmit rate.\"",
        "1 is \"Delay-Constrained Optimal Link Scheduling in Wireless Sensor Networks\", 2 is \"VANET Modeling and Clustering Design Under Practical Traffic, Channel and Mobility Conditions\"",
        "Given above information, for an author who has written the paper with the title \"Stochastic Performance Analysis of a Wireless Finite-State Markov Channel\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004632": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'High-Accuracy Localization for Assisted Living: 5G systems will turn multipath channels from foe to friend.':",
        "Document: \"Radio Channel Properties for Vehicular Communication: Merging Lanes Versus Urban Intersections. Vehicle-to-vehicle (V2V) communication is a challenging but fast-growing technology. It has the potential to enhance road safety by helping the driver to avoid collisions during basic maneuvers such as crossing street intersections, changing lanes, merging on a highway, and driving safely in blind turns. The significance of V2V safety applications increases further where the visual line-of-sight (...\"",
        "Document: \"Delay and Doppler Spreads of Nonstationary Vehicular Channels for Safety-Relevant Scenarios. Vehicular communication channels are characterized by a nonstationary time-frequency-selective fading process due to rapid changes in the environment. The nonstationary fading process can be characterized by assuming local stationarity for a region with finite extent in time and frequency. For this finite region, the wide-sense stationarity and uncorrelated scattering assumption approximately holds, and we are able to calculate a time-frequency-dependent local scattering function (LSF). In this paper, we estimate the LSF from a large set of measurements collected in the DRIVEWAY'09 measurement campaign, which focuses on scenarios for intelligent transportation systems (ITSs). We then obtain the time-frequency-varying power delay profile (PDP) and the time-frequency-varying Doppler power spectral density (DSD) from the LSF. Based on the PDP and the DSD, we analyze the time-frequency-varying root-mean-square (RMS) delay spread and the RMS Doppler spread. We show that the distribution of these channel parameters follows a bimodal Gaussian mixture distribution. High RMS delay spread values are observed in situations with rich scattering, whereas high RMS Doppler spreads are obtained in drive-by scenarios.\"",
        "1 is \"Game theory and the flat-fading gaussian interference channel\", 2 is \"Geometry-based directional model for mobile radio channels - principles and implementation.\"",
        "Given above information, for an author who has written the paper with the title \"High-Accuracy Localization for Assisted Living: 5G systems will turn multipath channels from foe to friend.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004662": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Rate Allocation and Adaptation for Incremental Redundancy Truncated HARQ':",
        "Document: \"Outage Minimization via Power Adaptation and Allocation in Truncated Hybrid ARQ. In this work, we analyze hybrid ARQ (HARQ) transmission over the independent block fading channel. We consider two scenarios with respect to the message sent by the receiver via the feedback channel: i) \u201cconventional\u201d, one-bit feedback used to inform the transmitter about the decoding success/failure (ACK/NACK), and ii) the multi-bit feedback message, where on top of ACK/NACK, the transmitter is provided with additional information about the state of the receiver. To minimize the outage probability under long-term average and peak power constraints, we cast the problems into the dynamic programming (DP) framework and solve them for Nakagami-m fading channels. An approximate, closed-form solution for the high signal-to-noise ratio (SNR) regime is proposed using geometric programming (GP). The obtained results quantify the advantage of the multi-bit feedback over the conventional one-bit approach, and show that the power optimization can provide significant gains over conventional power-constant HARQ transmissions even in the presence of peak-power constraints.\"",
        "Document: \"Exact expression for the BER of rectangular QAM with arbitrary constellation mapping. The exact closed-form expression for the bit-error rate (BER) of rectangular quadrature amplitude modulation (QAM) is given. The presented formula is independent of the bit mapping and it is thus particularly useful in the design and analysis of modulation schemes employing non-Gray mapping. Compared with the so-called expurgated bound and the union bound, our expression is shown to accurately pre...\"",
        "1 is \"(Gray) Mappings for Bit-Interleaved Coded Modulation\", 2 is \"Preprocessed And Postprocessed Quantization Index Modulation Methods For Digital Watermarking\"",
        "Given above information, for an author who has written the paper with the title \"Rate Allocation and Adaptation for Incremental Redundancy Truncated HARQ\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004665": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Nonparametric spectral analysis with missing data via the EM algorithm':",
        "Document: \"Differential space-code modulation for interference suppression. Space-time coding has been receiving much attention due to its potentials offered by fully exploiting the spatial and temporal diversities of multiple transmit and receive antennas. A differential space-time modulation (DSTM) scheme was previously proposed for demodulation without channel state information, which is attractive in fast fading channels where accurate channel estimates are difficult to obtain. However, this technique is sensitive to interference and is likely to deteriorate or even break down in a wireless environment, where interference (including intentional and unintentional jamming) signals exist. We propose a new coding and modulation scheme, referred to as the differential space-code modulation (DSCM), which is interference resistant. Our focus is on single-user communications. We show that DSCM outperforms DSTM significantly when interference is present. This advantage is achieved at the cost of a lower data rate or a wider bandwidth or a combination of both. To alleviate this problem, a high-rate DSCM (HR-DSCM) scheme is also presented, which increases the data rate considerably at the cost of a slightly higher bit-error rate (BER), while still maintaining the interference suppression capability\"",
        "Document: \"High resolution imaging for MIMO forward looking ground penetrating radar. Forward-Looking Ground Penetrating Radar (FLGPR) can be used for detecting landmines. The detection process involves generating synthetic aperture radar (SAR) images using the standard backprojection (BP). The BP approach suffers from poor resolution and high sidelobe problems. This paper focuses on enhancing imaging resolution and reducing sidelobes using the Sparse Iterative Covariance-based Estimation (SPICE). A MIMO FLGPR developed by the Army Research Laboratory (ARL) is used for analysis.\"",
        "1 is \"Capacity performance of multicell large-scale antenna systems.\", 2 is \"Minimum probability of error for asynchronous Gaussian multiple-access channels\"",
        "Given above information, for an author who has written the paper with the title \"Nonparametric spectral analysis with missing data via the EM algorithm\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004700": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'On the impact of UML analysis models on source-code comprehensibility and modifiability':",
        "Document: \"Defining Complexity Metrics for Object-Relational Databases. New Object-Relational Databases Management Systems (ORDBMSs) are replacing existing relational ones. In spite of the high expressiveness, application systems built upon ORDBMS are more complex and difficult to maintain due to the mixing of both paradigms, the relational and the object-oriented. All these reasons made necessary to dispose on metrics for measuring the complexity of this kind of databases and controlling its correct design. However, not always it is easy to propose correct metrics. This paper describes the method used by the authors for proposing metrics and the process of feedback needed for obtaining them in a correct way.\"",
        "Document: \"Evaluating the effect of composite states on the understandability of UML statechart diagrams. UML statechart diagrams have become an important technique for describing the dynamic behavior of a software system. They are also a significant element of OO design, especially in code generation frameworks such as Model Driven Architecture (MDA). In previous works we have defined a set of metrics for evaluating structural properties of UML statechart diagrams and have validated them as early understandability indicators, through a family of controlled experiments. Those experiments have also revealed that the number of composite states had, apparently, no influence on the understandability of the diagrams. This fact seemed a bit suspicious to us and we decided to go a step further. So in this work we present a controlled experiment and a replication, focusing on the effect of composite states on the understandability of UML statechart diagrams. The results of the experiment confirm, to some extent, our intuition that the use of composite states improves the understandability of the diagrams, so long as the subjects of the experiment have had some previous experience in using them. There are educational implications here, as our results justify giving extra emphasis to the use of composite states in UML statechart diagrams in Software Engineering courses.\"",
        "1 is \"Validating a size measure for effort estimation in model-driven Web development\", 2 is \"How do practitioners use conceptual modeling in practice?\"",
        "Given above information, for an author who has written the paper with the title \"On the impact of UML analysis models on source-code comprehensibility and modifiability\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004701": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'On Optimal Training And Beamforming In Uncorrelated Mimo Systems With Feedback':",
        "Document: \"Forward-link resource allocation for a two-cell voice network with multiple service classes. Resource allocation is studied for a forward-link two-cell code division multiple access (CDMA) voice network with multiple service classes. System resources are transmitted power and codes. The service classes are specified by different user utility functions that relate utility to received signal-to-interference-plus-noise-ratio (SINR). The objective of the resource allocation is to maximize total utility over the two cells. The optimal power allocation is characterized by a set of distances, or radii, from the desired base station. Each radius corresponds to the set of active users in a particular service class, and can be enforced through a pricing scheme. We also consider setting prices to maximize revenue. In general, the prizes and power allocation that maximize revenue differ from those that maximize utility.\"",
        "Document: \"The Cost of Free Spectrum. AbstractThere has been growing interest in increasing the amount of radio spectrum available for unlicensed broadband wireless access. That includes \u201cprime\u201d spectrum at lower frequencies, which is also suitable for wide area coverage by licensed cellular providers. While additional unlicensed spectrum would allow for market expansion, it could influence competition among providers and increase congestion (interference) among consumers of wireless services. We study the value (social welfare and consumer surplus) obtained by adding unlicensed spectrum to an existing allocation of licensed spectrum among incumbent service providers. We assume a population of customers who choose a provider based on the minimum delivered price, given by the weighted sum of the price of the service and a congestion cost, which depends on the number of subscribers in a band. We consider models in which this weighting is uniform across the customer population and where the weighting is high or low, reflecting different sensitivities to latency. For the models considered, we find that the social welfare depends on the amount of additional unlicensed spectrum, and can actually decrease over a significant range of unlicensed bandwidths. Furthermore, with nonuniform weighting, introducing unlicensed spectrum can also reduce consumer welfare.\"",
        "1 is \"Capacity bounds for Cooperative diversity\", 2 is \"Transmitter precoding in synchronous multiuser communications\"",
        "Given above information, for an author who has written the paper with the title \"On Optimal Training And Beamforming In Uncorrelated Mimo Systems With Feedback\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004760": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'The role of the environment in agreement technologies':",
        "Document: \"Integrating a Wiki in an Ontology Driven Web Site: Approach, Architecture and Application in the Archaeological Domain. This paper describes an approach to the design and implementation of ontology driven dynamic web sites combining ontologies and wiki technologies. The core of the architectural solution proposed is completely based on ontologies rather than on more traditional forms of persistent data storage facilities, such as relational databases. This approach provides a flexible support to the design and implementation of web portals in which navigation schemes are not entirely pre- determined but are instead influenced by actual relationships among the contents of the ontology, that are used to generate web pages as well as hyperlinks. A wiki technology is integrated with this approach in order to create pages not directly derived by elements of the ontology, but also to enrich the textual contents with suitable formatting, images and hyperlinks. The application of this approach to the realization of a web portal is also described; the portal is devoted to enable a number of scientific communities to share archaeological knowledge about the Silk Road domain.\"",
        "Document: \"Modeling heterogeneous speed profiles in discrete models for pedestrian simulation. We present a discrete model extending the floor field approach allowing heterogeneity in the walking speed of the simulated population of pedestrians. Whereas some discrete models allow pedestrians to move more than a single cell per time step, in the present work we maintain a maximum speed of one cell per step but we model lower speeds by having pedestrians yielding their movement in some turns. Different classes of pedestrians are associated to different desired walking speeds and we define a stochastic mechanism ensuring that they maintain an average speed close to this threshold.\"",
        "1 is \"Timewarp rigid body simulation\", 2 is \"A normative organisation programming language for organisation management infrastructures\"",
        "Given above information, for an author who has written the paper with the title \"The role of the environment in agreement technologies\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004805": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Addressing failures in exascale computing':",
        "Document: \"Improving the Accuracy vs. Speed Tradeoff for Simulating Shared-Memory Multiprocessors with ILP Processors. Previous simulators for shared-memory architectures have imposed a large tradeoff between simulation accuracy and speed. Most such simulators model simple processors that do not exploit common instruction-level parallelism (ILP) features, consequently exhibiting large errors when used to model current systems. A few newer simulators model current ILP processors in detail, but we find them to be about ten times slower. We propose a new simulation technique, based on a novel adaptation of direct execution, that alleviates this accuracy vs. speed tradeoff.We compare the speed and accuracy of our new simulator, DirectRSIM, with three other simulators -- RSIM (a detailed simulator for multiprocessors with ILP processors) and two representative simple-processor based simulators. Compared to RSIM, on average, DirectRSIM is 3.6 times faster and exhibits a relative error of only 1.3% in total execution time. Compared to the simple-processor based simulators, DirectRSIM is far superior in accuracy, and yet is only 2.7 times slower.\"",
        "Document: \"CrashTest'ing SWAT: accurate, gate-level evaluation of symptom-based resiliency solutions. Current technology scaling is leading to increasingly fragile components, making hardware reliability a primary design consideration. Recently researchers have proposed low-cost reliability solutions that detect hardware faults through software-level symptom monitoring. SWAT (SoftWare Anomaly Treatment), one such solution, demonstrated with microarchitecture-level simulations that symptom-based solutions can provide high fault coverage and a low Silent Data Corruption (SDC) rate. However, more accurate evaluations are needed to validate such solutions for hardware faults in real-world processor designs. In this paper, we evaluate SWAT's symptom-based detectors on gate-level faults using an FPGA-based, full-system prototype. With this platform, we performed a gate-level accurate fault injection campaign of 51,630 fault injections in the OpenSPARC T1 core logic across five SPECInt 2000 benchmarks. With an overall SDC rate of 0.79%, our results are comparable to previous microarchitecture-level evaluations of SWAT, demonstrating the effectiveness of symptom-based software detectors for permanent faults in real-world designs.\"",
        "1 is \"Adaptive strategy for one-sided communication in MPICH2\", 2 is \"Heterogeneous Habanero-C (H2c): A Portable Programming Model For Heterogeneous Processors\"",
        "Given above information, for an author who has written the paper with the title \"Addressing failures in exascale computing\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004863": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Unsupervised Semantic-based Aggregation of Deep Convolutional Features.':",
        "Document: \"Action Recognition Using Context-Constrained Linear Coding. Although traditional bag-of-words model has shown promising results for action recognition, it takes no consideration of the relationship among spatio-temporal points; furthermore, it also suffers serious quantization error. In this letter, we propose a novel coding strategy called context-constrained linear coding (CLC) to overcome these limitations. We first calculate the contextual distance bet...\"",
        "Document: \"Adaptive Graph Cut Based Binarization of Video Text Images. Interactive image segmentation which needs the user to give certain hard constraints has shown promising performance for object segmentation. In this paper, we consider characters in text image as a special kind of object, and propose an adaptive graph cut based text binarization method to segment text from background. The main contributions of the paper lie in: 1) in order to make the binarization local adaptive with uneven background, the text region image is firstly roughly split into several sub-images on which graph cut is applied, and 2) considering the unique characteristics of the text, we propose to automatically classify some pixels as text or background with high confidence, severed as hard constraints seeds for graph cut to extract text from background by spreading the seeds into the whole sub-image. The experimental results show that our approach could get better performance in both character extraction accuracy and recognition accuracy.\"",
        "1 is \"Document image binarization using background estimation and stroke edges\", 2 is \"WinCuts: manipulating arbitrary window regions for more effective use of screen space\"",
        "Given above information, for an author who has written the paper with the title \"Unsupervised Semantic-based Aggregation of Deep Convolutional Features.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004864": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Modular Hierarchical Feature Learning With Deep Neural Networks For Face Verification':",
        "Document: \"Handwritten Chinese Address Recognition. A handwritten Chinese address recognition (HCAR) system is proposed in this paper. Handwritten Chinese address recognition is a difficult problem. Handwritten Chinese characters are characterized by large vocabulary, complicate structure, irregular distortion and touching characters etc. Proposed approach takes good advantage of Chinese address knowledge, and applies key character extraction and holistic word matching to solving the problem. Different from conventional approach, proposed approach can avoid the character segmentation error successfully. Experimental results show the proposed approach is very effective.\"",
        "Document: \"Deep nonlinear metric learning with independent subspace analysis for face verification. Face verification is the task of determining by analyzing face images, whether a person is who he/she claims to be. It is a very challenge problem, due to large variations in lighting, background, expression, hairstyle and occlusion. The crucial problem is to compute the similarity of two face vectors. Metric learning has provides a viable solution to this problem. Until now, many metric learning algorithms have been proposed, but they are usually limited to learning a linear transformation (i.e. finding a global Mahalanobis metric). In this brief, we propose a nonlinear metric learning method, which learns an explicit mapping from the original space to an optimal subspace, using deep Independent Subspace Analysis network. Compared to kernel methods, which can also learn nonlinear transformations, our method is a deep and local learning architecture, and therefore exhibits more powerful ability to learn the nature of highly variable dataset. We evaluate our method on the LFW benchmark, and results show very comparable performance to the state-of-art methods (achieving 92.28% accuracy), while maintaining simplicity and good generalization ability.\"",
        "1 is \"Chaotic invariants of Lagrangian particle trajectories for anomaly detection in crowded scenes\", 2 is \"Learning to Localize Objects with Structured Output Regression\"",
        "Given above information, for an author who has written the paper with the title \"Modular Hierarchical Feature Learning With Deep Neural Networks For Face Verification\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004927": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Towards Licenses Compatibility and Composition in the Web of Data.':",
        "Document: \"Discovery hub: on-the-fly linked data exploratory search. Exploratory search systems help users learn or investigate a topic. The richness of the linked open data can be used to assist this task. We present a method that selects and ranks linked data resources that are semantically related to the user's interest. The objective is to focus the user's attention on a meaningful subset of highly informative resources. We extended spreading activation to typed graphs and coupled it with a graph sampling technique. The results selection and ranking is performed on--the-fly and doesn't require pre-processing. This allows addressing remote SPARQL endpoints. We describe first implementation on top of DBpedia. It is used by the Discovery Hub exploratory search system to select interesting resources, to support faceted browsing of the results, to provide explanations and to offer redirections to third-party services. Results of a user evaluation conclude the article.\"",
        "Document: \"A Generic RDF Transformation Software and its Application to an Online Translation Service for Common Languages of Linked Data. In this article we present a generic template and software solution for developers to support the many cases where we need to transform RDF. It relies on the SPARQL Template Transformation Language STTL which enables Semantic Web developers to write specific yet compact RDF transformers toward other languages and formats.We first briefly recall the STTL principles and software features. We then demonstrate the support it provides to programmers by presenting a selection of STTL-based RDF transformers for common languages. The software is available online as a Web service and all the RDF transformers presented in this paper can be tested online.\"",
        "1 is \"Learning from syntax generalizations for automatic semantic annotation\", 2 is \"Community detection in graphs\"",
        "Given above information, for an author who has written the paper with the title \"Towards Licenses Compatibility and Composition in the Web of Data.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004979": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Robust control design for nonlinear stochastic partial differential systems with Poisson noise: Fuzzy implementation.':",
        "Document: \"Robust control design for nonlinear stochastic partial differential systems with Poisson noise: Fuzzy implementation. This study addresses the robust H\u221e control design problem for nonlinear stochastic partial differential systems (NSPDSs) with Poisson noise under the environment of random external disturbance in the spatio-temporal domain. For NSPDSs with Poisson noise, the robust H\u221e control design needs to solve a complex Hamilton\u2013Jacobi integral inequality (HJII) for robust control despite random external disturbance. In general, it is very difficult to solve the nonlinear partial differential HJII. In order to simplify the design procedure, a fuzzy stochastic partial differential system is proposed to approximate the NSPDS based on fuzzy interpolation approach. Then a fuzzy stochastic spatial state space model is developed to represent the fuzzy stochastic partial differential system via the semi-discretization finite difference scheme and the Kronecker product. Based on this model the robust H\u221e control design is proposed to achieve the robust control of NSPDSs via solving linear matrix inequalities (LMIs) instead of an HJII. The proposed robust fuzzy H\u221e controller has an efficient ability to attenuate the effect of spatio-temporal external disturbance on the controlled output of NSPDSs from the area energy point of view. Finally, a robust H\u221e control of the nervous system is given to confirm the control performance of the proposed robust control design method for NSPDSs with Poisson noise.\"",
        "Document: \"Study on Indefinite Stochastic Linear Quadratic Optimal Control with Inequality Constraint. This paper studies the indefinite stochastic linear quadratic (LQ) optimal control problem with an inequality constraint for the terminal state. Firstly, we prove a generalized Karush-Kuhn-Tucker (KKT) theorem under hybrid constraints. Secondly, a new type of generalized Riccati equations is obtained, based on which a necessary condition (it is also a sufficient condition under stronger assumptions) for the existence of an optimal linear state feedback control is given by means of KKT theorem. Finally, we design a dynamic programming algorithm to solve the constrained indefinite stochastic LQ issue.\"",
        "1 is \"Optimum MIMO-OFDM Detection With Pilot-Aided Channel State Information\", 2 is \"Reliable dissipative control for stochastic impulsive systems\"",
        "Given above information, for an author who has written the paper with the title \"Robust control design for nonlinear stochastic partial differential systems with Poisson noise: Fuzzy implementation.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005039": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Offset modulated single-carrier FDMA with flexible user bandwidth':",
        "Document: \"Energy-efficient resource allocation for OFDMA relay systems with imperfect CSIT. In this paper, we propose an energy-efficient resource allocation scheme for the downlink of decode-and-forward relay-assisted orthogonal frequency division multiple access systems. The resource allocation is designed based on imperfect channel state information at the transmitter. We jointly optimize the power allocation, data rate allocation, and subcarrier allocation to maximize the system energy efficiency (EE). We formulate the EE problem as a probabilistic mixed non-convex optimization problem, in which the individual power constraints and the outage probability requirements are considered. An iterative algorithm based on bisection method is given to transform the optimization problem into a standard concave form. Also, we consider a proportional fairness design for the EE maximization problem. Simulation results illustrate the effectiveness of the proposed algorithm.\"",
        "Document: \"Minimum neighbour and extended kalman filter estimator: a practical distributed channel assignment scheme for dense wireless local area networks. Dense deployments of wireless local area networks (WLANs) are becoming a norm in many cities around the world. However, increased interference and traffic demands can severely limit the aggregate throughput achievable unless an effective channel assignment scheme is used. In this work, a simple and effective distributed channel assignment (DCA) scheme is proposed. It is shown that in order to maximise throughput, each access point (AP) simply chooses the channel with the minimum number of active neighbour nodes (i.e. nodes associated with neighbouring APs that have packets to send). However, application of such a scheme to practice depends critically on its ability to estimate the number of neighbour nodes in each channel, for which no practical estimator has been proposed before. In view of this, an extended Kalman filter (EKF) estimator and an estimate of the number of nodes by AP are proposed. These not only provide fast and accurate estimates but can also exploit channel switching information of neighbouring APs. Extensive packet level simulation results show that the proposed minimum neighbour and EKF estimator (MINEK) scheme is highly scalable and can provide significant throughput improvement over other channel assignment schemes.\"",
        "1 is \"MIMO Multichannel Beamforming in Rayleigh-Product Channels with Arbitrary-Power Co-Channel Interference and Noise.\", 2 is \"Fast Global Optimal Power Allocation in Wireless Networks by Local D.C. Programming.\"",
        "Given above information, for an author who has written the paper with the title \"Offset modulated single-carrier FDMA with flexible user bandwidth\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005110": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Visual learning and recognition of 3D objects using two-dimensional principal component analysis: A robust and an efficient approach':",
        "Document: \"Mathematical Morphology Based Corner Detection Scheme: A Non-Parametric Approach. A simple non-parametric approach for detecting corner points using mathematical morphology is proposed in this paper. The proposed method has two stages, the first stage deals with the extraction of significant corner clusters from an image, while the second stage concentrates on detection and localization of true corner points from the extracted corner clusters. The proposed method unlike other morphology based methods detects all true corner points present in the extracted corner clusters. The experimental results reveal that the proposed method outperforms existing morphology based corner\"",
        "Document: \"Appearance Based Models in Document Script Identification. In this paper we employ appearance based models for document script identification. They are employed to identify scripts at both paragraph and word level. Elaborate experimentation has been conducted which has revealed that they are robust enough to handle highly confusing scripts and their performance does not degrade drastically even in the presence of noise. A generic script identification has been attempted, to identify both Asian and European scripts by considering a dataset of twenty different languages.\"",
        "1 is \"A new method for representing and matching shapes of natural objects\", 2 is \"Alphabet Independent and Dictionary Scaled Matching\"",
        "Given above information, for an author who has written the paper with the title \"Visual learning and recognition of 3D objects using two-dimensional principal component analysis: A robust and an efficient approach\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005205": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'CLEF 2008: Ad Hoc Track Overview.':",
        "Document: \"Direct: applying the DIKW hierarchy to large-scale evaluation campaigns. We describe the effort of designing and developing a digital library system able to manage the different types of information resources produced during a large-scale evaluation campaign and to support the different stages of it. In this context, we present DIRECT, the system which has been adopted to manage the CLEF evaluation campaigns since 2005.\"",
        "Document: \"AWARE: Exploiting Evaluation Measures to Combine Multiple Assessors. We propose the Assessor-driven Weighted Averages for Retrieval Evaluation (AWARE) probabilistic framework, a novel methodology for dealing with multiple crowd assessors that may be contradictory and/or noisy. By modeling relevance judgements and crowd assessors as sources of uncertainty, AWARE takes the expectation of a generic performance measure, like Average Precision, composed with these random variables. In this way, it approaches the problem of aggregating different crowd assessors from a new perspective, that is, directly combining the performance measures computed on the ground truth generated by the crowd assessors instead of adopting some classification technique to merge the labels produced by them. We propose several unsupervised estimators that instantiate the AWARE framework and we compare them with state-of-the-art approaches, that is,Majoriity Vote and Expectation Maximization, on TREC collections. We found that AWARE approaches improve in terms of their capability of correctly ranking systems and predicting their actual performance scores.\n\n\"",
        "1 is \"Fusion of Retrieval Models at CLEF 2008 Ad-Hoc Persian Track.\", 2 is \"Word sense disambiguation in queries\"",
        "Given above information, for an author who has written the paper with the title \"CLEF 2008: Ad Hoc Track Overview.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005220": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Fast density clustering strategies based on the k-means algorithm.':",
        "Document: \"An accelerator for attribute reduction based on perspective of objects and attributes. Feature selection is an active area of research in pattern recognition, machine learning and artificial intelligence, which greatly improves the performance of forecasting or classification. In rough set theory, attribute reduction, as a special form of feature selection, aims to retain the discernability of the original attribute set. To solve this problem, many heuristic attribute reduction algorithms have been proposed in the literature. However, these methods are computationally time-consuming for large scale datasets. Recently, an accelerator was introduced by computing reducts on gradually reducing the size of the universe. Although the accelerator can considerably shorten the computational time, it remains a challenging issue. To further enhance the efficiency of these algorithms, we develop a new accelerator for attribute reduction, which simultaneously reduces the size of the universe and the number of attributes at each iteration of the process of reduction. Based on the new accelerator, several representative heuristic attribute reduction algorithms are accelerated. Experiments show that these accelerated algorithms can significantly reduce computational time while maintaining their results the same as before.\"",
        "Document: \"k-mw-modes: An algorithm for clustering categorical matrix-object data. Abstract   In data mining, the input of most algorithms is a set of  n  objects and each object is described by a feature vector. However, in many real database applications, an object is described by more than one feature vector. In this paper, we call an object described by more than one feature vector as a matrix-object and a data set consisting of matrix-objects as a matrix-object data set. We propose a  k -multi-weighted-modes (abbr.  k -mw-modes) algorithm for clustering categorical matrix-object data. In this algorithm, we define the distance between two categorical matrix-objects and a multi-weighted-modes representation of cluster prototypes is proposed. We give a heuristic method to choose the locally optimal multi-weighted-modes in the iteration of the  k -mw-modes algorithm. We validated the effectiveness and benefits of the  k -mw-modes algorithm on the five real data sets from different applications.\"",
        "1 is \"Optimal scale selection in dynamic multi-scale decision tables based on sequential three-way decisions.\", 2 is \"Computing Connected Dominated Sets with Multipoint Relays.\"",
        "Given above information, for an author who has written the paper with the title \"Fast density clustering strategies based on the k-means algorithm.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005235": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Research on secure data collection in wireless multimedia sensor networks':",
        "Document: \"Performance of virtual full-duplex relaying on cooperative multi-path relay channels. We consider a cooperative multi-path relay channel (MPRC) where multiple half-duplex relays assist in the packet transmissions from a source to its destination. A virtual full-duplex (FD) relaying scheme is proposed that allows the source to transmit a new packet simultaneously with the selected best relay, with the rest of the relays attempting to decode this new packet. Thus, a new source packet...\"",
        "Document: \"5G multimedia massive MIMO communications systems. In the fifth generation 5G wireless communication systems, a majority of the traffic demands are contributed by various multimedia applications. To support the future 5G multimedia communication systems, the massive multiple-input multiple-output MIMO technique is recognized as a key enabler because of its high spectral efficiency. The massive antennas and radio frequency chains not only improve the implementation cost of 5G wireless communication systems but also result in an intense mutual coupling effect among antennas because of the limited space for deploying antennas. To reduce the cost, an optimal equivalent precoding matrix with the minimum number of radio frequency chains is proposed for 5G multimedia massive MIMO communication systems considering the mutual coupling effect. Moreover, an upper bound of the effective capacity is derived for 5G multimedia massive MIMO communication systems. Two antennas that receive diversity gain models are built and analyzed. The impacts of the antenna spacing, the number of antennas, the quality-of-service QoS statistical exponent, and the number of independent incident directions on the effective capacity of 5G multimedia massive MIMO communication systems are analyzed. Comparing with the conventional zero-forcing precoding matrix, simulation results demonstrate that the proposed optimal equivalent precoding matrix can achieve a higher achievable rate for 5G multimedia massive MIMO communication systems. Copyright \u00a9 2016 John Wiley & Sons, Ltd.\"",
        "1 is \"Diversity and multiplexing: a fundamental tradeoff in multiple-antenna channels\", 2 is \"On the computation offloading at ad hoc cloudlet: architecture and service modes\"",
        "Given above information, for an author who has written the paper with the title \"Research on secure data collection in wireless multimedia sensor networks\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005272": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Mining Knowledge from Repeated Co-Occurrences: DIOGENE at TREC 2002':",
        "Document: \"ITC-irst at TREC 2003: the DIOGENE QA System. This paper describes a new version of the DIOGENE Question Answering (QA) system developed at ITC-Irst. The recent updates here presented are targeted to the participation to TREC-2003 and meet the specific requirements of this year's QA main task. In particular, extending the backbone already developed for our participation to the last two editions of the QA track, special attention was paid to deal with the principal novelty factors of the new challenge, namely the introduction of the so-called definition and list questions. Moreover, we experimented with a first attempt to integrate parsing as a deeper linguistic analysis technique to find similarities between the syntactic structure of the input questions and the retrieved text passages. The outcome of such experiments, as well as the variations of the system's architecture and the results achieved at TREC-2003 will be presented in the following sections.\"",
        "Document: \"Automatic Quality Estimation for ASR System Combination. \n\u2022Review on automatic speech recognition quality estimation (ASR QE).\u2022The application of ASR QE in ASR system combination for both single-microphone multiple-ASR system task and on multiple-microphone multiple-ASR system task.\u2022Ranking the system combination inputs based on predicted quality.\u2022Management of tied ranks.\u2022Automatically finding the optimum level of combination for each segment.\n\"",
        "1 is \"The Tree-to-Tree Correction Problem\", 2 is \"Making themost from multiple microphones in meeting recognition\"",
        "Given above information, for an author who has written the paper with the title \"Mining Knowledge from Repeated Co-Occurrences: DIOGENE at TREC 2002\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005413": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A New Approach for Diagnosability Analysis of Petri Nets Using Verifier Nets.':",
        "Document: \"Supervisory control and reactive synthesis: a comparative introduction. This paper presents an introduction to and a formal connection between synthesis problems for discrete event systems that have been considered, largely separately, in the two research communities of supervisory control in control engineering and reactive synthesis in computer science. By making this connection mathematically precise in a paper that attempts to be as self-contained as possible, we wish to introduce these two research areas to non-expert readers and at the same time to highlight how they can be bridged in the context of classical synthesis problems. After presenting general introductions to supervisory control theory and reactive synthesis, we provide a novel reduction of the basic supervisory control problem, non-blocking case, to a problem of reactive synthesis with plants and with a maximal permissiveness requirement. The reduction is for fully-observed systems that are controlled by a single supervisor/controller. It complements prior work that has explored problems at the interface of supervisory control and reactive synthesis. The formal bridge constructed in this paper should be a source of inspiration for new lines of investigation that will leverage the power of the synthesis techniques that have been developed in these two areas.\"",
        "Document: \"Concurrency bugs in multithreaded software: modeling and analysis using Petri nets. In this paper, we apply discrete-event system techniques to model and analyze the execution of concurrent software. The problem of interest is deadlock avoidance in shared-memory multithreaded programs. We employ Petri nets to systematically model multithreaded programs with lock acquisition and release operations. We define a new class of Petri nets, called Gadara nets, that arises from this modeling process. We investigate a set of important properties of Gadara nets, such as liveness, reversibility, and linear separability. We propose efficient algorithms for the verification of liveness of Gadara nets, and report experimental results on their performance. We also present modeling examples of real-world programs. The results in this paper lay the foundations for the development of effective control synthesis algorithms for Gadara nets.\"",
        "1 is \"Verification of Opacity and Diagnosability for Pushdown Systems.\", 2 is \"Berth planning and resources optimisation at a container terminal via discrete event simulation\"",
        "Given above information, for an author who has written the paper with the title \"A New Approach for Diagnosability Analysis of Petri Nets Using Verifier Nets.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005470": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'The Visual Object Tracking VOT2016 Challenge Results':",
        "Document: \"Application Of Papoulis\u2013Gerchberg Method In Image Super-Resolution and Inpainting. In this paper, we study the Papoulis\u2013Gerchberg (PG) method and its applications to domains of image restoration such as super-resolution (SR) and inpainting. We show that the method performs well under certain conditions. We then suggest improvements to the method to achieve better SR and inpainting results. The modification applied to the SR process also allows us to apply the method to a larger class of images by doing away with some of the restrictions inherent in the classical PG method. We also present results to demonstrate the performance of the proposed techniques.\"",
        "Document: \"Dynamic Data Driven Applications System Concept For Information Fusion. We present a framework to Information Fusion (IF) using the Dynamic Data Driven Applications Systems (DDDAS) concept. Existing literature at the intersection of these two topics supports environmental modeling (e.g. terrain understanding) for context enhanced applications. Taking advantage of sensor models, statistical methods, and situation-specific spatio-temporal fusion products derived from wide area sensor networks, DDDAS demonstrates robust multi-scale and multi-resolution geographical terrain computations. We highlight the complementary nature of these seemingly parallel approaches and propose a more integrated analytical framework in the context of a cooperative multimodal sensing application. In particular, we use a Wide-Area Motion Imagery (WAMI) application to draw parallels and contrasts between IF and DDDAS systems that warrants an integrated perspective. This elementary work is aimed at triggering a sequence of deeper insightful research towards exploiting sparsely sampled piecewise dense WAMI measurements - an application where the challenges of big-data with regards to mathematical fusion relationships and high-performance computation remain significant and will persist. Dynamic data-driven adaptive computations are required to effectively handle the challenges with exponentially increasing data volume for advanced information fusion systems solutions such as simultaneous target tracking and identification. (C) 2013 The Authors. Published by Elsevier B.V. Selection and peer review under responsibility of the organizers of the 2013 International Conference on Computational Science\"",
        "1 is \"A Riemannian Optimization Approach for Computing Low-Rank Solutions of Lyapunov Equations\", 2 is \"Multi-Target Tracking by Online Learning a CRF Model of Appearance and Motion Patterns\"",
        "Given above information, for an author who has written the paper with the title \"The Visual Object Tracking VOT2016 Challenge Results\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005520": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Increasing effective IPC by exploiting distant parallelism':",
        "Document: \"Software and hardware techniques to optimize register file utilization in VLIW architectures. High-performance microprocessors are currently designed with the purpose of exploiting instruction level parallelism (ILP). The techniques used in their design and the aggressive scheduling techniques used to exploit this ILP tend to increase the register requirements of the loops. This paper reviews hardware and software techniques that alleviate the high register demands of aggressive scheduling heuristics on VLIW cores. From the software point of view, instruction scheduling can stretch lifetimes and reduce the register pressure. If more registers than those available in the architecture are required, some actions (such as the injection of spill code) have to be applied to reduce this pressure, at the expense of some performance degradation. From the hardware point of view, this degradation could be reduced if a high-capacity register file were included without causing a negative impact on the design of the processor (cycle time, area and power dissipation). Novel organizations for the register file based on clustering and hierarchical organization are necessary to meet the technology constraints. This paper proposes the used of a clustered organization and proposes an aggressive instruction scheduling technique that minimizes the negative effect of the limitations imposed by the register file organization.\"",
        "Document: \"Runtime-Guided Mitigation of Manufacturing Variability in Power-Constrained Multi-Socket NUMA Nodes. Current large scale systems show increasing power demands, to the point that it has become a huge strain on facilities and budgets. Researchers in academia, labs and industry are focusing on dealing with this \"power wall\", striving to find a balance between performance and power consumption. Some commodity processors enable power capping, which opens up new opportunities for applications to directly manage their power behavior at user level. However, while power capping ensures a system will never exceed a given power limit, it also leads to a new form of heterogeneity: natural manufacturing variability, which was previously hidden by varying power to achieve homogeneous performance, now results in heterogeneous performance caused by different CPU frequencies, potentially for each core, to enforce the power limit.\n\nIn this work we show how a parallel runtime system can be used to effectively deal with this new kind of performance heterogeneity by compensating the uneven effects of power capping. In the context of a NUMA node composed of several multi-core sockets, our system is able to optimize the energy and concurrency levels assigned to each socket to maximize performance. Applied transparently within the parallel runtime system, it does not require any programmer interaction like changing the application source code or manually reconfiguring the parallel system. We compare our novel runtime analysis with an offline approach and demonstrate that it can achieve equal performance at a fraction of the cost.\n\n\"",
        "1 is \"P2P spatial query processing by delaunay triangulation\", 2 is \"ImageNet Large Scale Visual Recognition Challenge.\"",
        "Given above information, for an author who has written the paper with the title \"Increasing effective IPC by exploiting distant parallelism\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005616": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'SSDUP: a traffic-aware ssd burst buffer for HPC systems.':",
        "Document: \"Middleware-based distributed systems software process. Middleware facilitates the development of distributed systems by accommodating heterogeneity, hiding distribution details and providing a set of common and domain specific services. It plays a central and essential role for developing distributed systems. However, middleware is considered a mean rather than core elements of development process in the existing distributed systems software process. This paper explains the concept of middleware by categorizes middleware and analysis the problems of current middleware architectures. It also extracts three essential non-functional requirements of middleware and proposes a middleware-based distributed systems software process. The proposed software process consists in five phases: requirements analysis, design, validation, development and testing. The characteristics of middleware are considered in the entire software process.\"",
        "Document: \"PIE: A prior knowledge guided integrated likelihood estimation method for bias reduction in association studies using electronic health records data. This study proposes a novel Prior knowledge guided Integrated likelihood Estimation (PIE) method to correct bias in estimations of associations due to misclassification of electronic health record (EHR)-derived binary phenotypes, and evaluates the performance of the proposed method by comparing it to 2 methods in common practice. We conducted simulation studies and data analysis of real EHR-derived data on diabetes from Kaiser Permanente Washington to compare the estimation bias of associations using the proposed method, the method ignoring phenotyping errors, the maximum likelihood method with misspecified sensitivity and specificity, and the maximum likelihood method with correctly specified sensitivity and specificity (gold standard). The proposed method effectively leverages available information on phenotyping accuracy to construct a prior distribution for sensitivity and specificity, and incorporates this prior information through the integrated likelihood for bias reduction. Our simulation studies and real data application demonstrated that the proposed method effectively reduces the estimation bias compared to the 2 current methods. It performed almost as well as the gold standard method when the prior had highest density around true sensitivity and specificity. The analysis of EHR data from Kaiser Permanente Washington showed that the estimated associations from PIE were very close to the estimates from the gold standard method and reduced bias by 60%-100% compared to the 2 commonly used methods in current practice for EHR data. This study demonstrates that the proposed method can effectively reduce estimation bias caused by imperfect phenotyping in EHR-derived data by incorporating prior information through integrated likelihood.\"",
        "1 is \"Animating rotation with quaternion curves\", 2 is \"An algorithm for selecting a good value for the parameter  in radial basis function interpolation\"",
        "Given above information, for an author who has written the paper with the title \"SSDUP: a traffic-aware ssd burst buffer for HPC systems.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005628": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'On topology reconfiguration for defect-tolerant NoC-based homogeneous manycore systems':",
        "Document: \"Accelerating Lightpath setup via broadcasting in binary-tree waveguide in Optical NoCs. In this paper, we propose a binary-tree waveguide connected Optical-Network-on-Chip (ONoC) to accelerate the establishment of the lightpath. By broadcasting the control data in the proposed power-efficient binary-tree waveguide, the maximal hops for establishing lightpath is reduced to two. With extensive simulations and analysis, we demonstrate that the proposed ONoC significantly reduces the setup time, and then the packet latency.\"",
        "Document: \"Embedded test decompressor to reduce the required channels and vector memory of tester for complex processor circuit. An embedded test stimulus decompressor is presented for the test patterns decompression, which can reduce the required channels and vector memory of automatic test equipment (ATE) for complex processor circuit. The proposed decompressor mainly consists of a periodically alterable MUX network which has multiple configurations to decode the input information flexibly and efficiently. In order to reduce the number of test patterns and configurations, a test patterns compaction algorithm, using CI-Graph merging, is proposed. With the proposed periodically alterable MUX network and the patterns compaction algorithm, smaller test data volume and required external pins can be achieved as compared to previous techniques.\"",
        "1 is \"Thermal monitoring on FPGAs using ring-oscillators\", 2 is \"Layout-driven test-architecture design and optimization for 3D SoCs under pre-bond test-pin-count constraint\"",
        "Given above information, for an author who has written the paper with the title \"On topology reconfiguration for defect-tolerant NoC-based homogeneous manycore systems\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005668": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Orthogonal Linear Beamforming In Mimo Broadcast Channels':",
        "Document: \"A modular prewindowing framework for covariance FTF RLS algorithms. In a companion paper, a fast transversal filter (FTF) algorithm was derived for solving multichannel multiexperiment recursive least-squares (RLS) problems arising in adaptive FIR filtering. By introducing sequential processing of the different channels and experiments, the multichannel multiexperiment algorithm was decomposed into a set of intertwined single-channel single-experiment algorithms, resulting in a modular algorithm structure. The algorithm was derived under the prewindowing assumption. However, using an embedding into multichannel and multiexperiment problems, we show how the conventional FTF algorithms for the growing-window and sliding-window covariance cases follow naturally from the modular prewindowed algorithm. Furthermore, taking the sequential processing one step of granularity further, we derive modular multichannel FTF algorithms for these covariance cases also.\"",
        "Document: \"On the performance of joint linear minimum mean squared error (LMMSE) filtering and parameter estimation. We consider the problem of LMMSE estimation (such as Wiener and Kalman filtering) in the presence of a number of unknown parameters in the second-order statistics, that need to be estimated also. This well-known joint filtering and parameter estimation problem has numerous applications. It is a hybrid estimation problem in which the signal to be estimated by linear filtering is random, and the unknown parameters are deterministic. As the signal is random, it can also be eliminated, allowing parameter estimation from the marginal distribution of the data. An intriguing question is then the relative performance of joint vs. marginalized parameter estimation. In this paper, we consider jointly Gaussian signal and data and we first provide contributions to Cramer-Rao bounds (CRBs). We characterize the difference between the Hybrid Fisher Information Matrix (HFIM) and the classical marginalized FIM on the one hand, and between the FIM (with CRB asymptotically attained by ML) and the popular Modified FIM (MFIM, inverse of Modified CRB) which is a loose bound. We then investigate three iterative (alternating optimization) joint estimation approaches: Alternating Maximum A Posteriori for Signal and Maximum Likelihood for parameters (AMAPML), which in spite of a better HFIM suffers from inconsistent parameter bias, Expectation-Maximization (EM) which converges to (marginalized) ML (but with AMAPML signal estimate), and Variational Bayes (VB) which yields an improved signal estimate with the parameter estimate asymptotically becoming ML.\"",
        "1 is \"Massive MIMO for next generation wireless systems\", 2 is \"Optimal Design of Non-Regenerative MIMO Wireless Relays\"",
        "Given above information, for an author who has written the paper with the title \"Orthogonal Linear Beamforming In Mimo Broadcast Channels\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005693": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'GAMESH: A grid architecture for scalable monitoring and enhanced dependable job scheduling.':",
        "Document: \"An Open Secure Mobile Agent Framework for Systems Management.  The Mobile Agent (MA) technology is gaining importance in the distributed managementof networks and services for heterogeneous environments. MA-based managementsystems could represent an interesting alternative to traditional tools built upon theclient/server model, either SNMPor CMIPbased. The acceptance of MA solutions formanagement is currently limited by two main requirements: the need of interoperabilityand the request for security. Without security, management systems can not... \"",
        "Document: \"A k-hop Clustering Protocol for Dense Mobile Ad-Hoc Networks. Dense Mobile Ad-hoc NETworks (MANET), i.e., geographical areas with relatively high and almost constant density of mobile terminals communicating in ad-hoc mode (such as in airport terminals, shopping malls, and university campuses) are becoming deployment scenarios of growing and growing commercial interest. The effective determination of k-hop clusters in MANET is crucial to efficiently support many relevant tasks, e.g., packet routing, and has recently started to be investigated. The paper claims that the assumption of dense MANET permits to propose novel k-hop clustering solutions that outperform existing ones in terms of both effectiveness and limited overhead. In particular, the paper presents an innovative k-clustering protocol, based on circle covering, and quantitatively evaluates its performance when adopting different decentralized heuristics. The proposed protocol, coupled with original heuristics considering the positions of neighbor cluster-heads, has demonstrated to achieve better performance than other solutions in the literature with a significantly lower message overhead.\"",
        "1 is \"Filtering Security Alerts for the Analysis of a Production SaaS Cloud\", 2 is \"ClusterProbe: An Open, Flexible and Scalable Cluster Monitoring Tool\"",
        "Given above information, for an author who has written the paper with the title \"GAMESH: A grid architecture for scalable monitoring and enhanced dependable job scheduling.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005736": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Constrained many-objective optimization: a way forward':",
        "Document: \"A Repair Mechanism For Active Inequality Constraint Handling. Constraint handling is an active field of research in the Genetic Algorithms community, considering that one or more constraints need to be satisfied in most real life optimization problems. Recently, we proposed a Most Probable Point based repair approach for handling equality constraints in Single-objective and Multi-objective optimization problems. In this work, we demonstrate the application of the repair approach to handle active inequality constraints. We show that the repair mechanism, which has so far been strictly applied to the domain of equality constraint handling can be used to obtain better results with faster convergence even in inequality constrained problems. We take up a number of standard Single-objective test problems having one or more active inequality constraints for our study. The applicability of the proposed procedure is demonstrated on a well studied Engineering design optimization problem. The present study contributes to the scarce body of literature available on repair mechanisms in inequality constraint handling and hence should motivate further research in this direction.\"",
        "Document: \"Parameters adaptation in Differential Evolution. Over the last few decades, a considerable number of Differential Evolution (DE) algorithms have been proposed with excellent performance on mathematical benchmarks. However, like any other optimization algorithm, the success of DE is highly dependent on its search operators and control parameters. Although a considerable number of investigations have been carried out for parameter selection, it is seen as a tedious task. In this paper, we propose a DE algorithm that uses an adaptive mechanism to select the best performing combination of parameters (amplification factor, crossover rate and the population size) during the course of a single run. The performance of the algorithm is analyzed on a set of 24 constrained optimization test problems. The results demonstrate that the proposed algorithm not only saves the computational time, but also shows better performance over the state-of-the-art algorithms.\"",
        "1 is \"Design of an intelligent supplier relationship management system: a hybrid case based neural network approach\", 2 is \"A memetic random-key genetic algorithm for a symmetric multi-objective traveling salesman problem\"",
        "Given above information, for an author who has written the paper with the title \"Constrained many-objective optimization: a way forward\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005817": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Maximum Likelihood PSD Estimation for Speech Enhancement in Reverberation and Noise.':",
        "Document: \"Alternative formulation and robustness analysis of the multichannel wiener filter for spatially distributed microphones. The multichannel Wiener filter (MWF) is a well-known multi-microphone noise reduction technique, which aims to estimate the speech component in one of the microphone signals. Assuming a single speech source, the rank-one property of the speech correlation matrix can be exploited to derive the so-called rank-one MWF (R1-MWF). In this paper, we present an alternative formulation of the MWF (A-MWF), which exploits the assumed rank-one property of the speech correlation matrix in a different way as the R1-MWF. Furthermore, we present a theoretical robustness analysis of the different MWF formulations in presence of spatially white noise. Experimental results show that similarly to the R1-MWF, the proposed A-MWF is less sensitive to estimation errors of the speech correlation matrix and yields a higher output SNR than the standard MWF.\"",
        "Document: \"A Multichannel Wiener Filter With Partial Equalization For Distributed Microphones. In speech enhancement applications, the multichannel Wiener filter (MWF) is widely used to reduce noise and thus improve signal quality. The MWF performs noise reduction by estimating the desired signal component in one of the microphones, referred to as the reference microphone. However, for distributed microphones, the selection of the reference microphone has a significant impact on the broadband output SNR of the MWF, largely depending on the acoustical transfer function (ATF) between the desired source and the reference microphone. In this paper, a multichannel Wiener filtering approach using a soft combined reference is presented. Simulation results show that the proposed scheme leads to a higher broadband output SNR compared to an arbitrarily selected reference microphone, moreover achieving a partial equalization of the overall acoustic system.\"",
        "1 is \"Switching adaptive filters for enhancing noisy and reverberant speech from microphone array recordings\", 2 is \"Greedy Layer-Wise Training of Deep Networks\"",
        "Given above information, for an author who has written the paper with the title \"Maximum Likelihood PSD Estimation for Speech Enhancement in Reverberation and Noise.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005829": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Simulating Virtual Humans in Networked Virtual Environments':",
        "Document: \"3D animation creation using space canvases for free-hand drawing. In this paper we present a novel 3D animation system using a set of easily manipulable space canvases that support free-hand drawing. Our aim is to support the traditional free-hand drawing while improving the functionality by imitating the 3D animation in terms of free-viewing and free animation. The system design emphasis is on the feeling of \"what you see is what you get\". In our system a user is allowed to create planar and curved canvases and place them in 3D space with six degrees of freedom. Free-hand strokes are drawn on each canvas by using any of a user's favorite input device; a mouse, a digital pen, etc. Various canvases organized in space form a scene and the animation key frames of those canvases and strokes can be recorded hierarchically to represent different levels of movement in the scene. The camera's movement is recorded when a user tunes the viewing parameters and chooses to add a key frame in the time line for the camera. The system is both intuitive by utilizing the advantage of 2D freehand drawing and has the capability of 3D manipulation of strokes, canvases and a camera without requiring the user to have knowledge of 3D graphics and animation. We demonstrate the usability and efficiency of our system by describing the creation process of several short animation movies.\"",
        "Document: \"A subject gateway in multiple languages: a prototype development and lessons learned. The Internet has a lot of rich information resources available in different languages aside from English, the major language of use. Those resources are, however, not easy to access because of language barriers. This paper proposes a collaborative model to develop a multilingual subject gateway, which is called the Internet Public Library Asia (IPL-Asia). It describes how IPL-Asia was conceived and developed, and some lessons learned from the experience. IPL-Asia is a subject gateway which collects high quality Internet resources expressed in Chinese, Japanese, Korean languages, and provides information about those resources in those three languages and English. This paper shows the metadata schema and an implementation of IPL-Asia. The schema is defined based on Dublin Core and IEEE LOM with some extensions for multilingual description. Criteria for selecting quality resources on the Internet were also defined. A metadata record is collaboratively described in multiple languages by catalogers whose mother languages are different. A metadata record is encoded in XML. Metadata records are stored and displayed using XML technologies, e.g., XSLT and XML database. This system provides user interfaces in multiple languages.\"",
        "1 is \"Face recognition using a hybrid supervised/unsupervised neural network\", 2 is \"Learning Semantic Deformation Flows With 3d Convolutional Networks\"",
        "Given above information, for an author who has written the paper with the title \"Simulating Virtual Humans in Networked Virtual Environments\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005933": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Efficient solutions to factored MDPs with imprecise transition probabilities':",
        "Document: \"Arbitrary conditional inference in variational autoencoders via fast prior network training. Variational Autoencoders (VAEs) are a popular generative model, but one in which conditional inference can be challenging. If the decomposition into query and evidence variables is fixed, conditionally trained VAEs provide an attractive solution. However, to efficiently support arbitrary queries over pre-trained VAEs when the query and evidence are not known in advance, one is generally reduced to MCMC sampling methods that can suffer from long mixing times. In this paper, we propose an idea of efficiently training small conditional prior networks to approximate the latent distribution of the VAE after conditioning on an evidence assignment; this permits generating query samples without retraining the full VAE. We experimentally evaluate three variations of conditional prior networks showing that (i) they can be quickly optimized for different decompositions of evidence and query and (ii) they quantitatively and qualitatively outperform existing state-of-the-art methods for conditional inference in pre-trained VAEs.\"",
        "Document: \"Practical solution techniques for first-order MDPs. Many traditional solution approaches to relationally specified decision-theoretic planning problems (e.g., those stated in the probabilistic planning domain description language, or PPDDL) ground the specification with respect to a specific instantiation of domain objects and apply a solution approach directly to the resulting ground Markov decision process (MDP). Unfortunately, the space and time complexity of these grounded solution approaches are polynomial in the number of domain objects and exponential in the predicate arity and the number of nested quantifiers in the relational problem specification. An alternative to grounding a relational planning problem is to tackle the problem directly at the relational level. In this article, we propose one such approach that translates an expressive subset of the PPDDL representation to a first-order MDP (FOMDP) specification and then derives a domain-independent policy without grounding at any intermediate step. However, such generality does not come without its own set of challenges-the purpose of this article is to explore practical solution techniques for solving FOMDPs. To demonstrate the applicability of our techniques, we present proof-of-concept results of our first-order approximate linear programming (FOALP) planner on problems from the probabilistic track of the ICAPS 2004 and 2006 International Planning Competitions.\"",
        "1 is \"Generative and discriminative face modelling for detection\", 2 is \"Studying machine translation technologies for large-data CLIR tasks: a patent prior-art search case study\"",
        "Given above information, for an author who has written the paper with the title \"Efficient solutions to factored MDPs with imprecise transition probabilities\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005940": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Context-Aware Data Caching For 5g Heterogeneous Small Cells Networks':",
        "Document: \"Distributed wireless energy scheduling for wireless powered sensor networks. A wireless powered communication network is a potential application of wireless energy harvesting to improve convenience and flexibility. However, wireless energy transfer from a wireless energy source has to be scheduled to minimize energy usage while meeting quality of service (QoS) requirements of sensor nodes in the network. In this paper, we consider wireless powered sensor network whose sensor nodes have auxiliary energy sources in addition to dedicated wireless energy transfer. We propose a distributed wireless energy transfer scheduling to achieve the aforementioned objective and meet the requirements. We formulate a constrained stochastic game model to obtain a multi-policy constrained Nash equilibrium of wireless energy transfer request. This equilibrium instructs the sensor node to request for wireless energy transfer based on its local state. The performance evaluation shows that the analytical model is well verified by numerical simulations.\"",
        "Document: \"\u03bb-Augmented Tree for Robust Data Collection in Advanced Metering Infrastructure. AbstractTree multicast configuration of smart meters SMs can maintain the connectivity and meet the latency requirements for the Advanced Metering Infrastructure AMI. However, such topology is extremely weak as any single failure suffices to break its connectivity. On the other hand, the impact of a SM node failure can be more or less significant: a noncut SM node will have a limited local impact compared to a cut SM node that will break the network connectivity. In this work, we design a highly connected tree with a set of backup links to minimize the weakness of tree topology of SMs. A topology repair scheme is proposed to address the impact of a SM node failure on the connectivity of the augmented tree network. It relies on a loop detection scheme to define the criticality of a SM node and specifically targets cut SM node by selecting backup parent SM to cover its children. Detailed algorithms to create such AMI tree and related theoretical and complexity analysis are provided with insightful simulation results: sufficient redundancy is provided to alleviate data loss at the cost of signaling overhead. It is however observed that biconnected tree provides the best compromise between the two entities.\"",
        "1 is \"Reputation-based cooperative spectrum sensing with trusted nodes assistance\", 2 is \"Anomaly Detection and Diagnosis for Automatic Radio Network Verification.\"",
        "Given above information, for an author who has written the paper with the title \"Context-Aware Data Caching For 5g Heterogeneous Small Cells Networks\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006037": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Opportunistic Routing in Intermittently Connected Mobile P2P Networks.':",
        "Document: \"Cross-Layer Design for Proportional Delay Differentiation and Network Utility Maximization in Multi-Hop Wireless Networks. One major problem of cross-layer control algorithms in multi-hop wireless networks is that they lead to large end-to-end delays. Recently there have been many studies devoted to solving the problem to guarantee order-optimal per-flow delay. However, these approaches also bring the adverse effect of sacrificing a lot of network utility. In this paper, we solve the large-delay problem without sacrificing network utility. We take a fundamentally different approach of delay differentiation, which is based on the observation that flows in a network usually have different requirements for end-to-end delay. We propose a novel joint rate control, routing and scheduling algorithm called CLC_DD, which ensures that the flow delays are proportional to certain pre-specified delay priority parameters. By adjusting delay priority parameters, the end-to-end delays of preferential flows achieved by CLC_DD can be as small as those achieved by delay-order-optimal algorithms. In contrast to high network utility loss in previous approaches, we prove that our approach achieves maximum network utility. Furthermore, we incorporate opportunistic routing into the cross-layer design framework to improve network performance under the environment of dynamic wireless channels.\"",
        "Document: \"Joint Variable Width Spectrum Allocation and Link Scheduling for Wireless Mesh Networks. In wireless mesh networks with frequency-agile radios, an algorithm of dynamically combining consecutive channels has recently been proposed. However, the available channel widths are limited in the algorithm. In order to further improve the fairness or the throughput under given fairness, we propose a joint variable width spectrum allocation and link scheduling optimization algorithm. Our algorithm is composed of time division multiple access for no interface conflict and frequency division multiple access for no signal interference. In the first phase, we use as few time slots as possible to assign at least one time slots to each radio link with Max-Min fairness. In the second phase, our design jointly allocates the lengths of time slots as well as the spectral widths and center frequencies of radio links in each time slot. Numerical results indicate that compared to the existing algorithm, our algorithm significantly increases the fairness or the throughput under given fairness.\"",
        "1 is \"Secure implementation of identification systems\", 2 is \"Methods for restoring MAC layer fairness in IEEE 802.11 networks with physical layer capture\"",
        "Given above information, for an author who has written the paper with the title \"Opportunistic Routing in Intermittently Connected Mobile P2P Networks.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006061": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Comparison Of Spatial And Temporal Characteristic Between Reflection-Type Tactile Sensor And Human Cutaneous Sensation':",
        "Document: \"Compression Performances Of Computer Vision Based Coding. This paper presents new results in the field of very low bitrate coding and compression using 3D informations. Contrary to prior art in model-based coding where 3D models have to be known, the 3D models are automatically computed from the original video sequence. The camera parameters and the scene content are supposed unknown and the video sequence is processed on the fly. A stream of 3D models is then extracted and compressed, using adapted compression techniques. We finally show the results of the proposed compression scheme, and the efficiency of this approach.\"",
        "Document: \"Determination of Sign of Gaussian Curvature of Surface Having General Reflectance Property. A new method for determining sign of the Gaussian curvature at points on the surface of an object using images taken under three different lighting directions is presented. A smooth surface is segmented into two regions based on the sign of Gaussian curvature. The boundaries of these two regions are independent of the orientation of the object in 3D space and the position of the viewer. Thus they are useful for various applications such as object recognition and pose estimation. The present method does not require knowledge of the directions and strengths of the light sources except the rotation orientation of the directions about the viewing direction if they are linearly independent. It is applicable for almost any kind of surface reflecting properties.\"",
        "1 is \"Immersive planar displays using roughly aligned projectors\", 2 is \"Improving Contact Realism through Event-Based Haptic Feedback\"",
        "Given above information, for an author who has written the paper with the title \"Comparison Of Spatial And Temporal Characteristic Between Reflection-Type Tactile Sensor And Human Cutaneous Sensation\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006083": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'The Visual Object Tracking VOT2016 Challenge Results':",
        "Document: \"Real-Time Facial Pose Identification With Hierarchically Structured Ml Pose Classifier. Since pose-varying face images form nonlinear convex manifold in high dimensional image space, it is difficult to model their pose distribution in terms of a simple probabilistic density function. To solve this difficulty, we divide the pose space into many constituent pose classes and treat the continuous pose estimation problem as a discrete pose-class identification problem. We propose to use a hierarchically structured ML (Maximum Likelihood) pose classifiers in the reduced feature space to decrease the computation time for pose identification, where pose space is divided into several pose groups and each group consists of a number of similar neighboring poses. We use the CONDENSATION algorithm to find a newly appearing face and track the face with a variety of poses in real-time. Simulation results show that our proposed pose identification using the hierarchically structured ML pose classifiers can perform a faster pose identification than conventional pose identification using the flat structured ML pose classifiers. A real-time facial pose tracking system is built with high speed hierarchically structured ML pose classifiers.\"",
        "Document: \"Two Co-Adaptation Schemes Of Evolution And Learning For An Optimal Vq Codebook. This paper proposes two co-adaptation schemes of self-organizing maps that incorporate the Kohonen's learning into the GA evolution in an attempt to find an optimal vector quantization codebook of images. The Kohonen's learning rule used for vector quantization of images is sensitive to the choice of its initial parameters and the resultant codebook does not guarantee a minimum distortion. To tackle these problems, we co-adapt the codebooks by evolution and learning in a way that the evolution performs the global search and makes inter-codebook adjustments by altering the codebook structures while the learning performs the local search and makes intra-codebook adjustments by interacting each codebook with its environment. Two kinds of co-adaptation schemes such as Lamarckian and Baldwin co-adaptation are considered in our work. Simulation results show that the evolution guided by a local learning provides the fast convergence, the co-adapted codebook produces better reconstruction image quality than the non-learned equivalent, and Lamarckian co-adaptation turns out more appropriate for the VQ problem.\"",
        "1 is \"Lambertian Reflectance and Linear Subspaces\", 2 is \"Human tracking using convolutional neural networks.\"",
        "Given above information, for an author who has written the paper with the title \"The Visual Object Tracking VOT2016 Challenge Results\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006121": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Unequal error protection distributed network-channel coding based on LT codes for wireless sensor networks':",
        "Document: \"Two-layer rateless codes over lattices for Gaussian broadcast channels. In this paper, we present a class of rateless coset codes, which consists of two layers. The first layer is protected by a systematic lattice-Kite code, which takes as input a sequence of coset leaders and delivers as output a sequence of coset leaders with redundancy. The second layer is for shaping, which can be implemented by using a (possibly coded) binary sequence to select lattice points from each coset. Based on the presented two-layer lattice-Kite code, we propose a rateless transmission scheme for two-user Gaussian broadcast channels (TU-GBC), where the common information and the private information are carried by the first layer and the second layer, respectively. The proposed scheme allows the user with good channel conditions to collect more private information. Simulation results are given to show the effectiveness of the proposed rateless transmission scheme. \u00a9 2013 IEEE.\"",
        "Document: \"Unequal error protection by partial superposition transmission using low-density parity-check codes. In this study, the authors consider designing low-density parity-check (LDPC) coded modulation systems to achieve unequal error protection (UEP). They propose a new UEP approach by partial superposition transmission (PST) called UEP-by-PST. In the UEP-by-PST system, the information sequence is distinguished as two parts, the more important data (MID) and the less important data (LID), both of which are coded with LDPC codes. The codeword that corresponds to the MID is superimposed on the codeword that corresponds to the LID. The system performance can be analysed by using discretised density evolution. Also proposed in this study is a criterion from a practical point of view to compare the efficiencies of different UEP approaches. Numerical results show that, over both additive white Gaussian noise channels and uncorrelated Rayleigh fading channels, (i) UEP-by-PST provides higher coding gain for the MID compared with the traditional equal error protection approach, but with negligible performance loss for the LID; and (ii) UEP-by-PST is more efficient with the proposed practical criterion than the UEP approach in the digital video broadcasting system.\"",
        "1 is \"Secure partial repair in wireless caching networks with broadcast channels\", 2 is \"An introduction to contemporary cryptology\"",
        "Given above information, for an author who has written the paper with the title \"Unequal error protection distributed network-channel coding based on LT codes for wireless sensor networks\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006143": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Output tracking control for autonomous spacecraft rendezvous':",
        "Document: \"A modified distributed optimization method for both continuous-time and discrete-time multi-agent systems. Abstract   This paper discusses the distributed optimization problem for the continuous-time and discrete-time multi-agent systems. For such a problem, each agent possesses a local convex cost function only known by itself and all the agents converge to the optimizer of the sum of the local cost function through estimating the optimal states of the local cost function and exchanging states information between agents. Sufficient conditions for convergence to the optimizer of the continuous-time and discrete-time algorithms are provided by making use of the Lyapunov method. We also obtain the least convergence rate for the modified algorithm. Moreover, numerical simulations are supplied to testify the results we present.\"",
        "Document: \"On Global Stability of Delayed BAM Stochastic Neural Networks with Markovian Switching. In this paper, the stability analysis problem is investigated for stochastic bi-directional associative memory (BAM) neural networks with Markovian jumping parameters and mixed time delays. Both the global asymptotic stability and global exponential stability are dealt with. The mixed time delays consist of both the discrete delays and the distributed delays. Without assuming the symmetry of synaptic connection weights and the monotonicity and differentiability of activation functions, we employ the Lyapunov---Krasovskii stability theory and the It\u00f4 differential rule to establish sufficient conditions for the delayed BAM networks to be stochastically globally exponentially stable and stochastically globally asymptotically stable, respectively. These conditions are expressed in terms of the feasibility to a set of linear matrix inequalities (LMIs). Therefore, the global stability of the delayed BAM with Markovian jumping parameters can be easily checked by utilizing the numerically efficient Matlab LMI toolbox. A simple example is exploited to show the usefulness of the derived LMI-based stability conditions.\"",
        "1 is \"Bayesian Error Concealment With DCT Pyramid for Images\", 2 is \"An integrated approach utilizing artificial neural networks and SELDI mass spectrometry for the classification of human tumours and rapid identification of potential biomarkers.\"",
        "Given above information, for an author who has written the paper with the title \"Output tracking control for autonomous spacecraft rendezvous\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006150": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Queue-Aware Joint Remote Radio Head Activation and Beamforming for Green Cloud Radio Access Networks.':",
        "Document: \"Low complexity soft-input soft-output block decision feedback equalization. A low complexity soft-input soft-output (SISO) block decision feedback equalizer (BDFE) is presented for turbo equalization. The proposed method employs a sub-optimum sequence-based detection, where the soft-output of the equalizer is calculated by evaluating an approximation of the sequence-based a posteriori probability (APP) of the data symbol. The sequence-based APP approximation is enabled by the adoption of both soft a priori information and soft decision feedback, and it leads to better performance and faster convergence compared to symbol-based detection methods as used by most other low complexity equalizers. The performance and convergence property of the proposed algorithm is analyzed by using extrinsic information transfer (EXIT) chart. Both analytical and simulation results show that the new equalizer can achieve a performance similar to that of trellis-based equalization algorithms, with a complexity similar to linear SISO minimum mean square error equalizers.\"",
        "Document: \"Optimal diversity combining based on linear estimation of rician fading channels. Optimal receiver diversity combining employing linear channel estimation is examined. Based on the statistical properties of pilot-assisted least-squares (LS) and minimum mean square error (MMSE) channel estimation, an optimal diversity receiver for wireless systems employing practical linear channel estimation on Rician fading channels is proposed. Exact analytical expressions for the symbol error rates of LS and MMSE channel estimation aided optimal diversity combining are derived. It is shown that an MPSK wireless system with MMSE channel estimation has the same SER when the MMSE channel estimation is replaced by LS estimation. This is an interesting counter-example to the common perception that channel esti- mation with smaller mean square error leads to smaller SER. Extensive simulation results validate the theoretical results. I. INTRODUCTION The properties of least-squares (LS) and minimum mean square error (MMSE) channel estimation are investigated, and analytical expressions are provided to describe the statistical relationship between channel estimation error and pilot symbol power. It is shown that, under certain system configurations, the conventional MRC receivers is no longer optimum at the presence of channel estimation error. A new optimal decision rule for coherent diversity receivers is proposed by taking into account the effect of channel estimation errors. Exact error probability expressions for the proposed optimal coherent diversity receivers employing both LS channel estimation and MMSE channel estimation in M-ary phase shift keying (MPSK) systems are derived. Due to the presence of channel estimation error, the classical moment generating function (MGF) and characteristic function (CHF) methods cannot be directly applied in the error performance analysis. Instead, a complex Gaussian distribution-based functional equivalency is employed for the evaluation of error probabilities. Interestingly, both analytical and simulation results show that the wireless MPSK system with LS channel estimation has the same error probability as the system with MMSE channel estimation replacing LS channel estimation, even though MMSE channel estimation outperforms LS channel estimation in terms of mean square error. The rest of this paper is organized as follows. The statistics of pilot assisted linear channel estimation are investigated in Section II. Section III derives an optimal decision rule for diversity receivers operating with linear estimation of i.i.d. fading channels. The error probabilities of the receivers in Rician fading channels are derived in Section IV. Numerical examples are given in Section V, and Section VI concludes the paper.\"",
        "1 is \"An energy efficient hierarchical clustering algorithm for wireless sensor networks\", 2 is \"Convergence Analysis Of Mixed Timescale Cross-Layer Stochastic Optimization\"",
        "Given above information, for an author who has written the paper with the title \"Queue-Aware Joint Remote Radio Head Activation and Beamforming for Green Cloud Radio Access Networks.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006152": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Family model mining for function block diagrams in automation software':",
        "Document: \"Comparing Structure-Oriented and Behavior-Oriented Variability Modeling for Workflows. Workflows exist in Wally different variants in order to adapt the behavior of systems to different circumstances and to arising user's needs. Variability modeling is a way of keeping track at the model level of the currently supported and used workflow variants. Variability modeling approaches for workflows address two directions: structure-oriented approaches explicitly specify the workflow variants by means of linguistic constructs, while behavior-oriented approaches define the set of all valid compositions of workflow components by means of ontological annotations and temporal logic constraints. In this paper, we describe how both structure-oriented and behavior-oriented variability modeling can be captured in an eXtreme Model-Driven Design paradigm (XMDD). We illustrate this via a concrete case (a variant-rich bioinformatics workflow realized with the jABC platform for XMDD), and we compare the two approaches in order to identify their profiles and synergies.\"",
        "Document: \"Implementing type-safe software product lines using parametric traits. A software product line (SPL) is a set of related software systems with well-defined commonality and variability that are developed by reusing common artifacts. In this paper, we present a novel technique for implementing SPLs by exploiting mechanisms for fine-grained reuse which are orthogonal to class-based inheritance. In our approach the concepts of type, behavior, and state are separated into different and orthogonal linguistic concepts: interfaces, traits and classes, respectively. We formalize our proposal by means of Featherweight Parametric Trait Java (FPTJ), a minimal core calculus where units of product functionality are modeled by parametric traits. Traits are a well-known construct for fine-grained reuse of behavior. Parametric traits are traits parameterized by interface names and class names. Parametric traits are applied to interface names and class names to generate traits that can be assembled in other (possibly parametric) traits or in classes that are used to build products. The composition of product functionality is realized by explicit operators of the calculus, allowing code manipulations for modeling product variability. The FPTJ type system ensures that the products in the SPL are type-safe by inspecting the parametric traits and classes shared by different products only once. Therefore, type-safety of an extension of a (type-safe) FPTJ SPL can be guaranteed by inspecting only the newly added parts.\"",
        "1 is \"Monitoring Temporal Properties of Continuous Signals\", 2 is \"Specifying adaptation semantics\"",
        "Given above information, for an author who has written the paper with the title \"Family model mining for function block diagrams in automation software\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006157": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'From diversity to creativity: stimulating group brainstorming with cultural differences and conversationally-retrieved pictures':",
        "Document: \"Undergraduate Students' Preferences for Friendsourcing on Facebook Vs. Group Messaging Applications. When people have a question one strategy they can use to get an answer is friendsourcing broadcasting the question to one or more of their social networks. For example, people can post a question via a status update on Facebook or send a message to a group chat on a messaging app. To better understand people's decisions about what media to use for question asking and answering, we interviewed 17 undergraduate students about their friendsourcing practices. We found that interviewees preferred to friendsource via group messaging rather than Facebook. They gave two reasons for this preference: (a) increased likelihood of finding the kinds of answers they wanted and (b) ease of managing self-presentational and interactional concerns. We provide design suggestions for enhancing friendsourcing on group apps and Facebook.\"",
        "Document: \"Effects of implicit sharing in collaborative analysis. When crime analysts collaborate to solve crime cases, they need to share insights in order to connect the clues, identify a pattern, and attribute the crime to the right culprit. We designed a collaborative analysis tool to explore the value of implicitly sharing insights and notes, without requiring analysts to explicitly push information or request it from each other. In an experiment, pairs of remote individuals played the role of crime analysts solving a set of serial killer crimes with both partners having some, but not all, relevant clues. When implicit sharing of notes was available, participants remembered more clues related to detecting the serial killer, and they perceived the tool as more useful compared to when implicit sharing was not available.\"",
        "1 is \"Productive failure in CSCL groups\", 2 is \"Coupling Interactions and Performance: Predicting Team Performance from Thin Slices of Conflict.\"",
        "Given above information, for an author who has written the paper with the title \"From diversity to creativity: stimulating group brainstorming with cultural differences and conversationally-retrieved pictures\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006185": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Construction of interval-valued fuzzy preference relations from ignorance functions and fuzzy preference relations. Application to decision making':",
        "Document: \"The Structure of the Type-Reduced Set of a Continuous Type-2 Fuzzy Set. This paper is concerned with the structure of the type-reduced set (TRS) of the continuous type-2 fuzzy set, in both its interval and generalised forms. In each case the TRS structure is approached by first investigating the discretised set. The TRS of a continuous interval type-2 fuzzy set is shown to be a continuous straight line, and that of a generalised type-2 fuzzy set, a continuous, convex curve.\"",
        "Document: \"Visualizing Consensus In Group Decision Making Situations. In the resolution of group decision making problems the consensus process, that is, the process where experts discuss about the alternatives to narrow their differences, is usually held with all the experts gathered together in a place where they can speak and discuss about the alternatives in the problem. However, in situations where it is not possible to bring them all together it is usually difficult for the experts to identify the closeness of the opinions of the rest of the experts, and thus, it can be difficult to have a clear view of the current state of the consensus process.In this paper we present a tool that creates consensus diagrams that help experts to easily comprehend the current consensus state in the decision problem. This tool is based on several consistency, consensus and similarity measures and with the application of a clustering algorithm, it identifies and represents the different groups of experts with similar opinions, identifies a possible candidate for spokesperson for each group and easily depicts the consistency level expressed by each one of the experts.\"",
        "1 is \"Fundamental relationship between bilateral filtering, adaptive smoothing, and the nonlinear diffusion equation\", 2 is \"Computing with words and its relationships with fuzzistics\"",
        "Given above information, for an author who has written the paper with the title \"Construction of interval-valued fuzzy preference relations from ignorance functions and fuzzy preference relations. Application to decision making\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006209": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Modeling and real-time scheduling of semiconductor manufacturing line based on simulation':",
        "Document: \"Differential annealing for global optimization. This paper propose a hybrid stochastic approach called differential annealing algorithm. The algorithm integrated the advantages of differential evolution and simulated annealing. It can be considered as a swarm-based simulated annealing with differential operator or differential evolution with the Boltzmann-type selection operator. The proposed algorithm is tested on benchmark functions, along with simulated annealing and differential evolution. Results show that differential annealing outperforms the comparative group under the same amount of function evaluations.\"",
        "Document: \"A fast two-stage ACO algorithm for robotic path planning. Ant colony optimization (ACO) algorithms are often used in robotic path planning; however, the algorithms have two inherent problems. On one hand, the distance elicitation function and transfer function are usually used to improve the ACO algorithms, whereas, the two indexes often fail to balance between algorithm efficiency and optimization effect; On the other hand, the algorithms are heavily affected by environmental complexity. Based on the scent pervasion principle, a fast two-stage ACO algorithm is proposed in this paper, which overcomes the inherent problems of traditional ACO algorithms. The basic idea is to split the heuristic search into two stages: preprocess stage and path planning stage. In the preprocess stage, the scent information is broadcasted to the whole map and then ants do path planning under the direction of scent information. The algorithm is tested in maps of various complexities and compared with different algorithms. The results show the good performance and convergence speed of the proposed algorithm, even the high grid resolution does not affect the quality of the path found. \u00a9 2011 Springer-Verlag London Limited.\"",
        "1 is \"A new asynchronous parallel global optimization method based on simulated annealing and differential evolution\", 2 is \"A correlation-driven optimal service selection approach for virtual enterprise establishment\"",
        "Given above information, for an author who has written the paper with the title \"Modeling and real-time scheduling of semiconductor manufacturing line based on simulation\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006263": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Optimizing model parameter for entity summarization across knowledge graphs.':",
        "Document: \"Feature selection for fast speech emotion recognition. In speech based emotion recognition, both acoustic features extraction and features classification are usually time consuming,which obstruct the system to be real time. In this paper, we proposea novel feature selection (FSalgorithm to filter out the low efficiency features towards fast speech emotion recognition.Firstly, each acoustic feature's discriminative ability, time consumption and redundancy are calculated. Then, we map the original feature space into a nonlinear one to select nonlinear features,which can exploit the underlying relationship among the original features. Thirdly, high discriminative nonlinear feature with low time consumption is initially preserved. Finally, a further selection is followed to obtain low redundant features based on these preserved features. The final selected nonlinear features are used in features' extraction and features' classification in our approach, we call them qualified features. The experimental results demonstrate that recognition time consumption can be dramatically reduced in not only the extraction phase but also the classification phase. Moreover, a competitive of recognition accuracy has been observed in the speech emotion recognition.\"",
        "Document: \"Real-time decentralized voltage control in distribution networks. Voltage control plays an important role in the operation of electricity distribution networks, especially when there is a large penetration of renewable energy resources. In this paper, we focus on voltage control through reactive power compensation and study how different information structures affect the control performance. In particular, we first show that only using voltage measurements to determine reactive power compensation is insufficient to maintain voltage in the acceptable range. Then we proposes two fully decentralized algorithms by slightly adding additional information into the control design. The two algorithms are guaranteed to stabilize the voltage in the acceptable range regardless of the system operating condition. The one with higher complexity can further minimize a cost of reactive power compensation in a particular form. Both of the two algorithms use only local measurements and local variables and require no communication.\"",
        "1 is \"Moment: Maintaining Closed Frequent Itemsets over a Stream Sliding Window\", 2 is \"Contrast - based Image Attention Analysis by Using Fuzzy Growing\"",
        "Given above information, for an author who has written the paper with the title \"Optimizing model parameter for entity summarization across knowledge graphs.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006376": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A family of space-time block codes achieving full diversity with linear receivers':",
        "Document: \"The New Interference Alignment Scheme for the MIMO Interference Channel. In this paper, we propose a new interference alignment (IA) scheme designing jointly the linear transmitter and receiver for the MIMO interference channel system, using minimum total mean square error criterion, subject to individual transmit power constraints. We show that transmitter and receiver under such criterion could be realized through a joint iterative algorithm. The convergence of the proposed algorithm is discussed. We also proposed a robust MMSE-based iterative design with imperfect channel state information (CSI). The proposed robust MMSE-based iterative interference alignment scheme is shown to be less sensitive to channel estimation errors. Simulation results show that the proposed schemes outperform the existing IA schemes with fast convergence.\"",
        "Document: \"Improving Performance Using Interference Alignment on Modulation Signal for MIMO System. In this paper, an interference alignment system for the distributed MIMO network is investigated. We propose a new precoding scheme using interference alignment in modulation signal domain. The simulation results show that the proposed one-dimensional precoding scheme can provide significant performance improvement over the conventional schemes using interference alignment.\"",
        "1 is \"Overcoming interference in spatial multiplexing MIMO cellular networks\", 2 is \"Dimension Reduction of Large-Scale Second-Order Dynamical Systems via a Second-Order Arnoldi Method\"",
        "Given above information, for an author who has written the paper with the title \"A family of space-time block codes achieving full diversity with linear receivers\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006453": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A dynamic pipeline for RNA sequencing on multicore processors':",
        "Document: \"An Evolutionary Trade-Off Between Protein Turnover Rate And Protein Aggregation Favors A Higher Aggregation Propensity In Fast Degrading Proteins. We previously showed the existence of selective pressure against protein aggregation by the enrichment of aggregation-opposing 'gatekeeper' residues at strategic places along the sequence of proteins. Here we analyzed the relationship between protein lifetime and protein aggregation by combining experimentally determined turnover rates, expression data, structural data and chaperone interaction data on a set of more than 500 proteins. We find that selective pressure on protein sequences against aggregation is not homogeneous but that short-living proteins on average have a higher aggregation propensity and fewer chaperone interactions than long-living proteins. We also find that short-living proteins are more often associated to deposition diseases. These findings suggest that the efficient degradation of high-turnover proteins is sufficient to preclude aggregation, but also that factors that inhibit proteasomal activity, such as physiological ageing, will primarily affect the aggregation of short-living proteins.\"",
        "Document: \"A dynamic pipeline for RNA sequencing on multicore processors. We present a concurrent algorithm for mapping short and long RNA sequences on multicore processors. Our solution processes the data, initially stored on disk, in batches of reads which are passed between the consecutive stages of a pipeline. A major operational reorganization of the original static pipeline, combined with a complete reimplementation based on POSIX threads, renders a dissociated execution between threads and stages/task types, so that threads can compute any type of pending task resulting in a dynamic pipeline. The experiments on a multicore platform reveal that this reorganization yields significantly higher performance, specially for architectures equipped with a small to moderate number of cores. As an additional contribution, our experiments also reveal that the use of 16-nucleotide (nt) seeds during the one of the stages of the pipeline, instead of the 15-nt length that was proposed originally, yields a remarkable reduction in the execution time of the global alignment process while maintaining the sensitivity of the algorithm.\"",
        "1 is \"Fully constrained least squares linear spectral mixture analysis method for material quantification in hyperspectral imagery\", 2 is \"SIFT: predicting amino acid changes that affect protein function.\"",
        "Given above information, for an author who has written the paper with the title \"A dynamic pipeline for RNA sequencing on multicore processors\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006452": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'VLSI Block Placement With Alignment Constraints.':",
        "Document: \"LP based white space redistribution for thermal via planning and performance optimization in 3D ICs. Thermal issue is a critical challenge in 3D IC circuit design. Incorporating thermal vias into 3D IC is a promising way to mitigate thermal issues by lowering down the thermal resistances between device layers. However, it is usually difficult to get enough space at target regions to insert thermal vias. In this paper, we propose a novel analytical algorithm to re-allocate white space for 3D ICs to facilitate via insertion. Experimental results show that after reallocating whitespaces, thermal vias and total wirelength could be reduced by 14% and by 2%, respectively. It also shows that whitespace distribution with via planning alone will degrade performance by 9% while performance-aware via planning method can reduce thermal via number by 60% and the performance is kept nearly unchanged.\n\n\"",
        "Document: \"SMPP: Generic SAT Solver over Reconfigurable Hardware Accelerator. To further exploit the potential of reconfigurable computing, fine-grain, super massive parallel processing SAT solver over reconfigurable hardware accelerator is proposed in this paper as SMPP. By analyzing the traditional SAT solver, we proposed a novel way to partition the original problem into software part and hardware part. Software part uses semi-confliction guided backtrack to extract equivalent fixed scale sub-problem sequence. Partition scheme of SMPP exploited the stage effect of inference engine in modern software SAT solver. Hardware part uses tiny processing cells to handle small scale SAT problem in linear time proportional to the number of clause. Theoretical analyze and experiment results show that the speed up factor is significant.\"",
        "1 is \"Managing power and performance for System-on-Chip designs using Voltage Islands\", 2 is \"A fuel-cell-battery hybrid for portable embedded systems\"",
        "Given above information, for an author who has written the paper with the title \"VLSI Block Placement With Alignment Constraints.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006475": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'D2Taint: Differentiated and dynamic information flow tracking on smartphones for numerous data sources':",
        "Document: \"iWatcher: simple, general architectural support for software debugging. We propose Intelligent Watcher (iWatcher), a combination of hardware and software support that can detect large variations of software bugs with only modest hardware changes to current processor implementations. iWatcher lets programmers associate specified functions to \"watched\" memory locations or objects. Access to any such location automatically triggers the monitoring function in the hardware. Relative to other approaches, iWatcher detects many real bugs at a fraction of the execution-time overhead\"",
        "Document: \"A Natural BEM with non-uniform grids on the elliptic boundary and its coupling method. In this paper, for solving Poisson equation in 2-D unbounded domains, a natural BEM with non-uniform grids on the elliptic boundary is investigated, the coupling of NBEM-FEM with non-uniform grids on the elliptic artificial boundary is discussed, and the corresponding convergence theorems of the NBEM and the coupling method are proved. The moving mesh method based on the arc-length equidistribution principle is introduced into the NBEM and the coupling method respectively to solve exterior problems. Numerical examples confirm the convergence and demonstrate the advantage in accuracy and efficiency for the proposed methods.\"",
        "1 is \"RapiLog: reducing system complexity through verification\", 2 is \"Comparing disparity based label segregation in augmented and virtual reality\"",
        "Given above information, for an author who has written the paper with the title \"D2Taint: Differentiated and dynamic information flow tracking on smartphones for numerous data sources\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006508": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Scalable Block-Based Parallel Lattice Reduction Algorithm for an SDR Baseband Processor':",
        "Document: \"A multimodal wireless baseband core using a coarse-grained dynamic reconfigurable processor. Software Defined Ratio (SDR) is indispensable technology when wireless baseband processing has to support a wide variety of wireless communication standards. This paper describes the implementation and software optimization for IEEE 802.16e (Mobile WiMAX) on ADRES, a coarse-grained dynamic reconfigurable processor. The paper also explains switching between 802.11a (WLAN) and 802.16e at runtime. The switching time is 2.6 milliseconds which is shorter than 5.0 milliseconds, 1 frame time period of 802.16e specification.\"",
        "Document: \"Algorithm-architecture co-design of soft-output ML MIMO detector for parallel application specific instruction set processors. Emerging SDR baseband platforms are usually based on multiple DLP+ILP processors with massive parallelism [10]. Although these platforms would theoretically enable advanced SDR signal processing, existing work implemented basic systems and simple algorithms. Importantly, MIMO is not fully supported in most implementations [7][9][11]. [1] implemented MIMO but with a simple linear detector. Our work explores the feasibility for SDR implementations of soft-output ML MIMO detectors, which brings 6--12 dB SNR gains when compared to popular linear detectors. Although soft-output ML MIMO detectors are considered to be challenging even for ASICs [3][4], we combine architecture-friendly algorithms, application specific instructions, code transformations and ILP/DLP explorations to make SDR implementations feasible. In our work, a 2x4 ADRES based ASIP with 16-way SIMD can deliver 193Mbps for 2x2 64QAM, and 368Mbps for 2x2 16QAM transmissions. To the best of our knowledge, this is the first work exploring SDR based soft-output ML MIMO detectors.\"",
        "1 is \"A Self-reconfiguring Platform\", 2 is \"From Theory to Practice: Sub-Nyquist Sampling of Sparse Wideband Analog Signals\"",
        "Given above information, for an author who has written the paper with the title \"Scalable Block-Based Parallel Lattice Reduction Algorithm for an SDR Baseband Processor\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006587": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Integrated Synthesis and Execution of Optimal Plans for Multi-Robot Systems in Logistics':",
        "Document: \"Ontology-based data access: An application to intermodal logistics. In this paper, we investigate ontology-based data access (OBDA) to build information systems whose purposes are (i) gathering data from a network of intermodal terminals, and (ii) computing performance indicators of the network. This application domain is characterized by large amounts of data and relatively simple data models, making it a natural challenge for logic-based knowledge representation and reasoning techniques. Considering relational database (RDB) technology as a yardstick, we show that careful engineering of OBDA can achieve RDB-like scalability even in demanding applications. To the best of our knowledge, this is the first study evaluating the potential of OBDA in a typical business-size application.\"",
        "Document: \"Automated Verification of Neural Networks: Advances, Challenges and Perspectives. Neural networks are one of the most investigated and widely used techniques in Machine Learning. In spite of their success, they still find limited application in safety- and security-related contexts, wherein assurance about networksu0027 performances must be provided. In the recent past, automated reasoning techniques have been proposed by several researchers to close the gap between neural networks and applications requiring formal guarantees about their behavior. In this work, we propose a primer of such techniques and a comprehensive categorization of existing approaches for the automated verification of neural networks. A discussion about current limitations and directions for future investigation is provided to foster research on this topic at the crossroads of Machine Learning and Automated Reasoning.\"",
        "1 is \"Sequent calculi for propositional nonmonotonic logics\", 2 is \"Polsat: A Portfolio LTL Satisfiability Solver.\"",
        "Given above information, for an author who has written the paper with the title \"Integrated Synthesis and Execution of Optimal Plans for Multi-Robot Systems in Logistics\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006697": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A modified bit-map-assisted dynamic queue protocol for multiaccess wireless networks with finite buffers':",
        "Document: \"Self-Interference Suppression in Doubly-Selective Channel Estimation Using Superimposed Training. Channel estimation for frequency-selective time- varying channels is considered using superimposed training. We employ a discrete prolate spheroidal basis expansion model (DPS-BEM) to describe the time-varying channel. A periodic (non-random) training sequence is arithmetically added (superimposed) at low power to the information sequence at the transmitter before modulation and transmission. In existing first-order statistics-based channel estimators, the information sequence acts as interference resulting in a poor signal-to- noise ratio (SNR). In this paper a data-dependent superimposed training sequence is used to either totally or partially cancel out the effects of the unknown information sequence at the receiver on channel estimation. In total cancellation, at certain frequencies, the information-bearing components are nulled. To compensate for this information loss, we propose a partially-data- dependent (PDD) superimposed training scheme where a tradeoff is made between interference cancellation and frequency integrity. An iterative method is also used to enhance channel estimation and data detection and illustrated via a simulation example.\"",
        "Document: \"Decision-Directed Tracking Of Doubly-Selective Channels Using Exponential Basis Models. We present a decision directed tracking approach to doubly selective channel estimation, exploiting the complex exponential basis expansion model (CE BEM) for overall channel variations, and an autoregressive (AR) model to update the HEM coefficients. We track the HEM coefficients via Kalman filtering, aided by symbol decisions from a decision feedback equalizer (DFE). The time gap between symbol decisions and required channel estimates, arising from the decision directed. tracking, is bridged by CE HEM based channel prediction using the estimated HEM coefficients. Simulation examples demonstrate its superior performance over several existing AR or CE HEM based channel tracking schemes.\"",
        "1 is \"Soft Sensing and Optimal Power Control for Cognitive Radio\", 2 is \"Fully Decentralized Multi-Agent Reinforcement Learning with Networked Agents.\"",
        "Given above information, for an author who has written the paper with the title \"A modified bit-map-assisted dynamic queue protocol for multiaccess wireless networks with finite buffers\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006757": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Fractal Based Channel Estimation For Wcdma Systems':",
        "Document: \"Multiple-Antenna Receiving And Frequency Domain Equalization In Transmitted-Reference Uwb Systems. A multiple-antenna receiving and combining scheme is proposed for high-data-rate transmitted-reference (TR) Ultra-Wideband (UWB) systems. The nonlinearity of the inter-symbol interference (ISI) model is alleviated via simple antenna combining. Under the simplified ISI model, frequency domain equalization (FDE) is adopted and greatly reduces the complexity of the equalizer. A simple estimation algorithm for the simplified ISI mode) is presented. Simulation results demonstrate that compared to the single receive antenna scheme, the proposed method can obtain a significant diversity gain and eliminate the BER floor effect. Moreover, compared to the complex second-order time domain equalizer, FDE showed better performance robustness in the case of imperfect model estimation.\"",
        "Document: \"Space-Time Adaptive Multiuser Detection for Direct-Sequence UWB-MIMO Systems. In this paper, a multiuser detection (MUD) scheme employing joint adaptive filter and space-time signal processing is discussed and experimentally investigated for direct-sequence (DS) ultra-wideband (UWB) multiple-input multiple-output (MIMO) systems. In dense multipath environments, by taking advantage of the space-time filter structure in conjunction with simple transmit and/or receive diversity approaches, the investigated MUD scheme is capable of significantly improving the performance in combating multiuser interference (MUI) compared with the temporal MUD method based on single-antenna systems. The effects of transmit and/or receive diversity are also examined via the Monte-Carlo simulations.\"",
        "1 is \"Power Allocation for Conventional and Buffer-Aided Link Adaptive Relaying Systems with Energy Harvesting Nodes\", 2 is \"A Matrix Factorization-Based Structure for Digital Filters\"",
        "Given above information, for an author who has written the paper with the title \"Fractal Based Channel Estimation For Wcdma Systems\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006798": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Success probabilities in Gauss-Poisson networks with and without cooperation':",
        "Document: \"On downlink transmission without transmit channel state information and with outage constraints. This paper investigates downlink transmission over a quasi-static fading Gaussian broadcast channel (BC), to model delay-sensitive applications over slowly time-varying fading channels. System performance is characterized by the outage capacity region. In contrast to most previous work, here the problem is studied under the key assumption that the transmitter knows only the probability distributions of the fading coefficients, not their realizations. For scalar-input channels, two coding schemes are studied. The first scheme is called blind dirty paper coding (B-DPC), which utilizes a robustness property of dirty paper coding to perform precoding at the transmitter. The second scheme is called statistical superposition coding (S-SC), in which each receiver adaptively performs successive decoding with the process statistically governed by the realized fading. Both B-DPC and S-SC schemes achieve the outage capacity region, which dominates the outage rate region of time-sharing, irrespective of the particular fading distributions. The S-SC scheme can be extended to BCs with mnltiple transmit antennas.\"",
        "Document: \"Spatial Statistical Modeling for Heterogeneous Cellular Networks - An Empirical Study. Modeling the spatial distribution of multi-tier base stations (BSs) is an important issue for understanding and validating the analysis and design of heterogeneous cellular networks (HCNs). In this paper, we use different spatial statistical models to describe the spatial distribution of BSs, by fitting real data sets of BS locations in HCNs. Classical statistics like the L function, and cellular network performance metrics like the coverage probability are used to evaluate the goodness-of-fit. The results reveal that notable distinctions exist between modeling HCNs and single-tier networks. Although each tier in a HCN may be most accurately fitted by statistical models other than the Poisson spatial distribution, it is surprising that multiple tiers of independent Poisson spatial distributions provide an accurate description of the overall HCN. The impacts of different network parameters and the dependency between tiers on the modeling accuracy are extensively investigated.\"",
        "1 is \"Increased capacity per unit-cost by oversampling\", 2 is \"Distributed interference compensation for wireless networks\"",
        "Given above information, for an author who has written the paper with the title \"Success probabilities in Gauss-Poisson networks with and without cooperation\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006825": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'An Adaptive Skew Handling Join Algorithm for Large-scale Data Analysis.':",
        "Document: \"Equilibrium analysis of bitcoin block withholding attack: A generalized model. \u2022Both the cost of cooperation and the cost of partial proof are considered.\u2022A parameter known as payoff per time is designed to measure the performance.\u2022All possible parameters are directly related to the computational power.\u2022Information conceal mechanism is found to decrease the occurrence of BWH attack.\"",
        "Document: \"Background modeling and foreground extraction scheme for HD traffic bayonet. In a HD traffic bayonet, HD camera captures a few images once the vehicles drive cross the underground sensing coils. Time interval between two adjacent images is usually uncertain. In this paper, a background modeling and foreground extraction scheme is proposed for these images captured in the bayonet. The proposed scheme contains three modules: initial background modeling, foreground extraction and background update. It can quickly extract the foregrounds appear in the scene, including people, vehicles or other objects, which benefits to the classification, identification analysis, and storage of the data. The experiments prove that the scheme is efficient, with good universality, and not need training in advance.\"",
        "1 is \"Accelerating XPath location steps\", 2 is \"Gradient domain high dynamic range compression\"",
        "Given above information, for an author who has written the paper with the title \"An Adaptive Skew Handling Join Algorithm for Large-scale Data Analysis.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006893": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Strong Lyapunov Functions for Systems Satisfying the Conditions of La Salle':",
        "Document: \"Construction of Lyapunov functionals for coupled differential and continuous time difference equations. A new stability analysis technique for systems composed of a differential equation coupled with a continuous time difference equation is proposed. It is based on the explicit construction of Lyapunov functionals from the knowledge of Lyapunov functions for subsystems. Robustness results of iISS type are inferred from these functionals.\"",
        "Document: \"Stability and Robustness Analysis for Switched Systems with Time-Varying Delays. A new technique is presented for the stability and robustness analysis of nonlinear switched time-varying systems with uncertainties and time-varying delays. The delays are allowed to be discontinuous (but are required to be piecewise continuous) and arbitrarily long with known upper bounds. The technique uses an adaptation of Halanay's inequality and a trajectory-based technique, and is used for designing switched controllers to stabilize linear time-varying systems with time-varying delays.\"",
        "1 is \"Nonlinear control under delays that depend on delayed states\", 2 is \"Path-following for linear systems with unstable zero dynamics\"",
        "Given above information, for an author who has written the paper with the title \"Strong Lyapunov Functions for Systems Satisfying the Conditions of La Salle\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006897": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Requirements engineering tools go mobile':",
        "Document: \"Repeatable Quality Assurance Techniques for Requirements Negotiations. Many software projects fail because early life-cycle defects such as ill-defined requirements are not identified and removed. Therefore, quality assurance (QA) techniques for defect detection and prevention play an important role. The effectiveness and efficiency of QA approaches has been empirically evaluated. In this paper we discuss QA techniques optimized for requirements negotiations. In particular, we focus on negotiations using the EasyWinWin approach. We present (1) repeatable techniques for checking quality throughout negotiations as well as (2) role-oriented inspection techniques helping a project team to reduce unnecessary complexity and to mitigate risks stemming from defects in requirements negotiation results. We present the results of a thorough feasibility study we conducted to test our approach.\"",
        "Document: \"Negotiation constellations in reactive product line evolution. Software product lines are inevitably subject to continuous evolution due to changing customer needs, market developments, or technology trends. Reactive evolution means that changes to the product line are driven by the requirements arising when deriving new products. In this process heterogeneous stakeholders need to negotiate about these emerging requirements. It is also crucial that stakeholders develop a shared understanding about the challenges of implementing the new requirements in the product line. Existing approaches to product line evolution do however not sufficiently address these negotiation challenges. The aim of this position paper is thus to identify key stakeholder roles and negotiation constellations for reactive product line evolution. We illustrate these constellations using the WinWin negotiation model and discuss implications for tool support.\"",
        "1 is \"Blended Interaction: understanding natural human---computer interaction in post-WIMP interactive spaces\", 2 is \"Model-Based Customization and Deployment of Eclipse-Based Tools: Industrial Experiences\"",
        "Given above information, for an author who has written the paper with the title \"Requirements engineering tools go mobile\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006899": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Adaptable, efficient, and modular coordination of distributed extended transactions':",
        "Document: \"Robust Recursive State Estimation with Random Measurement Droppings. A recursive state estimation procedure is derived for a linear time varying system with both parametric uncertainties and stochastic measurement droppings. This estimator has a similar form as that of Kalman filter with intermittent observations, but its parameters should be adjusted when a plant output measurement arrives. A new recursive form is derived for the pseudo-covariance matrix of estimation errors. Based on a Riemannian metric for positive definite matrices, some necessary and sufficient conditions have been obtained for the strict contractiveness of this recursion. It has also been proved that under some controllability and observability conditions, as well as some weak requirements on measurement arrival probability, the update gain of this recursive robust state estimator and the mean of its squared estimation errors converge in probability one respectively to a corresponding stationary distribution. Numerical simulation results show that estimation accuracy of the suggested procedure is more robust against parametric modelling errors than Kalman filter.\"",
        "Document: \"On the Convergence and Stability of a Robust State Estimator. Convergence and stability of the robust state estimator obtained in is reinvestigated in this technical note. Some new relations have been established for matrix updates in the recursive state estimation. It is proved that under certain stabilizability and detectability conditions, this robust estimator converges to a stable time invariant system, provided that plant nominal parameters are time invariant and the filter design parameter is fixed. These results are consistent with existent ones, but different from them at the point that there are no orthogonality constraints on uncertainty related system matrices, and therefore widen theoretical guarantees for the effectiveness of the estimation procedure.\"",
        "1 is \"Two-dimensional loop shaping\", 2 is \"Site Initialization, Recovery, and Backup in a Distributed Database System\"",
        "Given above information, for an author who has written the paper with the title \"Adaptable, efficient, and modular coordination of distributed extended transactions\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006919": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Non-work-conserving effects in MapReduce: diffusion limit and criticality':",
        "Document: \"Benefits of high speed interconnects to cluster file systems: a case study with Lustre. Cluster file systems and storage area networks (SAN) make use of network IO to achieve higher IO bandwidth. Effective integration of networking mechanisms is important to their performance. In this paper, we perform an evaluation of a popular cluster file system, Lustre, over two of the leading high speed cluster interconnects: InfiniBand and Quadrics. Our evaluation is performed with both sequential IO and parallel IO benchmarks in order to explore the capacity of Lustre under different communication characteristics. Experimental results show that direct implementations of Lustre over both interconnects can improve its performance, compared to an IP emulation over InfiniBand (IPoIB). The performance of Lustre over Quadrics is comparable to that of Lustre over InfiniBand with the platforms we have. Latest InfiniBand products can embrace latest technologies, such as PCI-Express and DDR, and provide higher capacity. Our results show that over a Lustre file system with two object storage fervers (OSSs), InfiniBand with PCI-Express technology can improve Lustre write performance by 24%. Furthermore, our experimental results indicate that Lustre meta-data operations do not scale with an increasing number of OSSs, in spite of using high performance interconnects\"",
        "Document: \"Identifying Opportunities for Byte-Addressable Non-Volatile Memory in Extreme-Scale Scientific Applications. Future exascale systems face extreme power challenges. To improve power efficiency of future HPC systems, non-volatile memory (NVRAM) technologies are being investigated as potential alternatives to existing memories technologies. NVRAMs use extremely low power when in standby mode, and have other performance and scaling benefits. Although previous work has explored the integration of NVRAM into various architecture and system levels, an open question remains: do specific memory workload characteristics of scientific applications map well onto NVRAMs' features when used in a hybrid NVRAM-DRAM memory system? Furthermore, are there common classes of data structures used by scientific applications that should be frequently placed into NVRAM?In this paper, we analyze several mission-critical scientific applications in order to answer these questions. Specifically, we develop a binary instrumentation tool to statistically report memory access patterns in stack, heap, and global data. We carry out hardware simulation to study the impact of NVRAM for both memory power and system performance. Our study identifies many opportunities for using NVRAM for scientific applications. In two of our applications, 31% and 27% of the memory working sets are suitable for NVRAM. Our simulations suggest at least 27% possible power savings and reveal that the performance of some applications is insensitive to relatively long NVRAM write-access latencies.\"",
        "1 is \"Parallel I/O Performance for Application-Level Checkpointing on the Blue Gene/P System\", 2 is \"Mean value technique for closed fork-join networks\"",
        "Given above information, for an author who has written the paper with the title \"Non-work-conserving effects in MapReduce: diffusion limit and criticality\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006921": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A framework for software process deployment and evaluation':",
        "Document: \"EML Learning Flow Expressiveness Evaluation. Educational Modelling Languages such as IMS LD, LAMS and LPCEL can be used to describe a learning process, but they depict different degrees of expressiveness to represent learning flows. This paper presents an evaluation of the expressiveness of these languages based over the analysis of different workflow control patterns including basic control flow, advanced branching and synchronization, and iteration; outlining the learning process design issues.\"",
        "Document: \"A development environment to customize assessment through students interaction with multimodal applications. Learning experiences based on multimodal interactive applications are becoming common at all educational levels. Designing assessments for learning applications is often addressed through learning analytics. Multimodal interactive applications generate a large amount of data about students' interaction that can provide insights about their profile, behavior and performance. Unfortunately, this information is usually not accessible or difficult to collect from such applications, especially for teachers without computer programming skills. In this work, we present a visual development environment that supports the creation of multimodal interactive applications for learning with non-intrusive monitoring capacities, thus providing teachers the opportunity to create their own learning analytics tools even if they are not skilled in computer programming.\"",
        "1 is \"An Ontology-Driven Approach Applied to Information Security.\", 2 is \"The influence of system characteristics on e-learning use\"",
        "Given above information, for an author who has written the paper with the title \"A framework for software process deployment and evaluation\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006938": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Controller-Dynamic-Linearization-Based Data-Driven ILC for Nonlinear Discrete-Time Systems With RBFNN':",
        "Document: \"Distributed Fault Accommodation for a Class of Interconnected Nonlinear Systems With Partial Communication. Motivated by applications related to large-scale and complex systems, there has recently been an increased interest in the design and analysis of distributed control and fault accommodation schemes. This note presents a distributed fault detection and accommodation design method for a class of nonlinear interconnected subsystems. A fault may occur in the subsystems local dynamics as well as in the interconnections between the subsystems. The subsystems exchange state information according to a tracking error based communication protocol, where each subsystem transmits its state information only when its tracking error exceeds a certain threshold. The robustness of the distributed fault detection scheme and the closed-loop stability properties of the fault accommodation design are established through a rigorous analysis.\"",
        "Document: \"Contamination Detection In Drinking Water Distribution Systems Using Sensor Networks. Recent advances in low power networked embedded systems and electrochemical/optical water quality sensors have enabled the generation of low-cost, low-power water quality sensor nodes. These sensor nodes leverage the idea of wireless sensor networks for water quality monitoring in drinking water distribution systems, where large numbers of spatially distributed sensor nodes operate cooperatively to monitor and detect contamination events in a timely manner. However, such approach it is expected to suffer from false alarm errors and missed detection errors due to sensor inaccuracies and modeling uncertainties. This paper aims at developing algorithms for network-wide fusion for processing such inaccurate sensor data in order to minimize detection errors and improve contamination detection probability by correlating imperfect sensor decisions in space and time, taking advantage of the large scale deployment and the drinking water distribution system topology. A simulation example of a simple water distribution system is used to illustrate the proposed methodology and provide intuition through a comparative study.\"",
        "1 is \"DTD inference for views of XML data\", 2 is \"Ontology and SWRL-Based Learning Model for Home Automation Controlling\"",
        "Given above information, for an author who has written the paper with the title \"Controller-Dynamic-Linearization-Based Data-Driven ILC for Nonlinear Discrete-Time Systems With RBFNN\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006969": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Domain-Specific Preferences for Causal Reasoning and Planning.':",
        "Document: \"A general approach to belief change in answer set programming. We address the problem of belief change in (nonmonotonic) logic programming\nunder answer set semantics. Unlike previous approaches to belief change in\nlogic programming, our formal techniques are analogous to those of\ndistance-based belief revision in propositional logic. In developing our\nresults, we build upon the model theory of logic programs furnished by SE\nmodels. Since SE models provide a formal, monotonic characterisation of logic\nprograms, we can adapt techniques from the area of belief revision to belief\nchange in logic programs. We introduce methods for revising and merging logic\nprograms, respectively. For the former, we study both subset-based revision as\nwell as cardinality-based revision, and we show that they satisfy the majority\nof the AGM postulates for revision. For merging, we consider operators\nfollowing arbitration merging and IC merging, respectively. We also present\nencodings for computing the revision as well as the merging of logic programs\nwithin the same logic programming framework, giving rise to a direct\nimplementation of our approach in terms of off-the-shelf answer set solvers.\nThese encodings reflect in turn the fact that our change operators do not\nincrease the complexity of the base formalism.\"",
        "Document: \"Proof-complexity results for nonmonotonic reasoning. It is well-known that almost all nonmonotonic formalisms have a higher worst-case complexity than classical reasoning. In some sense, this observation denies one of the original motivations of nonmonotonic systems, which was the expectation taht nonmonotonic rules should help to speed-up the reasoning process, and not make it more difficult. In this paper, we look at this issue from a proof-theoretical perspective. We consider analytic calculi for certain nonmonotonic logis and analyze to what extent the presence of nonmonotonic rules can simplify the search for proofs. In particular, we show that there are classes of first-order formulae which have only extremely long \u201cclassical\u201d proofs, i.e., proofs without applications of nonmonotonic rules, but there are short proofs using nonmonotonic inferences. Hence,despite the increase of complexity in the worst case, there are instances where nonmonotonic reasoning can be much simpler than classical (cut-free) reasoning.\"",
        "1 is \"Practical Bayesian Optimization of Machine Learning Algorithms\", 2 is \"A Distributed Algorithm to Evaluate Quantified Boolean Formulae\"",
        "Given above information, for an author who has written the paper with the title \"Domain-Specific Preferences for Causal Reasoning and Planning.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006976": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'How Software Development Group Leaders Influence Team Members' Innovative Behavior.':",
        "Document: \"The Challenges Of Case Design Integration In The Telecommunication Application Domain. The magnitude of the problems facing the telecommunication software industry is presently at a point at which software engineers should become deeply involved. This paper presents a research project on advanced telecommunication technology carried out in Europe, called BOOST (Broadband Object-Oriented Service Technology). The project involved cooperative work among telecommunication companies, research centres and universities from several countries. The challenges to integrate CASE tools to support software development within the telecommunication application domain are discussed. A software process model that encourages component reusability, named the X model, is described as part of a software life cycle model for the telecommunication software industry.\"",
        "Document: \"Media Usage Survey: Overall Comparison of Faculty and Students.   Recent developments in the use of technologies in education have provided unique opportunities for teaching and learning. This paper describes the results of a survey conducted at Western University (Canada) in 2013, regarding the use of media by students and instructors. The results of this study support the assumption that the media usage of students and instructors include a mixture of traditional and new media. The main traditional media continue to be important, and some new media have emerged as seemingly on equal footing or even more important than the traditional forms of media. Some new media that have recently been in the public spotlight do not seem to be as important as expected. These new media may still be emerging but it is not possible to know their ultimate importance at this point. There was some variation in media usage across different Faculties but perhaps not as much variation as might have been expected. \"",
        "1 is \"Introducing knowledge redundancy practice in software development: Experiences with job rotation in support work\", 2 is \"FreeLing: An Open-Source Suite of Language Analyzers\"",
        "Given above information, for an author who has written the paper with the title \"How Software Development Group Leaders Influence Team Members' Innovative Behavior.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006997": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A new method for predicting essential proteins based on topology potential':",
        "Document: \"HybridDock: A Hybrid Protein-Ligand Docking Protocol Integrating Protein- and Ligand-Based Approaches. Structure-based molecular docking and ligand-based similarity search are two commonly used computational methods in computer-aided drug design. Structure-based docking tries to utilize the structural information on a drug target like protein, and ligand-based screening takes advantage of the information on known ligands for a target. Given their different advantages, it would be desirable to use both protein- and ligand-based approaches in drug discovery when information for both the protein and known ligands is available. Here, we have presented a general hybrid docking protocol, referred to as HybridDock, to utilize both the protein structures and known ligands by combining the molecular docking program MDock and the ligand-based similarity search method SHAFTS, and evaluated our hybrid docking protocol on the CSAR 2013 and 2014 exercises. The results showed that overall our hybrid docking protocol significantly improved the performance in both binding affinity and binding mode predictions, compared to the sole MDock program. The efficacy of the hybrid docking protocol was further confirmed using the combination of DOCK and SHAFTS, suggesting an alternative docking approach for modern drug design/discovery.\"",
        "Document: \"A two-step logistic regression algorithm for identifying individual-cancer-related genes. The identification of cancer-related genes is important towards the understanding of complex genetic diseases. Although many machine learning algorithms are proposed to identify disease-related genes, they often either have poor performance to identify locus heterogeneity cancer-related genes or are not applicable to predict individual-disease-related genes due to the lack of positive instances (imbalanced classification). To overcome these two issues, a two-step logistic regression (LR) based algorithm is proposed in this study for identifying individual-cancer-related genes. A set of high potential cancer-class-related genes is first generated in step 1, followed by a second round of LR-based algorithm conducted on this smaller dataset for identifying individual-cancer-related genes. Numerical experiments show that the proposed two-step LR-based algorithm not only works well for locus heterogeneity data, but also has good performance to handle the imbalanced classification problem. The individual-cancer-related gene identification experiments achieve AUC values of around 0.85 when the threshold of posterior probability is chosen between 0.3 and 0.6. All evaluations are conducted by using the leave-one-out cross validation method.\"",
        "1 is \"Real-time compressive tracking\", 2 is \"Accurate mass spectrometry based protein quantification via shared peptides.\"",
        "Given above information, for an author who has written the paper with the title \"A new method for predicting essential proteins based on topology potential\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007000": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Support environment for active rule design':",
        "Document: \"An Intelligent Database System Application: The Design of EMS. Intelligence in database systems is the capability of modeling both the complex structure of data and its dynamic evolution. The Chimera intelligent database prototype, designed in the context of the IDEA Esprit project, provides an object-oriented data model, deductive rules to express computations, and production rules to implement active reactions to database events. We present in this paper a high-level description of the main features of the Chimera conceptual interface, and we describe the design of the Energy Management System application in Chimera.\"",
        "Document: \"Array-Tree: A persistent data structure to compactly store frequent itemsets. Frequent itemset mining discovers correlations among data items in a transactional dataset. A huge amount of itemsets is often extracted, which is usually hard to process and analyze. The efficient management of the extracted frequent itemsets is still an open research issue. This paper presents a new persistent structure, the Array-Tree, that compactly stores frequent itemsets. It is an array-based structure exploiting both prefix-path sharing and subtree sharing to reduce data replication in the tree, thus increasing its compactness. The Array-Tree can be profitably exploited to efficiently query extracted itemsets by enforcing user-defined item or support constraints. Experiments performed on real and synthetic datasets show both the compactness of the Array-Tree data representation and its efficient support to user queries.\"",
        "1 is \"Gephi: An Open Source Software for Exploring and Manipulating Networks\", 2 is \"Minerals: using data mining to detect router misconfigurations\"",
        "Given above information, for an author who has written the paper with the title \"Support environment for active rule design\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007009": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'HPPC 2009 panel: are many-core computer vendors on track?':",
        "Document: \"Long Term Trends for Embedded System Design. An embedded system is an application specific electronic sub-system used in a larger system such as an appliance, an instrument or a vehicle. An embedded system is generally made of software (called embedded software) and a hardware platform. The evolution of technologies is enabling to the integration of complex platforms in a single chip (called System-on-Chip, SoC) including one or several CPU sub-systems to execute software and sophisticated interconnect in addition to specific hardware sub-systems. Mastering the design of these embedded systems is a challenge for both system and semiconductor houses that used to apply only software strategy or only hardware strategy. This paper analyzes this evolution and defines long term roadmaps for embedded system design.\"",
        "Document: \"Colif: a Multilevel Design Representation for Application-Specific Multiprocessor System-on-Chip Design. Abstract: Application-specific multiprocessor system-on-chip is required for high-volume future embedded systems. However, obtaining a good application-specific architecture could be an overly complex problem if we consider all the possible customizations. In this paper, we present Colif, a design representation that clearly separates component behavior and communication infrastructure. In addition, it has a flexible communication model that spans multiple abstraction levels. These features are suitable for a design flow where customizing communications and component behaviors at different abstraction levels are the central issue. The paper introduces the main concepts of Colif and compares it to existing system modeling approaches.\"",
        "1 is \"NoC Design and Implementation in 65nm Technology\", 2 is \"Efficient Simulations Between Concurrent-Read Concurrent-Write PRAM Models\"",
        "Given above information, for an author who has written the paper with the title \"HPPC 2009 panel: are many-core computer vendors on track?\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007035": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Plasma fusion code coupling using scalable I/O services and scientific workflows':",
        "Document: \"Towards energy-aware autonomic provisioning for virtualized environments. As energy efficiency and associated costs become key concerns, consolidated and virtualized data centers and clouds are attractive computing platforms for data- and compute-intensive applications. Recently, these platforms are also being considered for more traditional high-performance computing (HPC) applications. However, maximizing energy efficiency, cost-effectiveness, and utilization for these applications while ensuring performance and other Quality of Service (QoS) guarantees, requires leveraging important and extremely challenging tradeoffs. These include, for example, the tradeoff between the need to efficiently create and provision Virtual Machines (VMs) on data center resources and the need to accommodate the heterogeneous resource demands and runtimes of the applications that run on them. In this paper we propose an energy-aware online provisioning approach for HPC applications on consolidated and virtualized computing platforms. Energy efficiency is achieved using a workload-aware, just-right dynamic provisioning mechanism and the ability to power down subsystems of a host system that are not required by the VMs mapped to it. Our preliminary evaluations show that our approach can improve energy efficiency with an acceptable QoS penalty.\"",
        "Document: \"Adaptive QoS Management for Collaboration in Heterogeneous Environments. Adaptive Quality-of-Service management is critical for enabling effective collaboration between distributed clients in a heterogeneous (wired and wireless) environment. This is because both client profiles (capabilities, interests and resources) and system resources can be significantly different and highly dynamic. This paper presents the design and prototype implementation of an adaptive QoS management framework for collaborative multimedia applications in distributed, heterogeneous environments. The overall goal of the framework is to locally adapt the shared information to meet the capabilities, interests and current state of each collaborating client while preserving the semantic content of the information to maintain effective sharing. Transformations investigated in this paper include gradual gradations and modality transformations. The framework builds on a publisher-subscriber messaging substrate that uses semantic profiles and provides each client with direct and immediate access to all information defined by its needs and capabilities, without having to maintain and update global rosters. It interfaces with the Simple Network Management Protocol (SNMP) to determine the state of the network by directly querying network elements. An experimental evaluation of the framework for wired and wireless clients is also presented.\"",
        "1 is \"Autoflex: Service Agnostic Auto-scaling Framework for IaaS Deployment Models.\", 2 is \"The Spring scheduling co-processor: Design, use, and performance\"",
        "Given above information, for an author who has written the paper with the title \"Plasma fusion code coupling using scalable I/O services and scientific workflows\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007094": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Fault detection and isolation based on the combination of a bank of interval observers and invariant sets':",
        "Document: \"Passive robust fault detection using fuzzy parity equations. In this paper, a new approach for robust fault detection based on fuzzy parity equations is presented. The problem of robustness is handled using a passive approach based on generating a fuzzy adaptive threshold. The fuzzy adaptive threshold will be generated using fuzzy parity equations. Fuzzy parity equations are a generalisation of crisp parity equations but in this case the parameters in the used model are modelled using fuzzy parameters. Then, the residuals produced by fuzzy parity equations will also be fuzzy numbers. The extension from crisp parity equations to fuzzy parity equations is based on the extension principle of Zadeh [Inform. Contr. 8 (1965) 338; Inform. Sci. 89 (1975) 43]. Two possible crisp parity equations can be considered, according to Gertler [Fault Detection and Diagnosis in Engineering Systems, Marcel Dekker, New York, 1998] the moving average (MA) parity equation and autoregressive-moving average (ARMA) parity equations. When generalising these crisp parity equations to the fuzzy case, different problems appear when MA or ARMA parity equation approach is used. The underlying approach of fuzzy parity equations is the approach based on interval parity equations. Interval parity equations is an extension of crisp parity equations in the case of substituting crisp parameters in model equations by intervals. These intervals reflect the uncertainty on model parameters. When applying the extension principle of Zadeh [Inform. Contr. 8 (1965) 338; Inform. Sci. 89 (1975) 43] to extend crisp parity equations to fuzzy parity equations, a set of interval parity equations must be evaluated at different \u03b1-cuts. Interval parity equations have been extensively studied and applied in the literature, using two approaches: the simulation approach [Robust model-based fault diagnosis: the state of the art, in: Proceedings of the IFAC Symposium (SAFEPROCESS\u201994), 1994] and the prediction approach [IEEE Expert Intell. Syst. Appl. (1997)]. These two approaches can be deduced directly from crisp parity equations, in particular, from ARMA and MA approaches. Properties of interval parity equations (MA and ARMA) will be presented and compared. Finally, the proposed fuzzy parity equations will be tested on an example.\"",
        "Document: \"Observer gain effect in linear interval observer-based fault detection. In case of model uncertainty is located in parameters (interval model), an interval observer has been shown to be a suitable strategy to generate an adaptive threshold to be used in residual evaluation. In interval observer-based fault detection methods, the observer gain plays an important role since it determines the minimum detectable fault for a given type of fault and allows enhancing the observer fault detection properties while diminishing model computational drawbacks (i.e. wrapping effect, computational complexity). In this paper, the effect of the observer gain on the time evolution of the residual sensitivity to a fault is analyzed. Then, using these sensitivity studies, the minimum detectable fault time evolution is established. Thus, three types of faults according their detectability time evolution are introduced: permanently (strongly) detected, non-permanently (weakly) detected or just non-detected. Finally, an example based on a mineral grinding- classification process will be used to illustrate the results derived.\"",
        "1 is \"A Survey of Recent Results in Networked Control Systems\", 2 is \"On the accessibility/controllability of fuzzy control systems\"",
        "Given above information, for an author who has written the paper with the title \"Fault detection and isolation based on the combination of a bank of interval observers and invariant sets\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007114": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Real-Time Fast Channel Clustering for LiDAR Point Cloud':",
        "Document: \"On relay node placement and assignment for two-tiered wireless networks. Wireless networks that operate on batteries are imposed with energy constraints and long distance communications between nodes are not desirable. Implementing Relay Nodes (RNs) can improve network capacity and save communication energy. A two-hop relay routing scheme is considered, in which the RNs are temporarily placed and have energy constraints. This paper investigates a joint optimization problem on Relay Node Placement (RNP) and route assignment for two-tiered wireless networks. A recursive Weighted Clustering Binary Integer Programming (WCBIP) algorithm is proposed to maximize the total number of information packets received at the Base Station (BS) during the network lifetime. We first present an optimization algorithm based on Binary Integer Programming (BIP) for Relay Node Assignment (RNA) with the current node locations. Subsequently, a weighted clustering algorithm is applied to move the RNs to the best locations to best serve their respectively associated Edge Nodes (ENs). The algorithm has the complexity of O(2n). The simulation results show that the proposed algorithm has significantly better performance than the other two relay placement schemes. Both theoretical analysis and practical design procedures are also presented with details.\"",
        "Document: \"Accurate and Reliable Detection of Traffic Lights Using Multiclass Learning and Multiobject Tracking. Automatic detection of traffic lights has great importance to road safety. This paper presents a novel approach that combines computer vision and machine learning techniques for accurate detection and classification of different types of traffic lights, including green and red lights both in circular and arrow forms. Initially, color extraction and blob detection are employed to locate the candida...\"",
        "1 is \"Rigid-Motion Scattering for Texture Classification.\", 2 is \"Modeling Supporting Regions For Close Human Interaction Recognition\"",
        "Given above information, for an author who has written the paper with the title \"Real-Time Fast Channel Clustering for LiDAR Point Cloud\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007148": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Energy-efficient sensor selection for data quality and load balancing in wireless sensor networks':",
        "Document: \"Green Internet of Things for Smart World. Smart world is envisioned as an era in which objects (e.g., watches, mobile phones, computers, cars, buses, and trains) can automatically and intelligently serve people in a collaborative manner. Paving the way for smart world, Internet of Things (IoT) connects everything in the smart world. Motivated by achieving a sustainable smart world, this paper discusses various technologies and issues regarding green IoT, which further reduces the energy consumption of IoT. Particularly, an overview regarding IoT and green IoT is performed first. Then, the hot green information and communications technologies (ICTs) (e.g., green radio frequency identification, green wireless sensor network, green cloud computing, green machine to machine, and green data center) enabling green IoT are studied, and general green ICT principles are summarized. Furthermore, the latest developments and future vision about sensor cloud, which is a novel paradigm in green IoT, are reviewed and introduced, respectively. Finally, future research directions and open problems about green IoT are presented. Our work targets to be an enlightening and latest guidance for research with respect to green IoT and smart world.\"",
        "Document: \"Pricing Models for Sensor-Cloud. Incorporating ubiquitous wireless sensor networks (WSNs) and powerful cloud computing (CC), Sensor-Cloud (SC) is attracting growing attention from both academia and industry. However, pricing for SC is barely explored. In this paper, filling this gap, five SC pricing models (i.e., SCPM1, SCPM2, SCPM3, SCPM4 and SCPM5) are proposed first. Particularly, they charge a SC user, based on 1) the lease period of the user, 2) the required working time of SC, 3) the SC resources utilized by the user, 4) the volume of sensory data obtained by the user, 5) the SC path that transmits sensory data from the WSN to the user, respectively. Further, analysis is also presented to study and demonstrate the performance of the proposed SCPMs. We believe that the pricing designs and analysis performed in this work could be a very valuable guidance for future researches regarding pricing in SC.\"",
        "1 is \"Multi-authority attribute based encryption\", 2 is \"Generating a privacy footprint on the internet\"",
        "Given above information, for an author who has written the paper with the title \"Energy-efficient sensor selection for data quality and load balancing in wireless sensor networks\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007152": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Emerging Technologies Initiative \u2018Smart Grid Communications\u2019: Information Technology for Smart Utility Grids':",
        "Document: \"Optimizing Data Access For Wind Farm Control Over Hierarchical Communication Networks. We investigate a centralized wind farm controller which runs periodically. The controller attempts to reduce the damage a wind turbine sustains during operation by estimating fatigue based on the wind turbine state. The investigation focuses on the impact of information access and communication networks on the controller performance. We start by investigating the effects of a communication network that introduces delays in the information access for the central controller. The control performance as measured by accumulated fatigue is shown to be significantly impacted by communication delays and also by the choice of the time instances at which sensor information is accessed. In order to optimize the latter, we introduce an information quality metric and a mathematical model based on Markov chains, which are compared performance-wise to a heuristic approach for finding this parameter. This information quality metric is called mismatch probability, mmPr, and is used to express quantitatively the information accuracy in a given scenario. Lastly, measurements of different communication technologies have been performed in order to carry out the analysis in a practically relevant scenario with respect to the communication network delays. These measurements are done in regard to packet loss and communication delays, and the simulations are rerun using either the traces from the measurements or scenarios constructed from the delay parameters.\"",
        "Document: \"Dependability evaluation of a replication service for mobile applications in dynamic ad-hoc networks. In order to increase availability and reliability of stateful applications, redundancy as provided by replication in cluster solutions is a well-known and frequently utilized approach. For mobile services in dynamic ad-hoc networks, such replication mechanisms have to be adapted to deal with the frequently higher communication delays and with the intermittent connectivity. Dynamic clustering strategies in which the replica set is adjusted to the current network state can help to handle the network dynamicity. The paper develops a stochastic Petri net model (and its corresponding Markov chain representation) to analyze the resulting availability and replica consistency in such dynamic clusters. The numerical results are interpreted in the context of a vehicular (c2c) communication use-case and can be used to determine optimized cluster configuration parameters.\"",
        "1 is \"Impact of Position Errors on Path Loss Model Estimation for Device-to-Device Channels\", 2 is \"Performance evaluation of feasible and holistic CSH-MU handoff solution for seamless emergency service provisioning\"",
        "Given above information, for an author who has written the paper with the title \"Emerging Technologies Initiative \u2018Smart Grid Communications\u2019: Information Technology for Smart Utility Grids\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007198": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Rank identification for an analog ranked order filter':",
        "Document: \"Temperature behavior of combination selection based mismatch calibration with 65 nm CMOS technology. The temperature behaviour of a combination selection based mismatch calibration is discussed. The functionality of the calibration structure has already been presented. Clear benefits in implementation area and accuracy can be reached when using mismatch calibration based on combination selection of fine-tuning transistors. However, with the high accuracy requirements, the effects of temperature must be taken into the account. Temperature compensation circuitry for combination selection based mismatch calibration is developed, designed and simulated in digital 65 nm CMOS technology. The new temperature compensated and mismatch calibrated current source achieves 99% accuracy in 4\u00c2\u00bf confidence over the temperature range of 40 degrees in centigrade. This range can still be extended by recalibrating the current source in intervals of 20 degrees in centigrade.\"",
        "Document: \"On the Spatial Distribution of Local Non-parametric Facial Shape Descriptors. In this paper we present a method to form pattern specific facial shape descriptors called basis-images for non-parametric LBPs (Local Binary Patterns) and some other similar face descriptors such as Modified Census Transform (MCT) and LGBP (Local Gabor Binary Pattern). We examine the distribution of different local descriptors among the facial area from which some useful observations can be made. In addition, we test the discriminative power of the basis-images in a face detection framework for the basic LBPs. The detector is fast to train and uses only a set of strictly frontal faces as inputs, operating without non-faces and bootstrapping. The face detector performance is tested with the full CMU+MIT database.\"",
        "1 is \"Face detection with the modified census transform\", 2 is \"Interactive streaming of stored multiview video using redundant frame structures.\"",
        "Given above information, for an author who has written the paper with the title \"Rank identification for an analog ranked order filter\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007272": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A context adaptive bit-plane coder with maximum-likelihood-based stochastic bit-reshuffling technique for scalable video coding':",
        "Document: \"Edge Preserving Interpolation of Digital Images Using Fuzzy Inference. This paper presents a novel edge preserving interpolation method for digital images. This new method reduces drastically the blurring and jaggy artifacts at the high-contrast edges, which are generally found in the interpolated images using conventional methods. This high performance is achieved by two proposed operations: a fuzzy-inference based edge preserving interpolator and a highly oblique edge compensation scheme developed based on an edge orientation detector. The former synthesizes the interpolated pixels to match the image local characteristics. Hence, edge sharpness can be retained. However, due to the small footage of the fuzzy interpolation method, it cannot avoid edge jaggedness along the highly oblique edges that have very sharp angles against one of the coordinates. Therefore, a segment matching technique is developed to identify precisely the orientation of the highly oblique edges. Combining these two techniques, we improve significantly the visual quality of the interpolated images, particularly at the high-contrast edges. Both the synthesized images (such as letters) and the natural scenes (captured by camera) have been tested and the results are very promising.\"",
        "Document: \"Virtual view synthesis using backward depth warping algorithm. The virtual view synthesis reference software offered by the MPEG standard committee adopts the forward warping technique in projecting the depth map from the reference view to the target (virtual) view location. Often, this warping process results in many artifacts, holes and cracks, due to quantization errors and occlusion. In this study, we propose a backward warping process to replace the forward warping process, and the artifacts (particularly the ones produced by quantization) are significantly reduced. The subjective quality of the synthesized virtual view images is thus much improved.\"",
        "1 is \"Genetic watermarking based on transform-domain techniques\", 2 is \"A network-flow-based optimal sample preparation algorithm for digital microfluidic biochips\"",
        "Given above information, for an author who has written the paper with the title \"A context adaptive bit-plane coder with maximum-likelihood-based stochastic bit-reshuffling technique for scalable video coding\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007300": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A scalable null model for directed graphs matching all degree distributions: In, out, and reciprocal':",
        "Document: \"Link Prediction on Evolving Data Using Matrix and Tensor Factorizations. The data in many disciplines such as social networks, web analysis, etc. is link-based, and the link structure can be exploited for many different data mining tasks. In this paper, we consider the problem of temporal link prediction: Given link data for time periods 1 through T, can we predict the links in time period T + 1? Specifically, we look at bipartite graphs changing over time and consider matrix- and tensor-based methods for predicting links. We present a weight-based method for collapsing multi-year data into a single matrix. We show how the well-known Katz method for link prediction can be extended to bipartite graphs and, moreover, approximated in a scalable way using a truncated singular value decomposition. Using a CANDECOMP/PARAFAC tensor decomposition of the data, we illustrate the usefulness of exploiting the natural three-dimensional structure of temporal link data. Through several numerical experiments, we demonstrate that both matrix and tensor-based techniques are effective for temporal link prediction despite the inherent difficulty of the problem.\"",
        "Document: \"Tensor Decompositions and Applications. This survey provides an overview of higher-order tensor decompositions, their applications, and available software. A tensor is a multidimensional or $N$-way array. Decompositions of higher-order tensors (i.e., $N$-way arrays with $N \\geq 3$) have applications in psycho-metrics, chemometrics, signal processing, numerical linear algebra, computer vision, numerical analysis, data mining, neuroscience, graph analysis, and elsewhere. Two particular tensor decompositions can be considered to be higher-order extensions of the matrix singular value decomposition: CANDECOMP/PARAFAC (CP) decomposes a tensor as a sum of rank-one tensors, and the Tucker decomposition is a higher-order form of principal component analysis. There are many other tensor decompositions, including INDSCAL, PARAFAC2, CANDELINC, DEDICOM, and PARATUCK2 as well as nonnegative variants of all of the above. The N-way Toolbox, Tensor Toolbox, and Multilinear Engine are examples of software packages for working with tensors.\"",
        "1 is \"Automatically tuned linear algebra software\", 2 is \"An Estimator of Propagation of Cascading Failure\"",
        "Given above information, for an author who has written the paper with the title \"A scalable null model for directed graphs matching all degree distributions: In, out, and reciprocal\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007341": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Tag interoperability in cultural web-based applications':",
        "Document: \"CHIP Demonstrator: Semantics-Driven Recommendations and Museum Tour Generation. \n The main objective of the CHIP project is to demonstrate how Semantic Web technologies can be deployed to provide personalized\n access to digital museum collections. We illustrate our approach with the digital database ARIA of the Rijksmuseum Amsterdam.\n For the semantic enrichment of the Rijksmuseum ARIA database we collaborated with the CATCH STITCH project to produce mappings\n to Iconclass, and with the MultimediaN E-culture project to produce the RDF/OWL of the ARIA and Adlib databases. The main\n focus of CHIP is on exploring the potential of applying adaptation techniques to provide personalized experience for the museum\n visitors both on the Web site and in the museum.\n \n \"",
        "Document: \"The VU Sound Corpus: Adding More Fine-grained Annotations to the Freesound Database. This paper presents a collection of annotations (tags or keywords) for a set of 2,133 environmental sounds taken from the Freesound database (www.freesound.org). The annotations are acquired through an open-ended crowd-labeling task, in which participants were asked to provide keywords for each of three sounds. The main goal of this study is to find out (i) whether it's feasible to collect keywords for a large collection of sounds through crowdsourcing, and (ii) how people talk about sounds, and what information they can infer from hearing a sound in isolation. Our main finding is that it is not only feasible to perform crowd-labeling for a large collection of sounds, it is also very useful to highlight different aspects of the sounds that authors may fail to mention. Our data is freely available, and can be used to ground semantic models, improve search in audio databases, and to study the language of sound.\"",
        "1 is \"Social semantic query expansion\", 2 is \"FrameNet, current collaborations and future goals\"",
        "Given above information, for an author who has written the paper with the title \"Tag interoperability in cultural web-based applications\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007365": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Stochastic programming models for general redundancy-optimization problems':",
        "Document: \"Optimal contracts for the agency problem with multiple uncertain information. There is usually such a kind of agency problem where one principal authorizes one agent to perform more than one task at the same time. However, the potential output of each task cannot be exactly predicted in advance, so there exist simultaneously multiple types of uncertain information about the potential outputs of all the tasks. In this case, how to design the optimal contract and how to investigate the impacts of the diversity of uncertain information on such an optimal contract become important and challenging for decision makers. Motivated by this, to filter out the uncertainty in the possible incomes, we firstly focus on the optimal contract when both the two participators' potential incomes are measured by their respective expected incomes. Following that, as an important innovation, confidence level is introduced to quantify the degree of the agent's risk aversion, and the effects of the agent's attitude toward risk on the optimal contract and the principal's income are taken into account. Based on this view, two classes of uncertain agency models are developed, and then the sufficient and necessary conditions for the optimal contracts are presented with the detailed proofs and analyses. Compared with the traditional agency model, the innovations and advantages of the proposed work are briefly summarized, and the effectiveness of the work is further demonstrated by the computational results in a portfolio selection problem.\"",
        "Document: \"Random fuzzy alternating renewal processes. Random fuzzy theory offers an appropriate mechanism to model random fuzzy phenomena, with a random fuzzy variable defined as a function from a credibility space to a collection of random variables. Based on this theory, this paper presents the results of an investigation into the representation of properties of alternating renewal processes that are described by sequences of positive random fuzzy vectors. It provides a theorem on the limit value of the average chance of a given random fuzzy event in terms of \u201csystem being on at time t\u201d. The resultant model coincides with that attainable by stochastic analysis when the random fuzzy vectors degenerate to random vectors.\"",
        "1 is \"Mean-variance model for portfolio optimization problem in the simultaneous presence of random and uncertain returns.\", 2 is \"A Stock Model With Jumps For Uncertain Markets\"",
        "Given above information, for an author who has written the paper with the title \"Stochastic programming models for general redundancy-optimization problems\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007438": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A Joint Control-Communication Design for Reliable Vehicle Platooning in Hybrid Traffic.':",
        "Document: \"On optimal diversity in network-coding-based routing in wireless networks. Network coding (NC) based opportunistic routing has been well studied, but the impact of routing diversity on the performance of NC-based routing remains largely unexplored. Towards understanding the importance of routing diversity in NC-based routing, we study the problems of estimating and minimizing the data delivery cost in NC-based routing. In particular, we propose an analytical framework for estimating the total number of packet transmissions for NC-based routing in arbitrary topologies. We design a greedy algorithm that minimizes the total transmission cost of NC-based routing and determines the corresponding forwarder set for each node. We prove the optimality of this algorithm and show that 1) nodes on the shortest path may not always be favored when selecting forwarders for NC-based routing and 2)the minimal cost of NC-based routing is upper-bounded by the cost of shortest path routing. Based on the greedy, optimal algorithm, we design and implement ONCR, a distributed minimal cost NC-based routing protocol. Using the NetEye sensor testbed, we comparatively study the performance of ONCR and existing approaches such as the single path routing protocol CTP and the NC-based opportunistic routing protocols MORE and CodeOR. Results show that ONCR achieves close to 100% delivery reliability while having the lowest delivery cost among all the protocols and 25-28% less than the second best protocol CTP. This low delivery cost also enables ONCR to achieve the highest network goodput, i.e., about two-fold improvement over MORE and CodeOR. Our findings demonstrate the significance of optimizing data forwarding diversity in NC-based routing for data delivery reliability, efficiency, and goodput.\"",
        "Document: \"Mobility-Assisted Spatiotemporal Detection in Wireless Sensor Networks. Wireless sensor networks (WSNs) deployed for mission-critical applications face the fundamental challenge of meeting stringent spatiotemporal performance requirements using nodes with limited sensing capacity. Although advance network planning and dense node deployment may initially achieve the required performance, they often fail to adapt to the unpredictability of physical reality. This paper explores efficient use of mobile sensors to address the limitations of static WSNs in target detection. We propose a data fusion model that enables static and mobile sensors to effectively collaborate in target detection. An optimal sensor movement scheduling algorithm is developed to minimize the total moving distance of sensors while achieving a set of spatiotemporal performance requirements including high detection probability, low system false alarm rate and bounded detection delay. The effectiveness of our approach is validated by extensive simulations based on real data traces collected by 23 sensor nodes.\"",
        "1 is \"An efficient IBE scheme with tight security reduction in the random oracle model\", 2 is \"Decentralized Cognitive Radio Control Based on Inference from Primary Link Control Information\"",
        "Given above information, for an author who has written the paper with the title \"A Joint Control-Communication Design for Reliable Vehicle Platooning in Hybrid Traffic.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007467": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Top computational visualization R&D problems 2015: panel.':",
        "Document: \"Puzzle Generators and Symmetric Puzzle Layout. We present two abstract models of puzzles, permutation puzzles and cyclic puzzles, which can be modelled as puzzle graphs. Based on these models, we implement two puzzle generators and produce various layouts of the puzzles using graph drawing algorithms. Using these puzzle generators we can create new puzzles. Further, we can create new user interfaces of a puzzle by applying different layout algorithms. We implement a method for constructing symmetric layouts of puzzles, as symmetry is the most important aesthetic criteria for the puzzle layout.\"",
        "Document: \"Bounds on the crossing resolution of complete geometric graphs. The crossing resolution of a geometric graph is the minimum crossing angle at which any two edges cross each other. In this paper, we present upper and lower bounds to the crossing resolution of the complete geometric graphs.\"",
        "1 is \"Multi-dimensional sparse time series: feature extraction\", 2 is \"An open graph visualization system and its applications to software engineering\"",
        "Given above information, for an author who has written the paper with the title \"Top computational visualization R&D problems 2015: panel.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007484": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Retrieval, reuse, revision and retention in case-based reasoning':",
        "Document: \"Assessing Conceptual Similarity to Support Concept Mapping. Concept maps capture knowledge about the concepts and concept relationships in a domain, using a two-dimensional visually-based representation. Computer tools for concept mapping empower experts to directly construct, navigate, share, and criticize rich knowledge models. This paper de- scribes ongoing research on augmenting concept mapping tools with systems to support the user by proactively sug- gesting relevant concepts and associated resources (e.g., im- ages, video, and text pages) during concept map creation. Providing such support requires efficient and effective algo- rithms for judging concept similarity and the relevance of prior concepts to new concept maps. We discuss key issues for such algorithms and present four new approaches devel- oped for assessing conceptual similarity for concepts in con- cept maps. Two use precomputed summaries of structural and correlational information to determine the relevance of stored concepts to selected concepts in a new concept map, and two use information about the context in which the se- lected concept appears. We close by discussing their trade- offs and their relationships to research in areas such as in- formation retrieval and analogical reasoning.\"",
        "Document: \"An indexing vocabulary for case-based explanation. The success of case-based reasoning depends on effective retrieval of relevant prior cases. If retrieval is expensive, or if the cases retrieved are inappropriate, retrieval and adaptation costs will nullify many of the advantages of reasoning from prior experience. We propose an indexing vocabulary to facilitate retrieval of explanations in a casebased explanation system. The explanations we consider are explanations of anomalies (conflicts between new situations and prior expectations or beliefs). Our vocabulary groups anomalies according to the type of information used to generate the expectations or beliefs that failed, and according to how the expectations failed. We argue that by using this vocabulary to characterize anomalies, and retrieving explanations that were built to account for similarly-characterized past anomalies, a case-based explanation system can restrict retrieval to explanations likely to be relevant. In addition, the vocabulary can be used to organize general explanation strategies that suggest paths for explanation in novel situations.\"",
        "1 is \"Metaphors and heuristic-driven theory projection (HDTP)\", 2 is \"Density biased sampling: an improved method for data mining and clustering\"",
        "Given above information, for an author who has written the paper with the title \"Retrieval, reuse, revision and retention in case-based reasoning\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007549": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Competitions for Benchmarking: Task and Functionality Scoring Complete Performance Assessment':",
        "Document: \"Problems and solutions for anchoring in multi-robot applications. Robotic systems that carry out inferential activities over symbolic representations require a process that keeps a connection between physical objects and their symbolic image. Typically, this problem has been faced with ad-hoc solutions hardwired in the code. Recently, Coradeschi and Saffiotti have formalized this problem and they have called it anchoring. We propose a symbolic modelling approach to deal with the anchoring problem, in applications involving several embodied agents, by applying standard AI techniques. We discuss how such a modelling approach supports the process of instantiation of concepts by aggregating percepts possibly affected by imprecision and uncertainty. Percepts may come from several sensors possibly distributed both in the environment and on several mobile agents. Furthermore, we show how a tracking model can be used to maintain the link between percepts and conceptual instances in time. This approach to the anchoring problem has been implemented in a software module called MAP (MAP Anchors Perceptions), that has been tested in a robotic soccer application.\"",
        "Document: \"Head Impact Severity Measures for Small Social Robots Thrown During Meltdown in Autism. Social robots have gained a lot of attention recently as they have been reported to be effective in supporting therapeutic services for children with autism. However, children with autism may exhibit a multitude of challenging behaviors that could be harmful to themselves and to others around them. Furthermore, social robots are meant to be companions and to elicit certain social behaviors. Hence, the presence of a social robot during the occurrence of challenging behaviors might increase any potential harm. In this paper, we identified harmful scenarios that might emanate between a child and a social robot due to the manifestation of challenging behaviors. We then quantified the harm levels based on severity indices for one of the challenging behaviors (i.e. throwing of objects). Our results showed that the overall harm levels based on the selected severity indices are relatively low compared to their respective thresholds. However, our investigation of harm due to throwing of a small social robot to the head revealed that it could potentially cause tissue injuries, subconcussive or even concussive events in extreme cases. The existence of such behaviors must be accounted for and considered when developing interactive social robots to be deployed for children with autism.\"",
        "1 is \"Learning of closed-loop motion control\", 2 is \"On the Convergence of Stochastic Iterative Dynamic Programming Algorithms\"",
        "Given above information, for an author who has written the paper with the title \"Competitions for Benchmarking: Task and Functionality Scoring Complete Performance Assessment\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007590": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A CNN Accelerator on FPGA Using Depthwise Separable Convolution.':",
        "Document: \"Variational Inference Based Sparse Signal Detection for Next Generation Multiple Access. The next generation multiple access (NGMA) schemes are considered to support massive access for a large number of devices, which motivates us to develop a low-complexity approach for next generation systems. Since the generalized spatial modulation (SM) can be adopted to the system, a number of compressive sensing (CS) reconstruction algorithms are deployed for the detection of sparse signals, whi...\"",
        "Document: \"Collaborative relay beamforming based on the worst-case SINR for multiuser in cellular systems. Recently collaborative relay systems have been studied to increase the spectral efficiency and extend the range of wireless communications. In this paper, we consider the collaborative relay beamforming (CRBF) for multiuser systems. In the proposed method, we select base station (BS) antennas for each user and optimize their CRBF weights under two different types of power constraints, which are the total relay power constraint and individual relay power constraints, by exploiting the notion of the maximization of the worst-case received signal-to-interference-and-noise ratio (SINR). The optimization problem of the CRBF weights can be efficiently solved using a semidefinite relaxation (SDR) technique.We derive an iterative CRBF scheme to jointly select the BS antennas and optimize the CRBF weights to maximize the worst SINR among all users. Simulation results validate our theoretical analysis and demonstrate the tradeoffs between the number of BS antennas and the number of relays in the CRBF system.\"",
        "1 is \"VLSI cell placement techniques\", 2 is \"Performance of space-time codes for a large number of antennas\"",
        "Given above information, for an author who has written the paper with the title \"A CNN Accelerator on FPGA Using Depthwise Separable Convolution.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007639": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Parallel Niche Pareto Alineaga - An Evolutionary Multiobjective Approach On Multiple Sequence Alignment':",
        "Document: \"A multiobjective gravitational search algorithm applied to the static routing and wavelength assignment problem. One of the most favorable technology for exploiting the huge bandwidth of optical networks is known as Wavelength Division Multiplexing (WDM). Given a set of demands, the problem of setting up all connection requests is known as Routing and Wavelength Assignment (RWA) problem. In this work, we suggest the use of computational swarm intelligent for solving the RWA problem. A new heuristic based on the law of gravity and mass interactions (Gravitational Search Algorithm, GSA) is chosen for this purpose, but adapted to a multiobjective context (MO-GSA). To test the performance of theMO-GSA, we have used a real-world topology, the Nippon Telegraph and Telephone (NTT, Japan). network and six sets of demands. After performing several comparisons with other approaches published in the literature, we can conclude that this algorithm outperforms the results obtained by other authors.\"",
        "Document: \"A Parallel Genetic Programming Tool Based on PVM. This paper presents a software package suited for investigating Parallel Genetic Programming (PGP) utilizing Parallel Virtual Machine (PVM) language as means of communicating distributed populations. We show the usefulness of PVM by means of an example developed with this software tool. The example has been run on several processors in a parallel way.\"",
        "1 is \"Design-space exploration of the most widely used cryptography algorithms\", 2 is \"Self Adaptivity In Grid Computing\"",
        "Given above information, for an author who has written the paper with the title \"Parallel Niche Pareto Alineaga - An Evolutionary Multiobjective Approach On Multiple Sequence Alignment\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007655": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Resistance of orthogonal Gaussian fingerprints to collusion attacks':",
        "Document: \"Digital image source coder forensics via intrinsic fingerprints. Recent development in multimedia processing and network technologies has facilitated the distribution and sharing of multimedia through networks, and increased the security demands of multimedia contents. Traditional image content protection schemes use extrinsic approaches, such as watermarking or fingerprinting. However, under many circumstances, extrinsic content protection is not possible. Therefore, there is great interest in developing forensic tools via intrinsic fingerprints to solve these problems. Source coding is a common step of natural image acquisition, so in this paper, we focus on the fundamental research on digital image source coder forensics via intrinsic fingerprints. First, we investigate the unique intrinsic fingerprint of many popular image source encoders, including transform-based coding (both discrete cosine transform and discrete wavelet transform based), subband coding, differential image coding, and also block processing as the traces of evidence. Based on the intrinsic fingerprint of image source encoders, we construct an image source coding forensic detector that identifies which source encoder is applied, what the coding parameters are along with confidence measures of the result. Our simulation results show that the proposed system provides trustworthy performance: for most test cases, the probability of detecting the correct source encoder is over 90%.\"",
        "Document: \"Nonlinear collusion attacks on independent fingerprints for multimedia. Digital fingerprinting is a technology for tracing the distribution of multimedia content and protecting them from unauthorized redistribution. Collusion attack is a cost effective attack against digital fingerprinting where several copies with the same content but different fingerprints are combined to remove the original fingerprints. In this paper, we investigate average and nonlinear collusion attacks of independent Gaussian fingerprints and study both their effectiveness and the perceptual quality. We also propose the bounded Gaussian fingerprints to improve the perceptual quality of the fingerprinted copies. We further discuss the tradeoff between the robustness against collusion attacks and the perceptual quality of a fingerprinting system.\"",
        "1 is \"Cooperative multicast for maximum network lifetime\", 2 is \"Detecting and correcting malicious data in VANETs\"",
        "Given above information, for an author who has written the paper with the title \"Resistance of orthogonal Gaussian fingerprints to collusion attacks\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007662": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Immersive remote grasping: realtime gripper control by a heterogenous robot control system.':",
        "Document: \"Presence-enhancing real walking user interface for first-person video games. For most first-person video games it is important that players have a high level of feeling presence in the displayed game environment. Virtual reality (VR) technologies have enormous potential to enhance gameplay since players can experience the game immersively from the perspective of the player's virtual character. However, the VR technology itself, such as tracking devices and cabling, has until recently restricted the ability of users to really walk over long distances. In this paper we introduce a VR-based user interface for presence-enhancing gameplay with which players can explore the game environment in the most natural way, i. e., by real walking. While the player walks through the virtual game environment, we guide him/her on a physical path which is different from the virtual path and fits into the VR laboratory space. In order to further increase the VR experience, we introduce the concept of transitional environments. Such a transitional environment is a virtual replica of the laboratory environment, where the VR experience starts and which enables a gradual transition to the game environment. We have quantified how much humans can unknowingly be redirected and whether or not a gradual transition to a first-person game via a transitional environment increases the user's sense of presence.\"",
        "Document: \"Excuse me! Perception of Abrupt Direction Changes Using Body Cues and Paths on Mixed Reality Avatars. We evaluate abrupt turn signalling using Mixed Reality avatars with two methods: A method where the avatar signals using its body, and a method where a path is rendered on the floor. Results indicate that study participants prefer the body method but that the path method is more accurate when the path is longer.\n\n\"",
        "1 is \"A generation model to unify topic relevance and lexicon-based sentiment for opinion retrieval\", 2 is \"Effects of Field of View on Judgments of Self-Location: Distortions in Distance Estimations Even When the Image Geometry Exactly Fits the Field of View\"",
        "Given above information, for an author who has written the paper with the title \"Immersive remote grasping: realtime gripper control by a heterogenous robot control system.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007717": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Statistical machine learning makes automatic control practical for internet datacenters':",
        "Document: \"Teamscope: Scalable Team Evaluation via Automated Metric Mining for Communication, Organization, Execution, and Evolution. Teaching software development teams can be difficult to scale. Based on various cloud-based software development tools, Teamscope provides automated or semi-automated metrics to improve the scalability of a course with team projects. Metrics developed in Teamscope provide a synthesized view of a student team. Our preliminary results have shown the validity of these metrics. We also present a case study of applying metrics to teaching software development course in this paper.\"",
        "Document: \"Reducing WWW latency and bandwidth requirements by real-time distillation.  7. ReferencesLow Bandwidth Surfing in a High BandwidthWorldToday's WWW is beginning to burst at the seams due to lack of bandwidth from servers to clients. MostWWW pages are designed with high-bandwidth users in mind (i.e. 10 Mbit Ethernet) , yet a largepercentage of web clients are run over low-speed 28.8 or 14.4 modems, and users are increasinglyconsidering wireless services such as cellular modems running at 4800-9600 baud. A recent study by apopular server of shareware, Jumbo,... \"",
        "1 is \"R-MAT: A Recursive Model for Graph Mining\", 2 is \"Crowds in two seconds: enabling realtime crowd-powered interfaces\"",
        "Given above information, for an author who has written the paper with the title \"Statistical machine learning makes automatic control practical for internet datacenters\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007757": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Imagesemantics: User-Generated Metadata, Content Based Retrieval & Beyond':",
        "Document: \"An exploratory study on joint analysis of visual classification in narrow domains and the discriminative power of tags. The popularity of social media sharing sites such as Flickr has driven a significant amount of research on the analysis of information contained in the tags used to annotate images. Many of such tags are not useful to describe the contents of an image and are often labeled as not descriptive or even noisy. In this work we focus on the descriptiveness of a tag in an exploratory way, within a relatively narrow domain, and with the help of a visual classifier. Preliminary experimental results demonstrate the possibility to infer descriptiveness of tags from a joint analysis of tag entropy calculations and the results of an automated visual classifier with a limited number of classes without taking tag content or tag co-occurrence into account. We postulate that these experiments can be extended and improved toward a working solution that might answer the question: Given a semantic category, which tags would you use for searching an image from that category?\"",
        "Document: \"A closer look at photographers' intentions: a test dataset. Taking a photo is a process typically triggered by an intention. Some people want to document the progress of a task, others just want to capture the moment to re-visit the situation later on. In this contribution we present a novel, openly available dataset with 1,309 photos and annotations specifying the intentions of the photographers, which were eventually validated using Amazon Mechanical Turk.\"",
        "1 is \"G-COPSS: A Content Centric Communication Infrastructure for Gaming Applications\", 2 is \"Understanding and Using Context\"",
        "Given above information, for an author who has written the paper with the title \"Imagesemantics: User-Generated Metadata, Content Based Retrieval & Beyond\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007787": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'On asymptotic capacity of coordinated multi-point MIMO channels':",
        "Document: \"BER analysis of the optimum multiuser detection with channel mismatch in MC-CDMA systems. In this paper, we analyze the bit-error-rate (BER) performance of the optimum multiuser detection (MUD) with channel mismatch in multicarrier code-division-multiple-access (MC-CDMA) systems. The BER performance of the optimum MUD without channel mismatch in MC-CDMA systems has been recently derived using the replica method. However, it is left unjustified, since the replica method is not a rigorous approach. In addition, it is NP-hard to implement an optimum MUD algorithm. To justify the BER performance and to make the optimum MUD feasible, based on Pearl's belief propagation (BP) scheme, we put together a low-complexity iterative MUD algorithm for MC-CDMA systems. Furthermore, channel mismatch is introduced into the BP-based MUD algorithm to make the scenario general. With channel mismatch, the analytical results of the BP-based MUD algorithm conform perfectly to, and the simulation results of the BP-based MUD algorithm conform very closely to the BER performance of the optimum MUD derived using the replica method, which is a nontrivial extension of the existing replica approach mentioned above. Without channel mismatch, the problem becomes a special case of our contribution.\"",
        "Document: \"A Novel Strategy Using Factor Graphs And The Sum-Product Algorithm For Satellite Broadcast Scheduling Problems. This paper presents a low complexity algorithmic framework for finding a broadcasting schedule in a low-altitude satellite system, i.e., the satellite broadcast scheduling (SBS) problem, based on the recent modeling and computational methodology of factor graphs. Inspired by the huge success of the low density parity check (LDPC) codes in the field of error control coding, in this paper, we transform the SBS problem into an LDPC-like problem through a factor graph instead of using the conventional neural network approaches to solve the SBS problem. Based on a factor graph framework, the soft-information, describing the probability that each satellite will broadcast information to a terminal at a specific time slot, is exchanged among the local processing in the proposed framework via the sum-product algorithm to iteratively optimize the satellite broadcasting schedule. Numerical results show that the proposed approach not only can obtain optimal solution but also enjoys the low complexity suitable for integral-circuit implementation.\"",
        "1 is \"Asymptotic Capacity and Optimal Precoding in MIMO Multi-Hop Relay Networks\", 2 is \"Density evolution for two improved BP-Based decoding algorithms of LDPC codes\"",
        "Given above information, for an author who has written the paper with the title \"On asymptotic capacity of coordinated multi-point MIMO channels\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007860": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Geometric Occlusion Analysis in Depth Estimation using Integral Guided Filter for Light-Field Image.':",
        "Document: \"Camera Calibration from Two Shadow Trajectories. We introduce an efficient method for recovering the camera parameters automatically from the cast shadows of two 3D points observed over time. Compared to previous related work, our method has less restrictions in the sense that object-to-shadow correspondences do not have to be available in the image. We demonstrate how the horizon line may be recovered from only shadow points, and how the camera intrinsic and extrinsic parameters are determined using the pole-polar relationship and minimizing the algebraic distance of the principal point. The approach is fully validated on both synthetic and real data, and tested against various sources of error. We finally present an application to metrology from shadows only - i.e. when the object is not visible in the image.\"",
        "Document: \"Action recognition using subtensor constraint. Human action recognition from videos draws tremendous interest in the past many years. In this work, we first find that the trifocal tensor resides in a twelve dimensional subspace of the original space if the first two views are already matched and the fundamental matrix between them is known, which we refer to as subtensor. Then we use the subtensor to perform the task of action recognition under three views. We find that treating the two template views separately or not considering the correspondence relation already known between the first two views omits a lot of useful information. Experiments and datasets are designed to demonstrate the effectiveness and improved performance of the proposed approach.\"",
        "1 is \"Bubble rap: social-based forwarding in delay tolerant networks\", 2 is \"Exposing digital forgeries through chromatic aberration\"",
        "Given above information, for an author who has written the paper with the title \"Geometric Occlusion Analysis in Depth Estimation using Integral Guided Filter for Light-Field Image.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007872": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'An Enhanced Approach To Improve Enterprise Competency Management':",
        "Document: \"OWL-FC: an upper ontology for semantic modeling of Fuzzy Control. This work introduces an OWL-based upper ontology, called OWL-FC (Ontology Web Language for Fuzzy Control), capable to support a semantic definition of Fuzzy Control. It focuses on the fuzzy rules representation by providing domain independent ontology, supporting interoperability and favoring domain ontologies re-usability. The main contribution is that OWL-FC exploits Fuzzy Logic in OWL to model vagueness and uncertainty of the real world. Moreover, OWL-FC enables automatic discovery and execution of fuzzy controllers, by means of context aware parameter setting: appropriate controllers can be activated, depending on the parameters proactively identified in the work environment. In fact, the semantic modeling of concepts allows the characterization of constraints and restrictions for the identification of the right matches between concepts and individuals. OWL-FC ontology provides a wide, semantic-based interoperability among different domain ontologies, through the specification of fuzzy concepts, independently by the application domain. Then, OWL-FC is coherent to the Semantic Web infrastructure and avoids inconsistencies in the ontology.\"",
        "Document: \"Spatiotemporal hotspots analysis for exploring the evolution of diseases: an application to oto-laryngopharyngeal diseases. This paper presents a spatiotemporal analysis of hotspot areas based on the Extended Fuzzy C-Means method implemented in a geographic information system. This method has been adapted for detecting spatial areas with high concentrations of events and tested to study their temporal evolution. The data consist of georeferenced patterns corresponding to the residence of patients in the district of Naples (Italy) to whom a surgical intervention to the oto-laryngopharyngeal apparatus was carried out between the years 2008 and 2012.\"",
        "1 is \"A survey of trust in computer science and the Semantic Web\", 2 is \"An architecture for dynamic security perimeters of virtual collaborative networks\"",
        "Given above information, for an author who has written the paper with the title \"An Enhanced Approach To Improve Enterprise Competency Management\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007877": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Dynamic Multipoint Relay Candidate Selection For Broadcast Data Aggregation In Mobile Ad-Hoc Networks':",
        "Document: \"Autonomous transmission power control for CSMA/CA-based wireless networks. In this paper, an autonomous transmission power control scheme to enhance spatial reuse performance for CSMA/CA-based wireless networks is proposed. For practical purpose, the proposed scheme is designed based on the log-normal shadowing model, unlike other previous works which were built under the two-ray ground reflection model. Simulation results show that the proposed scheme outperforms the other previous power control schemes.\"",
        "Document: \"Analysis of packet loss for real-time traffic in wireless mobile networks with ARQ feedback. In this paper, the provision of quality-of-service (QoS) for real-time traffic over a wireless channel deploying automatic repeat request (ARQ) error control is investigated. By introducing the concepts of ARQ capacity and effective capacity, an analytic model has been derived to evaluate the loss probabilities in both the network layer and the physical layer. In contrast to the previous results, this model quantifies the interaction between the network and physical layers. As shown by the simulation experiments, our analysis can predict the real metrics under a wide range of conditions. This enables the call admission controller in wireless networks to control and optimize traffic QoS using instantaneous channel status information.\"",
        "1 is \"Error performance analysis of signal superposition coded cooperative diversity\", 2 is \"Comparison of Opportunistic Spectrum Multichannel Medium Access Control Protocols\"",
        "Given above information, for an author who has written the paper with the title \"Dynamic Multipoint Relay Candidate Selection For Broadcast Data Aggregation In Mobile Ad-Hoc Networks\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007919": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Video Popularity Prediction by Sentiment Propagation via Implicit Network':",
        "Document: \"Visualizing timelines: evolutionary summarization via iterative reinforcement between text and image streams. We present a novel graph-based framework for timeline summarization, the task of creating different summaries for different timestamps but for the same topic. Our work extends timeline summarization to a multimodal setting and creates timelines that are both textual and visual. Our approach exploits the fact that news documents are often accompanied by pictures and the two share some common content. Our model optimizes local summary creation and global timeline generation jointly following an iterative approach based on mutual reinforcement and co-ranking. In our algorithm, individual summaries are generated by taking into account the mutual dependencies between sentences and images, and are iteratively refined by considering how they contribute to the global timeline and its coherence. Experiments on real-world datasets show that the timelines produced by our model outperform several competitive baselines both in terms of ROUGE and when assessed by human evaluators.\"",
        "Document: \"Evolutionary timeline summarization: a balanced optimization framework via iterative substitution. Classic news summarization plays an important role with the exponential document growth on the Web. Many approaches are proposed to generate summaries but seldom simultaneously consider evolutionary characteristics of news plus to traditional summary elements. Therefore, we present a novel framework for the web mining problem named Evolutionary Timeline Summarization (ETS). Given the massive collection of time-stamped web documents related to a general news query, ETS aims to return the evolution trajectory along the timeline, consisting of individual but correlated summaries of each date, emphasizing relevance, coverage, coherence and cross-date diversity. ETS greatly facilitates fast news browsing and knowledge comprehension and hence is a necessity. We formally formulate the task as an optimization problem via iterative substitution from a set of sentences to a subset of sentences that satisfies the above requirements, balancing coherence/diversity measurement and local/global summary quality. The optimized substitution is iteratively conducted by incorporating several constraints until convergence. We develop experimental systems to evaluate on 6 instinctively different datasets which amount to 10251 documents. Performance comparisons between different system-generated timelines and manually created ones by human editors demonstrate the effectiveness of our proposed framework in terms of ROUGE metrics.\"",
        "1 is \"Protein complex identification by supervised graph local clustering.\", 2 is \"A tutorial on support vector regression\"",
        "Given above information, for an author who has written the paper with the title \"Video Popularity Prediction by Sentiment Propagation via Implicit Network\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007924": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Delay-dependent robust asymptotic state estimation of Takagi-Sugeno fuzzy Hopfield neural networks with mixed interval time-varying delays':",
        "Document: \"Direct delay decomposition approach to synchronization of chaotic fuzzy cellular neural networks with discrete, unbounded distributed delays and Markovian jumping parameters. In this paper, the problem of synchronization of chaotic fuzzy cellular neural networks (FCNNs) with discrete, unbounded distributed delays and Markovian jumping parameters (MJPs) is investigated. Sufficient delay-dependent stability criteria are obtained in terms of linear matrix inequalities (LMIs) to ensure the chaotic delayed FCNNs to be stochastic asymptotically synchronous with the help of free-weighting matrix and some inequality techniques. The information of the delayed plant states can be taken into full consideration. Here, the delay interval is decomposed into two subintervals by using the tuning parameter such that 0 < < 1 . By developing a delay decomposition approach and constructing suitable Lyapunov-Krasovskii functional (LKF), sufficient conditions for synchronization are established for each subinterval. Numerical example and its simulations are provided to demonstrate the effectiveness and less conservatism of the derived results.\"",
        "Document: \"Optimal control for linear singular system using genetic programming. In this paper, optimal control for linear singular system with quadratic performance is obtained using genetic programming (GP). The goal is to provide optimal control with reduced calculus effort by comparing the solutions of the matrix Riccati differential equation (MRDE), obtained from well known traditional Runge-Kutta (RK) method and genetic programming method. To obtain the optimal control, the solution of MRDE is computed based on grammatical evolution. Accuracy of the solution of the GP approach to the problem is qualitatively better. An illustrative numerical example is presented for the proposed method.\"",
        "1 is \"LMI-based stability analysis of impulsive high-order Hopfield-type neural networks\", 2 is \"Observer-based control for fractional-order continuous-time systems\"",
        "Given above information, for an author who has written the paper with the title \"Delay-dependent robust asymptotic state estimation of Takagi-Sugeno fuzzy Hopfield neural networks with mixed interval time-varying delays\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007928": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Efficient heuristics for energy-aware routing in networks with bundled links.':",
        "Document: \"TCP Over the IEEE 802.15.3 MAC: Analysis and Simulation. High data rate wireless technologies are now becoming a reality, and have spurred the development of new applications that\n were previously hindered by the lack of capacity. In particular, it is now possible to stream high definition movies from\n a laptop to a sound system and flat screen television seamlessly. One of the key enablers of such applications is the IEEE\n 802.15.3 medium access control (MAC) protocol, which is designed to support bandwidth intensive applications in wireless personal\n area networks (WPANs). A key observation is that a significant number of multi-media applications rely on the transmission\n control protocol (TCP). Unfortunately, little works have conducted a thorough performance study of TCP over the IEEE 802.15.3\n MAC. Moreover, the IEEE 802.15.3 specification does not specify any strategies for allocating time slots. This paper therefore\n contributes to the current state-of-the-art in the following manners. From our extensive analytical and simulation studies,\n we reveal the impacts of different channel time allocation methods and acknowledgment policies on the performance of TCP,\n with particular attention on round trip time, congestion window growth, and packet recovery. We then present the following\n guidance to application developers: (1) channel time allocations (CTAs) should be distributed evenly over the superframe and\n have durations determined by TCP\u2019s maximum congestion window, (2) CTA positioning has no impact on TCP\u2019s performance, and\n (3) the specified delayed acknowledgment policy needs to be augmented with an adaptive algorithm that adjusts its burst size\n dynamically to varying bit error rate (BER).\"",
        "Document: \"Power-aware routing in networks with delay and link utilization constraints. This paper addresses the NP-hard problem of switching off bundled links whilst retaining the QoS provided to existing applications. We propose a fast heuristic, called Multiple Paths by Shortest Path First (MSPF), and evaluated its performance against two state-of-the-art techniques: GreenTE, and FGH. MSPF improves the energy saving on average by 5% as compared to GreenTE with only 1% CPU time. While yielding equivalent energy savings, MSPF requires only 0.35% of the running time of FGH. Finally, for Maximum Link Utilization (MLU) below 50% and delay no longer than the network diameter, MSPF reduces the power usage of the GE\u0301ANT topology by up to 91%.\"",
        "1 is \"An efficient genetic algorithm for maximum coverage deployment in wireless sensor networks.\", 2 is \"Three-dimensional shape analysis using moments and Fourier descriptors\"",
        "Given above information, for an author who has written the paper with the title \"Efficient heuristics for energy-aware routing in networks with bundled links.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008086": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Group decision analysis based on fuzzy preference relations: Logarithmic and geometric least squares methods':",
        "Document: \"Risk decision analysis in emergency response: A method based on cumulative prospect theory. Emergency response of a disaster is generally a risk decision-making problem with multiple states. In emergency response analysis, it is necessary to consider decision-maker's (DM's) psychological behavior such as reference dependence, loss aversion and judgmental distortion, whereas DM's behavior is neglected in the existing studies on emergency response. In this paper, a risk decision analysis method based on cumulative prospect theory (CPT) is proposed to solve the risk decision-making problem in emergency response. The formulation and solution procedure of the studied emergency response problem are given. Then, according to CPT, the values of potential response results concerning each criterion are calculated. Consider the interdependence or conflict among criteria, Choquet integral is used to determine the values of each potential response result. Accordingly, the weights of probabilities of all potential response results are calculated. Furthermore, by aggregating the values and weights of response results, the prospect value of each response action (alternative) is determined, and overall prospect value of each response action is obtained by aggregating the prospect value and the cost of each action. According to the obtained overall prospect values, a ranking of all response actions can be determined. Finally, based on the background of emergency evacuation from barrier lake downstream villages, an example is given to illustrate the feasibility and validity of the proposed method.\"",
        "Document: \"Analysis of Joint Provision of Public Goods in Online Communities. Online community (OC, or termed as \"virtual community\") provides a social sphere for people to share their information, knowledge and other resources. It is becoming an unprecedentedly ubiquitous online phenomenon. The contents which are jointly provided by participants are considered as public goods (Kollock, 1999). For public goods provided by collective action, there have been many papers since Olson (1965) and Hardin (1968). The major concern and conclusion are that enough public goods cannot be provided by rational individuals in collective action. This argument is also applicable to OC because OC will collapse if everyone shirks and free rides others. In the attention economy, however, information overloading becomes a major concern as well, and in response many search and recommendation technologies and services were developed in the Internet including OC. In this case, it seems that \"too much\" rather than \"not enough\" public goods are provided in OC. Surprisingly, phenomenon indicates that both \"not enough\" and \"too much\" are possible, which contradicts the conclusion of traditional literatures. Hence, this study aims to address why both possibilities exist in OC. How are different results realized by interaction behaviors among participants? Theoretically, we addressed where the optimal level of provision and community size lie. Accordingly, possible managerial strategies are discussed and suggested.\"",
        "1 is \"Fuzzy MADM: An outranking method\", 2 is \"A two-phased semantic optimization modeling approach on supplier selection in eProcurement\"",
        "Given above information, for an author who has written the paper with the title \"Group decision analysis based on fuzzy preference relations: Logarithmic and geometric least squares methods\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008175": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Local search algorithms for partial MAXSAT':",
        "Document: \"Visualizations and Analyses of Elementary School Students' Actions on Squeak Environments. With the spread of computers and networks into schools,it becomes more and more important to make them usefulfor education. We are currently working on the projectof holding Squeak workshops in schools. In these workshops,students try to make their original works, but thereis no method to see the building process of their works andshare their ideas. In this paper, we propose an action historymodel for managing user operations on SqueakToys. In themodel, the object manipulation events for the SqueakToysapplication level are captured and stored with the associationsto mouse and keyboard events for the operating systemlevel, so that we can focus on the important part of thebuilding process. We have also designed the playback functionof action histories at varying speed, as a visualizationof action histories. In addition, analysis on the action historiesand its utilization are discussed.\"",
        "Document: \"Global concurrency control mechanisms for a local network consisting of systems without concurrency control capability. A powerful and expandable system can be economically realized by a local computer network consisting of various kinds of microprocessor-based systems. The following three problems must be solved to organize a distributed processing system using nonidentical elements: (1) communication, (2) query conversion, and (3) global concurrency control. Except in the case when all transactions are read-only ones, (3) must be handled. Since each system in a network does not usually have concurrency control capability or may not use the identical mechanism, it is necessary to develop a global concurrency control mechanism for a local network consisting of systems without such capability. In this paper two such mechanisms are presented. By assigning ordered numbers to the component systems, a consistent and deadlock-free global mechanism is realized for a semijoin-based query procedure. To improve efficiency, a mechanism permitting dynamic modification capability of ordering is also presented.\"",
        "1 is \"On constructing radiation hybrid maps (extended abstract)\", 2 is \"Consistency in Hierarchical Database Systems\"",
        "Given above information, for an author who has written the paper with the title \"Local search algorithms for partial MAXSAT\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008240": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Building and Using A Scalable Display Wall System':",
        "Document: \"W2ANE: when words are not enough: online multimedia language assistant for people with aphasia. In this paper, we introduce W2ANE, an Online Multimedia Language Assistant for individuals with aphasia, a language disorder that affects millions of people. W2ANE offers a rich online multimedia library (OMLA) supported by an adaptable and adaptive vocabulary scaffold (ViVA). The system, accessible over the Internet, provides a platform for applications such as looking up unknown words, constructing phrases for communication, practicing pronunciations, and accessing content. W2ANE also enables resource sharing and remote collaboration.\"",
        "Document: \"Building collaborative graphical interfaces in the audicle. Emergence is the formation of complex patterns from simpler rules or systems. This paper motivates and describes new graphical interfaces for controlling sound designed for strongly-timed, collaborative computer music ensembles. While the interfaces are themselves minimal and often limiting, the overall collaboration can produce results novel beyond the simple sum of the components -- leveraging the very uniqueness of an ensemble: its strength in numbers. The interfaces are human-controlled and machine-synchronized across a dozen or more computers. Group control, as well as sound synthesis mapping at each endpoint, can be programmed quickly and even on-the-fly, providing a second channel of real-time control. We show examples of these interfaces as interchangeable plug-ins for the Audicle environment, and also document how they are used in a laptop ensemble.\"",
        "1 is \"Bimodal expression of emotion by face and voice\", 2 is \"Weak ordering\u2014a new definition\"",
        "Given above information, for an author who has written the paper with the title \"Building and Using A Scalable Display Wall System\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008266": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Gauss meets Canadian traveler: shortest-path problems with correlated natural dynamics':",
        "Document: \"Multimodal affect recognition in learning environments. We propose a multi-sensor affect recognition system and evaluate it on the challenging task of classifying interest (or disinterest) in children trying to solve an educational puzzle on the computer. The multimodal sensory information from facial expressions and postural shifts of the learner is combined with information about the learner's activity on the computer. We propose a unified approach, based on a mixture of Gaussian Processes, for achieving sensor fusion under the problematic conditions of missing channels and noisy labels. This approach generates separate class labels corresponding to each individual modality. The final classification is based upon a hidden random variable, which probabilistically combines the sensors. The multimodal Gaussian Process approach achieves accuracy of over 86%, significantly outperforming classification using the individual modalities, and several other combination schemes.\"",
        "Document: \"On Recovering Structure of Affect. This paper presents novel human computation experiments geared towards uncovering the structure of affect. Using Mechanical Turk workers across 2 separate studies, we empirically verified some of the popular beliefs about the structure of affect, but also provide some new evidence. We replicate and reveal not only the statistical structure of the dimensions of affect, but also the effect of cultural influences. We close with a proposition for a framework for doing this kind of large scale research and provide recommendations and opportunities for innovations in research around emotional theory.\"",
        "1 is \"Conversational Speech Transcription Using Context-Dependent Deep Neural Networks\", 2 is \"Background subtraction using low rank and group sparsity constraints\"",
        "Given above information, for an author who has written the paper with the title \"Gauss meets Canadian traveler: shortest-path problems with correlated natural dynamics\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008335": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Design and implementation of a belief-propagation scheduler for multicast traffic in input-queued switches.':",
        "Document: \"Gated asymptotic modEls (GAMEs): a new tool for the stability analysis of queueing systems. No abstract available.\n\n\"",
        "Document: \"Design and implementation of a belief-propagation scheduler for multicast traffic in input-queued switches. Scheduling multicast traffic in input-queued switches to maximize throughput requires solving a hard combinatorial optimization problem in a very short time. This task advocates the design of algorithms that are simple to implement and efficient in terms of performance. We propose a new scheduling algorithm, based on message passing and inspired by the belief propagation paradigm, meant to approximate the provably-optimal scheduling policy for multicast traffic. We design and implement both a software and a hardware version of the algorithm, the latter running on a NetFPGA. We compare the performance and the power consumption of the two versions when integrated in a software router. Our main findings are that our algorithm outperforms other centralized greedy scheduling policies, achieving a better tradeoff between complexity and performance, and it is amenable to practical high-performance implementations.\"",
        "1 is \"Scheduling nonuniform traffic in a packet-switching system with small propagation delay\", 2 is \"Content-Aware Caching And Traffic Management In Content Distribution Networks\"",
        "Given above information, for an author who has written the paper with the title \"Design and implementation of a belief-propagation scheduler for multicast traffic in input-queued switches.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008357": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A Parallel Yet Pipelined Architecture for Efficient Implementation of the Advanced Encryption Standard Algorithm on Reconfigurable Hardware.':",
        "Document: \"SODA: software defined FPGA based accelerators for big data. FPGA has been an emerging field in novel big data architectures and systems, due to its high efficiency and low power consumption. It enables the researchers to deploy massive accelerators within one single chip. In this paper, we present a software defined FPGA based accelerators for big data, named SODA, which could reconstruct and reorganize the acceleration engines according to the requirement of the various data-intensive applications. SODA decomposes large and complex applications into coarse grained single-purpose RTL code libraries that perform specialized tasks in out-of-order hardware. We built a prototyping system with constrained shortest path Finding (CSPF) case studies to evaluate SODA framework. SODA is able to achieve up to 43.75X speedup at 128 node application. Furthermore, hardware cost of the SODA framework demonstrates that it can achieve high speedup with moderate hardware utilization.\"",
        "Document: \"Service-Oriented Architecture on FPGA-Based MPSoC. The integration of software services-oriented architecture (SOA) and hardware multiprocessor system-on-chip (MPSoC) has been pursued for several years. However, designing and implementing a service-oriented system for diverse applications on a single chip has posed significant challenges due to the heterogeneous architectures, programming interfaces, and software tool chains. To solve the problem,...\"",
        "1 is \"Cryptanalysis of block-wise stream ciphers suitable for\u00a0the\u00a0protection of multimedia and ubiquitous systems\", 2 is \"Optimal paths in weighted timed automata\"",
        "Given above information, for an author who has written the paper with the title \"A Parallel Yet Pipelined Architecture for Efficient Implementation of the Advanced Encryption Standard Algorithm on Reconfigurable Hardware.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008431": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Improved Trellis-Based Algorithm for Locating and Breaking Cycles in Bipartite Graphs with Applications to LDPC Codes':",
        "Document: \"A class of low-density parity-check codes constructed based on Reed-Solomon codes with two information symbols. This paper presents an algebraic method for constructing regular low-density parity-check (LDPC) codes based on Reed-Solomon codes with two information symbols. The construction method results in a class of LDPC codes in Gallager's original form. Codes in this class are free of cycles of length 4 in their Tanner graphs and have good minimum distances. They perform well with iterative decoding.\"",
        "Document: \"On primitive BCH codes with unequal error protection capabilities. Presents a class of binary primitive BCH codes that have unequal-error-protection (UEP) capabilities. The authors use a previous result on the span of their minimum weight vectors to show that binary primitive BCH codes, containing second-order punctured Reed-Muller (RM) codes of the same minimum distance, are binary-cyclic UEP codes. The values of the error correction levels for this class of binary LUEP codes are estimated\"",
        "1 is \"Non-Binary Protograph-Based LDPC Codes: Enumerators, Analysis, and Designs\", 2 is \"Regular and irregular progressive edge-growth tanner graphs\"",
        "Given above information, for an author who has written the paper with the title \"Improved Trellis-Based Algorithm for Locating and Breaking Cycles in Bipartite Graphs with Applications to LDPC Codes\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008560": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Group Action Recognition In Soccer Videos':",
        "Document: \"Contour tracking with abrupt motion. Traditional contour tracking methods can not handle abrupt motion or low frame rate video. This is because the basis of the traditional tracking lies in the assumption that the motion is smooth between consecutive frames. However, the abrupt motion destroys the foundation of this assumption. In this paper, we integrate the stochastic search into the level set evolution to reinstitute the continuity. Our approach can be viewed as a two-layer hierarchical level set-based tracking framework in which Particle Swarm Optimization (PSO) and level set evolution are fused seamlessly. In the first layer, the PSO is adopted to capture the global motion of the object. The coarse contour is obtained by applying the global motion to the contour in the previous frame. For the second layer, the level set evolution based on the coarse contour is carried out to track the local deformation, which results in the actual contour. The promising experimental results for numerous real videos reveal the effectiveness of our approach.\"",
        "Document: \"An scheme improved on iterative detection algorithm for V-BLAST. Traditional detection algorithm for V-BLAST is based on Order Successive Interference Cancellation (OSIC). Using iterative algorithm, the low-diversity substreams are decoded by decisions form of high-diversity substreams, the performance is highly improved at a cost of too much iterative repetition. To decrease computational complexity, an improved scheme for iterative detection method was proposed in this paper. Improving the performance of the first substream greatly and reducing iterative repetition, the improved scheme achieves the balance of performance and computation complexity. Simulation results show that the improve scheme obtains nearly the same performance with that of normal iterative OSIC algorithm, but the computational complexity is low comparatively.\"",
        "1 is \"The DRESUN Testbed for Research in FA/C Distributed Situation Assessment: Extensions to the Model of External Evidence\", 2 is \"Optimal combination of stereo camera calibration from arbitrary stereo images\"",
        "Given above information, for an author who has written the paper with the title \"Group Action Recognition In Soccer Videos\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008578": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Hierarchy-based access control in distributed environments':",
        "Document: \"Distributed cooperation and diversity for hybrid wireless networks. In this paper, we propose a new Distributed Cooperation and Diversity Combining framework. Our focus is heterogeneous networks with devices equipped with two types of radio frequency (RF) links: short-range high-rate interface (e.g., IEEE802.11), and a long-range low-rate interface (e.g., cellular) communicating in fading channels. Within this framework, we propose and evaluate a set of distributed cooperation techniques operating at different hierarchical levels with resource constraints such as short-range RF bandwidth. We propose a Priority Maximum-Ratio Combining (PMRC) for pre-demodulation combining, a post soft-demodulation combining, and a decode-and-forward technique. We show that the proposed techniques achieve significant improvements on Signal to Noise Ratio (SNR), Bit Error Rate (BER) and throughput through analysis, simulation, and experimentation on our platform prototype. Our results also indicate that, under several communication scenarios we are considering, PMRC can improve the throughput performance by over an order of magnitude.\"",
        "Document: \"An autonomic and permissionless Android covert channel. Demand for mobile devices continues to experience worldwide growth. Within the U.S., there is a significant shift away from broadband usage towards Smartphones as the primary Internet entry point for consumers. Although technological advancements have helped fuel demand for greater features and functionality to enhance the user experience, they have also drawn attention from malicious actors seeking to access and exfiltrate increasingly available sensitive and content rich personalized information. In traditional Android based exfiltration channels, the application engaged in information acquisition is granted permission to execute off-board communications. This tactic increases the possibility of detection by applications designed to identify this form of behavior. In this paper, we sever the acquisition / exfiltration bundling by assigning independent responsibilities to two apps communicating via a stealthy, permissionless, self-configuring and self-optimizing ultrasonic bridge. We present a framework for analyzing channel feasibility and performance, and apply it to 28 popular mobile devices. We demonstrate basic channel capability on 13 devices, achieving in certain cases, Bit Error Rates lower than 10\u22124 and Shannon capacity approaching 14 bps. We further demonstrate two performance boosting solutions that build on these results: a multichannel implementation which improves performance by nearly 80% and; a single channel Amplitude Shift Keying solution that increases capacity three-fold.\"",
        "1 is \"Improving loss resilience with multi-radio diversity in wireless networks\", 2 is \"Stable Peers: Existence, Importance, and Application in Peer-to-Peer Live Video Streaming\"",
        "Given above information, for an author who has written the paper with the title \"Hierarchy-based access control in distributed environments\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008583": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'On parallelizing on-line statistics for stochastic biological simulations':",
        "Document: \"An operational semantic for skeletons. In this work we present an operational semantic schema suitable for skeleton-based parallel languages supporting both task and data parallelism. The presented semantic describes both functional and parallel behavior of the skeletal language in a uniform way by means of a labeled transition system. We use Lithium language (namely a Java skeleton framework) as test-bed language to describe the methodology.\"",
        "Document: \"On parallelizing on-line statistics for stochastic biological simulations. This work concerns a general technique to enrich parallel version of stochastic simulators for biological systems with tools for on-line statistical analysis of the results. In particular, within the FastFlow parallel programming framework, we describe the methodology and the implementation of a parallel Monte Carlo simulation infrastructure extended with user-defined on-line data filtering and mining functions. The simulator and the on-line analysis were validated on large multi-core platforms and representative proof-of-concept biological systems.\"",
        "1 is \"Derivation and Refinement of Textual Syntax for Models\", 2 is \"Automatic bone age assessment for young children from newborn to 7-year-old using carpal bones.\"",
        "Given above information, for an author who has written the paper with the title \"On parallelizing on-line statistics for stochastic biological simulations\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008638": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Scalability of Broadcast Performance in Wireless Network-on-Chip.':",
        "Document: \"Physical channel characterization for medium-range nanonetworks using catalytic nanomotors. Molecular communication is a promising paradigm to implement nanonetworks, the interconnection of nanomachines. Catalytic nanomotors constitute one of the techniques that have been proposed for medium-range molecular communications. This paper presents a physical channel characterization that shows how nanomachines communicate using catalytic nanomotors as information carriers. Quantitative results of the packet transmission delay and loss probability are then obtained through simulation. Finally, some trade-offs that will arise when designing these networks are outlined.\"",
        "Document: \"Scalability Analysis of SIMO Non-Radiative Resonant Wireless Power Transfer Systems Based on Circuit Models. Resonant inductive coupling wireless power transfer (RIC-WPT) is a leading field of research due to the growing number of applications that can benefit from this technology: from biomedical implants to consumer electronics, fractionated spacecraft, and electric vehicles, amongst others. However, applications are currently limited to point-to-point-links and do not target single input-multiple outp...\"",
        "1 is \"Exploiting Unix File-System Races via Algorithmic Complexity Attacks\", 2 is \"Surviving wireless energy interference in RF-harvesting sensor networks: An empirical study\"",
        "Given above information, for an author who has written the paper with the title \"Scalability of Broadcast Performance in Wireless Network-on-Chip.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008672": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'An actor critic algorithm based on Grassmanian search':",
        "Document: \"Reinforcement Learning Based Algorithms for Average Cost Markov Decision Processes. This article proposes several two-timescale simulation-based actor-critic algorithms for solution of infinite horizon Markov Decision Processes with finite state-space under the average cost criterion. Two of the algorithms are for the compact (non-discrete) action setting while the rest are for finite-action spaces. On the slower timescale, all the algorithms perform a gradient search over corresponding policy spaces using two different Simultaneous Perturbation Stochastic Approximation (SPSA) gradient estimates. On the faster timescale, the differential cost function corresponding to a given stationary policy is updated and an additional averaging is performed for enhanced performance. A proof of convergence to a locally optimal policy is presented. Next, we discuss a memory efficient implementation that uses a feature-based representation of the state-space and performs TD(0) learning along the faster timescale. The TD(0) algorithm does not follow an on-line sampling of states but is observed to do well on our setting. Numerical experiments on a problem of rate based flow control are presented using the proposed algorithms. We consider here the model of a single bottleneck node in the continuous time queueing framework. We show performance comparisons of our algorithms with the two-timescale actor-critic algorithms of Konda and Borkar (1999) and Bhatnagar and Kumar (2004). Our algorithms exhibit more than an order of magnitude better performance over those of Konda and Borkar (1999).\"",
        "Document: \"An actor critic algorithm based on Grassmanian search. We propose the first online actor-critic scheme with adaptive basis to find a local optimal control policy for a Markov Decision Process (MDP) under the weighted discounted cost objective. We parameterize both the policy in the actor and the value function in the critic. The actor performs gradient search in the space of policy parameters using simultaneous perturbation stochastic approximation (SPSA) gradient estimates. This gradient computation requires estimates of value function that are provided by the critic by minimizing a mean square Bellman error objective. In order to obtain good estimates of the value function, the critic adaptively tunes the basis functions (or the features) to obtain the best representation of the value function using gradient search in the Grassmanian of features. Our control algorithm makes use of multi-timescale stochastic approximation. The actor updates its parameters along the slowest time scale. The critic uses two time scales to estimate the value function. For any given feature value, our algorithm performs gradient search in the parameter space via a residual gradient scheme on the faster timescale and, on a medium timescale, performs gradient search in the Grassman manifold of features. We provide an outline of the proof of convergence of our control algorithm to a locally optimum policy. We show empirical results using our algorithm as well as a similar algorithm that uses temporal difference (TD) learning in place of the residual gradient scheme for the faster timescale updates.\"",
        "1 is \"Bayes Error Estimation Using Parzen and k-NN Procedures.\", 2 is \"Fast distributed smoothing of relative measurements\"",
        "Given above information, for an author who has written the paper with the title \"An actor critic algorithm based on Grassmanian search\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008673": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Smart Touch: Improving Touch Accuracy for People with Motor Impairments with Template Matching.':",
        "Document: \"Point & click mediated interactions for large home entertainment displays. This work introduces and discusses the implementation details of a novel concept for a home entertainment system together with an affordable controlling interface that uses point & click interactions in order to create, mix and manipulate media screens within the same projection-based display. Scenarios for single and multiple viewers are discussed with users being able to create, reposition, resize, and control their own-defined media screens. The standard and familiar WIMP interaction techniques are transferred from PCs to home entertainment using a motion-sensing remote controller. An optional system feature is described for the automatic configuration of such media screens by analyzing the home environment using computer vision techniques. Observations from initial user studies are reported with regards to the perceived usefulness and acceptability of the proposed system. The main benefit introduced by this work is that of a large entertainment display that becomes shared and personalized while it is being adapted and fit into the home environment.\"",
        "Document: \"Small, medium, or large?: estimating the user-perceived scale of stroke gestures. We show that large consensus exists among users in the way they articulate stroke gestures at various scales (i.e., small, medium, and large), and formulate a simple rule that estimates the user-intended scale of input gestures with 87% accuracy. Our estimator can enhance current gestural interfaces by leveraging scale as a natural parameter for gesture input, reflective of user perception (i.e., no training required). Gesture scale can simplify gesture set design, improve gesture-to-function mappings, and reduce the need for users to learn and for recognizers to discriminate unnecessary symbols.\"",
        "1 is \"LightRing: always-available 2D input on any surface\", 2 is \"The potential of dwell-free eye-typing for fast assistive gaze communication\"",
        "Given above information, for an author who has written the paper with the title \"Smart Touch: Improving Touch Accuracy for People with Motor Impairments with Template Matching.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008688": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A Comprehensive Side-Channel Information Leakage Analysis of an In-Order RISC CPU Microarchitecture.':",
        "Document: \"Record Setting Software Implementation of DES Using CUDA. The increase in computational power of off-the-shelf hardware offers more and more advantageous tradeoffs among efficiency, cost and availability, thus enhancing the feasibility of of cryptanalytic attacks aiming to lower the security of widely used cryptosystems. In this paper we illustrate an GPU-based software implementation of the most efficent variant of Data Encryption Standard (DES), showing the performance of a software breaker which effectively exploits the multi-core Nvidia GT200 graphic architecture. The key point is to assess how well the structure of a symmetric key cipher can fit the GPU programming model and the single instruction multiple data architectural parallelism. The proposed breaker outperforms the fastest general purpose CPU-based implementations by an order of magnitude, and, due to the vast availability of GPUs on the market, the speedup translates into a sound improvement in the cost efficiency of the attack. As opposed to solutions based either on application specific or reconfigurable hardware, the proposed implementation does not require any specific technical knowledge from the attacker in order to be successfully built, once our implementation is available. This turns out in a better cost-availability tradeoff and minimizes the required setup time for such an attack to be mounted.\"",
        "Document: \"Exploring Cortex-M Microarchitectural Side Channel Information Leakage. The growing Internet of Things (IoT) market demands side-channel attack resistant, efficient, cryptographic implementations. Such implementations, however, are microarchitecture-specific, and cannot be implemented without an in-depth structural knowledge of the CPU and memory information leakage patterns; a description of such information leakages is presently not disclosed by any processor design company. In this work we propose the first Instruction Set Architecture (ISA) level framework for microarchitectural leakage characterization. Our framework allows to extract a microarchitectural leakage profile from a superscalar in-order processor; we infer detailed pipeline characteristics through the observation of instruction execution timings, and provide an identification of the datapath registers via a side-channel measuring setup. The extracted model can serve as a foundation for building solid countermeasures against side-channel attacks on software cryptographic implementations. We validate the extracted models on the ARM Cortex-M4 and ARM Cortex-M7 CPUs, two of the most widespread ARM microcontroller cores. Finally, as a further validation of the effectiveness of our derived model, we mount a successful attack on unprotected AES implementations for each of the examined platforms.\"",
        "1 is \"Building a side channel based disassembler\", 2 is \"Dynamic voltage and frequency scaling for shared resources in multicore processor designs\"",
        "Given above information, for an author who has written the paper with the title \"A Comprehensive Side-Channel Information Leakage Analysis of an In-Order RISC CPU Microarchitecture.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008714": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Adapting Object Recognition across Domains: A Demonstration':",
        "Document: \"Highlight substitution in light fields. Highlights occur especially when recording medical (color) images during micro-invasive operations. They disturb the physicians who can sometimes only guess the tissue at the position of the highlights. In this contribution we present a new technique of highlight removal. A so-called light field is generated from the recorded image sequence. Then a bi- nary highlight mask is computed for each image and used as confidence map for the light field pixels. The result is a light field in which pixels at highlight positions are interp o- lated by pixels which were not over-imposed by highlights. This leads to light fields with better images. We demon- strate and evaluate the technique on medical and synthetic image sequences.\"",
        "Document: \"MOBSY: Integration of Vision and Dialogue in Service Robots. Abstract MOBSY is a fully integrated autonomous mobile service robot sys - tem It acts as an  automatic dialogue  based  receptionist  for visitors of our  in - stitute MOBSY incorporates many techniques from different research areas into one working stand - alone system Especially the computer vision and dialogue as - pects are of main interest from the pattern recognition's point of view To summa - rize shortly, the involved techniques range from object classification over visual self - localization  and  recalibration  to  object  tracking  with  multiple  cameras A dialogue component has to deal with speech recognition, understanding and an - swer generation Further techniques needed are navigation, obstacle avoidance, and mechanisms to provide fault tolerant behavior This contribution introduces our mobile system MOBSY Among the main as - pects  vision  and  speech,  we  focus  also  on  the  integration  aspect,  both  on  the methodological and on the technical level We describe the task and the involved techniques Finally, we discuss the experiences that we gained with MOBSY dur - ing a live performance at the 25th anniversary of our institute\"",
        "1 is \"Handwritten Digit Recognition by Combined Classifiers\", 2 is \"Sensor Fusion For 3d Human Body Tracking With An Articulated 3d Body Model\"",
        "Given above information, for an author who has written the paper with the title \"Adapting Object Recognition across Domains: A Demonstration\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008731": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Semi-supervised Learning for Affective Common-Sense Reasoning.':",
        "Document: \"In-sample Model Selection for Trimmed Hinge Loss Support Vector Machine. In this letter, we target the problem of model selection for support vector classifiers through in-sample methods, which are particularly appealing in the small-sample regime. In particular, we describe the application of a trimmed hinge loss function to the Rademacher complexity and maximal discrepancy-based in-sample approaches and show that the selected classifiers outperform the ones obtained with other in-sample model selection techniques, which exploit a soft loss function, in classifying microarray data.\"",
        "Document: \"A heterogeneous and reconfigurable machine-vision system. This paper describes a new machine-vision system, a HERMIA heterogeneous and reconfigurable machine for image analysis. The architecture topology of the HERMIA machine is reconfigurable; moreover, the integration of its special modules allows a search for optimal strategies to solve vision problems. The general architecture and the hardware implementation are described. The software environment of the HERMIA machine provides a full iconic interface and a pictorial language oriented to vision in multiprocessor architectures. The preliminary system evaluation and applications are shown.\"",
        "1 is \"Three new graphical models for statistical language modelling\", 2 is \"A discriminative framework for detecting remote protein homologies.\"",
        "Given above information, for an author who has written the paper with the title \"Semi-supervised Learning for Affective Common-Sense Reasoning.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008732": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Relevance vector machine based infinite decision agent ensemble learning for credit risk analysis':",
        "Document: \"Authentication Protocol for an IoT-Enabled LTE Network. The Evolved Packet System-based Authentication and Key Agreement (EPS-AKA) protocol of the long-term evolution (LTE) network does not support Internet of Things (IoT) objects and has several security limitations, including transmission of the object\u2019s (user/device) identity and key set identifier in plaintext over the network, synchronization, large overhead, limited identity privacy, and security attack vulnerabilities. In this article, we propose a new secure and efficient AKA protocol for the LTE network that supports secure and efficient communications among various IoT devices as well as among the users. Analysis shows that our protocol is secure, efficient, and privacy preserved, and reduces bandwidth consumption during authentication.\"",
        "Document: \"Capturing Long-Term Dependencies for Protein Secondary Structure Prediction. Bidirectional recurrent neural network (BRNN) is a non-causal system that captures both upstream and downstream information for protein secondary structure prediction. Due to the problem of vanishing gradients, the BRNN can not learn remote information efficiently. To limit this problem, we propose segmented memory recurrent neural network (SMRNN) and obtain a bidirectional segmented-memory recurrent neural network (BSMRNN) by replacing the standard RNNs in BRNN with SMRNNs. Our experiment with BSMRNN for protein secondary structure prediction on the RS126 set indicates improvement in the prediction accuracy.\"",
        "1 is \"Differential Evolution With Neighborhood Mutation for Multimodal Optimization\", 2 is \"Domain Adaptation of Conditional Probability Models Via Feature Subsetting\"",
        "Given above information, for an author who has written the paper with the title \"Relevance vector machine based infinite decision agent ensemble learning for credit risk analysis\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008737": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Analysis and Implementation of the SNOW 3G Generator Used in 4G/LTE Systems.':",
        "Document: \"Modelling of complex cryptographic systems in terms of simple cellular automata. In this work, it is shown that binary sequences generated by a class of linear cellular automata equal the output sequences of nonlinear sequence generators. Emphasis is on cryptographic characteristics of such sequences (period, linear complexity or number of different output sequences). These simple linear automata easily model complex nonlinear generators with application in secret key cryptography.\"",
        "Document: \"Linear Cellular Automata as Discrete Models for Generating Cryptographic Sequences. In this paper, we develop a new cellular automata-based linear model for several nonlinear pseudorandom number generators with practical applications in symmetric cryptography. Such a model generates all the solutions of linear binary difference equations as well as many of these solutions are pseudo-random keystream sequences. In this way, a linear structure based on cellular automata may be used to generate not only difference equation solutions but also cryptographic sequences. The proposed model is very simple since it is based exclusively on. successive concatenations of a basic linear automaton.\"",
        "1 is \"Secure, redundant, and fully distributed key management scheme for mobile ad hoc networks: an analysis\", 2 is \"The Byzantine Generals Problem\"",
        "Given above information, for an author who has written the paper with the title \"Analysis and Implementation of the SNOW 3G Generator Used in 4G/LTE Systems.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008744": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Stochastic Approach to Separate Diffuse and Specular Reflections':",
        "Document: \"Hand shape recognition using distance transform and shape decomposition. Hand shape is a natural and human-friendly interface for human-computer interaction. This paper proposes a real- time and 2D vision-based hand shape recognition method. The method is robust to hand pose changes because the hand pose is neutralized after recognizing a hand pose using distance transform, principal component analysis (PCA), and histogram analysis. Also, the context-based recognition method using shape decomposition can effectively recognize tiny changes of fingers. The method worked at 44.8 fps and had a recognition rate of 83% on average in the experiment with 800 images including 5 hand shapes and 16 hand poses.\"",
        "Document: \"Emotion Recognition from Dance Image Sequences Using Contour Approximation. We present a novel approach that exploits shape context to recognize emotion from monocular dance image sequences. The method makes use of contour information as well as region-based shape information. The procedure of the method is as follows. First, we compute binary silhouette images and its bounding box from dance images. Next, we extract the quantitative features that represent the quality of the motion of a dance. Then, we find meaningful low-dimensional structures, removing redundant information but retaining essential information possessing high discrimination power, of the features using SVD (Singular Value Decomposition). Finally, we classify the low-dimensional features into predefined emotional categories using TDMLP (Time Delayed MultiLayer Perceptron). Experimental results demonstrate the validity of the proposed method.\"",
        "1 is \"Localization Based on Building Recognition\", 2 is \"Vision-based hand pose estimation: A review\"",
        "Given above information, for an author who has written the paper with the title \"Stochastic Approach to Separate Diffuse and Specular Reflections\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008766": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Adaptive Location Division Multiple Access for reliable safety message dissemination in VANETs':",
        "Document: \"Building the blocks of protocol design and analysis: challenges and lessons learned from case studies on mobile ad hoc routing and micro-mobility protocols. With the emergence of new application-specific sensor and Ad-hoc networks, increasingly complex and custom protocols will be designed and deployed. We propose a framework to systematically design and evaluate networking protocols based on a 'building block' approach. In this approach, each protocol is broken down into a set of parameterized modules called \"building blocks\", each having its own specific functionality. The properties of these building blocks and their interaction define the overall behavior of the protocol. In this paper, we aim to identify the major research challenges and questions in the building block approach. By addressing some of those questions, we point out potential directions to analyze and understand the behavior of networking protocols systematically. We discuss two case studies on utilizing the building block approach for analyzing Ad-hoc routing protocols and IP mobility protocols in a systematic manner.\"",
        "Document: \"Vehicles Meet Infrastructure: Toward Capacity\u2013Cost Tradeoffs for Vehicular Access Networks. Access infrastructure, such as Wi-Fi access points and cellular base stations (BSs), plays a vital role in providing pervasive Internet services to vehicles. However, the deployment costs of different access infrastructure are highly variable. In this paper, we make an effort to investigate the capacity\u2013cost tradeoffs for vehicular access networks, in which access infrastructure is deployed to provide a downlink data pipe to all vehicles in the network. Three alternatives of wireless access infrastructure are considered, i.e., cellular BSs, wireless mesh backbones (WMBs), and roadside access points (RAPs). We first derive a lower bound of downlink capacity for each type of access infrastructure. We then present a case study based on a perfect city grid of 400 $\\\\hbox{km}^{2}$ with 0.4 million vehicles, in which we examine the capacity\u2013cost tradeoffs of different deployment solutions in terms of capital expenditures (CAPEX) and operational expenditures (OPEX). The rich implications from our results provide fundamental guidance on the choice of cost-effective access infrastructure for the emerging vehicular networking.\"",
        "1 is \"ORB-SLAM: A Versatile and Accurate Monocular SLAM System.\", 2 is \"Aloha with preamble sampling for sporadic traffic in ad hoc wireless sensor networks\"",
        "Given above information, for an author who has written the paper with the title \"Adaptive Location Division Multiple Access for reliable safety message dissemination in VANETs\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008781": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Resource Partitioning Among Real-Time Applications':",
        "Document: \"Minimizing CPU energy in real-time systems with discrete speed management. This article presents a general framework to analyze and design embedded systems minimizing the energy consumption without violating timing requirements. A set of realistic assumptions is considered in the model in order to apply the results in practical real-time applications. The processor is assumed to have as a set of discrete operating modes, each characterized by speed and power consumption. The energy overhead and the transition delay incurred during mode switches are considered. Task computation times are modeled with a part that scales with the speed and a part having a fixed duration, to take I/O operations into account. The proposed method allows to compute the optimal sequence of voltage/speed changes that approximates the minimum continuous speed, which guarantees the feasibility of a given set of real-time tasks, without violating the deadline constraints. The analysis is performed both under fixed and dynamic priority assignments.\"",
        "Document: \"The Space of EDF Feasible Deadlines. It is well known that the performance of computer controlled systems is heavily affected by delays and jitter occurring in the control loops, which are mainly caused by the interference introduced by other concurrent activities. A common approach adopted to reduce delay and jitter in periodic task systems is to decrease relative deadlines as much as possible, but without jeopardising the schedulability of the task set. In this paper, we formally characterise the region of admissible deadlines so that the system designer can appropriately select the desired values to maximise a given performance index defined over the task set. Finally we also provide a sufficient region of feasible deadlines which is proved to be convex.\"",
        "1 is \"Real-Time Mach: Towards a Predictable Real-Time System\", 2 is \"Analysis Of Event-Driven Controllers For Linear Systems\"",
        "Given above information, for an author who has written the paper with the title \"Resource Partitioning Among Real-Time Applications\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008866": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'G-Tree: An Efficient and Scalable Index for Spatial Search on Road Networks':",
        "Document: \"Automatic image annotation by mining the web. Automatic image annotation has been becoming an attractive research subject. Most current image annotation methods are based on training techniques. The major weaknesses of such solutions include limited annotation vocabulary and labor-intensive involvement. However, Web images possess a lot of texts, and rich annotation of samples is provided. Therefore, this report provides a novel image annotation method by mining the Web that term-image correlation is obtained from the Web not by learning. Without question, there are many noises in that relation, and some cleaning works are necessary. In the system, entropy weighting and image clustering technique are employed. Our experiment results show that our solution can achieve a satisfactory performance.\"",
        "Document: \"Discovering the core semantics of event from social media. As social media is opening up such as Twitter and Sina Weibo,11Chinese microblogging website http://weibo.com/. large volumes of short texts are flooding on the Web. The ocean of short texts dilutes the limited core semantics of event in cyberspace by redundancy, noises and irrelevant content on the web, which make it difficult to discover the core semantics of event. The major challenges include how to efficiently learn the semantic association distribution by small-scale association relations and how to maximize the coverage of the semantic association distribution by the minimum number of redundancy-free short texts. To solve the above issues, we explore a Markov random field based method for discovering the core semantics of event. This method makes semantics collaborative computation for learning association relation distribution and makes information gradient computation for discovering k redundancy-free texts as the core semantics of event. We evaluate our method by comparing with two state-of-the-art methods on the TAC dataset and the microblog dataset. The results show our method outperforms other methods in extracting core semantics accurately and efficiently. The proposed method can be applied to short text automatic generation, event discovery and summarization for big data analysis. Proposing a Markov random field based method for discovering the core semantics of event.Learning the association relation distribution of event by small scale association relations.Maximizing the coverage of association relation distribution by the minimum number of short texts.\"",
        "1 is \"Query processing for high-volume XML message brokering\", 2 is \"Clustering and Identifying Temporal Trends in Document Databases\"",
        "Given above information, for an author who has written the paper with the title \"G-Tree: An Efficient and Scalable Index for Spatial Search on Road Networks\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008879": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Caching based socially-aware D2D communications in wireless content delivery networks: a hypergraph framework.':",
        "Document: \"The role of small cells, coordinated multipoint, and massive MIMO in 5G. 5G will have to support a multitude of new applications with a wide variety of requirements, including higher peak and user data rates, reduced latency, enhanced indoor coverage, increased number of devices, and so on. The expected traffic growth in 10 or more years from now can be satisfied by the combined use of more spectrum, higher spectral efficiency, and densification of cells. The focus of ...\"",
        "Document: \"The road to IMT-advanced communication systems: state-of-the-art and innovation areas addressed by the WINNER+ project. Phases I and II of the WINNER project contributed to the development, integration, and assessment of new mobile network techniques from 2004 to 2007. Some of these techniques are now in the 3GPP LTE and IEEE 802.16 (WiMAX) standards, while others are under consideration for LTE-Advanced and 802.16m. The WINNER+ project continues this forward-looking work for IMT-advanced technologies and their evolution, with a particular focus on 3GPP LTE-Advanced. This article provides an overview of the WINNER system concept and several of its key innovative components.\"",
        "1 is \"Designing intelligent energy harvesting communication systems\", 2 is \"Diversity-multiplexing tradeoff in multiple-access channels\"",
        "Given above information, for an author who has written the paper with the title \"Caching based socially-aware D2D communications in wireless content delivery networks: a hypergraph framework.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008935": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'An opportunistic resource management model to overcome resource-constraint in the Internet of Things':",
        "Document: \"A novel lifecycle model for Web-based application development in small and medium enterprises. Software engineering\u2019s lifecycle models have proven to be very important for traditional software development. However, can\n these models be applied to the development of Web-based applications as well? In recent years, Web-based applications have\n become more and more complicated and a lot of efforts have been placed on introducing new technologies such as J2EE, PhP,\n and.NET, etc., which have been universally accepted as the development technologies for Web-based applications. However, there\n is no universally accepted process model for the development of Web-based applications. Moreover, shaping the process model\n for small medium-sized enterprises (SMEs), which have limited resources, has been relatively neglected. Based on our previous\n work, this paper presents an expanded lifecycle process model for the development of Web-based applications in SMEs. It consists\n of three sets of processes, i.e., requirement processes, development processes, and evolution processes. Particularly, the\n post-delivery evolution processes are important to SMEs to develop and maintain quality web applications with limited resources\n and time.\"",
        "Document: \"A hybrid swarm intelligence algorithm for multiuser scheduling in HSDPA. Multiuser scheduling is an important aspect in the performance optimization of a wireless network since it allows multiple users to access a shared channel efficiently by exploiting multiuser diversity. To perform efficient scheduling, channel state information (CSI) for users is required, and is obtained via their respective feedback channels. In this paper, a more realistic imperfect CSI feedback, in the form of a finite set of Channel Quality Indicator (CQI) values, is assumed as specified in the HSDPA standard. A mathematical model of the problem is developed for use in the optimization process. A hybrid heuristic approach based on particle swarm optimization and simulated annealing is used to solve the problem. Simulation results indicate that the hybrid approach outperforms individual implementations of both simulated annealing and particle swarm optimization.\"",
        "1 is \"Visibility-based probabilistic roadmaps for motion planning\", 2 is \"Kinematic And Dynamic Vehicle Models For Autonomous Driving Control Design\"",
        "Given above information, for an author who has written the paper with the title \"An opportunistic resource management model to overcome resource-constraint in the Internet of Things\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008989": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'TESTOR: deriving test sequences from model-based specifications':",
        "Document: \"An experience in using a tool for evaluating a large set of natural language requirements. Requirements analysis is an important phase in a software project. It is often performed in an informal way by specialists who review documents looking for ambiguities, technical inconsistencies and incompleteness. Automatic evaluation of Natural Language (NL) requirements documents has been proposed as a means to improve the quality of the system under development. We show how the tool QuARS Express, introduced in a quality analysis process, is able to manage complex and structured requirement documents containing metadata, and to produce an analysis report rich of categorized information that points out linguistic defects and indications about the writing style of NL requirements. In this paper we report our experience using this tool in the automatic analysis of a large collection of natural language requirements, produced inside the MODCONTROL project.\"",
        "Document: \"Sustainable Safety in Mobile Multi-robot Systems via Collective Adaptation. To be safe, mobile multi-robots systems need to be able to adapt to unexpected behaviours of robots as well as to exogenous changes in the environment. In this paper, we describe a novel approach for the development of multi-robots systems where robots collectively collaborate with each other to satisfy their goals and to adapt their behaviour in a collective way satisfying the overall system safety. Safety-specific self-adaptation capabilities of the approach are generic and independent from the functional behaviour of the robots. An example dealing with safety for autonomous UAV is provided as well.\"",
        "1 is \"Describing Software Architecture Styles Using Graph Grammars\", 2 is \"Temporal logic for scenario-based specifications\"",
        "Given above information, for an author who has written the paper with the title \"TESTOR: deriving test sequences from model-based specifications\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008995": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A semantic enriched data model for sensor network interoperability':",
        "Document: \"Detect and correlate information system events through verbose logging messages analysis. Detecting and tracking events from logging data is a critical element for security and system administrators and thus attracts more and more research efforts. However, there exists a major limitation in current processes of Event Logging analysis, related to the verbosity and language-dependence of messages produced by many logging systems. In this paper, a novel methodology was proposed to tackle this limitation by analysing event messages through a Natural Language Processing task in order to annotate them with semantic metadata. These metadata are further used to enable semantic searches or domain ontology population that help administrator to filter only relevant event and to correlate them for a prompt and efficient response and incident analysis.\"",
        "Document: \"Multimedia Social Network Modeling: A Proposal. In this paper we present a preliminary work concerning the definition of a novel data model for Multimedia Social Networks (MSNs), i.e. networks that combine information on users belonging to one or more social communities together with the multimedia content that is generated and used within a related environment. The proposed model is based on the hypergraph structure and allows us to represent in a simple way all the different kinds of relationships that are typical of a social network, in particular between multimedia content, between users and multimedia content and between users themselves, at the same time supporting several kinds of applications by means of the introduction of several ranking functions. In addition, we provide a strategy for mapping the proposed model into an object relational data model, for efficiently storing the related data. We also describe a first prototype that can store and integrate information from different OSNs (Facebook, Twitter) and multimedia sharing systems (Flickr, Lastfm), providing some facilities for browsing the entire hypergraph.\"",
        "1 is \"Semi-automatic Composition ofWeb Services using Semantic Descriptions\", 2 is \"Dynamic Provision of Computing Resources from Grid Infrastructures and Cloud Providers\"",
        "Given above information, for an author who has written the paper with the title \"A semantic enriched data model for sensor network interoperability\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009035": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'The I4U Mega Fusion and Collaboration for NIST Speaker Recognition Evaluation 2016.':",
        "Document: \"Improving k-means by outlier removal. We present an Outlier Removal Clustering (ORC) algorithm that provides outlier detection and data clustering simultaneously. The method employs both clustering and outlier discovery to improve estimation of the centroids of the generative distribution. The proposed algorithm consists of two stages. The first stage consist of purely K-means process, while the second stage iteratively removes the vectors which are far from their cluster centroids. We provide experimental results on three different synthetic datasets and three map images which were corrupted by lossy compression. The results indicate that the proposed method has a lower error on datasets with overlapping clusters than the competing methods.\"",
        "Document: \"Automatic versus human speaker verification: The case of voice mimicry. \u2022First use of channel compensated speaker verification system in mimicry corpus.\u2022Performance evaluation of three automatic speaker verification systems and human listening panel using a mimicry corpus.\u2022Evidence that a random listener is spoofed by an impersonator but automatic system is not.\u2022Obtained evidence that disguise attack is harder to detect than mimicry attack.\"",
        "1 is \"Voice conversion based on weighted frequency warping\", 2 is \"A GMM supervector Kernel with the Bhattacharyya distance for SVM based speaker recognition\"",
        "Given above information, for an author who has written the paper with the title \"The I4U Mega Fusion and Collaboration for NIST Speaker Recognition Evaluation 2016.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009070": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A Highly Efficient VLSI Architecture for H.264/AVC CAVLC Decoder':",
        "Document: \"Combined CAVLC Decoder and Inverse Quantizer for Efficient H.264/AVC Decoding. This paper proposes an efficient architecture, which combines the context-based adaptive variable length coding (CAVLC) decoder and inverse quantization (IQ) together to simplify the H.264/AVC decoder. The IQ function is effectively moved to the run before stage in the CAVLC decoder. With this efficient arrangement, it can easily implement the interface between CAVLC decoder and IQ without additional logic circuit. However, the authors also use pipeline skill to improve the performance. Because there are data dependency properties in the CAVLC decoder, it should modify the algorithm in the standard to realize the pipeline skill. The authors implement this architecture with UMC 0.18 mum cell library. The simulation results show the operation frequency can achieve 200 MHz. The total number of logic gate counts is 9.23k. For the real-time requirement, it achieves 1080HD (1920times1088) @30 frames/sec while the clock frequency is set to 195 MHz\"",
        "Document: \"An Efficient Criterion for Mode Decision in H.264/AVC. In this paper, an efficient cost function for mode decision in H.264/AVC is proposed. The proposed cost function is based on integer transform coefficients, where the rate and the distortion are jointly modeled by the number of nonzero quantized coefficients, the sum of absolute integer transformed differences (SAITD) and sum of squared integer transformed differences (SSITD). Comparing to the high-complexity cost function, which should be calculated from real bit-consumption and true reconstructed distortion for each coding mode, the proposed efficient cost function can achieve 79.93% and 22.61% time savings of computing rate-distortion cost and overall encoding, respectively, while introducing only slight degradation with 1.05% bit-rate increment and 0.049 dB PSNR drop\"",
        "1 is \"Out-of-loop rate control for video codec hardware/software co-design\", 2 is \"A segmentation algorithm for color images\"",
        "Given above information, for an author who has written the paper with the title \"A Highly Efficient VLSI Architecture for H.264/AVC CAVLC Decoder\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009158": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Scaling distributed machine learning with the parameter server':",
        "Document: \"Fair and balanced: learning to present news stories. Relevance, diversity and personalization are key issues when presenting content which is apt to pique a user's interest. This is particularly true when presenting an engaging set of news stories. In this paper we propose an efficient algorithm for selecting a small subset of relevant articles from a streaming news corpus. It offers three key pieces of improvement over past work: 1) It is based on a detailed model of a user's viewing behavior which does not require explicit feedback. 2) We use the notion of submodularity to estimate the propensity of interacting with content. This improves over the classical context independent relevance ranking algorithms. Unlike existing methods, we learn the submodular function from the data. 3) We present an efficient online algorithm which can be adapted for personalization, story adaptation, and factorization models. Experiments show that our system yields a significant improvement over a retrieval system deployed in production.\"",
        "Document: \"Structured Correspondence Topic Models for Mining Captioned Figures in Biological Literature. A major source of information (often the most crucial and informative part) in scholarly articles from scientific journals, proceedings and books are the figures that directly provide images and other graphical illustrations of key experimental results and other scientific contents. In biological articles, a typical figure often comprises multiple panels, accompanied by either scoped or global captioned text. Moreover, the text in the caption contains important semantic entities such as protein names, gene ontology, tissues labels, etc., relevant to the images in the figure. Due to the avalanche of biological literature in recent years, and increasing popularity of various bio-imaging techniques, automatic retrieval and summarization of biological information from literature figures has emerged as a major unsolved challenge in computational knowledge extraction and management in the life science. We present a new structured probabilistic topic model built on a realistic figure generation scheme to model the structurally annotated biological figures, and we derive an efficient inference algorithm based on collapsed Gibbs sampling for information retrieval and visualization. The resulting program constitutes one of the key IR engines in our SLIF system that has recently entered the final round (4 out 70 competing systems) of the Elsevier Grand Challenge on Knowledge Enhancement in the Life Science. Here we present various evaluations on a number of data mining tasks to illustrate our method.\"",
        "1 is \"Distance dependent Chinese restaurant processes\", 2 is \"F4: large-scale automated forecasting using fractals\"",
        "Given above information, for an author who has written the paper with the title \"Scaling distributed machine learning with the parameter server\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009324": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A robust training strategy against extraneous acoustic variations for spontaneous speech recognition':",
        "Document: \"Supervised Adversarial Networks for Image Saliency Detection. In the past few years, Generative Adversarial Network (GAN) became a prevalent research topic. By defining two convolutional neural networks (G-Network and D-Network) and introducing an adversarial procedure between them during the training process, GAN has ability to generate good quality images that look like natural images from a random vector. Besides image generation, GAN may have potential to deal with wide range of real world problems. In this paper, we follow the basic idea of GAN and propose a novel model for image saliency detection, which is called Supervised Adversarial Networks (SAN). Specifically, SAN also trains two models simultaneously: the G-Network takes natural images as inputs and generates corresponding saliency maps (synthetic saliency maps), and the D-Network is trained to determine whether one sample is a synthetic saliency map or ground-truth saliency map. However, different from GAN, the proposed method uses fully supervised learning to learn both G-Network and D-Network by applying class labels of the training set. Moreover, a novel kind of layer call conv-comparison layer is introduced into the D-Network to further improve the saliency performance by forcing the high-level feature of synthetic saliency maps and ground-truthes as similar as possible. Experimental results on Pascal VOC 2012 database show that the SAN model can generate high quality saliency maps for many complicate natural images.\"",
        "Document: \"Applying Convolutional Neural Networks concepts to hybrid NN-HMM model for speech recognition. Convolutional Neural Networks (CNN) have showed success in achieving translation invariance for many image processing tasks. The success is largely attributed to the use of local filtering and max-pooling in the CNN architecture. In this paper, we propose to apply CNN to speech recognition within the framework of hybrid NN-HMM model. We propose to use local filtering and max-pooling in frequency domain to normalize speaker variance to achieve higher multi-speaker speech recognition performance. In our method, a pair of local filtering layer and max-pooling layer is added at the lowest end of neural network (NN) to normalize spectral variations of speech signals. In our experiments, the proposed CNN architecture is evaluated in a speaker independent speech recognition task using the standard TIMIT data sets. Experimental results show that the proposed CNN method can achieve over 10% relative error reduction in the core TIMIT test sets when comparing with a regular NN using the same number of hidden layers and weights. Our results also show that the best result of the proposed CNN model is better than previously published results on the same TIMIT test sets that use a pre-trained deep NN model.\"",
        "1 is \"A minimax classification approach with application to robust speech recognition\", 2 is \"Dynamic speech spectrum representation and tracking variable number of vocal tract resonance frequencies with time-varying Dirichlet process mixture models\"",
        "Given above information, for an author who has written the paper with the title \"A robust training strategy against extraneous acoustic variations for spontaneous speech recognition\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009338": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'The design, implementation and evaluation of SMART: a scheduler for multimedia applications':",
        "Document: \"WARP: Enabling fast CPU scheduler development and evaluation. Developing CPU scheduling algorithms and understanding their impact in practice can be difficult and time consuming due to the need to modify and test operating system kernel code and measure the resulting performance on a consistent workload of real applications. To address this problem, we have developed WARP, a trace-driven virtualized scheduler execution environment that can dramatically simplify and speed the development of CPU schedulers. WARP is easy to use as it can run unmodified kernel scheduling code and can be used with standard user-space debugging and performance monitoring tools. It accomplishes this by virtualizing operating system and hardware events to decouple kernel scheduling code from its native operating system and hardware environment. A simple kernel tracing toolkit can be used with WARP to capture traces of all CPU scheduling related events from a real system. WARP can then replay these traces in its virtualized environment with the same timing characteristics as in the real system. Traces can be used with different schedulers to provide accurate comparisons of scheduling performance for a given application workload. We have implemented a WARP Linux prototype. Our results show that WARP can use application traces captured from its toolkit to accurately reflect the scheduling behavior of the real Linux operating system. Furthermore, testing scheduler behavior using WARP with application traces can be two orders of magnitude faster than running the applications using Linux.\"",
        "Document: \"Optimizing The Design And Implementation Of The Linux Arm Hypervisor. Modem hypervisor designs for both ARM and x86 virtualization rely on running an operating system kernel, the hypervisor OS kernel, to support hypervisor functionality. While x86 hypervisors effectively leverage architectural support to run the kernel, existing ARM hypervisors map poorly to the virtualization features of the ARM architecture, resulting in worse performance. We identify the key reason for this problem is the need to multiplex kernel mode state between the hypervisor and virtual machines, which each run their own kernel. To address this problem, we take a fundamentally different approach to hypervisor design that runs the hypervisor together with its OS kernel in a separate CPU mode from kernel mode. Using this approach, we redesign KVM/ARM to leverage a separate ARM CPU mode for running both the hypervisor and its OS kernel. We show what changes are required in Linux to implement this on current ARM hardware as well as how newer ARM architectural support can be used to support this approach without any changes to Linux other than to KVM/ARM itself. We show that our redesign and optimizations can result in an order of magnitude performance improvement for KVM/ARM, and can provide faster performance than x86 on key hypervisor operations. As a result, many aspects of our design have been successfully merged into mainline Linux.\"",
        "1 is \"File-system development with stackable layers\", 2 is \"Reducing the cost of branches\"",
        "Given above information, for an author who has written the paper with the title \"The design, implementation and evaluation of SMART: a scheduler for multimedia applications\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009452": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Sensing, Compression, and Recovery for WSNs: Sparse Signal Modeling and Monitoring Framework.':",
        "Document: \"Robust EM kernel-based methods for linear system identification. Recent developments in system identification have brought attention to regularized kernel-based methods. This type of approach has been proven to compare favorably with classic parametric methods. However, current formulations are not robust with respect to outliers. In this paper, we introduce a novel method to robustify kernel-based system identification methods. To this end, we model the output measurement noise using random variables with heavy-tailed probability density functions (pdfs), focusing on the Laplacian and the Student\u2019s t distributions. Exploiting the representation of these pdfs as scale mixtures of Gaussians, we cast our system identification problem into a Gaussian process regression framework, which requires estimating a number of hyperparameters of the data size order. To overcome this difficulty, we design a new maximum a posteriori (MAP) estimator of the hyperparameters, and solve the related optimization problem with a novel iterative scheme based on the Expectation\u2013Maximization (EM) method. In the presence of outliers, tests on simulated data and on a real system show a substantial performance improvement compared to currently used kernel-based methods for linear system identification.\"",
        "Document: \"Generalized System Identification with Stable Spline Kernels. Regularized least-squares approaches have been successfully applied to linear system identification. Recent approaches use quadratic penalty terms on the unknown impulse response defined by stable spline kernels, which control model space complexity by leveraging regularity and bounded-input bounded-output stability. This paper extends linear system identification to a wide class of nonsmooth stable spline estimators, where regularization functionals and data misfits can be selected from a rich set of piecewise linear-quadratic (PLQ) penalties. This class includes the 1-norm, Huber, and Vapnik, in addition to the least-squares penalty. By representing penalties through their conjugates, the modeler can specify any PLQ penalty for misfit and regularizer, as well as inequality constraints on the response. The interior point solver we implement (IPsolve) is locally quadratically convergent, with O(min(m,n)(2) (m + n)) arithmetic operations per iteration, where n is the number of unknown impulse response coefficients and m is the number of observed output measurements. IPsolve is competitive with available alternatives for system identification. This is shown by a comparison with TFOCS, libSVM, and the FISTA algorithm. The code is open source. The impact of the approach for system identification is illustrated with numerical experiments featuring robust formulations for contaminated data, relaxation systems, nonnegativity, and unimodality constraints on the impulse response, and sparsity promoting regularization. Incorporating constraints yields particularly significant improvements.\"",
        "1 is \"Interference-Controlled Load Sharing with Femtocell Relay for Macrocells in Cellular Networks\", 2 is \"Two algorithms for network size estimation for master/slave ad hoc networks\"",
        "Given above information, for an author who has written the paper with the title \"Sensing, Compression, and Recovery for WSNs: Sparse Signal Modeling and Monitoring Framework.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009495": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'On minimum delay duty-cycling protocol in sustainable sensor network':",
        "Document: \"Energy Efficient Clustering For Wsn-Based Structural Health Monitoring. In recent years, research on using wireless sensor networks (WSNs) for structural health monitoring (SHM) has attracted increasing attention. Unlike other monitoring applications, detection of possible structure damage requires significant amount of domain knowledge that computer science researchers are usually unfamiliar with. As a result, most previous work in WSN-based SHM was done by researchers in civil engineering. However, civil researchers often tend to solve practical engineering problems but rarely consider designing a system in an optimal way, particularly when the limited wireless bandwidth and restricted resources of WSNs need to be addressed. Through the collaboration with civil researchers, we demonstrate that optimization design can significantly help improve the performance of a WSN-based SHM system. We consider a fundamental problem in SHM: modal analysis, which is used to obtain the dynamic structural vibration characteristics. Cluster-based modal analysis approach is adopted. In each cluster, the vibration characteristics are identified and then are assembled together. Different from other applications, clustering in this approach should meet some extra requirements of modal analysis. Moreover, cluster size should be optimized to minimize the total energy consumption. This clustering problem is formally formulated and proven to be NP complete. Two centralized and one distributed algorithms are proposed to solve the problem. The effectiveness and efficiency of the proposed cluster-based modal analysis along with the clustering algorithms are evaluated using both simulation and experiments.\"",
        "Document: \"Distributed Sensing for High Quality Structural Health Monitoring Using Wireless Sensor Networks. In recent years, using wireless sensor networks (WSNs) for structural health monitoring (SHM) has attracted increasing attention. Traditional centralized SHM algorithms developed by civil engineers can achieve the highest damage detection quality since they have the raw data from all the sensor nodes. However, directly implementing these algorithms in a typical WSN is impractical considering the large amount of data transmissions and extensive computations required. Correspondingly, many SHM algorithms have been tailored for WSNs to become distributed and less complicated. However, the modified algorithms usually cannot achieve the same damage detection quality of the original centralized counterparts. In this paper, we select a classical SHM algorithm: the eigen-system realization algorithm (ERA), and propose a distributed version for WSNs. In this approach, the required computations in the ERA are updated incrementally along a path constructed from the deployed sensor nodes. This distributed version is able to achieve the same quality of the original ERA using much smaller wireless transmissions and computations. The efficacy of the proposed approach is demonstrated through both simulation and experiment.\"",
        "1 is \"A service-oriented architecture for QoS configuration and management of Wireless Sensor Networks\", 2 is \"Approximability of Two Variants of Multiple Knapsack Problems\"",
        "Given above information, for an author who has written the paper with the title \"On minimum delay duty-cycling protocol in sustainable sensor network\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009577": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'HS3-DPG: Hierarchical Simulation for 3-D P/G Network':",
        "Document: \"Fast Random Walk Based Capacitance Extraction for the 3D IC Structures with Cylindrical Inter-Tier-Vias. Three-dimensional integrated circuits (3D ICs) make use of the vertical dimension for smaller footprint, higher speed, lower power consumption, and better timing performance. In 3D ICs, the inter-tier-via (ITV) is a critical enabling technique because it forms vertical signal and power paths. Accordingly, it is imperative to accurately and efficiently extract the electrostatic capacitances of ITVs using field solvers. Unfortunately, the cylindrical via shape presents major challenges to most of the existing methods. To address this issue, we develop a novel floating random walk (FRW) method by rotating the transition cube to suit the cylindrical surface, devising a special space management technique, and proposing accelerating techniques for structures with large-sized through-silicon-vias (TSVs). Experiments on typical ITV structures suggest that the proposed techniques is up to hundreds times faster than a simple FRW approach and the boundary element method (BEM) based algorithms, without loss of accuracy. In addition, compared with extracting the square-approximation structures, the proposed techniques can reduce the error by 10X. Large and multi-dielectric structures have also been tested to demonstrate the versatility of the proposed techniques.\"",
        "Document: \"A Novel Entropy Production Based Full-Chip TSV Fatigue Analysis. Through-silicon vias (TSVs) are subject to thermal fatigue due to stress over time, no matter how small the stress is. Existing works on TSV fatigue all rely on measurement-based parameters to estimate the lifetime, and cannot consider detailed thermal profiles. In this paper, we propose a new method for TSV fatigue prediction using entropy production during thermal cycles, which is validated by theoretical analysis and measurement results. By combining thermodynamics and mechanics laws, the fatigue process can be quantitatively evaluated with detailed thermal profiles. Experimental results show that interestingly, the landing pad possesses the most easy-to-fail region, which generates up to 50% more entropy compared with the TSV body. The impact of landing pad dimension and TSV geometries are also studied, providing guidance for reliability enhancement. Finally, full-chip fatigue analysis is performed based on stress superposition. To the best of the authors' knowledge, this is the first TSV fatigue model that is free of measurement data fitting, the first that is capable of considering detailed thermal profiles, and the first framework for efficient full-chip TSV fatigue analysis.\n\n\"",
        "1 is \"iTEM: a temperature-dependent electromigration reliability diagnosis tool\", 2 is \"Just-in-Time Instruction Set Extension - Feasibility and Limitations for an FPGA-Based Reconfigurable ASIP Architecture\"",
        "Given above information, for an author who has written the paper with the title \"HS3-DPG: Hierarchical Simulation for 3-D P/G Network\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009594": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Denoising Convolutional Autoencoder Based B-Mode Ultrasound Tongue Image Feature Extraction':",
        "Document: \"An Improved Mmusic Algorithm Of Direction Of Arrival Estimation. An improved algorithm is presented for a signal's direction of arrival estimation to reduce the computational complexity of the existing modified multiple signal classification (MMUSIC) algorithm. In this paper, FFT algorithm is introduced to narrow the searching range of the direction angle. Afterwards, a compromise approach-based SVD and orthogonal-triangular decomposition takes the place of traditional double SVDs. Then, during matrix calculation, the noise subspace is achieved for the signal's direction of arrival. Finally, the system simulation demonstrates the efficiency and reliability of this novel MMUSIC algorithm. Compared with the existing algorithm, MMUSIC effectively saves the computational complexity with the approximation estimation performance. Copyright (c) 2011 John Wiley & Sons, Ltd.\"",
        "Document: \"A parametric optimization approach for uncertain linear quadratic models. As it is well known, the optimal control of linear quadratic model is given in a feedback form, which is determined by the solution of a Riccati equation. However, the corresponding Riccati equation cannot be solved analytically in many cases. Even if an analytic solution can be obtained, it might be a complex time-oriented function. In this paper, we introduce an approximate model with parameter for simplifying the form of optimal control of uncertain linear quadratic model. First, we discuss an optimal control problem of uncertain linear quadratic model and deduce an analytic expression of optimal control. Then we formulate an approximate model with parameter and present a parametric optimization method for solving the optimal parameter. Finally, a production planning problem is given to illustrate the efficiency of the proposed approximate model and parametric optimization approach.\"",
        "1 is \"Copilot - a coprocessor-based kernel runtime integrity monitor\", 2 is \"A Survey on Deep Learning in Medical Image Analysis.\"",
        "Given above information, for an author who has written the paper with the title \"Denoising Convolutional Autoencoder Based B-Mode Ultrasound Tongue Image Feature Extraction\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009641": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Predicting the probability of ice storm damages to electricity transmission facilities based on ELM and Copula function':",
        "Document: \"High-Performance Consensus Control in Networked Systems With Limited Bandwidth Communication and Time-Varying Directed Topologies. Communication data rates and energy constraints are two important factors that have to be considered in the coordination control of multiagent networks. Although some encoder-decoder-based consensus protocols are available, there still exists a fundamental theoretical problem: how can we further reduce the update rate of control input for each agent without the changing consensus performance? In t...\"",
        "Document: \"Data Mining And Simulation: A Grey Relationship Demonstration. Fuzzy data has grown to be an important factor in data mining. Whenever uncertainty exists, simulation can be used as a model. Simulation is very flexible, although it can involve significant levels of computation. This article discusses fuzzy decision-making using the grey related analysis method. Fuzzy models are expected to better reflect decision-making uncertainty, at some cost in accuracy relative to crisp models. Monte Carlo simulation is used to incorporate experimental levels of uncertainty into the data and to measure the impact of fuzzy decision tree models using categorical data. Results are compared with decision tree models based on crisp continuous data.\"",
        "1 is \"Image-adaptive watermarking using visual models\", 2 is \"Fuzzy sampled-data control for uncertain vehicle suspension systems.\"",
        "Given above information, for an author who has written the paper with the title \"Predicting the probability of ice storm damages to electricity transmission facilities based on ELM and Copula function\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009644": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Control Design of a Marine Vessel System Using Reinforcement Learning.':",
        "Document: \"Adaptive Neural Impedance Control of a Robotic Manipulator With Input Saturation. In this paper, adaptive impedance control is developed for an n-link robotic manipulator with input saturation by employing neural networks. Both uncertainties and input saturation are considered in the tracking control design. In order to approximate the system uncertainties, we introduce a radial basis function neural network controller, and the input saturation is handled by designing an auxiliary system. By using Lyapunov's method, we design adaptive neural impedance controllers. Both state and output feedbacks are constructed. To verify the proposed control, extensive simulations are conducted.\"",
        "Document: \"Adaptive Fuzzy Neural Network Control for a Constrained Robot Using Impedance Learning. This paper investigates adaptive fuzzy neural network (NN) control using impedance learning for a constrained robot, subject to unknown system dynamics, the effect of state constraints, and the uncertain compliant environment with which the robot comes into contact. A fuzzy NN learning algorithm is developed to identify the uncertain plant model. The prominent feature of the fuzzy NN is that there...\"",
        "1 is \"An Efficient and Practical Solution to Remote Authentication: Smart Card\", 2 is \"Neural networks for the recognition and pose estimation of 3D objects from a single 2D perspective view\"",
        "Given above information, for an author who has written the paper with the title \"Control Design of a Marine Vessel System Using Reinforcement Learning.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009676": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Decision Strategies in Mediated Multiagent Negotiations: An Optimization Approach':",
        "Document: \"Nonmonotonic inheritance systems. The use of frames and other hierarchical representational schemes has become fairly pervasive in intelligent systems. Central to these representations is the inheritance of properties from one object to another by way of is-a-kind and is-an-element-of links. Complexities and difficulties arise when the values inherited are typical (defeasible) and are transmitted through multiple links; in such cases, many of the issues inherent in nonmonotonic logics arise. Some procedures for making inferences in inheritance network that have both default (defeasible) and absolute knowledge (nondefeasible) are suggested. The basis for these suggestions is the use of possibility qualification for representing typical or default values\"",
        "Document: \"On the fusion of possibilistic and probabilistic information in biometric decision-making. (Svetlana Yanushkevich Special Session) Information used in decision-making in biometric systems can come from both sensors and human observers. The sensor provided information generally has a probabilistic type of uncertainty whereas human provided linguistic information typically introduces a possibilistic type of uncertainty. Here we are faced with a problem in which we must fuse information with different types of uncertainty. We provide a unified framework for the representation of these different types of information using a set measure approach for the representation of uncertain information.\"",
        "1 is \"SLIDE: A simple adaptive defuzzification method\", 2 is \"A Restart Cma Evolution Strategy With Increasing Population Size\"",
        "Given above information, for an author who has written the paper with the title \"Decision Strategies in Mediated Multiagent Negotiations: An Optimization Approach\", which reference is related? Just choose 1 or 2 without further explanation."
    ]
}