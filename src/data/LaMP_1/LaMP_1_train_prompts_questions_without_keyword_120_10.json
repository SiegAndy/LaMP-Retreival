{
    "0025": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Lower Bounds on Near Neighbor Search via Metric Expansion':",
        "Document: \"Trace reconstruction with constant deletion probability and related results. We provide several new results for the trace reconstruction problem. In this setting, a binary string yields a collection of traces, where each trace is independently obtained by independently deleting each bit with a fixed probability \u03b4. Each trace therefore consists of a random subsequence of the original sequence. Given the traces, we wish to reconstruct the original string with high probability. The questions are how many traces are necessary for reconstruction, and how efficiently can the reconstruction be performed. Our primary result is that for some universal constant \u03b3 and uniformly chosen strings of length n, for any \u03b4 n) traces in poly(n) time with high probability. We also obtain algorithms that require a number of traces exponential in \u00d5 (\u221an) for any \u03b4\"",
        "Document: \"Optimal Amortized Regret In Every Interval. Consider the classical problem of predicting the next bit in a sequence of bits. A standard performance measure is regret (loss in payoff) with respect to a set of experts. For example if we measure performance with respect to two constant experts one that always predicts 0's and another that always predicts l's it is well known that one can get regret O(root T) with respect to the best expert by using, say, the weighted majority algorithm [LW89]. But this algorithm does not provide performance guarantee in any interval. There are other algorithms (see [BM07, FSSW97, Vov99]) that ensure regret O(root x log T) in any interval of length x. In this paper we show a randomized algorithm that in an amortized sense gets a regret of O(root x) for any interval when the sequence is partitioned into intervals arbitrarily. We empirically estimated the constant in the O() for T upto 2000 and found it to be small - around 2.1. We also experimentally evaluate the efficacy of this algorithm in predicting high frequency stock data.\"",
        "Document: \"Better Bounds for Frequency Moments in Random-Order Streams. Estimating frequency moments of data streams is a very well studied problem (1-3,9,12) and tight bounds are known on the amount of space that is necessary and sufficient when the stream is adversarially ordered. Recently, motivated by various practical considerations and applications in learning and statistics, there has been growing interest into studying streams that are randomly ordered (3,4,6-8,11). In the paper we improve the previous lower bounds on the space required to estimate the frequency moments of a randomly ordered streams.\"",
        "Document: \"Algorithms for $\\ell_p$ Low-Rank Approximation. We consider the problem of approximating a given matrix by a low-rank matrix so as to minimize the entrywise $ell_p$-approximation error, for any $p geq 1$; the case $p = 2$ is the classical SVD problem. We obtain the first provably good approximation algorithms for this version of low-rank approximation that work for every value of $p geq 1$, including $p = infty$. Our algorithms are simple, easy to implement, work well in practice, and illustrate interesting tradeoffs between the approximation quality, the running time, and the rank of the approximating matrix.\"",
        "Document: \"Reducing TCAM power consumption and increasing throughput. TCAMs have been an emerging technology for packet forwarding in the networking industry. They are fast and easy to use. However, due to their inherent parallel structure they consume high power - much higher than SRAMs or DRAMs. A system using four TCAMs could consume upto 60 watts. The power issue is one of the chief disadvantages of TCAMs over RAM based methods for forwarding. For a system using multiple TCAMs we present methods to significantly reduce TCAM power consumption for forwarding, making it comparable to RAM based forwarding solutions. Using our techniques one can use a TCAM for forwarding at 3 to 4 watts worst case. Our techniques also have an interesting connotation to TCAM forwarding rates. For a static distribution of requests we present methods that make the forwarding rate of a system proportional to the number of TCAMs. So if a system has four TCAMs, one could achieve a four fold performance of that of a single TCAM for a static distribution of requests.\"",
        "Document: \"An O(log*n) approximation algorithm for the asymmetric p-center problem. The input to the asymmetricp-center problem consists of an integerpand ann\u00d7ndistance matrixDdefined on a vertex setVof sizen, wheredijgives the distance fromitoj. The distances are assumed to obey the triangle inequality. For a subsetS\u2286Vthe radius ofSis the minimum distanceRsuch that every point inVis at a distance at mostRfrom some point inS. Thep-center problem consists of picking a setS\u2286Vof sizepto minimize the radius. This problem is known to be NP-complete.\"",
        "Document: \"Design tradeoffs for SSD performance. Solid-state disks (SSDs) have the potential to revolutionize the storage system landscape. However, there is little published work about their internal organization or the design choices that SSD manufacturers face in pursuit of optimal performance. This paper presents a taxonomy of such design choices and analyzes the likely performance of various configurations using a trace-driven simulator and workload traces extracted from real systems. We find that SSD performance and lifetime is highly workload-sensitive, and that complex systems problems that normally appear higher in the storage stack, or even in distributed systems, are relevant to device firmware.\"",
        "Document: \"Approximating the smallest grammar: Kolmogorov complexity in natural models. We consider the problem of finding the smallest context-free grammar that generates exactly one given string of length n. The size of this grammar is of theoretical interest as an efficiently computable variant of Kolmogorov complexity. The problem is of practical importance in areas such as data compression and pattern extraction.The smallest grammar is known to be hard to approximate to within a constant factor, and an o(logn/log logn) approximation would require progress on a long-standing algebraic problem [10]. Previously, the best proved approximation ratio was O(n1/2) for the Bisection algorithm [8]. Our main result is an exponential improvement of this ratio; we give an O(log (n/g*)) approximation algorithm, where g* is the size of the smallest grammar.We then consider other computable variants of Kolomogorov complexity. In particular we give an O(log2 n) approximation for the smallest non-deterministic finite automaton with advice that produces a given string. We also apply our techniques to \"advice-grammars\" and \"edit-grammars\", two other natural models of string complexity.\"",
        "Document: \"Entropy based nearest neighbor search in high dimensions. In this paper we study the problem of finding the approximate nearest neighbor of a query point in the high dimensional space, focusing on the Euclidean space. The earlier approaches use locality-preserving hash functions (that tend to map nearby points to the same value) to construct several hash tables to ensure that the query point hashes to the same bucket as its nearest neighbor in at least one table. Our approach is different - we use one (or a few) hash table and hash several randomly chosen points in the neighborhood of the query point showing that at least one of them will hash to the bucket containing its nearest neighbor. We show that the number of randomly chosen points in the neighborhood of the query point q required depends on the entropy of the hash value h(p) of a random point p at the same distance from q at its nearest neighbor, given q and the locality preserving hash function h chosen randomly from the hash family. Precisely, we show that if the entropy I(h(p)|q, h) = M and g is a bound on the probability that two far-off points will hash to the same bucket, then we can find the approximate nearest neighbor in O(np) time and near linear \u00d5(n) space where p = M/log(1/g). Alternatively we can build a data structure of size \u00d5(n1/(1-p)) to answer queries in \u00d5(d) time. By applying this analysis to the locality preserving hash functions in [17, 21, 6] and adjusting the parameters we show that the c nearest neighbor can be computed in time \u00d5(np) and near linear space where p \u2248 2.06/c as c becomes large.\n\n\"",
        "Document: \"An improved algorithm finding nearest neighbor using Kd-trees. We suggest a simple modification to the Kd-tree search algorithm for nearest neighbor search resulting in an improved performance. The Kd-tree data structure seems to work well in finding nearest neighbors in low dimensions but its performance degrades even if the number of dimensions increases to more than two. Since the exact nearest neighbor search problem suffers from the curse of dimensionality we focus on approximate solutions; a c-approximate nearest neighbor is any neighbor within distance at most c times the distance to the nearest neighbor. We show that for a randomly constructed database of points if the query point is chosen close to one of the points in the data base, the traditional Kd-tree search algorithm has a very low probability of finding an approximate nearest neighbor; the probability of success drops exponentially in the number of dimensions d as e-\u03a9(d/c). However, a simple change to the search algorithm results in a much higher chance of success. Instead of searching for the query point in the Kd-tree we search for a random set of points in the neighborhood of the query point. It turns out that searching for e\u03a9(d/c) such points can find the c-approximate nearest neighbor with a much higher chance of success.\"",
        "1 is \"Clustering data streams\", 2 is \"Parallel Repetition: Simplification and the No-Signaling Case\"",
        "Given above information, for an author who has written the paper with the title \"Lower Bounds on Near Neighbor Search via Metric Expansion\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "0075": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Learning Relational Event Models from Video.':",
        "Document: \"An Autonomous Sensor for 3D Reconstruction. We describe an automated approach to the reconstruction of 3D interiors from laser range data and digital images. This is achieved using a scanning laser rangefinder and digital camera that are mounted on an autonomous mobile platform known as the AEST. The objective is to reproduce complete interiors that are accurate enough for surveying, virtual studio and Augmented Reality applications. The AEST selects and navigates to a series of capture points to progressively reconstruct a 3D textured model to the required degree of accuracy. Navigation and structural information is used to register the data from each new capture point relative to the partial model. The user interface is a web browser with a radio link to the AEST. Results can be viewed in a VRML window as they are obtained. The AEST has been developed in EU-ACTS project RESOLV.\"",
        "Document: \"Building a Model of a Road Junction Using Moving Vehicle Information. We describe a program to construct a model of a road junction using data from a single camera. The model specifies the ground plane orientation in camera coordinates and the positions of traffic lanes, and is obtained entirely from observations of vehicle movements, with no static image analysis. At present, the model is restricted to representing straight lane segments. We describe our methods for segmentation, object tracking, ground plane estimation and lane identification. Throughout, we emphasise techniques which are computationally cheap and can be used with fairly low resolution data and a low frame rate. We nevertheless obtain a reliable model by using the statistics of large numbers of vehicle movements.\"",
        "Document: \"Exploiting petri-net structure for activity classification and user instruction within an industrial setting. Live workflow monitoring and the resulting user interaction in industrial settings faces a number of challenges. A formal workflow may be unknown or implicit, data may be sparse and certain isolated actions may be undetectable given current visual feature extraction technology. This paper attempts to address these problems by inducing a structural workflow model from multiple expert demonstrations. When interacting with a naive user, this workflow is combined with spatial and temporal information, under a Bayesian framework, to give appropriate feedback and instruction. Structural information is captured by translating a Markov chain of actions into a simple place/transition petri-net. This novel petri-net structure maintains a continuous record of the current workbench configuration and allows multiple sub-sequences to be monitored without resorting to second order processes. This allows the user to switch between multiple sub-tasks, while still receiving informative feedback from the system. As this model captures the complete workflow, human inspection of safety critical processes and expert annotation of user instructions can be made. Activity classification and user instruction results show a significant on-line performance improvement when compared to the existing Hidden Markov Model or pLSA based state of the art. Further analysis reveals that the majority of our model's classification errors are caused by small de-synchronisation events rather than significant workflow deviations. We conclude with a discussion of the generalisability of the induced place/transition petri-net to other activity recognition tasks and summarise the developments of this model.\"",
        "Document: \"Building Qualitative Event Models Automatically from Visual Input. We describ e an implemented technique for generating event models automatically based on qualitative reasoning and a statistical analysis of video input. Using an existing tracking program which generates labelled contours for objects in every frame, the view from a fixed camera is partitioned into semantically relevant regions based on the paths followed by movingobjects. The paths are indexed with temporal information so objects moving along the same path at different speeds can be distinguished. Using a notion of proximity based on the speed of the moving objects and qualitative spatial reasoning techniques, event models describing the behaviour of pairs of objects can be built, again using statistical methods. The system has been tested on a traffic domain and learns various event models expressed in the qualitative calculus which represent human observable events. The system can then be used to recognise subsequent selected event occurrences or unusual behaviours.\"",
        "Document: \"Associating People Dropping off and Picking up Objects. Several interesting monitoring applications concern peop le entering a pre- scribed area, where they deposit an object in their possessi on, or collect an object deposited earlier. One example arises in the use of bi cycle racks. We propose a novel method for associating each person who deposits an object with the person who later collects it. Our main contribution is to deal with ambiguity in the visual data through the use of global constraints on what is possible. The method is evaluated on a set of practical experiments in a bicycle rack, and applied to online theft detection by comparing the colour profile of associated individuals.\"",
        "Document: \"Learning variable-length Markov models of behavior.  In recent years therehasbeen an increasedinterest in the modelling and recognition of human activities involving highly structured and semantically rich behaviour such as dance, aerobics, and sign language. A novel approachispresented for automatically acquiring stochastic models of the high-level structureof an activity without the assumption of any prior knowledge. The process involves temporal segmentation into plausible atomic behaviour components and the use of variable length Markov... \"",
        "Document: \"Egocentric activity recognition using Histograms of Oriented Pairwise Relations. This paper presents an approach for recognising activities using video from an egocentric (first-person view) setup. Our approach infers activity from the interactions of objects and hands. In contrast to previous approaches to activity recognition, we do not require to use an intermediate such as object detection, pose estimation, etc. Recently, it has been shown that modelling the spatial distribution of visual words corresponding to local features further improves the performance of activity recognition using the bag-of-visual words representation. Influenced and inspired by this philosophy, our method is based on global spatio-temporal relationships between visual words. We consider the interaction between visual words by encoding their spatial distances, orientations and alignments. These interactions are encoded using a histogram that we name the Histogram of Oriented Pairwise Relations (HOPR). The proposed approach is robust to occlusion and background variation and is evaluated on two challenging egocentric activity datasets consisting of manipulative task. We introduce a novel representation of activities based on interactions of local features and experimentally demonstrate its superior performance in comparison to standard activity representations such as bag-of-visual words.\"",
        "Document: \"Attribute Multiset Grammars for Global Explanations of Activities. Recognizing multiple interleaved activities in a video requires implicitly partitioning the detections for each activity. Furthermore, constraints between activities are impor- tant in finding valid explanations for all detections. We use Attribute Multiset Gram- mars (AMGs) as a formal representation for a domain's knowledge to encode intra- and inter-activity constraints. We show how AMGs can be used to parse all the observa- tions into 'feasible' global explanations. We also present an algorithm for building a Bayesian network (BN) given an AMG and a set of detections. The set of labellings of the BN corresponds to the set of all possible parse trees. Finding the best explanation then amounts to finding the maximum a posteriori labeling of the BN. The technique is successfully applied to two different problems including the challenging problem of associating pedestrians and carried objects entering and departing a building.\"",
        "Document: \"Extending the Point Distribution Model Using Polar Coordinates. The Point Distribution Model (PDM) has already proved useful for many tasks involving the location or tracking of deformable objects. A principal limitation lies in the fact that non-linear variation must be approximated by a combination of linear variations, resulting in a non-optimal model which can produce implausible object shapes. The Polynomial Regression PDM improves on the PDM by allowing polynomial deformation. However, computational complexity is greatly increased, and the model still fails for objects in which bending or pivot- ing occurs. We propose an extension to the PDM which selectively uses polar coordinates at little computational cost, and give examples to show that models produced are both more compact and less likely to generate implausible shapes than either of the above methods. We also give an algorithm which automatically classifies model landmark points into the Cartesian or polar domain, based on training set analysis.\"",
        "Document: \"Towards an architecture for cognitive vision using qualitative spatio-temporal representations and abduction. In recent years there has been increasing interest in constructing cognitive vision systems capable of interpreting the high level semantics of dynamic scenes. Purely quantitative approaches to the task of constructing such systems have met with some success. However, qualitative analysis of dynamic scenes has the advantage of allowing easier generalisation of classes of different behaviours and guarding against the propagation of errors caused by uncertainty and noise in the quantitative data. Our aim is to integrate quantitative and qualitative modes of representation and reasoning for the analysis of dynamic scenes. In particular, in this paper we outline an approach for constructing cognitive vision systems using qualitative spatial-temporal representations including prototypical spatial relations and spatio-temporal event descriptors automatically inferred from input data. The overall architecture relies on abduction: the system searches for explanations, phrased in terms of the learned spatio-temporal event descriptors, to account for the video data.\"",
        "1 is \"Cognitive Interpretation of Everyday Activities: Toward Perceptual Narrative Based Visuo-Spatial Scene Interpretation.\", 2 is \"Video behavior profiling for anomaly detection.\"",
        "Given above information, for an author who has written the paper with the title \"Learning Relational Event Models from Video.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00112": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Flexible Ontology Population from Text: The OwlExporter':",
        "Document: \"Parallel OWL Reasoning: Merge Classification. Our research is motivated by the ubiquitous availability of multiprocessor computers and the observation that available Web Ontology Language (OWL) reasoners only make use of a single processor. This becomes rather frustrating for users working in ontology development, especially if their ontologies are complex and require long processing times using these OWL reasoners. We present a novel algorithm that uses a divide and conquer strategy for parallelizing OWL TBox classification, a key task in description logic reasoning. We discuss some interesting properties of our algorithm, e. g., its suitability for distributed reasoning, and present an empirical study using a set of benchmark ontologies, where a speedup of up to a factor of 4 has been observed when using 8 workers in parallel.\"",
        "Document: \"Applying an ALC ABox Consistency Tester to Modal Logic SAT Problems. In this paper we present the results of applying HAM-ALC, a description logic system for ALCNR, to modal logic SAT problems.\"",
        "Document: \"A Generic Framework for Description Logics with Uncertainty. We propose an extension to Description Logics (DLs) with uncertainty which unifies and/or generalizes a number of existing frame- works for DLs with uncertainty. To this end, we first give a classification of these frameworks and identify the essential features as well as proper- ties of the various combination functions allowed in the underlying uncer- tainty formalisms they model. This also allows us express the semantics of the DL elements in a flexible manner. We illustrate how various DLs with uncertainty can be expressed in our generic framework.\"",
        "Document: \"High Performance Reasoning with Very Large Knowledge Bases. In this contribution we present an empirical analysis of the perfor- mance of the ALCN HR+ description logic system RACE applied to TBoxes with a very large number of primitive concept definitions.Adaptions of previously known techniques as well as new optimization techniques for efficiently dealing with these kinds of knowledge bases are discussed. 1 Motivation\"",
        "Document: \"A Framework for Explaining Reasoning in Description Logics. We present a resolution based framework to explain rea- soning in description logics and demonstrate its applicabil- ity to explain unsatisfiability and inconsistency queries w.r.t TBoxes and ABoxes in ALC. During the construction pro- cess, a refutation graph is used as the guide to generate expla- nations.\"",
        "Document: \"Adapting Optimization Techniques to Description Logics with Concrete Domains. In this paper, we demonstrate that the main standard optimization techniques dependency directed backtracking and model merging can be adapted to description logics with concrete domains. We propose al- gorithms for these techniques for the logics ALC(D )a ndALCRP(D). Important results of this study are (1) a new requirement for concrete domains in order to enable dependency directed backtracking for all clash types of description logics with concrete domains, and (2) the flat and deep model merging techniques can be fully adapted to ALC(D) but their applicability to the logic ALCRP(D) is limited.\"",
        "Document: \"Parallel TBox Classification in Description Logics --First Experimental Results. One of the most frequently used inference services of description logic reasoners classifies all named classes of OWL ontologies into a subsumption hierarchy. Due to emerging OWL ontologies from the web community consisting of up to hundreds of thousand of named classes and the increasing availability of multi-processor and multi-or many-core computers, we extend our work on parallel TBox classification and propose a new algorithm that is sound and complete and demonstrates in a first experimental evaluation a low overhead w.r.t. subsumption tests (less than 3%) if compared with sequential classification.\"",
        "Document: \"Using Patterns To Explain Inferences In Alchi. With the increasing number of applications of Description Logics (DLs), unsatisfiable concepts and inconsistent knowledge bases become quite common, especially when the knowledge bases are large and complex. This makes it challenging, even for experienced knowledge engineers, to identify and resolve these unsatisfiabilities and inconsistencies manually. It is thus crucial to provide services to explain how and why a result is derived. Motivated by the possibility of applying resolution technique in first-order logic to construct explanations for DLs, we present an algorithm that uses patterns to generate explanations for unsatisfiability and inconsistency reasoning in ALCHI, obtained by extending our previous work on ALC. The use of resolution proofs to provide explanations for DL reasoners is due to their focus which, through literals involved in the process, contributes directly to the contradiction, hence acting as filters to discard irrelevant information. We also establish the soundness and completeness of the algorithm. The proposed solution approach is independent of the underlying DL reasoners, which suggests its potential application for any DL framework.\"",
        "Document: \"An enhanced graph-oriented approach for change management in distributed biomedical ontologies and linked data. This paper reports the summary and results of our research on providing a graph oriented formalism to represent, analyze and validate the evolution of bio-ontologies, with emphasis on the FungalWeb Ontology. In this approach Category theory along with rule-based hierarchical distributed (HD) graph transformation have been employed to propose a more specific semantics for analyzing ontological changes and transformations between different versions of an ontology, as well as tracking the effects of a change in different levels of abstractions.\"",
        "Document: \"Flexible Ontology Population from Text: The OwlExporter. Ontology population from text is becoming increasingly important for NLP applications. Ontologies in OWL format provide for a standardized means of modeling, querying, and reasoning over large knowledge bases. Populated from natural language texts, they offer significant advantages over traditional export formats, such as plain XML. The development of text analysis systems has been greatly facilitated by modern NLP frameworks, such as the General Architecture for Text Engineering (GATE). However, ontology population is not currently supported by a standard component. We developed a GATE resource called the OwlExporter that allows to easily map existing NLP analysis pipelines to OWL ontologies, thereby allowing language engineers to create ontology population systems without requiring extensive knowledge of ontology APIs. A particular feature of our approach is the concurrent population and linking of a domain- and NLP-ontology, including NLP-specific features such as safe reasoning over coreference chains.\"",
        "1 is \"Pushing the EL Envelope\", 2 is \"Using Clustering Algorithms in Legacy Systems Remodularization\"",
        "Given above information, for an author who has written the paper with the title \"Flexible Ontology Population from Text: The OwlExporter\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00155": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Can Mean Shift Trackers Perform Better?':",
        "Document: \"Robust Codebook-Based Video Background Subtraction. Dynamic backgrounds and sudden illumination changes are two of the major problems associated with background subtraction techniques. In this paper, we present a novel approach to background subtraction that addresses both of these challenges. In particular, we present an improved codebook background modelling and subtraction technique. We utilise image segmentation on the background image and model the background with a codebook for each pixel along with a pseudo background layer. We perceive background motion as an occlusion of one background layer by a nearby background layer. In other words, sliding of one background layer over a neighbouring layer causes background motion and will hence result in false segmentation. We present our approach of codeword spreading across layer boundaries to handle background motion. Furthermore, we present a two-step update of the background codebook to handle both sudden and gradual illumination changes.\"",
        "Document: \"An Improved Segmentation Method For Non-Melanoma Skin Lesions Using Active Contour Model. Computer-Aided Diagnosis (CAD) systems are widely used to classify skin lesions in dermoscopic images. The segmentation of the lesion area is the initial and key step to automate this process using a CAD system. In this paper, an improved segmentation algorithm is developed based on the following steps: (1) color space transform to the perception-oriented CIECAM02 color model, (2) preprocessing step to correct specular reflection, (3) contrast enhancement using an homomorphic transform filter (HTF) and nonlinear sigmoidal function (NSF) and (4) segmentation with relative entropy (RE) and active contours model (ACM). To validate the proposed technique, comparisons with other three state-of-the-art segmentation algorithms were performed for 210 non-melanoma lesions. From these experiments, an average true detection rate of 91.01, false positive rate of 6.35 and an error probability of 7.8 were obtained. These experimental results indicate that the proposed technique is useful for CAD systems to detect non-melanoma skin lesions in dermoscopy images.\"",
        "Document: \"Can Mean Shift Trackers Perform Better?. Many tracking algorithms have difficulties dealing with occlusions and background clutters, and consequently don't converge to an appropriate solution. Tracking based on the mean shift algorithm has shown robust performance in many circumstances but still fails e.g. when encountering dramatic intensity or colour changes in a pre-defined neighbour hood. In this paper, we present a robust tracking algorithm that integrates the advantages of mean shift tracking with those of tracking local invariant features. These features are integrated into the mean shift formulation so that tracking is performed based both on mean shift and feature probability distributions, coupled with an expectation maximisation scheme. Experimental results show robust tracking performance on a series of complicated real image sequences.\"",
        "Document: \"An improved Internet-based melanoma screening system with dermatologist-like tumor area extraction algorithm. In this paper, we present an Internet-based melanoma screening system. Our web server is accessible from all over the world and performs the following procedures when a remote user uploads a dermoscopy image: separates the tumor area from the surrounding skin using highly accurate dermatologist-like tumor area extraction algorithm, calculates a total of 428 features for the characterization of the tumor, classifies the tumor as melanoma or nevus using a neural network classifier, and presents the diagnosis. Our system achieves a sensitivity of 85.9% and a specificity of 86.0% on a set of 1258 dermoscopy images using cross-validation.\"",
        "Document: \"Texture Segmentation of Dermoscopy Images using Gabor Filters and G-Means Clustering. Dermoscopy is one of the major imaging modalities used in the diagnosis of melanoma and other pigmented skin lesions. Due to the difficulty and subjectivity of human interpretation, automated analysis of dermoscopy images has become an important research area. Image segmentation is often the first step in this analysis. Numerous dermoscopy image segmentation methods have been proposed in the literature. Although, texture is one of the most significant features distinguishing a lesion from its surrounding skin, existing segmentation methods have solely relied on color information. By utilizing the G-means clustering algorithm that determines the number of clusters automatically, we propose a simple yet robust dermoscopy image segmentation method based on texture features. Preliminary experiments show that we can obtain good segmentation results without considering color information and texture information alone is sufficient to distinguish a lesion from its surrounding skin.\"",
        "Document: \"Rough colour quantisation. Colour quantisation algorithms are essential for displaying true colour images using a limited palette of distinct colours. The choice of a good colour palette is crucial as it directly determines the quality of the resulting image. Colour quantisation can also be seen as a clustering problem where the task is to identify those clusters that best represent the colours in an image. In this paper, we use a rough c-means clustering algorithm for colour quantisation of images. Experimental results on a standard set of images show that this rough colour quantisation approach performs significantly better than other, purpose built colour reduction algorithms.\"",
        "Document: \"Real-time implementation of order-statistics-based directional filters. Vector filters based on order-statistics have proved successful in removing impulsive noise from colour images while preserving edges and fine image details. Among these filters, the ones that involve the cosine distance function (directional filters) have particularly high computational requirements, which limits their use in time-critical applications. In this paper, we introduce two methods to speed up these filters. Experiments on a diverse set of colour images show that the proposed methods provide substantial computational gains without significant loss of accuracy.\"",
        "Document: \"Detection of Blue-White Veil Areas in Dermoscopy Images Using Machine Learning Techniques. As a result of the advances in skin imaging technology and the development of suitable image processing techniques, during the last decade, there has been a significant increase of interest in the computer-aided diagnosis of skin cancer. Dermoscopy is a non-invasive skin imaging technique which permits visualization of features of pigmented melanocytic neoplasms that are not discernable by examination with the naked eye. One of the useful features in dermoscopic diagnosis is the blue-white veil (irregular, structureless areas of confluent blue pigmentation with an overlying white \" ground-glass \" film) which is mostly associated with invasive melanoma. In this preliminary study, a machine learning approach to the detection of blue-white veil areas in dermoscopy images is presented. The method involves pixel classification based on relative and absolute color features using a decision tree classifier. Promising results were obtained on a set of 224 dermoscopy images.\"",
        "Document: \"Melanoma Classification Using Dermoscopy Imaging and Ensemble Learning. Malignant melanoma, the deadliest form of skin cancer, is one of the most rapidly increasing cancers in the world. Early diagnosis is crucial, since if detected early, it can be cured through a simple excision. In this paper, we present an effective approach to melanoma classification from dermoscopic images of skin lesions. First, we perform automatic border detection to delineate the lesion from the background skin. Shape features are then extracted from this border, while colour and texture features are obtained based on a division of the image into clinically significant regions. The derived features are then used in a pattern classification stage for which we employ a dedicated ensemble learning approach to address the class imbalance in the training data. Our classifier committee trains individual classifiers on balanced subspaces, removes redundant predictors based on a diversity measure and combines the remaining classifiers using a neural network fuser. Experimental results on a large dataset of dermoscopic skin lesion images show our approach to work well, to provide both high sensitivity and specificity, and the use of our classifier ensemble to lead to statistically better recognition performance.\"",
        "Document: \"Fast implementation of vector directional filters. Vector filters based on order-statistics have proved successful in removing impulsive noise from color images while preserving edges and fine image details. Among these filters, the ones that involve the cosine distance function (directional filters) have particularly high computational requirements, which limits their use in time critical applications. In this paper, we introduce two methods to speed up these filters. Experiments on a diverse set of color images show that the proposed methods provide substantial computational gains without significant loss of accuracy.\"",
        "1 is \"Color quantization of images\", 2 is \"Why can LDA be performed in PCA transformed space?\"",
        "Given above information, for an author who has written the paper with the title \"Can Mean Shift Trackers Perform Better?\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00209": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Bounds on the time for parallel RAM's to compute simple functions':",
        "Document: \"Functional interpretations of feasibly constructive arithmetic. A notion of feasible function of finite type based on the typed lambda calculus is introduced which generalizes the familiar type 1 polynomial-time functions. An intuitionistic theory IPV \u03c9 is presented for reasoning about these functions. Interpretations for IPV \u03c9 are developed both in the style of Kreisel's modified realizability and G\u00f6del's Dialectica interpretation. Applications include alternative proofs for Buss's results concerning the classical first-order system S 1 2 and its intuitionistic counterpart IS 1 2 as well as proofs of some of Buss's conjectures concerning IS 1 2 , and a proof that IS 1 2 cannot prove that extended Frege systems are not polynomially bounded.\"",
        "Document: \"An observation on time-storage trade off. Recently there have been several attempts to prove that every set of strings in @@@@ (i.e., recognizable in deterministic polynomial time) can be recognized in deterministic storage (log n)2. The methods used in the attempts were based on that of [1], in which it is shown that every context free language can be accepted in storage (log n)2 Our thesis in the present paper is that these attempts must fail. We define a specific set SP of strings which is clearly in @@@@, but in a certain well-defined sense cannot be recognized in storage (log n)2 using the techniques in [1]. We conjecture that no Turing machine recognizes SP within storage (log n)2, and show that if this conjecture is false, then in fact every member of @@@@ can be recognized within storage (log n)2.\"",
        "Document: \"Formal theories for linear algebra. We introduce two-sorted theories in the style of [CN10] for the complexity classes \u2295L and DET, whose complete problems include determinants over Z2 and Z, respectively. We then describe interpretations of Soltys' linear algebra theory LAp over arbitrary integral domains, into each of our new theories. The result shows equivalences of standard theorems of linear algebra over Z2 and Z can be proved in the corresponding theory, but leaves open the interesting question of whether the theorems themselves can be proved.\"",
        "Document: \"Characterizations of Pushdown Machines in Terms of Time-Bounded Computers. A class of machines called auxiliary pushdown machines is introduced. Several types of pushdown automata, including stack automata, are characterized in terms of these machines. The computing power of each class of machines in question is characterized in terms of time-bounded Turing machines, and corollaries are derived which answer some open ques- tions in the field.\"",
        "Document: \"Relativizing small complexity classes and their theories. Existing definitions of the relativizations of NC1, L and NL do not preserve the inclusions NC1 \u2286 L, NL \u2286 AC1. We start by giving the first definitions that preserve them. Here for L and NL we define their relativizations using Wilsonu0027s stack oracle model, but limit the height of the stack to a constant (instead of log(n)). We show that the collapse of any two classes in {AC0(m), TC0, NC1, L, NL} implies the collapse of their relativizations. Next we develop theories that characterize the relativizations of subclasses of P by modifying theories previously defined by the second two authors. A function is provably total in a theory iff it is in the corresponding relativized class. Finally we exhibit an oracle a that makes ACk(\u03b1) a proper hierarchy. This strengthens and clarifies the separations of the relativized theories in [Takeuti, 1995]. The idea is that a circuit whose nested depth of oracle gates is bounded by k cannot compute correctly the (k + 1) compositions of every oracle function.\"",
        "Document: \"The Solvability of the Derivability Problem for One-Normal Systems. A one-normal system is a Post production system on a finite alphabet {s1, s2, \u00b7 \u00b7 \u00b7, s&sgr;} with productions siP \u2192 PEij, where i ranges over a subset of {1, 2, \u00b7 \u00b7 \u00b7, &sgr;} and, for fixed i, j takes on the values 1, 2, \u00b7 \u00b7 \u00b7, ni. The following derivability problem is shown to be solvable for each such system: Given two words P and Q, decide whether Q can be derived from P by successive applications of the production rules. The result was proved by Hao Wang for the monogenic case (i.e., when each ni = 1).\"",
        "Document: \"Deterministic CFL's are accepted simultaneously in polynomial time and log squared space. We propose to prove the theorem in the title. Let PLOSS be the class of sets recognizable on a deterministic Turing machine simultaneously in polynomial time and log squared space. Using the notation of Bruss and Meyer [1], PLOSS &equil; &ugr;k TISP(nk,k log2n).\"",
        "Document: \"Exponential Lower Bounds for Monotone Span Programs. Monotone span programs are a linear-algebraic model of computation which were introduced by Karchmer and Wigderson in 1993 [1]. They are known to be equivalent to linear secret sharing schemes, and have various applications in complexity theory and cryptography. Lower bounds for monotone span programs have been difficult to obtain because they use non-monotone operations to compute monotone functions, in fact, the best known lower bounds are quasipolynomial for a function in (nonmonotone) P [2]. A fundamental open problem is to prove exponential lower bounds on monotone span program size for any explicit function. We resolve this open problem by giving exponential lower bounds on monotone span program size for a function in monotone P. This also implies the first exponential lower bounds for linear secret sharing schemes. Our result is obtained by proving exponential lower bounds using Razborov's rank method [3], a measure that is strong enough to prove lower bounds for many monotone models. As corollaries we obtain new proofs of exponential lower bounds for monotone formula size, monotone switching network size, and the first lower bounds for monotone comparator circuit size for a function in monotone P. We also obtain new polynomial degree lower bounds for Nullstellensatz refutations using an interpolation theorem of Pudlak and Sgall [4]. Finally, we obtain quasipolynomial lower bounds on the rank measure for the st-connectivity function, implying tight bounds for st-connectivity in all of the computational models mentioned above.\"",
        "Document: \"Storage requirements for deterministic / polynomial time recognizable languages. A striking example of practical tradeoffs between storage space and execution time is provided by the IBM 1401 Fortran compiler. On another level, there is an interesting relation between the time and storage required to recognize context free languages. The recognition algorithm in [Y] requires time no more than 0(n3), but requires at least linear storage, whereas the algorithm in [LI requires recognition space no more than 0((log n)2) and requires more than polynomial time. An intriguing question is whether (log n)2 space is enough to recognize all languages recognizable in deterministic polynomial time. The above question has been narrowed down in [C] to the storage required to recognize a particular language called SP. This paper presents further evidence in support of the conjecture that SP cannot be recognized using storage (log n)k for any k. In section 2 we consider a game on directed acyclic graphs (dags) and show that at least 0(n1/4) markers are needed to play the game on some n node dags. The 0(n1/4) bound is used in section 3 to show that a fairly general machine to recognize SP also requires 0(n1/4) storage.\"",
        "Document: \"A second-order system for polytime reasoning based on Gr\u00e4del's theorem. We introduce a second-order system V1-Horn of bounded arithmetic formalizing polynomial-time reasoning, based on Gr\u00e4del's (Theoret. Comput. Sci. 101 (1992) 35) second-order Horn characterization of P. Our system has comprehension over P predicates (defined by Gr\u00e4del's second-order Horn formulas), and only finitely many function symbols. Other systems of polynomial-time reasoning either allow induction on NP predicates (such as Buss's S21 or the second-order V11), and hence are more powerful than our system (assuming the polynomial hierarchy does not collapse), or use Cobham's theorem to introduce function symbols for all polynomial-time functions (such as Cook's PV and Zambella's P-def). We prove that our system is equivalent to QPV and Zambella's P-def. Using our techniques, we also show that V1-Horn is finitely axiomatizable, and, as a corollary, that the class of \u2200\u03a31b consequences of S21 is finitely axiomatizable as well, thus answering an open question.\"",
        "1 is \"A characterization of span program size and improved lower bounds for monotone span programs\", 2 is \"Asynchronous consensus and broadcast protocols\"",
        "Given above information, for an author who has written the paper with the title \"Bounds on the time for parallel RAM's to compute simple functions\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00223": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Perception-based global illumination, rendering, and animation techniques.':",
        "Document: \"Uniform Color Space-Based High Dynamic Range Video Compression. Recently, there has been a significant progress in the research and development of the high dynamic range (HDR) video technology and the state-of-the-art video pipelines are able to offer a higher bit depth support to capture, store, encode, and display HDR video content. In this paper, we introduce a novel HDR video compression algorithm, which uses a perceptually uniform color opponent space, a novel perceptual transfer function to encode the dynamic range of the scene, and a novel error minimization scheme for accurate chroma reproduction. The proposed algorithm was objectively and subjectively evaluated against four state-of-the-art algorithms. The objective evaluation was conducted across a set of 39 HDR video sequences, using the latest x265 10-bit video codec along with several perceptual and structural quality assessment metrics at 11 different quality levels. Furthermore, a rating-based subjective evaluation (\n<inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$n=40$ </tex-math></inline-formula>\n) was conducted with six sequences at two different output bitrates. Results suggest that the proposed algorithm exhibits the lowest coding error amongst the five algorithms evaluated. Additionally, the rate\u2013distortion characteristics suggest that the proposed algorithm outperforms the existing state-of-the-art at bitrates \u2265 0.4 bits/pixel.\"",
        "Document: \"Towards high-fidelity multi-sensory virtual environments. Virtual environments are playing an increasingly important role for training people about real world situations, especially through the use of serious games. A key concern is thus the level of realism that virtual environments require in order to have an accurate match of what the user can expect in the real world with what they perceive in the virtual one. Failure to achieve the right level of realism runs the real risk that the user may adopt a different reaction strategy in the virtual world than would be desired in reality. High-fidelity, physically-based rendering has the potential to deliver the same perceptual quality of an image as if you were \u201cthere\u201d in the real world scene being portrayed. However, our perception of an environment is not only what we see, but may be significantly influenced by other sensory inputs, including sound, smell, feel, and even taste. Computation and delivery of all sensory stimuli at interactive rates is a computationally complex problem. To achieve true physical accuracy for each of the senses individually for any complex scene in real-time is simply beyond the ability of current standard desktop computers. This paper discusses how human perception, and in particular any cross-modal effects in multi-sensory perception, can be exploited to selectively deliver high-fidelity virtual environments. Selective delivery enables those parts of a scene which the user is attending to, to be computed in high quality. The remainder of the scene is delivered in lower quality, at a significantly reduced computational cost, without the user being aware of this quality difference.\"",
        "Document: \"Perceived aliasing thresholds in high-fidelity rendering. High-fidelity rendering is very computationally expensive making it difficult to achieve interactive rates except for simple scenes. Recent selective rendering techniques [Yee et al. 2001; Cater et al. 2003], which alter the number of rays cast per pixel, have been explored to achieve this goal. These approaches have shown that rendering times can be significantly reduced without perceptual degradation. In traditional ray-traced images aliasing is removed by supersampling the image plane. In this sketch we identify the threshold at which decreasing the number or rays shot per pixel would result in no perceptual degradation. We conduct psychophysical experiments using four realistic environments and one test environment as a comparison. This test scene was designed to exhibit high spatial frequencies and thus was a worst case for aliasing. We determine the computational bound by varying the number of rays shot per pixel in both still images and animations. The lighting simulation system Radiance [Ward 1994] is adapted for use in these experiments. The results can be used in the design of more effective perceptual selective rendering algorithms; using the computatinal bound as an indication of where to threshold the rendering process [Sundstedt et al. 2005]. Selective rendering will alter this threshold based on perceptual importance of pixels within the image. This will reduce computation time while maintaining a perceptually high quality result for realistic scenes.\"",
        "Document: \"High-dynamic-range video solution. The natural world presents our visual system with a wide, ever-changing range of colors and intensities. Existing video cameras are only capable of capturing a limited part of this wide range with sufficient resolution. High-dynamic-range (HDR) images can represent most of the real world's luminances, but until now capturing HDR images with a linear-response function has been limited to static scenes. This demonstration showcases a novel complete HDR video solution. The system includes a unique HDR video camera capable of capturing a full HDTV video stream consisting of 20 f-stops dynamic range at a resolution of 1920 x 1080 pixels at 30 frames per second; an encoding method for coping with the huge amount of data generated by the camera (achieving a compression ratio of up to 100:1 and real-time decompression); and a new 22-inch desktop HDR display for directly visualizing the dynamic HDR content.\"",
        "Document: \"Instant Caching for Interactive Global Illumination. The ability to interactively render dynamic scenes with global illumination is one of the main challenges in computer graphics. The improvement in performance of interactive ray tracing brought about by significant advances in hardware and careful exploitation of coherence has rendered the potential of interactive global illumination a reality. However, the simulation of complex light transport phenomena, such as diffuse interreflections, is still quite costly to compute in real time. In this paper we present a caching scheme, termed Instant Caching, based on a combination of irradiance caching and instant radiosity. By reutilising calculations from neighbouring computations this results in a speedup over previous instant radiosity-based approaches. Additionally, temporal coherence is exploited by identifying which computations have been invalidated due to geometric transformations and updating only those paths. The exploitation of spatial and temporal coherence allows us to achieve superior frame rates for interactive global illumination within dynamic scenes, without any precomputation or quality loss when compared to previous methods; handling of lighting and material changes are also demonstrated.\"",
        "Document: \"Modeling light scattering for virtual heritage. Computer graphics, in particular high-fidelity rendering, make it possible to recreate cultural heritage on a computer, including a precise lighting simulation. Achieving maximum accuracy is of the highest importance when investigating how a site might have appeared in the past. Failure to use such high fidelity means there is a very real danger of misrepresenting the past. Although we can accurately simulate the propagation of light in the environment, little work has been undertaken into the effect that light scattering due to participating media (such as dust in the atmosphere) has on the perception of the site. In this article, we present the high-fidelity rendering pipeline including participating media. We also investigate how the appearance of an archaeological reconstruction is affected when dust is included in the simulation. The chosen site for our study is the ancient Egyptian temple of Kalabsha.\"",
        "Document: \"Selective rendering for efficient ray traced stereoscopic images. Depth-related visual effects are a key feature of many virtual environments. In stereo-based systems, the depth effect can be produced by delivering frames of disparate image pairs, while in monocular environments, the viewer has to extract this depth information from a single image by examining details such as perspective and shadows. This paper investigates via a number of psychophysical experiments, whether we can reduce computational effort and still achieve perceptually high-quality rendering for stereo imagery. We examined selectively rendering the image pairs by exploiting the fusing capability and depth perception underlying human stereo vision. In ray-tracing-based global illumination systems, a higher image resolution introduces more computation to the rendering process since many more rays need to be traced. We first investigated whether we could utilise the human binocular fusing ability and significantly reduce the resolution of one of the image pairs and yet retain a high perceptual quality under stereo viewing condition. Secondly, we evaluated subjects\u2019 performance on a specific visual task that required accurate depth perception. We found that subjects required far fewer rendered depth cues in the stereo viewing environment to perform the task well. Avoiding rendering these detailed cues saved significant computational time. In fact it was possible to achieve a better task performance in the stereo viewing condition at a combined rendering time for the image pairs less than that required for the single monocular image. The outcome of this study suggests that we can produce more efficient stereo images for depth-related visual tasks by selective rendering and exploiting inherent features of human stereo vision.\"",
        "Document: \"Expanding low dynamic range videos for high dynamic range applications. In this paper we introduce an algorithm and related methods that expand the contrast range of Low Dynamic Range (LDR) videos in order to regenerate missing High Dynamic Range (HDR) data. For content generated from single exposure LDR sequences, this is clearly an under constrained problem. We achieved the expansion by inverting established tone mapping operator, a process we term inverse tone mapping. This approach is augmented by a number of methods which help expand the luminance for the required pixels while avoiding artifacts. These methods may be used to convert the large libraries of available legacy LDR content for use, for instance, on new content-starved HDR devices. Moreover, these same methods may be used to provide animated emissive surfaces for image based lighting (IBL). We demonstrate results for all the above applications and validate the resultant HDR videos with original HDR references using the HDR Visual Difference Predictor (HDR-VDP) image metric.\"",
        "Document: \"Illuminating the past: state of the art. Virtual reconstruction and representation of historical environments and objects have been of research interest for nearly two decades. Physically based and historically accurate illumination allows archaeologists and historians to authentically visualise a past environment to deduce new knowledge. This report reviews the current state of illuminating cultural heritage sites and objects using computer graphics for scientific, preservation and research purposes. We present the most noteworthy and up-to-date examples of reconstructions employing appropriate illumination models in object and image space, and in the visual perception domain. Finally, we also discuss the difficulties in rendering, documentation, validation and identify probable research challenges for the future. The report is aimed for researchers new to cultural heritage reconstruction who wish to learn about methods to illuminate the past.\"",
        "Document: \"Perceptually guided high-fidelity rendering exploiting movement bias in visual attention. A major obstacle for real-time rendering of high-fidelity graphics is computational complexity. A key point to consider in the pursuit of \u201crealism in real time\u201d in computer graphics is that the Human Visual System (HVS) is a fundamental part of the rendering pipeline. The human eye is only capable of sensing image detail in a 2\u02c6 foveal region, relying on rapid eye movements, or saccades, to jump between points of interest. These points of interest are prioritized based on the saliency of the objects in the scene or the task the user is performing. Such \u201cglimpses\u201d of a scene are then assembled by the HVS into a coherent, but inevitably imperfect, visual perception of the environment. In this process, much detail, that the HVS deems unimportant, may literally go unnoticed. Visual science research has identified that movement in the background of a scene may substantially influence how subjects perceive foreground objects. Furthermore, recent computer graphics work has shown that both fixed viewpoint and dynamic scenes can be selectively rendered without any perceptual loss of quality, in a significantly reduced time, by exploiting knowledge of any high-saliency movement that may be present. A high-saliency movement can be generated in a scene if an otherwise static objects starts moving. In this article, we investigate, through psychophysical experiments, including eye-tracking, the perception of rendering quality in dynamic complex scenes based on the introduction of a moving object in a scene. Two types of object movement are investigated: (i) rotation in place and (ii) rotation combined with translation. These were chosen as the simplest movement types. Future studies may include movement with varied acceleration. The object's geometry and location in the scene are not salient. We then use this information to guide our high-fidelity selective renderer to produce perceptually high-quality images at significantly reduced computation times. We also show how these results can have important implications for virtual environment and computer games applications.\"",
        "1 is \"High dynamic range texture compression for graphics hardware\", 2 is \"LIBSVM: A library for support vector machines\"",
        "Given above information, for an author who has written the paper with the title \"Perception-based global illumination, rendering, and animation techniques.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00226": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'The Use of Conceptual Graphs for Interactive Student Modelling and Adaptive Web Explanations':",
        "Document: \"USFD: Twitter NER with Drift Compensation and Linked Data. This paper describes a pilot NER system for Twitter, comprising the USFD system entry to the W-NUT 2015 NER shared task. The goal is to correctly label entities in a tweet dataset, using an inventory of ten types. We employ structured learning, drawing on gazetteers taken from Linked Data, and on unsupervised clustering features, and attempting to compensate for stylistic and topic drift - a key challenge in social media text. Our result is competitive; we provide an analysis of the components of our methodology, and an examination of the target dataset in the context of this task.\"",
        "Document: \"Large-scale, parallel automatic patent annotation. When researching new product ideas or filing new patents, inventors need to retrieve all relevant pre-existing know-how and/or to exploit and enforce patents in their technological domain. However, this process is hindered by lack of richer metadata, which if present, would allow more powerful concept-based search to complement the current keyword-based approach. This paper presents our approach to automatic patent enrichment, tested in large-scale, parallel experiments on USPTO and EPO documents. It starts by defining the metadata annotation task and examines its challenges. The text analysis tools are presented next, including details on automatic annotation of sections, references and measurements. The key challenges encountered were dealing with ambiguities and errors in the data; creation and maintenance of large, domain-independent dictionaries; and building an efficient, robust patent analysis pipeline, capable of dealing with terabytes of data. The accuracy of automatically created metadata is evaluated against a human-annotated gold standard, with results of over 90% on most annotation types.\"",
        "Document: \"Tailoring Automatically Generated Hypertext. This paper describes an approach for tailoring the content and structure of automatically generated hypertext. The implemented system HYLITE is based on applied Natural Language Generation (NLG) techniques, a re-usable user modelling component (VIEWGEN), and a flexible architecture with module feedback. The user modelling component is used by the language generation modules to adapt the hypertext content and links to user beliefs and preferences and to the previous interaction. Unlike previous adaptive NLG systems, which have their own, application-specific user models, HYLITE has re-used a generic agent modelling framework (VIEWGEN) instead. Apart from avoiding the development costs of a new model, this also enabled a more extendable system architecture. Another distinct feature of our approach is making NLG techniques adaptable by the user, i.e., providing users with control over the user model and the hypertext adaptivity.\"",
        "Document: \"Classifying Twitter favorites: Like, bookmark, or Thanks?. AbstractSince its foundation in 2006, Twitter has enjoyed a meteoric rise in popularity, currently boasting over 500 million users. Its short text nature means that the service is open to a variety of different usage patterns, which have evolved rapidly in terms of user base and utilization. Prior work has categorized Twitter users, as well as studied the use of lists and re-tweets and how these can be used to infer user profiles and interests. The focus of this article is on studying why and how Twitter users mark tweets as \"favorites\"-a functionality with currently poorly understood usage, but strong relevance for personalization and information access applications. Firstly, manual analysis and classification are carried out on a randomly chosen set of favorited tweets, which reveal different approaches to using this functionality i.e., bookmarks, thanks, like, conversational, and self-promotion. Secondly, an automatic favorites classification approach is proposed, based on the categories established in the previous step. Our machine learning experiments demonstrate a high degree of success in matching human judgments in classifying favorites according to usage type. In conclusion, we discuss the purposes to which these data could be put, in the context of identifying users' patterns of interests.\"",
        "Document: \"Extracting a domain ontology from linguistic resource based on relatedness measurements. Creating domain-specific ontologies is one of the main bottlenecks in the development of the semantic Web. Learning an ontology from linguistic resources is helpful to reduce the costs of ontology creation. In this paper, we describe a method to extract the most related concepts from HowNet, a Chinese-English bilingual knowledge dictionary, in order to create a customized ontology for a particular domain. We introduce a new method to measure relatedness (rather than similarity between concepts), which overcomes some of the traditional problems associated with similar concepts being far apart in the hierarchy. Experiments show encouraging results.\"",
        "Document: \"Ontology-based information extraction for business intelligence. Business Intelligence (BI) requires the acquisition and aggregation of key pieces of knowledge from multiple sources in order to provide valuable information to customers or feed statistical BI models and tools. The massive amount of information available to business analysts makes information extraction and other natural language processing tools key enablers for the acquisition and use of that semantic information. We describe the application of ontology-based extraction and merging in the context of a practical e-business application for the EU MUSING Project where the goal is to gather international company intelligence and country/region information. The results of our experiments so far are very promising and we are now in the process of building a complete end-to-end solution.\"",
        "Document: \"Perceptron Learning for Chinese Word Segmentation. We explored a simple, fast and effective learning algorithm, the uneven margins Perceptron, for Chinese word segmen- tation. We adopted the character-based classification framework and trans- formed the task into several binary clas- sification problems. We participated the close and open tests for all the four corpora. For the open test we only used the utf-8 code knowledge for discrimi- nation among Latin characters, Arabic numbers and all other characters. Our system performed well on the as, cityu and msr corpora but was clearly worse than the best result on the pku corpus.\"",
        "Document: \"Using Uneven Margins SVM and Perceptron for Information Extraction. The classification problem derived from information extraction (IE) has an imbalanced training set. This is particularly true when learning from smaller datasets which often have a few positive training examples and many negative ones. This paper takes two popular IE algorithms -- SVM and Perceptron -- and demonstrates how the introduction of an uneven margins parameter can improve the results on imbalanced training data in IE. Our experiments demonstrate that the uneven margin was indeed helpful, especially when learning from few examples. Essentially, the smaller the training set is, the more beneficial the uneven margin can be. We also compare our systems to other state-of-the-art algorithms on several benchmarking corpora for IE.\"",
        "Document: \"The Use of Conceptual Graphs for Interactive Student Modelling and Adaptive Web Explanations. The paper discusses the use of domain ontologies encoded with conceptual graphs in two recent works in Artificial Intelligence in Education and Natural Language Processing: STyLE-OLM (an interactive learner modelling system that extracts extended models of the learners' cognition) and RYLITE+ (a natural language generation system that generates adaptive Web pages based on a learner model).\"",
        "Document: \"Hawkes Processes For Continuous Time Sequence Classification: An Application To Rumour Stance Classification In Twitter. Classification of temporal textual data sequences is a common task in various domains such as social media and the Web. In this paper we propose to use Hawkes Processes for classifying sequences of temporal textual data, which exploit both temporal and textual information. Our experiments on rumour stance classification on four Twitter datasets show the importance of using the temporal information of tweets along with the textual content.\"",
        "1 is \"Distributed Representations of Words and Phrases and their Compositionality.\", 2 is \"Debugging unsatisfiable classes in OWL ontologies\"",
        "Given above information, for an author who has written the paper with the title \"The Use of Conceptual Graphs for Interactive Student Modelling and Adaptive Web Explanations\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00304": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Collaborative mapping of an earthquake-damaged building via ground and aerial robots':",
        "Document: \"Development Of A Distributed Actuation Device Consisting Of Soft Gel Actuator Elements. It is desirable that soft objects like organs are manipulated by soft actuation devices. This paper presents a newly developed device using a large number of actuator elements made of a soft gel material, which applies a distributed driving force on objects. The material used in the device a's ICPF and the structure of the elements is EFD. Experimental results showed that an object placed on such a device could move at a speed of 0.62 mm/sec. This paper refers also to fundamental experiments and a concept for a more efficient drive.\"",
        "Document: \"Semi-Autonomous Control Of 6-Dof Crawler Robot Having Flippers For Getting Over Unknown-Steps. A rescue crawler robot having flipper arm has high ability to get over rough terrain, but it is hard to control its flippers in remote control. The authors air at development of a semi-autonomous control system for the crawler robot. In the system, moving direction is specified by an operator at remote place. Joypad (Sony PS2 controller) is used as input devise. For increasing its stability and robustness about change of the environment, its flippers are controlled according to sensor informations which can be obtained in real-time. Concretely, the robot recognizes the environment (upward step or downward step) using its physical model, its postural information, flippers' contact and distance between the body and ground, and controls these flippers. In this movie, we show you performance of our proposed control method using \"Aladdin\".\"",
        "Document: \"Envelope Effect Study On Collision Vibration Perception Through Investigating Just Noticeable Difference Of Time Constant. It is known that human can feel the vibratory envelope on periodic sinusoidal vibrations; however, the role of the envelope on non-periodic transient vibrations is still not clear. This study investigated Just Noticeable Difference (JND) of the time constant in the decaying sinusoidal vibration model, which is one reasonable factor relating to discrimination of tapped materials. The authors conducted psychophysical experiments to evaluate JNDs for two referenced time constants (10.8 and 50 ms) among five frequencies (150, 250, 500, 800 and 1000 Hz). The analysis showed that significant frequency effect on JNDs was only found for the lower JNDs of reference time constant 50 ms. JNDs of the time constant was lower around 250 Hz (150 to 500 Hz, average JND was 12.8%) and was higher at high frequencies (800 to 1000 Hz, average JND was 27.9%). No significant frequency effects were found in the upper JNDs of reference 50 ms (average JND was 23%) and upper JNDs of reference 10.8 ms (average JND was 65%).\"",
        "Document: \"The Tournament Results of the Different Leagues of RoboCup-2001. A complete overview of all results from the RoboCup 2001 tournaments in the different leagues is given here. In most leagues,\n the competition consisted of two parts. In the first part, teams were placed in groups were they played round robin games.\n After this, the teams were ranked in their groups. Based on the ranking, a second part with elimination rounds took place\n to determine the places.\n \"",
        "Document: \"Virtual Active Touch: Perception of Virtual Gratings Wavelength through Pointing-Stick Interface. Tactile feedback enhances the usability and enjoyment of human-computer interfaces. Many feedback techniques have been devised to present tactile stimuli corresponding to a user's hand movements taking account of the concept of active touch. However, hand movements may not necessarily be required for achieving natural tactile feedback. Here, we propose a virtual-active-touch method that achieves haptic perception without actual/direct hand movements. In this method, a cursor manipulated by a force-input device is regarded as a virtual finger of the operator on the screen. Tactile feedback is provided to the operator in accordance with cursor movements. To validate the translation of virtual roughness gratings, we compare the virtual-active-touch interface with an interface that involves actual hand movements. By using the appropriate force-to-velocity gain for the pointing-stick interface, we show that the virtual-active-touch method presents the surface wavelengths of the gratings, which is a fundamental property for texture roughness, and that the gain significantly influences the textures experienced by the operators. Furthermore, we find that the perceived wavelengths of objects scaled and viewed on a small screen are skewed. We conclude that although some unique problems remain to be solved, we may be able to perceive the surface wavelengths solely with the intentions of active touch through virtual-active-touch interfaces.\"",
        "Document: \"Estimation of ground surface radiation sources from dose map measured by moving dosimeter and 3D map. The Fukushima nuclear power plant accident that occurred in 2011 eastern Japan has created local spots with a high dose level in residential areas in eastern Japan. It is necessary to build a map of the distribution of radiation activities that shows the locations and intensities of radiation sources to assist in decontamination work. The proposed method estimates the intensities of point-like sources from the radiation dose measured by moving a dosimeter in three dimensions and 3D map of the environment, assuming that the radioactive sources are located on the ground surface. The radiation dose is measured as a few counts including a wide variability because in the proposed method, low levels of radiation are measured by moving a dosimeter. For estimating the intensities, the maximum a posteriori probability (MAP) estimation is computed on the basis of the radiation characteristics of attenuation and stochastic counts. Experiments of estimation in real environments were conducted. The experimental results showed that using the proposed method, it is possible to estimate the distributions of radiation sources using the radiation dose measured by a dosimeter and the measured 3D shape of the ground and building surface.\"",
        "Document: \"Shared autonomy system for tracked vehicles on rough terrain based on continuous three-dimensional terrain scanning. Tracked vehicles are frequently used as search-and-rescue robots for exploring disaster areas. To enhance their ability to traverse rough terrain, some of these robots are equipped with swingable subtracks. However, manual control of such subtracks also increases the operator's workload, particularly in teleoperation with limited camera views. To eliminate this trade-off, we have developed a shared autonomy system using an autonomous controller for subtracks that is based on continuous three-dimensional terrain scanning. Using this system, the operator has only to specify the direction of travel to the robot, following which the robot traverses rough terrain using autonomously generated subtrack motions. In our system, real-time terrain slices near the robot are obtained using two or three LIDAR (laser imaging detection and ranging) sensors, and these terrain slices are integrated to generate three-dimensional terrain information. In this paper, we introduce an autonomous controller for subtracks and validate the reliability of a shared autonomy system on actual rough terrains through experimental results. \u00a9 2011 Wiley Periodicals, Inc.\"",
        "Document: \"On motion planning of mobile robots which coexist and cooperate with human. In this paper a motion planning method for mobile robots that coexist and cooperate with human beings to avoid collision is proposed. Human motion, which has large uncertainty, is predicted by a motion predictor using a stochastic process model to generate probability distribution maps of human existence. An evaluating function is defined considering the danger of collision and efficiency of trajectories. A genetic algorithm determines the robot trajectory by optimizing the function. This procedure is repeated for each sampling time. The robot moves maintaining high safety against various possible human motion. Simulation results revealed that robots can reach goals avoiding danger along appropriate trajectories and that this method is effective especially for the case where optimal trajectories dynamically vary according to change of human motion.\"",
        "Document: \"Remote vertical exploration by Active Scope Camera into collapsed buildings. Remote robotic explorations for collapsed buildings in a severe disaster are demanded. However, rescue robots cannot approach the rubble due to safety risks. This study proposes a remote vertical exploration system for collapsed buildings with a robotic inspection system hoisted by a crane. An Active Scope Camera (ASC) has many advantages for the vertical exploration such as a light and flexible continuum body to produce distributed driving forces. The purpose of this paper is to confirm the feasibility of the vertical exploration system with the ASC. The vertical explorations have proper problems related to contact and hanging conditions of the scope cable. We developed a new ASC that has a two-step bending mechanism to produce larger head movement in multi-DOF. We also evaluated the performances of the prototype when the contact areas were small. Finally, we conducted a remote vertical exploration experiments at the simulated collapsed building in 6 m height. The robot could explore in six different pathways by changing head directions and running the rubbles within seven trials. The experimental results showed that the proposed system has high potential to get inserted in the deep area in the rubble.\"",
        "Document: \"Real-Time Robot Trajectory Estimation And 3d Map Construction Using 3d Camera. Our research objective is Simultaneous Localization and Mapping (SLAM) in rubble environment. The map construction requires estimation of robot trajectory in 3D space. However, it is hard to estimate it by using odometry or gyro in rubble. In this paper, the authors proposed real-time SLAM based on 3D scan match. 3D camera is used for measurement of 3D shape and its texture in real-time. 3D map and robot trajectory are estimated by combining these 3D scan data. ICP algorithm is used for the matching method. The authors modified ICP algorithm as fast and robust one for real-time 3D map construction.\"",
        "1 is \"Hierarchical adaptive planning in environments with uncertain, spatially-varying disturbance forces\", 2 is \"Tactile Displays: Guidance for Their Design and Application.\"",
        "Given above information, for an author who has written the paper with the title \"Collaborative mapping of an earthquake-damaged building via ground and aerial robots\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00306": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Molecular binding in structure-based drug design: a case study of the population-based annealing genetic algorithms':",
        "Document: \"The synthesis rules in a Chinese text-to-speech system. The synthesis rules developed for a successfully implemented Chinese text-to-speech system are described in detail. The design approach is based on a syllable concatenation that is rooted in the special characteristics of the Chinese language. Special attention given to the lexical tones and other prosodic rules, such as concatenation rules, sandhi rules, stress rules, intonation patterns, syllable duration rules, pause insertion rules, and energy modification rules. The rules are derived from the acoustic properties of Mandarin Chinese and therefore are useful not only in designing other Chinese text-to-speech systems, but also in understanding the characteristics of Mandarin sentences and processing Mandarin speech signals for other purposes such as segmentation or recognition\"",
        "Document: \"A Novel Real Time Monitor System of 3D Printing Layers for Better Slicing Parameter Setting. We proposed a novel real time monitor system of 3D printer with dual cameras, which capture and reconstruct the printed result layer by layer. With the reconstructed image, we can apply computer vision technique to evaluate the difference with the ideal path generate by G-code. The difference gives us clues to classify which might be the possible factor of the result. Hence we can produce advice to user for better slicing parameter settings. We believe that this system can give helps to beginner or users of 3D printer that struggle in parameter settings in the future.\n\n\"",
        "Document: \"Automatic Animated Face Modeling Using Multiview Video. An image-based 3-D modeling system is presented in this paper. Our modeling system consists of three main stages: camera calibration, depth estimation and 3-D geometry reconstruction. All of these steps are executed automatically. In the camera calibration stage, some patterns are used that help to determine the camera's position in an environment. The camera's intrinsic and external parameters are determined using epipolar geometry. After the camera parameters are determined, the camera's location in each projected frame is determined. The depth for each pixel in a base image is estimated from the camera's focus to the object's surface by measuring the similarity between the base image and the neighboring images. The object's 3-D geometry is reconstructed with texture from the base image using the depth information.\"",
        "Document: \"Project GROPEHaptic displays for scientific visualization. We began in 1967 a project to develop a haptic+display for 6-D force fields of interacting protein molecules. We approached it in four stages: a 2-D system, a 3-D system tested with a simple task, a 6-D system tested with a simple task, and a full 6-D molecular docking system, our initial goal. This paper summarizes the entire project---the four systems, the evaluation experiments, the results, and our observations. The molecular docking system results are new.Our principal conclusions are:\u2022 Haptic display as an augmentation to visual display can improve perception and understanding both of force fields and of world models populated with impenetrable objects.\u2022 Whereas man-machine systems can outperform computer-only systems by orders of magnitude on some problems, haptic-augmented interactive systems seem to give about a two-fold performance improvement over purely graphical interactive systems. Better technology may give somewhat more, but a ten-fold improvement does not seem to be in the cards.\u2022 Chemists using GROPE-III can readily reproduce the true docking positions for drugs whose docking is known (but not to them) and can find very good docks for drugs whose true docks are unknown. The present tool promises to yield new chemistry research results; it is being actively used by research chemists.\u2022 The most valuable result from using GROPE-III for drug docking is probably the radically improved situation awareness that serious users report. Chemists say they have a new understanding of the details of the receptor site and its force fields, and of why a particular drug docks well or poorly.\u2022 We see various scientific/education applications for haptic displays but believe entertainment, not scientific visualization, will drive and pace the technology.\u2022 The hardware-software system technology we have used is barely adequate, and our experience sets priorities for future development.\u2022 Some unexpected perceptual phenomena were observed. All of these worked for us, not against us.\"",
        "Document: \"Automatic Chinese food identification and quantity estimation. Computer-aided food identification and quantity estimation have caught more attention in recent years because of the growing concern of our health. The identification problem is usually defined as an image categorization or classification problem and several researches have been proposed. In this paper, we address the issues of feature descriptors in the food identification problem and introduce a preliminary approach for the quantity estimation using depth information. Sparse coding is utilized in the SIFT and Local binary pattern feature descriptors, and these features combined with gabor and color features are used to represent food items. A multi-label SVM classifier is trained for each feature, and these classifiers are combined with multi-class Adaboost algorithm. For evaluation, 50 categories of worldwide food are used, and each category contains 100 photographs from different sources, such as manually taken or from Internet web albums. An overall accuracy of 68.3% is achieved, and success at top-N candidates achieved 80.6%, 84.8%, and 90.9% accuracy accordingly when N equals 2, 3, and 5, thus making mobile application practical. The experimental results show that the proposed methods greatly improve the performance of original SIFT and LBP feature descriptors. On the other hand, for quantity estimation using depth information, a straight forward method is proposed for certain food, while transparent food ingredients such as pure water and cooked rice are temporarily excluded.\"",
        "Document: \"A tool for structure alignment of molecules. In this paper, a novel tool is proposed to align two molecules (not just proteins) based on their 3D structural data, and the user can observe the result of alignment visually via the tool. Most existing tools are designed only for alignment of proteins. Here, a new tool is developed to address shared structural features between protein structures and tRNA structures, that is, molecular mimicry, although they are two very different types of molecules. In order to align two molecules A and B, geometric hashing is applied to globally find initial matching of approximately overlapped atoms, thus parts of molecule A can be matched to parts of molecule B. Next, a fine tuning process is introduced, which is based on local optimization of overlapped parts, and the iterative closest point (ICP) is used until the number of overlapped atoms within a given distance threshold can not be increased any more. The results show that our method is useful to structurally align two molecules, not restricted to align two proteins only. Besides, our tool outperforms in terms of RMSD and number of matched atom pairs in comparison to other tools.\"",
        "Document: \"An efficient representation of complex materials for real-time rendering. In this paper, we propose an appearance representation for general complex materials which can be applied in real-time rendering framework. By combining a single parametric shading function (such as the Phong model) and the proposed spatial-varying residual function (SRF), this representation can recover the appearance of complex materials with little loss of visual fidelity. The difference between the real data and the parametric shading is directly fitted by a specific function for easy reconstruction. It is simple, flexible and easy to be implemented on programmable graphics hardware. Experiments show that the mean square error (MSE) between the reconstructed appearance and real photographs is less than 5%.\"",
        "Document: \"Level-of-detail representation of bidirectional texture functions for real-time rendering. This paper presents a new technique for rendering bidirectional texture functions (BTFs) at different levels of detail (LODs). Our method first decomposes each BTF image into multiple subbands with a Laplacian pyramid. Each vector of Laplacian coefficients of a texel at the same level is regarded as a Laplacian bidirectional reflectance distribution function (BRDF). These vectors are then further compressed by applying principal components analysis (PCA). At the rendering stage, the LOD parameter for each pixel is calculated according to the distance from the viewpoint to the surface. Our rendering algorithm uses this parameter to determine how many levels of BTF Laplacian pyramid are required for rendering. Under the same sampling resolution, a BTF gradually transits to a BRDF as the camera moves away from the surface. Our method precomputes this transition and uses it for multiresolution BTF rendering. Our Laplacian pyramid representation allows real-time anti-aliased rendering of BTFs using graphics hardware. In addition to provide visually satisfactory multiresolution rendering for BTFs, our method has a comparable compression rate to the available single-resolution BTF compression techniques.\"",
        "Document: \"On Visual Similarity Based 3d Model Retrieval. A large number of 3D models are created and available on the Web, since more and more 3D modelling and digitizing tools are developed for ever increasing applications. The techniques for content-based 3D model retrieval then become necessary. In this paper, a visual similarity-based 3D model retrieval system is proposed. This approach measures the similarity among 3D models by visual similarity, and the main idea is that if two 3D models are similar they also look similar from all viewing angles. Therefore, one hundred orthogonal projections of an object, excluding symmetry, are encoded both by Zernike moments and Fourier descriptors as features for later retrieval. The visual similarity-based approach is robust against similarity transformation, noise, model de-generacy etc., and provides 42%, 94% and 25% better performance (precision-recall evaluation diagram) than three other competing approaches: (1)the spherical harmonics approach developed by Funkhouser et al., (2)the MPEG-7 Shape 3D descriptors, and (3)the MPEG-7 Multiple View Descriptor. The proposed system is on the Web for practical trial use (http://3d.csie.ntu.edu.tw), and the database contains more than 10,000 publicly available 3D models collected from WWW pages. Furthermore, a user friendly interface is provided to retrieve 3D models by drawing 2D shapes. The retrieval is fast enough on a server with Pentium IV 2.4GHz CPU, and it takes about 2 seconds and 0.1 seconds for querying directly by a 3D model and by hand drawn 2D shapes, respectively.\"",
        "Document: \"Reusable Radiosity Objects. Abstract: Because of the view independence and photo realistic image generation in diffuseenvironment, radiosity is suitable for an interactive walkthrough system. Thedrawback is time-consuming in form factor estimation, and furthermore, inserting,deleting or moving an object makes the whole costly rendering process repeat. Tosolve this problem, we encapsulate necessary information for form factor calculationand visibility estimation in each object, which is called a reusable radiosity object....\"",
        "1 is \"Predicting the effectiveness of Na\u00efve data fusion on the basis of system characteristics\", 2 is \"KINEMATICS-BASED SYNTHESIS OF REALISTIC TALKING FACES\"",
        "Given above information, for an author who has written the paper with the title \"Molecular binding in structure-based drug design: a case study of the population-based annealing genetic algorithms\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00314": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Model-based collaborative filtering as a defense against profile injection attacks':",
        "Document: \"Differential Context Relaxation for Context-Aware Travel Recommendation. Context-aware recommendation (CARS) has been shown to be an effective approach to recommendation in a number of domains. However, the problem of identifying appropriate contextual variables remains: using too many contextual variables risks a drastic increase in dimensionality and a loss of accuracy in recommendation. In this paper, we propose a novel treatment of context identifying influential contexts for different algorithm components instead of for the whole algorithm. Based on this idea, we take traditional user-based collaborative filtering (CF) as an example, decompose it into three context-sensitive components, and propose a hybrid contextual approach. We then identify appropriate relaxations of contextual constraints for each algorithm component. The effectiveness of context relaxation is demonstrated by comparison of three algorithms using a travel data set: a contenxt-ignorant approach, contextual pre-filtering, and our hybrid contextual algorithm. The experiments show that choosing an appropriate relaxation of the contextual constraints for each component of an algorithm outperforms strict application of the context.\"",
        "Document: \"Experience Discovery: hybrid recommendation of student activities using social network data. The aim of the Experience Discovery project is to recommend extracurricular activities to high school and middle school students in urban areas. In implementing this system, we have been able to make use of both usage data and data drawn from a social networking site. Using pilot data, we are able to show that very simple aggregation techniques applied to the social network can improve recommendation accuracy.\"",
        "Document: \"Personalizing Navigation in Folksonomies Using Hierarchical Tag Clustering. The popularity of collaborative tagging, otherwise known as \"folksonomies\", emanate from the flexibility they afford users in navigating large information spaces for resources, tags, or other users, unencumbered by a pre-defined navigational or conceptual hierarchy. Despite its advantages, social tagging also increases user overhead in search and navigation: users are free to apply any tag they wish to a resource, often resulting in a large number of tags that are redundant, ambiguous, or idiosyncratic. Data mining techniques such as clustering provide a means to overcome this problem by learning aggregate user models, and thus reducing noise. In this paper we propose a method to personalize search and navigation based on unsupervised hierarchical agglomerative tag clustering. Given a user profile, represented as a vector of tags, the learned tag clusters provide the nexus between the user and those resources that correspond more closely to the user's intent. We validate this assertion through extensive evaluation of the proposed algorithm using data from a real collaborative tagging Web site.\"",
        "Document: \"Integrating Context Similarity With Sparse Linear Recommendation Model. Context-aware recommender systems extend traditional recommender systems by adapting their output to users' specific contextual situations. Most of the existing approaches to context-aware recommendation involve directly incorporating context into standard recommendation algorithms (e.g., collaborative filtering, matrix factorization). In this paper, we highlight the importance of context similarity and make the attempt to incorporate it into context-aware recommender. The underlying assumption behind is that the recommendation lists should be similar if their contextual situations are similar. We integrate context similarity with sparse linear recommendation model to build a similarity-learning model. Our experimental evaluation demonstrates that the proposed model is able to outperform several state-of-the-art context-aware recommendation algorithms for the top-N recommendation task.\"",
        "Document: \"Meta-Path Selection for Extended Multi-Relational Matrix Factorization. Multi-relational matrix factorization is an effective technique for incorporating heterogenous datainto prediction tasks, such as personalized recommendation. Recent research has extended the set of relationsthat can be applied within heterogeneous network settings by composing non-local relations using network meta-paths. One of the key problems in applying this technique is that the set of possible non-local relations is essentially unbounded. In this paper, we demonstrate that an information gain based technique for heuristic pruning of relations can enhance the performance of multi-relational matrix factorization recommenders.\"",
        "Document: \"Using social tags to infer context in hybrid music recommendation. Contextual factors can greatly influence users' decisions in selecting items, such as songs when listening to music. The goal of a context-aware recommender system is to adapt its recommendations not just to the general preferences of users, but also to the context in which users are seeking those recommendations. In the domain of music recommendation, the explicit contextual factors and their values might not be known to the system, a priori. Moreover, the contextual state of a user can be dynamic and change during an interaction with the system. In this paper, we present a hybrid context-aware recommender system which infers contextual information from the sequence of songs listened to or specified by a user and uses this information to produce context-aware recommendations. Our system mines popular tags for songs from social media Web sites and uses a topic modeling approach to learn latent topics representing various contexts. We then model each song as a set of latent topics capturing the general characteristics of that song. This representation is used to track and detect changes in user's choice of music, as reflected in a playlist of song sequence, and adjust the recommendations to better meet the current context of the user. Using our approach, the contextual information can be integrated with any traditional recommendation algorithm to produce context-aware recommendations. For our system, we designed and evaluated two hybrid methods. The first hybrid combines collaborative filtering and content-based recommendation techniques, and the second hybrid additionally incorporates information about pairwise song associations. Our evaluation results show that both the hybrid approach and the contextualization can enhance the performance of baseline music recommendation method.\"",
        "Document: \"Adapting to User Preference Changes in Interactive Recommendation. Recommender systems have become essential tools in many application areas as they help alleviate information overload by tailoring their recommendations to users' personal preferences. Users' interests in items, however, may change over time depending on their current situation. Without considering the current circumstances of a user, recommendations may match the general preferences of the user, but they may have small utility for the user in his/her current situation. We focus on designing systems that interact with the user over a number of iterations and at each step receive feedback from the user in the form of a reward or utility value for the recommended items. The goal of the system is to maximize the sum of obtained utilities over each interaction session. We use a multi-armed bandit strategy to model this online learning problem and we propose techniques for detecting changes in user preferences. The recommendations are then generated based on the most recent preferences of a user. Our evaluation results indicate that our method can improve the existing bandit algorithms by considering the sudden variations in the user's feedback behavior.\"",
        "Document: \"Segment-Based Injection Attacks against Collaborative Filtering Recommender Systems. Significant vulnerabilities have recently been identi- fied in collaborative filtering recommender systems. Researchers have shown that attackers can manipulate a system's recommendations by injecting biased profiles into it. In this paper, we examine attacks that concentrate on a targeted set of users with similar tastes, biasing the system's responses to these users. We show that such attacks are both pragmatically reasonable and also highly effective against both user-based and item-based algorithms. As a result, an attacker can mount such a \"segmented\" attack with little knowledge of the specific system being targeted and with strong likelihood of success.\"",
        "Document: \"Similarity-Based Context-Aware Recommendation. Context-aware recommender systems CARS take context into consideration when modeling user preferences. There are two general ways to integrate context with recommendation: contextual filtering and contextual modeling. Currently, the most effective context-aware recommendation algorithms are based on a contextual modeling approach that estimate deviations in ratings across different contexts. In this paper, we propose context similarity as an alternative contextual modeling approach and examine different ways to represent context similarity and incorporate it into recommendation. More specifically, we show how context similarity can be integrated into the sparse linear method and matrix factorization algorithms. Our experimental results demonstrate that learning context similarity is a more effective approach to context-aware recommendation than modeling contextual rating deviations.\"",
        "Document: \"Modeling topic trends on the social web using temporal signatures. The Social Web makes visible the ebb and flow of popular interest in topics both newsworthy (\"GulfSpill\") and trivial (\"Lolcat\"). Understanding this emergent behavior is a fundamental goal for Social Web research. Key problems include discovering emergent topics from online text sources, modeling burst activity, and predicting the future trajectory of a given topic. Past work has addressed such problems individually for specific applications, but has lacked a generalizable framework for performing both classification and prediction of topic usage. Our approach is to model a topic as a temporally ordered sequence of derived feature states and capture characteristic changes in the topic trend. These sequences are drawn from a dynamic segmentation of frequency data based on change point analysis. We employ Partitioning Around Medoids clustering on these segments to produce signatures which highlight characteristic patterns of usage growth and decay. We demonstrate how this signature model can be used to define distinctive classes of topics in multiple online contexts, including tagging systems and web-based information retrieval. Additionally, we show how the model can predict the general trajectory of interest in a particular topic.\"",
        "1 is \"CoFIND \u2014 an experiment in N-dimensional collaborative filtering\", 2 is \"Using Data Mining and Recommender Systems to Facilitate Large-Scale, Open, and Inclusive Requirements Elicitation Processes\"",
        "Given above information, for an author who has written the paper with the title \"Model-based collaborative filtering as a defense against profile injection attacks\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00317": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Achieving high throughput and TCP Reno fairness in delay-based TCP over large networks':",
        "Document: \"Low lighting image enhancement using local maximum color value prior. We study the problem of low lighting image enhancement. Previous enhancement methods for images under low lighting conditions usually fail to consider the factor of image degradation during image formation. As a result, the lost contrast could not be recovered after enhancement. This paper will adaptively recover the contrast and adjust the exposure for low lighting images. Our first contribution is the modeling of image degradation in low lighting conditions. Then, the local maximum color value prior is proposed, i.e., in most regions of well exposed images, the local maximum color value of a pixel will be very high. By combining the image degradation model and local maximum color value prior, we propose to recover the un-degraded images under low lighting conditions. Last, an adaptive exposure adjustment module is proposed to obtain the final result. We show that our approach enables better enhancement comparing with popular image editing tools and academic algorithms.\"",
        "Document: \"Trellis-based R-D optimal quantization in H.263+. We describe a trellis-based algorithm which enables R-D optimum quantization decisions in the H.263+ video coding standard. The use of the trellis allows the quantization decisions for all coefficients in a block to be made jointly, and contrasts with more commonly used R-D optimizations which operate on one coefficient at a time. Experiments conducted using H.263+ for video coding rates of 40-50 kbps show an average improvement of 3.5% in bit rate, or equivalently 0.17 dB in PSNR, relative to implementations which follow the International Telecommunications Union (ITU) Test Model specifications in making quantization decisions.\"",
        "Document: \"Efficient Video Coding Using Legacy Algorithmic Approaches. We show that for high bit rates, a video coding algorithm using a suitable combination of the QM coder and on other methods first published over 20 years ago can deliver video quality rivaling that of H.264 at lower complexity. This has implications both technically, since encoders built using these methods can be more power efficient, and commercially, given the complex licensing and intellectual property issues that accompany newer coding methods such as H.264 and MPEG-4. The methods described in this paper are the basis for the recent decision of the MPEG standards group to begin work on what is referred to as the \u201cType-1 Video Coding\u201d standard, which, in addition to aiming for high coding efficiency, is intended to minimize royalty issues.\"",
        "Document: \"Accelerating HEVC using heterogeneous platforms. The high efficiency video coding (HEVC) standard achieves double compression efficiency compared with H.264/advanced video coding at the cost of huge computational complexity. Parallelizing HEVC encoding is an efficient way of fulfilling this computational requirement. The parallelization algorithms considered in HEVC, such as Tiles or wavefront parallel processing (WPP), rely on creating picture partitions that can be processed concurrently in a multi-core architecture. However, this paper focuses on the design of a heterogeneous parallel architecture composed of a graphic processing unit (GPU) plus a multi-core central processing unit (CPU) to take advantage of these techniques. Experimental results indicate that our approach outperforms WPP in terms of speed-up and reduces the delay introduced by alternative techniques such as the group of pictures-based processing pattern. Moreover, the proposed algorithms obtain speed-up values of over $$4 \\times $$4 on an Intel quad-core CPU and an NVIDIA GPU with negligible quality losses.\"",
        "Document: \"Convex Optimization Based Bit Allocation for Light Field Compression Under Weighting and Consistency Constraints. Compared with conventional image and video, light field images introduce the weight channel, as well as the visual consistency of rendered view, information that has to be taken into account when compressing the pseudo-temporal-sequence (PTS) created from light field images. In this paper, we propose a novel frame level bit allocation framework for PTS coding. A joint model that measures weighted distortion and visual consistency, combined with an iterative encoding system, yields the optimal bit allocation for each frame by solving a convex optimization problem. Experimental results show that the proposed framework is effective in producing desired distortion distribution based on weights, and achieves up to 24.7% BD-rate reduction comparing to the default rate control algorithm.\"",
        "Document: \"A pixel-based outlier-free motion estimation algorithm for scalable video quality enhancement. Scalable video quality enhancement refers to the process of enhancing low quality frames using high quality ones in scalable video bitstreams with time-varying qualities. A key problem in the enhancement is how to search for correspondence between high quality and low quality frames. Previous algorithms usually use block-based motion estimation to search for correspondences. Such an approach can hardly estimate scale and rotation transforms and always introduces outliers to the motion estimation results. In this paper, we propose a pixel-based outlier-free motion estimation algorithm to solve this problem. In our algorithm, the motion vector for each pixel is calculated with respect to estimate translation, scale, and rotation transforms. The motion relationships between neighboring pixels are considered via the Markov random field model to improve the motion estimation accuracy. Outliers are detected and avoided by taking both blocking effects and matching percentage in scale-invariant feature transform field into consideration. Experiments are conducted in two scenarios that exhibit spatial scalability and quality scalability, respectively. Experimental results demonstrate that, in comparison with previous algorithms, the proposed algorithm achieves better correspondence and avoids the simultaneous introduction of outliers, especially for videos with scale and rotation transforms.\"",
        "Document: \"TCP-ACC: An active congestion compensation TCP for wireless networks. TCP is a widely used protocol in modern communication networks. However, the performances of existing TCP congestion control algorithms degrade severely in wireless networks due to wireless-related packet losses and packet reorderings, in addition to congestion. In this paper, a novel TCP algorithm, named TCP-ACC, is proposed. The algorithm detects the level of packet reordering as well as packet losses by combining a packet reordering measurement and congestion control so as to avoid unnecessary slowing down of the data transmission rate while preventing congestion and maintaining good fairness. Theoretical analysis and experiment results show that the algorithm achieves significant throughput improvement in wireless networks as compared with other state-of-the-art algorithms.\"",
        "Document: \"A Probabilistic Approach to Identifying the Number of Frequency Hoppers for Spectrum Sensing. Characterizing the number and type of transmitters occupying a given frequency band is a critical aspect of spectrum sensing specifically and cognitive radio generally. We present an analytical framework based on probability to identify the number of frequency hopping transmitters of one specific type in a band of interest, and show that the probability mass functions associated with the different potential number of transmitters quickly becomes Gaussian as the number of channel observations increases. Simulation results confirm that the approach can lead with high probability to a correct decision regarding the number of interferers. Thus, the methods here can serve as a valuable complement to other spectrum sensing approaches.\"",
        "Document: \"Using a Vector of Observations to Identify the Number of Frequency-Hopping Transmitters. We present a time-distributed analytical framework for identifying the most likely number of frequency-hopping transmitters in an environment, considering both a perfect detection scenario as well as a scenario in which misdetection occurs. A maximum likelihood (ML) estimation formulation is adopted and a set of analytical equations are developed for the two measurement scenarios of (1) a single integrated measurement over the observation time window, and (2) an observation vector consisting of measurements at successive time instants. Simulation results confirm the correctness of the analytical formulation. We show that this approach can lead with high probability to a correct decision regarding the number of interferers, and explore the tradeoffs involved in different misdetection probabilities and in the two different measurement scenarios.\"",
        "Document: \"3G wireless multimedia: technologies and practical issues. This paper provides an overview of the emerging wireless communication standards, end-to-end wireless streaming systems, and relevant wireless multimedia technologies. It highlights some of the challenges in the deployment of 3G wireless multimedia services, using PacketVideo's solutions as an example.\"",
        "1 is \"QoS-Aware Web Service Recommendation by Collaborative Filtering\", 2 is \"A structured fixed-rate vector quantizer derived from a variable-length scalar quantizer. I. Memoryless sources\"",
        "Given above information, for an author who has written the paper with the title \"Achieving high throughput and TCP Reno fairness in delay-based TCP over large networks\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00322": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Pricing and QoS':",
        "Document: \"Specifying QoS for Multimedia Communications within Distributed Programming Environments. Because of t he increasing emphasis on d istributed ob ject programming for the provision o f multimedia telecommunications s ervices, it has become apparent t hat a unification between the distributed programming environment and techniques for QoS specification in multimedia communications must be made. Various QoS frameworks and QoS reference models have already been established and/or standardised. Each has approached the problem of QoS specification in its own way but it is possible to draw similarities across the board. This paper examines four different approaches to QoS specification and attempts to integrate these ideas into the distributed programming environment (DPE).\"",
        "Document: \"Architecture and design for resilient networked systems. There is a need for new architectures and designs of resilient networked systems that are capable of supporting critical services and infrastructures. The arguments have previously been well rehearsed, but much remains to be done, not least to demonstrate the feasibility of building such systems.\"",
        "Document: \"Group Support in Multimedia Communications Systems. Communication among multiple entities is becoming more and more widespread in computing and telecommunications. Although many existing communications protocols and services do offer some limited support for multicast or group communication, the new requirements of multipeer applications make it difficult to find efficient and comprehensive solutions. In this paper we discuss the required characteristics of group services and survey the extent of the support provided by today's services and protocols. In addition, a brief outline of standardisation efforts in this area within ISO and ITU is given and selected examples of research projects which deal with different aspects of group communication are presented.\"",
        "Document: \"Stopping small-sample stochastic approximation. The practical application of stochastic approximation methods requires a reliable means to stop the iterative process when the estimate is close to the optimizer or when further improvement in the estimate is doubtful. Conventional ideas on stopping stochastic approximation algorithms employ criteria based on a proxy distribution -- usually the asymptotic distribution. Yet difficulties may arise when applying such distributions to small (finite) samples. We propose an approach that uses the distribution of a statistically similar process called a surrogate for the proxy distribution rather than the asymptotic distribution. Under certain conditions, surrogate-based probability calculations are close to the actual probabilities. The question of how surrogate processes may be developed is also addressed. Two example applications are given.\"",
        "Document: \"Event detection and correlation for network environments. Autonomic communication has the aim of supporting fast-evolving network technologies and services, and of reducing the burden in managing complex and dynamic network environments. Networks with desirable self-* properties should be more adaptable to changing conditions and would enable greater flexibility and functional scalability. A necessary condition for realising these benefits is a heightened level of network awareness; this requires not merely the capacity to monitor the system and network state, but also the ability to characterise the operational environment and its dynamic shifts. As argued in the literature, patterns of change can be detected through cross-correlation of monitored or sensory inputs expressed in events. The profiling of the temporal ordering and other relationships encoded in these patterns can provide contextual information suitable for reasoning and adaptation tasks. In this paper, we present the design framework and initial evaluation of an Information Sensing system that aims to enable awareness through an integrated event detection-correlation mechanism. In the context of autonomic networks, it offers a more lightweight solution than traditional active database-oriented event systems, and it has better performance than log-post analysis processing; its design also provides for a distributed detection facility.\"",
        "Document: \"Game Theory for Multi-Access Edge Computing: Survey, Use Cases, and Future Trends. Game theory (GT) has been used with significant success to formulate, and either design or optimize, the operation of many representative communications and networking scenarios. The games in these scenarios involve, as usual, diverse players with conflicting goals. This paper primarily surveys the literature that has applied theoretical games to wireless networks, emphasizing use cases of upcoming multiaccess edge computing (MEC). MEC is relatively new and offers cloud services at the network periphery, aiming to reduce service latency backhaul load, and enhance relevant operational aspects such as quality of experience or security. Our presentation of GT is focused on the major challenges imposed by MEC services over the wireless resources. The survey is divided into classical and evolutionary games. Then, our discussion proceeds to more specific aspects which have a considerable impact on the game\u2019s usefulness, namely, rational versus evolving strategies, cooperation among players, available game information, the way the game is played (single turn, repeated), the game\u2019s model evaluation, and how the model results can be applied for both optimizing resource-constrained resources and balancing diverse tradeoffs in real edge networking scenarios. Finally, we reflect on lessons learned, highlighting future trends and research directions for applying theoretical model games in upcoming MEC services, considering both network design issues and usage scenarios.\"",
        "Document: \"SAND: A Scalable, Distributed and Dynamic Active Networks Directory Service. In the past a significant amount of work has been invested on architecting active node platforms that solve problems in various application areas by means of programmability. Yet, much less attention has been paid to the deployment aspects of these platforms in real networks. An open issue in particular is how active resources can be discovered and deployed. In this paper we present SAND, a scalable distributed and dynamic architecture that enables the discovery of active resources along and alongside a given network path. One of the main strengths of SAND is its customizability which renders it suitable to a multitude of network environments. As an active service, SAND does not have dependencies on any active platform and at the same time enables an active node to become part of a global infrastructure of discoverable active resources.\"",
        "Document: \"QoS Filters: Addressing the Heterogeneity Gap. Disparities in current computer technologies exist between networks, end-systems and user applications. Problems resulting from this heterogeneity gap are at their most acute in distributed multipeer environments. This paper addresses Quality of Service (QoS) disparities in heterogeneous multipeer internetworking and proposes the use of filters to bridge this aspect of the heterogeneity gap. These filter mechanisms must be sufficient adaptive to handle dynamic changes in both end-system and network capabilities. This paper discusses various filter mechanisms implemented at Lancaster University and the software developed to evaluate the feasibility of these mechanisms within a dynamic QoS controlled architecture.\"",
        "Document: \"Hydra: a decentralised group key management. Hydra is a scaleable decentralised architecture to create and distribute symmetric cryptographic keys to large multicast-based groups. The group is divided into a number of TTL-scoped regions in order to achieve flexible and efficient key management, particularly in face of group membership changes. Hydra does not employ a manager for subgroup managers, and hence, it is not vulnerable to failures of single entities.\"",
        "Document: \"Performance enhancement via two-layer support for peer-to-peer systems using active networking. The stratification of Internet protocols segregates network functionality into two broad layers: the overlay layer (application level) and the underlay layer (network level). Overlay networks are typically not aware of the operation of underlay networks, and conversely underlay networks are blind of the services executing at the overlay layer. Even though there is a prolific deployment of overlay networking based services, this architectural design is proving to have a number of deficiencies. Typically, such services make poor use of underlying networking resources, leading to degraded user-perceived quality of service. We propose a two-layer coordination and control framework aimed at optimizing network performance and enhancing user-perceived service. The framework and corresponding middleware structure make use of active networking technology. We choose peer-to-peer systems as our overlay study case, and discuss the problems associated with providing two-layer optimization and application support for such systems. Finally, we draw conclusions about the potential benefits of this approach and point towards possible directions of future work.\"",
        "1 is \"Dynamic Energy Trading for Energy Harvesting Communication Networks: A Stochastic Energy Trading Game.\", 2 is \"Caching Locator/ID mappings: An experimental scalability analysis and its implications\"",
        "Given above information, for an author who has written the paper with the title \"Pricing and QoS\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00338": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'I know what your packet did last hop: using packet histories to troubleshoot networks':",
        "Document: \"Part III: routers with very small buffers. Internet routers require buffers to hold packets during times of congestion. The buffers need to be fast, and so ideally they should be small enough to use fast memory technologies such as SRAM or all-optical buffering. Unfortunately, a widely used rule-of-thumb says we need a bandwidth-delay product of buffering at each router so as not to lose link utilization. This can be prohibitively large. In a recent paper, Appenzeller et al. challenged this rule-of-thumb and showed that for a backbone network, the buffer size can be divided by pN without sacrificing throughput, where N is the number of ows sharing the bottleneck. In this paper, we explore how buffers in the backbone can be significantly reduced even more, to as little as a few dozen packets, if we are willing to sacrifice a small amount of link capacity. We argue that if the TCP sources are not overly bursty, then fewer than twenty packet buffers are sufficient for high throughput. Specifically, we argue that O(log W) buffers are sufficient, where W is the window size of each ow. We support our claim with analysis and a variety of simulations. The change we need to make to TCP is minimal--each sender just needs to pace packet injections from its window. Moreover, there is some evidence that such small buffers are sufficient even if we don't modify the TCP sources so long as the access network is much slower than the backbone, which is true today and likely to remain true in the future. We conclude that buffers can be made small enough for all-optical routers with small integrated optical buffers.\"",
        "Document: \"Prototyping Fast, Simple, Secure Switches for Etha. We recently published our proposal for Ethane: A cleanslate approach to managing and securing enterprise networks. The goal of Ethane is to make enterprise networks (e.g. networks in companies, universities, and home offices) much easier to manage. Ethane is built on the premise that the only way to manage and secure networks is to make sure we can identify the origin of all traffic, and hold someone (or some machine) accountable for it. So first, Ethane authenticates every human, computer and switch in the network, and tracks them at all times. Every packet can be immediately identified with its sender. Second, Ethane implements a network-wide policy language in terms of users, machines and services. Before a flow is allowed into the network, it is checked against the policy.\"",
        "Document: \"Analysis of a Packet Switch with Memories Running at Slower than the Line Rate. Our work is motivated by the desire to build a very high speed packet-switch with extremely high line-rates. In this paper, we consider building a packet-switch from multiple, lower speed packet-switches operat- ing independently and in parallel. In particular, we consider a (perhaps obvi- ous) parallel packet switch (PPS) architecture in which arriving traffic is demultiplexed over identical, lower speed packet-switches, switched to the correct output port, then recombined (multiplexed) before departing from the system. Essentially, the packet-switch performs packet-by-packet load- balancing, or \"inverse-multiplexing\" over multiple independent packet- switches. Each lower-speed packet switch, operates at a fraction of the line- rate, ; for example, if each packet-switch operates at rate no memory buffers are required to operate at the full line-rate of the system. Ideally, a PPS would share the benefits of an output-queued switch; i.e. the delay of individual packets could be precisely controlled, allowing the provision of guaranteed qualities of service. In this paper, we ask the question: Is it possi- ble for a PPS to precisely emulate the behavior of an output-queued packet- switch with the same capacity and with the same number of ports? The main result of this paper is that it is theoretically possible for a PPS to emulate a FCFS output-queued packet-switch if each layer operates at a rate of approximately . This simple result is analogous to Clos' theorem for a three-stage circuit switch to be strictly non-blocking. We further show that the PPS can emulate any QoS queueing discipline if each layer operates at a rate of approximately .\"",
        "Document: \"Encouraging reusable network hardware design. The NetFPGA platform is designed to enable students and researchers to build networking systems that run at line-rate, and to create re-usable designs to share with others. Our goal is to eventually create a thriving developer-community, where developers around the world contribute reusable modules and designs for the benefit of the community as a whole. To this end, we have created a repository of ldquoUser Contributed Designsrdquo at NetFPGA.org. But creating an ldquoopen-source hardwarerdquo platform is quite different from software oriented open-source projects. Designing hardware is much more time consuming-and more error prone-than designing software, and so demands a process that is more focussed on verifying that a module really works as advertised, else others will be reluctant to use it. We have designed a novel process for contributing new designs. Each contributed design is specified entirely by a set of tests it passes. A developer includes a list of tests that their design will pass, along with an executable set of tests that the user can check against. Through this process, we hope to establish the right expectations for someone who reuses a design, and to encourage sound design practices with solid, repeatable and integrated testing. In this paper we describe the philosophy behind our process, in the hope that others may learn from it, as well as describe the details of how someone contributes a new design to the NetFPGA repository.\"",
        "Document: \"Slicing home networks. Despite the popularity of home networks, they face a number of systemic problems: (i)Broadband networks are expensive to deploy; and it is not clear how the cost can be shared by several service providers; (ii) Home networks are getting harder to manage as we connect more devices, use new applications, and rely on them for entertainment, communication and work|it is common for home networks to be poorly managed, insecure or just plain broken; and (iii) It is not clear how home networks will steadily improve, after they have been deployed, to provide steadily better service to home users. In this paper we propose slicing home networks as a way to overcome these problems. As a mechanism, slicing allows multiple service providers to share a common infrastructure; and supports many policies and business models for cost sharing. We propose four requirements for slicing home networks: bandwidth and traffic isolation between slices, independent control of each slice, and the ability to modify and improve the behavior of a slice. We explore how these requirements allow cost-sharing, outsourced management of home networks, and the ability to customize a slice to provide higher-quality service. Finally, we describe an initial prototype that we are deploying in homes.\"",
        "Document: \"The Bay Bridge: A High Speed Bridge/Router.  The Bay Bridge is a highperformancebridge/routercapableofbridgingandroutingover 100,000packetspersecondbetweentwonetworkports.Thefirstprototypeprovidesaplatformfortheinvestigationofveryhighspeedbridgingandroutinginhardware. Highthroughputisachievedbyaspecialisedprocessor:The Protocol Convert er,aprogrammabledevicefortranslatingbetweennetworkprotocols andmakingforwarding/routingdecisions.Thefirstimplementationis anencapsulatingtwo-portremotebridgewithtwonetworkinterfaces:... \"",
        "Document: \"Dynamic Algorithms with Worst-Case Performance for Packet Classification. Packet classification involves -- given a set of rules -- finding the highest priority rule matching an incoming packet. When designing packet classification algorithms, three metrics need to be considered: query time, update time and storage requirements. The algorithms proposed to-date have been heuristics that exploit structure inherent in the classification rules, and/or trade off one or more metrics for others. In this paper, we describe two new simple dynamic classification algorithms, Heap-on-Trie or HoT and Binarysearchtree-on-Trie or BoT for general classifiers. The performance of these algorithms is considered in the worst-case, i.e., without assumptions about structure in the classification rules. They are also designed to perform well (though not necessarily the \"best\") in each of the metrics simultaneously.\"",
        "Document: \"Putting home users in charge of their network. Policy-makers, ISPs and content providers are locked in a debate about who can control the Internet traffic that flows into our homes. In this paper we argue that the user, not the ISP or the content provider, should decide how traffic is prioritized to and from the home. Home users know most about their preferences, and if they can express them well to the ISP, then both the ISP and user are better off. To test the idea we built a prototype that lets users express highlevel preferences that are translated to low-level semantics and used to control the network.\"",
        "Document: \"RCP-AC: Congestion Control to Make Flows Complete Quickly in Any Environment. We believe that a congestion control algorithm should make flows finish quickly - as quickly as possible, while staying stable and fair among flows. Recently, we proposed RCP (Rate Control Protocol) which enables typical Internet-sized flows to complete one to two orders of magnitude faster than the existing (TCP Reno) and the proposed (XCP) congestion control algorithm. Like XCP, RCP uses explicit feedback from routers, but doesn't require per-packet calculations. A router maintains just one rate that it gives to all flows, making it simple and inherently fair. Flows finish quickly because RCP aggressively gives excess bandwidth to flows, making it work well in the common case. However - and this is a design tradeoff - RCP will experience short-term transient overflows when network conditions change quickly (e.g. a route change or flash crowds). In this paper we extend RCP and propose RCP-AC (Rate Control Protocol with Acceleration Control) that allows the aggressiveness of RCP to be tuned, enabling fast completion of flows over a broad set of operating conditions.\"",
        "Document: \"Leveraging SDN layering to systematically troubleshoot networks. Today's networks are maintained by \"masters of complexity\": network admins who have accumulated the wisdom to troubleshoot complex problems, despite a limiting toolset. This position paper advocates a more structured troubleshooting approach that leverages architectural layering in Software-Defined Networks (SDNs). In all networks, high-level intent (policy) must correctly map to low-level forwarding behavior (hardware configuration). In SDNs, intent is explicitly expressed, forwarding semantics are explicitly defined, and each architectural layer fully specifies the behavior of the network. Building on these observations, we show how recently-developed troubleshooting tools fit into a coherent workflow that detects mistranslations between layers to precisely localize sources of errant control logic. Our goals are to explain the overall picture, show how the pieces fit together to enable a systematic workflow, and highlight the questions that remain. Once this workflow is realized, network admins can formally verify that their network is operating correctly, automatically troubleshoot bugs, and systematically track down their root cause -- freeing admins to fix problems, rather than diagnose their symptoms.\"",
        "1 is \"Language-based control and mitigation of timing channels\", 2 is \"Space Decomposition Techniques for Fast Layer-4 Switching\"",
        "Given above information, for an author who has written the paper with the title \"I know what your packet did last hop: using packet histories to troubleshoot networks\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00356": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'On soft-input soft-output decoding using ':",
        "Document: \"Systematic recursive construction of LDPC codes. This letter presents a systematic and recursive method to construct good low-density parity-check (LDPC) codes, especially those with high rate. The proposed method uses a parity check matrix of a quasi-cyclic LDPC code with given row and column weights as a core upon which the larger code is recursively constructed with extensive use of pseudorandom permutation matrices. This construction preserves the minimum distance and girth properties of the core matrix and can generate either regular, or irregular LDPC codes. The method provides a unique representation of the code in compact notation.\"",
        "Document: \"Reliability-Based Soft-Decision Decoding with Iterative Information Set Reduction. In this correspondence, the reliability-based decoding approach using the reprocessing of the most reliable information set only is extended into the iterative reprocessing of several information sets. At the end of each information set reprocessing, some information bits are delivered by the decoder. Consequently, information sets with decreasing cardinality values are considered at each iteration. A tight upper bound on the error performance achieved by this new method is derived. Compared to previously proposed competitive approaches, this new method reduces the number of candidate codewords needed to achieve practically optimum decoding. Importantly, it also preserves the very simple structured implementation of the order statistic decoding.\"",
        "Document: \"Generalized and doubly generalized LDPC codes with random component codes for the binary erasure channel. In this paper, a method for the asymptotic analysis of generalized low-density parity-check (GLDPC) codes and doubly generalized low-density parity-check (D-GLDPC) codes over the binary erasure channel (BEC), based on extrinsic information transfer (EXIT) chart, is described. This method overcomes the problem consisting of the impossibility to evaluate the EXIT function for the check or variable component codes, in situations where the information functions or split information functions for component codes are unknown. According to the proposed technique, GLDPC codes and D-GLDPC codes where the generalized check and variable component codes are random codes with minimum distance at least 2, are considered. A technique is then developed which finds the EXIT chart for the overall GLDPC or D-GLDPC code, by evaluating the expected EXIT function for each check and variable component code. This technique is finally combined with the differential evolution algorithm in order to generate some good GLDPC and D-GLDPC edge distributions. Numerical results of long, random codes, are presented which confirm the effectiveness of the proposed approach. They also reveal that D-GLDPC codes can outperform standard LDPC codes and GLDPC codes in terms of both waterfall performance and error floor.\"",
        "Document: \"On the computation of the minimum distance of low-density parity-check codes. Low-density parity-check (LDPC) codes in their broader-sense definition are linear codes whose parity-check matrices have fewer 1s than 0s. Finding their minimum distance is therefore in general an NP-hard problem. We propose a randomized algorithm called nearest nonzero codeword search (NNCS) approach to tackle this problem for iteratively decodable LDPC codes. The principle of the NNCS approach is to search codewords locally around the all-zero codeword perturbed by minimal noise, anticipating that the resultant nearest nonzero codewords will most likely contain the minimum-Hamming- weight codeword whose Hamming weight is equal to the minimum distance of the linear code. This approach has its roots in Berrou et al.'s error-impulse method and a form of Fossorier's list decoding for LDPC codes.\"",
        "Document: \"Ordinary graphs and subplane partitions. We introduce a generalization of symmetric (v,k,@l) block designs, and show how these could potentially be used to construct projective planes of nonprime-power order.\"",
        "Document: \"Correlated MIMO Rayleigh fading systems with transmit channel state information. We analyze the capacity of correlated multiple-input-multiple-output (MIMO) channels when channel state information (CSI) is available at both the transmitter and the receiver. We show that the achievable rate by using simple beamforming can approach this capacity when the MIMO channels are highly correlated. To simplify the coding/decoding and power adaption, we apply truncated channel inversion (TCI) in MIMO correlated channels and evaluate the achievable rates. The results show that in highly correlated MIMO fading channels, beamforming combined with TCI can get close to capacity. This provides a simplified near optimum coding, as beamforming with TCI requires only one code designed for the AWGN channel and one coder/decoder pair.\"",
        "Document: \"Cryptanalysis of keystream generator by decimated sample based algebraic and fast correlation attacks. This paper proposes a novel approach for cryptanalysis of keystream generators consisting of the composition of a linear finite state machine (LFSM) and nonlinear mapping. The proposed approach includes a dedicated decimation of the sample for cryptanalysis based on the following: Suppose certain B bits of the LFSM initial state as known and identify time instances where certain arguments of the nonlinear function depend only on these B bits and are equal to zero. As opposed to previously reported methods, the proposed one also identifies and uses certain characteristics of the LFSM state-transition matrix in order to reduce the nonlinearity of the system of overdefined equations employed in an algebraic attack scenario, or to reduce the noise introduced by the linearization of the nonlinear function which corrupts the linear equations employed in a correlation attack scenario. The proposed method is employed for developing efficient algorithms for cryptanalysis of the nonlinear combination keystream generator reported at INDOCRYPT 2004.\"",
        "Document: \"Augmented Belief Propagation Decoding Of Low-Density Parity Check Codes. We propose an augmented belief propagation (BP) decoder for low-density parity check (LDPC) codes which can be utilized on memoryless or intersymbol interference channels. The proposed method is a heuristic algorithm that eliminates a large number of pseudocodewords that can cause nonconvergence in the BP decoder. The augmented decoder is a multistage iterative decoder, where, at each stage, the original channel messages on select symbol,nodes are replaced by saturated messages. The key element of the proposed method is the symbol selection process, which is based on the appropriately defined subgraphs of the code graph and/or the reliability of the information received from the channel. We demonstrate by examples that this decoder can be implemented to achieve substantial gains (compared to the standard locally-operating BP decoder) for short LDPC codes decoded on both memoryless and intersymbol interference Gaussian channels. Using the Margulis code example, we also show that the augmented decoder reduces the error floors. Finally, we discuss types of BP decoding errors and relate them to the augmented BP decoder.\"",
        "Document: \"Decoding low-density parity check codes with normalized APP-based algorithm. We propose a normalized a posteriori probability (APP) based algorithm for the decoding of low-density parity check (LDPC) codes. The normalized APP-based algorithm utilizes normalization to improve the accuracy of the soft values delivered by the simplified APP-based algorithm from one iteration to another during the iterative decoding, and can achieve very good tradeoff between decoding complexity and performance\"",
        "Document: \"Weight Distributions of Non-Binary Multi-Edge Type LDPC Code Ensembles: Analysis and Efficient Evaluation. Non-binary multi-edge type ensembles of low-density parity-check codes are analyzed in terms of non-binary codeword weight distribution and its growth rate. In particular, an exact expression of the growth rate for small weights is developed. As a side result, the stopping set distributions of these ensembles are developed. Examples of weight distributions are provided, showing that the derived closed-form expressions can be easily evaluated. The obtained results can thus be exploited to analyze and design non-binary low-density parity-check codes that fall within the multi-edge type framework such as, but not limited to, protograph-based codes.\"",
        "1 is \" techniques\", 2 is \"Prefix-synchronized run-length-limited sequences\"",
        "Given above information, for an author who has written the paper with the title \"On soft-input soft-output decoding using \", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00367": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'The Complexity of Learning Acyclic CP-Nets.':",
        "Document: \"Systematic versus non-systematic methods for solving incremental satisfiability. Propositional satisfiability (SAT) problem is fundamental to the theory of NP-completeness. Indeed, using the concept of \"polynomial-time reducibility\" all NP-complete problems can be polynomially reduced to SAT. Thus, any new technique for satisfiability problems will lead to general approaches for thousands of hard combinatorial problems. In this paper, we introduce the incremental propositional satisfiability problem that consists of maintaining the satisfiability of a propositional formula anytime a conjunction of new clauses is added. More precisely, the goal here is to check whether a solution to a SAT problem continues to be a solution anytime a new set of clauses is added and if not, whether the solution can be modified efficiently to satisfy the old formula and the new clauses. We will study the applicability of systematic and approximation methods for solving incremental SAT problems. The systematic method is based on the branch and bound technique while the approximation methods rely on stochastic local search and genetic algorithms.\"",
        "Document: \"A new temporal CSP framework handling composite variables and activity constraints. A well known approach to managing the numeric and the symbolic aspects of time is to view them as constraint satisfaction problems (CSPs). Our aim is to extend the temporal CSP formalism in order to include activity constraints and composite variables. Indeed, in many real life applications the set of variables involved by the temporal constraint problem to solve is not known in advance. More precisely, while some temporal variables (called events) are available in the initial problem, others are added dynamically to the problem during the resolution process via activity constraints and composite variables. Activity constraints allow some variables to be activated (added to the problem) when activity conditions are true. Composite variables are defined on finite domains of events. We propose in this paper two methods based respectively on constraint propagation and stochastic local search (SLS) for solving temporal constraint problems with activity constraints and composite variables. We call these problems conditional and composite temporal constraint satisfaction problems (CCTCSPs). Experimental study we conducted on randomly generated CCTCSPs demonstrates the efficiency of our exact method based on constraint propagation in the case of middle constrained and over constrained problems while the SLS based method is the technique of choice for under constrained problems and also in case we want to trade search time for the quality of the solution returned (number of solved constraints)\"",
        "Document: \"Remote Sensing, Gis and Cellular Automata for Urban Growth Simulation. Cities are complex spatial systems and modeling their dynamics of growth using traditional modeling techniques is a challenging task. Cellular automata (CA) have been widely used for modeling urban growth because of their computational simplicity, their explicit representation of time and space and their ability to generate complex patterns from the interaction of simple components of the system using simple rules. Integrating GIS tools and remote sensing data with CA has the potential to provide realistic simulation of the future urban growth of cities. The proposed approach is applied to model the growth of the City of Montreal.   Land use/land cover maps derived from Landsat data acquired in 1975 and 1990 were used to train a CA model which was then used to project the land use in 2005.\u00a0 A comparison of the projected and actual land uses for 2005   is presented and discussed.\"",
        "Document: \"Efficient Handling of Relational Database Combinatorial Queries Using CSPs. A combinatorial query is a request for tuples from multiple relations that satisfy a conjunction of constraints on tuple attribute values. Managing combinatorial queries using the traditional database systems is very challenging due to the combinatorial nature of the problem. Indeed, for queries involving a large number of constraints, relations and tuples, the response time to satisfy these queries becomes an issue. To overcome this difficulty in practice we propose a new model integrating the Constraint Satisfaction Problem (CSP) framework into the database systems. Indeed, CSPs are very popular for solving combinatorial problems and have demonstrated their ability to tackle, in an efficient manner, real life large scale applications under constraints. In order to compare the performance in response time of our CSP-based model with the traditional way for handling combinatorial queries and implemented by MS SQL Server, we have conducted several experiments on large size databases. The results are very promizing and show the superiority of our method comparing to the traditional one.\"",
        "Document: \"Solving conditional and composite constraint satisfaction problems. Constraint Satisfaction Problems (CSPs) have been widely used to solve combinatorial problems. In order to deal with dynamic CSPs where the information regarding any possible change is known a priori and can thus be enumerated beforehand, conditional constraints and composite variables have been studied in the past decade. Indeed, these two concepts allow the addition of variables and their related constraints in a dynamic manner during the resolution process. More precisely, a conditional constraint restricts the participation of a variable in a feasible scenario while a composite variable allows us to express a disjunction of variables where only one will be added to the problem to solve. In this paper we introduce a unique CSP framework including conditional constraints and composite variables. We call this model, a Conditional and Composite CSP (or CCCSP). In order to solve a CCCSP, we propose two methods respectively based on Stochastic Local Search (SLS) and backtrack search with constraint propagation. The experimental comparison of these two methods, on randomly generated consistent CCCSPs, demonstrates the efficiency of the exact method based on constraint propagation in the case of middle and under constrained problems while the SLS based method is the technique of choice for highly constrained problems and also in case we want to trade search time for the quality of the solution returned (number of solved constraints).\"",
        "Document: \"Combining Constrained CP-Nets and Quantitative Preferences for Online Shopping. Constraints and preferences coexist in a wide variety of real world applications. In a previous work we have proposed a preference-based online shopping system that handles both constraints as well as preferences where these latter can be in a qualitative or a quantitative form. Given online shoppers' requirements and preferences, the proposed system provides a set of suggested products meeting the users' needs and desires. This is an improvement to the current shopping websites where the clients are restricted to choose among a set of alternatives and not necessarily those meeting their needs and satisfaction. For a better management of constraints and preferences, we extend in this paper the well known constrained CP-Net model to quantitative constraints and integrate it into our system. This extended constrained CP-Net takes a set of constraints and preferences expressing user's requirements and desires, and returns a set of outcomes provided in the form of list of suggestions. This latter list is sorted according to user's preferences. An experimental evaluation has been conducted in order to assess the time efficiency of the proposed model to return the list of suggestions to the user. The results show that the response time is acceptable when the number of attributes is of manageable size.\"",
        "Document: \"Managing Conditional and Composite CSPs. Conditional CSPsand Composite CSPshave been known in the CSP discipline for fifteen years, especially in scheduling, planning, diagnosis and configuration domains. Basically a conditional constraintrestricts the participation of a variable in a feasible scenario while a composite variable allows us to express a disjunction of variables or sub CSPs where only one will be added to the problem to solve. In this paper we combine the features of Conditional CSPsand Composite CSPsin a unique framework that we call Conditional and Composite CSPs (CCCSPs). Our framework allows the representation of dynamic constraint problems where all the information corresponding to any possible change are available a priori. Indeed these latter information are added to the problem to solve in a dynamic manner, during the resolution process, via conditional (or activity) constraints and composite variables. A composite variable is a variable whose possible values are CSP variables. In other words this allows us to represent disjunctive variables where only one will be added to the problem to solve. An activity constraint activates a non active variable (this latter variable will be added to the problem to solve) if a given condition holds on some other active variables. In order to solve the CCCSP, we propose two methods that are respectively based on constraint propagation and Stochastic Local Search (SLS). The experimental study, we conducted on randomly generated CCCSPs demonstrates the efficiency of a variant of the MAC strategy (that we call MAC+) over the other constraint propagation techniques. We will also show that MAC+ outperforms the SLS method MCRW for highly consistent CCCSPs. MCRW is however the procedure of choice for under constrained and middle constrained problems and also for highly constrained problems if we trade search time for the quality of the solution returned (number of solved constraints).\"",
        "Document: \"Heuristic techniques for variable and value ordering in CSPs. A Constraint Satisfaction Problem (CSP) is a powerful framework for representing and solving constraint problems. When solving a CSP using a backtrack search method, one important factor that reduces the size of the search space drastically is the order in which variables and values are examined. Many heuristics for static and dynamic variable ordering have been proposed and the most popular and powerful are those that gather information about the failures during the constraint propagation phase, in the form of constraint weights. These later heuristics are called conflict driven heuristics. In this paper, we propose two of these heuristics respectively based on Hill Climbing (HC) and Ant Colony Optimization (ACO) for weighing constraints. In addition, we propose two new value ordering techniques, respectively based on HC and ACO, that rank the values based on their ability to satisfy the constraints attached to their corresponding variables. Several experiments were conducted on various types of problems including random, quasi random and patterned problems. The results show that the proposed variable ordering heuristics, are successful especially in the case of hard random problems. Also, when using the proposed value and variable ordering together, we can improve the performance particularly in the case of random problems.\"",
        "Document: \"Managing qualitative preferences and constraints in a dynamic environment. The problem of finding the set of pareto optimals for constraints and qualitative preferences together is of great interest to many application areas. It can be viewed as a preference constrained optimization problem where the goal is to find one or more feasible solutions that are not dominated by other feasible outcomes. Our work aims to enhance the current literature of the problem by providing solving methods targeting the problem in a dynamic environments. We target the problem with an eye on adopting and benefiting from the current constraint satisfaction techniques.\"",
        "Document: \"An efficient hierarchical parallel genetic algorithm for graph coloring problem. Graph coloring problems (GCPs) are constraint optimization problems with various applications including scheduling, time tabling, and frequency allocation. The GCP consists in finding the minimum number of colors for coloring the graph vertices such that adjacent vertices have distinct colors. We propose a parallel approach based on Hierarchical Parallel Genetic Algorithms (HPGAs) to solve the GCP. We also propose a new extension to PGA, that is Genetic Modification (GM) operator designed for solving constraint optimization problems by taking advantage of the properties between variables and their relations. Our proposed GM for solving the GCP is based on a novel Variable Ordering Algorithm (VOA). In order to evaluate the performance of our new approach, we have conducted several experiments on GCP instances taken from the well known DIMACS website. The results show that the proposed approach has a high performance in time and quality of the solution returned in solving graph coloring instances taken from DIMACS website. The quality of the solution is measured here by comparing the returned solution with the optimal one.\"",
        "1 is \"Self-Paced Learning for Latent Variable Models.\", 2 is \"Constraint-Based Attribute and Interval Planning\"",
        "Given above information, for an author who has written the paper with the title \"The Complexity of Learning Acyclic CP-Nets.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00374": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Service design, implementation and description (tutorial)':",
        "Document: \"Event-Based coordination of process-oriented composite applications. A process-oriented composite application aggregates functionality from a number of other applications and coordinates these applications according to a process model. Traditional approaches to develop process-oriented composite application rely on statically defined process models that are deployed into a process management engine. This approach has the advantage that application designers and users can comprehend the dependencies between the applications involved in the composition by referring to the process model. A major disadvantage however is that once deployed the behaviour of every execution of the composite application is expected to abide by its process model until this model is changed and re-deployed. This makes it difficult to enrich the application with even minor features, to plug-in new applications into the composition, or to hot-fix the composite application to meet special circumstances or demands (e.g. to personalise the application). This paper describes a technique for translating a process-oriented application into an event-based application which is more amenable to such runtime adaptation. The process-based and event-based views of the application can then co-exist and be synchronised offline if the changes become permanent and it is found desirable to reflect them in the process model.\"",
        "Document: \"Overview of some patterns for architecting and managing composite web services. The composition of Web services has gained a considerable momentum as a paradigm for enabling Business-to-Business (B2B) Collaborations. Numerous technologies supporting this new paradigm are rapidly emerging, thereby creating a need for methodologies that bring these technologies together. The identification and documentation of relevant patterns, both at the analysis and design levels, is an important step in this direction.\"",
        "Document: \"Specification of composite trading activities in supply chain management. Negotiating with suppliers and with customers is a key part of supply chain management. However, with recent technological advances, the mechanisms available to carry out such activities have become increasingly sophisticated, and the environment in which these activities take place has become highly dynamic. As a consequence, the overall planning of these complex trades, and the coordination of the various component activities, need to be carefully considered. In this setting, it is crucial that the intended behaviour, and through that, the desired outcomes, of these composite trading activities be expressed in a suitably precise manner. Using an approach based on the generation of negotiation plans, this paper describes (i) an approach to the specification of such complex activities, and (ii) a corresponding execution model. The proposal is illustrated and validated by means of a scenario taken from a recent trading agent competition.\"",
        "Document: \"A programming language for web service development. There is now widespread acceptance of Web services and service-oriented architectures. But despite the agreement on key Web services standards there remain many challenges. Programming environments based on WSDL support go some way to facilitating Web service development. However Web services fundamentally rely on XML and Schema, not on contemporary programming language type systems such as those of Java or .NET. Moreover, Web services are based on a messaging paradigm and hence bring forward the traditional problems of messaging systems including concurrency control and message correlation. It is easy to write simple synchronous Web services using traditional programming languages; however more realistic scenarios are surprisingly difficult to implement. To alleviate these issues we propose a programming language which directly supports Web service development. The language leverages XQuery for native XML processing, supports implicit message correlation and has high level join calculus-style concurrency control. We illustrate the features of the language through a motivating example.\"",
        "Document: \"Approximate clone detection in repositories of business process models. Evidence exists that repositories of business process models used in industrial practice contain significant amounts of duplication. This duplication may stem from the fact that the repository describes variants of the same processes and/or because of copy/pasting activity throughout the lifetime of the repository. Previous work has put forward techniques for identifying duplicate fragments (clones) that can be refactored into shared subprocesses. However, these techniques are limited to finding exact clones. This paper analyzes the problem of approximate clone detection and puts forward two techniques for detecting clusters of approximate clones. Experiments show that the proposed techniques are able to accurately retrieve clusters of approximate clones that originate from copy/pasting followed by independent modifications to the copied fragments.\"",
        "Document: \"Property propagation rules for prioritizing and synchronizing trading activities. With the growing number of marketplaces and trading partners in the e-commerce (electronic commerce) environment, software tools designed to act on behalf of human traders are increasingly used to automate trading activities. We describe a model for constructing trading engines, which are capable of concurrently participating in multiple interrelated negotiations with heterogeneous protocols. These tree-structured engines are configured by means of a single generic synchronization construct, which enables the incremental composition of complex trading schemes, including a number of well-known strategies from the financial trading domain. The construct is augmented by a priority-based scheduling algorithm, which selects a set of nodes for negotiation based on their estimated profit, the time remaining and the desired degree of concurrency. The model also provides iterative negotiation, which is essential in any complex trading environment.\"",
        "Document: \"Correlation Patterns in Service-Oriented Architectures. When a service engages in multiple interactions concurrently, it is generally required to correlate incoming messages with messages previously sent or received. Features to deal with this correlation requirement have been incorporated into standards and tools for service implementation, but the supported sets of features are ad hoc as there is a lack of an overarching framework from which their expressiveness can be evaluated. This paper introduces a set of patterns that provide a basis for evaluating languages and protocols for service implementation in terms of their support for correlation. The proposed correlation patterns are grounded in a formal model that views correlation mechanisms as means of grouping atomic message events into conversations and processes. The paper also provides an evaluation of relevant standards in terms of the patterns, specifically WS-Addressing and BPEL, and discusses how these standards have and could continue to evolve to address a wider set of correlation scenarios.\"",
        "Document: \"Enabling Personalized Composition and Adaptive Provisioning of Web Services. The proliferation of interconnected computing devices is fostering the emergence of environments where Web services made available to mobile users are a commodity. Unfortunately, inherent limitations of mobile devices still hinder the seamless access to Web services, and their use in supporting complex user activities. In this paper, we describe the design and implementation of a distributed, adaptive, and context-aware framework for personalized service composition and provisioning adapted to mobile users. Users specify their preferences by annotating existing process templates, leading to personalized service-based processes. To cater for the possibility of low bandwidth communication channels and frequent disconnections, an execution model is proposed whereby the responsibility of orchestrating personalized processes is spread across the participating services and user agents. In addition, the execution model is adaptive in the sense that the runtime environment is able to detect exceptions and react to them according to a set of rules.\"",
        "Document: \"Diagnosing behavioral differences between business process models: An approach based on event structures. Companies operating in multiple markets or segments often need to manage multiple variants of the same business process. Such multiplicity may stem for example from distinct products, different types of customers or regulatory differences across countries in which the companies operate. During the management of these processes, analysts need to compare models of multiple process variants in order to identify opportunities for standardization or to understand performance differences across variants. To support this comparison, this paper proposes a technique for diagnosing behavioral differences between process models. Given two process models, it determines if they are behaviorally equivalent, and if not, it describes their differences in terms of behavioral relations \u2013 like causal dependencies or conflicts \u2013 that hold in one model but not in the other. The technique is based on a translation from process models to event structures, a formalism that describes the behavior as a collection of events (task instances) connected by binary behavioral relations. A na\u00efve version of this translation suffers from two limitations. First, it produces redundant difference statements because an event structure describing a process may contain unnecessary event duplications. Second, this translation is not directly applicable to process models with cycles as the corresponding event structure is infinite. To tackle the first issue, the paper proposes a technique for reducing the number of events in an event structure while preserving the behavior. For the second issue, relying on the theory of complete unfolding prefixes, the paper shows how to construct a finite prefix of the unfolding of a possibly cyclic process model where all possible causes of every activity is represented. Additionally, activities that can occur multiple times in an execution of the process are distinguished from those that can occur at most once. The finite prefix thus enables the diagnosis of behavioral differences in terms of activity repetition and causal relations that hold in one model but not in the other. The method is implemented as a prototype that takes as input process models in the Business Process Model and Notation (BPMN) and produces difference statements in natural language. Differences can also be graphically overlaid on the process models.\"",
        "Document: \"Framework for monitoring and testing web application scalability on the cloud. By allowing resources to be acquired on-demand and in variable amounts, cloud computing provides an appealing environment for deploying pilot projects and for performance testing of Web applications and services. However, setting up cloud environments for performance testing still requires a significant amount of manual effort. To aid performance engineers in this task, we developed a framework that integrates several common benchmarking and monitoring tools. The framework helps performance engineers to test applications under various configurations and loads. Furthermore, the framework supports dynamic server allocation based on incoming load using a response-time-aware heuristics. We validated the framework by deploying and stress-testing the MediaWiki application. An experimental evaluation was conducted aimed at comparing the response-time-aware heuristics against Amazon Auto-Scale.\"",
        "1 is \"Decision procedures for multiple auctions\", 2 is \"GraphGrep: A fast and universal method for querying graphs\"",
        "Given above information, for an author who has written the paper with the title \"Service design, implementation and description (tutorial)\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00408": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'The Dagstuhl beginners guide to reproducibility for experimental networking research':",
        "Document: \"Report of the Third Workshop on the Usage of NetFlow/IPFIX in Network Management. The Network Management Research Group (NMRG) organized in 2010 the Third Workshop on the Usage of NetFlow/IPFIX in Network Management, as part of the 78th IETF Meeting in Maastricht. Yearly organized since 2007, the workshop is an opportunity for people from both academia and industry to discuss the latest developments of the protocol, possibilities for new applications, and practical experiences. This report summarizes the presentations and the main conclusions of the workshop.\"",
        "Document: \"A Customer Service Management Architecture for the Internet. Managing services on the Internet is becoming more and more complex and time consuming for service providers since services are increasing both in number and complexity. Also the number of users per service is going up. A solution to this problem is to allow the service users themselves to partly manage the services they are using. This is called Customer Service Management, and it will both increase the perceived value of the services to the users as well as lower the operational costs of service management for the service provider. This paper presents an architecture for Customer Service Management in the Internet.\"",
        "Document: \"Estimating bandwidth requirements using flow-level measurements. Bandwidth provisioning is used to dimension links so that a desired performance is met within a network (e.g. QoS metrics). Practical approaches for the estimation of bandwidth needs rely on rough traffic measurements overlooking traffic fluctuations that may have a negative impact on the network performance. On the other hand, effective theoretical solutions for estimating bandwidth needs require traffic measurements at very small timescales, which limit their applicability due to, for example, transmission rates. This paper describes our research goal of proposing a reliable and scalable solution for estimating bandwidth requirements by means of flow-level traffic measurements, as well as our proposed approach to achieve it. This research, currently in its initial phase, is planned to result in a Ph.D. thesis at the end of four years.\"",
        "Document: \"Software Defined Networking to Improve Mobility Management Performance. In mobile networks, efficient IP mobility management is a crucial issue for the mobile users changing their mobility anchor points during handover. In this regard several mobility management methods have been proposed. However, those are insufficient for the future mobile Internet in terms of scalability and resource utilization as they mostly follow the centralized management approach owning several inherent restrictions. In this research a novel mobility management approach relying on the OpenFlow-based SDN architecture is proposed. Such an approach manages mobility in a scalable fashion while optimally utilizing the available resources. This approach is also appropriate for the cloud-based Long Term Evolution (LTE) system, in order to (i) keeping sessions active during handover, and (ii) providing traffic redirection when a virtual machine (e.g., a mobility anchor point), migrates from one virtualization platform to another, while keeping the on-going sessions running, as well. This research is currently in its initial phase and is planned to eventuate as a Ph.D. thesis at the end of a four year period.\"",
        "Document: \"QoE-driven in-network optimization for Adaptive Video Streaming based on packet sampling measurements. HTTP Adaptive Streaming (HAS) is becoming the de-facto standard for adaptive streaming solutions. In HAS, a video is temporally split into segments which are encoded at different quality rates. The client can then autonomously decide, based on the current buffer filling and network conditions, which quality representation it will download. Each of these players strives to optimize their individual quality, which leads to bandwidth competition, causing quality oscillations and buffer starvations. This article proposes a solution to alleviate these problems by deploying in-network quality optimization agents, which monitor the available throughput using sampling-based measurement techniques and optimize the quality of each client, based on a HAS Quality of Experience (QoE) metric. This in-network optimization is achieved by solving a linear optimization problem both using centralized as well as distributed algorithms. The proposed hybrid QoE-driven approach allows the client to take into account the in-network decisions during the rate adaptation process, while still keeping the ability to react to sudden bandwidth fluctuations in the local network. The proposed approach allows improving existing autonomous quality selection heuristics by at least 30%, while outperforming an in-network approach using purely bitrate-driven optimization by up to 19%.\"",
        "Document: \"Inside dropbox: understanding personal cloud storage services. Personal cloud storage services are gaining popularity. With a rush of providers to enter the market and an increasing offer of cheap storage space, it is to be expected that cloud storage will soon generate a high amount of Internet traffic. Very little is known about the architecture and the performance of such systems, and the workload they have to face. This understanding is essential for designing efficient cloud storage systems and predicting their impact on the network. This paper presents a characterization of Dropbox, the leading solution in personal cloud storage in our datasets. By means of passive measurements, we analyze data from four vantage points in Europe, collected during 42 consecutive days. Our contributions are threefold: Firstly, we are the first to study Dropbox, which we show to be the most widely-used cloud storage system, already accounting for a volume equivalent to around one third of the YouTube traffic at campus networks on some days. Secondly, we characterize the workload users in different environments generate to the system, highlighting how this reflects on network traffic. Lastly, our results show possible performance bottlenecks caused by both the current system architecture and the storage protocol. This is exacerbated for users connected far from storage data-centers. All measurements used in our analyses are publicly available in anonymized form at the SimpleWeb trace repository: http://traces.simpleweb.org/dropbox/\"",
        "Document: \"Internet bad neighborhoods: the spam case. A significant part of current attacks on the Internet comes from compromised hosts that, usually, take part in botnets. Even though bots themselves can be distributed all over the world, there is evidence that most of the malicious hosts are, in fact, concentrated in small fractions of the IP address space, on certain networks. Based on that, the Bad Neighborhood concept was introduced. The general idea of Bad Neighborhoods is to rate a subnetwork by the number of malicious hosts that have been observed in that subnetwork. Even though Bad Neighborhoods were successfully employed in mail filtering, the very concept was not investigated in further details. Therefore, in this work we provide a closer look on it, by proposing four definitions for spam-based Bad Neighborhoods that take into account the way spammers operate. We apply the definitions to real world data sets and show that they provide valuable insight into the behavior of spammers and the networks hosting them. Among our findings, we show that 10% of the Bad Neighborhoods are responsible for the majority of spam.\"",
        "Document: \"Replacing the Ethernet access mechanism with the real-time access mechanism of twentenet. The way in which a Local Area Network access mechanism (Medium Access Control protocol) designed for a specific type of physical service can be used on top of another type of physical service is discussed using a particular example. In the example, an Ethernet physical layer is used to provide service to the Twentenet real-time access mechanism. Relevant Ethernet and Twentenet concepts are explained, the approach taken is introduced, and problems encountered, along with the actual synthesis of both networks, are described.\"",
        "Document: \"On the performance of grooming strategies for offloading IP flows onto lightpaths in hybrid networks. Hybrid networks take data forwarding decisions at multiple network levels. In order to make an efficient use of hybrid networks, traffic engineering solutions (e.g., routing and data grooming techniques) are commonly employed. Within the specific context of a self-managed hybrid optical and packet switching network, one important aspect to be considered is how to efficiently and autonomically move IP flows from the IP level over lightpaths at the optical level. The more IP traffic is moved (offloaded), leaving the least amount of traffic on the IP level, the better. Based on that, we investigate in this paper different strategies to move IP flows onto lightpaths while observing the percentage of offloaded IP traffic per strategy.\"",
        "Document: \"Ethics in Data Sharing: Developing a Model for Best Practice. As an outcome of a seminar on the 'Ethics in Data Sharing', we sketch a model of best practice for sharing data in research. We illustrate this model with two current and timely real-life cases from the context of computer and network security.\"",
        "1 is \"Downlink multicell processing with limited-backhaul capacity\", 2 is \"Lognormal and Pareto distributions in the Internet\"",
        "Given above information, for an author who has written the paper with the title \"The Dagstuhl beginners guide to reproducibility for experimental networking research\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00412": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'On Feasibility of 5G-Grade Dedicated RF Charging Technology for Wireless-Powered Wearables.':",
        "Document: \"Wireless Access in Vehicular Environments Using BitTorrent and Bargaining. Wireless Access in Vehicular Environment (WAVE) technology such as IEEE 802.11p has emerged as a state-of-the-art solution to vehicular communications. The major challenges in WAVE arise due to the fast changing communication environment and short durations of communications due to the mobility. As a result, it is difficult to transmit a large amount of data in such a network for vehicle-to-roadside and/or vehicle-to-vehicle communications. To overcome this problem, we propose a solution based on the idea of BitTorrent used for peer-to-peer networking, and the concept of bargaining game used in game theory. Similar to the distribution of data to peers in BitTorrent, the roadside units (RSUs) randomly distribute the data to the passing vehicles. Then, the on board units (OBUs) on the vehicles with different data, exchange the information among each other using bargaining considering channel adaptations and fairness in their achieved utility. We formulate two optimization problems - one for the RSUs and the other for the OBUs. For OBUs, the bargaining solutions are proposed which are based on three fairness criteria. For RSUs, depending on the traffic pattern, distribution of packets to the OBUs is optimized considering the different priority of the packets so that the overall utilities of the OBUs are maximized. Simulation results show that the proposed schemes can ensure fairness among the OBUs, and adapt to different traffic scenarios with different vehicular traffic intensity.\"",
        "Document: \"Robust Scheduling and Power Control for Vertical Spectrum Sharing in STDMA Wireless Networks. We study the robust transmission scheduling and power control problem for spectrum sharing between secondary and primary users in a spatial reuse time-division multiple access (STDMA) network. The objective is to find a robust minimum-length schedule for secondary users (in terms of time slots) subject to the interference constraints for primary users and the traffic demand of secondary users. We consider the fact that power allocation based on average (or estimated) link gains can be improper since actual link gains can be different from the average link gains. Therefore, transmission of the secondary links may fail and require more time slots. We also consider this demand uncertainty arising from channel gain uncertainty. We propose a column generation-based algorithm to solve the scheduling and power control problem for secondary users. The column generation method breaks the problem down to a restricted master problem and a pricing problem. However, the classical column generation method can have convergence problem due to primal degeneracy. We propose an improved column generation algorithm to stabilize and accelerate the column generation procedure by using the perturbation and exact penalty methods. Furthermore, we propose an efficient heuristic algorithm for the pricing problem based on a greedy algorithm. For the simulation scenario considered in this paper, the proposed stabilized column generation algorithm can obtain the optimal schedules with 18.85% reduction of the number of iterations and 0.29% reduction of the number of time slots. Also, the heuristic algorithm can achieve the optimality with 0.39% of cost penalty but 1.67\u00d710-4 times reduction of runtime.\"",
        "Document: \"A Microeconomic Model for Hierarchical Bandwidth Sharing in Dynamic Spectrum Access Networks. We consider the problem of hierarchical bandwidth sharing in dynamic spectrum access (or cognitive radio) environment. In the system model under consideration, licensed service (i.e., primary service) can share/sell its available bandwidth to an unlicensed service (i.e., secondary service), and again, this unlicensed service can share/sell its allocated bandwidth to other services (i.e., tertiary and quaternary services). We formulate the problem of hierarchical bandwidth sharing as an interrelated market model used in microeconomics for which a multiple-level market is established among the primary, secondary, tertiary, and quaternary services. We use the concept of demand and supply functions to obtain the equilibrium at which all the services are satisfied with the amount of allocated bandwidth and the price. These demand and supply functions are derived based on the utility of the connections using the different services (i.e., primary, secondary, tertiary, and quaternary services). For distributed implementation of the hierarchical bandwidth sharing model in a system in which global information is not available, iterative algorithms are proposed through which each service adapts its strategies to reach the equilibrium. The system stability condition is analyzed for these algorithms. Finally, we demonstrate the application of the proposed model to achieve dynamic bandwidth sharing in an integrated WiFi-WiMAX network.\"",
        "Document: \"On Multiuser Resource Allocation in Relay-Based Wireless-Powered Uplink Cellular Networks. We propose relay-based wireless-powered uplink cellular networks in which users first harvest energy from RF transmissions of base station/relay nodes and then use that energy for uplink transmission. Given the limited total transmission time and available energy at the relay node, we propose different resource allocation frameworks for the proposed relay-based networks considering two different r...\"",
        "Document: \"Markov-based analysis of end-to-end batch transmission in a multi-hop wireless network. We present a novel model for analyzing end-to-end transmission of a batch of packets in a multi-hop wireless network with automatic repeat request (ARQ)-based error control mechanism implemented at each node. For a batch of packets, we derive complete statistics (in terms of probability mass function) for end-to-end latency and the number of packets successfully delivered to the destination node. The analytical model is validated by means of simulation. Typical numerical results obtained from the model reveal the trade-off between end-to-end latency and reliability which would be an important issue in design and engineering of multi-hop wireless networks. The presented analytical model would be useful is analyzing and optimizing flow control and congestion control protocols in multi-hop wireless networks such as sensor networks.\"",
        "Document: \"Tcp Performance Under Dynamic Link Adaptation In Cellular Multi-Rate Wcdma Networks. This paper models and analyzes the performance of TCP (Transmission Control Protocol) under joint rate and power adaptation with constrained BER requirements for downlink data transmission in a multi-cell VSF (Variable Spreading Factor WCDMA system. The performance of TCP in a wide-area internet(1) environment is evaluated by using computer simulations considering user mobility, short-term fading (i.e., multipath fading) and long-term fading (i.e., shadowing). The motivation is to explore the inter-layer protocol interactions and to identify suitable transport and radio link layer mechanisms to improve wireless TCP performance in a cellular WCDMA environment.\"",
        "Document: \"Cooperative Packet Delivery in Hybrid Wireless Mobile Networks: A Coalitional Game Approach. We consider the problem of cooperative packet delivery to mobile nodes in a hybrid wireless mobile network, where both infrastructure-based and infrastructure-less (i.e., ad hoc mode or peer-to-peer mode) communications are used. We propose a solution based on a coalition formation among mobile nodes to cooperatively deliver packets among these mobile nodes in the same coalition. A coalitional game is developed to analyze the behavior of the rational mobile nodes for cooperative packet delivery. A group of mobile nodes makes a decision to join or to leave a coalition based on their individual payoffs. The individual payoff of each mobile node is a function of the average delivery delay for packets transmitted to the mobile node from a base station and the cost incurred by this mobile node for relaying packets to other mobile nodes. To find the payoff of each mobile node, a Markov chain model is formulated and the expected cost and packet delivery delay are obtained when the mobile node is in a coalition. Since both the expected cost and packet delivery delay depend on the probability that each mobile node will help other mobile nodes in the same coalition to forward packets to the destination mobile node in the same coalition, a bargaining game is used to find the optimal helping probabilities. After the payoff of each mobile node is obtained, we find the solutions of the coalitional game which are the stable coalitions. A distributed algorithm is presented to obtain the stable coalitions and a Markov-chain-based analysis is used to evaluate the stable coalitional structures obtained from the distributed algorithm. Performance evaluation results show that when the stable coalitions are formed, the mobile nodes achieve a nonzero payoff (i.e., utility is higher than the cost). With a coalition formation, the mobile nodes achieve higher payoff than that when each mobile node acts alone.\"",
        "Document: \"A stochastic power control game for two-tier cellular networks with energy harvesting small cells. Energy harvesting in cellular networks is an emerging technique to enhance the sustainability of power-constrained wireless devices. This paper considers the co-channel deployment of a macrocell overlaid with small cells. The small cell base stations (SBSs) harvest their energy from environment sources whereas the macro base station (MBS) uses conventional power supply. Given a stochastic energy arrival process, this paper derives a power control policy for the downlink transmission of both MBS and SBSs such that they can obtain an equilibrium of their own objectives on a long-term basis (e.g., maximizing the transmission rate for SBSs while maintaining the target signal-to-interference-plus-noise ratio (SINR) at the macro users) on a given transmission channel. To this end, we propose a single controller stochastic game and develop a power control policy as a solution of a quadratic programming problem. Numerical results demonstrate the significance of the developed optimal power control policy over the conventional fixed and random power control policies.\"",
        "Document: \"Adaptive radio resource allocation in OFDMA systems: a survey of the state-of-the-art approaches. Orthogonal frequency division multiplexing (OFDM)-based orthogonal frequency division multiple access (OFDMA) has emerged as a promising transmission technology for next generation wireless systems. In a multiuser scenario, adaptive radio resource allocation can significantly improve the performance of OFDMA systems. In this article, an overview of the major state-of-the-art approaches to adaptive resource allocation in the OFDMA systems is provided. Several open research issues are outlined. Copyright \u00a9 2008 John Wiley & Sons, Ltd.\"",
        "Document: \"Queue-aware uplink bandwidth allocation and rate control for polling service in IEEE 802.16 broadband wireless networks. IEEE 802.16 standard defines the air interface specifications for broadband access in wireless metropolitan area networks. Although the medium access control signaling has been well-defined in the IEEE 802.16 specifications, resource management and scheduling, which are crucial components to guarantee quality of service performances, still remain as open issues. In this paper, we propose adaptive queue-aware uplink bandwidth allocation and rate control mechanisms in a subscriber station for polling service in IEEE 802.16 broadband wireless networks. While the bandwidth allocation mechanism adaptively allocates bandwidth for polling service in the presence of higher priority unsolicited grant service, the rate control mechanism dynamically limits the transmission rate for the connections under polling service. Both of these schemes exploit the queue status information to guarantee the desired quality of service (QoS) performance for polling service. We present a queuing analytical framework to analyze the proposed resource management model from which various performance measures for polling service in both steady and transient states can be obtained. We also analyze the performance of best-effort service in the presence of unsolicited grant service and polling service. The proposed analytical model would be useful for performance evaluation and engineering of radio resource management alternatives in a subscriber station so that the desired quality of service performances for polling service can be achieved. Analytical results are validated by simulations and typical numerical results are presented.\"",
        "1 is \"Millimeter Wave Channel Modeling and Cellular Capacity Evaluation.\", 2 is \"System modeling and performance evaluation of rate allocation schemes for packet data services in wideband CDMA systems\"",
        "Given above information, for an author who has written the paper with the title \"On Feasibility of 5G-Grade Dedicated RF Charging Technology for Wireless-Powered Wearables.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00575": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A linear logic of authorization and knowledge':",
        "Document: \"New modalities for access control logics: permission, control and ratification. We present a new modal access control logic, ACL+, to specify, reason about and enforce access control policies. The logic includes new modalities for permission, control, and ratification to overcome some limits of current access control logics. We present a Hilbert-style proof system for ACL+ and a sound and complete Kripke semantics for it. We exploit the Kripke semantics to define Seq-ACL+: a sound, complete and cut-free sequent calculus for ACL+, implying that ACL+ is at least semi-decidable. We point at a Prolog implementation of Seq-ACL+ and discuss possible extensions of ACL+ with axioms for subordination between principals.\"",
        "Document: \"Reconstruction Error Bounds for Compressed Sensing under Poisson or Poisson-Gaussian Noise Using Variance Stabilization Transforms. Most existing bounds for signal reconstruction from compressive measurements make the assumption of additive signal-independent noise. However in many compressive imaging systems, the noise statistics are more accurately represented by Poisson or Poisson-Gaussian noise models. In this paper, we derive upper bounds for signal reconstruction error from compressive measurements which are corrupted by Poisson or Poisson-Gaussian noise. The features of our bounds are as follows: (1) The bounds are derived for a probabilistically motivated, computationally tractable convex estimator with principled parameter selection. The estimator penalizes signal sparsity subject to a constraint that imposes an upper bound on a term based on variance stabilization transforms to approximate the Poisson or Poisson-Gaussian negative log-likelihoods. (2) They are applicable to signals that are sparse as well as compressible in any orthonormal basis, and are derived for compressive systems obeying realistic constraints such as non-negativity and flux-preservation. We present extensive numerical results for signal reconstruction under varying number of measurements and varying signal intensity levels.\"",
        "Document: \"Effective Chemistry for Synchrony and Asynchrony. We study from an implementation viewpoint what constitutes a reasonable and effective notion of structural equivalence of terms in a calculus of concurrent processes and propose operational effectiveness criteria in the form of confluence, coherence and standardization properties on an oriented version of the structural laws. We revisit Berry and Boudol's Chemical Abstract Machine (CHAM) framework using operational effectiveness criteria. We illustrate our ideas with a new formulation of a CHAM for Tees with external choice, one which is operationally effective unlike previous CHAM formulations, and demonstrate that the new CHAM is fully abstract with respect to the LTS semantics for Tees. We then show how this approach extends to the synchronous calculus Sees, for which a CHAM had hitherto not been proposed.\"",
        "Document: \"Equivalence-based Security for Querying Encrypted Databases: Theory and Application to Privacy Policy Audits. To reduce costs, organizations may outsource data storage and data processing to third-party clouds. This raises confidentiality concerns, since the outsourced data may have sensitive information. Although semantically secure encryption of the data prior to outsourcing alleviates these concerns, it also renders the outsourced data useless for any relational processing. Motivated by this problem, we present two database encryption schemes that reveal just enough information about structured data to support a wide-range of relational queries. Our main contribution is a definition and proof of security for the two schemes. This definition captures confidentiality offered by the schemes using a novel notion of equivalence of databases from the adversary's perspective. As a specific application, we adapt an existing algorithm for finding violations of a rich class of privacy policies to run on logs encrypted under our schemes and observe low to moderate overheads.\"",
        "Document: \"Information Retrieval on the web and its evaluation.   Internet is one of the main sources of information for millions of people. One can find information related to practically all matters on internet. Moreover if we want to retrieve information about some particular topic we may find thousands of Web Pages related to that topic. But our main concern is to find relevant Web Pages from among that collection. So in this paper I have discussed that how information is retrieved from the web and the efforts required for retrieving this information in terms of system and users efforts. \"",
        "Document: \"An Authorization Logic With Explicit Time. We present an authorization logic that permits reasoning with explicit time. Following a proof-theoretic approach, we study the meta-theory of the logic, including cut elimination. We also demonstrate formal connections to proof-carrying authorization's existing approach for handling time and comment on the enforceability of our logic in the same framework. Finally, we illustrate the expressiveness of the logic through examples, including those with complex interactions between time, authorization, and mutable state.\"",
        "Document: \"A Proof-Carrying File System. We present the design and implementation of PCFS, a file system that adapts proof-carrying authorization to provide direct, rigorous, and efficient enforcement of dynamic access policies. The keystones of PCFS are a new authorization logic BL that supports policies whose consequences may change with both time and system state, and a rigorous enforcement mechanism that combines proof verification with conditional capabilities. We prove that our enforcement using capabilities is correct, and evaluate our design through performance measurements and a case study.\"",
        "Document: \"Information Flow Control for Event Handling and the DOM in Web Browsers. Web browsers routinely handle private information. Owing to a lax security model, browsers and JavaScript in particular, are easy targets for leaking sensitive data. Prior work has extensively studied information flow control (IFC) as a mechanism for securing browsers. However, two central aspects of web browsers--the Document Object Model (DOM) and the event handling mechanism--have so far evaded thorough scrutiny in the context of IFC. This paper advances the state-of-the-art in this regard. Based on standard specifications and the code of an actual browser engine, we build formal models of both the DOM (up to Level 3) and the event handling loop of a typical browser, enhance the models with fine-grained taints and checks for IFC, prove our enhancements sound and test our ideas through an instrumentation of WebKit, an in-production browser engine. In doing so, we observe several channels for information leak that arise due to subtleties of the event loop and its interaction with the DOM.\"",
        "Document: \"Consumable Credentials in Linear-Logic-Based Access-Control Systems. We present a method to implement consumable creden- tials in a logic-based distributed authorization system. S uch credentials convey use-limited authority (e.g., to open a door once) or authority to utilize resources that are them- selves limited (e.g., concert tickets). We design and imple - ment mechanisms to enforce the consumption of credentials in a distributed system, and to protect credentials from non - productive consumption as might result from misbehavior or failure. We explain how these mechanisms can be used to support a distributed authorization system that uses a line ar access-control logic. Finally, we give several usage exam- ples in the framework, and evaluate the performance of our implementation for use in a ubiquitous computing deploy- ment at our institution.\"",
        "Document: \"Ontology Based Information Retrieval for Learning Styles of Autistic People. In this paper an ontology based prototype system for information retrieval on the Internet is described. User is interested in the focused results regarding a product with some specific characteristics. A product may have different characteristics like size, length, color, functionality based parameters etc. It is, however, difficult for autistic people to identify appropriate keywords clue to their lack of ability to process and retain the information. Therefore, a large amount of unwanted and irrelevant data is included in the outcome. In this proposal user may type the search queries using some words. The objective is to find the right set of keywords from the search paragraph and retrieval of correct patterns or products from the web. This is based on memories of such people and their learning styles that help them find the desired result.\"",
        "1 is \"The program counter security model: automatic detection and removal of control-flow side channel attacks\", 2 is \"A New One-Pass Tableau Calculus for PLTL\"",
        "Given above information, for an author who has written the paper with the title \"A linear logic of authorization and knowledge\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00601": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Integrating flexibility and fuzziness into a question driven query model.':",
        "Document: \"Multiagent Approach for Identifying Cancer Biomarkers. This paper addresses an important and vital problem within the general area of disease recognition, namely identifying disease biomarker genes. Given the complexity of this domain, the basic idea tacked in this paper is employing multiple agents to handle this problem. Though the developed methodology is general enough to be applied to any other domain, we concentrate on identifying cancer biomarkers in this paper. Our approach is mainly based on detecting the minimum set of genes that could successfully identify cancer samples. Multiple agents are involved in the process. After each agents applies its own rules and reports candidate cancer biomarkers, the agents negotiate to agree on the actual biomarkers. The latter process may require further investigation of the characteristics of each of the reported genes because some of them may have the same functionality and the target is a compromise of the best representative of each functionality. A degree of confidence in each candidate biomarker influences the negotiation process. The so far conducted experiments reported very encouraging results with high classification rate; none of the involved agents could alone achieve a close success~rate.\"",
        "Document: \"Graph-based approach for outlier detection in sequential data and its application on stock market and weather data. Outlier detection has a large variety of applications ranging from detecting intrusion in a computer network, to forecasting hurricanes and tornados in weather data, to identifying indicators of potential crisis in stock market data, etc. The problem of finding outliers in sequential data has been widely studied in the data mining literature and many techniques have been developed to tackle the problem in various application domains. However, many of these techniques rely on the peculiar characteristics of a specific type of data to detect the outliers. As a result, they cannot be easily applied to different types of data in other application domains; they should at least be tuned and customized to adapt to the new domain. They also may need certain amount of training data to build their models. This makes them hard to apply especially when only a limited amount of data is available. The work described in this paper tackle the problem by proposing a graph-based approach for the discovery of contextual outliers in sequential data. The developed algorithm offers a higher degree of flexibility and requires less amount of information about the nature of the analyzed data compared to the previous approaches described in the literature. In order to validate our approach, we conducted experiments on stock market and weather data; we compared the results with the results from our previous work. Our analysis of the results demonstrate that the algorithm proposed in this paper is successful and effective in detecting outliers in data from different domains, one financial and the other meteorological.\"",
        "Document: \"State Of The Art In The Realistic Modeling Of Plant Venation Systems. The modeling of plants is an active area of research in computer graphics and the geometrical modeling of veins is essential for obtaining plant images with a realistic appearance. The venation patterns determine the anisotropy of plant tissues, and affect the shadowing and masking of light incident on these tissues, hence the images. The current trend in the area of plant image generation is to aim for realism using biologically-based and predictable algorithms. Such algorithms do not depend on ad hoc parameters that have to be tuned whenever a new image is being generated. A number of plant image generation algorithms are now available in the literature that have achieved an impressive level of sophistication in many aspects, generating realistic looking images. One notable exception is the geometrical modeling of veins. This remains as an open problem in computer graphics. In this paper we review the state of art of the realistic simulation of plant venation systems from a geometric modeling perspective and propose a specified set of requirements for evaluating possible solutions for this problem.\"",
        "Document: \"An Effective LmRMR for Financial Variable Selection and its Applications. Financial variables are of primary importance in financial modeling, fraud detection, financial distress management, price modeling, credit and risk evaluations and in evaluating the return on assets and portfolios. There usually exist a large number of financial variables, where their exhaustive integration in a model increases its dimensionality and the associated computational time. We extensively tackle this problem in this paper. In this paper, we present a modified version of mRMR feature selection model to deal with financial features by ranking features first and then finding the best subset and uncertainty related to it using likelihood evaluation. The wellknown measurement formula of mRMR is considered for ranking financial features using correlation similarity measurement and the concept of minimum redundancy and maximum relevance of financial features and return of assets. Then, likelihood calculations inherently account for the mutual correlations between the variables as well as between the variables and the return on asset and result in a unique \u2018likelihood\u2019 value that has a correlation with the return on asset that can be maximized by adding and removing variables from the subset. We conducted experimental studies on Dow Jones Industrial Average to study the effectiveness and applicability of the proposed approach both in terms of financial variable selection as well as its application in Stock trading recommendation model and potential price forecasting. The performance is evaluated and the proposed approach shows promise.\"",
        "Document: \"Double-step incremental generation of lines and circles. An explicit discussion of the possible speed-up of curve generation due to an increase in the size of increment has not appeared in literature. This paper proposes a methodology and its rationale for scan-converting lines and circles two pixels per iteration. The double-step line and circle algorithms require the same amount of integer arithmetic per iteration as the single-step algorithms but only half the number of iterations. The algorithms take advantage of some simple properties of discrete loci of mathematical curves in the raster plane.\"",
        "Document: \"A geometric approach to clearance based path optimization. For path planning, an optimal path is defined both by its length and by its clearance from obstacles. Many motion planning techniques such as the roadmap method, the cell decomposition method, and the potential field method generate low quality paths with redundant motions which are post-processed to generate high quality approximations of the optimal path. In this paper, we present a O(h2 (log n + k)) algorithm to optimize a path between a source and a destination in a plane based on a preset clearance from obstacles and overall length, where h is a multiple of the number of vertices on the given path, n is a multiple of the number of obstacle vertices, and k is the average number of obstacle edges against which the clearance check is done for each of the O(h2) queries to determine whether a potential edge of the path is collision-free. This improves the running time of the geometric algorithm presented by Bhattacharya and Gavrilova (2007) which already generates a high quality approximation of the optimal path.\"",
        "Document: \"Simulating the Aurora Borealis. We present an algorithm to simulate the aurora borealis, commonly known as the \u9a74northern lights\u9a74, a natural phenomenon of great visual beauty and considerable scientific interest. The algorithm is based on the current understanding of the physical origin of the aurora. High-energy electrons originating in the Sun and entering the Earth's atmosphere in narrow regions centered on the magnetic poles mainly cause this natural display. These electrons collide with atmospheric atoms, which are excited to higher energy levels. The excited atoms emit rapidly varying visible light in a curtain-like volume as they return to lower energy levels thereby creating the aurora. By simulating these light emissions along with the spatial and temporal distribution of the entering electrons, we are able to render the major visual aspects of auroral displays. This approach also allows the representation of time-dependent features that characterize the dynamic nature of the aurorae. The applicability of this auroral model for artistic and research purposes are illustrated through comparisons of synthetic images with photographs of real auroral displays.\"",
        "Document: \"Exact Computation of Delaunay and Power Triangulations. In this paper we present a topologically correct and efficient version of the algorithm by Guibas and Stolfi (Algorithmica 7 (1992), pp. 381-413) for the exact computation of Delaunay and power triangulations in two dimensions. The algorithm avoids numerical errors and degeneracies caused by the accumulation of rounding errors in fixed length floating point arithmetic when constructing these triangulations.\"",
        "Document: \"Geometrically robust image watermarking using star patterns. Digital copyright protection has become increasingly important in recent times due to the rapid growth of the Internet and the proliferation of P2P technologies. Copyright protection of digital images and videos are of particular interest since they can easily be pirated and distributed illegally across networks. Existing watermarking methods can embed a signature into a digital image and tend to be robust against signal noise and but less so against geometric transforms. Methods that are more robust against geometric manipulation are often implemented at great computational cost. This paper proposes a lower cost method using point patterns inspired in part by automated star trackers used in astronomy. The method is evaluated and presents some advantages over existing approaches. Weaknesses of the method and areas for future work are also discussed.\"",
        "Document: \"The relationship between a rectangle and a triangle. A procedure is developed that decides for given rectangle and triangle whether the triangle is contained in the rectangle, the rectangle is contained in the triangle, the two figures overlap or whether they are disjoint. The procedure uses interval barycentric coordinates, which allow a very transparent description of the test, as well as concise proof of correctness and completeness. The numerical execution needs only a few interval and non-interval arithmetic operations. Representative numerical examples are given for some of the cases. Contrasting this interval approach, a direct approach is considered that requires fewer arithmetic and logical operations. The direct approach has the disadvantage of being logically involved, resulting in a large number of cases that must be distinguished, while being computationally less expensive.\"",
        "1 is \"Graph structured views and their incremental maintenance\", 2 is \"Ray tracing animated scenes using coherent grid traversal\"",
        "Given above information, for an author who has written the paper with the title \"Integrating flexibility and fuzziness into a question driven query model.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00599": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Symbolic planning and control of robot motion [Grand Challenges of Robotics]':",
        "Document: \"On the time complexity of conflict-free vehicle routing. In this paper, we study the following problem: given n vehicles and origin-destination pairs in the plane, what is the minimum time needed to transfer each vehicle from its origin to its destination, avoiding conflicts with other vehicles? The environment is free of obstacles, and a conflict occurs when the distance between any two vehicles is smaller than a velocity-dependent safety distance. We derive lower and upper bounds on the time needed to complete the transfer, in the case in which the origin and destination points can be chosen arbitrarily, proving that the transfer takes Theta(root n (L) over bar) time to complete, where (L) over bar is the average distance between origins and destinations. We also analyze the case in which origin and destination points are generated randomly according to a uniform distribution, and present an algorithm providing a constructive upper bound on the time needed for complete the transfer, proving that in the random case the transfer requires Theta(root n log n) time.\"",
        "Document: \"Distributed policies for equitable partitioning: Theory and applications. Abstract\u2014The most widely applied resource allocation strat- egy is to balance, or equalize, the total workload assigned to each resource. In mobile multi-agent systems, this principle directly leads to equitable partitioning policies in which,(i) the workspace is divided into subregions of equal measure, (ii) each agent is assigned to a unique subregion, and (iii) each agent is responsible,for service requests,originating,within,its own subregion. In this paper, we design distributed and adaptive policies that allow a team,of agents,to achieve a convex,and equitable,partition of a convex,workspace.,Our approach,is related to the classic Lloyd algorithm, and exploits the unique features of Power Diagrams. We discuss possible applications to routing of vehicles in stochastic and dynamic environments, and to wireless networks. Simulation results are presented,and discussed.\"",
        "Document: \"The Impact of Cooperative Perception on Decision Making and Planning of Autonomous Vehicles. In this article, we investigate how cooperative perception gives the impact on decision making and planning of autonomous vehicles on the road. Cooperative perception is the exchange of local sensing information with other vehicles or infrastructures via wireless communications, by which the perception range can be considerably extended up to the boundary of connected vehicles. This augmented perc...\"",
        "Document: \"Strategic dynamic vehicle routing with spatio-temporal dependent demands. We study a zero-sum game formulation of a dynamic vehicle routing problem: a system planner seeks to design dynamic routing policies for a team of vehicles to minimize the average waiting time of demands that are strategically placed in a region by an adversarial agent with unitary capacity operating from a depot. We characterize an equilibrium in the limiting case where vehicles travel arbitrarily slower than the agent (heavy load). We show that such an equilibrium consists of a routing policy based on performing successive TSP tours through outstanding demands and a unique power-law spatial density centered at the depot location.\"",
        "Document: \"Models, algorithms, and evaluation for autonomous mobility-on-demand systems. This tutorial paper examines the operational and economic aspects of autonomous mobility-on-demand (AMoD) systems, a rapidly emerging mode of personal transportation wherein robotic, self-driving vehicles transport customers in a given environment. We address AMoD systems along three dimensions: (1) modeling - analytical models capable of capturing the salient dynamic and stochastic features of customer demand, (2) control - coordination algorithms for the vehicles aimed at stability and subsequently throughput maximization, and (3) economic - fleet sizing and financial analyses for case studies of New York City and Singapore. Collectively, the models and algorithms presented in this paper enable a rigorous assessment of the value of AMoD systems. In particular, the case study of New York City shows that the current taxi demand in Manhattan can be met with about 8,000 robotic vehicles (roughly 70% of the size of the current taxi fleet), while the case study of Singapore suggests that an AMoD system can meet the personal mobility need of the entire population of Singapore with a number of robotic vehicles that is less than 40% of the current number of passenger vehicles. Directions for future research on AMoD systems are presented and discussed.\"",
        "Document: \"The effect of reversals for a stochastic source-seeking process inspired by bacterial chemotaxis. Many species of bacteria are motile, but they use different random strategies to determine where to swim in response to chemical gradients. We extend past work describing a chemotactic E. coli cell as an ergodic, stochastic hybrid system to model a variety of different strategies. We quantify differences in asymptotic performance and show that the processes described by our models converge to stationary distributions that are proportional to various powers of the distribution of chemicals in the environment. Our main goal is to understand the implications of the differences between E. coli's chemotaxis strategy and the more complicated strategy of the marine bacterium Vibrio alginolyticus, which, unlike E. coli, can swim both forward and backward. We argue that Vibrio's ability to reverse allows it to accumulate more tightly around nutrient sources, and we quantify the effects that reversals have on the stationary distribution of various processes. Our results provide intuition for designing minimalistic multi-agent robotic systems that are better suited for source-seeking tasks in particular environments.\"",
        "Document: \"Simultaneous input and state smoothing for linear discrete-time stochastic systems with unknown inputs. This paper considers the problem of simultaneously estimating the states and unknown inputs of linear discrete-time systems in the presence of additive Gaussian noise based on observations from the entire time interval. A fixed-interval input and state smoothing algorithm is proposed for this problem and the input and state estimates are shown to be unbiased and to achieve minimum mean squared error and maximum likelihood. A numerical example is included to demonstrate the performance of the smoother.\"",
        "Document: \"Equitable Partitioning Policies for Mobile Robotic Networks. The most widely applied strategy for workload sharing is to equalize the workload assigned to each resource. In mobile multi-agent systems, this principle directly leads to equitable partitioning policies in which (i) the workspace is divided into subregions of equal measure, (ii) there is a bijective correspondence between agents and subregions, and (iii) each agent is responsible for service requests originating within its own subregion. In this paper, we design provably correct, spatially-distributed and adaptive policies that allow a team of agents to achieve a convex and equitable partition of a convex workspace, where each subregion has the same measure. We also consider the issue of achieving convex and equitable partitions where subregions have shapes similar to those of regular polygons. Our approach is related to the classic Lloyd algorithm, and exploits the unique features of power diagrams. We discuss possible applications to routing of vehicles in stochastic and dynamic environments. Simulation results are presented and discussed.\"",
        "Document: \"Efficient Routing Algorithms for Multiple Vehicles With no Explicit Communications. In this paper, we consider a class of dynamic vehicle routing problems, in which a number of mobile agents in the plane must visit target points generated over time by a stochastic process. It is desired to design motion coordination strategies in order to minimize the expected time between the appearance of a target point and the time it is visited by one of the agents. We propose control strategies that, while making minimal or no assumptions on communications between agents, provide the same level of steady-state performance achieved by the best known decentralized strategies. In other words, we demonstrate that inter-agent communication does not improve the efficiency of such systems, but merely affects the rate of convergence to the steady state. Furthermore, the proposed strategies do not rely on the knowledge of the details of the underlying stochastic process. Finally, we show that our proposed strategies yield an efficient, pure Nash equilibrium in a game theoretic formulation of the problem, in which each agent's objective is to maximize the expected value of the ldquotime spent alonerdquo at the next target location. Simulation results are presented and discussed.\"",
        "Document: \"Locally-optimal multi-robot navigation under delaying disturbances using homotopy constraints. We study the problem of reliable motion coordination strategies for teams of mobile robots when any of the robots can be temporarily stopped by an exogenous disturbance at any time. We assume that an arbitrary multi-robot planner initially provides coordinated trajectories computed without considering such disturbances. We are interested in designing a control strategy that handles delaying disturbance such that collisions and deadlocks are provably avoided, and the travel time is minimized. The problem is analyzed in a coordination space framework, in which each dimension represents the position of a single robot along its planned trajectory. We demonstrate that to avoid deadlocks, the trajectory of the system in the coordination space must be homotopic to the trajectory corresponding to the planned solution. We propose a controller that abides this homotopy constraint while minimizing the travel time. Besides being provably deadlock-free, our experiments show that travel time is significantly smaller with our method than than with a reactive method.\"",
        "1 is \"Nash Equilibrium Problems With Scaled Congestion Costs and Shared Constraints.\", 2 is \"Motion feasibility of multi-agent formations\"",
        "Given above information, for an author who has written the paper with the title \"Symbolic planning and control of robot motion [Grand Challenges of Robotics]\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00615": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Applied Multi-Dimensional Fusion':",
        "Document: \"Scalable Video Fusion. A novel system is introduced that is able to fuse two or more sets of multimodal videos in the transform domain. This is achieved without drift and produces an embedded bitstream that offers fine grain scalability. Previous attempts to fuse in the transform domain have not been possible for video compression systems due to the complications of predictive loops within conventional video encoding. The compression system is based on an optimised spatiotemporal codec using the 3D Discrete Dual-tree Wavelet Transform (DDWT) together with a bit plane encoding method (SPIHT) and a coefficient sparsification process (noise shaping). Together, these methods can efficiently encode a video sequence without the need for motion compensation due to the directional (in space and time) selectivity of the transform. This system offers extremely flexible video fusion in dynamic bandwidth environments where there are variable client receiving capabilities.\"",
        "Document: \"Improved illumination invariant homomorphic filtering using the dual tree complex wavelet transform. A novel adaptation of the two dimensional Homomorphic filter is introduced using the Dual Tree Complex Wavelet Transform for improved illumination invariant processing. The Homomorphic filter is conventionally implemented within the log-Fourier domain using an isotropic high-pass filter based on the assumption that the illumination signal occupies low spatial frequencies. In this case however, low frequency structural reflectance content will be incorrectly attenuated. Our method implements the Homomorphic filter using the DT-CWT and exploits the property of cross scale persistence (for structural content) to generate a filter that retains cross scale content and therefore reduces incorrect attenuation of structural reflectance content.\"",
        "Document: \"Applied Multi-Dimensional Fusion. The purpose of the Applied Multi-dimensional Fusion Project is to investigate the benefits that data fusion and related techniques may bring to future military Intelligence Surveillance Target Acquisition and Reconnaissance systems. In the course of this work, it is intended to show the practical application of some of the best multi-dimensional fusion research in the UK. This paper highlights the work done in the area of multi-spectral synthetic data generation, super-resolution, joint fusion and blind image restoration, multi-resolution target detection and identification and assessment measures for fusion. The paper also delves into the future aspirations of the work to look further at the use of hyper-spectral data and hyper-spectral fusion. The paper presents a wide work base in multi-dimensional fusion that is brought together through the use of common synthetic data, posing real-life problems faced in the theatre of war. Work done to date has produced practical pertinent research products with direct applicability to the problems posed.\"",
        "Document: \"Region of interest coding of volumetric medical images. Three-dimensional wavelet coding of volumetric medical images provides better coding performance compared to corresponding 2D methods by exploiting the inter-slice correlation that exists in such data. It introduces however latencies when it comes to transmitting specific parts of the volume. This paper presents an extension to 3D- SPIHT which allows 3D Region of Interest (ROI) coding. ROI coding enables faster reconstruction of diagnostically useful regions in volumetric datasets by assigning higher priority to them in the bitstream. It also introduces the possibility for increased compression performance, by allowing certain parts of the volume to be coded in a lossy manner while others are coded losslessly. The necessary modifications to 3D-SPIHT for ROI coding are described and methods for specifying a 3D ROI without adding a significant overhead are suggested. Results are presented highlighting the benefits of the ROI extension.\"",
        "Document: \"Efficient methodology for hand-coding video algorithms for VLIW-type processors. Digital signal processors (DSP) based on very large instruction word (VLIW) architectures are potentially well suited for software-based implementation of video codecs. In order to achieve real-time performance, the implementation has to be efficiently mapped to the target architecture. Although sophisticated compilers are available for VLIW-type processors, time-critical video coding algorithms can often be more efficiently coded by hand. This is because compilers are still not intelligent enough to fine-tune the organization of data structures or make trade-offs on an abstract algorithmic level. However, hand-coding typically is a very demanding, error-prone and time-consuming task. Exploring as many scheduling opportunities as possible and making the right scheduling decisions is a complex task but is essential for achieving an efficient implementation. In order to address these problems, we have developed a systematic hand-coding methodology which is based on a framework of scheduling heuristics and a directed acyclic graph (DAG) representation with incorporated processor resources. The methodology helps to produce more efficient code in a shorter time compared to ad-hoc hand-coding approaches. Another benefit is that the implementation is well documented through the DAG representation which allows possible modifications to be easily accommodated. The methodology is explained on the examples of two video algorithms, the discrete cosine transform (DCT) and sum of absolute differences (SAD) which have been implemented on the Texas Instruments TMS320C6201 DSP. The performance results confirm the effectiveness of our methodology. The presented methodology may also provide a basis for a visual, semi-automated, interactive developing tool which gives the programmer more control over the implementation of critical algorithms or procedures.\"",
        "Document: \"On the performance of modern video coding standards with textured sequences. This work presents two studies on the topic of coding highly textured content with H.265/HEVC. The aim of the studies is to identify any potential for improvement in the performance of the codec with this type of content. Both studies employ a texture-focused video database developed by the authors. Study I evaluates the performance of H.265/HEVC relative to H.264/AVC for the case of static, dynamic and mixed texture content. Study II evaluates the effectiveness of the currently used objective quality measures with this type of content. The results suggest that there is potential for improvement in coding performance by matching the quality/error measure used to the type of content (textured/non-textured) and type of texture (static, dynamic, mixed) encountered.\"",
        "Document: \"Time Varying Volumetric Scene Reconstruction Using Scene Flow. Traditional volumetric scene reconstruction algorithms involve the evalua- tion of many millions of voxels which is highly time consuming. This paper presents an efficient algorithm based of future frame prediction that can dra- matically reduce the number of voxels to be evaluated in time varying scenes. The new prediction method, combining scene flow and morphological dila- tions, is evaluated against a simple model dilation method. Results show the proposed method outperforms a simple dilation method and has the poten- tial to improve the efficiency of volumetric scene reconstruction algorithms while retaining quality given accurate optical flows.\"",
        "Document: \"Wipe production in MPEG-2 compressed video. With the increased role of technology in video production, several types of complex video special effects editing have begun to appear. We consider wiping special effects editing in MPEG-2 compressed video without full frame decompression and motion estimation. We estimated the DCT coefficients and use these coefficients together with the existing motion vectors to produce these special effects editing in the compressed domain. The results show that both the objective and subjective quality of the edited video in the compressed domain closely follows the quality of the video edited in the uncompressed domain at the same bit rate.\"",
        "Document: \"Complexity evaluation for the implementation of a pre-FFT equalizer in an OFDM receiver. A pre-FFT equalizer (PFE) has been shown to offer a significant throughput efficiency improvement when applied to an OFDM receiver. Alternatively, the PFE can be used to increase the maximum delay spread conditions under which the OFDM system can operate effectively. Due to the manner of its operation, the PFE requires the use of modified adaptation algorithms if iterative, decision directed, adaptation is required. The computational complexity required to implement a PFE and a suitable adaptation strategy is evaluated. Initially, an LMS adaptation algorithm is investigated and evaluated in terms of its suitability for application in conjunction with the PFE to standards such as ETSI DVB-T and HIPERLAN/2 and IEEE 802.11a. The complexity requirements are found to be high, particularly in the case of DVB-T. The demand for a lower complexity adaptation algorithm is thus identified. As a result, a CSI-based adaptation method is subsequently considered. The complexity requirement of this algorithm is also analyzed and evaluated and is shown to be much lower than that of the LMS algorithm. Thus, it is shown that if the CSI-based adaptation method is used, the dominant complexity requirement is due to the implementation of the equalizing filter and not the adaptation method. Reduced filter complexity requirement is thus shown to be the key to enabling effective application of the PFE. The ATSC 8-VSB standard is identified as a possible source of techniques to reduce or facilitate the high complexity demands for implementation of the PFE filter\"",
        "Document: \"A frame rate dependent video quality metric based on temporal wavelet decomposition and spatiotemporal pooling. This paper presents an objective quality metric (FRQM), which characterises the relationship between variations in frame rate and perceptual video quality. The proposed method estimates the relative quality of a low frame rate video with respect to its higher frame rate counterpart, through temporal wavelet decomposition, subband combination and spatiotemporal pooling. FRQM was tested alongside six commonly used quality metrics (two of which explicitly relate frame rate variation to perceptual quality), on the publicly available BVI-HFR video database, that spans a diverse range of scenes and frame rates, up to 120fps. Results show that FRQM offers significant improvement over all other tested quality assessment methods with relatively low complexity.\"",
        "1 is \"Joint Semantic Segmentation And 3d Reconstruction From Monocular Video\", 2 is \"Morphological elastic graph matching applied to frontal face authentication under well-controlled and real conditions\"",
        "Given above information, for an author who has written the paper with the title \"Applied Multi-Dimensional Fusion\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00634": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'HCI, Solidarity Movements and the Solidarity Economy.':",
        "Document: \"The department of hidden stories: Playful digital storytelling for children in a public library. We detail the design of the Department of Hidden Stories (DoHS), a mobile-based game to support playful digital storytelling among primary school children in public libraries. Through a process of iterative design in collaboration with library staff and children's writers we designed DoHS to support the potential for playful storytelling through interactions with books. A deployment of DoHS with two classes of 8 to 10 years olds as part of their regular library visits revealed insights related to how to balance the expectations of a child-at-play and the requirement to further develop their creative reading and writing skills. Based on our experiences we recommend that designers create playful digitally based activities that encourage children to explore libraries and experience new interactions with physical books.\"",
        "Document: \"Contesting the City: Enacting the Political Through Digitally Supported Urban Walks. We present a method for the situated discovery and articulation of issues at the intersection between the politics of place making and city planning. We describe the construction and use of designed tools, such as historical political archives; counterfactual maps; and cards to invite situated dialogue between the social and institutional practices and mechanisms that produce our cities. Grounded in an account of the political as vernacular and embodied, our analysis advance understandings on the politics of design, and on the complex interrelationship between places and political spaces. We outline how HCI can adopt methods and develop sensitivities to support democratic practices and publics envisioning their urban futures.\"",
        "Document: \"Communities of practice in the distributed international environment.  The focus in this paper is on Communities of Practice in commercial organisations. We do this by exploring knowledge sharing in Lave and Wenger's (1991) theory of Communities of Practice and investigating how Communities of Practice may translate to a distributed international environment. The paper reports on two case studies that explore the functioning of Communities of Practice across international boundaries. \"",
        "Document: \"TryFilm: Situated Support for Interactive Media Productions. The emergence of participatory, on-demand and interactive media is changing the media production landscape. Producing interactive media is often more complex than creating traditional linear films, resulting in increased pressure for production teams. In this paper we explore what implications this has for cast and crew who participate in the production of such new media. We explore how collaborative technologies can support creative practitioners, within these challenging settings. We present TryFilm, a collaborative editing system, designed by the authors and deployed during an interactive film shoot by a small film company featuring a cast of early career actors.\"",
        "Document: \"Plastic is fantastic!: experimenting with the building affordances of fuse beads in physical computing. We present the use of plastic fuse beads as a prototyping approach in physical computing and as a material in electronic circuitry. We introduce the properties of this craft and material, and describe a collaborative experiment with a group of teenagers, in which this approach was tested as a participatory project. This open-format workshop demonstrated the feasibility, affordances and youth-friendliness of using this craft and material for simple DIY physical computing projects and for the tangible learning of basic principles of interaction design.\"",
        "Document: \"Enabling Polyvocality in Interactive Documentaries through \"Structural Participation\". Recent innovations in online, social and interactive media have led to the emergence of new forms of documentary, such as interactive documentaries ('i-Docs'), with qualities that lend themselves to more open and inclusive production structures. Still, little is known about the experience of making and/or participating-in these kinds of documentary. Our two-year in-the-wild study engaged a large community-of-interest in the production of an i-Doc to explore the ethically-desirable yet challenging aim of enabling multiple subjects to have agency and control over their representation in a documentary. Our study reveals insights into the experiences of participating in an i-Doc and highlights key sociotechnical challenges. We argue that new sociotechnical infrastructure is needed, that frames both \\\"executory\\\" and \\\"structural\\\" forms of participation as symbiotic elements of a co-design process.\"",
        "Document: \"Claims, observations and inventions: analysing the artifact. This paper describes the use of observation-invention pairs to illustrate important general points about the way that users interact with computers. The technique can be viewed as a form of artifact analysis.\"",
        "Document: \"Thea: A Technique For Human Error Assessment Early In Design. THEA is a technique designed for use by interactive system designers and engineers to help anticipate interaction failures. These may become problematic once designs become operational. The technique employs a cognitive error analysis based on an underlying model of human information processing. It is a highly structured approach, intended for use early in the development lifecycle as design concepts and requirements concerned with safety and usability - as well as functionality - are emerging. We believe the technique advances the systematic identification of human-computer interaction error through its straightforward application, requiring minimal formal knowledge of human factors or cognitive psychology.\"",
        "Document: \"APD-A Tool for Identifying Behavioural Patterns Automatically from Clickstream Data. Clickstream can be a rich source of data for analysing user behaviour, but the volume of these logs makes it difficult to identify and categorise behavioural patterns. In this paper, we introduce the Automatic Pattern Discovery (APD) method, a technique for automated processing of Clickstream data to identify a user's browsing patterns. The paper also includes case study that is used to illustrate the use of the APD and to evaluate its performance.\"",
        "Document: \"A Real-Time IVR Platform for Community Radio. Interactive Voice Response (IVR) platforms have been widely deployed in resource-limited settings. These systems tend to afford asynchronous push interactions, and within the context of health, provide medication reminders, descriptions of symptoms and tips on self-management. Here, we present the development of an IVR system for resource-limited settings that enables real-time, synchronous interaction. Inspired by community radio, and calls for health systems that are truly local, we developed \"Sehat ki Vaani\". Sehat ki Vaani is a real-time IVR platform that enables hosting and participation in radio chat shows on community-led topics. We deployed Sehat ki Vaani with two communities in North India on topics related to the management of Type 2 diabetes and maternal health. Our deployments highlight the potential for synchronous IVR systems to offer community connection and localised sharing of experience, while also highlighting the complexity of producing, hosting and participating in radio shows in real time through IVR. We discuss the relative strengths and weaknesses of synchronous IVR systems, and highlight lessons learnt for interaction design in this area.\n\n\"",
        "1 is \"Efficient data mining for web navigation patterns\", 2 is \"Evaluating WordNet-based Measures of Lexical Semantic Relatedness\"",
        "Given above information, for an author who has written the paper with the title \"HCI, Solidarity Movements and the Solidarity Economy.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00638": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A wireless public access infrastructure for supporting mobile context-aware IPv6 applications':",
        "Document: \"Correlating Sensors and Activities in an Intelligent Environment: A Logistic Regression Approach. An important problem in intelligent environments is how the system can identify and model users' activities. This paper describes a new technique for identifying correlations between sensors and activities in an intelligent environment. Intelligent systems can then use these correlations to recognize the activities in a space. The proposed approach is motivated by the need for distinguishing the critical set of sensors that identifies a specific activity from others that do not. We compare several correlation techniques and show that logistic regression is a suitable solution. Finally, we describe our approach and report preliminary results.\"",
        "Document: \"Sensing Danger - Challenges in Supporting Health and Safety Compliance in the Field. Many workers operate in environments that are inherently hazardous and that are subject to strict health and safety rules and regulations. We envisage a world in which physical work artefacts such as tools, are augmented with intelligent mobile nodes that are able to observe the working activities taking place, evaluate compliance with health and safety regulations and assist or actively enforce compliance with these regulations. This vision creates a new field of work in the area of health and safety aware intelligent mobile sensor networks. In this paper we describe a number of new challenges faced when developing mobile systems for compliance with health and safety regulations.\"",
        "Document: \"Future wireless applications for a networked city: services for visitors and residents. Future wireless networks will offer near-ubiquitous high-bandwidth communications to mobile users. In addition, the accurate position of users will be known, either through network services or via additional sensing devices such as GPS. These characteristics of future mobile environments will enable the development of location-aware and, more generally, context-sensitive applications. In an attempt to explore the system, application, and user issues associated with the development and deployment of such applications, we began to develop the Lancaster GUIDE system in early 1997, finishing the first phase of the project in 1999. In its entirety, GUIDE comprises a citywide wireless network based on 802.11, a context-sensitive tour guide application with, crucially, significant content, and a set of supporting distributed systems services. Uniquely in the field, GUIDE has been evaluated using members of the general public, and we have gained significant experience in the design of usable context-sensitive applications. We focus on the applications and supporting infrastructure that will form part of GUIDE II, the successor to the GUIDE system. These developments are designed to expand GUIDE outside the tour guide domain, and to provide applications and services for residents of the city of Lancaster, offering a vision of the future mobile environments that will emerge once ubiquitous high-bandwidth coverage is available in most cities.\"",
        "Document: \"Structural Learning of Activities from Sparse Datasets. A major challenge in pervasive computing is to develop systems that can reliably recognize human activity patterns, such as bathing from sensor data. Typical sensor deployments generate sparse datasets with thousands of sensor readings and few instances of activities. The imbalance between the number of features (i.e. sensors firing) and the classification targets (i.e. activities) complicates the learning process. In this paper, we propose a novel framework for discovering relationships between sensor signals and observed human activities from sparse datasets. The framework builds on the use of Bayesian networks for modeling activities by representing statistical dependencies between sensors. This allows us to solve two key problems: firstly, how to automatically determine an effective structure for a Bayesian network that recognizes a particular activity without human intervention; and, secondly, we address the pragmatic problem of sparse training data, where the data available to train the activity recognizers is limited. In our approach, we \"learn' the structure of the Bayesian networks automatically from the sensor data. We optimize this process in 3 ways: firstly, we perform multicollinearity analysis to focus on orthogonal sensor data with minimal redundancy. Secondly, we propose Efron's bootstrapping to generate large training sets that capture important features of an activity. Finally, we find the best Bayesian network that explains our data using a heuristic search that is unbiased to the ordering between consecutive variables. We evaluate our approach using a data set gathered from MIT's Place- Lab. The inferred networks correctly identify activities for 85% of the time.\"",
        "Document: \"Preserving privacy in environments with location-based applications. The increase in location-based applications makes protecting personal location information a major challenge. Users must be able to control their location privacy but should be able to do so without constant intervention\u00fdthat is, they should be freed from having to click \u00fdok\u00fd whenever an application requests access to their location information. The article describes a system that lets users automate control of their location information, thereby minimizing the extent to which the system intrudes on their lives.\"",
        "Document: \"Developing Adaptive Applications: The MOST Experience. Future computer environments will include mobile computers which will either be disconnected, weakly inter-connected by low speed wireless networks such as GSM, or fully inter-connected by high speed networks ranging from Ethernet to ATM. Such environments place unique demands on systems, requiring software to dynamically adapt to rapid and significant uctuations in communications quality-of-service (QoS). This paper reviews existing adaptation techniques and describes an experiment in developing an adaptive mobile application and associated distributed systems platform. The experiences gained during this experiment are presented and analysed to provide a basis for the engineering of future adaptive systems.\"",
        "Document: \"Using bluetooth device names to support interaction in smart environments. An increasing trend in mobile and pervasive computing is the augmentation of everyday public spaces with local computation - leading to so called smart environments. However, there are no well accepted techniques for supporting spontaneous interaction between mobile users and these smart environments, though a wide range of techniques have been explored ranging from gesture recognition to downloading applications to a user's phone. In this paper we explore an approach to supporting such interaction based on the use of Bluetooth Device (user-friendly) Names as a control channel between users' mobile phones and computational resources in their local environment. Such an approach has many advantages over existing techniques though it is not without limitations. Our work focuses specifically on the use of Device Names to control and customize applications on large public displays in a campus environment. This paper describes our basic approach, a number of applications that we have constructed using this technique and the results of our evaluation work which has included a range of user studies and field trials. The paper concludes with an assessment of the viability of using our approach for interaction scenarios involving mobile users and computationally rich environments.\"",
        "Document: \"Places to Stay on the Move: Software Architectures for Mobile User Interfaces. Architectural design has an important effect on usability, most notably on temporal properties. This paper investigates software architecture options for mobile user-interfaces, in particular those for collaborative systems. One of the new features of mobile systems as compared with fixed networks is the connection point to the physical network, the point of presence (PoP), which forms an additional location for code and data. This allows architectures that bring computation closer to the users hence reducing feedback and feedthrough delays. A consequence of using PoPs is that code and data have to be mobile within the network leading to potential security problems.\"",
        "Document: \"Self-Defining Memory Cues: Creative Expression and Emotional Meaning. This paper explores how people generate cues for capturing personal meaningful daily events, which can be used for later recall. Such understanding can be explored to inform the design and development of personal informatics systems, aimed to support reflection and increased self-awareness. We describe a diary study with six participants and discuss initial findings showing the qualities of daily meaningful events, the value of different types of cues and their distinct contents for supporting episodic recall.\"",
        "Document: \"Trustworthy by design. Driven by changes in working practices and technology trends, organizations are increasingly reliant on mobile workers and the data they capture. However, while significant work has been carried out on increasing the usability of mobile devices and applications, little attention has been paid to the quality of data captured by mobile workers. If this data is inaccurate or untrustworthy, serious consequences can ensue. In this paper we study a system targeted at mobile workers in the highways sector that is deliberately designed to increase the accuracy and trustworthiness of the data collected. The resulting Inspections application has been very positively received by workers and we present lessons that we believe can be applied to other applications of this type.\"",
        "1 is \"Aether: an awareness engine for CSCW\", 2 is \"B4: experience with a globally-deployed software defined wan\"",
        "Given above information, for an author who has written the paper with the title \"A wireless public access infrastructure for supporting mobile context-aware IPv6 applications\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00662": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A Semantic Web Services Architecture':",
        "Document: \"Flaw selection strategies for partial-order planning. Several recent studies have compared the relative efficiency of alternative flaw selection strategies for partial-order causal link (POCL) planning. We review this literature, and present new experimental results that generalize the earlier work and explain some of the discrepancies in it. In particular, we describe the Least-Cost Flaw Repair (LCFR) strategy developed and analyzed by Joslin and Pollack (1994), and compare it with other strategies, including Gerevini and Schubert's (1996) ZLIFO strategy. LCFR and ZLIFO make very different, and apparently conflicting claims about the most effective way to reduce search-space size in POCL planning. We resolve this conflict, arguing that much of the benefit that Gerevini and Schubert ascribe to the LIFO component of their ZLIFO strategy is better attributed to other causes. We show that for many problems, a strategy that combines least-cost flaw selection with the delay of separable threats will be effective in reducing search-space size, and will do so without excessive computational overhead. Although such a strategy thus provides a good default, we also show that certain domain characteristics may reduce its effectiveness.\"",
        "Document: \"A Planning Component for RETSINA Agents. In the RETSINA multi-agent system, each agent is provided with an internal planning component\u2014the RETSINA planner. Each agent, using its in- ternal planner, formulates detailed plans and executes them to achieve local and global goals. Knowledge of the domain is distributed among the agents, therefore each agent has only partial knowledge of the state of the world. Furthermore, the domain changes dynamically, therefore the knowledge available might become obsolete. To deal with these issues, each agent's planner allows it to interleave planning and execution of information gathering actions, to overcome its partial knowledge of the domain and acquire information needed to complete and execute its plans. In- formation necessary for an agent's local plan can be acquired through cooperation by the local planner firing queries to other agents and monitoring for their results. In addition, the local planner deals with the dynamism of the domain by monitor- ing it to detect changes that can affect plan construction and execution. Teams of agents, each of which incorporates a local RETSINA planner have been imple- mented. These agents cooperate to solve problems in different domains that range from portfolio management to command and control decision support systems .\"",
        "Document: \"A hybrid particle swarm optimization approach for the sequential ordering problem. The sequential ordering problem is a version of the asymmetric travelling salesman problem where precedence constraints on vertices are imposed. A tour is feasible if these constraints are fulfilled, and the objective is to find a feasible solution with minimum cost. A particle swarm optimization approach hybridized with a local search procedure is discussed in this paper. The method is shown to be very effective in guiding a sophisticated local search previously introduced in the literature towards high quality regions of the search space. Differently from standard particle swarm algorithms, the proposed hybrid method tends to fast convergence to local optima. A mechanism to self-adapt a parameter and to avoid stagnation is therefore introduced. Extensive experimental results, where the new method is compared with the state-of-the-art algorithms, show the effectiveness of the new approach.\"",
        "Document: \"Supporting Mobile Service Usage through Physical Mobile Interaction. Although mobile services can be used ubiquitously, their employment and the interaction with them are still restricted by the constraints of mobile devices. In order to facilitate and leverage mobile interaction with services, we present a generic framework that combines Semantic Web Service technology and Physical Mobile Interaction. This interaction paradigm uses mobile devices to extract information from augmented physical objects and use it for a more intuitive and convenient invocation of associated services. For that purpose, the presented framework exploits Web Service descriptions for the automatic and dynamic generation of customizable user interfaces that support and facilitate Physical Mobile Interaction. This generic approach to mobile interaction with services through the interaction with physical objects promises to meet the complementary development of the Internet of Things. A user study with a prototype application for mobile ticketing confirms our concept and shows its limits.\"",
        "Document: \"A Dynamic Routing Strategy For The Real-Time Management Of A Fleet Of Tank Trucks. The off-line-determined schedule of deliveries of fuel to filling stations often looses its efficiency because of delays owing to traffic and weather conditions, the receiving of new and urgent orders, and the temporary unavailability of some tank trucks. In this paper a dynamic routing strategy for the real-time management of a fleet of tank trucks is proposed, with the main purpose of minimizing delays in delivering fuel. The proposed strategy relies on two in-series stages: the \"trip re-scheduling\" and the \"trip optimization\". The former considers all the trips still to be performed and assigns each of them to the tank trucks of the fleet, whereas the latter considers all the filling stations still to be served, then creates a reduced number of trips and assigns each of these trips to the tank trucks. In this paper the mixed-integer programming formulation of the two stages are provided and described in detail.\"",
        "Document: \"Agent-based Petri net models for AGV management in manufacturing systems. The problem of managing and controlling automatic guided vehicles (AGV) in manufacturing shop floor systems is addressed. In such systems, resources are connected through a network of paths that are limited-capacity shared resources as, in general, two or more AGVs cannot use the same section at the same time. The proposed approach makes use of the integration between a multi-agent system and Petri nets. The behaviour and the interactions among AGVs, path sections, and all other resources in the shop floor system are represented by means of Petri nets, whereas agents handle decisional activities. Two architectures are proposed: in the first, decisional agents are associated with path sections (path agents) whereas in the second scheme decisional agents are AGVs themselves (AGV agents)\"",
        "Document: \"Allocating crude oil supply to port and refinery tanks: a simulation-based decision support system. This work focuses on the problem of allocating the crude oil loads of tanker ships to port and refinery tanks (PRT). Two discrete scheduling aspects mainly influence this process: the tankers' arrivals and the sequence of crude lots processed in the refinery. A simulation-based approach that can be applied as a simulator of the physics of the crude oil flow in the refinery system, as a learning support for personnel training, and as a decision support system (DSS) is proposed. The results of the application of the implemented system on a real small\u2013medium-sized refinery system are presented.\"",
        "Document: \"Modelling and automated composition of user-centric services. User-centric services bring additional constraints to the problem of automated service composition. While in business-centric settings the services are orchestrated in order to accomplish a specific business task, user-centric service composition should allow the user to decide and control which tasks are executed and how. This requires the ability not only to automatically compose different, often unrelated, services on the fly, but also to generate a flexible interaction protocol that allows the user to control and coordinate composition execution. In this paper we present a novel automated composition approach that aims to support user-centric service provisioning. Specifically, we associate the service to so-called service objects and provide a declarative notation to express composition requirements in terms of the evolution of those objects. On top of these objects we also define the user control activities and constraints. Using the automated planning techniques, our approach generates a service composition that orchestrates services in a way it is requested by the user.\"",
        "Document: \"The RETSINA MAS Infrastructure. RETSINA is an implemented Multi-Agent System infrastructure that has been developed for several years and applied in many domains ranging from financial portfolio management to logistic planning. In this paper, we distill from our experience in developing MASs to clearly define a generic MAS infrastructure as the domain independent and reusable substratum that supports the agents' social interactions. In addition, we show that the MAS infrastructure imposes requirements on an individual agent if the agent is to be a member of a MAS and take advantage of various components of the MAS infrastructure. Although agents are expected to enter a MAS and seamlessly and effortlessly interact with the agents in the MAS infrastructure, the current state of the art demands agents to be programmed with the knowledge of what infrastructure they will utilize, and what are various fall-back and recovery mechanisms that the infrastructure provides. By providing an abstract MAS infrastructure model and a concrete implemented instance of the model, RETSINA, we contribute towards the development of principles and practice to make the MAS infrastructure \u201cinvisible\u201d and ubiquitous to the interacting agents.\"",
        "Document: \"Perci: Pervasive Service Interaction with the Internet of Things. The advancement of ubiquitous computing technologies has greatly improved the availability of digital resources in the real world. Here, the authors investigate mobile interaction with tagged, everyday objects and associated information that's based on the Internet of Things and its technologies. Their framework for integrating Web services and mobile interaction with physical objects relies on information typing to increase interoperability. Two prototypes for mobile interaction with smart posters build upon this framework to realize multi-tag interaction with physical user interfaces. The authors' evaluation identifies usability issues regarding the design of physical mobile interactions, interfaces, and applications.\"",
        "1 is \"Semantic Bridging of Independent Enterprise Ontologies\", 2 is \"Smarter Groups - Reasoning on Qualitative Information from Your Desktop\"",
        "Given above information, for an author who has written the paper with the title \"A Semantic Web Services Architecture\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00732": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Managing emergent character-based narrative.':",
        "Document: \"Towards Data-Driven Drama Management: Issues in Data Collection and Annotation. One of the key questions in the design and development of interactive drama is structuring an experience for participants such that an engaging, coherent narrative is presented while enabling a high degree of perceived meaningful interactivity. This paper proposes a new approach to the design of intelligent drama managers (DMs) where DM strategies are learned from a corpus of data collected from pen-and-paper RPG game sessions with expert human game masters. In particular, this paper focuses on the issues relating to the collection and annotation of relevant data from recorded gameplay sessions.\"",
        "Document: \"On the Applicability of Trusted Computing in Distributed Authorization Using Web Services. Distributed authorization provides the ability to control access to resources spread over the Internet. Typical authorization systems consider a range of security information like user identities, role identities or even temporal, spatial and contextual information associated with the access requestor. However, the ability to include computing platform related information has been quite limited due to constraints in identification and validation of platforms when distributed. Trusted computing is an exciting technology that can provide new ways to bridge this gap. In this paper, we provide the first steps necessary to achieving distributed authorization using trusted computing platforms. We introduce the notion of a Property Manifest that can be used in the specification of authorization policies. We provide an overview of our authorization architecture, its components and functions. We then illustrate the applicability of our system by implementing it in a Web service oriented architecture.\"",
        "Document: \"Analysis Of Existing Authorization Models And Requirements For Design Of Authorization Framework For The Service Oriented Architecture. Although there are several efforts underway to provide security for the Service Oriented Architecture (SOA), there is no specification or standard for authorization. We analyse the currently available authorization models for the Web services and business process layers comprising the SOA and highlight the features that are missing in each of them. Based on our analyst's of existing authorization models, we envisage an authorization framework for the SOA to provide extensions to both the security layers of Web services as well as business processes. We lay out separate design principles for authorization services in each of these layers.\"",
        "Document: \"Property Based Attestation and Trusted Computing: Analysis and Challenges. Trusted computing attestation mechanism relies on hash measurements to realize remote party attestation in distributed systems. Property based attestation enables more meaningful attestation by abstracting low level binary values to high level security properties or functions of systems. The contribution of this paper is two fold. In the first part of the paper, we provide an analysis of the different types of property based attestation mechanisms that have been proposed in the recent years. We categorize these mechanisms as derivation based, delegation based and enforcement based and analyze each of them with a particular focus on their limitations. In the second part, we provide a list of challenges for property based attestation. We believe this to be an useful exercise to help better understand the issues that limit the practical applicability of property based attestation in real world systems.\"",
        "Document: \"Ghost worlds \u2013 time and consequence in MMORPGs. MMORPGs are an increasingly popular form of entertainment, yet are limited in their ability to tell stories when compared to other media. This paper analyses some of the underlying reasons for this inability, using techniques form narrative analysis. One of the basic problems identified is that the design of MMORPGs inhibits the use of techniques used in other media to create engaging stories by manipulating the presentation of time. The other issue identified is the problems MMORPGs experience in presenting stories with meaningful consequence. A means to a possible solution to these problems, in separating the personal player view point from that of the overall world view, is discussed.\"",
        "Document: \"Security Issues in Asynchronous Transfer Mode. This paper addresses the design and management of security services for ATM networks. Various options for the positioning of security services within the ATM protocol stack are discussed. After considering these possibilities, it is proposed to place the security layer between the AAL and ATM layers. The proposed security layer provides confidentiality, integrity and data origin authentication in the user plane. The developed security design can be transparently integrated into the B-ISDN Protocol Reference Model without in any way violating the existing standards.\"",
        "Document: \"Towards a Secure Access Control Architecture for the Internet of Things. In this paper, we propose an access control architecture for IoT systems by developing a hybrid model with attributes, capabilities and role-based access control. We apply attributes for role-membership assignment and in permission evaluation, Membership of roles grants capabilities which are used to access specific services provided by things. This approach improves policy management for IoT systems with a large number of things and users.\"",
        "Document: \"ALOPA: Authorization Logic for Property Attestation in Trusted Platforms. Property based attestation is an extension of the proposed trusted computing attestation mechanism where binary measurements are abstracted to meaningful platform properties. In this paper, we propose ALOPA - A uthorization Lo gic for P roperty A ttestation, a logic based language for the specification and evaluation of authorization policies using properties in trusted platforms. Access control policies specified using ALOPA govern the access of platforms to resources on the basis of the platform's identity and a collection of rules based on platform properties, which determine, for any platform and any resource, the types of accesses the platform is allowed on the resource. Such an approach seems promising for developing secure distributed applications using property attestation based authorization for trusted platforms.\"",
        "Document: \"Trusted Administration of Large-Scale Cryptographic Role-Based Access Control Systems. There has been an increasing trend towards outsourcing data to the cloud to cope with the massive increase in the amount of data. Hence trusted enforcement of access control policies on outsourced data in the cloud has become a significant issue. In this paper we address trusted administration and enforcement of role-based access control policies on data stored in the cloud. Role-based access control (RBAC) simplifies the management of access control policies by creating two mappings; roles to permissions and users to roles. Recently crypto-based RBAC (C-RBAC) schemes have been developed which combine cryptographic techniques and access control to secured data in an outsourced environment. In such schemes, data is encrypted before outsourcing it and the ciphertext data is stored in the untrusted cloud. This ciphertext can only be decrypted by those users who satisfy the role-based access control policies. However such schemes assume the existence of a trusted administrator managing all the users and roles in the system. Such an assumption is not realistic in large-scale systems as it is impractical for a single administrator to manage the entire system. Though administrative models for RBAC systems have been proposed decentralize the administration tasks associated with the roles, these administrative models cannot be used in the C-RBAC schemes, as the administrative policies cannot be enforced in an untrusted distributed cloud environment. In this paper, we propose a trusted administrative model AdC-RBAC to manage and enforce role-based access policies for C-RBAC schemes in large-scale cloud systems. The AdC-RBAC model uses cryptographic techniques to ensure that the administrative tasks such as user, permission and role management are performed only by authorized administrative roles. Our proposed model uses role-based encryption techniques to ensure that only administrators who have the permissions to manage a role can add/revoke users to/from the role and owners can verify that a role is created by qualified administrators before giving out their data. We show how the proposed model can be used in an untrusted cloud while guaranteeing its security using cryptographic and trusted access control enforcement techniques.\"",
        "Document: \"Trust management for trusted computing platforms in web services. The concept of trusted platforms using trusted computing technology such as the Trusted Platform Module (TPM) is becoming significant in that such technologies are being increasingly available in PCs and mobile devices today. When such trusted platforms are used in applications, one of the key design issues is the ability to capture platform level requirements and to represent them as security policies for authorization decision making. This paper makes some contributions which we believe are an important first step in achieving policy based decision making with trusted platforms. It outlines a platform based trust management framework for specification of trust policies. In this context, we argue the need for a higher level abstraction that is able to capture the lower level state of the platform and use this in the evaluation of trust between the communicating entities. We extend the notion of trusted platform properties by introducing the concept of Component Property Certificates, which can be used in specifying and building trust relationships. We then illustrate how component property certificates can be used in the specification of trust policies of different granularities.\"",
        "1 is \"A Framework for Organization-Aware Agents.\", 2 is \"An exploration of user engagement in HCI\"",
        "Given above information, for an author who has written the paper with the title \"Managing emergent character-based narrative.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00762": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A system for translating locative prepositions from English into French':",
        "Document: \"Instance Selection by Border Sampling in Multi-class Domains. Instance selection is a pre-processing technique for machine learning and data mining. The main problem is that previous approaches still suffer from the difficulty to produce effective samples for training classifiers. In recent research, a new sampling technique, called Progressive Border Sampling (PBS), has been proposed to produce a small sample from the original labelled training set by identifying and augmenting border points. However, border sampling on multi-class domains is not a trivial issue. Training sets contain much redundancy and noise in practical applications. In this work, we discuss several issues related to PBS and show that PBS can be used to produce effective samples by removing redundancies and noise from training sets for training classifiers. We compare this new technique with previous instance selection techniques for learning classifiers, especially, for learning Na\u00efve Bayes-like classifiers, on multi-class domains except for one binary case which was for a practical application.\"",
        "Document: \"Supervised versus unsupervised binary-learning by feedforward neural networks. Binary classification is typically achieved by supervised learning methods. Nevertheless, it is also possible using unsupervised schemes. This paper describes a connectionist unsupervised approach to binary classification and compares its performance to that of its supervised counterpart. The approach consists of training an autoassociator to reconstruct the positive class of a domain at the output layer. After training, the autoassociator is used for classification, relying on the idea that if the network generalizes to a novel instance, then this instance must be positive, but that if generalization fails, then the instance must be negative. When tested on three real-world domains, the autoassociator proved more accurate at classification than its supervised counterpart, MLP, on two of these domains and as accurate on the third (Japkowicz, Myers, & Gluck, 1995). The paper seeks to generalize these results and concludes that, in addition to learning a concept in the absence of negative examples, 1) autoassociation is more efficient than MLP in multi-modal domains, and 2) it is more accurate than MLP in multi-modal domains for which the negative class creates a particularly strong need for specialization or the positive class creates a particularly weak need for specialization. In multi-modal domains for which the positive class creates a particularly strong need for specialization, on the other hand, MLP is more accurate than autoassociation.\"",
        "Document: \"Node similarity in the citation graph. Published scientific articles are linked together into a graph, the citation graph, through their citations. This paper explores the notion of similarity based on connectivity alone, and proposes several algorithms to quantify it. Our metrics take advantage of the local neighborhoods of the nodes in the citation graph. Two variants of link-based similarity estimation between two nodes are described, one based on the separate local neighborhoods of the nodes, and another based on the joint local neighborhood expanded from both nodes at the same time. The algorithms are implemented and evaluated on a subgraph of the citation graph of computer science in a retrieval context. The results are compared with text-based similarity, and demonstrate the complementarity of link-based and text-based retrieval.\"",
        "Document: \"Parallelizing Feature Selection. Classification is a key problem in machine learning/data mining. Algorithms for classification have the ability to predict the class of a new instance after having been trained on data representing past experience in classifying instances. However, the presence of a large number of features in training data can hurt the classification capacity of a machine learning algorithm. The Feature Selection problem involves discovering a subset of features such that a classifier built only with this subset would attain predictive accuracy no worse than a classifier built from the entire set of features. Several algorithms have been proposed to solve this problem. In this paper we discuss how parallelism can be used to improve the performance of feature selection algorithms. In particular, we present, discuss and evaluate a coarse-grained parallel version of the feature selection algorithm FortalFS. This algorithm performs well compared with other solutions and it has certain characteristics that makes it a good candidate for parallelization. Our parallel design is based on the master--slave design pattern. Promising results show that this approach is able to achieve near optimum speedups in the context of Amdahl's Law.\"",
        "Document: \"One-class classification - From theory to practice: A case-study in radioactive threat detection. \u2022Summarize and extend our previous work on gamma-ray spectra classification;\u2022Explore the problem from the perspective of one-class within-class imbalance;\u2022Demonstrate how within-class imbalance impacts one-class classifiers; and\u2022Demonstrate solutions to one-class within-class imbalance in our domain\"",
        "Document: \"A novelty detection approach to classification. Novelty Detection techniques are concept-learning methods that proceed by recognizing positive instances of a concept rather than differentiating between its positive and negative instances. Novelty Detection approaches consequently require very few, if any, negative training instances. This paper presents a particular Novelty Detection approach to classification that uses a Redundancy Compression and Non-Redundancy Differentiation technique based on the [Gluck & Myers, 1993] model of the hippocampus, a part of the brain critically involved in learning and memory. In particular, this approach consists of training an autoencoder to reconstruct positive input instances at the output layer and then using this autoencoder to recognize novel instances. Classification is possible, after training, because positive instances are expected to be reconstructed accurately while negative instances are not. The purpose of this paper is to compare HIPPO, the system that implements this technique, to C4.5 and feedforward neural network classification on several applications.\"",
        "Document: \"Cost-Based Sampling of Individual Instances. In many practical domains, misclassification costs can differ greatly and may be represented by class ratios, however, most learning algorithms struggle with skewed class distributions. The difficulty is attributed to designing classifiers to maximize the accuracy. Researchers call for using several techniques to address this problem including; under-sampling the majority class, employing a probabilistic algorithm, and adjusting the classification threshold. In this paper, we propose a general sampling approach that assigns weights to individual instances according to the cost function. This approach helps reveal the relationship between classification performance and class ratios and allows the identification of an appropriate class distribution for which, the learning method achieves a reasonable performance on the data. Our results show that combining an ensemble of Naive Bayes classifiers with threshold selection and under-sampling techniques works well for imbalanced data.\"",
        "Document: \"Evaluation Methods for Ordinal Classification. Ordinal classification is a form of multi-class classification where there is an inherent ordering between the classes, but not a meaningful numeric difference between them. Little attention has been paid as to how to evaluate these problems, with many authors simply reporting accuracy, which does not account for the severity of the error. Several evaluation metrics are compared across a dataset for a problem of classifying user reviews, where the data is highly skewed towards the highest values. Mean squared error is found to be the best metric when we prefer more (smaller) errors overall to reduce the number of large errors, while mean absolute error is also a good metric if we instead prefer fewer errors overall with more tolerance for large errors.\"",
        "Document: \"Visualizing Classifier Performance on Different Domains. In this paper, a new dynamic clustering algorithm basedon random sampling is proposed. The algorithm addresseswell known challenges in clustering such as Dynamism, Stability, and Scaling. The core of the proposed method isbased on the definition of a ...\"",
        "Document: \"Sampling Online Social Networks Using Coupling from the Past. Recent research has focused on sampling online social networks (OSNs) using traditional Markov Chain Monte Carlo (MCMC) techniques such as the Metropolis-Hastings algorithm (MH). While these methods have exhibited some success, the techniques suffer from slow mixing rates by themselves, and the resulting sample is usually approximate. An appealing solution is to apply the state of the art MCMC technique, Coupling From The Past (CFTP), for perfect sampling of OSNs. In this initial research, we explore theoretical and methodological issues such as customizing the update function and generating small sets of non-trivial states to adapt CFTP for sampling OSNs. Our research proposes the possibility of achieving perfect samples from large and complex OSNs using CFTP.\"",
        "1 is \"Insights into the dialogue processing of VERBMOBIL\", 2 is \"Flow Clustering Using Machine Learning Techniques\"",
        "Given above information, for an author who has written the paper with the title \"A system for translating locative prepositions from English into French\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00802": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Indexing multi-dimensional uncertain data with arbitrary probability density functions':",
        "Document: \"Dynamic anonymization: accurate statistical analysis with privacy preservation. A statistical database (StatDB) retrieves only aggregate results, as opposed to individual tuples. This paper investigates the construction of a privacy preserving StatDB that can (i) accurately answer an infinite number of counting queries, and (ii) effectively protect privacy against an adversary that may have acquired all the previous query results. The core of our solutions is a novel technique called dynamic anonymization. Specifically, given a query, we on the fly compute a tailor-made anonymized version of the microdata, which maximizes the precision of the query result. Privacy preservation is achieved by ensuring that the combination of all the versions deployed to process the past queries does not allow accurate inference of sensitive information. Extensive experiments with real data confirm that our technique enables highly effective data analysis, while offering strong privacy guarantees.\"",
        "Document: \"Personalized privacy preservation. We study generalization for preserving privacy in publication of sensitive data. The existing methods focus on a universal approach that exerts the same amount of preservation for all persons, with- out catering for their concrete needs. The consequence is that we may be offering insufficient protection to a subset of people, while applying excessive privacy control to another subset. Motivated by this, we present a new generalization framework based on the concept of personalized anonymity. Our technique performs the minimum generalization for satisfying everybody's requirements, and thus, retains the largest amount of information from the microdata. We carry out a careful theoretical study that leads to valuable insight into the behavior of alternative solutions. In particular, our analysis mathematically reveals the circumstances where the previous work fails to protect privacy, and establishes the superiority of the proposed solutions. The theoretical findings are verified with extensive experiments.\"",
        "Document: \"Obfuscating the Topical Intention in Enterprise Text Search. The text search queries in an enterprise can reveal the users' topic of interest, and in turn confidential staff or business information. To safeguard the enterprise from consequences arising from a disclosure of the query traces, it is desirable to obfuscate the true user intention from the search engine, without requiring it to be re-engineered. In this paper, we advocate a unique approach to profile the topics that are relevant to the user intention. Based on this approach, we introduce an $(\\epsilon_1, \\epsilon_2)$-privacy model that allows a user to stipulate that topics relevant to her intention at $\\epsilon_1$ level should appear to any adversary to be innocuous at $\\epsilon_2$ level. We then present a Top Priv algorithm to achieve the customized $(\\epsilon_1, \\epsilon_2)$-privacy requirement of individual users through injecting automatically formulated fake queries. The advantages of Top Priv over existing techniques are confirmed through benchmark queries on a real corpus, with experiment settings fashioned after an enterprise search application.\"",
        "Document: \"Private Release of Graph Statistics using Ladder Functions. Protecting the privacy of individuals in graph structured data while making accurate versions of the data available is one of the most challenging problems in data privacy. Most efforts to date to perform this data release end up mired in complexity, overwhelm the signal with noise, and are not effective for use in practice. In this paper, we introduce a new method which guarantees differential privacy. It specifies a probability distribution over possible outputs that is carefully defined to maximize the utility for the given input, while still providing the required privacy level. The distribution is designed to form a 'ladder', so that each output achieves the highest 'rung' (maximum probability) compared to less preferable outputs. We show how our ladder framework can be applied to problems of counting the number of occurrences of subgraphs, a vital objective in graph analysis, and give algorithms whose cost is comparable to that of computing the count exactly. Our experimental study confirms that our method outperforms existing methods for counting triangles and stars in terms of accuracy, and provides solutions for some problems for which no effective method was previously known. The results of our algorithms can be used to estimate the parameters of suitable graph models, allowing synthetic graphs to be sampled.\"",
        "Document: \"Mobile App Tagging. Mobile app tagging aims to assign a list of keywords indicating core functionalities, main contents, key features or concepts of a mobile app. Mobile app tags can be potentially useful for app ecosystem stakeholders or other parties to improve app search, browsing, categorization, and advertising, etc. However, most mainstream app markets, e.g., Google Play, Apple App Store, etc., currently do not explicitly support such tags for apps. To address this problem, we propose a novel auto mobile app tagging framework for annotating a given mobile app automatically, which is based on a search-based annotation paradigm powered by machine learning techniques. Specifically, given a novel query app without tags, our proposed framework (i) first explores online kernel learning techniques to retrieve a set of top-N similar apps that are semantically most similar to the query app from a large app repository; and (ii) then mines the text data of both the query app and the top-N similar apps to discover the most relevant tags for annotating the query app. To evaluate the efficacy of our proposed framework, we conduct an extensive set of experiments on a large real-world dataset crawled from Google Play. The encouraging results demonstrate that our technique is effective and promising.\n\n\"",
        "Document: \"iReduct: differential privacy with reduced relative errors. Prior work in differential privacy has produced techniques for answering aggregate queries over sensitive data in a privacy-preserving way. These techniques achieve privacy by adding noise to the query answers. Their objective is typically to minimize absolute errors while satisfying differential privacy. Thus, query answers are injected with noise whose scale is independent of whether the answers are large or small. The noisy results for queries whose true answers are small therefore tend to be dominated by noise, which leads to inferior data utility. This paper introduces iReduct, a differentially private algorithm for computing answers with reduced relative error. The basic idea of iReduct is to inject different amounts of noise to different query results, so that smaller (larger) values are more likely to be injected with less (more) noise. The algorithm is based on a novel resampling technique that employs correlated noise to improve data utility. Performance is evaluated on an instantiation of iReduct that generates marginals, i.e., projections of multi-dimensional histograms onto subsets of their attributes. Experiments on real data demonstrate the effectiveness of our solution.\"",
        "Document: \"Software process evaluation: A machine learning approach. Software process evaluation is essential to improve software development and the quality of software products in an organization. Conventional approaches based on manual qualitative evaluations (e.g., artifacts inspection) are deficient in the sense that (i) they are time-consuming, (ii) they suffer from the authority constraints, and (iii) they are often subjective. To overcome these limitations, this paper presents a novel semi-automated approach to software process evaluation using machine learning techniques. In particular, we formulate the problem as a sequence classification task, which is solved by applying machine learning algorithms. Based on the framework, we define a new quantitative indicator to objectively evaluate the quality and performance of a software process. To validate the efficacy of our approach, we apply it to evaluate the defect management process performed in four real industrial software projects. Our empirical results show that our approach is effective and promising in providing an objective and quantitative measurement for software process evaluation.\"",
        "Document: \"Optimal random perturbation at multiple privacy levels. Random perturbation is a popular method of computing anonymized data for privacy preserving data mining. It is simple to apply, ensures strong privacy protection, and permits effective mining of a large variety of data patterns. However, all the existing studies with good privacy guarantees focus on perturbation at a single privacy level. Namely, a fixed degree of privacy protection is imposed on all anonymized data released by the data holder. This drawback seriously limits the applicability of random perturbation in scenarios where the holder has numerous recipients to which different privacy levels apply. Motivated by this, we study the problem of multi-level perturbation, whose objective is to release multiple versions of a dataset anonymized at different privacy levels. The challenge is that various recipients may collude by sharing their data to infer privacy beyond their permitted levels. Our solution overcomes this obstacle, and achieves two crucial properties. First, collusion is useless, meaning that the colluding recipients cannot learn anything more than what the most trustable recipient (among the colluding recipients) already knows alone. Second, the data each recipient receives can be regarded (and hence, analyzed in the same way) as the output of conventional uniform perturbation. Besides its solid theoretical foundation, the proposed technique is both space economical and computationally efficient. It requires O (n+m) expected space, and produces a new anonymized version in O (n + log m) expected time, where n is the cardinality of the original dataset, and m the number of versions released previously. Both bounds are optimal under the realistic assumption that n \u00bb m.\"",
        "Document: \"HubPPR: Effective Indexing for Approximate Personalized PageRank. Personalized PageRank (PPR) computation is a fundamental operation in web search, social networks, and graph analysis. Given a graph G, a source s, and a target t, the PPR query \u03a0(s, t) returns the probability that a random walk on G starting from s terminates at t. Unlike global PageRank which can be effectively pre-computed and materialized, the PPR result depends on both the source and the target, rendering results materialization infeasible for large graphs. Existing indexing techniques have rather limited effectiveness; in fact, the current state-of-the-art solution, BiPPR, answers individual PPR queries without pre-computation or indexing, and yet it outperforms all previous index-based solutions. Motivated by this, we propose HubPPR, an effective indexing scheme for PPR computation with controllable tradeoffs for accuracy, query time, and memory consumption. The main idea is to pre-compute and index auxiliary information for selected hub nodes that are often involved in PPR processing. Going one step further, we extend HubPPR to answer top-k PPR queries, which returns the k nodes with the highest PPR values with respect to a source s, among a given set T of target nodes. Extensive experiments demonstrate that compared to the current best solution BiPPR, HubPPR achieves up to 10x and 220x speedup for PPR and top-k PPR processing, respectively, with moderate memory consumption. Notably, with a single commodity server, HubPPR answers a top-k PPR query in seconds on graphs with billions of edges, with high accuracy and strong result quality guarantees.\"",
        "Document: \"GBLENDER: visual subgraph query formulation meets query processing. Due to the complexity of graph query languages, the need for visual query interfaces that can reduce the burden of query formulation is fundamental to the spreading of graph data management tools to wider community. We present a novel HCI (human-computer interaction)-aware graph query processing paradigm, where instead of processing a query graph after its construction, it interleaves visual query construction and processing to improve system response time. We demonstrate a system called GBLENDER that exploits GUI latency to prune false results and prefetch candidate data graphs by employing a novel action-aware indexing scheme and a data structure called spindle-shaped graphs (SPIG). We demonstrate various innovative features of GBLENDER and its promising performance in evaluating subgraph containment and similarity queries.\"",
        "1 is \"Spatial keyword query processing: an experimental evaluation\", 2 is \"SkewTune: mitigating skew in mapreduce applications\"",
        "Given above information, for an author who has written the paper with the title \"Indexing multi-dimensional uncertain data with arbitrary probability density functions\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00807": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'About Universal Hybrid Networks of Evolutionary Processors of Small Size':",
        "Document: \"Investigations On Natural Computing In The Institute Of Mathematics And Computer Science. We describe the investigations on natural computing in the Institute of Mathematics and Computer Science of the Academy of Sciences of Moldova during last fifteen years. Most of these investigations are inspired by results and ideas belonging to Corresponding Member of the Romanian Academy Gheorghe Paun.\"",
        "Document: \"On the number of nodes in universal networks of evolutionary processors. We consider the networks of evolutionary processors (NEP) introduced by J. Castellanos, C. Mart\u00ed n-Vide, V. Mitrana and J. Sempere recently. We show that every recursively enumerable (RE) language can be generated by an NEP with three nodes modulo a terminal alphabet and moreover, NEPs with four nodes can generate any RE language. Thus, we improve existing universality result from five nodes down to four nodes. For mNEPs (a variant of NEPs where operations of different kinds are allowed in the same node) we obtain optimal results: each RE language can be generated by an mNEP with one node modulo a terminal alphabet, and mNEPs with two nodes can generate any RE language; this is not possible for mNEPs with one node. Some open problems are formulated.\"",
        "Document: \"P Systems with Insertion and Deletion Exo-Operations. It is known that insertion-deletion (P) systems with two symbols context-free insertion and deletion rules are not computationally complete. It is thus interesting to consider conditions that would allow such systems to reach computational completeness. In this paper we consider insertion-deletion P systems with insertion and deletion operations applied only at the ends of string (we call them exo-operations). We show that such systems with one-symbol insertion and deletion of up to two symbols are computationally complete, and so are systems with insertion of up to two symbols and one-symbol deletion. The question about the computational power of insertion-deletion P systems with one-symbol insertion and one-symbol deletion operations applied at the ends of string is open. However, the tissue P systems reach computationally completeness even in this case.\"",
        "Document: \"New Small Universal Circular Post Machines. We consider a new kind of machines with a circular tape and moving in one direction only, so-called Circular Post machines. Using 2- tag systems we construct some new small universal machines of this kind.\"",
        "Document: \"About Precise Characterization Of Languages Generated By Hybrid Networks Of Evolutionary Processors With One Node. A hybrid network of evolutionary processors (an HNEP) is a graph where each node is associated with an evolutionary processor (a special rewriting system), a set of words, an input filter and an output filter. Every evolutionary processor is given with a finite set of one type of point mutations (an insertion, a deletion or a substitution of a symbol) which can be applied to certain positions of a string over the domain of the set of these rewriting rules. The HNEP functions by rewriting the words that can be found at the nodes and then re-distributing the resulting strings according to a communication protocol based on a filtering mechanism. The filters are defined by certain variants of random-context conditions. In this paper we complete investigation of HNEPs with one node and present a precise description of languages generated by them.\"",
        "Document: \"Solving PP-Complete and #P-Complete Problems by P Systems with Active Membranes. Membrane computing is a formal framework of distributed parallel multiset processing. Due to massive parallelism and exponential space some intractable computational problems can be solved by P systems with active membranes in a polynomial number of steps. In this paper we generalize this approach from decisional problems to the computational ones, by providing a solution of a #P-complete problem, namely to compute the permanent of a binary matrix. The implication of this result to the PP complexity class is discussed and compared to known results about NP\u2009\u222a\u2009co\u2009\u2212\u2009 NP.\"",
        "Document: \"Self-describing Turing machines. After a sketchy historical account on the question of self-describeness and self-reproduction, and after discussing the definition of suitable encodings for self-describeness, we give the construction of several self-describing Turing machines, namely self-describing machines with, respectively, 350, 267, 224 and 206 instructions.\"",
        "Document: \"One-membrane symport with few extra symbols. Membrane systems with symbol objects are formal models of distributed parallel multiset processing. Symport rules move multiple objects to a neighbouring region. It is known that for P systems with symport rules of weight at most 3 and a single membrane, seven superfluous symbols are enough for computational completeness, and one is necessary. We present the improvements of the lower bounds on the generative power of P systems with symport of weight bounded by 3 and 4, in particular, establishing that six and two extra symbols suffice, respectively. Besides maximally parallel P systems, we also consider sequential ones. In fact, all presented non-universality lower bound results, together with all upper bound results, hold also in this case, yielding the current state of the art.\"",
        "Document: \"On the size of computationally complete hybrid networks of evolutionary processors. A hybrid network of evolutionary processors (an HNEP) is a graph where each node is associated with an evolutionary processor (a special rewriting system), a set of words, an input filter and an output filter. Every evolutionary processor is given with a finite set of one type of point mutations (an insertion, a deletion or a substitution of a symbol) which can be applied to certain positions of a string over the domain of the set of these rewriting rules. The HNEP functions by rewriting the words that can be found at the nodes and then re-distributing the resulting strings according to a communication protocol based on a filtering mechanism. The filters are defined by certain variants of random-context conditions. HNEPs can be considered as both language generating devices (GHNEPs) and language accepting devices (AHNEPs). In this paper, by improving the previous results, we prove that any recursively enumerable language can be determined by a GHNEP and an AHNEP with 7 nodes. We also show that the families of GHNEPs and AHNEPs with 2 nodes are not computationally complete.\"",
        "Document: \"Computational power of symport/antiport: history, advances, and open problems. We first give a historical overview of the most important results obtained in the area of P systems and tissue P systems with symport/antiport rules, especially with respect to the development of computational completeness results improving descriptional complexity parameters. We consider the number of membranes (cells in tissue P systems), the weight of the rules, and the number of objects. Then we establish our newest results: P systems with only one membrane, symport rules of weight three, and with only seven additional objects remaining in the skin membrane at the end of a halting computation are computationally complete; P systems with minimal cooperation, i.e., P systems with symport/antiport rules of size one and P systems with symport rules of weight two, are computationally complete with only two membranes with only three and six, respectively, superfluous objects remaining in the output membrane at the end of a halting computation.\"",
        "1 is \"Parallel communicating grammar systems with bounded resources\", 2 is \"Asynchronous and maximally parallel deterministic controlled non-cooperative p systems characterize NFIN and coNFIN\"",
        "Given above information, for an author who has written the paper with the title \"About Universal Hybrid Networks of Evolutionary Processors of Small Size\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00919": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Smoothing and Decomposition for Analysis Sparse Recovery':",
        "Document: \"Fast gradient-based algorithms for constrained total variation image denoising and deblurring problems. This paper studies gradient-based schemes for image denoising and deblurring problems based on the discretized total variation (TV) minimization model with constraints. We derive a fast algorithm for the constrained TV-based image deburring problem. To achieve this task, we combine an acceleration of the well known dual approach to the denoising problem with a novel monotone version of a fast iterative shrinkage/thresholding algorithm (FISTA) we have recently introduced. The resulting gradient-based algorithm shares a remarkable simplicity together with a proven global rate of convergence which is significantly better than currently known gradient projections-based methods. Our results are applicable to both the anisotropic and isotropic discretized TV functionals. Initial numerical results demonstrate the viability and efficiency of the proposed algorithms on image deblurring problems with box constraints.\"",
        "Document: \"Convexity Properties Associated with Nonconvex Quadratic Matrix Functions and Applications to Quadratic Programming. We establish several convexity results which are concerned with nonconvex quadratic matrix (QM) functions: strong duality\n of quadratic matrix programming problems, convexity of the image of mappings comprised of several QM functions and existence\n of a corresponding S-lemma. As a consequence of our results, we prove that a class of quadratic problems involving several\n functions with similar matrix terms has a zero duality gap. We present applications to robust optimization, to solution of\n linear systems immune to implementation errors and to the problem of computing the Chebyshev center of an intersection of\n balls.\"",
        "Document: \"Doubly Constrained Robust Capon Beamformer With Ellipsoidal Uncertainty Sets. The doubly constrained robust (DCR) Capon beamformer with a spherical uncertainty set was introduced and studied by Stoica and Wang. Here, we consider the generalized DCR problem (GDCR) in which the uncertainty set is an ellipsoid rather than a sphere. Although, as noted previously by Stoica and Wang, this problem is nonconvex and appears to be intractable, we show that it can be solved efficiently. In fact, we prove that the GDCR beamformer can be obtained as a solution to a convex optimization problem. To this end, we first derive a strong duality result for nonconvex quadratic programs with two quadratic constraints over the complex domain. Specializing the results to our context leads to a semidefinite programming formulation of the GDCR beamformer\"",
        "Document: \"On Fienup Methods for Regularized Phase Retrieval. Alternating minimization, or Fienup methods, have a long history in phase retrieval. We provide new insights related to the empirical and theoretical analysis of these algorithms when used with Fourier measurements and combined with convex priors. In particular, we show that Fienup methods can be viewed as performing alternating minimization on a regularized nonconvex least-squares problem with re...\"",
        "Document: \"A first order method for finding minimal norm-like solutions of convex optimization problems. We consider a general class of convex optimization problems in which one seeks to minimize a strongly convex function over a closed and convex set which is by itself an optimal set of another convex problem. We introduce a gradient-based method, called the minimal norm gradient method, for solving this class of problems, and establish the convergence of the sequence generated by the algorithm as well as a rate of convergence of the sequence of function values. The paper ends with several illustrating numerical examples.\"",
        "Document: \"On the Solution of the Tikhonov Regularization of the Total Least Squares Problem. Total least squares (TLS) is a method for treating an overdetermined system of linear equations ${\\bf A} {\\bf x} \\approx {\\bf b}$, where both the matrix ${\\bf A}$ and the vector ${\\bf b}$ are contaminated by noise. Tikhonov regularization of the TLS (TRTLS) leads to an optimization problem of minimizing the sum of fractional quadratic and quadratic functions. As such, the problem is nonconvex. We show how to reduce the problem to a single variable minimization of a function ${\\mathcal{G}}$ over a closed interval. Computing a value and a derivative of ${\\mathcal{G}}$ consists of solving a single trust region subproblem. For the special case of regularization with a squared Euclidean norm we show that ${\\mathcal{G}}$ is unimodal and provide an alternative algorithm, which requires only one spectral decomposition. A numerical example is given to illustrate the effectiveness of our method.\"",
        "Document: \"A fast dual proximal gradient algorithm for convex minimization and applications. We consider the convex composite problem of minimizing the sum of a strongly convex function and a general extended valued convex function. We present a dual-based proximal gradient scheme for solving this problem. We show that although the rate of convergence of the dual objective function sequence converges to the optimal value with the rate O(1/k2), the rate of convergence of the primal sequence is of the order O(1/k).\"",
        "Document: \"On Fienup Methods for Sparse Phase Retrieval. Alternating minimization, or Fienup methods, have a long history in phase retrieval. We provide new insights related to the empirical and theoretical analysis of these algorithms when used with Fourier measurements and combined with convex priors. In particular, we show that Fienup methods can be viewed as performing alternating minimization on a regularized nonconvex least-squares problem with respect to amplitude measurements. Furthermore, we prove that under mild additional structural assumptions on the prior (semialgebraicity), the sequence of signal estimates has a smooth convergent behavior toward a critical point of the nonconvex regularized least-squares objective. Finally, we propose an extension to Fienup techniques, based on a projected gradient descent interpretation and acceleration using inertial terms. We demonstrate experimentally that this modification combined with an    $ell _1$   prior constitutes a competitive approach for sparse phase retrieval.\"",
        "Document: \"On the Convergence of Alternating Minimization for Convex Programming with Applications to Iteratively Reweighted Least Squares and Decomposition Schemes. This paper is concerned with the alternating minimization (AM) method for solving convex minimization problems where the decision variables vector is split into two blocks. The objective function is a sum of a differentiable convex function and a separable (possibly) nonsmooth extended real-valued convex function, and consequently constraints can be incorporated. We analyze the convergence rate of the method and establish a nonasymptotic sublinear rate of convergence where the multiplicative constant depends on the minimal block Lipschitz constant. We then analyze the iteratively reweighted least squares (IRLS) method for solving convex problems involving sums of norms. Based on the results derived for the AM method, we establish a nonasymptotic sublinear rate of convergence of the IRLS method. In addition, we show an asymptotic rate of convergence whose efficiency estimate does not depend on the data of the problem. Finally, we study the convergence properties of a decomposition-based approach designed to solve a composite convex model.\"",
        "Document: \"Structured Total Maximum Likelihood: An Alternative to Structured Total Least Squares. Linear inverse problems with uncertain measurement matrices appear in many different applications. One of the standard techniques for solving such problems is the total least squares (TLS) method. Recently, an alternative approach has been suggested, based on maximizing an appropriate likelihood function assuming that the measurement matrix consists of random Gaussian variables. We refer to this technique as the total maximum likelihood (TML) method. Here we extend this strategy to the case in which the measurement matrix is structured so that the perturbations are not arbitrary but rather follow a fixed pattern. The resulting estimate is referred to as the structured TML (STML). As we show, the STML can be viewed as a regularized version of the structured TLS (STLS) approach in which the regularization consists of a logarithmic penalty. In contrast to the STLS solution, the STML always exists. Furthermore, its performance in practice tends to be superior to that of the STLS and competitive to other regularized solvers, as we illustrate via several examples. We also consider a few interesting special cases in which the STML can be computed efficiently either by reducing it into a one-dimensional problem regardless of the problem size or by a decomposition via a discrete Fourier transform.\"",
        "1 is \"Layering as optimization decomposition: A  mathematical theory of network architectures\", 2 is \"Bearing estimation for a distributed source: modeling, inherent accuracy limitations and algorithms\"",
        "Given above information, for an author who has written the paper with the title \"Smoothing and Decomposition for Analysis Sparse Recovery\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00935": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Robust Hyperspectral Unmixing with Correntropy based Metric':",
        "Document: \"Prediction of protein complexes based on protein interaction data and functional annotation data using kernel methods. Prediction of protein complexes is a crucial problem in computational biology. The increasing amount of available genomic data can enhance the identification of protein complexes. Here we describe an approach for predicting protein complexes based on integration of protein-protein interaction (PPI) data and protein functional annotation data. The basic idea is that proteins in protein complexes often interact with each other and protein complexes exhibit high functional consistency/even multiple functional consistency. We create a protein-protein relationship network (PPRN) via a kernel-based integration of these two genomic data. Then we apply the MCODE algorithm on PPRN to detect network clusters as numerically determined protein complexes. We present the results of the approach to yeast Sacchromyces cerevisiae. Comparison with well-known experimentally derived complexes and results of other methods verifies the effectiveness of our approach.\"",
        "Document: \"Computational systems biology: integration of sequence, structure, network, and dynamics. A report of the 4nd International Conference on Computational Systems Biology, 9-11 September 2010, Suzhou, China.\"",
        "Document: \"Haplotype assembly from aligned weighted SNP fragments. Given an assembled genome of a diploid organism the haplotype assembly problem can be formulated as retrieval of a pair of haplotypes from a set of aligned weighted SNP fragments. Known computational formulations (models) of this problem are minimum letter flips (MLF) and the weighted minimum letter flips (WMLF; Greenberg et al. (INFORMS J. Comput. 2004, 14, 211-213)). In this paper we show that the general WMLF model is NP-hard even for the gapless case. However the algorithmic solutions for selected variants of WMFL can exist and we propose a heuristic algorithm based on a dynamic clustering technique. We also introduce a new formulation of the haplotype assembly problem that we call COMPLETE WMLF (CWMLF). This model and algorithms for its implementation take into account a simultaneous presence of multiple kinds of data errors. Extensive computational experiments indicate that the algorithmic implementations of the CWMLF model achieve higher accuracy of haplotype reconstruction than the WMLF-based algorithms, which in turn appear to be more accurate than those based on MLF.\"",
        "Document: \"Modelling biological systems from molecules to dynamical networks. A report of the 5th IEEE International Conference on Systems Biology (IEEE ISB2011), 2-4 September 2011, Zhuhai, China.\"",
        "Document: \"Two-stage flux balance analysis of metabolic networks for drug target identification. Efficient identification of drug targets is one of major challenges for drug discovery and drug development. Traditional approaches to drug target identification include literature search-based target prioritization and in vitro binding assays which are both time-consuming and labor intensive. Computational integration of different knowledge sources is a more effective alternative. Wealth of omics data generated from genomic, proteomic and metabolomic techniques changes the way researchers view drug targets and provides unprecedent opportunities for drug target identification.In this paper, we develop a method based on flux balance analysis (FBA) of metabolic networks to identify potential drug targets. This method consists of two linear programming (LP) models, which first finds the steady optimal fluxes of reactions and the mass flows of metabolites in the pathologic state and then determines the fluxes and mass flows in the medication state with the minimal side effect caused by the medication. Drug targets are identified by comparing the fluxes of reactions in both states and examining the change of reaction fluxes. We give an illustrative example to show that the drug target identification problem can be solved effectively by our method, then apply it to a hyperuricemia-related purine metabolic pathway. Known drug targets for hyperuricemia are correctly identified by our two-stage FBA method, and the side effects of these targets are also taken into account. A number of other promising drug targets are found to be both effective and safe.Our method is an efficient procedure for drug target identification through flux balance analysis of large-scale metabolic networks. It can generate testable predictions, provide insights into drug action mechanisms and guide experimental design of drug discovery.\"",
        "Document: \"Discovery of co-occurring driver pathways in cancer. It has been widely realized that pathways rather than individual genes govern the course of carcinogenesis. Therefore, discovering driver pathways is becoming an important step to understand the molecular mechanisms underlying cancer and design efficient treatments for cancer patients. Previous studies have focused mainly on observation of the alterations in cancer genomes at the individual gene or single pathway level. However, a great deal of evidence has indicated that multiple pathways often function cooperatively in carcinogenesis and other key biological processes.In this study, an exact mathematical programming method was proposed to de novo identify co-occurring mutated driver pathways (CoMDP) in carcinogenesis without any prior information beyond mutation profiles. Two possible properties of mutations that occurred in cooperative pathways were exploited to achieve this: (1) each individual pathway has high coverage and high exclusivity; and (2) the mutations between the pair of pathways showed statistically significant co-occurrence. The efficiency of CoMDP was validated first by testing on simulated data and comparing it with a previous method. Then CoMDP was applied to several real biological data including glioblastoma, lung adenocarcinoma, and ovarian carcinoma datasets. The discovered co-occurring driver pathways were here found to be involved in several key biological processes, such as cell survival and protein synthesis. Moreover, CoMDP was modified to (1) identify an extra pathway co-occurring with a known pathway and (2) detect multiple significant co-occurring driver pathways for carcinogenesis.The present method can be used to identify gene sets with more biological relevance than the ones currently used for the discovery of single driver pathways.\"",
        "Document: \"Inferring transcriptional regulatory networks from high-throughput data. Inferring the relationships between transcription factors (TFs) and their targets has utmost importance for understanding the complex regulatory mechanisms in cellular systems. However, the transcription factor activities (TFAs) cannot be measured directly by standard microarray experiment owing to various post-translational modifications. In particular, cooperative mechanism and combinatorial control are common in gene regulation, e.g. TFs usually recruit other proteins cooperatively to facilitate transcriptional reaction processes.In this article, we propose a novel method for inferring transcriptional regulatory networks (TRN) from gene expression data based on protein transcription complexes and mass action law. With gene expression data and TFAs estimated from transcription complex information, the inference of TRN is formulated as a linear programming (LP) problem which has a globally optimal solution in terms of L(1) norm error. The proposed method not only can easily incorporate ChIP-Chip data as prior knowledge, but also can integrate multiple gene expression datasets from different experiments simultaneously. A unique feature of our method is to take into account protein cooperation in transcription process. We tested our method by using both synthetic data and several experimental datasets in yeast. The extensive results illustrate the effectiveness of the proposed method for predicting transcription regulatory relationships between TFs with co-regulators and target genes.\"",
        "Document: \"A fast haplotype inference method for large population genotype data. With the rapid progress of genotyping techniques, many large-scale, genome-wide disease studies are now under way. One of the challenges of large disease-association studies is developing a fast and accurate computing method for haplotype inference from genotype data. In this paper, a new computing method for population-based haplotype inference problem is proposed. The designed method does not assume haplotype blocks in the population and allows each individual haplotype to have its own structure, and thus is able to accommodate recombination and obtain higher adaptivity to the genotype data, specifically in the case of long marker maps. This method develops a dynamic programming algorithm, which is theoretically guaranteed to find exact maximum likelihood solutions of the variable order Markov chain model for haplotype inference problem within linear running time. Hence, it is fast and, as a result, practicable for large genotype datasets. Through extensive computational experiments on large-scale real genotype data, the proposed method is shown to be fast and efficient.\"",
        "Document: \"Inferring gene regulatory networks from expression data with prior knowledge by linear programming. Inferring gene regulatory networks from gene expression data is an important task in biological studies. In this work, we proposed an optimization model to infer regulatory relations among the functional genes from expression data based on the structural sparsity and/or prior knowledge. Specifically, we achieved the structural sparsity of the network by implementing a linear programming model, which also satisfies the conditions of the existing knowledge. The gene regulatory network is reconstructed by enforcing the sparse linkages with the consistency to the prior knowledge. The effectiveness of the method are demonstrated by several simulated experiments.\"",
        "Document: \"A smoothing Levenberg\u2013Marquardt method for NCP. In this paper, we convert the nonlinear complementarity problems to an equivalent smooth nonlinear equation system by using smoothing technique. Then we use Levenberg\u2013Marquardt type method to solve the nonlinear equation system. The method has the following merits: (i) any cluster point of the iteration sequence is a solution of the P0\u2212NCP; (ii) it generates a bounded sequence if the P0\u2212NCP has a nonempty and bounded solution set; (iii) if the generalized Jacobian is nonsingular at a solution point, then the whole sequence converges to the (unique) solution of the P0\u2212NCP superlinearly; (iv) for the P0\u2212NCP, if an accumulation point of the iteration sequence satisfies strict complementary condition, then the whole sequence converges to this accumulation point superlinearly.\"",
        "1 is \"From genomics to chemical genomics: new developments in KEGG.\", 2 is \"On-Line Selection of Discriminative Tracking Features\"",
        "Given above information, for an author who has written the paper with the title \"Robust Hyperspectral Unmixing with Correntropy based Metric\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "00978": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Improved Capacity Scaling in Wireless Networks With Infrastructure':",
        "Document: \"Aligned interference neutralization and the degrees of freedom of the 2 \u00d7 2 \u00d7 2 interference channel. We show that the 2 \u00d7 2 \u00d7 2 interference network, i.e., the multihop interference network formed by concatenation of two 2-user interference channels achieves the min-cut outer bound value of 2 DoF, for almost all values of channel coefficients, for both time-varying or fixed channel coefficients. The key to this result is a new idea, called aligned interference neutralization, that provides a way to align interference terms over each hop in a manner that allows them to be cancelled over the air at the last hop.\"",
        "Document: \"Network Coding for Two-Way Relay Channels using Lattices. In this paper, we propose a network coding using a lattice for the two-way relay channel with two nodes communi- cating bidirectionally through a relay, which we call modulo-and- forward (MF). Our scheme extends the network coding in the binary channel to the Gaussian channel case, where XOR in the binary case is replaced by mod\u039b for the Gaussian case, where \u039b is a high-dimensional lattice whose shaping gain is close to optimal. If the relay node re-transmits the received signal after the mod \u039b operation, we can reduce the complexity compared to decode-and-forward (DF) and can get a better power efficiency compared to amplify-and-forward (AF). When the transmission powers of two nodes are different, we use superposition coding and partial decoding at the relay node. Finally, we plot and compare the sum rates of three different schemes, i.e., AF, DF, and MF. We show that by applying the proposed scheme, we can get better performance than AF and DF schemes under some conditions.\"",
        "Document: \"Fixed power allocation with nulling for TDD-based cellular uplink. We propose two fixed power allocation schemes with nulling (FPA-N1 and FPA-N2) for time division duplex (TDD) based cellular uplink according to the location of mobile stations (MSs). In the FPA-N1 scheme, MSs located near a base station (BS) do not transmit data when the wireless channels between the MSs and their home cell BS experience deep fading. In the FPA-N2 scheme, MSs located near cell boundaries do not transmit data when the wireless channels between the MSs and neighboring cell BSs cause high interference channel gain because, in this case, their data transmission may induce large interference to neighboring cells. Numerical results show that the proposed power allocation scheme improves the uplink capacity in cellular networks.\"",
        "Document: \"An optimal soft handoff algorithm for rayleigh fading channels. In this paper, we design and analyze a soft handoff algorithm for code-division multiple access (CDMA) systems in a Rayleigh fading environment. For each mobile, this algorithm selects a set of base stations to be in handoff with the mobile based on the average channel conditions between the mobile and the base stations. For a given target value for the average number of base stations in handoff, our handoff algorithm is optimal in that it maximizes the effective channel gain seen by mobiles. We consider both power control and rate control in the forward and reverse links of CDMA systems, where maximizing the effective channel gain minimizes the average transmit power for the power controlled case and maximizes the average received power for the rate controlled case, respectively.\"",
        "Document: \"Predictive transmit beamforming for MIMO-OFDM in time-varying channels with limited feedback. A limited feedback-based transmit beamforming technique for multiple-input multiple-output orthogonal frequency division multiplexing (MIMO-OFDM) is investigated in time-varying channels. The performance of the system is significantly degraded by outdated feedback information even when the channel varies slowly. To compensate for the impairment in time-varying channels, the optimal transmit beamforming vector for a future channel, which maximizes the expected effective channel gain, is derived by applying the autoregressive (AR) model to the channels. These are obtained at the receiver. Following this, schemes for the selection of beamforming vectors are proposed to reduce the feedback amount. These can effectively reduce the amount of feedback information by utilizing both the frequency and time correlation of transmit beamforming vectors. Simulation results show that the proposed techniques outperform existing schemes in terms of the bit error rate (BER) performance with the same amount of feedback.\"",
        "Document: \"Capacity Evaluation of Various Multiuser MIMO Schemes in Downlink Cellular Environments. Presented in this paper is a study of the capacity evaluation of various multiuser MIMO schemes in cellular environments. The throughputs per user of the generalized zero-forcing with rank adaptation and vector perturbation schemes are compared with the capacity bound of the Gaussian MIMO broadcast channel, obtained by dirty paper coding under proportional fairness scheduling. The average cell throughputs of these schemes are also compared. From these comparisons, this study provides vital information for applying multiuser MIMO schemes in multicell environments\"",
        "Document: \"Marton-Marton coding for a broadcast relay network. We consider a discrete memoryless broadcast relay network that consists of one transmitter, two receivers and a relay. The transmitter sends independent messages to each receiver using Marton coding. The relay performs decode-and-forward using another Marton coding. We show the achievable rate region of the proposed scheme and also provide an outer bound for the channel.\"",
        "Document: \"Outage Analysis for MIMO Rician Channels and Channels with Partial CSI. We analyze the outage performance and diversity-multiplexing tradeoff (DMT), originally introduced by Zheng and Tse, for multiple antenna Rician channels or channels with partial channel state information at the transmitter (CSIT). The asymptotic behaviors of the outage are analyzed in the limit of high signal-to-noise ratio (SNR). We also analyze the SNR where the maximum diversity order (MDO) is obtained for Rician channels, which serves as a desired operation point. In addition, by applying the outage analysis for Rician channels we present the differential DMT (DDMT) to develop a better understanding of the asymptotic relationship between the outage probability, transmission rate, SNR, and Rician factor\"",
        "Document: \"Nested Lattice Codes for Gaussian Relay Networks With Interference. In this paper, we consider a class of single-source multicast relay networks. We assume that all outgoing channels of a node in the network to its neighbors are orthogonal while the incoming signals from its neighbors can interfere with each other. We first focus on Gaussian relay networks with interference and find an achievable rate using a lattice coding scheme. We show that the achievable rate of our scheme is within a constant bit gap from the information theoretic cut-set bound, where the constant depends only on the network topology, but not on the transmit power, noise variance, and channel gains. This is similar to a recent result by Avestimehr, Diggavi, and Tse, who showed an approximate capacity characterization for general Gaussian relay networks. However, our achievability uses a structured code instead of a random one. Using the idea used in the Gaussian case, we also consider a linear finite-field symmetric network with interference and characterize its capacity using a linear coding scheme.\"",
        "Document: \"Capacity of a class of multi-source relay networks. We consider a relay network having K source-destination pairs. Finding the capacity region of such a network with multiple unicast sessions is in general difficult. By focusing on a special class of such networks, we show that the capacity can be found. Namely, we consider a linear finite-field channel model, which can model interference in the network. Furthermore, we assume time-varying channels. We propose a block Markov encoding and relaying scheme that exploits such channel variations. By comparing its achievable sum-rate with the general cut-set upper bound, we show the sum capacity can be characterized for a certain class of channel distributions and network topologies.\"",
        "1 is \"RF Localization for Wireless Video Capsule Endoscopy.\", 2 is \"A New Upper Bound on the Capacity of a Class of Primitive Relay Channels\"",
        "Given above information, for an author who has written the paper with the title \"Improved Capacity Scaling in Wireless Networks With Infrastructure\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001091": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Real-Time Refinement: Semantics and Application':",
        "Document: \"Context aware architecture for sending adaptive HELLO messages in VANET. With vehicular Ad hoc Network (VANET) being considered a vital application of Mobile ad hoc Network (MANET), this paper will focus on exploiting the aspect of context aware system to leverage the efficiency of routing protocols in VANET. Our investigation will concentrate on the vehicle to vehicle (V2V) communication type rather than the vehicle to Infrastructures (V2I) type, owing to the lack of infrastructures and difficulties involved in providing a comprehensive covering for all roads because of the high expense of installation. Many challenges (e.g. routing issues and security) have emerged in this area, encouraging many researchers to investigate their research in attempting to meet these challenges. Routing protocols are considered as one of the critical dilemmas that need to be tackled; designing an efficient routing mechanism has an impact on enhancing the network performance in terms of disseminating messages to its desired destination. Thus we concentrate in this paper on dealing with the routing issues particularly in minimising overhead and bandwidth consumptions, which in turn will lead to effects on the time delay of packet dissemination. To do this, we introduce a novel technique and architecture that organises the broadcast process of HELLO beacon message by sending these messages according to the circumstances of each vehicle in the network, for instance changing a vehicle's direction and exceeding a predefined range of speed, which has an impact on changing the network topology in the road.\"",
        "Document: \"The implementation of an intelligent and video-based fall detection system using a neural network. This paper presents the development of a smart fall detector to minimise accidental falls which occur among elderly people, especially for indoor situations. A video-based detection system was utilised, as this can preserve privacy and monitor the physical activities of elderly people. In order to identify the correct situation among a set of predetermined situations, which consisted of praying, sitting, standing, bending, kneeling and lying down, a neural network system was incorporated in the fall detection computation algorithm. The neural network analysed the binary map image of the person and then identified which plausible situation the person was in at any particular instant in time. The fall detector's performance in successfully detecting falls was then evaluated using two statistical metrics: specificity and sensitivity. The performance of this fall detection system in identifying falls was also evaluated during two non-normal gait movements, stumbling and limping, so as to mimic the motions of a good proportion of the elderly people having these types of walking gait movements. It was shown that the implemented video-based fall detection system could be a promising solution for detecting indoor falls among senior citizens.\"",
        "Document: \"Security Solution for Mobile Ad Hoc Network of Networks (MANoN). Our definition for Mobile Ad hoc Network of Networks (MANoNs) are a group of large autonomous wireless nodes communicating on a peer-to-peer basis in a heterogeneous environment with no pre-defined infrastructure. In fact, each node by itself is an ad hoc network with its own management. Based on the Recommendation ITU-T M.3400 security management consisting of security administration, prevention and detection of malicious nodes and containment and recovery is considered to be one of the major problems MANoN is facing. This paper proposes a novel behaviour detection algorithm combined with threshold cryptography digital certificates to satisfy prevention and detection to securely manage our system. This technique will be evaluated using Network Simulator NS-2 to provide and check whether security requirements are met in a comprehensive manner.\"",
        "Document: \"ASDL: a wide spectrum language for designing web services. A Service oriented system emerges from composition of services. Dynamically composed reactive Web services form a special class of service oriented system, where the delays associated with communication, unreliability and unavailability of services, and competition for resources from multiple service requesters are dominant concerns. As complexity of services increase, an abstract design language for the specification of services and interaction between them is desired. In this paper, we present ASDL (Abstract Service Design Language), a wide spectrum language for modelling Web services. We initially provide an informal description of our computational model for service oriented systems. We then present ASDL along with its specification oriented semantics defined in Interval Temporal Logic (ITL): a sound formalism for specifying and reasoning about temporal properties of systems. The objective of ASDL is to provide a notation for the design of service composition and interaction protocols at an abstract level.\"",
        "Document: \"Security Management for Mobile Ad Hoc Network of Networks MANoN. Many military research efforts have concentrated on how to allow war-fighters to take advantage of all available information within the battlefield in a rapid and flexible manner. As a result, the development of the Global Information Grid GIG was the key enabler for this process; hence, adding to the development of the mobile networking part of the GIG, the concept of the Mobile Ad hoc Network of Networks MANoN is introduced. This article proposes a novel security management algorithm achieving the three management essentials: Security Administration; Prevention and Detection; and Containment and Recovery; based on the International Telecommunication Union's recommendation M.3400 to manage securely the future of military Network-Centric Warfare NCW. The authors will employ Interval Temporal Logic ITL as a method of handling both sequential and parallel composition in flexible timely constrains, in addition, this technique will be evaluated using the Network Simulator NS-2 to provide and check whether security requirements are met in a comprehensive manner.\"",
        "Document: \"Formalising policies of a mLearning system using CCA. The Calculus of Context-aware Ambients (CCA in short) has been proposed as a suitable notation for modelling mobile applications that are context-aware. This paper considers a real-world case study of an infostation-based mLearning system which enables mobile devices such as cellular phones, laptops and personal digital assistants to communicate to each other and to access a number of mlearning services within a university campus. Such a dynamic system must enforce complex policies to cope with mobility and context-awareness. We show how these policies can be formalised and verified using CCA. In particular an important liveness property of the mLearning system is proved using the reduction semantics of CCA.\"",
        "Document: \"Dynamic Access Control Policies: Specification and Verification. Security requirements deal with the protection of assets against unauthorized access (disclosure or modification) and their availability to authorized users. Temporal constraints of history-based access control policies are difficult to express naturally in traditional policy languages. We propose a compositional formal framework for the specification and verification of temporal access control po...\"",
        "Document: \"Automated support for the formal specification and design of real-time systems. In this paper, we present a tool, called the CoS-Workbench, for analysing and analysing and manipulating formal specifications of real-time processes. It is based on operational semantics for PARTY (Process Algebra with Real-Time from York) and for PPARTY (Probabilistic PARTY). Real-time processes are interpreted as labelled transition systems, which may be generated and displayed interactively. Nodes and transitions are given with time variables, which allows concise representation of systems in a dense time domain.\"",
        "Document: \"Automated dictionary construction from Arabic corpus for meaningful crime information extraction and document classification. Arabic is a very widely spoken language but very few mining tools have been developed to exploit the data that lies within bodies of Arabic text. Thus, this paper presents and then uses three automatic algorithm techniques specifically designed for Arabic. The target domain is crime profiling and the methods involve adaptive dictionary building. The bodies of text are mined for crime type, location and nationality. This work is then validated through three experiments, the results of which show that the techniques developed here are promising.\"",
        "Document: \"Provably correct derivation of algorithms using FermaT. The transformational programming method of algorithm derivation starts with a formal specification of the result to be achieved, plus some informal ideas as to what techniques will be used in the implementation. The formal specification is then transformed into an implementation, by means of correctness-preserving refinement and transformation steps, guided by the informal ideas. The transformation process will typically include the following stages: (1) Formal specification (2) Elaboration of the specification, (3) Divide and conquer to handle the general case (4) Recursion introduction, (5) Recursion removal, if an iterative solution is desired, (6) Optimisation, if required. At any stage in the process, sub-specifications can be extracted and transformed separately. The main difference between this approach and the invariant based programming approach (and similar stepwise refinement methods) is that loops can be introduced and manipulated while maintaining program correctness and with no need to derive loop invariants. Another difference is that at every stage in the process we are working with a correct program: there is never any need for a separate \u201cverification\u201d step. These factors help to ensure that the method is capable of scaling up to the development of large and complex software systems. The method is applied to the derivation of a complex linked list algorithm and produces code which is over twice as fast as the code written by Donald Knuth to solve the same problem.\"",
        "1 is \"Using the SCR* Toolset to Specify Software Requirements\", 2 is \"Reusing analogous components\"",
        "Given above information, for an author who has written the paper with the title \"Real-Time Refinement: Semantics and Application\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001129": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Linear hash functions':",
        "Document: \"A Perfect Parallel Dictionary. We describe new randomized parallel algorithms for the problems of interval allocation, construction of static dictionaries,\n and maintenance of dynamic dictionaries. All of our algorithms run optimally in constant time with high probability. Our main\n result is the construction of what we call a perfect dictionary, a scheme that allows p processors implementing a set M in space proportional to M to process batches of p insert, delete, and lookup instructions on M in constant time pet batch.\n \n Our best results are obtained for a new variant of the CRCW PRAM model of computation called the OR PRAM. For other variants\n of the CRCW PRAM we show slightly weaker results, with some resource bounds increased by a factor of \u2296(logk\n n), where k \u2208 \u2115 is fixed but arbitrarily large.\n \n \n \"",
        "Document: \"Exact lower time bounds for computing Boolean functions on CREW PRAMs. The time complexity of Boolean functions on abstract concurrent-read exclusive-write parallel random access machines (CREW PRAMs) is considered. We improve results of Cook, Dwork, and Reischuk (SIAM J. Comput.15 (1986), 87-97), and extend work of Kutylowski (SIAM J. Comput.20 (1991), 824-833), who proved a lower time bound for the OR function on such machines that equals the upper bound. We provide a general means for obtaining exact (i.e., correct up to an additive constant) lower bounds, which works for many Boolean functions, in particular all symmetric functions. The new approach is based on the fact that Boolean functions can be represented as polynomials with integer coefficients and that the degree of such a polynomial can be taken as a complexity measure. For some functions, e.g., AND and PARITY, the exact time bound also holds for nondeterministic machines. For probabilistic machines, we obtain exact lower time bounds for PARITY in the unbounded error model and, utilizing results by Szegedy (Ph.D. dissertation, University of Chicago, 1989), prove a general lower bound valid for all Boolean functions in the bounded error model. We further show that the (bounded error) probabilistic time complexity of Boolean functions on CREW PRAMs differs at most by a constant factor from the deterministic time complexity. We also obtain exact bounds for machines that allow a few processors to try to write to the same cell simultaneously. These bounds are stronger than those which follow automatically from the exclusive-write bounds. No tight bounds for this model were known before.\"",
        "Document: \"Succinct Data Structures for Retrieval and Approximate Membership. The retrieval problem is the problem of associating data with keys in a set. Formally, the data structure must store a function f : U ! f0; 1gr that has specied values on the elements of a given set S U, jSj = n, but may have any value on elements outside S. All known methods (e. g. those based on perfect hash functions), induce a space overhead of (n) bits over the optimum, regardless of the evalu- ation time. We show that for any k, query time O(k) can be achieved using space that is within a factor 1 +e k of optimal, asymptotically for large n. The time to construct the data structure is O(n), expected. If we allow logarithmic evaluation time, the additive overhead can be re- duced to O(log logn) bits whp. A general reduction transfers the results on retrieval into analogous results on approximate membership, a prob- lem traditionally addressed using Bloom lters. Thus we obtain space bounds arbitrarily close to the lower bound for this problem as well. The evaluation procedures of our data structures are extremely simple. For the results stated above we assume free access to fully random hash functions. This assumption can be justied using space o(n) to simulate full randomness on a RAM.\"",
        "Document: \"Lower bound arguments with \u201cinaccessible\" numbers. The first result presented in this paper is a lower bound of \u03a9(log n ) for the computation time of concurrent-write parallel random access machines (PRAMS) with operation set, multiplication by constants that recognize the \u201cthreshold set\u201d tx\u0304\u03f5 Z n |x 1 +\u2026+ x n - \u030a 1 \u2a7dx n \u223c for inputs from 0, 1, 2,\u2026, 2 O ( n - log n )\u223c n . The same bound holds for PRAMS with arbitrary binary operations, if the size of the input numbers is not restricted. The second lower bound regards languages in R n corresponding to KNAPSACK, MINIMUM PERFECT MATCHING, SHORTEST PATH, and TRAVELING SALESPERSON on linear decision trees (LDTs) with the restriction that the number of negative coefficients ai in each test \u03a3 1\u2a7d i \u2a7d n\u03b1 i x i : \u03b1 0 is bounded by f ( n ). The lower bounds on the depth of such LDTs that recognize these languages are \u03a9( 2\u230a 2f(n)\u230b ) for KNAPSACK and \u03a9(2 \u230a\u221an 4f(n) \u230b) for the graph problems. The common new tool in the proofs of these lower bounds is the method of constructing \u201chard\u201d instances ( x 1 , \u2026, x n ) of the respective problem by building up the input numbers x 1 from \u201cmutually inaccessible\u201d numbers, i.e., numbers of different orders of magnitude.\"",
        "Document: \"A comparison of two lower-bound methods for communication complexity. The methods \u201cRank\u201d and \u201cFooling Set\u201d for proving lower bounds on the deterministic communication complexity of Boolean functions are compared. The main results are as follows. 1. (i) For almost all Boolean functions of 2 n variables the Rank method provides the lower bound n on communication complexity, whereas the Fooling Set method provides only the lower bound d ( n ) \u2a7d log 2 n + log 2 10. A specific sequence {\u0192 2n } n = 1 \u221e of Boolean functions, where \u0192 2n has 2 n variables, is constructed such that the Rank method provides exponentially higher lower bounds for \u0192 2n than the Fooling Set method. 2. (ii) A specific sequence { h 2 n } n = 1 \u221e of Boolean functions is constructed such that the Fooling Set method provides a lower bound of n for h 2 n , whereas the Rank method provides only (log 2 3) 2 \u00b7 n \u2248 0.79 \u00b7 n as a lower bound. 3. (iii) It is proved that lower bounds obtained by the Fooling Set method are better by at most a factor of two compared with lower bounds obtained by the Rank method. These three results together solve the last problem about the comparison of lower bound methods on communication complexity left open in Aho et al. (1983). Finally, it is shown that an extension of the Fooling Set method provides lower bounds that are tight (up to a polynomial) for all Boolean functions.\"",
        "Document: \"Lower bounds for sorting of sums. This paper addresses the following question: What is the complexity of sorting n numbers x 1 ,\u2026, x n (by comparisons), if it is known in advance that x 1 ,\u2026, x n are all sums of up to d out m numbers (n= \u03a3 0\u2a7ds\u2a7dd m s ) ? A lower bound due to Fredman concerning \u201cSorting X + Y \u201d is extended to the following result: Let d \u2a7e 2 be fixed, n , m as above. Then every comparison tree for n inputs that sorts all inputs of the form ( \u03a3 r\u03f5S w r \u2223 S \u2286 {1,\u2026, m }, \u2223 S \u2223 \u2a7d d ), for w \u03f5 R m , has depth \u03a9(m d ) = \u03f5(n) . This lower bound is optimal. Furthermore, the case of sorting all subset sums of a vector is considered ( d = m ): Let n = 2 m . Then every comparison tree for n inputs that sorts all inputs of the form (\u03a3 r\u03f5S w r \u2223S \u2286 {1,\u2026,m}), w \u03f5 R m , has depth \u2a7e 2 \u231e m 3 \u231f = \u03a9(n 1 3 . This lower bound is exponentially larger than those previously known for this problem.\"",
        "Document: \"Experimental Variations of a Theoretically Good Retrieval Data Structure. A retrieval data structure implements a mapping from a set S of n keys to range R = {0, 1}(r), e.g. given by a list of key-value pairs (x, v) is an element of S x R, but an element outside S may be mapped to any value. Asymptotically, minimal perfect hashing allows to build such a data structure that needs n log(2)e + nr + o(n) bits of memory and has constant evaluation time. Recently, data structures based on other approaches have been proposed that have linear construction time, constant evaluation time and space consumption O(nr) bits or even (1 + epsilon)nr bits for arbitrary epsilon > 0. This paper explores the practicability of one such theoretically very good proposal, bridging a gap between theory and real data structures.\"",
        "Document: \"Optimal partitioning for dual pivot quicksort. Dual pivot quicksort refers to variants of classical quicksort where in the partitioning step two pivots are used to split the input into three segments. This can be done in different ways, giving rise to different algorithms. Recently, a dual pivot algorithm due to Yaroslavskiy received much attention, because it replaced the well-engineered quicksort algorithm in Oracle's Java 7 runtime library. Nebel and Wild (ESA 2012) analyzed this algorithm and showed that on average it uses 1.9nln n+O(n) comparisons to sort an input of size n, beating standard quicksort, which uses 2nln n+O(n) comparisons. We introduce a model that captures all dual pivot algorithms, give a unified analysis, and identify new dual pivot algorithms that minimize the average number of key comparisons among all possible algorithms up to lower order or linear terms. This minimum is 1.8n ln n+O(n).\"",
        "Document: \"Upper and Lower Bounds for the Dictionary Problem (Abstract). We give a randomized algorithm for the dictionary problem with O(1) worst case time for lookup and O(1) expected amortized time for insertion and deletion. We also prove an (log n) lower bound on the amortized worst case time complexity of any deterministic algorithm based on hashing. Furthermore, if the worst case lookup time is restricted to k, then the lower bound becomes (k\u00b7.n\n1/k\n).\"",
        "Document: \"Matching Upper and Lower Bounds for Simulation of Several Tapes on One Multidimensional Tape. We prove a \nQ</font\n>(t(n)d\u00d6{t(n)}/logi(n))\\Theta (t(n)\\sqrt[d]{{t(n)}}/\\log i(n))\n bound for the simulation of t(n) steps of a Turing machine using several one-dimensional work tapes on a Turing machine using one d-dimensional work tape, d  2. The lower bound holds for the problem of recognizing languages on machines with a separate one-way input tape.\"",
        "1 is \"A Lower Bound of \u00bdn\u00b2 on Linear Search Programs for the Knapsack Problem\", 2 is \"Depth-first search and linear grajh algorithms\"",
        "Given above information, for an author who has written the paper with the title \"Linear hash functions\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001176": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'The 2005 PASCAL visual object classes challenge':",
        "Document: \"Building Support Vector Machines with Reduced Classifier Complexity. Support vector machines (SVMs), though accurate, are not preferred in applications requiring great classification speed, due to the number of support vectors being large. To overcome this problem we devise a primal method with the following properties: (1) it decouples the idea of basis functions from the concept of support vectors; (2) it greedily finds a set of kernel basis functions of a specified maximum size (dmax) to approximate the SVM primal cost function well; (3) it is efficient and roughly scales as O(ndmax2) where n is the number of training examples; and, (4) the number of basis functions it requires to achieve an accuracy close to the SVM accuracy is usually far less than the number of SVM support vectors.\"",
        "Document: \"Estimating predictive variances with kernel ridge regression. In many regression tasks, in addition to an accurate estimate of the conditional mean of the target distribution, an indication of the predictive uncertainty is also required. There are two principal sources of this uncertainty: the noise process contaminating the data and the uncertainty in estimating the model parameters based on a limited sample of training data. Both of them can be summarised in the predictive variance which can then be used to give confidence intervals. In this paper, we present various schemes for providing predictive variances for kernel ridge regression, especially in the case of a heteroscedastic regression, where the variance of the noise process contaminating the data is a smooth function of the explanatory variables. The use of leave-one-out cross-validation is shown to eliminate the bias inherent in estimates of the predictive variance. Results obtained on all three regression tasks comprising the predictive uncertainty challenge demonstrate the value of this approach.\"",
        "Document: \"Optimization Techniques for Semi-Supervised Support Vector Machines. Due to its wide applicability, the problem of semi-supervised classification is attracting increasing attention in machine learning. Semi-Supervised Support Vector Machines (S3VMs) are based on applying the margin maximization principle to both labeled and unlabeled examples. Unlike SVMs, their formulation leads to a non-convex optimization problem. A suite of algorithms have recently been proposed for solving S3VMs. This paper reviews key ideas in this literature. The performance and behavior of various S3VMs algorithms is studied together, under a common experimental setting.\"",
        "Document: \"Feature selection and transduction for prediction of molecular bioactivity for drug design. Motivation: In drug discovery a key task is to identify characteristics that separate active (binding) compounds from inactive (non-binding) ones. An automated prediction system can help reduce resources necessary to carry out this task. Results: Two methods for prediction of molecular bioactivity for drug design are introduced and shown to perform well in a data set previously studied as part of the KDD (Knowledge Discovery and Data Mining) Cup 2001. The data is characterized by very few positive examples, a very large number of features (describing three-dimensional properties of the molecules) and rather different distributions between training and test data. Two techniques are introduced specifically to tackle these problems: a feature selection method for unbalanced data and a classifier which adapts to the distribution of the the unlabeled test data (a so-called transductive method). We show both techniques improve identification performance and in conjunction provide an improvement over using only one of the techniques. Our results suggest the importance of taking into account the characteristics in this data which may also be relevant in other problems of a similar type.\"",
        "Document: \"Feature Selection for Support Vector Machines by Means of Genetic Algorithms. The problem of feature selection is a difficult combinatorial task in Machine Learning and of high practical relevance, e.g. in bioinformatics. Genetic Algorithms (GAs) offer a natural way to solve this problem. In this paper we present a special Genetic Algorithm, which especially takes into account the existing bounds on the generalization error for Support Vector Machines (SVMs). This new approach is compared to the traditional method of performing cross-validation and to other existing algorithms for feature selection.\"",
        "Document: \"Training a support vector machine in the primal. Most literature on support vector machines (SVMs) concentrates on the dual optimization problem. In this letter, we point out that the primal problem can also be solved efficiently for both linear and nonlinear SVMs and that there is no reason for ignoring this possibility. On the contrary, from the primal point of view, new families of algorithms for large-scale SVM training can be investigated.\"",
        "Document: \"The Greedy Miser: Learning under Test-time Budgets.   As machine learning algorithms enter applications in industrial settings, there is increased interest in controlling their cpu-time during testing. The cpu-time consists of the running time of the algorithm and the extraction time of the features. The latter can vary drastically when the feature set is diverse. In this paper, we propose an algorithm, the Greedy Miser, that incorporates the feature extraction cost during training to explicitly minimize the cpu-time during testing. The algorithm is a straightforward extension of stage-wise regression and is equally suitable for regression or multi-class classification. Compared to prior work, it is significantly more cost-effective and scales to larger data sets. \"",
        "Document: \"The 2005 PASCAL visual object classes challenge. The PASCAL Visual Object Classes Challenge ran from February to March 2005. The goal of the challenge was to recognize objects from a number of visual object classes in realistic scenes (i.e. not pre-segmented objects). Four object classes were selected: motorbikes, bicycles, cars and people. Twelve teams entered the challenge. In this chapter we provide details of the datasets, algorithms used by the teams, evaluation criteria, and results achieved.\"",
        "Document: \"A dynamic bayesian network click model for web search ranking. As with any application of machine learning, web search ranking requires labeled data. The labels usually come in the form of relevance assessments made by editors. Click logs can also provide an important source of implicit feedback and can be used as a cheap proxy for editorial labels. The main difficulty however comes from the so called position bias - urls appearing in lower positions are less likely to be clicked even if they are relevant. In this paper, we propose a Dynamic Bayesian Network which aims at providing us with unbiased estimation of the relevance from the click logs. Experiments show that the proposed click model outperforms other existing click models in predicting both click-through rate and relevance.\"",
        "Document: \"Modeling delayed feedback in display advertising. In performance display advertising a key metric of a campaign effectiveness is its conversion rate -- the proportion of users who take a predefined action on the advertiser website, such as a purchase. Predicting this conversion rate is thus essential for estimating the value of an impression and can be achieved via machine learning. One difficulty however is that the conversions can take place long after the impression -- up to a month -- and this delayed feedback hinders the conversion modeling. We tackle this issue by introducing an additional model that captures the conversion delay. Intuitively, this probabilistic model helps determining whether a user that has not converted should be treated as a negative sample -- when the elapsed time is larger than the predicted delay -- or should be discarded from the training set -- when it is too early to tell. We provide experimental results on real traffic logs that demonstrate the effectiveness of the proposed model.\"",
        "1 is \"A Discussion on the Classifier Projection Space for Classifier Combining\", 2 is \"Less is more: probabilistic models for retrieving fewer relevant documents\"",
        "Given above information, for an author who has written the paper with the title \"The 2005 PASCAL visual object classes challenge\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001186": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Proving linearizability via non-atomic refinement':",
        "Document: \"Verifying linearisability with potential linearisation points. Linearisability is the key correctness criterion for concurrent implementations of data structures shared by multiple processes. In this paper we present a proof of linearisability of the lazy implementation of a set due to Heller et al. The lazy set presents one of the most challenging issues in verifying linearisability: a linearisation point of an operation set by a process other than the one executing it. For this we develop a proof strategy based on refinement which uses thread local simulation conditions and the technique of potential linearisation points. The former allows us to prove linearisability for arbitrary numbers of processes by looking at only two processes at a time, the latter permits disposing with reasoning about the past. All proofs have been mechanically carried out using the interactive prover KIV.\"",
        "Document: \"Formal Fault Tree Analysis - Practical Experiences. Safety is an important requirement for many modern systems. To ensure safety of complex critical systems, well-known safety analysis methods have been formalized. This holds in particular for automation sytsems and transportation systems. In this paper we present the formalization of one of the most wide spread safety analysis methods: fault tree analysis (FTA). Formal FTA allows to rigorously reason about completeness of a faulty tree. This means it is possible to prove whether a certain combination of component failures is critical for system failure or not. This is a big step forward as informal reasoning on cause-consequence relations is very error-prone. We report on our experiences with a real world case study from the domain of railroads. The here presented case study is - to our knowledge - the first complete formal fault tree analysis for an infinite state system. Until now only finite state systems have been analyzed with formal FTA by using model checking.\"",
        "Document: \"A Modeling Framework for the Development of Provably Secure E-Commerce Applications. Developing security-critical applications is very difficult and the past has shown that many applications turned out to be erroneous after years of usage. For this reason it is desirable to have a sound methodology for developing security-critical E-Commerce applications. We present an approach to model these applications with the unified modeling language (UML) [1] extended by a UML profile to tailor our models to security applications. Our intent is to (semi-) automatically generate a formal specification suitable for verification as well as an implementation from the model. Therefore we offer a development method seamlessly integrating semi-formal and formal methods as well as the implementation. This is a significant advantage compared to other approaches not dealing with all aspects from abstract models down to code. Based on this approach we can prove security properties on the abstract protocol level as well as the correctness of the protocol implementation in Java with respect to the formal model using the refinement approach. In this paper we concentrate on the modeling with UML and some details regarding the transformation of this model into the formal specification. We illustrate our approach on an electronic payment system called Mondex [10]. Mondex has become famous for being the target of the first ITSEC evaluation of the highest level E6 which requires formal specification and verification.\"",
        "Document: \"Mechanizing a Correctness Proof for a Lock-Free Concurrent Stack. Distributed algorithms are inherently complex to verify. In this paper we show how to verify that a concurrent lock-free implementation of a stack is correct by mechanizing the proof that it is linearizable, linearizability being a correctness notion for concurrent objects. Our approach consists of two parts: the first part is independent of the example and derives proof obligations local for one process which imply linearizabilty. The conditions establish a (special sort of non-atomic) refinement relationshipbetween the specification and the concurrent implementation. These are used in the second part to verify the lock-free stack implementation. We use the specification language Z to describe the algorithms and the KIV theorem prover to mechanize the proof.\"",
        "Document: \"Mechanized proofs of opacity: a comparison of two techniques. Software transactional memory (STM) provides programmers with a high-level programming abstraction for synchronization of parallel processes, allowing blocks of codes that execute in an interleaved manner to be treated as atomic blocks. This atomicity property is captured by a correctness criterion called opacity, which relates the behaviour of an STM implementation to those of a sequential atomic specification. In this paper, we prove opacity of a recently proposed STM implementation: the Transactional Mutex Lock (TML) by Dalessandro et\u00a0al. For this, we employ two different methods: the first method directly shows all histories of TML to be opaque (proof by induction), using a linearizability proof of TML as an assistance; the second method shows TML to be a refinement of an existing intermediate specification called TMS2 which is known to be opaque (proof by simulation). Both proofs are carried out within interactive provers, the first with KIV and the second with both Isabelle and KIV. This allows to compare not only the proof techniques in principle, but also their complexity in mechanization. It turns out that the second method, already leveraging an existing proof of opacity of TMS2, allows the proof to be decomposed into two independent proofs in the way that the linearizability proof does not.\"",
        "Document: \"Verification of Mondex electronic purses with KIV: from transactions to a security protocol. The Mondex case study about the specification and refinement of an electronic purse as defined in the Oxford Technical Monograph PRG-126 has recently been proposed as a challenge for formal system-supported verification. In this paper we report on two results. First, on the successful verification of the full case study using the KIV specification and verification system. We demonstrate that even though the hand-made proofs were elaborated to an enormous level of detail we still could find small errors in the underlying data refinement theory, as well as the formal proofs of the case study. Second, the original Mondex case study verifies functional correctness assuming a suitable security protocol. We extend the case study here with a refinement to a suitable security protocol that uses symmetric cryptography to achieve the necessary properties of the security-relevant messages. The definition is based on a generic framework for defining such protocols based on abstract state machines (ASMs). We prove the refinement using a forward simulation.\"",
        "Document: \"Towards a Thread-Local Proof Technique for Starvation Freedom. Today, numerous elaborate algorithms for the effective synchronization of concurrent processes operating on shared memory exist. Of particular importance for the verification of such concurrent algorithms are thread-local proof techniques, which allow to reason about the sequential program of one process individually. While thread-local verification of safety properties has received a lot of attention in recent years, this is less so for liveness properties, in particular for liveness under the assumption of fairness. In this paper, we propose a new thread-local proof technique for starvation freedom. Starvation freedom states that under a weakly fair schedule every process will eventually make progress. We contrast our new proof technique with existing global proof techniques based on ranking functions, and employ it exemplarily for the proof of starvation freedom of ticket locks, the standard locking algorithm of the Linux kernel.\"",
        "Document: \"A systematic verification approach for mondex electronic purses using ASMs. In previous work we solved the challenge to mechanically verify the Mondex challenge about the specification and refinement of an electronic purse, using the given data refinement framework. In this paper we show that using ASM refinement and generalized forward simulations instead of the original approach allows to find a more systematic proof. Our technique of past and future invariants and simulations avoids the need to define a lot of properties for intermediate states during protocol runs. The new proof can be better automated in KIV. The systematic development of a generalized forward simulation uncovered a weakness of the protocol that could be exploited in a denial of service attack. We show a modification of the protocol that avoids this weakness, and that is even slightly easier to verify.\"",
        "Document: \"Relational concurrent refinement part II: Internal operations and outputs. Two styles of description arise naturally in formal specification: state-based and behavioural. In state-based notations, a system is characterised by a collection of variables, and their values determine which actions may occur throughout a system history. Behavioural specifications describe the chronologies of actions\u2014interactions between a system and its environment. The exact nature of such interactions is captured in a variety of semantic models with corresponding notions of refinement; refinement in state based systems is based on the semantics of sequential programs and is modelled relationally. Acknowledging that these viewpoints are complementary, substantial research has gone into combining the paradigms. The purpose of this paper is to do three things. First, we survey recent results linking the relational model of refinement to the process algebraic models. Specifically, we detail how variations in the relational framework lead to relational data refinement being in correspondence with traces\u2013divergences, singleton failures and failures\u2013divergences refinement in a process semantics. Second, we generalise these results by providing a general flexible scheme for incorporating the two main \u201cerroneous\u201d concurrent behaviours: deadlock and divergence, into relational refinement. This is shown to subsume previous characterisations. In doing this we derive relational refinement rules for specifications containing both internal operations and outputs that corresponds to failures\u2013divergences refinement. Third, the theory has been formally specified and verified using the interactive theorem prover KIV.\"",
        "Document: \"Proving System Correctness with KIV. Without Abstract\"",
        "1 is \"Symbolic Implementation of the Best Transformer\", 2 is \"Specification and verification challenges for sequential object-oriented programs\"",
        "Given above information, for an author who has written the paper with the title \"Proving linearizability via non-atomic refinement\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001192": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Integrating tabu search and VLSN search to develop enhanced algorithms: A case study using bipartite boolean quadratic programs.':",
        "Document: \"Experimental analysis of heuristics for the bottleneck traveling salesman problem. In this paper we develop efficient heuristic algorithms to solve the bottleneck traveling salesman problem (BTSP). Results of extensive computational experiments are reported. Our heuristics produced optimal solutions for all the test problems considered from TSPLIB, JM-instances, National TSP instances, and VLSI TSP instances in very reasonable running time. We also conducted experiments with specially constructed `hard' instances of the BTSP that produced optimal solutions for all but seven problems. Some fast construction heuristics are also discussed. Our algorithms could easily be modified to solve related problems such as the maximum scatter TSP and testing hamiltonicity of a graph.\"",
        "Document: \"The bipartite quadratic assignment problem and extensions. \u2022We introduced a new model that subsumes the well-known quadratic assignment problem.\u2022We developed meta-heuristic algorithms using three neighborhoods and their unions.\u2022Experimental analysis disclosed the superiority of hybrid algorithms.\u2022Complexity results and polynomially solvable special cases are also established.\"",
        "Document: \"Domination analysis of some heuristics for the traveling salesman problem. In this paper we propose a general algorithm for the asymmetric traveling salesman problem (ATSP) on a complete digraph on n nodes which computes a tour with cost no more than a pre-specified upper bound. A special case of this algorithm is shown to have complexity O(n2) and domination number at least \u2211k=0n\u22122(k!). Extending this result, we show that by investing O(nk) effort, for k\u2a7e2 and integer, it is possible to obtain a solution which is at least as good as\u03b8=\u2211i=0\u230a(n\u22122)/(k\u22121)\u230b(n\u22121\u2212i(k\u22121))!n\u2212k\u2212i(k\u22121)!\u2212(n\u2212k\u22121\u2212i(k\u22121)))!+(z\u22122)!alternative tours, where z=nmod(k\u22121). Further, we present a patching algorithm for the ATSP and show that it has a domination number at least (n\u22122)!.\"",
        "Document: \"On combined minmax-minsum optimization. In this paper we present a modification of Minoux's algorithm for solving the combined minmax-minsum combinatorial optimization problem (MMCOP) in view of improving its average performance. Computational results are presented which establish the superiority of the modified algorithm over the existing algorithms. We then study the relationship between the optimal solutions of MMCOP and an associated minsum problem in the context of matroids. Further, we introduce combined minmax-minsum linear programming problem (MMLP). MMLP can be transformed into a linear program with additional constraints and variables. We present an efficient simplex based algorithm to solve MMLP which treats these additional constraints implicitly. Our computational results show that the proposed algorithm performs better than the simplex method used directly on a linear program equivalent to MMLP.\"",
        "Document: \"An O(n4) Algorithm for the QAP Linearization Problem. An instance of the quadratic assignment problem (QAP) with cost matrix Q is said to be linearizable if there exists an instance of the linear assignment problem (LAP) with cost matrix C such that for each assignment, the QAP and LAP objective function values are identical. Several sufficiency conditions are known that guarantee linearizability of a QAP. However, no polynomial time algorithm is known to test if a given instance of QAP is linearizable. In this paper, we give a necessary and sufficient condition for an instance of a QAP to be linearizable and develop an O(n4) algorithm to solve the corresponding linearization problem, where n is the size of the QAP.\"",
        "Document: \"A simplex algorithm for piecewise-linear fractional programming problems. Generalizations of the well-known simplex method for linear programming are available to solve the piecewise linear programming problem and the linear fractional programming problem. In this paper we consider a further generalization of the simplex method to solve piecewise linear fractional programming problems unifying the simplex method for linear programs, piecewise linear programs, and the linear fractional programs. Computational results are presented to obtain further insights into the behavior of the algorithm on random test problems.\"",
        "Document: \"Minimum perfect bipartite matchings and spanning trees under categorization. Network optimization problems under categorization arise when the edge set is partitioned into several sets and the objective function is optimized over each set in one sense, then among the sets in some other sense. Here we find that unlike some other problems under categorization, the minimum perfect bipartite matching and minimum spanning tree problems (under categorization) are NP-complete, even on bipartite outerplanar graphs. However for one objective function both problems can be solved in polynomial time on arbitrary graphs if the number of sets is fixed.\"",
        "Document: \"The asymmetric bottleneck traveling salesman problem: Algorithms, complexity and empirical analysis. We consider the asymmetric bottleneck traveling salesman problem on a complete directed graph on n nodes. Various lower bound algorithms are proposed and the relative strengths of each of these bounds are examined using theoretical and experimental analysis. A polynomial time @?n/2@?-approximation algorithm is presented when the edge-weights satisfy the triangle inequality. We also present a very efficient heuristic algorithm that produced provably optimal solutions for 270 out of 331 benchmark test instances. Our algorithms are applicable to the maxmin version of the problem, known as the maximum scatter TSP. Extensive experimental results on these instances are also given.\"",
        "Document: \"Integrating tabu search and VLSN search to develop enhanced algorithms: A case study using bipartite boolean quadratic programs. \u2022We develop efficient heuristic algorithms based on tabu search, VLSN search, and a hybrid algorithm that integrates the two.\u2022The computational study establishes that effective integration of simple tabu search with VLSN search results in superior outcomes.\u2022We obtain solutions better than the best previously known for almost all medium and large size benchmark instances.\u2022Landscape analysis of benchmark instances is given and this offers additional insights into the structure of these problems.\"",
        "Document: \"A fast algorithm for a class of bottleneck problems. We show that if a bottleneck problem of size m with an ordered list of element costs can be solved in O(xi(m)) time, then the problem with an unordered list of element costs can be solved in O(xi(m) log* m) time.\"",
        "1 is \"Optimization via simulation: A review\", 2 is \"The symmetric quadratic traveling salesman problem.\"",
        "Given above information, for an author who has written the paper with the title \"Integrating tabu search and VLSN search to develop enhanced algorithms: A case study using bipartite boolean quadratic programs.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001282": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'The DaQuinCIS Broker: Querying Data and Their Quality in Cooperative Information Systems':",
        "Document: \"Quality-driven extraction, fusion and matchmaking of semantic web API descriptions. The composition of Web APIs provides a great opportunity to Web engineers that can reuse existing software components available on the Web. Finding the best API, fulfilling a set of user requirements, among the many described on the Web is a key step in order to develop an effective Web application; however, Web engineers have little support in solving this problem due to poor search mechanisms and to the heterogeneity of sources and descriptions. Semantic technologies and matching algorithms provide accurate methods to match user requirements against a set of descriptions. Nonetheless, semantic descriptions of APIs are not available in practice. In this paper, we propose a method to extract information on Web APIs published in several Web sources and create semantic descriptions that can be then fused to deliver comprehensive descriptions associated with APIs. During the extraction process, we take into account that collected information has different levels of accuracy, currency, and trustworthiness to state a confidence level of the results. The method is based on the evaluation of the quality of the involved sources, the extracted values, and the overall descriptions. The resulting semantic descriptions are then matched with expressive user requirements to address the API selection problem.\"",
        "Document: \"From Data Quality to Big Data Quality. AbstractThis article investigates the evolution of data quality issues from traditional structured data managed in relational databases to Big Data. In particular, the paper examines the nature of the relationship between Data Quality and several research coordinates that are relevant in Big Data, such as the variety of data types, data sources and application domains, focusing on maps, semi-structured texts, linked open data, sensor & sensor networks and official statistics. Consequently a set of structural characteristics is identified and a systematization of the a posteriori correlation between them and quality dimensions is provided. Finally, Big Data quality issues are considered in a conceptual framework suitable to map the evolution of the quality paradigm according to three core coordinates that are significant in the context of the Big Data phenomenon: the data type considered, the source of data, and the application domain. Thus, the framework allows ascertaining the relevant changes in data quality emerging with the Big Data phenomenon, through an integrative and theoretical literature review.\"",
        "Document: \"Multilevel Schema Integration. We present a methodology for data schema integration, able to merge a set of schemas and the top-down chains of refinement planes produced during their design. The result of this process, that we call multilevel integration, is an integrated schema plus an associated top-down chain of schemas. The integrated schema and the chain are related to the input schemas by nice properties, giving rise to a two-dimensional structure that we call grid. The methodology we define is based on the formal definition of schema refinement and on the notion of schema transformation, i.e., a replacement mechanism that maps a schema into another one. In particular we concentrate on sequences of transformations, constituting the formal counterpart of a chain of refinement. The overall generation process is summarized through the notion of abstraction tree, a useful tool for discovering and solving the conflicts arising during the multilevel integration activity.\"",
        "Document: \"The UM-MAIS Methodology for Multi-channel Adaptive Web Information Systems. Multichannel Adaptive Web Information Systems (WISs) are emerging as a new class of information systems, characterized by their powerful use of mobility and context-awareness. Different methodologies have been proposed so far for the analysis and design of Multichannel Adaptive WISs, specifically focused on the front-end layer or the back-end layer, but no methodology has aimed to cover all the lifecycle and to design all the components that characterize Multichannel Adaptive WIS. This paper fills such a gap, by presenting UM-MAIS (Unified Methodology for Multichannel Adaptive Information Systems), a new methodology that capitalizes on well-established existing methods. It supports the analysis and design of the various components of Multichannel Adaptive WISs (including the user's experience) in a comprehensive and unified manner with special emphasis on context modeling, personalization, and adaptation.\"",
        "Document: \"Design and use of ER repositories: methodologies and experiences in egovernment initiatives. In this paper we describe the main results of a fifteen years research activity in the area of repositories of Entity-Relationship conceptual schemas. We first introduce a set of integration/abstraction primitives that are used in order to organize a large set of conceptual schemas in a repository. We describe the methodology conceived to produce the repository of schemas of central public administrations in Italy. Then we describe an heuristic methodology, applied in the production of the set of schemas of the public administrations of an italian region. We also compare the former exact methodology and the heuristic one according to their correctness, completeness, and efficiency. Finally, we show how such repositories can be used in eGovernment initiatives for planning activities and in the identification of projects. Further work highlights possible evolutions of the repositories toward enhanced semantic representations and usage.\"",
        "Document: \"A Framework And A Methodology For Data Quality Assessment And Monitoring. Abstract:  Data  quality  (DQ)  is  emerging ,as  a ,new ,relevant  area  for  the  improvement ,of  the ,effectiveness  of  organizations. Despites  the  consequences ,of  poor ,quality  of  data ,are  often  experienced ,in  everyday ,life  of  enterprises, very few organizations adopt specific methodologies for assessing and monitoring quality of their data. Inthis paper we present the first results of an Italian project whose,goal is to produce an enhanced,version of well known approaches  to  Basel  II  operational  risk  evaluation,  with  a  significant  relevance  to  information  and  data  quality, and  its effects on  operational risk. In  particular in  this paper we focus on  the definition  of  an  assessment methodology,and a supporting tool for DQ.\"",
        "Document: \"Assessing Social Value In Open Data Initiatives: A Framework. Open data initiatives are characterized, in several countries, by a great extension of the number of data sets made available for access by public administrations, constituencies, businesses and other actors, such as journalists, international institutions and academics, to mention a few. However, most of the open data sets rely on selection criteria, based on a technology-driven perspective, rather than a focus on the potential public and social value of data to be published. Several experiences and reports confirm this issue, such as those of the Open Data Census. However, there are also relevant best practices. The goal of this paper is to investigate the different dimensions of a framework suitable to support public administrations, as well as constituencies, in assessing and benchmarking the social value of open data initiatives. The framework is tested on three initiatives, referring to three different countries, Italy, the United Kingdom and Tunisia. The countries have been selected to provide a focus on European and Mediterranean countries, considering also the difference in legal frameworks (civic law vs. common law countries).\"",
        "Document: \"A Review of the First Cooperative Projects in the Italian e-Government Initiative. The Italian approach to e-Government is based on the development and deployment of the Unitary Network, a \"secure Intranet\" interconnecting all the public administrations. The Cooperative Architecture, currently designed on top of it, is the reference distributed computing model in which each administration is represented as a Domain, exchanging data and application services with the others through Cooperative Gateways. Some issues have been, and currently need to be, addressed, such as the presence of legacy systems, the need of a cooperative development process, the identification of the more effective cooperative approaches. In this paper some first experimental projects carried out in the years 1998-2000, aiming at validating the Cooperative Architecture, will be described; then some lessons gained by these first experiences will be presented.\"",
        "Document: \"Visual Query Systems for Databases: A Survey. Visual query systems (VQSs) are query systems for databases that use visual representations to depict the domain of interest and express related requests. VQSs can be seen as an evolution of query languages adopted into database management systems; they are designed to improve the effectiveness of the human\u2013computer communication. Thus, their most important features are those that determine the nature of the human\u2013computer dialogue. In order to survey and compare existing VQSs used for querying traditional databases, we first introduce a classification based on such features, namely the adopted visual representations and the interaction strategies. We then identify several user types and match the VQS classes against them, in order to understand which kind of system may be suitable for each kind of user. We also report usability experiments which support our claims. Finally, some of the most important open problems in the VQS area are described.\"",
        "Document: \"Efficiency vs Efficacy Driven Service Portfolio Management in a Public Administration (Invited Paper). The paper discusses a framework for managing the ICT-enabled planning of the service production process in a Public Administration (PA). PAs deliver a vast amount of services to citizens, each characterized by a production cost and a customer value. The production cost is the effort needed for the service production, it is influenced by several factors, e.g. By the degree of reuse in the service production process among services that are related by similarities. The customer value of services is usually measured as the trade off between benefits the service provides to customers and sacrifices customers have to deal with. In this paper we deal with a typical problem a PA must face in planning the service production process, namely to decide whether to privilege for a given budget efficiency (maximization of the number of services produced within the budget) or else efficacy (maximization of the overall customer value of services produced within the budget). The framework proposed in the paper is made up of (i) a model for the representation of a repository of services, (ii) a model for the definition of a service portfolio, and (iii) different approaches to be used in decision making for the achievement of efficiency vs. efficacy service production objectives. Two real life case studies are discussed to provide evidence of the impacts of the proposed framework.\"",
        "1 is \"Mapping conceptual to logical models for ETL processes\", 2 is \"Clarifying Business Models: Origins, Present, And Future Of The Concept\"",
        "Given above information, for an author who has written the paper with the title \"The DaQuinCIS Broker: Querying Data and Their Quality in Cooperative Information Systems\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001398": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Evaluating the trustworthiness of advice about seller agents in e-marketplaces: A personalized approach':",
        "Document: \"Analyzing the structure of argumentative discourse. Consider a discourse situation where the speaker tries to convince the hearer of a particular point of view. The first task for the hearer is to understand what it is the speaker wants him to believe--to analyze the structure of the argument being presented, before judging credibility and eventually responding.This paper describes a model for the analysis of arguments that includes:\u2022 a theory of expected coherent structure which is used to limit analysis to the reconstruction of particular transmission forms;\u2022 a theory of linguistic clues which assigns a functional interpretation to special words and phrases used by the speaker to indicate the structure of the argument;\u2022 a theory of evidence relationships which includes the demand for pragmatic analysis to accommodate beliefs not currently held.The implications of this particular design for dialogue analysis in general are thus:\u2022 structure is an important feature to extract in a representation to control the processing;\u2022 linguistic constructions can be assigned useful interpretations;\u2022 pragmatic analysis is crucial in cases where the participants differ in beliefs.\"",
        "Document: \"Resolving Plan Ambiguity for Response Generation. Recognizing the plan underlying a query aids in the generation of an appropriate response. In this paper, we address the problem of how to generate coopera- tive responses when the user's plan is ambiguous. We show that it is not always necessary to resolve the ambiguity, and provide a procedure that estimates whether the ambiguity matters to the task of formu- lating a response. If the ambiguity does matter, we propose to resolve the ambiguity by entering into a clarification dialogue with the user and provide a pro- cedure that performs this task. Together, these pro- cedures allow a question-answering system to take advantage of the interactive and collaborative nature of dialogue in recognizing plans and resolving ambi- guity.\"",
        "Document: \"An incentive mechanism for eliciting fair ratings of sellers in e-marketplaces. In this paper, we propose a novel incentive mechanism for eliciting fair ratings of selling agents from buying agents. In our mechanism, buyers model other buyers and select the most trustworthy ones as their neighbors from whom they can ask advice about sellers. In addition, however, sellers model the reputation of buyers. Reputable buyers always provide fair ratings of sellers, and are likely to be neighbors of many other buyers. In marketplaces operating with our mechanism, sellers will increase quality and decrease prices of products to satisfy reputable buyers. In consequence, our mechanism creates incentives for buyers to provide fair ratings of sellers.\"",
        "Document: \"What is Initiative?. This paper presents some alternate theories forexplaining the term \u2019initiative\u2018, as it is used in thedesign of mixed-initiative AI systems. Although thereis now active research in the area of mixed initiativeinteractive systems, there appears to be no trueconsensus in the field as to what the term\u2019initiative\u2018 actually means. In describing differentpossible approaches to the modeling of initiative, weaim to show the potential importance of eachparticular theory for the design of mixed initiativesystems. The paper concludes by summarizing some ofthe key points in common to the theories, and bycommenting on the inherent difficulties of theexercise, thereby elucidating the limitations whichare necessarily encountered in designing such theoriesas the basis for designing mixed-initiative systems.\"",
        "Document: \"Temporal Reasoning During Plan Recognition. This paper presents a strengthened algorithm for temporal reasoning during plan recognition, which improves on a straightforward application of Allen's reasoning algorithm. This is made possible by viewing plans as both hierarchical structures and temporal networks. As a result, we can show how to use as constraints the temporal relations explicitly given in input to improve the results of plan recognition. We also discuss how to combine the given constraints with those prestored in the system's plan library to make more specific the temporal constraints indicated in the plans being recognised.\"",
        "Document: \"A Strengthened Algorithm For Temporal Reasoning About Plans. Alien's interval algebra has been shown to be useful for representing plans. We present a strengthened algorithm for temporal reasoning about plans, which improves on straightforward applications of the existing reasoning algorithms for the algebra. This is made possible by viewing plans as both temporal networks and hierarchical structures. The temporal network view allows us to check for inconsistencies as well as propagate the effects of new temporal constraints, whereas the hierarchical view helps us to get the strengthened results by taking into account the dependency relationships between actions.We further apply our algorithm to the process of plan recognition through the analysis of natural language input. We show that such an application has two useful effects: the temporal relations derived from the natural language input can be used as constraints to reduce the number of candidate plans, and the derived constraints can be made more specific by combining them with the prestored constraints in the plans being recognized.\"",
        "Document: \"Smart cheaters do prosper: defeating trust and reputation systems. Traders in electronic marketplaces may behave dishonestly, cheating other agents. A multitude of trust and reputation systems have been proposed to try to cope with the problem of cheating. These systems are often evaluated by measuring their performance against simple agents that cheat randomly. Unfortunately, these systems are not often evaluated from the perspective of security---can a motivated attacker defeat the protection? Previously, it was argued that existing systems may suffer from vulnerabilities that permit effective, profitable cheating despite the use of the system. In this work, we experimentally substantiate the presence of these vulnerabilities by successfully implementing and testing a number of such 'attacks', which consist only of sequences of sales (honest and dishonest) that can be executed in the system. This investigation also reveals two new, previously-unnoted cheating techniques. Our success in executing these attacks compellingly makes a key point: security must be a central design goal for developers of trust and reputation systems.\"",
        "Document: \"Tutoring An Entire Game With Dynamic Strategy Graphs: The Mixed-Initiative Sudoku Tutor. In this paper, we develop a mixed-initiative intelligent tutor for the game of Sudoku called MITS. We begin by developing a characterization of the strategies used in Sudoku as the basis for teaching the student how to play. In order to reason about interaction with the student, we also introduce a student modeling component motivated by the mixed-initiative model of Fleming and Cohen that tracks what the student knows and understands. In contrast to other systems for tutoring games, we are able to interact with students to complete an entire game. This is achieved by retaining a model of acceptable next moves (called a strategy graph) and dynamically adjusting this model as the student plays the game. We present the overall architecture of the system followed by an explanation of the modules that encapsulate the rules of Sudoku. We also outline formulae for reasoning about interaction with the student that support mixed-initiative where either the system or the student can elect to direct the playing of the game. An implementation of the system is discussed, including examples of MITS interacting with students in order to tutor the game. To conclude, we discuss how this research is useful not only to gain insight into how to tutor students about strategy games but also to understand how to support mixed-initiative interaction in tutorial settings.\"",
        "Document: \"Trusting advice from other buyers in e-marketplaces: the problem of unfair ratings. In electronic marketplaces populated by self-interested agents, buyer agents would benefit by modeling the reputation of seller agents, in order to make effective decisions about which agents to trust. One method for representing reputation is to ask other agents in the system (called advisor agents) to provide ratings of the seller agents. The problem of unfair ratings exists in almost every reputation system, including both unfairly high and unfairly low ratings. We begin by surveying some existing approaches to this problem, characterizing their capabilities and categorizing them in terms of two main dimensions: public-private and global-local. The impact of reputation system architectures on approach selection is also discussed. Based on the study, we propose a novel personalized approach for effectively handling unfair ratings in an enhanced centralized reputation system. Experimental results demonstrate that the approach effectively adjusts the trustworthiness of advisor agents according to the percentages of unfair ratings provided by them. We then argue for the merits of our model as the basis for designing social networks to share reputation ratings of sellers in electronic marketplaces.\"",
        "Document: \"The Interpretation of Temporal Relations in Narrative. This paper describes an algorithm for the interpretation of temporal relations between events mentioned in narrative (such as which event occurs before another). These relations are decided through three different levels of linguistic concepts: aspectual information for verbs, time relations for tenses, and time relations between clauses and/or sentences. One contribution of this paper is to present a more rigorous description for time relations of tenses, which is able to express all the 16 tenses in English and incorporate the interval properties of events from the aspectual analysis into the tense relations. For interpreting time relations between clauses, we emphasize the use of anaphoric references to events and introduce the concept of a situational description for an event (including the participants, place, time duration, etc.), used to make the interpreting algorithm deterministic, i.e. the set of interpreting rules are applied in a fixed order rather than in parallel. Last, we suggest a tree-like structure for the representation of temporal relations between events, which allows us to include vaguely specified relations (which may be clarified later), to facilitate the interpretation of subsequent temporal relations.\"",
        "1 is \"Strategic Interactions In A Supply Chain Game\", 2 is \"Point-based value iteration: an anytime algorithm for POMDPs\"",
        "Given above information, for an author who has written the paper with the title \"Evaluating the trustworthiness of advice about seller agents in e-marketplaces: A personalized approach\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001410": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Where Are the Rules?':",
        "Document: \"What's in a Relationship: An Ontological Analysis. In a series of publications, we have proposed a foundational system of ontological categories which has been successfully used to evaluate and improve the quality of conceptual modeling grammars and models. In this article, we continue this work by using this foundational ontology to provide real-world semantics and sound modeling guidelines for one of the most fundamental (and yet one of the most problematic) constructs in conceptual modeling, namely, the relationship type. In addition, we systematically compare our approach with a classical ontological treatment of this construct in the literature, provided by the BWW framework.\"",
        "Document: \"Towards a conceptual model and framework for management games. Management games have a long history in management and social science education, and a large number of such games has been developed and is being used in university education and in professional training. With the increasing use of computers in recent decades, most of them have been developed in computerized form. However, typically, these games are being developed in isolation, without (re-)using any general model, or methodology, or simulation engineering framework. In this paper we propose a basic conceptual model for business management games based on the classical Lemonade Stand Game and we show how to construct incremental extensions of this model and how to implement them as web-based simulations using standard web technologies.\n\n\"",
        "Document: \"On the Foundations of Web-Based Registries for Business Rules. In the last eight years, registries for e-business, such as ebXML or UDDI, enabling enterprise of any size and in any geographical location to conduct their businesses on the World Wide Web, were developed. Applications in domains such as insurance (for example, insurance rating), financial services (loans, claims routing and management, fraud detection), government (tax calculations), telecom customer (care and billing), e-commerce (personalizing the user's experience, recommender systems, auctions), and so on benefit greatly from using rule engines. Therefore, sharing rulesets becomes a necessity for many B2B businesses. This work presents a basic architecture of building a Web-based registry for rules. The main goal of the registry is to allow rulesets discovery. Registry entries contain both required ruleset related data (such as ruleset URI or properties describing their intended scope) and optional metadata covering additional properties such as last modified date.\"",
        "Document: \"Agent-Based Simulations with Beliefs and SPARQL-Based Ask-Reply Communication. \n This paper presents the results of extending an agent-based simulation framework by adding a full-fledged model of beliefs\n and by supporting ask-reply communication with the help of the W3C RDF query language SPARQL. Beliefs are the core component of any cognitive agent architecture. They are also the basis of ask-reply communication between\n agents, which allows social learning. Our approach supports the conceptual distinctions between facts and beliefs, and between\n sincere answers and lies.\n \n \"",
        "Document: \"Stable Semantics of Temporal Deductive Databases.  We define a preferential semantics based on stable generated models for a very generalclass of temporal deductive databases. We allow two kinds of temporal information tobe represented and queried: timepoint and timestamp formulas, and show how each ofthem can be translated into the other. Because of their generality, our formalism andour semantics can serve as a basis for comparing and extending other temporal deductivedatabase frameworks.1 IntroductionA deductive database... \"",
        "Document: \"Towards simulation of organizational norms. Unlike social norms, which are the unplanned, unexpected result of the interactions among human individuals, organizational norms are stipulated by the organization with the purpose of constraining the behavior of organizational actors in the context of business processes. We propose a simple conceptual model of organizations and organizational norms as an extension of the metamodel of the Agent-Object-Relationship (AOR) simulation language. In our approach an organization is modeled as an institutional agent with organizational units and human actors as subagents that participate in business processes involving other agents, which are possibly affiliated with other organizations. For simplicity, we consider only the most basic form of behavior, which is reactive behavior described in the form of reaction rules, and the most basic types of organizational norms, which are rights and duties defined for organizational positions and roles.\"",
        "Document: \"Towards a Mapping from Java Vocabulary to RDFS. This paper describe how essential parts of the Java vocabulary can be described using RDFS. First part of the paper describe a mapping solution from Java vocabulary to RDF Schema. In the second part we define a new RDFS extension to express additional vocabulary elements into RDFS.\"",
        "Document: \"On the General Ontological Foundations of Conceptual Modeling. As pointed out in the pioneering work of [WSW99, EW01], an upper level ontology allows to evaluate the ontological correctness of a conceptual model and to develop guidelines how the constructs of a conceptual modeling language should be used. In this paper we adopt the General Ontological Language (GOL), proposed in [DHHS01], for this purpose. We discuss a number of issues that arise when applying the concepts of GOL to UML class diagrams as a conceptual modeling language. We also compare our ontological analysis of some parts of the UML with the one proposed in [EW01].\"",
        "Document: \"Towards Radical Agent-Oriented Software Engineering Processes Based on AOR Modeling. We propose a new agent-oriented software engineering process, called RAP, which follows the Rational Unified Process (RUP) in many ways, but is based on Agent-Object-Relationship (AOR) modeling instead of object-oriented modeling. Two particular features of the proposed methodology are: it is supported by a foundational ontology, and it employs a certain form of agent-based discrete event simulation for achieving more agility in the development process.\"",
        "Document: \"Bridging concrete and abstract syntaxes in model-driven engineering: a case of rule languages. The paper covers the problem of bridging the gap between abstract and textual concrete syntaxes of software languages in the model-driven engineering (MDE) context. This problem has been well studied in the context of programming languages, but due to the obvious difference in the definitions of abstract syntax, MDE requires a new set of engineering principles. We first explore different approaches to defining abstract and concrete syntaxes in the MDE context. Next, we investigate the current state of languages and techniques used for bridging between textual concrete and abstract syntaxes in the context of MDE. Finally, we report on lessons learned in experimenting with the current technologies. In order to provide a comprehensive coverage of the problem under study, we have selected a case of Web rule languages. Web rule languages leverage various types of syntax specification languages; and they are complex in nature and large in terms of the language elements. Thus, they provide us with a realistic analysis framework based on which we can draw general conclusions. Based on the series of experiments that we conducted with the analyzed languages, we propose a method for approaching such problems and report on the empirical results obtained from the data collected during our experiments. Copyright \u00a9 2009 John Wiley & Sons, Ltd.\"",
        "1 is \"Benchmarking RDF Schemas for the Semantic Web\", 2 is \"Using Psychology To Understand Conceptual Modelling\"",
        "Given above information, for an author who has written the paper with the title \"Where Are the Rules?\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001416": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Facial expression recognition on hexagonal structure using LBP-based histogram variances':",
        "Document: \"Preliminary Research on Fractal Video Compression on Spiral Architecture. Fractal Video Compression (FVC) is of extensive interest for over 20 years. Instead of being implemented on square image structure, Spiral Architecture (SA) based fractal image compression is proposed in this paper to illustrate the great potential of FVC on SA. Conceptually, a new definition of range block and domain block is presented on this enhanced image structure. Compared with the conventional square image architecture, spiral architecture provides higher fidelity to fractal image compression, which is demonstrated by the experimental results.\"",
        "Document: \"Long-Term Person Re-identification Using True Motion from Videos. Most person re-identification approaches and benchmarks assume that pedestrians go across the surveillance network without significant appearance changes in a brief period, which explicitly restricts person re-identification to a short-term event and incurs inter-sample similarity measurement by appearance matching. However, pedestrians are likely to reappear in the surveillance network after a long-time interval (long-term) and change their wearing in many real-world scenarios. These scenarios inevitably cause appearances between subjects more ambiguous and indistinguishable. In this paper we consider these scenarios and propose a unified feature representation based on true motion cues from videos named FIne moTion encoDing (FITD). Our hypothesis is that people keep constant motion patterns under non-distraction walking condition. Therefore, the motion characteristics are more reliable than static appearance feature to describe a walking person. Particularly, we extract motion patterns hierarchically by encoding trajectory-aligned descriptors with Fisher vectors in a spatial-aligned pyramid. To verify benefits of the proposed FITD, we collect a new dataset typically for the long-term situations. Extensive experiments demonstrate the merits of our FITD especially for the long-term scenarios.\"",
        "Document: \"A New Approach for Fractal Image Compression on a Virtual Hexagonal Structure. In this paper, we propose a Fractal Image Compression method on a virtual hexagonal image structure by adopting Fisher's basic method on the traditional square image structure. The modification on the definition of range block and domain block is implemented in order to utilize the enhanced image structure. The results of the proposed approach applied to testing images are analyzed and higher fidelity is obtained. The further research directions are discussed.\"",
        "Document: \"An MRF-based depth upsampling: upsample the depth map with its own property. In this paper, we propose a novel method for upsampling the noisy low resolution depth map with the guidance of the companion color image. The problem is modeled with an Markov Random Field (MRF)-based optimization framework. The novelty relies on the smoothness term that is modeled with an exponential function as the error norm. By using this novel error norm, our method can take the property of the depth map into account. Depth discontinuity cues are not only obtained from the color image but also the depth map itself. Our method has much better performance in preserving sharp depth discontinuities and suppressing the texture copy artifacts. Experimental results show that our method outperforms state-of-art solutions in both visual quality and accuracy.\"",
        "Document: \"Human Action Recognition by Radon Transform. A new feature description is used for human behaviour representation and recognition. The feature is based on Radon transforms of extracted silhouettes. Key postures are selected based on the Radon transform. Key postures are combined to construct an action template for each sequence. Linear Discriminant Analysis (LDA) is applied to the set of key postures to obtain low dimensional feature vectors. Different classification methods are used to classify each sequence. Experiments are carried out based on a publically available human behaviour database and the results are exciting.\"",
        "Document: \"Adaptive Fusion of Gait and Face for Human Identification in Video. Most work on multi-biometric fusion is based on static fusion rules which cannot respond to the changes of the environment and the individual users. This paper proposes adaptive multi-biometric fusion, which dynamically adjusts the fusion rules to suit the real-time external conditions. As a typical example, the adaptive fusion of gait and face in video is studied. Two factors that may affect the relationship between gait and face in the fusion are considered, i.e., the view angle and the subject-to-camera distance. Together they determine the way gait and face are fused at an arbitrary time. Experimental results show that the adaptive fusion performs significantly better than not only single biometric traits, but also those widely adopted static fusion rules including SUM, PRODUCT, MIN, and MAX.\"",
        "Document: \"Edge Map Improvement On Spiral Architecture. Edge map is considered as an important entity containing most of object features in an image. Many computer vision systems rely on the use of the boundary line information to perform the object recognition tasks. However, with the exception of images acquired from highly restricted environment, common edge detectors do not guarantee the production of continuous boundaries of objects. In this paper, a local processing based edge-linking algorithm is proposed. We set a criterion involving multiple properties of the pixel to find the best candidate points for edge linking. In addition, this algorithm is implemented on Spiral Architecture.\"",
        "Document: \"Face recognition using message passing based clustering method. Traditional subspace analysis methods are inefficient and tend to be affected by noise as they compare the test image to all training images, especifically when there are large numbers of training images. To solve such problem, we propose a fast face recognition (FR) technique called APLDA by combining a novel clustering method affinity propagation (AP) with linear discriminant analysis (LDA). By using AP on the reduced features derived from LDA, a representative face image for each subject can be reached. Thus, our APLDA uses only the representative images rather than all training images for identification. Obviously, APLDA is much more computationally efficient than Fisherface. Also, unlike Fisherface who uses pattern classifier for identification, APLDA performs the identification using AP once again to cluster the test image into one of the representative images. Experimental results also indicate that APLDA outperforms Fisherface in terms of recognition rate.\"",
        "Document: \"Refined Gaussian Weighted Histogram Intersection and Its Application in Number Plate Categorization. This paper proposes a refined Gaussian weighted histogram intersection for content-based image matching and applies the method for number plate categorization. Number plate images are classified into two groups based on their colour similarities with the model image of each group. The similarities of images are measured by the matching rates between their colour histograms. Histogram intersection (HI) is used to calculate the matching rates of histograms. Since the conventional histogram intersection algorithm is strictly based on the matching between bins of identical colours, the final matching rate could easily be affected by colour variation caused by various environment changes. In our recent paper [9], a Gaussian weighted histogram intersection (GWHI) algorithm has been proposed to facilitate the histogram matching via taking into account matching of both identical colours and similar colours. The weight is determined by the distance between two colours. When applied to number plate categorization, the GWHI algorithm demonstrates to be more robust to colour variations and produces a classification with much lower intra-class distance and much higher interclass distance than previous HI algorithms. However, the processing speed of this GWHI method is still not satisfying. In this paper, the GWHI method is further refined, where a colour quantization method is utilized to reduce the number of colours without introducing apparent perceptual colour distortion. New experimental results demonstrate that using the refined GWHI method, image categorization can be done more efficiently.\"",
        "Document: \"Multi-view Gait Recognition Based on Motion Regression Using Multilayer Perceptron. It has been shown that gait is an efficient biometric feature for identifying a person at a distance. However, it is a challenging problem to obtain reliable gait feature when viewing angle changes because the body appearance can be different under the various viewing angles. In this paper, the problem above is formulated as a regression problem where a novel View Transformation Model (VTM) is constructed by adopting Multilayer Perceptron (MLP) as regression tool. It smoothly estimates gait feature under an unknown viewing angle based on motion information in a well selected Region of Interest (ROI) under other existing viewing angles. Thus, this proposal can normalize gait features under various viewing angles into a common viewing angle before gait similarity measurement is carried out. Encouraging experimental results have been obtained based on widely adopted benchmark database.\"",
        "1 is \"Ordering Heuristics for Reliability Evaluation of Multistate Networks\", 2 is \"Object recognition with color cooccurrence histograms\"",
        "Given above information, for an author who has written the paper with the title \"Facial expression recognition on hexagonal structure using LBP-based histogram variances\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001418": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Topic 3: Scheduling and Load Balancing':",
        "Document: \"Implementation of a reliable remote memory pager. Traditional operating systems use magnetic disks as paging devices, even though the cost of a disk transfer measured in processor cycles continues to increase. In this paper we explore the use of remote main memory for paging. We describe the design, implementation and evaluation of a pager that uses main memory of remote workstations as a faster-than-disk paging device and provides reliability in case of single workstation failures. Our pager has been implemented as a block device driver linked to the DEC OSF/1 operating system, without any modifications to the kernel code. Using several test applications we measure the performance of remote memory paging over an Ethernet interconnection network and find it to be faster than traditional disk paging. We evaluate the performance of various reliability policies and prove their feasibility even over low bandwidth networks, like Ethernet. We conclude that the benefits of reliable remote memory paging in workstation clusters are significant today and will probably increase in the near future.\"",
        "Document: \"ITA: Innocuous Topology Awareness for Unstructured P2P Networks. One of the most appealing characteristics of unstructured P2P overlays is their enhanced self-* properties, which results from their loose, random structure. In addition, most of the algorithms which make searching in unstructured P2P systems scalable, such as dynamic querying and 1-hop replication, rely on the random nature of the overlay to function efficiently. The underlying communications network (i.e., the Internet), however, is not as randomly constructed. This leads to a mismatch between the distance of two peers on the overlay and the hosts they reside on at the IP layer, which in turn leads to its misuse. The crux of the problem arises from the fact that any effort to provide a better match between the overlay and the IP layer will inevitably lead to a reduction in the random structure of the P2P overlay, with many adverse results. With this in mind, we propose ITA, an algorithm which creates a random overlay of randomly connected neighborhoods providing topology awareness to P2P systems, while at the same time has no negative effect on the self-* properties or the operation of the other P2P algorithms. Using extensive simulations, both at the IP router level and autonomous system level, we show that ITA reduces communication latencies by as much as 50 percent. Furthermore, it not only reduces by 20 percent the number of IP network messages which is critical for ISPs carrying the burden of transporting P2P traffic, but also distributes the traffic load more evenly on the routers of the IP network layer.\"",
        "Document: \"Topic 3: Scheduling and Load Balancing. \n Scheduling and Load Balancing techniques are key issues for the performance of applications executed in parallel and distributed\n environments, and for the efficient utilization of these computational resources. Research in this field has a long history\n and is well consolidated. Nevertheless, the evolution of parallel and distributed systems toward clusters, computational grids,\n and global computing environments, introduces new challenging problems that require a new generation of scheduling and load\n balancing algorithms. Topic 3 in Euro-Par 2004 covers all aspects related to scheduling and load balancing from application\n and system levels, to theoretical foundations and practical tools. All these aspects are addressed by contributed papers.\n \n \"",
        "Document: \"MOR: monitoring and measurements through the onion router. A free and easy to use distributed monitoring and measurement platform would be valuable in several applications: monitoring network or server infrastructures, performing research experiments using many ISPs and test nodes, or checking for network neutrality violations performed by service providers. In this paper we present MOR, a technique for performing distributed measurement and monitoring tasks using the geographically diverse infrastructure of the Tor anonymizing network. Through several case studies, we show the applicability and value of MOR in revealing the structure and function of large hosting infrastructures and detecting network neutrality violations. Our experiments show that about 7.5% of the tested organizations block at least one popular application port and about 5.5% of them modify HTTP headers.\"",
        "Document: \"Regular Expression Matching on Graphics Hardware for Intrusion Detection. The expressive power of regular expressions has been often exploited in network intrusion detection systems, virus scanners, and spam filtering applications. However, the flexible pattern matching functionality of regular expressions in these systems comes with significant overheads in terms of both memory and CPU cycles, since every byte of the inspected input needs to be processed and compared against a large set of regular expressions. In this paper we present the design, implementation and evaluation of a regular expression matching engine running on graphics processing units (GPUs). The significant spare computational power and data parallelism capabilities of modern GPUs permits the efficient matching of multiple inputs at the same time against a large set of regular expressions. Our evaluation shows that regular expression matching on graphics hardware can result to a 48 times speedup over traditional CPU implementations and up to 16 Gbit/s in processing throughput. We demonstrate the feasibility of GPU regular expression matching by implementing it in the popular Snort intrusion detection system, which results to a 60% increase in the packet processing throughput.\"",
        "Document: \"Efficient content-based detection of zero-day worms. Recent cybersecurity incidents suggest that Internet worms can spread so fast that in-time human-mediated reaction is not possible, and therefore initial response to cyberattacks has to be automated. The first step towards combating new unknown worms is to be able to detect and identify them at the first stages of their spread. In this paper, we present a novel method for detecting new worms based on identifying similar packet contents directed to multiple destination hosts. We evaluate our method using real traffic traces that contain real worms. Our results suggest that our approach is able to identify novel worms while at the same time the generated false alarms reach as low as zero percent.\"",
        "Document: \"Main memory caching of Web documents. An increasing amount of information is currently becoming available through World Wide Web servers. Document requests to popular Web servers arrive every few tens of milliseconds at peak rate. To reduce the overhead imposed by frequent document requests, we propose the notion of caching a World Wide Web server's documents in its main memory (which we call Main Memory Web Caching). We show that even a small amount of main memory (512 Kbytes) that is used as a document cache, is enough to hold more than 60% of the documents requested.\"",
        "Document: \"Using Reference Counters in Update-Based Coherent Memory.  . As the disparity between processor and memory speed continues to widen, the exploitationof locality of reference in shared-memory multiprocessors becomes an increasingly importantproblem in parallel processing. In this paper, we explore the problem of managing locality at theoperating system level. In specific, we study the use of reference counters in making informed decisionsabout page placement and movement. We use trace-driven simulation of real applications toevaluate the... \"",
        "Document: \"Improving the accuracy of network intrusion detection systems under load using selective packet discarding. Under conditions of heavy traffic load or sudden traffic bursts, the peak processing throughput of network intrusion detection systems (NIDS) may not be sufficient for inspecting all monitored traffic, and the packet capturing subsystem inevitably drops excess arriving packets before delivering them to the NIDS. This impedes the detection ability of the system and leads to missed attacks. In this work we present selective packet discarding, a best effort approach that enables the NIDS to anticipate overload conditions and minimize their impact on attack detection. Instead of letting the packet capturing subsystem randomly drop arriving packets, the NIDS proactively discards packets that are less likely to affect its detection accuracy, and focuses on the traffic at the early stages of each network flow. We present the design of selective packet discarding and its implementation in Snort NIDS. Our experiments show that selective packet discarding significantly improves the detection accuracy of Snort under increased traffic load, allowing it to detect attacks that would have otherwise been missed.\"",
        "Document: \"Privacy-preserving social plugins. The widespread adoption of social plugins, such as Facebook's Like and Google's +1 buttons, has raised concerns about their implications to user privacy, as they enable social networking services to track a growing part of their members' browsing activity. Existing mitigations in the form of browser extensions can prevent social plugins from tracking user visits, but inevitably disable any kind of content personalization, ruining the user experience. In this paper we propose a novel design for privacy-preserving social plugins that decouples the retrieval of user-specific content from the loading of a social plugin. In contrast to existing solutions, this design preserves the functionality of existing social plugins by delivering the same personalized content, while it protects user privacy by avoiding the transmission of user-identifying information at load time. We have implemented our design in SafeButton, an add-on for Firefox that fully supports seven out of the nine social plugins currently provided by Facebook, including the Like button, and partially due to API restrictions the other two. As privacy-preserving social plugins maintain the functionality of existing social plugins, we envisage that they could be adopted by social networking services themselves for the benefit of their members. To that end, we also present a pure JavaScript design that can be offered transparently as a service without the need to install any browser add-ons.\"",
        "1 is \"Fast and scalable pattern matching for content filtering\", 2 is \"Load balancing for term-distributed parallel retrieval\"",
        "Given above information, for an author who has written the paper with the title \"Topic 3: Scheduling and Load Balancing\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001435": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Combining phase identification and statistic modeling for automated parallel benchmark generation':",
        "Document: \"A Maya use case: adaptable scientific workflows with ADIOS for general relativistic astrophysics. There are many challenges in analyzing and visualizing data from current cutting-edge general relativistic astrophysics simulations. Many of the associated tasks are time-consuming with large performance degradation due to the magnitude and complexity of the data. The Adaptable I/O System (ADIOS) is a componentization of the I/O layer that has demonstrated remarkable I/O performance improvements on applications running on leadership class machines while also offering new in-memory \"staging\" operations for transforming data in-situ. We have been incorporating ADIOS staging technologies into our Maya numerical relativity code based on Cactus infrastructure and Carpet mesh refinement. Incorporating ADIOS into the Maya code is the first step toward enabling a more adaptable Maya workflow. We provide descriptions how we intend to leverage FlexPath (an ADIOS transport method) to provide a richer user experience in real-time visualization and interactive steering.\"",
        "Document: \"Plasma fusion code coupling using scalable I/O services and scientific workflows. In order to understand the complex physics of mother nature, physicist often use many approximations to understand one area of physics and then write a simulation to reduce these equations to ones that can be solved on a computer. Different approximations lead to different equations that model different physics, which can often lead to a completely different simulation code. As computers become more powerful, scientists can either write one simulation that models all of the physics or they produce several codes each for different portions of the physics and then 'couple' these codes together. In this paper, we concentrate on the latter, where we look at our code coupling approach for modeling a full device fusion reactor. There are many approaches to code coupling. Our first approach was using Kepler workflows to loosely couple three codes via files (memory-to-disk-to-memory coupling). This paper describes our new approach moving towards using memory-to-memory data exchange to allow for a tighter coupling. Our approach focuses on a method which brings together scientific workflows along with staging I/O methods for code coupling. Staging methods use additional compute nodes to perform additional tasks such as data analysis, visualization, and NxM transfers for code coupling. In order to transparently allow application scientist to switch from memory to memory coupling to memory to disk to memory coupling, we have been developing a framework that can switch between these two I/O methods and then automate other workflow tasks. Our hybrid approach allows application scientist to easily switch between in-memory coupling and file-based coupling on-the-fly, which aids debugging these complex configurations.\"",
        "Document: \"Exploring Data Staging Across Deep Memory Hierarchies for Coupled Data Intensive Simulation Workflows. As applications target extreme scales, data staging and in-situ/in-transit data processing have been proposed to address the data challenges and improve scientific discovery. However, further research is necessary in order to understand how growing data sizes from data intensive simulations coupled with the limited DRAM capacity in High End Computing systems will impact the effectiveness of this approach. In this paper, we explore how we can use deep memory levels for data staging, and develop a multi-tiered data staging method that spans bothDRAM and solid state disks (SSD). This approach allows us to support both code coupling and data management for data intensive simulation workflows. We also show how an adaptive application-aware data placement mechanism can dynamically manage and optimize data placement across the DRAM ands storage levels in this multi-tiered data staging method. We present an experimental evaluation of our approach using wolf resources: an Infiniband cluster (Sith) and a Cray XK7system (Titan), and using combustion (S3D) and fusion (XGC1) simulations.\"",
        "Document: \"High-throughput Analysis of Large Microscopy Image Datasets on CPU-GPU Cluster Platforms. Analysis of large pathology image datasets offers significant opportunities for the investigation of disease morphology, but the resource requirements of analysis pipelines limit the scale of such studies. Motivated by a brain cancer study, we propose and evaluate a parallel image analysis application pipeline for high throughput computation of large datasets of high resolution pathology tissue images on distributed CPU-GPU platforms. To achieve efficient execution on these hybrid systems, we have built runtime support that allows us to express the cancer image analysis application as a hierarchical data processing pipeline. The application is implemented as a coarse-grain pipeline of stages, where each stage may be further partitioned into another pipeline of fine-grain operations. The fine-grain operations are efficiently managed and scheduled for computation on CPUs and GPUs using performance aware scheduling techniques along with several optimizations, including architecture aware process placement, data locality conscious task assignment, data prefetching, and asynchronous data copy. These optimizations are employed to maximize the utilization of the aggregate computing power of CPUs and GPUs and minimize data copy overheads. Our experimental evaluation shows that the cooperative use of CPUs and GPUs achieves significant improvements on top of GPU-only versions (up to 1.6\u00d7) and that the execution of the application as a set of fine-grain operations provides more opportunities for runtime optimizations and attains better performance than coarser-grain, monolithic implementations used in other works. An implementation of the cancer image analysis pipeline using the runtime support was able to process an image dataset consisting of 36,848 4Kx4K-pixel image tiles (about 1.8TB uncompressed) in less than 4 minutes (150 tiles/second) on 100 nodes of a state-of-the-art hybrid cluster system.\"",
        "Document: \"In Situ Methods, Infrastructures, and Applications on High Performance Computing Platforms. The considerable interest in the high performance computing (HPC) community regarding analyzing and visualization data without first writing to disk, i.e., in situ processing, is due to several factors. First is an I/O cost savings, where data is analyzed/visualized while being generated, without first storing to a filesystem. Second is the potential for increased accuracy, where fine temporal sampling of transient analysis might expose some complex behavior missed in coarse temporal sampling. Third is the ability to use all available resources, CPU's and accelerators, in the computation of analysis products. This STAR paper brings together researchers, developers and practitioners using in situ methods in extreme-scale HPC with the goal to present existing methods, infrastructures, and a range of computational science and engineering applications using in situ analysis and visualization.\"",
        "Document: \"A Type System for High Performance Communication and Computation. The manner in which data is represented, accessed and transmitted has an affect upon the efficiency of any computing system. In the domain of high performance computing, traditional frameworks like MPI have relied upon a relatively static type system with a high degree of a priori knowledge shared among the participants. However, modern scientific computing is increasingly distributed and dynamic, requiring the ability to dynamically create multi-platform workflows, to move processing to data, and to perform both in situ and streaming data analysis. Traditional approaches to data type description and communication in middleware, which typically either require a priori agreement on data types, or resort to highly inefficient representations like XML, are insufficient for the new domain of dynamic science. This paper describes a different approach, using FFS, a middleware library that implements efficient manipulation of application-level data. FFS provides for highly efficient binary data communication, XML-like examination of unknown data, and both third-party and in situ data processing via dynamic code generation. All of these capabilities are fully dynamic at run-time, without requiring a priori agreements or knowledge of the exact form of the data being communicated or analyzed.\"",
        "Document: \"Combining Phase Identification and Statistic Modeling for Automated Parallel Benchmark Generation. Parallel application benchmarks are indispensable for evaluating/optimizing HPC software and hardware. However, it is very challenging and costly to obtain high-fidelity benchmarks reflecting the scale and complexity of state-of-the-art parallel applications. Hand-extracted synthetic benchmarks are time- and labor-intensive to create. Real applications themselves, while offering most accurate performance evaluation, are expensive to compile, port, reconfigure, and often plainly inaccessible due to security or ownership concerns. This work contributes APPrime, a novel tool for trace-based automatic parallel benchmark generation. Taking as input standard communication-I/O traces of an application's execution, it couples accurate automatic phase identification with statistical regeneration of event parameters to create compact, portable, and to some degree reconfigurable parallel application benchmarks. Experiments with four NAS Parallel Benchmarks (NPB) and three real scientific simulation codes confirm the fidelity of APPrime benchmarks. They retain the original applications' performance characteristics, in particular their relative performance across platforms. Also, the result benchmarks, already released online, are much more compact and easy-to-port compared to the original applications.\n\n\"",
        "Document: \"A Self-Managing Wide-Area Data Streaming Service using Model-based Online Control. Efficient and robust data streaming services are a critical requirement of emerging Grid applications, which are based on seamless interactions and coupling between geographically distributed application components. Furthermore the dynamism of Grid environments and applications requires that these services be able to continually manage and optimize their operation based on system state and application requirements. This paper presents a design and implementation of such a self-managing data-streaming service based on online control strategies. A Grid-based fusion workflow scenario is used to evaluate the service and demonstrate its feasibility and performance.\"",
        "Document: \"Runtime I/O re-routing + throttling on HPC storage. Massively parallel storage systems are becoming more and more prevalent on HPC systems due to the emergence of a new generation of data-intensive applications. To achieve the level of I/O throughput and capacity that is demanded by data intensive applications, storage systems typically deploy a large number of storage devices (also known as LUNs or data stores). In doing so, parallel applications are allowed to access storage concurrently, and as a result, the aggregate I/O throughput can be linearly increased with the number of storage devices, reducing the application's end-to-end time. For a production system where storage devices are shared between multiple applications, contention is often a major problem leading to a significant reduction in I/O throughput. In this paper, we describe our efforts to resolve this issue in the context of HPC using a balanced re-routing + throttling approach. The proposed scheme re-routes I/O requests to a less congested storage location in a controlled manner so that write performance is improved while limiting the impact on read.\"",
        "Document: \"DataStager: scalable data staging services for petascale applications. Known challenges for petascale machines are that (1) the costs of I/O for high performance applications can be substantial,\n especially for output tasks like checkpointing, and (2) noise from I/O actions can inject undesirable delays into the runtimes\n of such codes on individual compute nodes. This paper introduces the flexible \u2018DataStager\u2019 framework for data staging and\n alternative services within that jointly address (1) and (2). Data staging services moving output data from compute nodes\n to staging or I/O nodes prior to storage are used to reduce I/O overheads on applications\u2019 total processing times, and explicit\n management of data staging offers reduced perturbation when extracting output data from a petascale machine\u2019s compute partition.\n Experimental evaluations of DataStager on the Cray XT machine at Oak Ridge National Laboratory establish both the necessity\n of intelligent data staging and the high performance of our approach, using the GTC fusion modeling code and benchmarks running\n on 1000+ processors.\"",
        "1 is \"Towards dynamic data-driven optimization of oil well placement\", 2 is \"Sequoia: programming the memory hierarchy\"",
        "Given above information, for an author who has written the paper with the title \"Combining phase identification and statistic modeling for automated parallel benchmark generation\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001465": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Scheduling for shared window joins over data streams':",
        "Document: \"Continuous analytics over discontinuous streams. Continuous analytics systems that enable query processing over steams of data have emerged as key solutions for dealing with massive data volumes and demands for low latency. These systems have been heavily influenced by an assumption that data streams can be viewed as sequences of data that arrived more or less in order. The reality, however, is that streams are not often so well behaved and disruptions of various sorts are endemic. We argue, therefore, that stream processing needs a fundamental rethink and advocate a unified approach toward continuous analytics over discontinuous streaming data. Our approach is based on a simple insight - using techniques inspired by data parallel query processing, queries can be performed over independent sub-streams with arbitrary time ranges in parallel, generating partial results. The consolidation of the partial results over each sub-stream can then be deferred to the time at which the results are actually used on an on-demand basis. In this paper, we describe how the Truviso Continuous Analytics system implements this type of order-independent processing. Not only does the approach provide the first real solution to the problem of processing streaming data that arrives arbitrarily late, it also serves as a critical building block for solutions to a host of hard problems such as parallelism, recovery, transactional consistency, high availability, failover, and replication.\"",
        "Document: \"Resilient distributed datasets: a fault-tolerant abstraction for in-memory cluster computing. We present Resilient Distributed Datasets (RDDs), a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a fault-tolerant manner. RDDs are motivated by two types of applications that current computing frameworks handle inefficiently: iterative algorithms and interactive data mining tools. In both cases, keeping data in memory can improve performance by an order of magnitude. To achieve fault tolerance efficiently, RDDs provide a restricted form of shared memory, based on coarse-grained transformations rather than fine-grained updates to shared state. However, we show that RDDs are expressive enough to capture a wide class of computations, including recent specialized programming models for iterative jobs, such as Pregel, and new applications that these models do not capture. We have implemented RDDs in a system called Spark, which we evaluate through a variety of user applications and benchmarks.\"",
        "Document: \"A feasibility study of hierarchical multithreading. Many studies have shown that significant amounts of parallelism exist at different granularities. Execution models such as superscalar and VLIW exploit parallelism from a single thread. Multithreaded processors make a step towards exploiting parallelism from different threads, but are not geared to exploit parallelism at different granularities (fine and medium grain). In this paper we present a feasibility study of a new execution model for exploiting both adjacent and distant parallelism in the dynamic instruction stream. Our model, called hierarchical multithreading, uses a two-level hierarchical arrangement of processing elements. The lower level of the hierarchy exploits instruction-level parallelism and fine-grain threadlevel parallelism, whereas the upper level exploits more distant parallelism. Detailed simulation studies with a cycleaccurate simulator are presented, showing the feasibility of hierarchical multithreading. Conclusions are drawn about the best ways to obtain the most from the hierarchical multithreading scheme.\"",
        "Document: \"Realizing the potential of data science. Data science promises new insights, helping transform information into knowledge that can drive science and industry.\n\n\"",
        "Document: \"Research in Data Broadcast and Dissemination. The proliferation of the Internet and intranets, the development of wireless and satellite networks, and the availability of asymmetric, high-bandwidth links to the home, have fueled the development of a wide range of new \u201cdissemination-based\u201d applications. These applications involve the timely distribution of data to a large set of consumers, and include stock and sports tickers, traffic information systems, electronic personalized newspapers, and entertainment delivery. Dissemination-oriented applications have special characteristics that render traditional client-server data management approaches ineffective. These include: \r\n\r\n\r\ntremendous scale.\r\n\r\n\r\na high-degree of overlap in user data needs.\r\n\r\n\r\nasymmetric data flow from sources to consumers.\"",
        "Document: \"Diagnosing Machine Learning Pipelines with Fine-grained Lineage. We present the Hippo system to enable the diagnosis of distributed machine learning (ML) pipelines by leveraging fine-grained data lineage. Hippo exposes a concise yet powerful API, derived from primitive lineage types, to capture fine-grained data lineage for each data transformation. It records the input datasets, the output datasets and the cell-level mapping between them. It also collects sufficient information that is needed to reproduce the computation. Hippo efficiently enables common ML diagnosis operations such as code debugging, result analysis, data anomaly removal, and computation replay. By exploiting the metadata separation and high-order function encoding strategies, we observe an O(10^3)x total improvement in lineage storage efficiency vs. the baseline of cell-wise mapping recording while maintaining the lineage integrity. Hippo can answer the real use case lineage queries within a few seconds, which is low enough to enable interactive diagnosis of ML pipelines.\"",
        "Document: \"Getting It All from the Crowd.   Hybrid human/computer systems promise to greatly expand the usefulness of query processing by incorporating the crowd for data gathering and other tasks. Such systems raise many database system implementation questions. Perhaps most fundamental is that the closed world assumption underlying relational query semantics does not hold in such systems. As a consequence the meaning of even simple queries can be called into question. Furthermore query progress monitoring becomes difficult due to non-uniformities in the arrival of crowdsourced data and peculiarities of how people work in crowdsourcing systems. To address these issues, we develop statistical tools that enable users and systems developers to reason about tradeoffs between time/cost and completeness. These tools can also help drive query execution and crowdsourcing strategies. We evaluate our techniques using experiments on a popular crowdsourcing platform. \"",
        "Document: \"Functional Dependency Generation and Applications in Pay-As-You-Go Data Integration Systems. Recently, the opportunity of extracting structured data from the Web has been identified by a number of research projects. One such example is that millions of relational-style HTML tables can be extracted from the Web. Traditional data integration approaches do not scale over such corpora with hundreds of small tables in one domain. To solve this problem, previous work has proposed pay-as-you-go data integration systems to provide, with little up-front cost, base services over loosely-integrated informa- tion. One key component of such systems, which has received little attention to date, is the need for a framework to gauge and improve the quality of the integration. We propose a frame- work based on functional dependencies(FDs). Unlike in tradi- tional database design, where FDs are specified as statements of truth about all possible instances of the database; in web envi- ronment, FDs are not specified over the data tables. Instead, we generate FDs by counting-based algorithms over many data sources, and extend the FDs with probabilities to capture the in- herent uncertainties in them. Given these probabilistic FDs, we show how to solve two problems to improve data and schema qual- ity in a pay-as-you-go system: (1) pinpointing dirty data sources and (2) normalizing large mediated schemas. We describe these techniques and evaluate them over real-world data sets extracted from the Web.\"",
        "Document: \"The case for PIQL: a performance insightful query language. Large-scale, user-facing applications are increasingly moving from relational databases to distributed key/value stores for high-request-rate, low-latency workloads. Often, this move is motivated not only by key/value stores' ability to scale simply by adding more hardware, but also by the easy to understand predictable performance they provide for all operations. For complex queries, this approach often requires onerous explicit index management and imperative data lookup by the developer. We propose PIQL, a Performance Insightful Query Language that allows developers to express many queries found on these websites while still providing strict bounds on the number of I/O operations that will be performed.\"",
        "Document: \"PIQL: a performance insightful query language. Large-scale websites are increasingly moving from relational databases to distributed key-value stores for high request rate, low latency workloads. Often this move is motivated not only by key-value stores' ability to scale simply by adding more hardware, but also by the easy to understand predictable performance they provide for all operations. While this data model works well, lookups are only done by primary key. More complex queries require onerous, explicit index management and imperative data lookups by the developer. We demonstrate PIQL, a Performance Insightful Query Language that allows developers to express many of the queries found on these websites, while still providing strict bounds on the number of I/O operations for any query.\"",
        "1 is \"Transaction Time Support Inside a Database Engine\", 2 is \"WATCHMAN: A Data Warehouse Intelligent Cache Manager\"",
        "Given above information, for an author who has written the paper with the title \"Scheduling for shared window joins over data streams\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001485": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Deployment Study Length: How Long Should a System Be Evaluated in the Wild?':",
        "Document: \"Augmenting mobile 3G using WiFi. We investigate if WiFi access can be used to augment 3G capacity in mobile environments. We rst conduct a detailed study of 3G and WiFi access from moving vehicles, in three different cities. We find that the average 3G and WiFi availability across the cities is 87% and 11%, respectively. WiFi throughput is lower than 3G through-put, and WiFi loss rates are higher. We then design a system, called Wiffler, to augments mobile 3G capacity. It uses two key ideas leveraging delay tolerance and fast switching -- to overcome the poor availability and performance of WiFi. For delay tolerant applications, Wiffler uses a simple model of the environment to predict WiFi connectivity. It uses these predictions to delays transfers to offload more data on WiFi, but only if delaying reduces 3G usage and the transfers can be completed within the application's tolerance threshold. For applications that are extremely sensitive to delay or loss (e.g., VoIP), Wiffler quickly switches to 3G if WiFi is unable to successfully transmit the packet within a small time window. We implement and deploy Wiffler in our vehicular testbed. Our experiments show that Wiffler significantly reduces 3G usage. For a realistic workload, the reduction is 45% for a delay tolerance of 60 seconds.\"",
        "Document: \"A general model of wireless interference. We develop a general model to estimate the throughput and goodput between arbitrary pairs of nodes in the presence of interference from other nodes in a wireless network. Our model is based on measurements from the underlying network itself and is thus more accurate than abstract models of RF propagation such as those based on distance. The seed measurements are easy to gather, requiring only O(N) measurements in an N-node networks. Compared to existing measurement-based models, our model advances the state of the art in three important ways. First, it goes beyond pairwise interference and models interference among an arbitrary number of senders. Second, it goes beyond broadcast transmissions and models the more common case of unicast transmissions. Third, it goes beyond homogeneous nodes and models the general case of heterogeneous nodes with different traffic demands and different radio characteristics. Using simulations and measurements from two different wireless testbeds, we show that the predictions of our model are accurate in a wide range of scenarios.\"",
        "Document: \"Timecard: controlling user-perceived delays in server-based mobile applications. Providing consistent response times to users of mobile applications is challenging because there are several variable delays between the start of a user's request and the completion of the response. These delays include location lookup, sensor data acquisition, radio wake-up, network transmissions, and processing on both the client and server. To allow applications to achieve consistent response times in the face of these variable delays, this paper presents the design, implementation, and evaluation of the Timecard system. Timecard provides two abstractions: the first returns the time elapsed since the user started the request, and the second returns an estimate of the time it would take to transmit the response from the server to the client and process the response at the client. With these abstractions, the server can adapt its processing time to control the end-to-end delay for the request. Implementing these abstractions requires Timecard to track delays across multiple asynchronous activities, handle time skew between client and server, and estimate network transfer times. Experiments with Timecard incorporated into two mobile applications show that the end-to-end delay is within 50 ms of the target delay of 1200 ms over 90% of the time.\"",
        "Document: \"Analyzing the MAC-level behavior of wireless networks in the wild. We present Wit, a non-intrusive tool that builds on passive monitoring to analyze the detailed MAC-level behavior of operational wireless networks. Wit uses three processing steps to construct an enhanced trace of system activity. First, a robust merging procedure combines the necessarily incomplete views from multiple, independent monitors into a single, more complete trace of wireless activity. Next, a novel inference engine based on formal language methods reconstructs packets that were not captured by any monitor and determines whether each packet was received by its destination. Finally, Wit derives network performance measures from this enhanced trace; we show how to estimate the number of stations competing for the medium. We assess Wit with a mix of real traces and simulation tests. We find that merging and inference both significantly enhance the originally captured trace. We apply Wit to multi-monitor traces from a live network to show how it facilitates 802.11 MAC analyses that would otherwise be difficult or rely on less accurate heuristics.\"",
        "Document: \"Understanding and Mitigating Packet Corruption in Data Center Networks. We take a comprehensive look at packet corruption in data center networks, which leads to packet losses and application performance degradation. By studying 350K links across 15 production data centers, we find that the extent of corruption losses is significant and that its characteristics differ markedly from congestion losses. Corruption impacts fewer links than congestion, but imposes a heavier loss rate; and unlike congestion, corruption rate on a link is stable over time and is not correlated with its utilization. Based on these observations, we developed CorrOpt, a system to mitigate corruption. To minimize corruption losses, it intelligently selects which corrupting links can be safely disabled, while ensuring that each top-of-rack switch has a minimum number of paths to reach other switches. CorrOpt also recommends specific actions (e.g., replace cables, clean connectors) to repair disabled links, based on our analysis of common symptoms of different root causes of corruption. Our recommendation engine has been deployed in over seventy data centers of a large cloud provider. Our analysis shows that, compared to current state of the art, CorrOpt can reduce corruption losses by three to six orders of magnitude and improve repair accuracy by 60%.\"",
        "Document: \"ProjecToR: Agile Reconfigurable Data Center Interconnect. We explore a novel, free-space optics based approach for building data center interconnects. It uses a digital micromirror device (DMD) and mirror assembly combination as a transmitter and a photodetector on top of the rack as a receiver (Figure 1). Our approach enables all pairs of racks to establish direct links, and we can reconfigure such links (i.e., connect different rack pairs) within 12 us. To carry traffic from a source to a destination rack, transmitters and receivers in our interconnect can be dynamically linked in millions of ways. We develop topology construction and routing methods to exploit this flexibility, including a flow scheduling algorithm that is a constant factor approximation to the offline optimal solution. Experiments with a small prototype point to the feasibility of our approach. Simulations using realistic data center workloads show that, compared to the conventional folded-Clos interconnect, our approach can improve mean flow completion time by 30-95% and reduce cost by 25-40%.\"",
        "Document: \"Sustaining cooperation in multi-hop wireless networks. Multi-hop wireless networks are vulnerable to free-riders because they require nodes to forward packets for each other. Deployed routing protocols ignore this issue while proposed solutions incorporate complicated mechanisms with the intent of making free-riding impossible. We present Catch, a protocol that falls between these extremes. It achieves nearly the low mechanism requirements of the former while imposing nearly as effective barriers to free-riding as the latter. Catch is made possible by novel techniques based on anonymous messages. These techniques enable cooperative nodes to detect nearby free-riders and disconnect them from the rest of the network. Catch has low overhead and is broadly applicable across routing protocols and traffic workloads. We evaluate it on an 802.11 wireless testbed as well as through simulation.\"",
        "Document: \"Optical Layer Failures in a Large Backbone. We analyze optical layer outages in a large backbone, using data for over a year from thousands of optical channels carrying live IP layer traffic. Our analysis uncovers several findings that can help improve network management and routing. For instance, we find that optical links have a wide range of availabilities, which questions the common assumption in fault-tolerant routing designs that all links have equal failure probabilities. We also find that by monitoring changes in optical signal quality (not visible at IP layer), we can better predict (probabilistically) future outages. Our results suggest that backbone traffic engineering strategies should consider current and past optical layer performance and route computation should be based on the outage-risk profile of the underlying optical links.\"",
        "Document: \"Measurement-based characterization of 802.11 in a hotspot setting. We analyze wireless measurements taken during the SIGCOMM 2004 conference to understand how well 802.11 operates in real deployments. We find that the overhead of 802.11 is high, with only 40% of the transmission time spent in sending original data. Most of the remaining time is consumed by retransmissions due to packet losses that are caused by both contention and transmission errors. Our analysis also shows that wireless nodes adapt their transmission rates with an extremely high frequency. We comment on the difficulties and opportunities of working with wireless traces, rather than the wired traces of wireless activity that are presently more common.\"",
        "Document: \"Translating XSLT programs to Efficient SQL queries. We present an algorithm for translating XSLT programs into SQL. Our context is that of virtual XML publishing, in which a single XML view is defined from a relational database, and subsequently queried with XSLT programs. Each XSLT program is translated into a single SQL query and run entirely in the database engine. Our translation works for a large fragment of XSLT, which we define, that includes descendant/ancestor axis, recursive templates, modes, parameters, and aggregates. We put considerable effort in generating correct and efficient SQL queries and describe several optimization techniques to achieve this efficiency. We have tested our system on all 22 SQL queries of the TPC-H database benchmark which we represented in XSLT and then translated back to SQL using our translator.\"",
        "1 is \"Mobile-izing health workers in rural India\", 2 is \"Inter-datacenter bulk transfers with netstitcher\"",
        "Given above information, for an author who has written the paper with the title \"Deployment Study Length: How Long Should a System Be Evaluated in the Wild?\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001511": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'What's next in XML and databases?':",
        "Document: \"Answering XML Queries over Heterogeneous Data Sources.  This work describes an architecture for integrating heterogeneous data sources under an XML global schema, following the local-as-view approach (local sources\" schemas are described as views over the global schema). In this context, we focus on the problem of translating the user's query against the XML global schema into a SQL query over the local data sources. \"",
        "Document: \"Browsing Linked Data Catalogs with LODAtlas. The Web of Data is growing fast, as exemplified by the evolution of the Linked Open Data (LOD) cloud over the last ten years. One of the consequences of this growth is that it is becoming increasingly difficult for application developers and end-users to find the datasets that would be relevant to them. Semantic Web search engines, open data catalogs, datasets and frameworks such as LODStats and LOD Laundromat, are all useful but only give partial, even if complementary, views on what datasets are available on the Web. We introduce LODAtlas, a portal that enables users to find datasets of interest. Users can make different types of queries about both the datasets' metadata and contents, aggregated from multiple sources. They can then quickly evaluate the matching datasets' relevance, thanks to LODAtlas' summary visualizations of their general metadata, connections and contents.\"",
        "Document: \"XML content warehousing: Improving sociological studies of mailing lists and web data.   In this paper, we present the guidelines for an XML-based approach for the sociological study of Web data such as the analysis of mailing lists or databases available online. The use of an XML warehouse is a flexible solution for storing and processing this kind of data. We propose an implemented solution and show possible applications with our case study of profiles of experts involved in W3C standard-setting activity. We illustrate the sociological use of semi-structured databases by presenting our XML Schema for mailing-list warehousing. An XML Schema allows many adjunctions or crossings of data sources, without modifying existing data sets, while allowing possible structural evolution. We also show that the existence of hidden data implies increased complexity for traditional SQL users. XML content warehousing allows altogether exhaustive warehousing and recursive queries through contents, with far less dependence on the initial storage. We finally present the possibility of exporting the data stored in the warehouse to commonly-used advanced software devoted to sociological analysis. \"",
        "Document: \"RDF in the clouds: a survey. The Resource Description Framework (RDF) pioneered by the W3C is increasingly being adopted to model data in a variety of scenarios, in particular data to be published or exchanged on the Web. Managing large volumes of RDF data is challenging, due to the sheer size, the heterogeneity, and the further complexity brought by RDF reasoning. To tackle the size challenge, distributed storage architectures are required. Cloud computing is an emerging paradigm massively adopted in many applications for the scalability, fault-tolerance, and elasticity feature it provides, enabling the easy deployment of distributed and parallel architectures. In this article, we survey RDF data management architectures and systems designed for a cloud environment, and more generally, those large-scale RDF data management systems that can be easily deployed therein. We first give the necessary background, then describe the existing systems and proposals in this area, and classify them according to dimensions related to their capabilities and implementation techniques. The survey ends with a discussion of open problems and perspectives.\"",
        "Document: \"Lazy query evaluation for Active XML. In this paper, we study query evaluation on Active XML documents (AXML for short), a new generation of XML documents that has recently gained popularity. AXML documents are XML documents whose content is given partly extensionally, by explicit data elements, and partly intensionally, by embedded calls to Web services, which can be invoked to generate data.A major challenge in the efficient evaluation of queries over such documents is to detect which calls may bring data that is relevant for the query execution, and to avoid the materialization of irrelevant information. The problem is intricate, as service calls may be embedded anywhere in the document, and service invocations possibly return data containing calls to new services. Hence, the detection of relevant calls becomes a continuous process. Also, a good analysis must take the service signatures into consideration.We formalize the problem, and provide algorithms to solve it. We also present an implementation that is compliant with XML and Web services standards, and is used as part of the ActiveXML system. Finally, we experimentally measure the performance gains obtained by a careful filtering of the service calls to be triggered.\"",
        "Document: \"The WebStand Project. In this short paper we present the state of advance ment of the French ANR WebStand project. The objective of this project is to construct a customizable XML based warehouse platform to acquire, transform, analyze, store, query and expor t data from the web, in particular mailing lists, with the fina l intension of using this data to perform sociological studies foc used on social groups of World Wide Web. We are currently using this system to analyze the standardization process of the W3C, through its social network of standard setters.\"",
        "Document: \"Efficient XQuery rewriting using multiple views. We consider the problem of rewriting XQuery queries using multiple materialized XQuery views. The XQuery dialect we use to express views and queries corresponds to tree patterns (returning data from several nodes, at different granularities, ranging from node identifiers to full XML subtrees) with value joins. We provide correct and complete algorithms for finding minimal rewritings, in which no view is redundant. Our work extends the state of the art by considering more flexible views than the mostly XPath 1.0 dialects previously considered, and more powerful rewritings. We implemented our algorithms and assess their performance through a set of experiments.\"",
        "Document: \"Efficient query answering against dynamic RDF databases. A promising method for efficiently querying RDF data consists of translating SPARQL queries into efficient RDBMS-style operations. However, answering SPARQL queries requires handling RDF reasoning, which must be implemented outside the relational engines that do not support it. We introduce the database (DB) fragment of RDF, going beyond the expressive power of previously studied RDF fragments. We devise novel sound and complete techniques for answering Basic Graph Pattern (BGP) queries within the DB fragment of RDF, exploring the two established approaches for handling RDF semantics, namely reformulation and saturation. In particular, we focus on handling database updates within each approach and propose a method for incrementally maintaining the saturation; updates raise specific difficulties due to the rich RDF semantics. Our techniques are designed to be deployed on top of any RDBMS(-style) engine, and we experimentally study their performance trade-offs.\"",
        "Document: \"XQueC: A query-conscious compressed XML database. XML compression has gained prominence recently because it counters the disadvantage of the verbose representation XML gives to data. In many applications, such as data exchange and data archiving, entirely compressing and decompressing a document is acceptable. In other applications, where queries must be run over compressed documents, compression may not be beneficial since the performance penalty in running the query processor over compressed data outweighs the data compression benefits. While balancing the interests of compression and query processing has received significant attention in the domain of relational databases, these results do not immediately translate to XML data. In this article, we address the problem of embedding compression into XML databases without degrading query performance. Since the setting is rather different from relational databases, the choice of compression granularity and compression algorithms must be revisited. Query execution in the compressed domain must also be rethought in the framework of XML query processing due to the richer structure of XML data. Indeed, a proper storage design for the compressed data plays a crucial role here. The XQueC system (XQuery Processor and Compressor) covers a wide set of XQuery queries in the compressed domain and relies on a workload-based cost model to perform the choices of the compression granules and of their corresponding compression algorithms. As a consequence, XQueC provides efficient query processing on compressed XML data. An extensive experimental assessment is presented, showing the effectiveness of the cost model, the compression ratios, and the query execution times.\"",
        "Document: \"Efficient Data and Program Integration Using Binding Patterns.  : In this work, we investigate data and program integration in a fully distributedpeer-to-peer mediation architecture. The challenge in making such a system succeed at alarge scale is twofold. First, sharing a resource should be easy; therefore, we need a simpleconcept for modeling resources. Second, we need an ecient architecture for distributedquery execution, capable of handling well costly computations and large data transfers.To model heterogeneous resources, we propose using the... \"",
        "1 is \"Hadoop++: making a yellow elephant run like a cheetah (without it even noticing)\", 2 is \"Adversarial support vector machine learning\"",
        "Given above information, for an author who has written the paper with the title \"What's next in XML and databases?\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001552": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Latent association analysis of document pairs':",
        "Document: \"Design and implementation of a pluggable fault tolerant CORBA infrastructure. In this paper we present the design and implementation of a Pluggable Fault Tolerant CORBA Infrastructure that provides fault tolerance for CORBA applications by utilizing the pluggable protocols framework that is available for most CORBA ORBS. Our approach does not require modification to the CORBA ORB, and requires only minimal modifications to the application. Moreover; it avoids the difficulty of retrieving and assigning the ORB state, by incorporating the fault tolerance mechanisms into the ORB. The Pluggable Fault Tolerant CORBA Infrastructure achieves performance that is similar to, or better than, that of other Fault Tolerant CORBA systems, while providing strong replica consistency.\"",
        "Document: \"Design And Implementation Of A Consistent Time Service For Fault-Tolerant Distributed Systems. Clock-related operations are one of the many sources of replica non-determinism and of replica inconsistency in fault-tolerant distributed systems. In passive replication, if the primary replica crashes, the next clock value returned by the new primary replica might have actually rolled back in time, which can lead to undesirable consequences for the replicated application. The same problem can happen for active replication when the result of the first replica to respond is taken as the next clock value. In this paper we describe the design and implementation of a Consistent Time Service for fault-tolerant distributed systems. The Consistent Time Service introduces a group clock that is consistent across the replicas and that ensures the determinism of the replicas with respect to clock-related operations. The group clock is monotonically increasing, is transparent to tie application, and is fault-tolerant. The Consistent Time Service guarantees the consistency of the group clock even when faults occur, when new replicas are added into the group, and when failed replicas recover.\"",
        "Document: \"Decentralized search and retrieval for mobile networks using SMS. This paper describes the iTrust over SMS decentralized search and retrieval system for mobile networks. Any mobile device in the iTrust network can communicate with any other mobile device in the iTrust network to distribute, search for, and retrieve information. Third-party developers can use the iTrust over SMS API on the Android platform to add this search and retrieval functionality to existing applications quickly and easily. Developers of applications for other mobile device platforms can use the iTrust over SMS protocol to create compatible applications that can communicate using iTrust over SMS. In addition to the iTrust over SMS components and protocol, this paper presents a performance evaluation of the iTrust over SMS system, which shows that the probability of information retrieval is high, even if some of the mobile devices are not available. It also shows that the average search latency is consistently less if all of the participating nodes use the same mobile service provider, and is consistently more if the nodes use different mobile service providers.\"",
        "Document: \"A lossless, minimal latency protocol for gigabit ATM networks. Advances in fiber-optic and VLSI technology have led to the emergence of very high-speed networks based on asynchronous transfer mode (ATM). The time required to transmit the data into the network at the source is small compared to the delay to propagate the data from source to destination. Cell loss is also a major concern in ATM networks because waiting for the retransmission of lost cells delays the delivery of cells and requires substantial buffer space. The instant start protocol eliminates the costly bandwidth reservation delay before transmission can begin. Simultaneously, it provides lossless transmission even when the network cannot handle the offered rate of transmission. Unlike other lossless protocols, instant start requires relatively little special control hardware or processing at each switch\"",
        "Document: \"Protection against covert storage and timing channels. Existing technology is quite successful at preventing direct unauthorized communication in multilevel secure computer systems, but is almost completely ineffective at protecting such systems against covert storage and timing channels. In a covert channel, one process transmits secret information by modulating its rate of use of a shared resource, while another program detects that modulation by monitoring the responsiveness of the resource. The proposed protection technique involves screening all programs in a system by a data dependency analysis procedure that determines whether the results of those programs depend on the relative timing of operations within the system. Programs containing such timing dependencies are denied access to the system until certified by other means. The approach is reasonably inexpensive and completely rigorous and, when strictly applied, precludes all communication over covert storage and timing channels\"",
        "Document: \"A Reservation-Based Extended Transaction Protocol For Coordination Of Web Services. Web services can be used to automate business activities that span multiple enterprises over the Internet. Such business activities require a coordination protocol to reach consistent results among the participants in the business activity. In the current state of the art, either classical distributed transactions or extended transactions with compensating transactions are used However classical distributed transactions lock data in the databases of different enterprises for unacceptable durations or involve repeated retries, and compensating transactions can lead to inconsistencies in the databases of the different enterprises. In this article, we describe a novel reservation protocol that can be used to coordinate the tasks of a business activity. Instead of resorting to compensating transactions, the reservation protocol employs an explicit reservation phase and an explicit confirmation and cancellation phase. We show how our reservation protocol maps to the Web set-vices coordination specification, and describe out-implementation of the reservation protocol. Me compare the performance of the reservation protocol with that of the two-phase commit protocol and optimistic two-phase commit protocol. We also compare the probability of inconsistency for the reservation protocol with that for compensating transactions.\"",
        "Document: \"Asynchronous fault-tolerant total ordering algorithms. Two novel efficient algorithms for placing a total order on messages in an asynchronous fault-tolerant distributed system are presented. The algorithms are resilient to fewer than n/3 and n/2 faulty processes in an n-process system. Partial correctness and probabilistic termination are demonstrated; it is also shown that there does not exist a total ordering algorithm that is guaranteed to terminate. A comparison of the complexity of the algorithms is given.\"",
        "Document: \"Unification of Replication and Transaction Processing in Three-Tier Architectures. In this paper we describe a software infrastructure that unifies replication and transaction processing in three-tier architectures and, thus, provides high availability and fault tolerance for enterprise applications. The infrastructure is based on the Fault Tolerant CORBA and CORBA Object Transaction Service standards, and works with commercial-off-the-shelf application servers and database systems. The infrastructure replicates the application servers to protect the business logic processing. In addition, it replicates the transaction coordinator which renders the two-phase commit protocol non-blocking and, thus, avoids potentially long service disruptions caused by coordinator failure. The infrastructure handles the interactions between the application servers and the database servers through replicated gateways that prevent duplicate requests from reaching the database servers. The infrastructure implements client-side automatic failover mechanisms, which guarantees that clients know the outcome of the requests that they have made. The infrastructure starts the transactions at the application servers, and retries aborted transactions, caused by process or communication failures, automatically on the behalf of the clients.\"",
        "Document: \"Fast message ordering and membership using a logical token-passing ring. The Totem protocol supports consistent concurrent operations by placing a total order on broadcast messages. This total order is achieved by including a sequence number in a token circulated around a logical ring that is imposed on a set of processors in a broadcast domain. A membership algorithm handles reconfiguration, including restarting of a failed processor and remerging of a partitioned network. Effective flow-control allows the protocol to achieve message ordering rates two to three times higher than the best prior protocols. The single-ring total ordering protocol of Totem provides fault-tolerant agreed and safe delivery of messages within a broadcast domain\"",
        "Document: \"Strongly Consistent Replication And Recovery Of Fault-Tolerant Corba Applications. The Eternal system provides transparent fault tolerance for CORBA applications, without requiring the modification of either the application or the ORB. Eternal replicates the application objects, and ensures strong replica consistency by employing a reliable totally-ordered multicast protocol for conveying the HOP messages of the application. To achieve strong replica consistency during recovery, Eternal retrieves and transfers the three kinds of state - application-level state, ORB/POA-level state and infrastructure-level state - from an existing replica to a new or recovering replica, and logs and replays messages.\"",
        "1 is \"Predicting positive and negative links in online social networks\", 2 is \"Service Oriented Architecture for E-health Support Services Based on Grid Computing Over\"",
        "Given above information, for an author who has written the paper with the title \"Latent association analysis of document pairs\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001566": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A multiprocess network logic with temporal and spatial modalities':",
        "Document: \"SWIPE: eager erasure of sensitive data in large scale systems software. We describe SWIPE, an approach to reduce the life time of sensitive, memory resident data in large scale applications written in C. In contrast to prior approaches that used a delayed or lazy approach to the problem of erasing sensitive data, SWIPE uses a novel eager erasure approach that minimizes the risk of accidental sensitive data leakage. SWIPE achieves this by transforming a legacy C program to include additional instructions that erase sensitive data immediately after its intended use. SWIPE is guided by a highly-scalable static analysis technique that precisely identifies the locations to introduce erase instructions in the original program. The programs transformed using SWIPE enjoy several additional benefits: minimization of leaks that arise due to data dependencies; erasure of sensitive data with minimal developer guidance; and negligible performance overheads.\"",
        "Document: \"Automatically preparing safe SQL queries. We present the first sound program source transformation approach for automatically transforming the code of a legacy web application to employ PREPARE statements in place of unsafe SQL queries. Our approach therefore opens the way for eradicating the SQL injection threat vector from legacy web applications.\"",
        "Document: \"Incremental Verification of Architecture Specification Language for Real-Time Systems. The concept of software architecture has recently emerged as a new way to improve our ability to effectively construct large scale software systems. However, there is no formal architecture specification language available to model and analyze complex real-time systems. In this paper, an object-oriented logic-based architecture specification language for real-time systems is discussed. Representation of real-time properties and timing constraints, and their integration with the language to model real-time concurrent systems is given. Architecture based specification languages enable the construction of large system architectures and provide a means of testing and validation. In general, checking the timing constraints of real-time systems is done by applying model checking to the constraint expressed as a formula in temporal logic. The complexity of such a formal method depends on the size of the representation of the system. It is possible that this size could increase exponentially when the system consists of several concurrently executing real-time processes. This means that the complexity of the algorithm will be exponential in the number of processes of the system and thus the size of the system becomes a limiting factor. Such a problem has been defined in literature as the \"state explosion problem\". We propose a method of incremental verification of architectural specifications for real-time systems. The method has a lower complexity in a sense that it does not work on the whole state space, but only on a subset of it that is relevant to the property to be verified.\"",
        "Document: \"Decidable and expressive classes of probabilistic automata. k-Hierarchical probabilistic automata (HPA) are probabilistic automata whose states are partitioned into k+1 levels such that for any state and input symbol, at most one transition goes to a state at the same level, and others go to higher level states. We show that 1-HPA, with acceptance threshold 1/2 (in the finite and infinite word cases) can recognize non-regular languages, and the non-emptiness and non-universality problems for 1-HPA with threshold 1/2 are decidable in EXPTIME and are PSPACE-hard. We present a new sufficient condition when 1-HPA recognize regular languages. We show that the emptiness problem is undecidable for 2-HPAs.\"",
        "Document: \"Monitoring off-the-shelf components. Software is being developed from off-the-shelf third party components. The interface specification of such a component may be under specified or may not fully match the user requirement. In this paper, we address the problem of customizing such components to particular users. We achieve this by constructing a monitor that monitors the component and detects any bad behaviors. Construction of such monitors essentially involves synthesizing safety properties that imply a given property that is obtained from the interface specifications of the component and the goal specification of the user. We present various methods for synthesizing such safety properties when the given property is given by an automaton or a temporal logic formula. We show that our methods are sound and complete. These results are extensions of the results given in [11].\"",
        "Document: \"Reasoning about Qualitative Spatial Relationships. In this paper, we consider various spatial relationships that are of general interest in pictorial database systems and other applications. We present a set of rules that allow us to deduce new relationships from a given set of relationships. A deductive mechanism using these rules can be used in query-processing systems that retrieve pictures by content. The given set of rules is shown to be isound; that is, the deductions are logically correct. The rules are also shown to be icomplete for three-dimensional systems; that is, every relationship that is implied by a given consistent set of relationships iF is deducible from iF using the given rules. In addition, we show that the given set of rules is incomplete for two-dimensional systems. We also present efficient algorithms for the deduction and reduction problems. The deduction problem consists of computing all the relationships deducible from a given set, while the reduction problem consists of computing a minimal subset of a given set of relationships that implies all the relationships in the given set.\"",
        "Document: \"Modeling and Querying Moving Objects. In this paper we propose a data model for representing moving objects in database systems. It is called the Moving Objects Spatio-Temporal (MOST) data model. We also propose Future Temporal Logic (FTL) as the query language for the MOST model, and devise an algorithm for processing FTL queries in MOST.\"",
        "Document: \"An economic model for resource exchange in mobile peer to peer networks. Consider an urban area with hundreds of thousands of vehicles. Drivers and passengers in these vehicles are interested in information relevant to their trip. For example, a driver would like his/her vehicle to continuously display on a map the available parking spaces around the current location. Or, the driver may be interested in the traffic conditions (e.g. average speed) one mile ahead. In this paper we examine the dissemination of information about resources in mobile peer-to-peer networks, where vehicles and sensors communicate with each other via short-range wireless transmission. Each disseminated resource represents information about a spatial-temporal event, such as the availability of a parking slot at a particular time. We propose an opportunistic dissemination paradigm, in which a moving object transmits the resources it carries to encountered vehicles and obtains new resources in exchange. We develop and analyze a family of economic models that provide incentive for collaboration in data dissemination. The proposed system has the potential to create a completely new information marketplace.\"",
        "Document: \"A query processor for prediction-based monitoring of data streams. Networks of sensors are used in many different fields, from industrial applications to surveillance applications. A common feature of these applications is the necessity of a monitoring infrastructure that analyzes a large number of data streams and outputs values that satisfy certain constraints. In this paper, we present a query processor for monitoring queries in a network of sensors with prediction functions. Sensors communicate their values according to a threshold policy, and the proposed query processor leverages prediction functions to compare tuples efficiently and to generate answers even in the absence of new incoming tuples. Two types of constraints are managed by the query processor: window-join constraints and value constraints. Uncertainty issues are considered to assign probabilistic values to the results returned to the user. Moreover, we have developed an appropriate buffer management strategy, that takes into account the contributions of the prediction functions contained in the tuples. We also present some experimental results that show the benefits of the proposal.\"",
        "Document: \"Formal Languages and Algorithms for Similarity Based Retrieval from Sequence Databases. Similarity based retrieval is of major importance for querying sequence databases. We consider formalisms based on automata, temporal logics and regular expressions for querying such databases. We define two different types of similarity measures--syntax based and semantics based. These measures are divided into a spectrum of measures based on the vector distance function that is employed. We consider norm vector distance functions and give efficient query processing algorithms when these measures are employed.\"",
        "1 is \"Retroactive and proactive database processing\", 2 is \"An Elementary Proof Of The Completeness Of Pdl\"",
        "Given above information, for an author who has written the paper with the title \"A multiprocess network logic with temporal and spatial modalities\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001646": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Pursuing an evader through cooperative relaying in multi-agent surveillance networks.':",
        "Document: \"Observer-Based Adaptive NN Control for a Class of Uncertain Nonlinear Systems With Nonsymmetric Input Saturation. This paper is concerned with the problem of adaptive tracking control for a class of uncertain nonlinear systems with nonsymmetric input saturation and immeasurable states. The radial basis function of neural network (NN) is employed to approximate unknown functions, and an NN state observer is designed to estimate the immeasurable states. To analyze the effect of input saturation, an auxiliary sy...\"",
        "Document: \"On Designing Event-Triggered Schemes for Networked Control Systems Subject to One-Step Packet Dropout. Different from time-triggered control strategies, the event-triggered control (ETC) scheme takes action (e.g., sensing and actuation) when needed, which will greatly reduce the communication traffic in networked control systems (NCSs). On the other hand, due to reduced number of transmissions, the ETC is more vulnerable to communication dropouts compared with the periodic sampling. This paper focu...\"",
        "Document: \"A study of heat transfer analysis for squeezing flow of a Casson fluid via differential transform method. In this article, differential transform method is proposed and applied for semi-analytic solution of heat transfer analysis for the squeezing flow of a Casson fluid between parallel circular plates. Similarity transformation reduces this model into an equivalent system of two strongly nonlinear ordinary differential equations. Fourth-order Runge\u2013Kutta method has also been applied to support our analytical solution, and the comparison shows an excellent agreement.\"",
        "Document: \"Towards a framework for automatic service composition in manufacturing grid. In order to realize automatic service composition in Manufacturing Grid (MG), a framework based on manufacturing domain-specific ontology (MGOnto) is proposed. MGOnto integrates three existing manufacturing ontologies, and represented in First Order Logic and Rules respectively. The framework consists of five core and three supporting services. It can reuse previously processed workflows in local repository and compose new workflows from services MG wide, by automatic service chaining which uses a MGOnto based backward recursive algorithm. A test bed is implemented and experiments on the example of an airfoil rib verify the feasibility of the framework.\"",
        "Document: \"Mechanical assembly of soft deployable structures and robots. This work describes the approaches of fabricating soft deployable structures using smart materials based soft hinge actuators combining a soft matrix and hinge-like movement through a rigid framework. The soft hinge actuator has the advantage of being simple to fabricate, inexpensive, lightweight and simple to actuate. This primary actuator can then be used to form modules capable of deployable deformation, which can then be assembled into deployable structures. The design of soft deployable structures is based on three principles: design of primary hinge actuators, assembly of modules and assembly of modules into large-scale deployable structures. This work describes both the non-dismountable and the dismountable modular assembling methods for constructing the deployable structures. The non-dismountable assembly is constructed by assembling the modules using 3D printed mechanical joints. Meanwhile, the dismountable assembly is constructed using identical soft deployable modules with embedded magnetic elements as the dismountable connectors. Then, by assembling identical soft deploy modules in different manners, different types of soft deployable structures can be easily and fast built and tested by re-utilizing the modules. The proposed approaches highlight the robust strategies for the rapid and simple prototyping of various complex soft deployable structures and robots.\"",
        "Document: \"A new metaheuristic algorithm: car tracking optimization algorithm. Over the last decade, several metaheuristic algorithms have emerged to solve numerical function optimization problems. Since the performance of these algorithms presents a suboptimal behavior, a large number of studies have been carried out to find new and better algorithms. Therefore, this paper proposes a new metaheuristic algorithm, namely the car tracking optimization algorithm; it is inspired by observing the programming methods of other metaheuristic algorithms. And the proposed algorithm has been tested over 55 benchmark functions, and the results have been compared with firefly algorithm (FA), cuckoo searching algorithm (CS), and vortex search algorithm (VS). The results indicate that the performance of the proposed algorithm surpasses FA, CS, and VS algorithm.\"",
        "Document: \"Efficiently indexing shortest paths by exploiting symmetry in graphs. Shortest path queries (SPQ) are essential in many graph analysis and mining tasks. However, answering shortest path queries on-the-fly on large graphs is costly. To online answer shortest path queries, we may materialize and index shortest paths. However, a straightforward index of all shortest paths in a graph of N vertices takes O(N2) space. In this paper, we tackle the problem of indexing shortest paths and online answering shortest path queries. As many large real graphs are shown richly symmetric, the central idea of our approach is to use graph symmetry to reduce the index size while retaining the correctness and the efficiency of shortest path query answering. Technically, we develop a framework to index a large graph at the orbit level instead of the vertex level so that the number of breadth-first search trees materialized is reduced from O(N) to O(|\u0394|), where |\u0394| \u2264 N is the number of orbits in the graph. We explore orbit adjacency and local symmetry to obtain compact breadth-first-search trees (compact BFS-trees). An extensive empirical study using both synthetic data and real data shows that compact BFS-trees can be built efficiently and the space cost can be reduced substantially. Moreover, online shortest path query answering can be achieved using compact BFS-trees.\"",
        "Document: \"High-speed digital-controlled variable voltage source with current monitor for EIT application. This paper presented the design method of a digital-controlled variable voltage source with current monitor function. The hardware of system mainly consists of FPGA, DAC, programmable gain amplifier, current sensing circuit and comparator. By using high-speed analog comparator to convert over-current time to length of digital pulse, the FPGA could monitor the amplitude of current dynamically. In aspect of software, CORDIC algorithm is implemented in FPGA to realize the DDS function and related algorithms. Simulation result shows that system has good performance over wide frequency range. With the advantages of accuracy and high stability, it is suitable for EIT application. \u00a9 2011 IEEE.\"",
        "Document: \"A Two-Stage Online Prediction Method for a Blast Furnace Gas System and Its Application. The byproduct gas in steel industry is one of the most significant energy resources of an enterprise. Due to the large quantity of yield, fluctuation, and various categories of users encountered in a blast furnace gas (BFG) system, it is very difficult to accurately predict the amount of gas to be generated and forecast the users' consumption demand. In this paper, a two-stage online prediction method based on an improved echo state network (ESN) is proposed to realize forecasting in the BFG system. In this method, one completes the prediction realized at the levels of BFG generation and consumption using a class of ESN with input compensation and parameter optimization. At the second stage, to predict gas holder level of the BFG system, the energy flows being predicted at the first stage are denoised, and their correlation with the holder level are determined by using a concept of grey correlation with time delay. Then the effect factors exhibiting high correlation levels are extracted to construct the model of the gas holder. The prediction system designed in this manner is applied in the Energy Center of Baosteel Co., Ltd, China. The results demonstrate that the prediction system exhibits high accuracy and can provide an effective guidance for balancing and scheduling of the byproduct energy.\"",
        "Document: \"Event-triggered control for stochastic nonlinear systems. In this work, we investigate the problem of event-triggered stabilization for a class of stochastic nonlinear systems. An event-triggered control (ETC) approach is proposed by introducing an additional internal dynamic variable. The presented event-triggered mechanism (ETM) can guarantee the existence of a positive lower bound on inter-event times (or called inter-execution times). In addition, the presented technique can ensure the second moment asymptotic stability of the closed-loop stochastic nonlinear system.\"",
        "1 is \"An evaluation of crowd counting methods, features and regression models.\", 2 is \"Clustering in diffusively coupled networks\"",
        "Given above information, for an author who has written the paper with the title \"Pursuing an evader through cooperative relaying in multi-agent surveillance networks.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001692": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Towards a Theoretical Framework for Learning Multi-modal Patterns for Embodied Agents':",
        "Document: \"Iterate averaging as regularization for stochastic gradient descent. We propose and analyze a variant of the classic Polyak--Ruppert averaging scheme, broadly used in stochastic gradient methods. Rather than a uniform average of the iterates, we consider a weighted average, with weights decaying in a geometric fashion. In the context of linear least squares regression, we show that this averaging scheme has a the same regularizing effect, and indeed is asymptotically equivalent, to ridge regression. In particular, we derive-finite sample bounds for the proposed approach that match the best known results for regularized stochastic gradient methods.\"",
        "Document: \"A Note on Learning with Integral Operators. A large number of learning algorithms, for exam- ple, spectral clustering, kernel Principal Compo- nents Analysis and many manifold methods, are based on estimating eigenvalues and eigenfunctions of operators defined by a similarity function or a kernel, given empirical data. Thus for the analysis of algorithms, it is an important problem to be able to assess the quality of such approximations. The contribution of our paper is two-fold: 1. We use a technique based on a concentration inequality for Hilbert spaces to provide new much simplified proofs for a number of results in spec- tral approximation. 2. Using these methods we provide several new re- sults for estimating spectral properties of the graph Laplacian operator extending and strengthening re- sults from (27). A broad variety of methods for machine learning and data analysis from Principal Components Analysis (PCA) to Ker- nel PCA, Laplacian-based spectral clustering and manifold methods, rely on estimating eigenvalues and eigenvectors of certain data-dependent matrices. In many cases these matri- ces can be interpreted as empirical versions of underlying in- tegral operators or closely related objects, such as continuous Laplace operators. Establishing connections between empir- ical operators and their continuous counterparts is essential to understanding these algorithms. In this paper, we propose a method for analyzing empirical operators based on concen- tration inequalities in Hilbert spaces. This technique together with perturbation theory results allows us to derive a number of results on spectral convergence in an exceptionally simple way. We note that the approach using concentration inequal- ities in a Hilbert space has already been proved useful for analyzing supervised kernel algorithms, see for example (3) and references therein. Here we build on this approach to provide a detailed and comprehensive study of perturbation\"",
        "Document: \"Speeding-Up Object Detection Training For Robotics With Falkon. Latest deep learning methods for object detection provide remarkable performance, but have limits when used in robotic applications. One of the most relevant issues is the long training time, which is due to the large size and imbalance of the associated training sets, characterized by few positive and a large number of negative examples (i.e. background). Proposed approaches are based on end-to-end learning by back-propagation [22] or kernel methods trained with Hard Negatives Mining on top of deep features [8]. These solutions are effective, but prohibitively slow for on-line applications.In this paper we propose a novel pipeline for object detection that overcomes this problem and provides comparable performance, with a 60x training speedup. Our pipeline combines (i) the Region Proposal Network and the deep feature extractor from [22] to efficiently select candidate RoIs and encode them into powerful representations, with (ii) the FALKON [23] algorithm, a novel kernel-based method that allows fast training on large scale problems (millions of points). We address the size and imbalance of training data by exploiting the stochastic subsampling intrinsic into the method and a novel, fast, bootstrapping approach.We assess the effectiveness of the approach on a standard Computer Vision dataset (PASCAL VOC 2007 [5]) and demonstrate its applicability to a real robotic scenario with the iCubWorld Transformations [18] dataset.\"",
        "Document: \"Some Properties of Regularized Kernel Methods. In regularized kernel methods, the solution of a learning problem is found by minimizing functionals consisting of the sum of a data and a complexity term. In this paper we investigate some properties of a more general form of the above functionals in which the data term corresponds to the expected risk. First, we prove a quantitative version of the representer theorem holding for both regression and classification, for both differentiable and non-differentiable loss functions, and for arbitrary offset terms. Second, we show that the case in which the offset space is non trivial corresponds to solving a standard problem of regularization in a Reproducing Kernel Hilbert Space in which the penalty term is given by a seminorm. Finally, we discuss the issues of existence and uniqueness of the solution. From the specialization of our analysis to the discrete setting it is immediate to establish a connection between the solution properties of sparsity and coefficient boundedness and some properties of the loss function. For the case of Support Vector Machines for classification, we also obtain a complete characterization of the whole method in terms of the Khun-Tucker conditions with no need to introduce the dual formulation.\"",
        "Document: \"On the Sample Complexity of Subspace Learning.   A large number of algorithms in machine learning, from principal component analysis (PCA), and its non-linear (kernel) extensions, to more recent spectral embedding and support estimation methods, rely on estimating a linear subspace from samples. In this paper we introduce a general formulation of this problem and derive novel learning error estimates. Our results rely on natural assumptions on the spectral properties of the covariance operator associated to the data distribu- tion, and hold for a wide class of metrics between subspaces. As special cases, we discuss sharp error estimates for the reconstruction properties of PCA and spectral support estimation. Key to our analysis is an operator theoretic approach that has broad applicability to spectral learning methods. \"",
        "Document: \"Model Selection for Regularized Least-Squares Algorithm in Learning Theory. We investigate the problem of model selection for learning algorithms depending on a continuous parameter. We propose a model selection procedure based on a worst-case analysis and on a data-independent choice of the parameter. For the regularized least-squares algorithm we bound the generalization error of the solution by a quantity depending on a few known constants and we show that the corresponding model selection procedure reduces to solving a bias-variance problem. Under suitable smoothness conditions on the regression function, we estimate the optimal parameter as a function of the number of data and we prove that this choice ensures consistency of the algorithm.\"",
        "Document: \"Learning with SGD and Random Features. Sketching and stochastic gradient methods are arguably the most common techniques to derive efficient large scale learning algorithms. In this paper, we investigate their application in the context of nonparametric statistical learning. More precisely, we study the estimator defined by stochastic gradient with mini batches and random features. The latter can be seen as form of nonlinear sketching and used to define approximate kernel methods. The considered estimator is not explicitly penalized/constrained and regularization is implicit. Indeed, our study highlights how different parameters, such as number of features, iterations, step-size and mini-batch size control the learning properties of the solutions. We do this by deriving optimal finite sample bounds, under standard assumptions. The obtained results are corroborated and illustrated by numerical experiments.\"",
        "Document: \"Learning from Examples as an Inverse Problem. Many works related learning from examples to regularization techniques for inverse problems, emphasizing the strong algorithmic and conceptual analogy of certain learning algorithms with regularization algorithms. In particular it is well known that regularization schemes such as Tikhonov regularization can be effectively used in the context of learning and are closely related to algorithms such as support vector machines. Nevertheless the connection with inverse problem was considered only for the discrete (finite sample) problem and the probabilistic aspects of learning from examples were not taken into account. In this paper we provide a natural extension of such analysis to the continuous (population) case and study the interplay between the discrete and continuous problems. From a theoretical point of view, this allows to draw a clear connection between the consistency approach in learning theory and the stability convergence property in ill-posed inverse problems. The main mathematical result of the paper is a new probabilistic bound for the regularized least-squares algorithm. By means of standard results on the approximation term, the consistency of the algorithm easily follows.\"",
        "Document: \"Exploiting global force torque measurements for local compliance estimation in tactile arrays. In this paper we tackle the problem of estimating the local compliance of tactile arrays exploiting global measurements from a single force and torque sensor. The proposed procedure exploits a transformation matrix (describing the relative position between the local tactile elements and the global force/torque measurements) to define a linear regression problem on the unknown local stiffness. Experiments have been conducted on the foot of the iCub robot, sensorized with a single force/torque sensor and a tactile array of 250 tactile elements (taxels) on the foot sole. Results show that a simple calibration procedure can be employed to estimate the stiffness parameters of virtual springs over a tactile array and to use these model to predict normal forces exerted on the array based only on the tactile feedback. Leveraging on previous works [1] the proposed procedure does not necessarily need a-priori information on the transformation matrix of the taxels which can be directly estimated from available measurements.\"",
        "Document: \"GURLS: a least squares library for supervised learning. We present GURLS, a least squares, modular, easy-to-extend software library for efficient supervised learning. GURLS is targeted to machine learning practitioners, as well as non-specialists. It offers a number state-of-the-art training strategies for medium and large-scale learning, and routines for efficient model selection. The library is particularly well suited for multi-output problems (multi-category/multi-label). GURLS is currently available in two independent implementations: Matlab and C++. It takes advantage of the favorable properties of regularized least squares algorithm to exploit advanced tools in linear algebra. Routines to handle computations with very large matrices by means of memory-mapped storage and distributed task execution are available. The package is distributed under the BSD license and is available for download at https://github.com/LCSL/GURLS.\"",
        "1 is \"Towards a theoretical foundation for Laplacian-based manifold methods\", 2 is \"Development of a finger-shaped tactile sensor and its evaluation by active touch\"",
        "Given above information, for an author who has written the paper with the title \"Towards a Theoretical Framework for Learning Multi-modal Patterns for Embodied Agents\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001739": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Integrated visualization for geometry PIG data':",
        "Document: \"Development of augmented forward collision warning system for Head-Up Display. In this paper, we present an augmented forward collision warning system for Head-Up Display (HUD). The convergence of HUD and Augmented Reality (AR) needs challenge and make innovative application in automobile industry. We focus on an advance of Forward Collision Warning System (FCWS) by the fusion of HUD and AR. The proposed system detects the frontal vehicles and pedestrians, assesses the imminent danger caused by the detected results, and alerts the augmented warnings to support safe driving practices.\"",
        "Document: \"Panoramic Vision System to Eliminate Driver\u2019s Blind Spots using a Laser Sensor and Cameras. In this paper, we propose a multipurpose panoramic vision system for eliminating the blind spot and informing the driver of approaching vehicles using three cameras and a laser sensor. A wide-angle camera is attached to the car trunk and two cameras are attached under each side-view mirror to eliminate the blind spot of a vehicle. A laser sensor is attached on the rear-left of the vehicle to gather information from around the vehicle. The proposed system performs a pre-processing to estimate several system parameters. We compute a warping map to compensate the wide-angle lens distortion of the rear camera. We estimate the focus-of-contraction (FOC) in the rear image, and interactively compute the homography between the rear image and the laser sensor data. Homographies between each side-view image and the rear image are also computed in the pre-processing step. Using the system parameters obtained in the pre-processing step, the proposed system generates a panoramic mosaic view to eliminate the blind spot. First we obtain the undistorted rear image using the warping map. Then, we find road boundaries and classify approaching vehicles in laser sensor data, and overlap the laser sensor data on the rear image for further visualization. Next, the system performs the image registration process after segmentation of road and background regions based on road boundaries. Finally, it generates various views such as a cylindrical panorama view, a top view, a side view and an information panoramic mosaic view for displaying varied safety information.\"",
        "Document: \"Localized earth mover's distance for robust histogram comparison. The Earth Mover's Distance (EMD) is a useful cross-bin distance metric for comparing two histograms. The EMD is based on the minimal cost that must be paid to transform one histogram into the other. But outlier noise in the histogram causes the EMD to be greatly exaggerated. In this paper, we propose the localized Earth Mover's Distance (LEMD). The LEMD separates noises from meaningful transportation of data by specifying local relations among bins, and gives a predefined penalty to those noises, according to the applications. An extended version of the tree-based transportation simplex algorithm is proposed for LEMD. The localized property of LEMD is formulated similarly to the original EMD with the thresholded ground distance, such as EMD-hat [7] and FastEMD [8]. However, we show that LEMD is more stable than EMD-hat for noise-added or shape-deformed data, and is faster than FastEMD that is the state of the art among EMD variants.\"",
        "Document: \"A hand-held approach to 3D reconstruction using light stripe projections onto a cube frame. The current paper presents a new light-striping approach for reconstructing a 3D model from a real object. The proposed system consists of a light plane projector, camera, and cube frame with LEDs attached. Instead of a strictly controlled camera and light emitters, a freely movable hand-held device is used that enables one to scan self-occluded objects. As in other light-striping systems, the correspondence problem is solved by projecting a light plane onto an object inside a frame. In the proposed system, the 3D coordinates of an illuminated light stripe are obtained using a calibration-free approach or dynamic optical triangulation. Furthermore, the computed 3D point data does not require any registration process because the data is directly measured based on unified world coordinates. Experimental results proved the accuracy of the measurements and consistency of the outcomes without any knowledge of the camera and light source parameters. \"",
        "Document: \"Delaunay Triangles Model for Image-Based Motion Retargeting. We present an automatic system for retargeting a human body motion extracted from an image sequence into a new character in a still image. In contrast to analyzing the articulated motion of its skeleton in the previous vision-based human body tracking and posture recognition system, we use direct 2-D image warping based on a silhouette. At first, we represent the performer's silhouette with the Delaunay Triangles Model (DTM) of which the boundary points are the critical points of the silhouette. We then use a set of affine transformations of Delaunay triangles for the human body motion, which is applied to a new character for the deformation of the subject's DTM. The final animation of the subject is texture mapped using backward Radial Basis Functions (RBFs). Although our algorithm presented in this paper is not applicable to the human body with self-occluded motion, it allows believable photo- realistic motion retargeting.\"",
        "Document: \"Background/Foreground Separation: Guided Attention based Adversarial Modeling (GAAM) versus Robust Subspace Learning Methods. Background-Foreground separation and appearance generation is a fundamental step in many computer vision applications. Existing methods like Robust Subspace Learning (RSL) suffer performance degradation in the presence of challenges like bad weather, illumination variations, occlusion, dynamic backgrounds and intermittent object motion. In the current work we propose a more accurate deep neural ne...\"",
        "Document: \"A model-based 3-D tracking of rigid objects from a sequence of multiple perspective views. A method of tracking multiple objects of known geometry using multiple cameras is proposed. Our approach differs from the previous approaches in that the object geometry is tightly integrated into the tracking process. The major contribution is threefold: Firstly, multiple cameras are used to improve the accuracy of the estimated posture parameters. Additional formalism required by considering multiple images is nicely integrated into the tracking model, and is handled effectively. Secondly, the feature tracking is facilitated by integrating the measurement and dynamic models into the matching process, thereby improving the accuracy and robustness of the feature correspondence. Thirdly, ambiguities that may arise in the course of the feature matching are resolved by the statistical analysis and the visibility test. The entire process from the image sequence to the posture parameters has been completely automated into a single, seamless process, and has been extensively tested on synthetic and real images.\"",
        "Document: \"Estimation of Illuminants for Plausible Lighting in Augmented Reality. This paper presents a practical method to estimate the positions of light sources in real environment, using a mirror sphere placed on a known natural marker. For the stable results of static lighting, we take the multiple images around a sphere and estimate the principal light directions of the vector clusters for each light source in running-time. We also estimate the moving illuminant for changes of the scene illumination, and augment the virtual objects onto the real image with the proper shading and shadows. Some experimental results show that the proposed method produces plausible AR visualization in real time.\"",
        "Document: \"A streaming engine for PC-Based 3d network games onto heterogeneous mobile platforms. In this paper, we present a new middle-ware, streaming engine that can implement existing OpenGL-based 3D network games onto heterogeneous platforms. The engine consists of capturing OpenGL command stream, scene graph reconstruction, data simplification, and compression and transmission. Without modifying the original source code, our system can extend 3D network games onto various platforms, using hierarchical geometry hashing, a client-server scene graph, and a simple NPR(Non-Photorealistic Rendering) technique to reduce the amount of transmission.\"",
        "Document: \"hSGM: hierarchical pyramid based stereo matching algorithm. In this paper, we propose a variant of Semi-Global Matching, hSGM which is a hierarchical pyramid based dense stereo matching algorithm. Our method aggregates the matching costs from the coarse to fine scale in multiple directions to determine the optimal disparity for each pixel. It has several advantages over the original SGM: a low space complexity and efficient implementation on GPU. We show several experimental results to demonstrate our method is efficient and obtains a good quality of disparity maps.\"",
        "1 is \"Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion\", 2 is \"Estimation of Illumination Distribution Using a Specular Sphere\"",
        "Given above information, for an author who has written the paper with the title \"Integrated visualization for geometry PIG data\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "001886": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Exploring heterogeneous information networks and random walk with restart for academic search.':",
        "Document: \"Regional subgraph discovery in social networks. This paper solves a region-based subgraph discovery problem. We are given a social network and some sample nodes which is supposed to belong to a specific region, and the goal is to obtain a subgraph that contains the sampled nodes with other nodes in the same region. Such regional subgraph discovery can benefit region-based applications, including scholar search, friend suggestion, and viral marketing. To deal with this problem, we assume there is a hidden backbone connecting the query nodes directly or indirectly in their region. The idea is that individuals belonging to the same region tend to share similar interests and cultures. By modeling such fact on edge weights, we search the graph to extract the regional backbone with respect to the query nodes. Then we can expand the backbone to derive the regional network. Experiments on a DBLP co-authorship network show the proposed method can effectively discover the regional subgraph with high precision scores.\"",
        "Document: \"Mining polyphonic repeating patterns from music data using bit-string based approaches. Mining repeating patterns from music data is one of the most interesting issues of multimedia data mining. However, less work are proposed for mining polyphonic repeating patterns. Hence, two efficient algorithms, A-PRPD (Apriori-based Polyphonic Repeating Pattern Discovery) and T-PRPD (Tree-based Polyphonic Repeating Pattern Discovery), are proposed to discover polyphonic repeating patterns from music data. Furthermore, a bit-string method is developed for improving the efficiency of the proposed algorithms. Experimental results show that the proposed algorithms, A-PRPD and T-PRPD, are both effective and efficient methods for mining polyphonic repeating patterns from synthetic music data and real data.\"",
        "Document: \"Integration of Transfer of Learning to the Adaptive Learning Environment. The Instructional Activity Model (IAM) is a general purpose model to generate an adaptive learning course which is compatible with the SCORM standard. IAM is composed of related Activity Tree (AT) nodes and capability nodes. Prerequisites are capabilities supposed to posses before learning an AT while contributions are capabilities after learning an AT. IAM model supports the adaptive learning sequencing by considering the relationships between AT and capability nodes. However, the IAM model does not take the transfer of learning into consideration. In this paper, we propose the mechanism to integrate the concept of learning transfer to the IAM model. In our proposed mechanism, the relationships between capabilities are considered based on the similarity measure between capabilities. The selection process of IAM model is also modified to reflect the relationships of capabilities.\"",
        "Document: \"Emotion-based music recommendation by association discovery from film music. With the growth of digital music, the development of music recommendation is helpful for users. The existing recommendation approaches are based on the users' preference on music. However, sometimes, recommending music according to the emotion is needed. In this paper, we propose a novel model for emotion-based music recommendation, which is based on the association discovery from film music. We investigated the music feature extraction and modified the affinity graph for association discovery between emotions and music features. Experimental result shows that the proposed approach achieves 85% accuracy in average.\"",
        "Document: \"Relevance feedback for category search in music retrieval based on semantic concept learning. Traditional content-based music retrieval systems retrieve a specific music object which is similar to what a user has requested. However, the need exists for the development of category search for the retrieval of a specific category of music objects which share a common semantic concept. The concept of category search in content-based music retrieval is subjective and dynamic. Therefore, this paper investigates a relevance feedback mechanism for category search of polyphonic symbolic music based on semantic concept learning. For the consideration of both global and local properties of music objects, a segment-based music object modeling approach is presented. Furthermore, in order to discover the user semantic concept in terms of discriminative features of discriminative segments, a concept learning mechanism based on data mining techniques is proposed to find the discriminative characteristics between relevant and irrelevant objects. Moreover, three strategies, the Most-Positive, the Most-Informative, and the Hybrid, to return music objects concerning user relevance judgments are investigated. Finally, comparative experiments are conducted to evaluate the effectiveness of the proposed relevance feedback mechanism. Experimental results show that, for a database of 215 polyphonic music objects, 60% average precision can be achieved through the use of the proposed relevance feedback mechanism.\"",
        "Document: \"Popularity Prediction of Social Multimedia Based on Concept Drift. Microblogging services such as Twitter and Plurk allow users to easily access and share different types of social multimedia (e.g. images and videos) over the online social world. However, information overload happens to users and prohibits them from reaching popular and important digital contents. This paper studies the problem of predicting the popularity of social multimedia which is embedded in short messages of microblogging social networks. Social multimedia exhibits the property that they might be persistently or periodically re-shared and thus their popularity might resurrect at some time and evolve over time. We exploit the idea of concept drift to capture this property. We formulate the problem using classification, and propose to tackle the tasks of Re-share classification and Popularity Score classification. Two categories of features are devised and extracted, including information diffusion and explicit multimedia meta information. We develop a concept drift-based popularity predictor, by ensembling multiple trained classifiers from social multimedia instances in different time intervals. The key lies in dynamically determining the ensemble weights of classifiers. Experiments conducted on the Plurk data show the high accuracy on the popularity classification and the promising results on detecting popular social multimedia.\"",
        "Document: \"Comparative Analysis of Exon Skipping Patterns in Human and Mouse. Alternative splicing of pre-mRNAs is a major mechanism of generating protein diversity in higher eukaryotes. It is well known that one of the major forms of alternative splicing is exon skipping, which alternatively skips specific exon during splicing. Previous research, by Miriami and his colleagues, was conducted by using a statistical method to identify two motifs both in upstream and downstream introns that were associated with exon skipping events. In this study, we employed pattern branching motif finding algorithm and approximate mining of consensus sequences algorithm. The data mining approach developed in the study showed its strength of being able to completely discover all possible motifs and made itself a more ideal approach compared to Miriami's.. statistical one. Furthermore, because of the similarities in genes between human and mouse, we compared the two species' motifs in our study. We also discovered the patterns are very likely associated with exon skipping event between human and mouse.\"",
        "Document: \"DSM-FI: an efficient algorithm for mining frequent itemsets in data streams. Online mining of data streams is an important data mining problem with broad applications. However, it is also a difficult problem since the streaming data possess some inherent characteristics. In this paper, we propose a new single-pass algorithm, called DSM-FI (data stream mining for frequent itemsets), for online incremental mining of frequent itemsets over a continuous stream of online transactions. According to the proposed algorithm, each transaction of the stream is projected into a set of sub-transactions, and these sub-transactions are inserted into a new in-memory summary data structure, called SFI-forest (summary frequent itemset forest) for maintaining the set of all frequent itemsets embedded in the transaction data stream generated so far. Finally, the set of all frequent itemsets is determined from the current SFI-forest. Theoretical analysis and experimental studies show that the proposed DSM-FI algorithm uses stable memory, makes only one pass over an online transactional data stream, and outperforms the existing algorithms of one-pass mining of frequent itemsets.\"",
        "Document: \"Online Mining of Recent Music Query Streams. Mining multimedia data is one of the most important issues in data mining. In this paper, we propose an online one-pass algorithm to mine the set of frequent temporal patterns in online music query streams with a sliding window. An effective bit-sequence representation is used to reduce the processing time and memory needed to slide the windows. Experiments show that the proposed algorithm only needs a half of memory requirement of original music query data, and just scans the data once\"",
        "Document: \"Team Formation for Generalized Tasks in Expertise Social Networks. Given an expertise social network and a task consisting of a set of required skills, the team formation problem aims at finding a team of experts who not only satisfy the requirements of the given task but also communicate to one another in an effective manner. To solve this problem, Lappas et al. has proposed the Enhance Steiner algorithm. In this work, we generalize this problem by associating each required skill with a specific number of experts. We propose three approaches to form an effective team for the generalized task. First, we extend the Enhanced-Steiner algorithm to a generalized version for generalized tasks. Second, we devise a density-based measure to improve the effectiveness of the team. Third, we present a novel grouping-based method that condenses the expertise information to a group graph according to required skills. This group graph not only drastically reduces the search space but also avoid redundant communication costs and irrelevant individuals when compiling team members. Experimental results on the DBLP dataset show the teams found by our methods performs well in both effectiveness and efficiency.\"",
        "1 is \"Finding top k most influential spatial facilities over uncertain objects\", 2 is \"Web usage mining: discovery and applications of usage patterns from Web data\"",
        "Given above information, for an author who has written the paper with the title \"Exploring heterogeneous information networks and random walk with restart for academic search.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002055": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Error probability of 2DPSK with phase noise':",
        "Document: \"Capacity Analysis and Power Allocation over Non-Identical MISO Rayleigh Fading Channels. We analyze the capacity of a multiple-input single- output system over Rayleigh fading channels. The channels are assumed to be independent and non-identically distributed. Simple, explicit and closed-form expressions of ergodic mutual information and outage probability are obtained. Moreover, two suboptimal but efficient analytical power allocation schemes for mutual information maximization and outage minimization are derived, respectively. In specific, for mutual information maximization, more power is assigned to those channels with higher channel variances, while for outage minimization the power allocation scheme follows the water-filling principle.\"",
        "Document: \"Computing And Bounding The Generalized Marcum Q-Function Via A Geometric Approach. The generalized Marcum Q-function, Q(m)(a,b), is here explained geometrically as the probability of a 2m-dimensional, real, Gaussian random vector, whose mean vector has a Frobenius norm of a, lying outside of a hypersphere of 2m dimensions, with radius b, and centered at the origin. Based on this new geometric interpretation, a new closed-form representation for Q(m) (a, b) is derived for the case where m is an odd multiple of 0.5. This representation involves only the exponential and the erfc functions, and thus is easy to handle, both numerically and analytically. For the case where m is an even multiple of 0.5, Q(m+0.5)(a,b) and Q(m-0.5)(a,b), which can be evaluated using our new representation mentioned above, are shown to be tight upper and lower bounds on Q(m) (a, b), respectively. They are shown in most cases to be much tighter than the existing bounds in the literature, and are valid for the entire ranges of a and b concerned. Their average is also a good approximation to Q(m) (a, b).\"",
        "Document: \"Robust Decoding Of Concatenated Rs-Convolutional Codes Over The Quasi-Static Fading Channel With No Explicit Csi Acquisition. Accurate channel state information (CSI) is crucial for reliable decoding over fading channels. In many existing works, the CSI acquisition relies greatly on accurate knowledge of the channel model information (CMI), which includes, in particular, the power spectrum of the fading process and the statistical distribution of the fading gain. In practice, it may be difficult to obtain or keep track of the CMI accurately. In this paper, we consider the decoding of Reed-Solomon convolutional concatenated (RSCC) codes over the quasi-static fading channel, and propose a robust, decision-directed, soft-output convolutional decoder which does not require explicit CSI acquisition. Simulation result shows that the proposed decoder has a lower bit-error rate for convolutional decoding than the conventional soft-output Viterbi algorithm and the BCJR algorithm with pilot-symbol-assisted channel estimation. The advantage of the former in terms of the accuracy of soft-decision output is demonstrated by the superiority of word-error performance in the RSCC decoding.\"",
        "Document: \"Receiver Design for Linearly Modulated Signals with Unknown Carrier Frequency and Phase Offset. This paper is concerned with receiver design for digital communications with linearly modulated signals over the additive, white Gaussian noise channel with unknown carrier frequency and phase offset. The modulation techniques examined are phase shift keying with and without differential encoding and M-ary quadrature amplitude modulation. Two types of receiver structures are proposed, namely, the symbol-by-symbol receiver with pilot-symbol assisted-modulation and the simultaneous data detection and carrier frequency/phase offset estimation receiver. Both receivers incorporate the use of the maximum likelihood frequency/phase estimator of a single sinusoid in noise that we recently derived in the work of Fu and Kam (2007 and 2006) to estimate and compensate for the effects of the unknown carrier frequency and phase offset. The error performance of the proposed receivers are examined via computer simulations.\"",
        "Document: \"A Robust And Efficient Detection Algorithm For The Photon-Counting Free-Space Optical System. We propose a Viterbi-type trellis-search algorithm to implement the FSO photon-counting sequence receiver proposed in [1] more efficiently and a selective-store strategy to overcome the error floor problem observed therein. The simulation results show that the performance can be improved significantly by our implementation method.\"",
        "Document: \"A symbol-by-symbol channel estimation receiver for space-time block coded systems and its performance analysis on the nonselective rayleigh fading channel. We present a symbol-by-symbol channel estimation receiver for an orthogonal space-time block coded system, and derive its analytical performance on a slow, nonselective, Rayleigh fading channel. Exact, closed-form expressions for its bit error probability (BEP) performance for M-ary phase shift-keying modulations are obtained, which enable us to theoretically predict the actual performance achieva...\"",
        "Document: \"Design of MAC with cooperative spectrum sensing in ad hoc cognitive radio networks. We propose a MAC for wireless ad hoc cognitive radio networks where secondary users employ cooperative spectrum sensing to mitigate the degradation of the channel between primary transmitter and secondary users. The sensing reports and fused decisions are transmitted based on random access of CSMA/CA and 802.11e EDCA on the control channel, whose access scheme determines the overall achievable throughput among the multi-channels. We propose several schemes and derive the upper bound of overall throughput. The saturation problem is also studied to address the optimization of the channel selection and the trade-off between cooperative sensing gain and channel reuse efficiency.\"",
        "Document: \"Orthogonal Space-Time Block Codes over Semi-Identical Channels with Channel Estimation. Assuming the channel gains associated with different receive antennas are not identically distributed, we study the orthogonal space-time block codes over non-identical channels with channel estimation. It is shown that the non-identical statistics lead to non-identical channel estimation errors, which make the conventional optimum decoder sub-optimum in this case. A new optimum decoder is derived. We show that it can be simplified to a symbol-by-symbol decoder under certain conditions. Analytical and simulation results show that our new decoder substantially outperforms the conventional decoder.\"",
        "Document: \"Exponential-Type Bounds on the First-Order Marcum Q-Function. This paper presents new bounds on the Marcum Q-function Q1(a, b). First, new, simple, arbitrarily tight, exponential lower bounds on the zeroth-order modifided Bessel function of the first kind I-0(x) and the Gaussian Q-function Q(x) are derived. Due to their elegant features, these new bounds are fundamental in their own rights. Then, when they are applied to the Marcum Q-function that is expressed as a function of I-0(x) and Q(x), various new exponential-type lower and upper bounds are developed.\"",
        "Document: \"Differential Diversity Reception of MDPSK over Independent Rayleigh Channels with Nonidentical Branch Statistics and Asymmetric Fading Spectrum. This paper is concerned with optimum diversity receiver structure and its performance analysis of differential phase shift keying (DPSK) with differential detection over nonselective, independent, nonidentically distributed, Rayleigh fading channels. The fading process in each branch is assumed to have an arbitrary Doppler spectrum with arbitrary Doppler bandwidth, but to have distinct, asymmetric fading power spectral density characteristic. Using 8-DPSK as an example, the average bit error probability (BEP) of the optimum diversity receiver is obtained by calculating the BEP for each of the three individual bits. The BEP results derived are given in exact, explicit, closed-form expressions which show clearly the behavior of the performance as a function of various system parameters.\"",
        "1 is \"Breadth-First Maximum Likelihood Detection in Multiuser CDMA\", 2 is \"On the symbol error probability of maximum-selection diversity reception schemes over a Rayleigh fading channel\"",
        "Given above information, for an author who has written the paper with the title \"Error probability of 2DPSK with phase noise\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002136": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'QoS-adaptive bandwidth scheduling in continuous media streaming':",
        "Document: \"Topology Control For Increasing Connectivity In Cooperative Wireless Ad Hoc Networks. We propose a novel topology control scheme that reduces the transmission power of nodes and increases the network connectivity, based on the fact that Cooperative Communication (CC) technology can bridge disconnected networks Simulation results demonstrate that our scheme greatly increases the connectivity for a given transmission power, compared to other topology control schemes\"",
        "Document: \"DSML: Dual Signal Metrics for Localization in Wireless Sensor Networks. In wireless sensor networks and wireless ad-hoc networks, localization systems have used diverse signal metrics such as Received Signal Strength Indicator (RSSI) and Time Difference of Arrival (TDoA) for accurate assignment of a node position. We propose a novel scheme that applies two signal metrics, which are TDoA and RSSI exclusively, into time- based positioning scheme (TPS). For energy-efficient coverage extension, the proposed scheme uses range check technique that reduces the communication energy consumption of nodes. With two location information of neighbor nodes, the node can calculate two candidate positions through bilateration. Without an additional beacon message reception, range check is applied to find the unique position between two candidate positions. Range check also can be carried out collaboratively in general environment with the information of two-hop neighbor nodes. At the performance evaluation, we analyze and test the reduced communication cost of nodes in the extended area. Also, it is shown that the ratio of unique position assignment is increased in the general environment by range check technique.\"",
        "Document: \"Construction of directional virtual backbones with minimum routing cost in wireless networks. It is well-known that the application of directional antennas can help conserve bandwidth and energy consumption in wireless networks. Thus, to achieve efficiency in wireless networks, we study a special virtual backbone (VB) using directional antennas, requiring that from one node to any other node in the network, there exists at least one directional shortest path all of whose intermediate directions should belong to the VB, named as Minimum rOuting Cost Directional VB (MOC-DVB). In addition, VB has been well studied in Unit Disk Graph (UDG). However, radio wave based communications in wireless networks may be interrupted by obstacles (e.g., buildings and mountains). Thus, in this paper, we model a network as a general directed graph. We prove that construction of a minimum MOC-DVB is an NP-hard problem in a general directed graph and in term of the size of MOC-DVB, there exists an unreachable lower bound of the polynomial-time selected MOC-DVB. Therefore, we propose a distributed approximation algorithm for constructing MOC-DVB with approximation ratio of 1 + ln K + 2ln \u03b4D, where K is the number of antennas on each node and \u03b4D is the maximum direction degree in the network. Extensive simulations demonstrate that our constructed MOC-DVB is much more efficient in the sense of MOC-DVB size and routing cost compared to other VBs.\"",
        "Document: \"Effect of localized optimal clustering for reader anti-collision in RFID networks: fairness aspects to the readers. This paper proposes an adaptive and dynamic localized scheme unique to hierarchical clustering in RFID networks, while reducing the overlapping areas of clusters and consequently reducing collisions among RFID readers. Drew on our LLC scheme that adjusts cluster coverage to minimize energy consumption, low-energy localized clustering for RFID networks (LLCR) addresses RFID reader anti-collision problem in this paper. LLCR is a RFID reader anti-collision algorithm that minimizes collisions by minimizing overlapping areas of clusters that each RFID reader covers. LLCR takes into account each RFID reader's energy state as well as RFID reader collisions. For the energy state factor, we distinguish homogeneous RFID networks from heterogeneous ones according to computing power of each RFID reader. Therefore, we have designed efficient homo-LLCR and hetero-LLCR schemes for each case. Our simulation-based performance evaluation shows that LLCR minimizes energy consumption and overlapping areas of clusters of RFID readers.\"",
        "Document: \"Delay minimization of tree-based neighbor discovery in mobile robot networks. In this paper, delay minimization schemes for tree-based neighbor discovery in mobile robot networks are proposed and analyzed. Depending on the tree construction scheme, the expected value of neighbor discovery delay is changed. In our study, we focus on M-ary and M-Binary tree-based neighbor discovery. Regarding the number of neighboring robots, M-ary tree-based neighbor discovery has low but steady performance whilst M-Binary tree-based neighbor discovery shows better performance for optimal M. The simulation results provide performance comparisons of these schemes.\"",
        "Document: \"Eliminating Duplicate Forwarding in Wireless Opportunistic Routing. Opportunistic routing is a wireless multi-hop routing approach that allows each packet to be relayed through any node that overhears the transmission. In this letter, we propose a method for avoiding duplicate forwarding in opportunistic routing. The proposed technique enables forwarding nodes to control relaying at their neighbors on a per-packet basis using a small amount of information piggybacked on packets. Our simulation results show that DFOR achieves higher throughput than existing relevant protocols by reducing unnecessary transmissions.\"",
        "Document: \"Topology Control in Cooperative Wireless Ad-Hoc Networks. Topology control is to determine the transmission power of each node so as to maintain network connectivity and consume the minimum transmission power. Cooperative Communication (CC) is a new technology that allows multiple nodes to simultaneously transmit the same data. It can save transmission power and extend transmission coverage. However, prior research work on topology control considers CC only in the aspect of energy saving, not that of coverage extension. We observe that CC can bridge (link) disconnected networks and therefore identify the challenges in the development of a centralized topology control scheme, named shape Cooperative Bridges, which reduces transmission power of nodes as well as increases network connectivity. We propose three algorithms that select energy efficient neighbor nodes, which assist a source node to communicate with a destination node: an optimal method and two greedy heuristics. In addition, we consider a distributed version of the proposed topology control scheme. Our findings are substantiated by an extensive simulation study, through which we show that the shape Cooperative Bridges scheme substantially increases the connectivity with tolerable increase of transmission power compared to other existing topology control schemes, which means that it outperforms in terms of a connectivity-to-power ratio.\"",
        "Document: \"The Chain Effect For The Reputation-Based Trust Model In Peer-To-Peer Computing. This letter analyzes a resource chain trust model for P2P reputation-based systems. Many researchers have given a lot of efforts to reputation-based system area and some of them have made good theoretical models. Problems are to spread malicious contents whereas the remark that such models only concentrate on the relationship between the node and its direct neighbors is still controversial. To solve the problems, we introduced the RCM (Resource Chain Model) and the Enhanced RCM. In this letter, we analyze the models and then show usage of our models can help us to find the best and safest location efficiently and decrease the number of malicious transaction.\"",
        "Document: \"Adaptive Path Planning for Randomly Deployed Wireless Sensor Networks. In this paper, we propose an adaptive path planning scheme considering the length of movement path and number of beacon messages of a mobile beacon for its energy efficiency, where the sensor nodes are randomly deployed. Contrary to the previous studies that utilize mobile beacons (nodes sending beacon messages) only on the basis of a random movement method or predefined static movement paths, the proposed scheme provides energy-efficient and adaptive movement path construction with low computational complexity. The movement path also includes beacon positions in which the mobile beacon broadcasts beacon messages containing the information of its current position. The random movement methods are not concerned about the energy of the mobile beacon. In randomly deployed environments, it is not easy to obtain precise field information for static movement path decisions. Thus, we propose the adaptive path planning scheme which can operate without this information in randomly deployed wireless sensor networks, and improve the energy efficiency of the mobile beacon. The candidate areas that limit the search space are devised so as to provide low complexity. The performance evaluation shows that the proposed scheme reduces the movement distance and number of beacon messages of the mobile beacon by comparison with other methods.\"",
        "Document: \"Performance Analysis of Multicast and Broadcast Services in Mobile WiMAX Systems. In this letter, we investigate the performance of multicast and broadcast services (MBS) in mobile WiMAX systems. We develop an analytical model for the session blocking probability and the session disruption probability. Numerical results demonstrate that the session blocking probability and disruption probability are significantly affected by the session arrival rate and session popularity.\"",
        "1 is \"NeXt generation/dynamic spectrum access/cognitive radio wireless networks: a survey\", 2 is \"Efficient and Effective Clustering Methods for Spatial Data Mining\"",
        "Given above information, for an author who has written the paper with the title \"QoS-adaptive bandwidth scheduling in continuous media streaming\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002189": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A Visual Evaluation Framework for In-Home Physical Rehabilitation':",
        "Document: \"Inter-frame dependency in multiview multi-description video streaming. Multiview video coding has been attracting increased popularity for facilitating stereo-vision and free-viewpoint video applications. Compares to conventional single view video coding, multiview video coding utilizes inter and intra view motion compensations to yield an improved coding efficiency, and multiple description video coding helps improving video streaming over multiple lossy channels; on the other hand, multiview and multiple description coding techniques increase video frame dependencies, with trades off video streaming quality with an increased drifting error probability. This paper examines the frame dependencies of multi-view video coding against configurable parameters such as the length of group-of-pictures, and the number of viewpoints.\"",
        "Document: \"Video Multicast over Wireless Ad Hoc Networks Using Distributed Optimization. Video multicast over wireless ad hoc networks is a quite challenging task. In this paper, we propose an optimized video multicast scheme. Firstly, we apply prioritized coding scheme and network coding scheme to eliminate the decoding hierarchy and delivery redundancy. Then, we maximize the aggregate throughput at all the receivers by jointly optimizing both the source rate allocation and the routing scheme. The proposed algorithm is fully distributed, thus very suitable for wireless ad hoc networks. Simulation results show that the proposed video multicast scheme yields a superior video quality compared to the double-tree routing scheme.\"",
        "Document: \"Optimal resource allocation for multimedia cloud based on queuing model. Multimedia cloud, as a specific cloud paradigm, addresses how cloud can effectively process multimedia services and provide QoS provisioning for multimedia applications. There are two major challenges in multimedia cloud. The first challenge is the service response time in multimedia cloud, and the second challenge is the cost of cloud resources. In this paper, we optimize resource allocation for multimedia cloud based on queuing model. Specifically, we optimize the resource allocation in both single-class service case and multiple-class service case. In each case, we formulate and solve the response time minimization problem and resource cost minimization problem, respectively. Simulation results demonstrate that the proposed optimal allocation scheme can optimally utilize the cloud resources to achieve a minimal mean response time or a minimal resource cost.\"",
        "Document: \"Recognizing human emotion from audiovisual information. In this paper, we present an emotion recognition system to classify human emotional state from audiovisual signals. We extract prosodic, mel-frequency cepstral coefficient (MFCC), and formant frequency features to represent the audio characteristics of the emotional speech. A face detection scheme, based on the HSV color model, is used to detect the face from the background. The facial expressions are represented by Gabor wavelet features. We perform feature selection by using a stepwise method based on Mahalanobis distance. A classification scheme involving the analysis of individual class and combinations of different classes is proposed. Our emotion recognition system is tested over a language and race independent database, and an overall recognition accuracy of 82.14% is achieved.\"",
        "Document: \"Wavelet-Based Texture Retrieval using Independent Component Analysis. In this paper, a novel approach to texture retrieval using independent component analysis (ICA) in wavelet domain is proposed. It is well recognized that the wavelet coefficients in different subbands are statistically correlated, resulting in the fact that the product of the marginal distributions of wavelet coefficients is not accurate enough to characterize the stochastic properties of texture images. To tackle this problem, we employ (ICA) in feature extraction to decorrelate the analysis coefficients in different subbands, followed by modeling the marginal distributions of the separated sources using generalized Gaussian density (GGD), and perform similarity measure based on the maximum likelihood criterion. It is demonstrated by simulation results on a database consisting of 1776 texture images that the proposed method improve the accuracy of texture image retrieval in terms of average retrieval rate, compared with the traditional method using GGD for feature extraction and Kullback-Leibler divergence for similarity measure.\"",
        "Document: \"Inter-subband Redundancy Prediction Using Neural Network for Video Coding. High performance video codec is mandatory for multimedia applications such as video-on-demand and video conferencing. Recent research has proposed numerous video coding techniques to meet the requirement in bandwidth, delay, loss and Quality-of-Service (QoS). In this paper, we present our investigations on inter-subband self-similarity within the wavelet-decomposed video frames using neural networks, and study the performance of applying the spatial network model to all video frames over time. The goal of our proposed method is to restore the highest perceptual quality for video transmitted over a highly congested network. Our contributions in this paper are: (1) A new coding model with neural network based, inter-subband redundancy (ISR) prediction for video coding using wavelet (2) The performance of 1D and 2D ISR prediction, including multiple levels of wavelet decompositions. Our result shows a short-term quality enhancement may be obtained using both 1D and 2D ISR prediction.\"",
        "Document: \"Distributed Rate Allocation in P2P Streaming. In peer-to-peer (P2P) streaming, each peer contributes its upload bandwidth to redistributing the data stream to its downstream peers. How to optimally utilize the upload bandwidth of each peer is an important issue. In this paper, we propose a fully distributed algorithm to optimize the link rate allocation in P2P streaming. We use dual decomposition to separate the optimization problem into multiple sub-problems, which are solved at each individual peer respectively. Through simulations, we demonstrate that the proposed rate allocation scheme can quickly converge, and achieve a higher throughput compared to proportional or equal allocation scheme.\"",
        "Document: \"Concept-based retrieval of art documents. This paper presents our work on the retrieval of art documents for color artistry concepts. First we show that the query-by-example paradigm popularly used in content-based retrieval can support only limited queryability. The paper then proposes a concept-based retrieval engine based on the generative grammar of elecepts methodology. In the latter, the language by which color artistry concepts are communicated in art documents is used to operate the retrieval processes. The concept language is explicated into a lexicon of elecepts and the associated generative grammar. Documents are then indexed with elecept indices, while the generative grammar is used to facilitate the query operation. More extensive color artistry concept queries can then be supported by post-coordination of the elecept indices.\"",
        "Document: \"On-line Signature Verification Using Most Discriminating Features and Fisher Linear Discriminant Analysis (FLD). In this work, we employ a combination of strategies for partitioning and detecting abnormal fluctuations in the horizontal and vertical trajectories of an On-line generated signature profile. Alternative partitions of these spatial trajectories are generated by splitting each of the related angle, velocity and pressure profiles into two regions representing both high and low activity. The overall process can be thought of as one that exploits inter-feature dependencies by decomposing signature trajectories based upon angle, velocity and pressure - information quite characteristic to an individual\u2019s signature. In the verification phase, distances of each partitioned trajectory of a test signature are calculated against a similarly partitioned template trajectory for a known signer. Finally, these distances become inputs to Fisher\u2019s Linear Discriminant Analysis (FLD). Experimental results demonstrate the superiority of our approach in On-line signature verification in comparison with other techniques.\"",
        "Document: \"Statistical Machine Learning vs Deep Learning in Information Fusion: Competition or Collaboration?. Information fusion is the process of coherently and intelligently combining knowledge extracted from different sensors/ modalities, in order to obtain more useful or discriminant information for the purpose of multimedia processing and biometrics, among others. The key to successful information fusion is to intelligently exploit the intrinsic relations between the data of different modalities. Statistical machine learning (SML) has played a major role in developing new information fusion methods, by incorporating prior knowledge and entropy metric, correlation analysis, inherent statistical structures of input data, and nonlinear relations. On the other hand, the recent development of deep learning (DL) draws enormous attention from the machine learning community. DL algorithms possess deep structures, requiring a large amount of data to train the huge number of parameters, an ultra-expensive process. However, the payoff is enormous; unprecedented success in many applications. This paper will first review recent development of both SML and DL in the context of information fusion, then analyze their pros and cons, and compare their performance in a number of application domains. Based on preliminary results, some thoughts will be presented on how SML and DL can work together to bring the study in machine learning to the next level, better serving human needs.\"",
        "1 is \"Algebraic-Method For Manipulation Of Dimensional Relationships In Geometric-Models\", 2 is \"Secure spread spectrum watermarking for multimedia\"",
        "Given above information, for an author who has written the paper with the title \"A Visual Evaluation Framework for In-Home Physical Rehabilitation\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002385": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'An Envisioned Approach for Modeling and Supporting User-Centric Query Activities on Data Warehouses':",
        "Document: \"A framework for enriching Data Warehouse analysis with Question Answering systems. Business Intelligence (BI) applications allow their users to query, understand, and analyze existing data within their organizations in order to acquire useful knowledge, thus making better strategic decisions. The core of BI applications is a Data Warehouse (DW), which integrates several heterogeneous structured data sources in a common repository of data. However, there is a common agreement in that the next generation of BI applications should consider data not only from their internal data sources, but also data from different external sources (e.g. Big Data, blogs, social networks, etc.), where relevant update information from competitors may provide crucial information in order to take the right decisions. This external data is usually obtained through traditional Web search engines, with a significant effort from users in analyzing the returned information and in incorporating this information into the BI application. In this paper, we propose to integrate the DW internal structured data, with the external unstructured data obtained with Question Answering (QA) techniques. The integration is achieved seamlessly through the presentation of the data returned by the DW and the QA systems into dashboards that allow the user to handle both types of data. Moreover, the QA results are stored in a persistent way through a new DW repository in order to facilitate comparison of the obtained results with different questions or even the same question with different dates.\"",
        "Document: \"Contextual ontology module learning from web snippets and past user queries. In this paper, we focus on modularization aspects for query reformulation in ontology-based question answering on the Web. The main objective is to automatically learn ontology modules that cover search terms of the user. Indeed, the main problem is that current approaches of ontology modularization consider only the input existant ontologies, instead of underlying semantics found in texts. This work proposes an approach of contextual ontology module learning covering particular search terms by analyzing past user queries and snippets provided by search engines. The obtained contextual modules will be used for query reformulation. The proposal has been evaluated on the ground of semantic cotopy measure of discovered ontology modules, relevance of search results.\"",
        "Document: \"Good location, terrible food: detecting feature sentiment in user-generated reviews. Abstract A growing corpus of online informal reviews is generated every day by non-experts, on social networks and blogs, about an unlimited range of products and services. Users do not only express holistic opinions, but often focus on specific features of their interest. The automatic understanding of \u201cwhat people think\u201d at the feature level can greatly support decision making, both for consumers and producers. In this paper, we present an approach to feature-level sentiment detection that integrates natural language processing with statistical techniques, in order to extract users\u2019 opinions about specific features of products and services from user-generated reviews. First, we extract domain features, and each review is modelled as a lexical dependency graph. Second, for each review, we estimate the polarity relative to the features by leveraging the syntactic dependencies between the terms. The approach is evaluated against a ground truth consisting of set of user-generated reviews, manually annotated by 39 human subjects and available online, showing its human-like ability to capture feature-level opinions.\"",
        "Document: \"Semantic search using modular ontology learning and case-based reasoning. In this paper, we present a semantic search approach based on Case-based reasoning and modular Ontology learning. A case is defined by a set of similar queries associated with its relevant results. The case base is used for ontology learning and for contextualizing the search process. Modular ontologies are designed to be used for case representation and indexing. Our work aims at improving ontology-based information retrieval by the integration of the traditional information retrieval process, the use of ontology learning (OL) and the Case-Based Reasoning (CBR) process. In fact, the proposed approach uses the CBR with semantic Web language markup -by ontology- for case representation and indexing. Ontology-based similarity is used to retrieve similar cases and to provide end users with alternative documents recommendations. The main contribution of this work is the use of a CBR mechanism and an ontological representation for two purposes: Resource Retrieval from Web and ontology learning and enrichment from cases. This approach builds a knowledge corpus -- represented by ontology modules - resulting from the collaboration actions of users. The experiment shows an improvement in terms of results' precision and ontology learning relevance.\"",
        "Document: \"A framework for semantic recommendations in situational applications. Information overload is an increasingly important concern as users access and generate steadily growing amounts of data. Besides, enterprise applications tend to grow more and more complex which hinders their usability and impacts business users' productivity. Personalization and recommender systems can help address these issues, by predicting items of interest for a given user and enabling a better selection of the proposed information. Recommendations have become increasingly popular in web environments, with sites like Amazon, Netflix or Google News. However, little has been done so far to leverage recommendations in corporate settings. This paper presents our approach to integrate recommender systems in enterprise environments, taking into account their specific constraints. We present an extensible framework enabling heterogeneous recommendations, based on a semantic model of users' situations and interactions. We illustrate this framework with a system suggesting structured queries and visualizations related to an unstructured document.\"",
        "Document: \"Analyses and Fundamental ideas for a Relation Extraction Approach. Relation extraction is a difficult open research problem with important applications in several fields such as knowledge management, web mining, ontology building, intelligent systems, etc. In our research, we focus on extracting relations among the ontological concepts in order to build a domain ontology. In this paper, firstly, we answer some crucial questions related to the text analyses, the word features and the various relation types. Secondly, we use this theoretical analysis and some issues to define the fundamental ideas of our new approach. Our objective is to extract multi-type relations from the text analyses and the existent relations (in the concept hierarchy). Our approach combines a verb centered method, lexical analyses, syntactic and statistic ones. It is based on an exclusive interest to the document style during the statistic process, a rich contextual modelling that strengthens the term co-occurrence selection, a lexical analysis, a use of the existent relations in the concept hierarchy and a stepping between the various extracted relations to facilitate the evaluation made by the domain experts. Thirdly, we present an illustrative example to explain the previous ideas.\"",
        "Document: \"Storytelling In Visual Analytics Tools For Business Intelligence. Stories help us communicate knowledge, share and interpret experiences. In this paper we discuss the use of storytelling in Business Intelligence (BI) analysis. We derive the actual practices in creating and sharing BI stories from in-depth interviews with expert BI analysts (both story \"creators\" and \"readers\"). These interviews revealed the need to extend current BI visual analysis applications to enable storytelling, as well as new requirements related to BI visual storytelling. Based on these requirements we designed and implemented a storytelling prototype tool that is integrated in an analysis tool used by our experts, and allows easy transition from analysis to story creation and sharing. We report experts' recommendations and reactions to the use of the prototype to create stories, as well as novices' reactions to reading these stories.\"",
        "Document: \"PQMPMS: a preference-enabled querying mechanism for personalized mobile search. A key challenge for personalized mobile search is to tailor the answers to the specific user by considering her contextual situation. To adapt the retrieved items to user's context, this paper presents a preference-enabled querying mechanism for personalized mobile search. By exploiting the user's dialogue history, we infer the weighted user preferences and interests. To further compute personalized answers, we aim to continuously collect the ratings given by the user's friends regarding relevant topics from stream-based data sources such as Twitter. An experiment shows that our approach allows to compute the most relevant answers, providing an increased quality of search experience for the user.\"",
        "Document: \"Context-based Hierarchical Clustering for the Ontology Learning. Ontologies provide a common layer which plays a major role in supporting information exchange and sharing. In this paper, we focus on the ontological concept extraction process from HTML documents. In order to improve this process, we propose an unsupervised hierarchical clustering algorithm namely \"Contextual Ontological Concept Extraction\" (COCE) which is an incremental use of the partitioning algorithm Kmeans and is guided by a structural context. Our context exploits the html structure and the location of words to select the semantically closer cooccurrents for each word and to improve the words weighting. Guided by this context definition, we perform an incremental clustering that refines the context of each word clusters to obtain semantically extracted concepts. The COCE algorithm offers the choice between either an automatic execution or a user's interaction. We experiment our algorithm on HTML documents related to the tourism domain. Our results show how the execution of our context-based algorithm which implements an incremental process and a successive refinement of clusters improves their conceptual quality and the relevance of the extracted ontological concepts.\"",
        "Document: \"Structured data-based q&a system using surface patterns. Question Answering (Q&A) systems, unlike other Information Retrieval (IR) systems, aim at providing directly the answer to the user, and not a list of documents in which the correct answer may be found. Our system is based on a data warehouse and provides composite answers made of a dataset and the corresponding chart visualizations. The question translation step is based on a new proposal for surface patterns that incorporate business semantic as well as domain-specific knowldege allowing a better coverage of questions.\"",
        "1 is \"Staying FIT: efficient load shedding techniques for distributed stream processing\", 2 is \"A general process mining framework for correlating, predicting and clustering dynamic behavior based on event logs\"",
        "Given above information, for an author who has written the paper with the title \"An Envisioned Approach for Modeling and Supporting User-Centric Query Activities on Data Warehouses\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002405": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Societally connected multimedia across cultures.':",
        "Document: \"Robust unsupervised feature selection via dual self-representation and manifold regularization. Unsupervised feature selection has become an important and challenging pre-processing step in machine learning and data mining since large amount of unlabelled high dimensional data are often required to be processed. In this paper, we propose an efficient method for robust unsupervised feature selection via dual self-representation and manifold regularization, referred to as DSRMR briefly. On the one hand, a feature self-representation term is used to learn the feature representation coefficient matrix to measure the importance of different feature dimensions. On the other hand, a sample self-representation term is used to automatically learn the sample similarity graph to preserve the local geometrical structure of data which has been verified critical in unsupervised feature selection. By using l2,1-norm to regularize the feature representation residual matrix and representation coefficient matrix, our method is robustness to outliers, and the row sparsity of the feature coefficient matrix induced by l2,1-norm can effectively select representative features. During the optimization process, the feature coefficient matrix and sample similarity graph constrain each other to obtain optimal solution. Experimental results on ten real-world data sets demonstrate that the proposed method can effectively identify important features, outperforming many state-of-the-art unsupervised feature selection methods in terms of clustering accuracy (ACC) and normalized mutual information (NMI).\"",
        "Document: \"Object detection using Non-Redundant Local Binary Patterns. Local Binary Pattern (LBP) as a descriptor, has been successfully used in various object recognition tasks because of its discriminative property and computational simplicity. In this paper a variant of the LBP referred to as Non-Redundant Local Binary Pattern (NRLBP) is introduced and its application for object detection is demonstrated. Compared with the original LBP descriptor, the NRLBP has advantage of providing a more compact description of object's appearance. Furthermore, the NRLBP is more discriminative since it reflects the relative contrast between the background and foreground. The proposed descriptor is employed to encode human's appearance in a human detection task. Experimental results show that the NRLBP is robust and adaptive with changes of the background and foreground and also outperforms the original LBP in detection task.\"",
        "Document: \"Single Image Smoke Detection. Despite the recent advances in smoke detection from video, detection of smoke from single images is still a challenging problem with both practical and theoretical implications. However, there is hardly any reported research on this topic in the literature. This paper addresses this problem by proposing a novel feature to detect smoke in a single image. An image formation model that expresses an image as a linear combination of smoke and non-smoke (background) components is derived based on the atmospheric scattering models. The separation of the smoke and non-smoke components is formulated as convex optimization that solves a sparse representation problem. Using the separated quasi-smoke and quasi-background components, the feature is constructed as a concatenation of the respective sparse coefficients. Extensive experiments were conducted and the results have shown that the proposed feature significantly outperforms the existing features for smoke detection.\"",
        "Document: \"Image Reconstruction from Sparse Projections Using S-Transform. Sparse projections are an effective way to reduce the exposure to radiation during X-ray CT imaging. However, reconstruction of images from sparse projection data is challenging. This paper introduces a new sparse transform, referred to as S-transform, and proposes an accurate image reconstruction method based on the transform. The S-transform effectively converts the ill-posed reconstruction problem into a well-defined one by representing the image using a small set of transform coefficients. An algorithm is proposed that efficiently estimates the S-transform coefficients from the sparse projections, thus allowing the image to be accurately reconstructed using the inverse S-transform. The experimental results on both simulated and real images have consistently shown that, compared to the popular total variation (TV) method, the proposed method achieves comparable results when the projections is sparse, and substantially improves the quality of the reconstructed image when the number of the projections is relatively high. Therefore, the use of the proposed reconstruction algorithm may permit reduction of the radiation exposure without trade-off in imaging performance.\"",
        "Document: \"Semantic action recognition by learning a pose lexicon. \u2022A novel semantic representation, pose lexicon, is proposed for action recognition.\u2022An extended hidden Markov alignment model is developed to learn a pose lexicon.\u2022A semantic action recognition method that is capable of zero-shot recognition is developed upon the lexicon.\u2022The efficacy of the proposed learning and recognition algorithms were evaluated on five datasets using cross-subject, cross-dataset and zero-shot protocols.\"",
        "Document: \"Reconstruction From Limited-Angle Projections Based On Delta - Mu Spectrum Analysis. This paper proposes a sparse representation of an image using discrete delta - u functions. A delta - u function is defined as the product of a Kronecker delta function and a step function. Based on the sparse representation, we have developed a novel and effective method for reconstructing an image from limited-angle projections. The method first estimates the parameters of the sparse representation from the incomplete projection data, and then directly calculates the image to be reconstructed. Experiments have shown that the proposed method can effectively recover the missing data and reconstruct images more accurately than the total-variation (TV) regularized reconstruction method.\"",
        "Document: \"On the Combination of Local Texture and Global Structure for Food Classification. This paper proposes a food image classification method using local textural patterns and their global structure to describe the food image. In this paper, a visual codebook of local textural patterns is created by employing Scale Invariant Feature Transformation (SIFT) interest point detector with the Local Binary Pattern (LBP) feature. In addition to describing the food image using local texture, the global structure of the food object is represented as the spatial distribution of the local textural structures and encoded using shape context. We evaluated the proposed method on the Pittsburgh Fast-Food Image (PFI) dataset. Experimental results showed that the proposed method could obtain better performance than the baseline experiment on the PFI dataset.\"",
        "Document: \"A real-time facial expression recognition system for online games. Multiplayer online games (MOGs) have become increasingly popular because of the opportunity they provide for collaboration, communication, and interaction. However, compared with ordinary human communication, MOG still has several limitations, especially in communication using facial expressions. Although detailed facial animation has already been achieved in a number of MOGs, players have to use text commands to control the expressions of avatars. In this paper, we propose an automatic expression recognition system that can be integrated into an MOG to control the facial expressions of avatars. To meet the specific requirements of such a system, a number of algorithms are studied, improved, and extended. In particular, Viola and Jones face-detection method is extended to detect small-scale key facial components; and fixed facial landmarks are used to reduce the computational load with little performance degradation in the recognition accuracy.\"",
        "Document: \"Pathological Gait Detection of Parkinson's Disease Using Sparse Representation. Parkinson's disease is a progressively degenerative neurological disorder which impacts the control of body movements. While there is no known permanent cure for the disorder, it is possible to monitor the progression and establish management regime that could help the medical team, patients and their family cope with the condition. Gait analysis becomes an attractive quantitative and non-invasive mechanism that can aid early detection and monitoring of the response of patients to the management schedules. In this paper, we model cycles of human gait as a sparsely represented signal using over-complete dictionary. This representation forms the basis of a classification that allows the recognition of symptomatic subjects. Experiments have been conducted using signals of vertical ground reaction force (GRF) from subjects with Parkinson's disease from the publicly available gait database (physionet.org). Our method achieved a classification accuracy of 83% in recognising pathological cases and represents a significant improvement on previously published results that use a selection of the Fourier transform coefficients as features.\"",
        "Document: \"A Spectral and Spatial Approach of Coarse-to-Fine Blurred Image Region Detection. Blur exists in many digital images, it can be mainly categorized into two classes: defocus blur which is caused by optical imaging systems and motion blur which is caused by the relative motion between camera and scene objects. In this letter, we propose a simple yet effective automatic blurred image region detection method. Based on the observation that blur attenuates high-frequency components o...\"",
        "1 is \"Learning shape segmentation using constrained spectral clustering and probabilistic label transfer\", 2 is \"Spatio-Temporal Lstm With Trust Gates For 3d Human Action Recognition\"",
        "Given above information, for an author who has written the paper with the title \"Societally connected multimedia across cultures.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002409": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Context-aware result inference in crowdsourcing.':",
        "Document: \"A location-aware publish/subscribe framework for parameterized spatio-textual subscriptions. With the rapid progress of mobile Internet and the growing popularity of smartphones, location-aware publish/subscribe systems have recently attracted significant attention. Different from traditional content-based publish/subscribe, subscriptions registered by subscribers and messages published by publishers include both spatial information and textual descriptions, and messages should be delivered to relevant subscribers whose subscriptions have high relevancy to the messages. To evaluate the relevancy between spatio-textual messages and subscriptions, we should combine the spatial proximity and textual relevancy. Since subscribers have different preferences - some subscribers prefer messages with high spatial proximity and some subscribers pay more attention to messages with high textual relevancy, it calls for new location-aware publish/subscribe techniques to meet various needs from different subscribers. In this paper, we allow subscribers to parameterize their subscriptions and study the location-aware publish/subscribe problem on parameterized spatio-textual subscriptions. One big challenge is to achieve high performance. To meet this requirement, we propose a filter-verification framework to efficiently deliver messages to relevant subscribers. In the filter step, we devise effective filters to prune large numbers of irreverent results and obtain some candidates. In the verification step, we verify the candidates to generate the answers. We propose three effective filters by integrating prefix filtering and spatial pruning techniques. Experimental results show our method achieves higher performance and better quality than baseline approaches.\"",
        "Document: \"Supporting efficient top-k queries in type-ahead search. Type-ahead search can on-the-fly find answers as a user types in a keyword query. A main challenge in this search paradigm is the high-efficiency requirement that queries must be answered within milliseconds. In this paper we study how to answer top-k queries in this paradigm, i.e., as a user types in a query letter by letter, we want to efficiently find the k best answers. Instead of inventing completely new algorithms from scratch, we study challenges when adopting existing top-k algorithms in the literature that heavily rely on two basic list-access methods: random access and sorted access. We present two algorithms to support random access efficiently. We develop novel techniques to support efficient sorted access using list pruning and materialization. We extend our techniques to support fuzzy type-ahead search which allows minor errors between query keywords and answers. We report our experimental results on several real large data sets to show that the proposed techniques can answer top-k queries efficiently in type-ahead search.\"",
        "Document: \"Crowdsourced Entity Alignment: A Decision Theory Based Approach. Crowdsourcing is a new computation paradigm that utilizes the wisdom of the crowd to solve problems which are difficult for computers (e.g., image annotation and entity alignment). In crowdsourced entity alignment tasks, there are usually large numbers of candidate pairs to be verified by the crowd workers, and each pair will be assigned to multiple workers to achieve high quality. Thus, two fundamental problems are raised: (1) question selection \u2013 what are the most beneficial questions that should be crowdsourced, and (2) question assignment \u2013 which workers should be assigned to answer a selected question? In this paper, we address these two problems by decision theory. Firstly, we define the problems on two budget constraints. The first takes the marginal gain into account, and the second focuses on the limited budget. Then, we formulate the decision-making problems under different budget constraints and build influence diagram to perform result inference. We propose two efficient algorithms to address these two problems. Finally, we conduct extensive experiments to validate the efficiency and effectiveness of our proposed algorithms on both synthetic and real data.\"",
        "Document: \"An Efficient Ride-Sharing Framework for Maximizing Shared Route. Ride-sharing (RS) has great values in saving energy and alleviating traffic pressure. Existing studies can be improved for better efficiency. Therefore, we propose a new ride-sharing model, where each driver has a requirement that if the driver shares a ride with a rider, the shared route percentage (i.e., the ratio of the shared route&#39;s distance to the driver&#39;s total travel distance) exceeds an e...\"",
        "Document: \"A partial-order-based framework for cost-effective crowdsourced entity resolution. Crowdsourced entity resolution has recently attracted significant attentions because it can harness the wisdom of crowd to improve the quality of entity resolution. However, existing techniques either cannot achieve high quality or incur huge monetary costs. To address these problems, we propose a cost-effective crowdsourced entity resolution framework, which significantly reduces the monetary cost while keeping high quality. We first define a partial order on the pairs of records. Then, we select a pair as a question and ask the crowd to check whether the records in the pair refer to the same entity. After getting the answer of this pair, we infer the answers of other pairs based on the partial order. Next, we iteratively select pairs without answers to ask until we get the answers of all pairs. We devise effective algorithms to judiciously select the pairs to ask in order to minimize the number of asked pairs. To further reduce the cost, we propose a grouping technique to group the pairs and we only ask one pair instead of all pairs in each group. We develop error-tolerant techniques to tolerate the errors introduced by the partial order and the crowd. We also study the budget-aware entity resolution, which, given a budget, finds the maximum number of matching pairs within the budget, and propose effective optimization techniques. Experimental results show that our method reduces the cost to 1.25% of existing approaches (or existing approaches take \\(80\\times \\) monetary cost of our method) while not sacrificing the quality.\"",
        "Document: \"TsingNUS: a location-based service system towards live city. We present our system towards live city, called TsingNUS, aiming to provide users with more user-friendly location-aware search experiences. TsingNUS crawls location-based user-generated content from the Web (e.g., Foursquare and Twitter), cleans and integrates them to provide users with rich well-structured data. TsingNUS provides three user-friendly search paradigms: location-aware instant search, location-aware similarity search and direction-aware search. Instant search returns relevant answers instantly as users type in queries letter by letter, which can help users to save typing efforts significantly. Location-aware similarity search enables fuzzy matching between queries and the underlying data, which can tolerate typing errors. The two features boost the search performance and improve the experiences for mobile users who often misspell the keywords due to the limitation of the mobile phone's keyboard. In addition, users have direction-aware search requirements in many applications. For example, a driver on the highway wants to find the nearest gas station or restaurant. She has a search requirement that the answers should be in front of her driving direction. TsingNUS enables direction-aware search to address this problem and allows users to search in specific directions. Moreover, TsingNUS incorporates continuous search to efficiently support continuously moving queries in a client-server system which can reduce the number of queries submitted to the server and communication cost between the client and server. We have implemented and deployed a system which has been commonly used and widely accepted.\"",
        "Document: \"An effective approach for searching closest sentence translations from the web. There are large numbers of well-translated sentence pairs on the Web, which can be used for translating sentences in different languages. It is an interesting problem to search the closest sentence translations from the Web for high-quality translation, which has attracted significant attention recently. However, it is not straightforward to develop an effective approach, as this task heavily depends on the effectiveness of the similarity model which is used to quantify the similarity between two sentences. In this paper, we propose several optimization techniques to address this problem. We devise a phrase-based model to quantify the similarity between two sentences. We judiciously select high-quality phrases from sentences, which can capture the key features of sentences and thus can be used to quantify similarity between sentences. Experimental results show that our approach has performance advantages compared with the state-of-the-art sentence matching methods\"",
        "Document: \"Approximate Query Processing: What Is New And Where To Go?: A Survey On Approximate Query Processing. Online analytical processing (OLAP) is a core functionality in database systems. The performance of OLAP is crucial to make online decisions in many applications. However, it is rather costly to support OLAP on large datasets, especially big data, and the methods that compute exact answers cannot meet the high-performance requirement. To alleviate this problem, approximate query processing (AQP) has been proposed, which aims to find an approximate answer as close as to the exact answer efficiently. Existing AQP techniques can be broadly categorized into two categories. (1) Online aggregation: select samples online and use these samples to answer OLAP queries. (2) Offline synopses generation: generate synopses offline based on a-priori knowledge (e.g., data statistics or query workload) and use these synopses to answer OLAP queries. We discuss the research challenges in AQP and summarize existing techniques to address these challenges. In addition, we review how to use AQP to support other complex data types, e.g., spatial data and trajectory data, and support other applications, e.g., data visualization and data cleaning. We also introduce existing AQP systems and summarize their advantages and limitations. Lastly, we provide research challenges and opportunities of AQP. We believe that the survey can help the partitioners to understand existing AQP techniques and select appropriate methods in their applications.\"",
        "Document: \"Efficient keyword search over data-centric XML documents. We in this paper investigate keyword search over data-centric XML documents. We first present a novel method to divide an XML document into self-integrated subtrees, which are connected subtrees and can capture different structural information of the XML document. We then propose the meaningful self-integrated trees, which contain all the keywords and describe how the keywords are interrelated, to answer keyword search over XML documents. In addition, we introduce the B+-tree index to accelerate the retrieval of those meaningful self-integrated trees. Moreover, to further enhance the performance of keyword search, we present Bloom Filter to improve the efficiency of generating those meaningful self-integrated trees. Finally, we conducted extensive experiments to evaluate the performance of our method, and the experimental results demonstrate that our method achieves high efficiency and outperforms the existing approaches significantly.\"",
        "Document: \"CDB: a crowd-powered database system. AbstractCrowd-powered database systems can leverage the crowd's ability to address machine-hard problems, e.g., data integration. Existing crowdsourcing systems adopt the traditional tree model to select a good query plan. However, the tree model can optimize the I/O cost but cannot optimize the monetary cost, latency and quality, which are three important optimization goals in crowdsourcing. To address this limitation, we demonstrate CDB, a crowd-powered database system. CDB proposes a new graph-based model that adopts a fine-grained tuple-level optimization model which significantly outperforms existing coarse-grained tree-based optimization models. Moreover, CDB provides a unified framework to simultaneously optimize the monetary cost, quality and latency. We have deployed CDB on well-known crowd-sourcing platforms and users can easily use our system to deploy their applications. We will demonstrate how to use CDB to address real-world applications, including web table integration and entity collection.\"",
        "1 is \"WhereNext: a location predictor on trajectory pattern mining\", 2 is \"Effective Phrase Prediction\"",
        "Given above information, for an author who has written the paper with the title \"Context-aware result inference in crowdsourcing.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002430": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Coalescence of XML-based really simple syndication (RSS) aggregator for blogosphere':",
        "Document: \"An Overview of Media Streams Caching in Peer-to-Peer Systems. Nowadays, the idea of media contents streaming through the Internet has become a very important issue. On the other hand, full caching for media objects is not a practical solution and leads to consume the cache storage in keeping few media objects because of its limited capacity. Furthermore, repeated traffic which is being sent to clients wastes the network bandwidth. Thus, utilizing the bandwid...\"",
        "Document: \"A survey of methods for maintaining mobile cache consistency. Maintaining cache consistency in mobile computing is challenging due to the limitations in the mobile environment such as narrow bandwidth, limited capabilities of mobile devices, disconnectivity, and user mobility. In this paper we survey the different methods used for maintaining mobile cache consistency in client-server architecture. Moreover, we categorize these methods based on the inheritance problems of mobile environment that they are dealing with, and present the strengths and the weaknesses of these methods. We conclude from this survey that, till now there is no optimum scheme that maintains the consistency of the mobile cache without degrading in caching system functionality.\"",
        "Document: \"The Effect of Sub Communities in a Community-Based Peer-to-Peer Model Based on Social Networks. The objective of this paper is proposing a general Peer-to-Peer model based-on social network to illustrate the effect of sub communities in the system. The whole system is divided into several communities based on the contents of the peers and each one is divided into several sub communities. A computer based model is created and social network parameters are calculated in order to show the efficiency of the model. The result confirms that a large community with many highly connected nodes can be substituted with many sub communities with normal nodes.\"",
        "Document: \"Ranking Semantic Similarity Association in Semantic Web. Discovering and ranking complex relationships in the semantic web is an important building block of semantic search applications. Although semantic web technologies define relations between objects but there are some complex (hidden) relationships that are valuable in different applications. Currently, users need to discover the relations between objects and find the level of semantic similarity between them. (I.e. find two similar papers). This paper presents a new approach for ranking semantic similarity association in semantic web document, based on semantic association concept.\"",
        "Document: \"A new method for job scheduling in two-levels hierarchical systems. The use of parallel and distributed systems has become very common in the last decade. Dividing data is one of the challenges in these types of systems. Divisible load theory (DLT) is one of the proposed methods for scheduling data distribution in parallel or distributed systems. Many researches have been done in this field, but scheduling a multi-installment heterogeneous system with two-level hierarchical topology in which communication mode is blocking has not been addressed. In this paper, we find the proper size of task for each sub tree. Finally, in the experiments section, we show that the proposed methods work correctly and give us the best scheduling.\"",
        "Document: \"New Optimal Load Allocation for Scheduling Divisible Data Grid Applications. In many data grid applications, data can be decomposed into multiple independent sub-datasets and distributed for parallel execution and analysis. This property has been successfully employed by using Divisible Load Theory (DLT), which has been proved as a powerful tool for modeling divisible load problems in data-intensive grid. There are some scheduling models have been studied but no optimal solution has been reached due to the heterogeneity of the grids. This paper proposes a new model called Iterative DLT (IDLT) for scheduling divisible data grid applications. Recursive numerical closed form solutions are derived to find the optimal workload assigned to the processing nodes. Experimental results show that the proposed IDLT model obtains better solution than other models (almost optimal) in terms of makespan .\"",
        "Document: \"Optimal workload allocation model for scheduling divisible data grid applications. In many data grid applications, data can be decomposed into multiple independent sub-datasets and distributed for parallel execution and analysis. This property has been successfully employed using Divisible Load Theory (DLT), which has been proved a powerful tool for modeling divisible load problems in data-intensive grids. There are some scheduling models that have been studied but no optimal solution has been reached due to the heterogeneity of the grids. This paper proposes a new model called the Iterative DLT (IDLT) for scheduling divisible data grid applications. Recursive numerical closed form solutions are derived to find the optimal workload assigned to the processing nodes. Experimental results show that the proposed IDLT model leads to a better solution than other models (almost optimal) in terms of makespan.\"",
        "Document: \"A Model for Checking the Integrity Constraints of Mobile Databases. In this paper we propose Three-Level (3-L) model, wherein the process of constraint checking to maintain the consistent state of mobile databases is realized at three different levels. Sufficient and complete tests proposed in the previous works together with the idea of caching relevant data items for checking the integrity constraints are adopted. This has improved the checking mechanism by preventing delays during the process of checking constraints and performing the update. Also, the 3-L model reduces the amount of data accessed given that much of the tasks are performed at the mobile host, and hence speeds up the checking process.\"",
        "Document: \"A Compact Bit String Accessibility Map for Secure XML Query Processing. One of the challenging issues related to specifying a fine-grained access control on the XML data is how to implement the accessibility map in a compact format with minimum affect on the XML query processing. In this paper, we propose a Compact Bit String Accessibility Map (CBSAM) to implement the accessibility map in a compact format. In order to achieve a secure and efficient XML query processing, the CBSAM is integrated with the region number labeling scheme. The experimental results illustrate that the CBSAM compresses the accessibility map with minimum affect on the XML query processing when the access locality among the XML nodes is high. (C) 2011 Published by Elsevier Ltd. Selection and/or peer-review under responsibility of [name organizer]\"",
        "Document: \"SICSDD: Techniques and Implementation. This paper presents the constituent techniques and the implementation of a semantic integrity subsystem for a distributed database (SICSDD). The subsystem provides complete functionality and an efficient strategy for constraint enforcement. Complete functionality is attained through a modular and extensible architecture in which several techniques are incorporated. These are either modifications/extensions of techniques developed by other researchers or new techniques proposed by us. The integration of these techniques is necessary to achieve efficient constraint enforcement, particularly in a distributed database.\"",
        "1 is \"New algorithms and applications of cyclic reference counting\", 2 is \"Fragile sensor fingerprint camera identification\"",
        "Given above information, for an author who has written the paper with the title \"Coalescence of XML-based really simple syndication (RSS) aggregator for blogosphere\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002447": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Comparative Handover Performance Analysis Of Ipv6 Mobility Management Protocols':",
        "Document: \"Preventing Out-of-Sequence Packets on the Route Optimization Procedure in Proxy Mobile IPv6. Proxy Mobile IPv6 is one of network-based mobility support protocols which does not require a mobile host to be involved in mobility support signaling. Even though Proxy Mobile IPv6 has many advantages than client-based mobility support protocols such as Mobile IPv6, unfortunately, it may cause an out-of-sequence problem during its route optimization procedure. Thus, in this paper, we develop an estimation function for an out-of-sequence time period and reveal the out-of-sequence problem caused by the route optimization procedure. Then we propose a scheme which minimizes the arrival of out-of-sequence packets to the mobile host and analyze the impact of out-of-sequence packets. The performance evaluation results demonstrate that the traffic rate and out-of-sequence time period mainly have an effect on the out-of-sequence problem.\"",
        "Document: \"Eliminating Duplicated Paths to Reduce Computational Cost of Rule Generation by Using SDN. Nowadays, SDN(Software Defined Network) has become a powerful technology that has abilities to program network flow paths into flow-table in switches for network control. In SDN architecture, data plane and control plane are decoupled, and then by manipulating flow-table on control plane network control is available. The function of SDN is used to supplement limitation or difficulties of other technologies. For example cloud computing system has difficulties to use existing security devices due to its dynamic and virtualization environment. For the solution, Cloudwatcher framework was designed. In Cloudwatcher, there are four algorithms to take network traffic to certain security node in cloud computing environment by using SDN. However when Cloudwatcher generates paths from source node to destination and security nodes, there is possibility of computational cost according to topology. In this paper, we propose an approach to reduce to the rule generation computation cost by excluding duplicated paths during rule generation time. Our evaluation results show that computation cost can be reduced by considerable amount according to topology environment.\"",
        "Document: \"Qualitative method-based the effective risk mitigation method in the risk management. In the paper, we presented the method of safeguard selection for the effective risk mitigation using a qualitative method. We provided the suitable selection method of safeguard\u2019s method/technique according to risk type, and performed cost-benefit analysis. In the selection of the safeguard method, we recommended the suitable method among risk avoidance, transference, prevention, threats reduction and impacts reduction, etc. according to risk type. After selecting the safeguard method, we chose the safeguard technique considering organization\u2019s IT system capability such as IT system and network structure, functionality, exclusiveness and achievability of safeguard, etc. And then, we applied the safeguard technique to the safeguard method for implement effective security technology. We performed cost-benefit analysis with candidate safeguards, considering organization\u2019s security budget. As performing this procedure, we can decide optimal safeguards with methods and techniques against risk\u2019s types before implementing safeguards. We also can prevent redundant works and security budgets waste as analyzing the efficiency of existing safeguard. Lastly, we reflected the organization\u2019s CEO opinions to require special safeguards for the specific information system related to their core business.\"",
        "Document: \"Experimental Performance Evaluation of Mobile IPv6 Handovers over Wireless LAN. Over the past several years there has been increasing the number of mobile devices. Mobile IPv6 enables mobile devices to communicate with each other while moving and has been implemented by several organizations. Though Mobile IPv6 implementations has been confirmed, the performance evaluation is one of the important next step to expect deployment in real environments. In this paper, we present experimental results on the performance of Mobile IPv6 handovers with the test parameters such as RTT, TCP and UDP throughput.\"",
        "Document: \"Adaptive bandwidth control using fuzzy inference in policy-based network management. This paper presents the fuzzy logic-based control structure for incoming traffic from an arbitrary node to provide admission control in a policy-based IP network management structure. The proposed control structure uses a scheme for deciding the network resource allocation depending on the requirement of predefined-policies and network states. The proposed scheme enhances policy adapting methods of existing binary methods, and can use resource of network more effectively to provide adaptive admission control, according to the unpredictable network states for predefined QoS policies. Simulation results show that the proposed controller improves the ratio of packet rejection up to 17%, because it performs the soft adaptation based on the network states instead of accept/reject actions in a conventional CAC(Connection Admission Controller).\"",
        "Document: \"An Energy Efficient Concentric Clustering Scheme in Wireless Sensor Networks. In wireless sensor networks, sensor nodes have limited power so the lifetime of the networks is very important design factor. The chain-based routing scheme is one of the famous routing schemes that are proposed for reducing energy dissipation of communication in the networks. The chain-based routing schemes construct the chains for routing path. The chain-based routing schemes can reduce energy dissipation of transmitting data to base station by using the concept of multi-hop routing. PEGASIS, concentric-clustering routing scheme, and extended concentric-clustering routing scheme belong to the chain-based routing schemes. PEGASIS and concentric-clustering routing scheme do not balance energy dissipation among the sensor nodes. Especially the extended concentric-clustering routing scheme can distribute energy dissipation among each cluster. The extended concentric-clustering routing scheme fixes the number of sensor nodes in each cluster for balancing of energy dissipation among each cluster. Therefore the extended concentric-clustering routing scheme is able to make lifetime of the networks longer than the other chain-based routing schemes. However the extended concentric-clustering routing scheme still has limitation. This scheme cannot distribute energy dissipation. This difference is occurred by difference among transmitting data size of each cluster head. Because of this difference, energy dissipations are unbalanced and lifetime of the sensor networks is reduced. To overcome these problems, we propose an energy efficient concentric-clustering scheme. In our scheme, the number of sensor nodes is differently deployed to balance energy dissipation. As the simulation results, our scheme can distribute energy dissipation among the whole networks better than the extended concentric-clustering routing scheme.\"",
        "Document: \"A Novel Inter-LMD Handoff Mechanism for Network-Based Localized Mobility Management. Network-based Localized Mobility Management (NetLMM) is an outstanding candidate solution for the mobility management controlled by the network. In NetLMM, mobile nodes (MNs) can be provided mobility services without any installation of mobility-support stack. However, there is a restriction that the MN is able to have the mobility only within a single localized mobility domain (LMD). In this paper, we propose a novel Inter-LMD handoff mechanism in order to eliminate the shortcoming of the current NetLMM protocol. The proposed Inter-LMD handoff mechanism enables that the MN hands off across LMDs, even if the MN does not have a functionality of Mobile IPv6 (MIPv6). According to the performance evaluation, the proposed Inter-LMD handoff mechanism has approximately 5.8% more overhead than the current Inter-LMD handoff of MIPv6-capable devices, while the current NetLMM protocol does not support the handoff of MIPv6-incapable devices.\"",
        "Document: \"Secure Fast Handover Scheme of Proxy Mobile IPv6. As wireless technologies have grown, many people want to use wireless networks during movements. Accordingly, Mobile IPv6 was developed by the Internet Engineering Task Force to support the mobility service. However, Mobile IPv6 cannot satisfy requirements of real-time applications such as video streaming service and VoIP service due to its high handover latency. To address this problem, Proxy Mobile IPv6 has been introduced by the Internet Engineering Task Force. Then, optimization issues including Fast Handover are under the proposal phase. In this paper, we propose a new mechanism for re-authentication during the Mobile Node moves from its attachment point to a new attachment point. The proposed mechanism uses the EAP Re-authentication Protocol. We discuss the enhancement of the performance as well as the effect of the environmental parameters.\"",
        "Document: \"Performance Analysis of Route Optimization on Proxy Mobile IPv6. In the Mobile IPv6 (MIPv6), a mobile node (MN) directly manages its signaling related mobility because the MIPv6 is a host based mobility protocol. On the other hands, the Proxy Mobile IPv6 (PMIPv6) provides a network based mobility in where special network entities manage all gnaling related mobility for supporting mobility service of MN. The Route Optimization (RO) mechanism for PMIPv6 is proposed while the specification of PMIPv6 is still focusing on basic operations. The RO mechanism establishes the enhanced communication path between MNs so that it can reduce transmission delay and network burden. In this paper, we analyze the performance of RO mechanism compared with the basic PMIPv6. The presented results confirm that the RO mechanism provide the improved performance during the MN communicates with the CN, due to the established communication path.\"",
        "Document: \"Active security management based on secure zone cooperation. Due to its open protocol, the Internet has revolutionized computer networks, but this revolution brings new risks and threats. The best way to protect computer networks is to prevent attackers from intruding, using fast automated procedures. However, the current state of protection is insufficient, because providing for all attacks or preventing unknown types of attack is almost impossible, and the methods used are manual. We solve this problem by using active security management, based on sharing information about attacks and cooperation between organizations. Secure Zone Cooperation, a framework that establishes mutual collaboration and cooperation between trusted zones, can protect systems and networks from potential attacks. This framework can predict and respond to attacks by exchanging security information and cooperating with each zone. It is a dynamic, powerful security architecture that rapidly enables security policy to be updated and response modules to be deployed.\"",
        "1 is \"Edge Pricing of Multicommodity Networks for Heterogeneous Selfish Users\", 2 is \"Risk assessment for large heterogeneous systems\"",
        "Given above information, for an author who has written the paper with the title \"Comparative Handover Performance Analysis Of Ipv6 Mobility Management Protocols\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002479": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A formal approach supporting the comparative predictive assessment of the interruption-tolerance of interactive systems':",
        "Document: \"Interacting with Autonomous Vehicles: Learning from other Domains. The rise of evermore autonomy in vehicles and the expected introduction of self-driving cars have led to a focus on human interactions with such systems from an HCI perspective over the last years. Automotive User Interface researchers have been investigating issues such as transition control procedures, shared control, (over)trust, and overall user experience in automated vehicles. Now, it is time to open the research field of automated driving to other CHI research fields, such as Human-Robot-Interaction (HRI), aeronautics and space, conversational agents, or smart devices. These communities have been dealing with the interplay between humans and automated systems for more than 30 years. In this workshop, we aim to provide a forum to discuss what can be learnt from other domains for the design of autonomous vehicles. Interaction design problems that occur in these domains, such as transition control procedures, how to build trust in the system, and ethics will be discussed.\n\n\"",
        "Document: \"A multi-formalism approach for model-based dynamic distribution of user interfaces of critical interactive systems. Evolution in the context of use requires evolutions in the user interfaces even when they are currently used by operators. User Centered Development promotes reactive answers to this kind of evolutions either by software evolutions through iterative development approaches or at runtime by providing additional information to the operators such as contextual help for instance. This paper proposes a model-based approach to support proactive management of context of use evolutions. By proactive management we mean mechanisms in place to plan and implement evolutions and adaptations of the entire user interface (including behaviour) in a generic way. The approach proposed handles both concentration and distribution of user interfaces requiring both fusion of information into a single UI or fission of information into several ones. This generic model-based approach is exemplified on a safety critical system from space domain. It presents how the new user interfaces can be generated at runtime to provide a new user interface gathering in a single place all the information required to perform the task. These user interfaces have to be generated at runtime as new procedures (i.e. sequences of operations to be executed in a semi-autonomous way) can be defined by operators at any time in order to react to adverse events and to keep the space system in operation. Such contextual, activity-related user interfaces complement the original user interfaces designed for operating the command and control system. The resulting user interface thus corresponds to a distribution of user interfaces in a focus+context way improving usability by increasing both efficiency and effectiveness.\"",
        "Document: \"Making the field of computing more inclusive. More accessible conferences, digital resources, and ACM SIGs will lead to greater participation by more people with disabilities.\"",
        "Document: \"Time modelling in Petri nets for the design of interaction active. This paper addresses the problem of the representation of time in interactive software models. We aim at providing solutions allowing the software designers to efficiently use time modelling during the design process, and to check that the software being built actually corresponds to the temporal requirements. The modelling approach makes a precise distinction between qualitative and quantitative time modelling. The qualitative aspects are presented according to basic constructs of Petri nets while quantitative aspects are introduced according to several extensions to the basic constructs of Petri nets. Each of those constructs is presented on a simple example.\"",
        "Document: \"Formal Support for the Engineering of CORBA-based Distributed Object Systems. We present a CASE tool based on an object-oriented Petri nets dialect called Cooperative Objects, dedicated to the design of CORBA systems. The notation is used for the formal behavioral specification of objects, and its associated tool puts an emphasis on supporting the design life cycle of CORBA systems. The tool offers enhanced interactivity to present the results derived from the capabilities of verification, validation and distributed interpretation provided by Cooperative Objects.\"",
        "Document: \"Turbulent Touch: Touchscreen Input for Cockpit Flight Displays. Touchscreen input in commercial aircraft cockpits offers potential advantages, including ease of use, modifiability, and reduced weight. However, tolerance to turbulence is a challenge for their deployment. To better understand the impact of turbulence on cockpit input methods we conducted a comparative study of user performance with three input methods -- touch, trackball (as currently used in commercial aircraft), and a touchscreen stencil overlay designed to assist finger stabilization. These input methods were compared across a variety of interactive tasks and at three levels of simulated turbulence (none, low, and high). Results showed that performance degrades and subjective workload increases as vibration increases. Touch-based interaction was faster than the trackball when precision requirements were low (at all vibrations), but it was slower and less accurate for more precise pointing, particularly at high vibrations. The stencil did not improve touch selection times, although it did reduce errors on small targets at high vibrations, but only when finger lift-off errors had been eliminated by a timeout. Our work provides new information on the types of tasks affected by turbulence and the input mechanisms that perform best under different levels of vibration.\"",
        "Document: \"A tool-supported design framework for safety critical interactive systems. This paper presents a design framework for safety critical interactive systems, based on a formal description technique called the ICO (Interactive Cooperative Object) formalism. ICO allows for describing, in a formal way, all the components of highly interactive (also called post-WIMP) applications. The framework is supported by a case tool called PetShop allowing for editing, verifying and execu...\"",
        "Document: \"Modeling a groupware editing tool with cooperative objects. This paper contains a solution to the case study proposed for the 2nd edition of the OO-MC workshop. In this paper, we merely recall the main features of the Cooperative Objects formalism, which is an object-oriented language, based on high-level Petri nets. We then include a Cooperative Object model describing the groupware editing tool described in the case study.\"",
        "Document: \"Model-based training: an approach supporting operability of critical interactive systems. Operation of safety critical systems requires qualified operators that have detailed knowledge about the system they are using and how it should be used. Instructional Design and Technology intends to analyze, design, implement, evaluate, maintain and manage training programs. Among the many methods and processes that are currently in use, the first one to be widely exploited was Instructional Systems Development (ISD) which has been further developed in many ramifications and is part of the Systematic Approach to Training (SAT) instructional design family. One of the key features of these processes (at least when they are refined) is the importance of Instructional Task Analysis, particularly the decomposition of a job in its tasks and sub-tasks in order to decide what knowledge and skills must be acquired by the trainee. This paper proposes to leverage this systematic approach using model-based approaches currently used for interactive systems engineering in order to design such training programs and thus to improve human reliability. The paper explains how task and interactive systems modeling can be bound to job analysis to ensure that each trainee meets the performance goals required. Such training ensures proper learning at the three levels of the Skills Rule Knowledge (SRK) levels of Rasmussen's. In the case study we describe the process for building a training program for operators of satellite ground segments, which is based on and compatible with the Ground Systems and Operations ECSS standard. Then, we propose to enhance this process with a) the application of a Systematic Approach to Training and b) the use of both a System Model and an Operator Task Model. The system model is build using the ICO notation while operators' goals and tasks are described using HAMSTERS notation.\"",
        "Document: \"Synergistic modelling of tasks, users and systems using formal specification techniques. This paper aims at clarifying the articulation between the task models and system models encountered in CHI design practices. We demonstrate how the use of a formal task model may enhance the design of interactive systems, by providing quantitative results on which designers may base their decisions. We also demonstrate that it is possible to describe both task and system models within the same fo...\"",
        "1 is \"Communication chains and multitasking\", 2 is \"On Adoption of Social Computing in the Engineering Community\"",
        "Given above information, for an author who has written the paper with the title \"A formal approach supporting the comparative predictive assessment of the interruption-tolerance of interactive systems\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002595": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Operational semantics for agents: the grey-box modeling approach':",
        "Document: \"Outline of a Generalization of Kinetic Theory to Study Opinion Dynamics. This paper presents an analytic framework to study the dynamics of the opinion in multi-agent systems. In the proposed framework, each agent is associated with an attribute which represents its opinion, and the opinion of an agent changes because of interactions with other agents, without supervised coordination. Each interaction involves only two agents, and it corresponds to an exchange of messages. The framework assumes that time is modeled as a sequence of discrete steps, which do not necessarily have the same duration, and that at each step two random agents interact. Various sociological phenomena can be incorporated in the proposed framework, and the framework allows studying macroscopic properties of a system starting from microscopic models of such phenomena, obtaining analytic results. In detail, the proposed framework is inspired by the kinetic theory of gas mixtures, which relies on the use of balance equations that can be properly adopted to study opinion dynamics in a multi-agent system. After a short introduction on the kinetic theory of gas mixtures, this paper shows how the main ideas behind it can be generalized to study the dynamics of the opinion in multi-agent systems starting from a detailed description of microscopic interactions among agents.\"",
        "Document: \"A FIPA Compliant Goal Delegation Protocol. This paper presents an interaction protocol, built on top of FIPA ACL, allowing an agent to delegate a goal to another agent, in the form of a proposition that the delegating agent intends its delegate to bring about. The proposed protocol addresses the concrete needs of a service that is to be deployed within the AgentCities network, but also helps to highlight some issues that are related to the FIPA ACL itself and its usage to build more complex agent interaction blocks.\"",
        "Document: \"An Introduction to the JADEL Programming Language. This paper summarizes the current state of development of JADEL, a novel programming language that eases the implementation of agents and multi-agent systems. First, the introduction of a novel agent programming language is motivated and the approach that was used to design JADEL is presented. Then, the characteristic features of JADEL are described by means of a didactic example. The paper is concluded with a short discussion about current and planned developments of JADEL.\"",
        "Document: \"A Portal for Ubiquitous Collaboration. This paper presents a software framework, called Collaborator, to provide a shared work-space supporting the activities of virtual teams. This sys- tem exploits seamless integration of standar d Web technologies with agent technologies, enhancing the classic Web communication mechanisms to su p- port synchronous sharing of applic ations, and its use through emerging tech- nologies such as: third generation of mobile networks and terminals, and new generation of home appliances. The system presented in the paper is the main result of an on-going European research project Collaborator (IST-2000-30045) that aims at specifying and developing a software distributed environment to support efficient synchronous collaborative work between virtual teams, and experiment such an environment in the construction and telecommunication working sectors\"",
        "Document: \"Agent-based Social Networks for Enterprise Collaboration. This paper discusses the relationship between agent technology and the recent wave of online social networks in the scope of the notable scenario of enterprise collaboration. In the brief introduction on the topic, we provide a coherent view of online social networks and social network systems. Then, we emphasize some features that can be considered as notable flaws that make everyday social networks and systems unusable to facilitate enterprise collaboration. Finally, we show how agent technology can address some of the mentioned flaws. The paper terminates with some concluding remarks and with an outline of future research directions.\"",
        "Document: \"Opinion dynamics in multi-agent systems: selected analytic models and verifying simulations. In this paper opinion dynamics in multi-agent systems is investigated analytically using a kinetic approach. Interactions among agents are interpreted as collisions among molecules in gases and opinion dynamics is described according to the Boltzmann equation. Starting from a microscopic description of single interactions, global properties of the opinion distribution are derived analytically. The proposed analytic model is general enough to allow reproducing features of real societies of agents, such as positive and negative influences and bounded confidence, which are typically used to study opinion distribution models. Analytic results relative to emergent and global characteristics of considered multi-agent systems are verified by simulations obtained via direct implementation of the proposed microscopic interactions rules. Simulations confirm analytic results.\"",
        "Document: \"Operational semantics for agents: the grey-box modeling approach. No abstract available.\"",
        "Document: \"Core Features of an Agent-Oriented Domain-Specific Language for JADE Agents. This paper presents the core features of JADEL, an agent-oriented domain-specific programming language for the construction of JADE agents, behaviours and ontologies. The work on JADEL originates from the need to assist programmers by means of tools that reduce the complexity and speed up the construction of a JADE agents and multi-agent systems. The features of JADEL discussed in this paper include abstractions for the main entities of JADE-agents, behaviours and ontologies-and they also encompass the features needed for the construction of domain-specific tasks, thus enriching JADE APIs with novel and simple notations.\"",
        "Document: \"Hyper-arc consistency of polynomial constraints over finite domains using the modified Bernstein form. This paper describes an algorithm to enforce hyper-arc consistency of polynomial constraints defined over finite domains. First, the paper describes the language of so called polynomial constraints over finite domains, and it introduces a canonical form for such constraints. Then, the canonical form is used to transform the problem of testing the satisfiability of a constraint in a box into the problem of studying the sign of a related polynomial function in the same box, a problem which is effectively solved by using the modified Bernstein form of polynomials. The modified Bernstein form of polynomials is briefly discussed, and the proposed hyper-arc consistency algorithm is finally detailed. The proposed algorithm is a subdivision procedure which, starting from an initial approximation of the domains of variables, removes values from domains to enforce hyper-arc consistency.\"",
        "Document: \"Secure, Trusted and Privacy-aware Interactions in Large-Scale Multiagent Systems. One of the inherent problems of large-scale, open multiagent systems is the lack of mechanisms and tools to guar- antee legally valid interactions. Agents are supposed to perform crucial tasks autonomously and on behalf of humans; however, (i) they are not legal persons on their own, and (ii) of a full legal corpus for the virtual world and its inhabitants is yet to come. Therefore, the ultimate responsible for the actions of an agent is its developer. In this paper we address an innovative model of interaction between agents that leads to an increase of the level of security and trust in privacy-aware, interaction-intens ive multiagent systems. In particular, after a brief introduction, we focus in Section II on some common problems related to trust and security in real-world, liable interactions. In Section III, we address these problems and outline some abstractions that we use to guarantee a sound level of security and privacy-awareness in interactions with third-party (possibly unknown) agents, whether human or not. Then, in Section IV we describe the design of an API that we implemented to provide developers with a general- purpose, reusable means to realize secure, trusted and privacy- aware multiagent systems. To conclude, in Section V we briefly discuss our model and outline directions of future development.\"",
        "1 is \"Coordination Artifacts: Environment-Based Coordination for Intelligent Agents\", 2 is \"General decidability theorems for infinite-state systems\"",
        "Given above information, for an author who has written the paper with the title \"Operational semantics for agents: the grey-box modeling approach\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002649": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'ReCBuLC: reproducing concurrency bugs using local clocks':",
        "Document: \"Hadoop+: Modeling and Evaluating the Heterogeneity for MapReduce Applications in Heterogeneous Clusters. Despite the widespread adoption of heterogeneous clusters in modern data centers, modeling heterogeneity is still a big challenge, especially for large-scale MapReduce applications. In a CPU/GPU hybrid heterogeneous cluster, allocating more computing resources to a MapReduce application does not always mean better performance, since simultaneously running CPU and GPU tasks will contend for shared resources. This paper proposes a heterogeneity model to predict the shared resource contention between the simultaneously running tasks of a MapReduce application when heterogeneous computing resources (e.g. CPUs and GPUs) are allocated. To support the approach, we present a heterogeneous MapReduce framework, Hadoop+, which enables CPUs and GPUs to process big data coordinately, and leverages the heterogeneity model to assist users in selecting the computing resources for different purposes. Our experimental results show three benefits. First, Hadoop+ exploits GPU capability, and achieves 1.4x to 16.1x speedups over Hadoop for 5 real applications when running individually. Second, the heterogeneity model can be used to allocate GPUs among multiple simultaneously running MapReduce applications, bringing up to 36.9% (17.6% in average) speedup when multiple applications are running simultaneously. Third, the model is verified to be able to select the optimal or most cost-effective resource consumption.\"",
        "Document: \"An Accelerator for High Efficient Vision Processing. In recent years, neural network accelerators have been shown to achieve both high energy efficiency and high performance for a broad application scope within the important category of recognition and mining applications. Still, both the energy efficiency and performance of such accelerators remain limited by memory accesses. In this paper, we focus on image applications, arguably the most important category among recognition and mining applications. The neural networks which are state-of-the-art for these applications are convolutional neural networks (CNNs), and they have an important property: weights are shared among many neurons, considerably reducing the neural network memory footprint. This property allows to entirely map a CNN within an SRAM, eliminating all DRAM accesses for weights. By further hoisting this accelerator next to the image sensor, it is possible to eliminate all remaining DRAM accesses, i.e., for inputs and outputs. In this paper, we propose such a CNN accelerator, placed next to a CMOS or CCD sensor. The absence of DRAM accesses combined with a careful exploitation of the specific data access patterns within CNNs allows us to design an accelerator which is highly energy-efficient. We present a single-core implementation down to the layout at 65 nm, with a modest footprint of 5.94mm $^{\\\\boldsymbol {2}}$ and consuming only 336mW, but still about $\\\\boldsymbol {30\\\\times }$ faster than high-end GPUs. For visual processing with higher resolution and frame-rate requirements, we further present a multicore implementation with elevated performance.\"",
        "Document: \"Software-hardware cooperative DRAM bank partitioning for chip multiprocessors. DRAM row buffer conflicts can increase the memory access latency significantly for single-threaded applications. In a chip multiprocessor system, multiple applications competing for DRAM will suffer additional row buffer conflicts due to interthread interference. This paper presents a new hardware and software cooperative DRAM bank partitioning method that combines page coloring and XOR cache mapping to evaluate the benefit potential of reducing interthread interference. Using SPECfp2000 as our benchmarks, our simulation results show that our scheme can boost the performance of the most benchmark combinations tested, with the speedups of up to 13%, 14% and 8.06% observed for two cores (with 16 banks), two cores (with 32 banks) and four cores (with 32 banks).\"",
        "Document: \"Exploiting the input sparsity to accelerate deep neural networks - poster. Efficient inference of deep learning models are challenging and of great value in both academic and industrial community. In this paper, we focus on exploiting the sparsity in input data to improve the performance of deep learning models. We propose an end-to-end optimization pipeline to generate programs for the inference with sparse input. The optimization pipeline contains both domain-specific and general optimization techniques and is capable of generating efficient code without relying on the off-the-shelf libraries. Evaluations show that we achieve significant speedups over the state-of-the-art frameworks and libraries on a real-world application, e.g., 9.8\u00d7 over TensorFlow and 3.6\u00d7 over Intel MKL on the detection in autonomous driving.\n\n\"",
        "Document: \"WiseThrottling: a new asynchronous task scheduler for mitigating I/O bottleneck in large-scale datacenter servers. Datacenter servers are stepping into an era marked by powerful multi-/many-core processors. Severe problems such as I/O contentions in those large-scale platforms pose an unprecedented challenge. Prior studies primarily considered I/O bandwidth as a major performance bottleneck. However, our work reveals that in many cases the fundamental cause of I/O contentions is the inefficiency of OS schedulers. Particularly, the modern system is not aware of this fact and thus suffers from poor I/O performance, especially for datacenter servers. Based on our findings, we propose a new software-based scheduling approach, WiseThrottling, to reduce I/O contention. WiseThrottling performs asynchronous and self-adjustment scheduling for concurrent tasks. We evaluate our approach across a wide range of C/OpenMP/MapReduce workloads on a 64-core server in Dawning Cluster datacenter. The experimental results exhibit that WiseThrottling is effective for reducing the I/O bottleneck and it can improve the overall system performance by up to 207 %.\"",
        "Document: \"Automatic Library Generation for BLAS3 on GPUs. High-performance libraries, the performance-critical building blocks for high-level applications, will assume greater importance on modern processors as they become more complex and diverse. However, automatic library generators are still immature, forcing library developers to manually tune library to meet their performance objectives. We are developing a new script-controlled compilation framework to help domain experts reduce much of the tedious and error-prone nature of manual tuning, by enabling them to leverage their expertise and reuse past optimization experiences. We focus on demonstrating improved performance and productivity obtained through using our framework to tune BLAS3 routines on three GPU platforms: up to 5.4x speedups over the CUBLAS achieved on NVIDIA GeForce 9800, 2.8x on GTX285, and 3.4x on Fermi Tesla C2050. Our results highlight the potential benefits of exploiting domain expertise and the relations between different routines (in terms of their algorithms and data structures).\"",
        "Document: \"Level by level: making flow- and context-sensitive pointer analysis scalable for millions of lines of code. We present a practical and scalable method for flow- and context-sensitive (FSCS) pointer analysis for C programs. Our method analyzes the pointers in a program level by level in terms of their points-to levels, allowing the points-to relations of the pointers at a particular level to be discovered based on the points-to relations of the pointers at this level and higher levels. This level-by-level strategy can enhance the scalability of the FSCS pointer analysis in two fundamental ways, by enabling (1) fast and accurate flow-sensitive analysis on full sparse SSA form using a flow-insensitive algorithm and (2) fast and accurate context-sensitive analysis using a full transfer function and a meet function for each procedure. Our level-by-level algorithm, LevPA, gives rises to (1) a precise and compact SSA representation for subsequent program analysis and optimization tasks and (2) a flow- and context-sensitive MAY/MUST mod (modification) set and read set for each procedure. Our preliminary results show that LevPA can analyze some programs with over a million lines of C code in minutes, faster than the state-of-the-art FSCS methods.\"",
        "Document: \"Vanishing Moment Method and Moment Solutions for Fully Nonlinear Second Order Partial Differential Equations. This paper concerns with numerical approximations of solutions of fully nonlinear second order partial differential equations (PDEs). A new notion of weak solutions, called moment solutions, is introduced for fully nonlinear second order PDEs. Unlike viscosity solutions, moment solutions are defined by a constructive method, called the vanishing moment method, and hence, they can be readily computed by existing numerical methods such as finite difference, finite element, spectral Galerkin, and discontinuous Galerkin methods. The main idea of the proposed vanishing moment method is to approximate a fully nonlinear second order PDE by a higher order, in particular, a quasilinear fourth order PDE. We show by various numerical experiments the viability of the proposed vanishing moment method. All our numerical experiments show the convergence of the vanishing moment method, and they also show that moment solutions coincide with viscosity solutions whenever the latter exist.\"",
        "Document: \"Mixed Finite Element Methods for the Fully Nonlinear Monge-Amp\u00e8re Equation Based on the Vanishing Moment Method. This paper studies mixed finite element approximations of the viscosity solution to the Dirichlet problem for the fully nonlinear Monge-Amp\u00e8re equation $\\det(D^2u^0)=f\\,(0)$ based on the vanishing moment method which was proposed recently by the authors in [X. Feng and M. Neilan, J. Scient. Comp., DOI 10.1007/s10915-008-9221-9, 2008]. In this approach, the second-order fully nonlinear Monge-Amp\u00e8re equation is approximated by the fourth order quasilinear equation $-\\varepsilon\\Delta^2 u^\\varepsilon + \\det{D^2u^\\varepsilon}=f$. It was proved in [X. Feng, Trans. AMS, submitted] that the solution $u^\\varepsilon$ converges to the unique convex viscosity solution $u^0$ of the Dirichlet problem for the Monge-Amp\u00e8re equation. This result then opens a door for constructing convergent finite element methods for the fully nonlinear second-order equations, a task which has been impracticable before. The goal of this paper is threefold. First, we develop a family of Hermann-Miyoshi-type mixed finite element methods for approximating the solution $u^\\varepsilon$ of the regularized fourth-order problem, which computes simultaneously $u^\\varepsilon$ and the moment tensor $\\sigma^\\varepsilon:=D^2u^\\varepsilon$. Second, we derive error estimates, which track explicitly the dependence of the error constants on the parameter $\\varepsilon$, for the errors $u^\\varepsilon-u^\\varepsilon_h$ and $\\sigma^0-\\sigma_h^\\varepsilon$. Finally, we present a detailed numerical study on the rates of convergence in terms of powers of $\\varepsilon$ for the error $u^0-u_h^\\varepsilon$ and $\\sigma^\\varepsilon-\\sigma_h^\\varepsilon$, and numerically examine what is the \u201cbest\u201d mesh size $h$ in relation to $\\varepsilon$ in order to achieve these rates. Due to the strong nonlinearity of the underlying equation, the standard perturbation argument for error analysis of finite element approximations of nonlinear problems does not work for the problem. To overcome the difficulty, we employ a fixed point technique which strongly relies on the stability of the linearized problem and its mixed finite element approximations.\"",
        "Document: \"ReCBuLC: reproducing concurrency bugs using local clocks. Multi-threaded programs play an increasingly important role in current multi-core environments. Exposing concurrency bugs and debugging such multi-threaded programs have become quite challenging due to their inherent non-determinism. In order to eliminate such non-determinism, many approaches such as record-and-replay and other similar bug reproducing systems have been proposed. However, those approaches often suffer significant performance degradation because they require a large amount of recorded information and/or long analysis and replay time. In this paper, we propose an effective approach, ReCBuLC, to take advantage of the hardware clocks available on modern processors. The key idea is to reduce the recording overhead and analyzing events' global order by using time stamps recorded in each thread. Those timestamps are used to determine the global orders of shared accesses. To avoid the large overhead incurred in accessing system-wide global clock, we opt to use local per-core clocks that incur much less access overhead. We then propose techniques to resolve differences among local clocks and obtain an accurate global event order. By using per-core clocks, state-of-the-art bug reproducing systems such as PRES and CLAP can reduce the recording overheads by 1% ~ 85%, and the analysis time by 84.66% ~ 99.99%, respectively.\n\n\"",
        "1 is \"Multi-GPU MapReduce on GPU Clusters\", 2 is \"A cache coherence scheme with fast selective invalidation\"",
        "Given above information, for an author who has written the paper with the title \"ReCBuLC: reproducing concurrency bugs using local clocks\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002689": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Using Argumentation to Understand Ambiguous Situations in Intelligent Environments':",
        "Document: \"Combining the real world with simulations for a robust testing of Ambient Intelligence services. This paper proposes a general architecture for testing, validating and verifying Ambient Intelligence (AmI) environments: AmISim. The development of AmI is a very complex task because this technology must often adapt to contextual information as well as unpredictable behaviours and environmental features. The architecture presented deals with AmI applications in order to cover the different components of these kinds of systems: environment, users, context and adaptation. This architecture is the first one that is able to cover all these features, which are needed in a full AmI system. The paper shows that AmISim is able to cover a complete AmI system and to provide a framework which can test scenarios that would be impossible to test in real environments or even with previous simulation approaches. Simulated and real elements coexist in AmISim for a robust testing, validation and verification of the AmI systems, which provide an easier and less costly deployment.\"",
        "Document: \"A Software Architecture for an Argumentation-Oriented Multi-Agent System. This paper proposes the materialization of a complete argumentation system ready to be built in conventional agent software platforms. In particular, an example for the Jadex agent platform is shown. The platform uses the BDI (Belief, Desire, Intention) model of agency. The main goal of this work is to foster usability of argumentation frameworks. The approach followed here to achieve it is the proposal of a formal representation for the dialogs that can occur, a specification of how to build the application domain model (i.e. the universe of discourse) and the necessary guidelines to manage these elements when building BDI agents.\"",
        "Document: \"On the Application of Clustering Techniques to Support Debugging Large-Scale Multi-Agent Systems. This work analyses the problematic of debugging a multi-agent system. It starts from the fact that MAS are a particular type\n of distributed systems in which the active entities are autonomous in the sense that behavior and knowledge of the whole system\n is distributed among agents. It situates the problem by firstly studying the classical approaches for conventional code debugging\n and also the techniques used in distributed systems in general. From this initial perspective, it tries to situate agent and\n multi-agent systems debugging. It finally proposes the use of conventional data mining tasks like clustering to, by summarising,\n help in debugging huge MAS.\n \"",
        "Document: \"Data Mining Applied to Irrigation Water Management. This work addresses the application of data mining to obtain artificial neural network based models for the application in water management during crops irrigation. This problem is very important in the zone of the South-East of Spain, as there is an important lack of rainfall there. These intelligent analysis techniques are used in order to optimize the consumption of such an appreciated and limited resource.\"",
        "Document: \"Semantic description of multimedia contents for the optimization of the advertising impact on TV program grids. The problem of advertising impact optimization in program grids consists to find a fully design of advertising contents in the program grid maximizing the satisfaction of advertisers and viewers. In this work, the problem of advertising impact optimization of program grids is approached. Standards for semantic description of multimedia contents are used for expressing contents in a television grid and the optimization process is based on semantic similarity measures between the descriptions of the TV contents. The overall optimization of the advertising impact is guaranteed using an evolutive approach.\"",
        "Document: \"Semantic Overlay Networks for Social Recommendation in P2P. In P2P systems, nodes typically connect to a small set of random peers to query them, and they propagate those queries along their own connections. To improve that mechanism, semantic overlay networks influence those connections depending on the content of the peers, clustering peers in overlapped groups (Semantic Overlay Networks). We propose using ontologies for describing semantic information of both, the shared items and the user profile. Once the peers are grouped by semantic information, we can take advantage of that distribution to add some new functionalities as recommendation, using ontologies comparison and the same principle to create SONs: the users with similar files to us, will probably have files that we are interested in.\"",
        "Document: \"Towards Socio-Chronobiological Computational Human Models. Testing and validating Ambient Intelligence (AmI) services by living labs is not always feasible. The costs involved, specially when considering a large number of users, may be prohibitive. In these cases, an artificial society is required to test the AmI software in a simulated environment. Numerous methodologies deal with the modeling of agents, but this paper contributes with a methodology capable of modeling human beings by using agents, CHROMUBE. This methodology is extended in this paper to include social interactions in its models. This extension of the methodology employs an architecture which maximizes code reuse and allows developers to model numerous kind of interactions (p.e.: voice, e-mail conversations, light panels ads, phone calls, etcetera). An implementation of the architecture is also given with UbikSim and a case study illustrates its use and potential.\"",
        "Document: \"Building Your Own Infrastructure Based 802.11 Fingerprinting Service. Indoor positioning is a basic requirement of Intelligent Environments. It is a building block for providing context-aware computing. Infrastructure based WiFi fingerprinting positioning technology (IBWFP) is one way of satisfying such requirement. In IBWFP, the basic landmarks for positioning are fixed 802.11 access points with a well known location. And it is infrastructure based because it refers to the fact that access points devote some computing capacity to collect 802.11 packets from other 802.11 mobile devices. Such information enables positioning a 802.11 mobile device indoors. This paper is a proposal for an inexpensive IBWFP. Its main contributions are a detailed accounting on how to build the physical system (business logic within the routers, configuration of the network in terms of number and location) and how to deploy and maintain the service on top of the physical system (i.e. Site survey, location engine creation, validation and management) with enough accuracy to exploit it. To the best of our knowledge, this is the first attempt to give a guideline to build your own inexpensive IBWFP technology covering all these practical aspects. Our intent is to push forward this technology by sharing our experience using it.\"",
        "Document: \"Toward A Framework For The Specification Of Hybrid Fuzzy Modeling. During the few last years, several successful approaches for the integration of soft computing techniques have been proposed in the area of data-driven fuzzy modeling (DDFM). However, there is a lack of methodological and general purpose hybridization in an easy and unified manner. This work outlines the design of a new DDFM framework called METHOD, offering the functionalities needed to combine techniques into hybrid strategies for DDFM tasks. Bearing in mind our main goal, a previous analysis of several existing DDFM techniques helps us: (1) to identify the most usual interaction schemes, by means of which methods are combined into DDFM hybrid strategies; (2) to exemplify requirements and effects for different techniques determining suitable combinations; and (3) to establish the universe of discourse based on which of these requirements and effect are defined. All these ideas are illustrated with examples. (C) 2005 Wiley Periodicals, Inc.\"",
        "Document: \"Robust design of multi-agent system interactions: A testing approach based on pattern matching. The definition of protocols between agents is not enough for guaranteeing the absence of undesirable communication in organizations and the presence of desirable ones in large multi-agent systems (MASs). This is a consequence of the complex system nature of MASs, which cause sophisticated behaviors to arise out of a multiplicity of relatively simple interactions among the independent agents composing them. With this motivation, this paper presents an approach for testing communication in MAS architectures. In this approach, designers are not only recommended to specify the desired communication protocols, but also the undesired patterns and organization structures in the agents' communications, allowing designers to define robust communication structures. For this purpose, this work presents (1) a language to define such patterns; (2) a set of already defined desired and undesired patterns which usually appear in general MASs; (3) a tool that allows developers to automatically detect these patterns in logs of MAS executions; and (4) a guideline that takes developers through the testing of the communications in MASs. The current approach is experienced with a case study, and the results show that the application of the current approach and the suppression of detected undesired patterns improve the effectiveness and efficiency of the corresponding MAS.\"",
        "1 is \"Towards Improving Supply Chain Coordination through Agent-Based Simulation\", 2 is \"CP-nets: a tool for representing and reasoning with conditional ceteris paribus preference statements\"",
        "Given above information, for an author who has written the paper with the title \"Using Argumentation to Understand Ambiguous Situations in Intelligent Environments\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002810": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A coarse-grained array based baseband processor for 100Mbps+ software defined radio':",
        "Document: \"Multi-Variant Execution of Parallel Programs. Multi-Variant Execution Environments (MVEEs) are a promising technique to protect software against memory corruption attacks. They transparently execute multiple, diversified variants (often referred to as replicae) of the software receiving the same inputs. By enforcing and monitoring the lock-step execution of the replicaeu0027s system calls, and by deploying diversity techniques that prevent an attacker from simultaneously compromising multiple replicae, MVEEs can block attacks before they succeed. Existing MVEEs cannot handle non-trivial multi-threaded programs because their undeterministic behavior introduces benign system call inconsistencies in the replicae, which trigger false positive detections and deadlocks in the MVEEs. This paper for the first time extends the generality of MVEEs to protect multi-threaded software by means of secure and efficient synchronization replication agents. On the PARSEC 2.1 parallel benchmarks running with four worker threads, our prototype MVEE incurs a run-time overhead of only 1.32x.\"",
        "Document: \"Taming Parallelism in a Multi-Variant Execution Environment. Exploit mitigations, by themselves, do not stop determined and well-resourced adversaries from compromising vulnerable software through memory corruption. Multi-variant execution environments (MVEEs) add additional assurance by executing multiple, diversified copies (variants) of the same program in lockstep while monitoring their behavior for signs of attacks (divergence). While executing multiple copies of the same program requires additional computational resources, modern MVEEs run many workloads at near-native speed and can detect adversaries before they leak secrets or achieve persistence on the host system. Multi-threaded programs are challenging to execute in lockstep by an MVEE. If the threads in a set of variants are not scheduled in the exact same order, the variants will diverge from each other in terms of the system calls they make. While benign, such divergence undermines the MVEEs ability detect divergence caused by malicious program inputs. To address this problem, we developed an MVEE-specific synchronization scheme that lets us execute a set of multithreaded variants in lockstep without causing benign divergence. Our fully-fledged MVEE runs the PARSEC 2.1 and SPLASH-2x parallel benchmarks (with four worker threads per variant) with a slowdown of less than 15% relative to unprotected execution. Addressing this longstanding compatibility issue makes MVEEs a viable defense for a far greater range of realistic workloads.\"",
        "Document: \"It's a TRaP: Table Randomization and Protection against Function-Reuse Attacks. Code-reuse attacks continue to evolve and remain a severe threat to modern software. Recent research has proposed a variety of defenses with differing security, efficiency, and practicality characteristics. Whereas the majority of these solutions focus on specific code-reuse attack variants such as return-oriented programming (ROP), other attack variants that reuse whole functions, such as the classic return-into-libc, have received much less attention. Mitigating function-level code reuse is highly challenging because one needs to distinguish a legitimate call to a function from an illegitimate one. In fact, the recent counterfeit object-oriented programming (COOP) attack demonstrated that the majority of code-reuse defenses can be bypassed by reusing dynamically bound functions, i.e., functions that are accessed through global offset tables and virtual function tables, respectively. In this paper, we first significantly improve and simplify the COOP attack. Based on a strong adversarial model, we then present the design and implementation of a comprehensive code-reuse defense which is resilient against reuse of dynamically-bound functions. In particular, we introduce two novel defense techniques: (i) a practical technique to randomize the layout of tables containing code pointers resilient to memory disclosure and (ii) booby trap insertion to mitigate the threat of brute-force attacks iterating over the randomized tables. Booby traps serve the dual purpose of preventing fault-analysis side channels and ensuring that each table has sufficiently many possible permutations. Our detailed evaluation demonstrates that our approach is secure, effective, and practical. We prevent realistic, COOP-style attacks against the Chromium web browser and report an average overhead of 1.1% on the SPEC CPU2006 benchmarks.\"",
        "Document: \"Backtracking and dynamic patching for free. We present a way to incorporate back-tracking and dynamic patching into existing debuggers, without requiring any change to their source code, the compiler or the run-time environment. An implementation on top of gdb is presented.\"",
        "Document: \"A backtracking instruction scheduler using predicate-based code hoisting to fill delay slots. Delayed branching is a technique to alleviate branch hazards without expensive hardware branch prediction mechanisms. For VLIW processors with deep pipelines and many issue slots, the instruction scheduler faces the difficult problem of filling the many delay slots. This paper proposes two solutions: a code hoisting technique that produces more candidate operations to be put in the delay slots and an adapted backtracking instruction scheduler that is capable of efficiently placing these candidate operations in the delay slots. We have demonstrated that the two mechanisms work wellon various multimedia and SPECINT2000 benchmarks. The code hoisting technique reduces the schedule length of a traditional scheduler without backtracking by 18%. Using the backtracking scheduler, this amount increases to 24%.\"",
        "Document: \"Automated reduction of the memory footprint of the Linux kernel. The limited built-in configurability of Linux can lead to expensive code size overhead when it is used in the embedded market. To overcome this problem, we propose the application of link-time compaction and specialization techniques that exploit the a priori known, fixed runtime environment of many embedded systems. In experimental setups based on the ARM XScale and i386 platforms, the proposed techniques are able to reduce the kernel memory footprint with over 16&percnt;. We also show how relatively simple additions to existing binary rewriters can implement the proposed techniques for a complex, very unconventional program, such as the Linux kernel. We note that even after specialization, a lot of seemingly unnecessary code remains in the kernel and propose to reduce the footprint of this code by applying code-compression techniques. This technique, combined with the previous ones, reduces the memory footprint with over 23&percnt; for the i386 platform and 28&percnt; for the ARM platform. Finally, we pinpoint an important code size growth problem when compaction and compression techniques are combined on the ARM platform.\"",
        "Document: \"To be or not to be cited in computer science. Traditional bias toward journals in citation databases diminishes the perceived value of conference papers and their authors.\"",
        "Document: \"Still Image Processing on Coarse-Grained Reconfigurable Array Architectures. Due to the increasing demands on efficiency, performance and flexibility reconfigurable computational architectures are very promising candidates in embedded systems design. Recently coarse-grained reconfigurable array architectures (CGRAs), such as the ADRES CGRA and its corresponding DRESC compiler are gaining more popularity due to several technological breakthroughs in this area. We investigate the mapping of two image processing algorithms, Wavelet encoding and decoding, and TIFF compression on this novel type of array architectures in a systematic way. The results of our experiments show that CGRAs based on ADRES and its DRESC compiler technology deliver improved performance levels for these two benchmark applications when compared to results obtained on a state-of-the-art commercial DSP platform, the c64x DSP from Texas Instruments. ADRES/DRESC can beat its performance by at least 50% in cycle count and the power consumption even drops to 10% of the published numbers of the c64x DSP.\"",
        "Document: \"A Coarse-Grained Array Accelerator for Software-Defined Radio Baseband Processing. A shrinking energy budget for mobile devices and increasingly complex communication standards make architecture development for software-defined radio very challenging. Coarse-grained array accelerators are strong candidates for achieving both high performance and low power. The C-programmable hybrid CGA-SIMD accelerator presented here targets emerging broadband cellular and wireless LAN standards, achieving up to 100-Mbps throughput with an average power consumption of 220 mW.\"",
        "Document: \"Compiler mitigations for time attacks on modern x86 processors. This paper studies and evaluates the extent to which automated compiler techniques can defend against timing-based side channel attacks on modern x86 processors. We study how modern x86 processors can leak timing information through side channels that relate to data flow. We study the efficiency, effectiveness, portability, predictability and sensitivity of several mitigating code transformations that eliminate or minimize key-dependent execution time variations. Furthermore, we discuss the extent to which compiler backends are a suitable tool to provide automated support for the proposed mitigations.\"",
        "1 is \"Simd Optimization Of The H.264/Svc Decoder With Efficient Data Structure\", 2 is \"Reducing Design Complexity of the Load/Store Queue\"",
        "Given above information, for an author who has written the paper with the title \"A coarse-grained array based baseband processor for 100Mbps+ software defined radio\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002817": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Image segmentation of biofilm structures using optimal multi-level thresholding.':",
        "Document: \"A theoretical comparison of two-class Fisher's and heteroscedastic linear dimensionality reduction schemes. We present a theoretical analysis for comparing two linear dimensionality reduction (LDR) techniques for two classes, a homoscedastic LDR scheme, Fisher's discriminant (FD), and a heteroscedastic LDR scheme, Loog-Duin (LD). We formalize the necessary and sufficient conditions for which the FD and LD criteria are maximized for the same linear transformation. To derive these conditions, we first show that the two criteria preserve the same maximum values after a diagonalization process is applied. We derive the necessary and sufficient conditions for various cases, including coincident covariance matrices, coincident prior probabilities, and for when one of the covariances is the identity matrix. We empirically show that the conditions are statistically related to the classification error for a post-processing one-dimensional quadratic classifier and the Chernoff distance in the transformed space.\"",
        "Document: \"Image segmentation of biofilm structures using optimal multi-level thresholding. The appreciation of biofilm structures in digital images can be subjective to the observer, and hence it is necessary to analyse the underlying images in useful parameters by means of quantification that is, ideally, free of errors. This paper proposes a combination of techniques for segmentation of biofilm images through an optimal multi-level thresholding algorithm and a set of clustering validity indices, including the determination of the best number of thresholds. The results, which are validated through Rand Index and a quantification process performed in a laboratory, are similar to the quantification and segmentation done by an expert.\"",
        "Document: \"Histogram methods in query optimization: the relation between accuracy and optimality. We have solved the following problem using pattern classification techniques (PCT): given two histogram methods, M/sub 1/ and M/sub 2/, used in query optimization, if the estimation accuracy of M/sub 1/ is greater than that of M/sub 2/, then M/sub 1/ has a higher probability of leading to the optimal query evaluation plan (QEP) than M/sub 2/. To the best of our knowledge, this problem has been open for at least two decades, the difficulty of the problem partially being due to the hurdles involved in the formulation itself. By formulating the problem from a pattern recognition perspective, we use PCT to present a rigorous mathematical proof of this fact, and show some uniqueness results. We also report empirical results demonstrating the power of these theoretical results on well-known histogram estimation methods.\"",
        "Document: \"An Evolutionary Approach for Correcting Random Amplified Polymorphism DNA Images. Random amplified polymorphism DNA (RAPD) analysis is a widely used technique in studying genetic relationships between individuals, in which processing the underlying images is a quite difficult problem, affected by various factors. Among these factors, noise and distortion affect the quality of images, and subsequently, accuracy in interpreting the data. We propose a method for processing RAPD images that allows to improve their quality and thereof, augmenting biological conclusions. This work presents a twofold objective that attacks the problem by considering two noise sources: band distortion and lane misalignment in the images. Genetic algorithms have shown good results in treating difficult problems, and the results obtained by using them in this particular problem support these directions for future work.\"",
        "Document: \"Applications of multilevel thresholding algorithms to transcriptomics data. Microarrays are one of the methods for analyzing the expression levels of genes in a massive and parallel way. Since any errors in early stages of the analysis affect subsequent stages, leading to possibly erroneous biological conclusions, finding the correct location of the spots in the images is extremely important for subsequent steps that include segmentation, quantification, normalization and clustering. On the other hand, genome-wide profiling of DNA-binding proteins using ChIP-seq and RNA-seq has emerged as an alternative to ChIP-chip methods. Due to the large amounts of data produced by next generation sequencing technology, ChIPseq and RNA-seq offer much higher resolution, less noise and greater coverage than its predecessor, the ChIPchip array. Multilevel thresholding algorithms have been applied to many problems in image and signal processing. We show that these algorithms can be used for transcriptomics and genomics data analysis such as sub-grid and spot detection in DNA microarrays, and also for detecting significant regions based on next generation sequencing data. We show the advantages and disadvantages of using multilevel thresholding and other algorithms in these two applications, as well as an overview of numerical and visual results used to validate the power of the thresholding methods based on previously published data.\"",
        "Document: \"A New Approach That Selects a Single Hyperplane from the Optimal Pairwise Linear Classifier. In this paper, we introduce a new approach to selecting the best hyper-plane classifier (BHC) from the optimal pairwise linear classifier is given. We first propose a procedure for selecting the BHC, and analyze the conditions in which the BHC is selected. In one of the cases, it is formally shown that the BHC and Fisher's classifier (FC) are coincident. The empirical and graphical analysis on synthetic data and real-life datasets from the UCI machine learning repository, which involves the optimal quadratic classifier, the BHC, the optimal pairwise linear classifier, and FC.\"",
        "Document: \"A geometric framework to visualize fuzzy-clustered data. Fuzzy clustering methods have been widely used in many applications. These methods, including fuzzy k-means and expectation maximization, allow an object to be assigned to multi-clusters with different degrees of membership. However, the memberships that result from fuzzy clustering algorithms are difficult to analyze and visualize, and usually are converted to 0-1 memberships. In this paper, we propose a geometric framework to visualize fuzzy-clustered data. The scheme provides a geometric visualization by grouping the objects with similar cluster membership, and shows clear advantages over existing methods, demonstrating its capabilities for viewing and navigating inter-cluster relationships in a spatial manner.\"",
        "Document: \"A novel approach for finding informative genes in ten subtypes of breast cancer. World wide, one in nine women are diagnosed with breast cancer in their lifetime and breast cancer is the second leading cause of death among women. Accurate diagnosis of the specific subtypes of this disease is vital to ensure that patients will have the best possible response to therapy. One way to discriminate subtypes of breast cancer is to study those genes that differentially express across different subtypes. In this study, we use different machine learning techniques to select the most informative genes corresponding to ten subtypes of breast cancer. In particular, we propose a new bottom-up hierarchical classification approach to select the most informative genes for different subtypes, while we identify the similarity level between these subtypes. Our results support that this new approach to gene selection yields a small subset of genes that can predict each of these ten subtypes with very high accuracy. Moreover, the proposed model provides an insightful structure for further analysis of these subtypes.\"",
        "Document: \"New Bounds and Approximations for the Error of Linear Classifiers. In this paper, we derive lower and upper bounds for the probability of error for a linear classifier, where the random vectors representing the underlying classes obey the multivariate normal distribution. The expression of the error is derived in the one-dimensional space, independently of the dimensionality of the original problem. Based on the two bounds, we propose an approximating expression for the error of a generic linear classifier. In particular, we derive the corresponding bounds and the expression for approximating the error of Fisher's classifier. Our empirical results on synthetic data, including up to five-hundred-dimensional featured samples, show that the computations for the error are extremely fast and quite accurate; the approximation differs from the actual error by at most epsilon = 0.0184340683.\"",
        "Document: \"Analysis of relevant physicochemical properties in obligate and non-obligate protein-protein interactions. Identification and analysis of types of protein-protein interactions (PPI) is an important problem in molecular biology because of its key role in many biological processes in living cells. In this paper, we focus on obligate and non-obligate complexes, their prediction and analysis. We propose a feature selection scheme called MRMR\"",
        "1 is \"Efficient Algorithm for Localized Support Vector Machine\", 2 is \"Evolution and generalization of a single neurone: II. complexity of statistical classifiers and sample size considerations\"",
        "Given above information, for an author who has written the paper with the title \"Image segmentation of biofilm structures using optimal multi-level thresholding.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002866": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'On stationarity in Internet measurements through an information-theoretic lens':",
        "Document: \"Cat and mouse: content delivery tradeoffs in web access. Web pages include extraneous material that may be viewed as undesirable by a user. Increasingly many Web sites also require users to register to access either all or portions of the site. Such tension between content owners and users has resulted in a \"cat and mouse\" game between content provided and how users access it.We carried out a measurement-based study to understand the nature of extraneous content and its impact on performance as perceived by users. We characterize how this content is distributed and the effectiveness of blocking mechanisms to stop it as well as countermeasures taken by content owners to negate such mechanisms. We also examine sites that require some form of registration to control access and the attempts made to circumvent it.Results from our study show that extraneous content exists on a majority of popular pages and that a 25-30% reduction in downloaded objects and bytes with corresponding latency reduction can be attained by blocking such content. The top ten advertisement delivering companies delivered 40% of all URLs matched as ads in our study. Both the server name and the remainder of the URL are important in matching a URL as an ad. A majority of popular sites require some form of registration and for such sites users can obtain an account from a shared public database. We discuss future measures and countermeasures on the part of each side.\"",
        "Document: \"Best paper -- Follow the money: understanding economics of online aggregation and advertising. The large-scale collection and exploitation of personal information to drive targeted online advertisements has raised privacy concerns. As a step towards understanding these concerns, we study the relationship between how much information is collected and how valuable it is for advertising. We use HTTP traces consisting of millions of users to aid our study and also present the first comparative study between aggregators. We develop a simple model that captures the various parameters of today's advertising revenues, whose values are estimated via the traces. Our results show that per aggregator revenue is skewed (5% accounting for 90% of revenues), while the contribution of users to advertising revenue is much less skewed (20% accounting for 80% of revenue). Google is dominant in terms of revenue and reach (presence on 80% of publishers). We also show that if all 5% of the top users in terms of revenue were to install privacy protection, with no corresponding reaction from the publishers, then the revenue can drop by 30%.\"",
        "Document: \"Efficient algorithms for predicting requests to Web servers. Internet traffic has grown significantly with the popularity of the Web. Consequently user perceived latency in retrieving Web pages has increased. Caching and prefetching at the client side, aided by hints from the server, are attempts at solving this problem. We suggest techniques to group resources that are likely to be accessed together into volumes, which are used to generate hints tailored to individual applications, such as prefetching, cache replacement, and cache validation. We discuss theoretical aspects of optimal volume construction, and develop efficient heuristics. Tunable parameters allow our algorithms to predict as many accesses as possible while reducing false predictions and limiting the size of hints. We analyze a collection of large server logs, extracting access patterns to construct and evaluate volumes. We examine sampling techniques to process only portions of the server logs while constructing equally good volumes. We show that it is possible to predict requests at low cost with a high degree of precision\"",
        "Document: \"Privacy in dynamic social networks. Anonymization of social networks before they are published or shared has become an important research question. Recent work on anonymizing social networks has looked at privacy preserving techniques for publishing a single instance of the network. However, social networks evolve and a single instance is inadequate for analyzing the evolution of the social network or for performing any longitudinal data analysis. We study the problem of repeatedly publishing social network data as the network evolves, while preserving privacy of users. Publishing multiple instances of the same network independently has privacy risks, since stitching the information together may allow an adversary to identify users in the networks. We propose methods to anonymize a dynamic network such that the privacy of users is preserved when new nodes and edges are added to the published network. These methods make use of link prediction algorithms to model the evolution of the social network. Using this predicted graph to perform group-based anonymization, the loss in privacy caused by new edges can be reduced. We evaluate the privacy loss on publishing multiple social network instances using our methods.\"",
        "Document: \"For sale : your data: by : you. Monetizing personal information is a key economic driver of online industry. End-users are becoming more concerned about their privacy, as evidenced by increased media attention. This paper proposes a mechanism called 'transactional' privacy that can be applied to personal information of users. Users decide what personal information about themselves is released and put on sale while receiving compensation for it. Aggregators purchase access to exploit this information when serving ads to a user. Truthfulness and efficiency, attained through an unlimited supply auction, ensure that the interests of all parties in this transaction are aligned. We demonstrate the effectiveness of transactional privacy for web-browsing using a large mobile trace from a major European capital. We integrate transactional privacy in a privacy-preserving system that curbs leakage of information. These mechanisms combine to form a market of personal information that can be managed by a trusted third party.\"",
        "Document: \"Stress testing traffic to infer its legitimacy. Adaptation in the face of performance degradation is the hallmark of well-behaved network traffic. For sufficiently robust applications, we propose distinguishing good from bad traffic on the basis of the response to artificial performance impairments. We explain the basic requirements for our scheme, and show how it could be generically applied at different levels in the protocol stack.\"",
        "Document: \"An Objective Reuse Metric: Model and Methology.  . Software reuse is an effective way to gain productivity in constructingsoftware systems. In order to continuously monitor the progressof reuse in the context of a project, we need an objective and repeatableway to measure the extent of reuse. This paper proposes a model andmethodology to automatically compute a general objective reuse measurefrom the source code of applications and reusable software repositories.Unlike consumer-oriented reuse measures in the literature, this measureis ... \"",
        "Document: \"What are our standards for validation of measurement-based networking research?. Standards? What standards?\"",
        "Document: \"Measuring privacy loss and the impact of privacy protection in web browsing. Various bits of information about users accessing Web sites. some of which are private, have been gathered since the inception of the Web. Increasingly the gathering, aggregation, and processing has been outsourced to third parties. The goal of this work is to examine the effectiveness of specific techniques to limit this diffusion of private information to third parties. We also examine the impact of these privacy protection techniques on the usability and quality of the Web pages returned. Using objective measures for privacy protection and page quality we examine their tradeoffs for different privacy protection techniques applied to a collection of popular Web sites as well as a focused set of sites with significant privacy concerns. We study privacy protection both at a browser and at a proxy.\"",
        "Document: \"Design, implementation, and evaluation of a client characterization driven web server. In earlier work we proposed a way for a Web server to detect connectivity information about clients accessing it in order to take tailored actions for a client request. This paper describes the design, implementation, and evaluation of such a working system. A Web site has a strong incentive to reduce the 'time-to-glass' to retain users who may otherwise lose interest and leave the site. We have performed a measurement study from multiple client sites around the world with various levels of connectivity to the Internet communicating with modified Apache Web servers under our control. The results show that clients can be classified in a correct and stable manner and that user-perceived latency can be reduced via tailored actions. Our measurements show that classification and determination of server actions are done without significant overhead on the Web server. We explore a variety of modified actions ranging from selecting a lower quality version of the resource to altering the manner of content delivery. By studying numerous performance related factors in a single unified framework and examining both individual actions as well as combination of actions, our modified Web server implementation shows the efficacy of various server actions.\"",
        "1 is \"Towards a SPDY'ier mobile web?\", 2 is \"A Data Mining Algorithm for Generalized Web Prefetching\"",
        "Given above information, for an author who has written the paper with the title \"On stationarity in Internet measurements through an information-theoretic lens\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002911": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'OFDM Power Loading Using Limited Feedback':",
        "Document: \"Training signal design for channel estimation in massive MIMO systems. In this paper, the design of training signals for channel estimation in massive multiple-input multiple-output (MIMO) systems is considered. Under a stationary, block Gauss-Markov channel model, a method for optimal pilot beam pattern design for enhanced channel estimation is proposed, exploiting both the properties of Kalman filtering and the spatio-temporal channel correlation. First, pilot beam pattern design is considered under the assumption of orthogonal beam patterns within a block. The orthogonality assumption is subsequently relaxed and the design problem is solved via a greedy approach. Numerical results show the efficacy of the proposed algorithm.\"",
        "Document: \"Conferencing on trees. In this paper, we develop an optimal scheme for conferencing (i.e. all-to-all communication), in communication networks. Our scheme uses the minimum sum rate. First we state a lower bound on sum rate and show how to achieve it on networks with tree topology. This result can be used on general networks considering the spanning tree which connects the conferencing nodes.\"",
        "Document: \"Quantized distributed relay network for physical layer network coding. Quantized distributed MIMO reception enhances communication between users using geographically separated nodes. Here we apply distributed reception to a two-user network with a relay composed of many \"dumb\" radio nodes. The network operates with a slotted structure commensurate with physical layer network coding. In the uplink, the user transmissions are simultaneously observed by all nodes. Subsequently, each relay node broadcasts a compressed version of the observed signal to the users. Given prior transmissions and channel state information, decoding algorithms can be run by the users. To demonstrate practicality, we compare performance of the system to physical layer network coding.\"",
        "Document: \"Quantized auction schemes for secondary spectrum markets. Auctions have been proposed as a way to provide economic incentives to dynamically allocate unused spectrum to users in need of it. Previously proposed auction schemes do not take into account the fact that users' power and bandwidth constraints might prevent them from transmitting their bid prices to the auctioneer with high precision, and that these transmitted bid prices must travel through a noisy channel. We propose auction schemes for secondary spectrum markets that address both these issues. In our schemes, a central clearing authority auctions spectrum to users who bid for it, while taking into account quantization of prices and noise in the channel explicitly. Our schemes are closely related to channel output feedback problems, user scheduling problems, and specifically to the technique of posterior matching. A natural objective of the clearing authority is to maximize its revenue by awarding spectrum to the highest bidder. Our simulation results show that this objective is asymptotically attained when the bids do not vary with time.\"",
        "Document: \"Combining Channel Output Feedback and CSI Feedback for MIMO Wireless Systems. The use of channel output feedback to improve the reliability of fading\nchannels has received scant attention in the literature. In most work on\nfeedback for fading channels, only channel state information (CSI) feedback has\nbeen exploited for coding at the transmitter. In this work, the design of a\ncoding scheme for multiple-input multiple-output (MIMO) fading systems with\nchannel output and channel state feedback at the transmitter is considered.\nUnder the assumption of additive white Gaussian noise and an independent and\nidentically distributed fading process, a simple linear coding strategy that\nachieves any rate up to capacity is proposed. The framework assumes perfect CSI\nat the transmitter and receiver. This simple linear processing scheme can\nprovide a doubly exponential probability of error decay with blocklength for\nall rates less than capacity. Remarkably, this encoding scheme actually\nconsists of two separate encoding blocks: one that adapts to the current CSI\nand one that adapts to the previous channel output feedback. This scheme is\nextended to the case when the CSI is quantized at the receiver and conveyed to\nthe transmitter over a limited rate feedback channel; for multiple-input\nsingle-output (MISO) fading systems it is shown the doubly exponential\nprobability of error decay is achieved as the blocklength increases.\"",
        "Document: \"Antenna Reliability Ordering Technique for Unequal Error Protection in Jointly Detected MIMO Systems. In this paper, we address the ordering of transmit antennas according to reliability for unequal error protection (UEP) in spatially multiplexed (SM) multiple-input multiple-output (MIMO) systems with joint detection at the receiver. When zeroforcing (ZF) detection is adopted, the reliabilities of transmit antennas are explicitly expressed as postequalization signal-to-noise ratios (SNRs). Thus, m...\"",
        "Document: \"On Scheduling for Multiple-Antenna Wireless Networks Using Contention-Based Feedback. Multiuser diversity gain is an effective technique for improving the performance of wireless networks. This gain can be exploited by scheduling the users with the best current channel conditions. However, this kind of scheduling requires that the base station (or access point) knows some kind of channel quality indicator (CQI) information for every user in the system. When the wireless link lacks ...\"",
        "Document: \"Feedforward Frameworks to Enhance Decoding in Precoded Multiuser MIMO Systems. It is difficult for users in multiuser multiple-input multiple-output (MU-MIMO) systems to obtain co-channel interference (CCI) statistics without user cooperation. We propose a technique through which each user can effectively obtain the statistics of the interference that it experiences in a precoded MU-MIMO system. This allows true maximum-likelihood detection to be performed in place of minimu...\"",
        "Document: \"Coded Distributed Diversity: A Novel Distributed Reception Technique for Wireless Communication Systems. In this paper, we consider a distributed reception scenario where a transmitter broadcasts a signal to multiple geographically separated receive nodes over fading channels, and each node forwards a few bits representing a processed version of the received signal to a fusion center. The fusion center then tries to decode the transmitted signal based on the forwarded information from the receive nodes and possible channel state information. We show that there is a strong connection between the problem of minimizing the symbol error probability at the fusion center in distributed reception and channel coding in coding theory. This connection allows us to design a unified framework for coded distributed diversity reception. We focus on linear block codes such as simplex codes or first-order Reed-Muller codes, which achieve the Griesmer bound with equality, to maximize the diversity gain. Due to our approach's simplicity, no complex offline optimization process is needed to design the coding structure at distributed receive nodes for the proposed coded receive diversity technique. The proposed technique can support a wide array of distributed reception scenarios, e.g., arbitrary M-ary symbol transmission at the transmitter and received signal processing with multiple bits at the receive nodes. Numerical studies show that coded receive diversity can achieve practical symbol error rates even with moderate signal-to-noise ratio and numbers of the receive nodes.\"",
        "Document: \"Multicell Cooperative Scheduling for Two-Tier Cellular Networks. Cellular wireless network architectures employing a tiered structure, consisting of traditional macro-cells and small-cells, have attracted much attention recently because of their potential to dramatically increase network capacity. These architectures can reduce the distances of transmit-receiver pairs, thus enhancing radio link qualities. However, a tiered cell deployment could incur severe cochannel interference. Prior work for single-tier networks has proposed coordinated multi-point (CoMP) transmission, which includes joint transmission, as a possible solution for overcoming the rate limitations induced by cochannel interference. Because users in each cell experience various interference conditions, each cell needs to support single transmission (in which only one cell transmits to a single user) and joint transmission. In a conventional implementation, a particular combination of transmissions over the entire network is fixed in advance for each time slot and opportunistic scheduling chooses one of the users supported by each transmission. However, when the number of users to be scheduled is small, the throughput improvement from opportunistic scheduling becomes limited. To overcome this problem, we propose a cumulative distribution function-based scheduling scheme which jointly chooses the transmitter set and the corresponding scheduled users in a two-tier network. It is shown that the throughput performance can be improved compared to those of the fixed slot resource allocation scheme because more users \"compete\" for slot resources.\"",
        "1 is \"Cognitive radio: brain-empowered wireless communications\", 2 is \"Millimeter-Wave Enhanced Local Area Systems: A High-Data-Rate Approach for Future Wireless Networks.\"",
        "Given above information, for an author who has written the paper with the title \"OFDM Power Loading Using Limited Feedback\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002933": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A new scalable multicast routing algorithm for interactive real-time applications':",
        "Document: \"A Distributed E2E Recovery Mechanism for MPLS Networks. This paper presents a new solution for inter-domain recovery within MPLS networks. The proposed mechanism is based on efficient collaboration between several entities called PCEs (Path Computation Elements). One PCE is designated at each domain. All PCEs should communicate in order to ensure suitable End-to-End(E2E) failure handling, recovery and restoration. Based on normative instructions described in the RFC 5298 and the Backward Recursive PCE-based Computation approach (BRPC) presented in RFC 5441, the new mechanism offers an opportunity to achieve E2E recovery using up-to-date information and giving a way to maintain correct network states with respect to intra-domain as well as inter-domain contexts, without care of heterogeneity and autonomy of crossed areas. Simulation results prove that, compared to existing E2E protection and on-demand backup path recovery mechanisms, the proposed solution is able to support, more efficiently, inter-domain recovery regardless of per-domains policies and rules, by ensuring lower recovery time, improved resources management and low packet loss/disorder bounds.\"",
        "Document: \"F-LQE: a fuzzy link quality estimator for wireless sensor networks. Radio Link Quality Estimation (LQE) is a fundamental building block for Wireless Sensor Networks, namely for a reliable deployment, resource management and routing. Existing LQEs (e.g. PRR, ETX, Four-bit, and LQI) are based on a single link property, thus leading to inaccurate estimation. In this paper, we propose F-LQE, that estimates link quality on the basis of four link quality properties: packet delivery, asymmetry, stability, and channel quality. Each of these properties is defined in linguistic terms, the natural language of Fuzzy Logic. The overall quality of the link is specified as a fuzzy rule whose evaluation returns the membership of the link in the fuzzy subset of good links. Values of the membership function are smoothed using EWMA filter to improve stability. An extensive experimental analysis shows that F-LQE outperforms existing estimators.\"",
        "Document: \"Net criticality revisited: an effective method to improve timing in physical design. Criticality metrics is a type of predictive models used in VLSI design. This work demonstrates that timing in physical design could be substantially improved if circuits were subjected to timing criticality analysis prior to layout and new criticality metrics were used to drive layout system. These new metrics are computed as ratios of net physical characteristics to the net delay bounds determined by an optimal bounds computation algorithm. Attempts to develop criticality metrics prior to layout were made before, but these metrics were not based on the bound ideology. The paper provides probabilistic interpretation of new criticality metrics and derivation of some important properties of these metrics. Evaluation of net criticality by new metrics can be easily merged with any layout system that allows weights to be assigned to nets on placement and/or routing steps. The methodology has been tested with a commercial layout system from a leading CAD provider. When the new criticality information was supplied to a basic commercial standard cell placer and router, timing was improved by 29.5% for the set of testcases. The achieved result is 12% better than timing generated by a targeted timing-driven layout system from the same provider. All additional computations related to the new criticality metrics require only negligible increase in run time of the basic layout system.\"",
        "Document: \"An efficient secure data aggregation scheme for wireless sensor networks. Wireless sensor networks (WSNs) often consists of a large number of low-cost sensor nodes that have strictly limited sensing, computation, and communication capabilities. Due to these unique specifications and a lack of tamper-resistant hardware, devising security protocols for WSNs is complex. Previous studies show that data transmission consumes much more energy than computation. Data aggregation can greatly help to reduce this consumption by eliminating redundant data. As wireless sensor networks are usually deployed in remote and hostile environments and used to transmit sensitive information, sensor nodes are prone to node compromise attacks and hence security issues such as data confidentiality and integrity are extremely important. A novel approach, which uses homomorphic encryption and additive digital signatures, is proposed to provide confidentiality, integrity for secure data aggregation in WSNs. The performance evaluation shows that the proposed scheme is efficient and scalable for large WSNs.\"",
        "Document: \"An efficient source authentication scheme in wireless sensor networks. Wireless sensor networks (WSN) are being widely deployed in military, healthcare and commercial environments. Since sensor networks pose unique challenges, traditional security methods, commonly used in enterprise networks, cannot be directly applied. In particular, broadcast source authentication is a critical security service in wireless sensor networks since it allows senders to broadcast messages to multiple receivers in a secure way. Public-key cryptography based solutions such as Elliptic Curve Cryptography (ECC) and Identity Based Cryptography (IBC) have been proposed but they all suffer from severe energy depletion attacks, resulting from a high computational and communication overheads. In this paper, we present a novel symmetric-key-based authentication scheme that exhibits low broadcast authentication overhead and thus avoiding the problem flaws inherent to the public key cryptography based schemes. Our scheme is built upon the integration of multi-level \u03bcTesla protocol, staggered authentication and the Bloom Filter. We show that our authentication scheme is very efficient in terms of energy consumption related to both computation and communication.\"",
        "Document: \"A testbed for the evaluation of link quality estimators in wireless sensor networks. Link quality estimation is a fundamental building block for the design of several different mechanisms and protocols in wireless sensor networks. The accuracy of link quality estimation greatly impacts the efficiency of these protocols. Therefore, a thorough experimental evaluation of link quality estimators (LQEs) is mandatory. This motivated us to build a benchmarking testbed - RadiaLE, that automates LQEs evaluation by analyzing their statistical properties. Our testbed includes (i.) hardware components that represent the WSN under test and (ii.) a software tool for setting up and controlling the experiments and also for analyzing the collected data, allowing for LQEs evaluation. To demonstrate the usefulness of RadiaLE, we carried out a comparative performance study of a set of well-known LQEs.\"",
        "Document: \"Efficiency of the RPL repair mechanisms for Low Power and Lossy Networks. Routing has been considered as one of the most important issues in 6LoWPAN [22] networks. For this reason, the Internet Engineering Task Force (IETF) has proposed a routing protocol called RPL (Routing Protocol for Low Power and Lossy Networks). This paper presents a performance evaluation of the repair mechanisms of RPL with storing mode, using the Contiki simulation environment. Performance metrics of interest are repair time, packet loss and power consumption. We also study the sensitivity of these metrics to network size and node location.\"",
        "Document: \"A collaborative key management scheme for distributed smart objects. AbstractAbstractSecure communication of Internet of Things (IoT) devices relies on cryptographic approaches whose robustness is based on the security of encryption keys. For instance, an adequate key management scheme must be used to properly manage the used keys. Designing an efficient and robust key management scheme for the connected devices is a challenging task. Reported key management schemes suffer from several deficiencies such as the single point of failure and the non\u2013trade\u2010off between scalability, connectivity, and resiliency. This paper first reviews key management schemes in the context of the IoT. Then, we propose a collaborative lightweight key management scheme that secures the keys during the life cycle of the embedded devices. Our proposal relies on the collaboration of the smart devices, where all the devices participate in the construction of the global key. Following the analytical and the comparative studies, we show that our proposal can be integrated in several communication protocols designated for embedded devices such as IEEE 802.15.4. Simulation results validate the analytical study and show that the proposed collaborative group key scheme exhibits superior performance when compared with two corresponding schemes reported in the literature. View Figure This paper proposes a collaborative key management scheme for smart devices. The proposed solution respects the restraints of IoT devices such as processing, storage and communication resources. The key update does not engender further costs.\"",
        "Document: \"Tabu Search Based Circuit Optimization. In this paper we address the problem of optimizing mixed CMOS/BiCMOS circuits. The problem is formulated as a constrained combinatorial optimization problem and solved using an tabu search algorithm. Only gates on the critical sensitizable paths are considered for optimization. Such a strategy leads to sizable circuit speed improvement with minimum increase in the overall circuit capacitance. Compared to earlier approaches, the presented technique produces circuits with remarkable increase in speed (greater than 20%) for very small increase in overall circuit capacitance (less than 3%).\"",
        "Document: \"Adaptive playout scheduling algorithm tailored for real-time packet-based voice conversations over wireless ad-hoc networks. The effective provision of real-time, packet-based voice conversations over multi-hop wireless ad-hoc networks faces several stringent constraints not found in conventional packet-based networks. Indeed, MANETs (mobile ad-hoc networks) are characterized by mobility of all nodes, bandwidth-limited channel, unreliable wireless transmission medium, etc. This environment will surely induce a high delay variation and packet loss rate impairing dramatically the user experienced quality of conversational services such as VoIP. Indeed, such services require the reception of each media unit before its deadline to guarantee a synchronous playback process. This requirement is typically achieved by artificially delaying received packets inside a de-jitter buffer. To enhance the perceptual quality the buffering delay should be adjusted dynamically throughout the vocal conversation. In this work, we describe the design of a playout algorithm tailored for real-time, packet-based voice conversations delivered over multi-hop wireless ad-hoc networks. The designed playout algorithm, which is denoted MAPA (mobility aware playout algorithm), adjusts the playout delay according to node mobility, which characterizes mobile ad-hoc networks, and talk-spurt, which is an intrinsic feature of voice signals. The detection of mobility is done in service passively at the receiver using several metrics gathered at the application layer. The perceptual quality is estimated using an augmented assessment approach relying on the ITU-T E-Model paradigm while including the time varying impairments observed by users throughout a packet-based voice conversation. Simulation results show that the tailored playout algorithm significantly outperforms conventional playout algorithms, specifically over a MANET with a high degree of mobility.\"",
        "1 is \"SecureDAV: a secure data aggregation and verification protocol for sensor networks\", 2 is \"Research challenges of autonomic computing\"",
        "Given above information, for an author who has written the paper with the title \"A new scalable multicast routing algorithm for interactive real-time applications\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "002991": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Multimedia Queries by Example and Relevance Feedback':",
        "Document: \"Is a Semantic Web Agent a Knowledge-Savvy Agent?. The issue of knowledge sharing has permeated the field of distributed AI and in particular, its successor, multiagent systems. Through the years, many research and engineering efforts have tackled the problem of encoding and sharing knowledge without the need for a single, centralized knowledge base. However, the emergence of modern computing paradigms such as distributed, open systems have highlighted the importance of sharing distributed and heterogeneous knowledge at a larger scale\u2014possibly at the scale of the Internet. The very characteristics that define the Semantic Web\u2014that is, dynamic, distributed, incomplete, and uncertain knowledge\u2014suggest the need for autonomy in distributed software systems.Semantic Web research promises more than mere management of ontologies and data through the definition of machine-understandable languages. The openness and decentralization introduced by multiagent systems and service-oriented architectures give rise to new knowledge management models, for which we can't make a priori assumptions about the type of interaction an agent or a service may be engaged in, and likewise about the message protocols and vocabulary used. We therefore discuss the problem of knowledge management for open multiagent systems, and highlight a number of challenges relating to the exchange and evolution of knowledge in open environments, which pertinent to both the Semantic Web and multiagent-system communities alike.\"",
        "Document: \"Experience-based reinforcement learning to acquire effective behavior in a multi-agent domain. In this paper, we discuss Profit-sharing, an experience-baised reinforcement learning approach (which is similar to a Monte-Carlo based reinforcement learning method) that can be used to learn robust and effective actions within uncertain, dynamic, multi-agent systems. We introduce the cut-loop routine that discards looping behavior, and demonstrate its effectiveness empirically within a simplified NEO (non-combatant evacuation operation) domain. This domain consists of several agents which ferry groups of evacuees to one of several shelters. We demonstrate that the cut-loop routine makes the Profit-sharing approach adaptive and robust within a dynamic and uncertain domain, without the need for predefined knowledge or subgoals. We also compare it empirically with the popular Q-learning approach.\"",
        "Document: \"Bringing semantics to web services: the OWL-S approach. Service interface description languages such as WSDL, and related standards, are evolving rapidly to provide a foundation for interoperation between Web services. At the same time, Semantic Web service technologies, such as the Ontology Web Language for Services (OWL-S), are developing the means by which services can be given richer semantic specifications. Richer semantics can enable fuller, more flexible automation of service provision and use, and support the construction of more powerful tools and methodologies. Both sets of technologies can benefit from complementary uses and cross-fertilization of ideas. This paper shows how to use OWL-S in conjunction with Web service standards, and explains and illustrates the value added by the semantics expressed in OWL-S.\"",
        "Document: \"A pragmatic approach for the semantic description and matching of pervasive resources. The increasing popularity of personal wireless devices has raised new demands for the efficient discovery of heterogeneous devices and services in pervasive environments. With the advancement of the electronic world, the diversity of available services is increasing rapidly. Traditional approaches for service discovery describe services at a syntactic level and the matching mechanisms available for these approaches are limited to syntactic comparisons based on attributes or interfaces. In order to overcome these limitations, there has been an increased interest in the use of semantic description and matching techniques to support effective service discovery. In this paper, we present a semantic matching approach to facilitate the discovery of device-based services in pervasive environments. The approach includes a ranking mechanism that orders services according to their suitability and also considers priorities placed on individual requirements in a request during the matching process. The solution has been systematically evaluated for its retrieval effectiveness and the results have shown that the matcher results agree reasonably well with human judgement. Another important practical concern is the efficiency and the scalability of the semantic matching solution. Therefore, we have evaluated the scalability of the proposed solution by investigating the variation in matching time in response to increasing numbers of advertisements and increasing request sizes, and have presented the empirical results.\"",
        "Document: \"Browsing Schedules - An Agent-Based Approach to Navigating the Semantic Web. The Semantic Web promises to change the way agents navigate, harvest and utilize information on the internet. By providing a structured, distributed representation for expressing concepts and relationships defined by multiple ontologies, it is now possible for agents to read and reason about published knowledge, without the need for scrapers, information agents, and centralized ontologies. Agents can utilize this knowledge to seek and invoke other agents and web services, thus supporting navigation across the Semantic Web. We demonstrate how agents support enhanced navigation within a conference-schedule domain, and present three agent-based services: the RETSINA Calendar Agent, which reasons about schedules marked up on the Semantic Web; the DMA2ICal Translation Agent which provides translation services between schedules grounded in different ontologies, and a Conference Agent that invokes the Calendar Agent.\"",
        "Document: \"Co-Presence Communities: Using Pervasive Computing to Support Weak Social Networks. Although the strongest social relationships feature most prominently in our lives, we also maintain a multitude of much weaker connections: the distant colleagues that we share a coffee with in the afternoon; the waitress at a our regular sandwich bar; or the \"familiar stranger' we meet each morning on the way to work. These are all examples of weak relationships which have a strong spatial-temporal component but with few support systems available. This paper explores the idea of \"Co-presence Communities' - a probabilistic definition of groups that are regularly collocated together - and how they might be used to support weak social networks. An algorithm is presented for mining the Copresence Community definitions from data collected by Bluetooth-enabled mobile phones. Finally, an example application is introduced which utilises these communities for disseminating information.\"",
        "Document: \"Evaluating Ontology Modules Using an Entropy Inspired Metric. The focus of ontology modularization to date has largely been on the creation of techniques to carry out ontology modularization. This creates a problem in evaluating the results of the different techniques. Ontology modularization techniques cannot solely be evaluated by examining their logical properties. Certain applications of ontology modularization, such as ontology reuse, require a new objective way to evaluate the results. This paper motivates the use of an entropy inspired measure to evaluate ontology modules by arguing that current objective measures of evaluation do not reconcile with the subjective measures employed by Ontology Engineers. Experiments are conducted to show that an entropy based evaluation of ontology modules is beneficial to an Ontology Engineer evaluating the results of ontology module extraction techniques.\"",
        "Document: \"Multimedia Queries by Example and Relevance Feedback. We describe the FALCON system for handling multimedia queries with relevance feedback. FALCON distinguishes itself in its ability to handle even disjunctive queries on metric spaces. Our experiments show that it performs well on both real and synthetic data in terms of precision/recall, speed of conver- gence, individual query speed, and scalability. Moreover, it can easily take advantage of off-the-shelf spatial- and metric- access methods.\"",
        "Document: \"Agent-based support for human/agent teams. In this paper, we present an interface agent, MokSAF, which facilitates time-critical team-planning tasks for teams of both humans and heterogeneous software agents. This agent assists in the formation of teams of humans (via other MokSAF agents) and task agents that can autonomously perform team subtasks. It provides a suitable interaction mechanism to instruct the various task agents in the team; and, by monitoring the human's progress, reallocate or modify the sub-tasks if the human fails to achieve that subtask. A military domain has been used to investigate this interface agent. The task consists of three military (human) commanders that each assemble a platoon, and plan routes so that all three platoons arrive at a given rendezvous by a specified time. An experimental study has been conducted to evaluate MokSAF and the assistance provided by one of three different task agents, and the results summarized.\"",
        "Document: \"Task Oriented Evaluation of Module Extraction Techniques. Ontology Modularization techniques identify coherent and often reusable regions within an ontology. The ability to identify such modules, thus potentially reducing the size or complexity of an ontology for a given task or set of concepts is increasingly important in the Semantic Web as domain ontologies increase in terms of size, complexity and expressivity. To date, many techniques have been developed, but evaluation of the results of these techniques is sketchy and somewhat ad hoc. Theoretical properties of modularization algorithms have only been studied in a small number of cases. This paper presents an empirical analysis of a number of modularization techniques, and the modules they identify over a number of diverse ontologies, by utilizing objective, task-oriented measures to evaluate the fitness of the modules for a number of statistical classification problems.\"",
        "1 is \"Protocols and Impossibility Results for Gossip-Based Communication Mechanisms\", 2 is \"The computational complexity of propositional STRIPS planning\"",
        "Given above information, for an author who has written the paper with the title \"Multimedia Queries by Example and Relevance Feedback\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003063": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Random, Persistent, and Adaptive Spectrum Sensing Strategies for Multiband Spectrum Sensing in Cognitive Radio Networks With Secondary User Hardware Limitation.':",
        "Document: \"Slotted Aloha In Multicell And Nakagami Fading Environment. The slotted Aloha (S-ALOHA) scheme in Nakagami fading channels with the presence of in-cell and cochannel-cell interference is studied. The cases of asynchronous cochannel-cells are especially considered. The analysis is based on the signal capture model. and gives closed-form expressions for the system throughput. Additional channel conditions and system parameters are examined in the study, including a minimal signal power requirement, lognormal shadowing and the cellular cluster size.\"",
        "Document: \"MAC protocol identification using support vector machines for cognitive radio networks. Cognitive radio is regarded as a potential solution to address the spectrum scarcity issue in wireless communication. In CR, an unlicensed network user (secondary user) is enabled to dynamically/adaptively access the frequency channels considering the current state of the external radio environment. In this article, we investigate the medium access control protocol identification for applications in cognitive MAC. MAC protocol identification enables CR users to sense and identify the MAC protocol types of any existing transmissions (primary or secondary users). The identification results will be used by CR users to adaptively change their transmission parameters in order to improve spectrum utilization, as well as to minimize potential interference to primary and other secondary users. MAC protocol identification also facilitates the implementation of communications among heterogeneous CR networks. In this article, we consider four MAC protocols, including TDMA, CSMA/CA, pure ALOHA, and slotted ALOHA, and propose a MAC identification method based on machine learning techniques. Computer simulations are performed to evaluate the MAC identification performance.\"",
        "Document: \"Link optimization for energy-constrained wireless networks with packet retransmissions. With the objective to minimize the energy consumption for packet based communications in energy-constrained wireless networks, this paper establishes a theoretical model for the joint optimization of the parameters at the physical layer and data link layer. Multilevel quadrature amplitude modulation (MQAM) and automatic repeat request (ARQ) techniques are considered in the system model. The optimization problem is formulated into a three dimensional nonlinear integer programming (NIP) problem with the modulation order, packet size, and retransmission limit as variables. For the retransmission limit, a simple search method is applied to degenerate the three dimensional problem into a two dimensional NIP problem, for which two optimization algorithms are proposed. One is the successive quadratic programming (SQP) algorithm, combining with the continuous relaxation based branch-and-bound method, which can obtain the global optimal solution since the continuous relaxation problem is proved to be hidden convex. The other is a low-complexity sub-optimal iterative algorithm, combining with the nearest-neighboring method, which can be implemented with a polynomial complexity. Numerical examples are given to illustrate the optimization solution, which suggests that the joint optimization of the physical/data link layer parameters contributes noticeably to the energy saving in energy-constrained wireless networks. Copyright \u00a9 2010 John Wiley & Sons, Ltd.\"",
        "Document: \"Analysis of Cooperative TDMA in Rayleigh Fading Channels. The time-division-multiple-access-based (TDMA) protocol has been widely utilized as a reliable media access control (MAC) mechanism. By allocating each user a dedicated time slot, a given TDMA user transmits its packets in its exclusively assigned time slots, whereas other users are in idle mode (not transmitting). However, in this paper, a cooperative TDMA (C-TDMA) method is investigated, which enables cooperative transmissions among multiple TDMA users to improve the probability of success of packet transmissions. Specifically, a TDMA user not only transmits packets in its assigned slots but monitors/overhears other users' packets in other time slots as well. It will then be able to assist the other users, if needed, to retransmit their failed packets through cooperative diversity. For performance evaluations, expressions of three metrics, i.e., network throughput, packet-dropping rate, and average packet delay, are derived, considering a Rayleigh fading channel. Both theoretical and simulation results are presented to demonstrate that the throughput, dropping rate, and delay performance are all improved significantly with the use of C-TDMA.\"",
        "Document: \"Power Adaptation for Multihop Networks With End-to-End BER Requirements. With the requirement of average end-to-end bit error rate (BER) performance, we study power-adaptation problems for multihop ad hoc networks to minimize the power consumption in fading channels, considering two scenarios with regard to the knowledge of the channel-state information (CSI). One is that the transmitter for each hop knows the instantaneous CSI (ICSI) of its hop and all other hops (ICSI/ICSI), and the other is that the transmitter of each hop has only the knowledge of the ICSI of its hop and the average CSI (ACSI) of all other hops (ICSI/ACSI). The two power-adaptation problems (with ICSI/ICSI and ICSI/ACSI) are formulated into nonlinear programming problems, and corresponding power-adaptation solutions/schemes are obtained. Numerical examples are presented to compare the networks with the two proposed power-adaptation schemes and a distributed power-adaptation scheme, in which the transmitter of each individual hop adjusts its transmission power based on its own ICSI and a specified BER requirement for this hop. It is shown that the two proposed schemes have similar power consumptions, and both achieve a noticeable power saving over the distributed scheme.\"",
        "Document: \"On the limits of predictability in real-world radio spectrum state dynamics: from entropy theory to 5G spectrum sharing. A range of applications in cognitive radio networks, from adaptive spectrum sensing to predictive spectrum mobility and dynamic spectrum access, depend on our ability to foresee the state evolution of radio spectrum, raising a fundamental question: To what degree is radio spectrum state (RSS) predictable? In this article we explore the fundamental limits of predictability in RSS dynamics by studying the RSS evolution patterns in spectrum bands of several popular services, including TV bands, ISM bands, cellular bands, and so on. From an information theory perspective, we introduce a methodology of using statistical entropy measures and Fano inequality to quantify the degree of predictability underlying real-world spectrum measurements. Despite the apparent randomness, we find a remarkable predictability, as large as 90 percent, in real-world RSS dynamics over a number of spectrum bands for all popular services. Furthermore, we discuss the potential applications of prediction-based spectrum sharing in 5G wireless communications.\"",
        "Document: \"Identification of legacy radios in a cognitive radio network using a radio frequency fingerprinting based method. Cognitive radio (CR) networks provide an open architecture for effectively utilizing communication resources through flexible opportunistic spectrum access methods. To successfully realize its benefits and minimize the misuses of a CR network, distinguishing radio/user classes (legacy radios/users versus secondary radios/users) and individual radio/user terminals (within one class/type) is a critical and challenging task in CR network operation. In this paper, we propose a radio frequency fingerprinting (RFF) based approach combined with machine learning algorithms to differentiate radio/user classes and terminals. In our experiments, the proposed method is implemented for distinguishing radio class (MOTOROLA walkie talkies (as legacy radios) versus Universal Software Radio Peripheral (USRP) (as secondary radios)) and distinguishing individual radio terminals within one radio class. The experimental results demonstrate that the proposed method is very effective in differentiating radio types and radio terminals.\"",
        "Document: \"Outage performance of a hybrid DS/FH spread-spectrum signal in an ISM band. This paper develops an analytical model for characterizing the coexistence interference in the unlicensed industrial, scientific and medical (ISM) bands, based on radio frequency measurement results in the 2.4 GHz ISM band. The modeling approach presented here offers a simple methodology for profiling the interference from devices occupying a common spectrum. Outage performance using the interference model is exemplified through the analysis for a hybrid direct-sequence frequency-hopping spread-spectrum signal. The work presented in this paper looks to provide insight by means of quantitative evaluation of signal outage in a coexistence environment that can be used for engineering design of applications for unlicensed bands as well as function as a tool in the development process for future spectrum management policy models.\"",
        "Document: \"An adaptive cooperation diversity scheme with best-relay selection in cognitive radio networks. In this correspondence, an adaptive cooperation diversity scheme with best-relay selection is proposed for multiple-relay cognitive radio networks to improve the performance of secondary transmissions while ensuring the quality of service (QoS) of primary transmissions. Exact closed-form expressions of the outage probability of secondary transmissions, referred to as secondary outage probability, are derived under the constraint of satisfying a required outage probability of primary transmissions (primary outage probability) for both the traditional noncooperation and the proposed adaptive cooperation schemes over Rayleigh fading channels. Numerical and simulation results show that, with a guaranteed primary outage probability, a floor of the secondary outage probability occurs in high signal-to-noise ratio (SNR) regions. Moreover, the outage probability floor of the adaptive cooperation scheme is lower than that of the non-cooperation scenario, which illustrates the advantage of the proposed scheme. In addition, we generalize the traditional definition of the diversity gain, which can not be applied directly in cognitive radio networks since mutual interference between the primary and secondary users should be considered. We derive the generalized diversity gain and show that, with a guaranteed primary outage probability, the full diversity order is achieved using the proposed adaptive cooperation scheme.\"",
        "Document: \"Power Adaptation in Multi-hop Sensor Networks for Energy Minimization. The power adaptation issue for multi-hop sensor networks is considered in this paper to minimize the energy consumption with a given requirement of the average end-to-end bit error rate (BER) performance. To optimize the power adaptation, a nonlinear programming problem is formulated and an analytical result is derived from its Karush-Kuhb-Tucker (KKT) necessary conditions. Numerical examples are presented to compare the networks with the proposed power adaptation scheme and a distributed power adaptation scheme, in which the transmitter of each individual hop adjusts its transmission power based on its own channel state information (CSI) and a specified BER requirement for this hop. A significant energy saving is achieved with the use of the proposed scheme.\"",
        "1 is \"Advanced ICA-based receivers for block fading DS-CDMA channels\", 2 is \"Performance analysis of cooperative diversity systems with opportunistic relaying and adaptive transmission.\"",
        "Given above information, for an author who has written the paper with the title \"Random, Persistent, and Adaptive Spectrum Sensing Strategies for Multiband Spectrum Sensing in Cognitive Radio Networks With Secondary User Hardware Limitation.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003145": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Synchronization and State Estimation of a Class of Hierarchical Hybrid Neural Networks With Time-Varying Delays.':",
        "Document: \"Stability and Stabilization of Discrete-Time Semi-Markov Jump Linear Systems via Semi-Markov Kernel Approach. This paper is concerned with the problems of stability and stabilization for a class of discrete-time semi-Markov jump linear systems (S-MJLSs). The discrete-time semi-Markov kernel (SMK) is introduced, where the probability density function of sojourn-time is dependent on both current and next system mode. As a consequence, different types of distributions and/or different parameters in a same type of distribution of sojourn-time, depending on the target mode towards which the system jumps, can coexist in each mode of a SMK. The underlying S-MJLSs are therefore more general than those considered in existing studies. A new stability concept generalizing the traditional mean square stability is proposed such that numerically testable criteria on the basis of SMK are obtained. Numerical examples are presented to illustrate the validity and advantage of the developed theoretical results.\"",
        "Document: \"Distributed Filtering for Fuzzy Time-Delay Systems With Packet Dropouts and Redundant Channels. This paper is concerned with the distributed H\u221e filtering problem for a class of discrete-time Takagi-Sugeno fuzzy systems with time-varying delays. The data communications among sensor nodes are equipped with redundant channels subject to random packet dropouts that are modeled by mutually independent Bernoulli stochastic processes. The practical phenomenon of the uncertain packet dropout rate is considered, and the norm-bounded uncertainty of the packet dropout rate is asymmetric to the nominal rate. Sufficient conditions on the existence of the desired distributed filters are established by employing the scaled small gain theorem to ensure that the closed-loop system is stochastically stable and achieves a prescribed average H\u221e performance index. Finally, an illustrative example is provided to verify the theoretical findings.\"",
        "Document: \"Extended Dissipative State Estimation for Markov Jump Neural Networks With Unreliable Links. This paper is concerned with the problem of extended dissipativity-based state estimation for discrete-time Markov jump neural networks (NNs), where the variation of the piecewise time-varying transition probabilities of Markov chain is subject to a set of switching signals satisfying an average dwell-time property. The communication links between the NNs and the estimator are assumed to be imperfect, where the phenomena of signal quantization and data packet dropouts occur simultaneously. The aim of this paper is to contribute with a Markov switching estimator design method, which ensures that the resulting error system is extended stochastically dissipative, in the simultaneous presences of packet dropouts and signal quantization stemmed from unreliable communication links. Sufficient conditions for the solvability of such a problem are established. Based on the derived conditions, an explicit expression of the desired Markov switching estimator is presented. Finally, two illustrated examples are given to show the effectiveness of the proposed design method.\"",
        "Document: \"Reduced-order fuzzy modeling for nonlinear switched systems. In this paper, the problem of model approximation is investigated for T-S fuzzy switched system with stochastic disturbance. For a high-order considered system, our attention is focused on the construction of a reduced-order model, which not only approximates the original system well with a Hankel-norm performance but also translates it into a lower-dimensional linear switched system. By average dwell time approach and the piecewise Lyapunov function technique, a sufficient condition is first proposed to guarantee the mean-square exponential stability with a Hankel-norm error performance for the error system. The model approximation is then converted into a convex optimization problem by using a linearization procedure.\"",
        "Document: \"Passivity and passification for Markov jump genetic regulatory networks with time-varying delays. This paper investigates the problems of passivity and passification for a class of nonlinear Markov jump genetic regulatory networks with time-varying delays and disturbances. By utilizing the Lyapunov\u2013Krasovskii functional method, a novel delay-dependent passivity criterion is established in terms of linear matrix inequalities to guarantee the Markov jump genetic regulatory networks to be passive. Then, based on the obtained passivity conditions, the passification problem is further solved by designing a mode-dependent state feedback controller. A numerical example is presented to demonstrate the effectiveness and validity of the proposed approaches.\"",
        "Document: \"Model reduction of A class of Markov jump nonlinear systems with time-varying delays via projection approach. In this paper, the problem of H \u221e model reduction for a class of discrete-time Markov jump nonlinear systems with time-varying delays is investigated. Based on the Lyapunov-Krasovskii functional method, sufficient stability conditions in terms of strict LMIs are derived for the model error system. Projection approach is employed to construct a reduced-order model such that the model error system is asymptotically stable and preserves a guaranteed H \u221e performance. A numerical example is presented to illustrate the effectiveness of the developed theoretical results and to make comparisons among systems with different structures and parameters.\"",
        "Document: \"Observer-Based Stabilization of Nonhomogeneous Semi-Markov Jump Linear Systems With Mode-Switching Delays. This technical note addresses the problem of simultaneous design of observer and controller for discrete-time nonhomogeneous semi-Markov jump linear systems (S-MJLSs) against mode mismatches between system and observer-based controller. The considered systems are more general than homogeneous S-MJLSs, and the mode mismatches are caused by time delays of mode switchings of controller. Based on the semi-Markov kernel approach, together with the Lyapunov function depending upon not only both the system and controller modes but also the time that has elapsed in the current mode, sufficient conditions on the existence of the desired mode-dependent observer-based controller are presented such that the augmented nonhomogeneous S-MJLS composed of a closed-loop control system and an estimation error system is \n<inline-formula xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$\\sigma$</tex-math></inline-formula>\n-error mean square stable. Finally, a practical example of a quarter-car active suspension is provided to validate the superiorities of the proposed control strategy.\"",
        "Document: \"Asynchronously switched control of a class of slowly switched linear systems. The stabilization problem for a class of switched linear systems with average dwell time (ADT) switching is reinvestigated in this paper. State-feedback controllers are designed, which takes the more practical case, asynchronous switching, into account, where the so-called \u201casynchronous switching\u201d\u00a0indicates that the switchings between the controllers and the system modes are in the presence of a time delay. By combining the asynchronous switching, an improved stabilization approach is given, and existence conditions of the controllers associated with the corresponding ADT switching are formulated in terms of a set of linear matrix inequalities. A numerical example is given to show the validity and potential of the obtained theoretical results.\"",
        "Document: \"ECR-MAC: An energy-efficient and receiver-based MAC protocol for Cognitive Sensor Networks in smart grid. Wireless Sensor Networks (WSNs) have been widely recognized as a promising solution for enhancing various aspects of electric power grid and realizing the vision of smart grid. However, energy crisis and challenging wireless environment in smart grid create a number of challenges for WSNs, as a result of which energy efficiency become critically important. On the other hand, cognitive radio (CR) technology is expected to play a vital role in smart grid networks. Cognitive Sensor Networks (CSNs) can effectively address some unique challenges of WSNs in smart grid. In this paper, we aim to design an energy efficient Medium Access Control (MAC) protocol for CSNs. In this regard, we propose ECR-MAC, which is an energy-efficient receiver-based MAC protocol for CSNs. ECR-MAC uses a energy-efficiency based auction mechanism and preamble sampling techniques for providing high energy efficiency and reliability. In addition, ECR-MAC explicitly accounts for the peculiarities of a CR environment. Analytical and simulation results demonstrate the effectiveness of ECR-MAC as a viable solution for CSNs.\"",
        "Document: \"Control for discrete-time fuzzy Markov jump systems with mode-dependent antecedent parts. This paper is concerned with the control problem for a class of discrete-time fuzzy Markov jump systems (MJSs). Unlike the common assumption in the existing literature, the antecedent parts of fuzzy rules are mode-dependent, i.e., the premise variables and/or their fuzzy partitions can be different in different modes. Based on a fuzzy-basis-dependent and mode-dependent Lyapunov function, the existence conditions of the desired mode-dependent state feedback controller are derived such that the closed-loop system is stochastically stable and achieves a guaranteed performance in the H\u221e sense. Two examples, including a practical example of robot arm, are used to demonstrate the applicability of the obtained theoretical results.\"",
        "1 is \"Neocognitron: A hierarchical neural network capable of visual pattern recognition\", 2 is \"Recent developments in blind channel equalization: from cyclostationarity to subspaces\"",
        "Given above information, for an author who has written the paper with the title \"Synchronization and State Estimation of a Class of Hierarchical Hybrid Neural Networks With Time-Varying Delays.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003190": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Modeling spatial and semantic cues for large-scale near-duplicated image retrieval':",
        "Document: \"Visual block link analysis for image re-ranking. Currently, image retrieval is becoming a popular feature in many search engines, most of which return images solely based on the metadata of pages in which the image appears. Since the page text may be inconsistent with the image content, image re-ranking is of great necessity. In this paper, we propose a Visual Block Rank approach for image re-ranking. We consider images as visual block sets and analyze the authority of images by exploring the underlying visual block link structure via LSA (Latent Semantic Analysis). The idea of Page-Rank is leveraged to our scheme for Visual Block Ranking and then the re-ranking order of images is deduced. We validate our approach with search results returned by Google Image. Experimental results show improvement of the accuracy and efficiency of our method over the state-of-the-art VisualRank algorithm.\"",
        "Document: \"Single image super-resolution based on nonlocal similarity and sparse representation. Images with high resolution (HR) are often desired and required in imaging applications. Super-resolution (SR) is a promising but still challenging task to obtain an HR image (or sequence) from the observed low-resolution (LR) image (or sequence). In this paper, we present an SR approach for single image, by combining the image observation model, image nonlocal similarity, and sparse representation of image patches. First, we regularize low rank of matrix that is composed of nonlocal similar patches, to obtain an initial estimate of HR image. Then, each patch of the estimated image is sparsely represented in terms of LR dictionary, and the sparse coding coefficients are used to generate an output HR patch in terms of HR dictionary, resulting in the enhancement of high-frequency components in the HR estimate. The two over-complete dictionaries are jointly learned from example patch pairs in advance. Finally, the refined HR estimate is further constrained by the observation model. Experimental results have shown that the proposed SR method has obvious visual improvement in preserving edges and structures, while achieving comparable overall objective quality to the state-of-the-art methods.\"",
        "Document: \"Intra coding for depth maps using adaptive boundary location. Depth maps, an essential part in the new generation of 3D video coding, allow rendering of arbitrary viewpoints of a video scene. Depth maps are characterized by sharp object boundaries, which significantly affect the rendering quality and account for the most bitrate for depth map coding. This paper proposes a novel intra coding method for depth maps based on a two-step adaptive boundary location process. By extracting a series of sub-blocks along a depth boundary and refining the boundary within sub-blocks, accurate predictions for blocks with arbitrary edge shapes can be realized. Experimental results show that the proposed scheme achieves bitrate reductions of up to 28% and 13% on average for seven test sequences of MPEG 3DV compared to original intra coding of H.264/AVC considering the same quality of synthesized views. Besides, subjective quality of virtual views is improved owning to well preserved boundary information.\"",
        "Document: \"Distributed lossless coding of hyperspectral images. In this paper we propose a novel distributed lossless compression scheme for hyperspectral images. All the images/bands are encoded independently, and the spectral correlation is exploited using distributed coding technologies in order to achieve low encoding complexity. At the encoder, sub-sampled images are successively encoded and transmitted. At the decoder, side information is generated with the knowledge of decoded sub-sampled images and other previously decoded bands. Reference bands are adaptively selected, and sliding window prediction or k nearest neighbor prediction is performed to capture the spatially varying spectral characteristics. Experimental results on AVRIS data demonstrate that the proposed scheme achieves competitive compression performance with respect to other state-of-the-art 3D codecs and with even lower encoding complexity than 2D codecs.\"",
        "Document: \"Adaptive Layerwise Quantization for Deep Neural Network Compression. Building efficient deep neural network models has become a hot-spot in recent years for deep learning research. Many works on network compression try to quantize a neural network with low bitwidth weights and activations. However, most of the existing network quantization methods set a fixed bitwidth for the whole network, which leads to large performance drop under high compression rate. In this paper we introduce an adaptive layerwise quantization method which quantizes the network with different bitwidth assigned to different layers. By using entropy of weights and activations as an importance indicator for each layer, we keep most of the layers under a high compression rate while a few most important layers receive more bit assignment. Experiments on CI-FAR10 and ImageNet2012 datasets demonstrate that our layerwise quantization could achieve smaller model size and less computation cost than the comparison fixed bitwidth methods with comparable accuracy, or higher accuracy with similar model size and computational complexity.\"",
        "Document: \"Unified 2D and 3D Pre-Training of Molecular Representations. Molecular representation learning has attracted much attention recently. A molecule can be viewed as a 2D graph with nodes/atoms connected by edges/bonds, and can also be represented by a 3D conformation with 3-dimensional coordinates of all atoms. We note that most previous work handles 2D and 3D information separately, while jointly leveraging these two sources may foster a more informative representation. In this work, we explore this appealing idea and propose a new representation learning method based on a unified 2D and 3D pre-training. Atom coordinates and interatomic distances are encoded and then fused with atomic representations through graph neural networks. The model is pre-trained on three tasks: reconstruction of masked atoms and coordinates, 3D conformation generation conditioned on 2D graph, and 2D graph generation conditioned on 3D conformation. We evaluate our method on 11 downstream molecular property prediction tasks: 7 with 2D information only and 4 with both 2D and 3D information. Our method achieves state-of-the-art results on 10 tasks, and the average improvement on 2D-only tasks is 8.3%. Our method also achieves significant improvement on two 3D conformation generation tasks.\"",
        "Document: \"Principal visual word discovery for automatic license plate detection. License plates detection is widely considered a solved problem, with many systems already in operation. However, the existing algorithms or systems work well only under some controlled conditions. There are still many challenges for license plate detection in an open environment, such as various observation angles, background clutter, scale changes, multiple plates, uneven illumination, and so on. In this paper, we propose a novel scheme to automatically locate license plates by principal visual word (PVW), discovery and local feature matching. Observing that characters in different license plates are duplicates of each other, we bring in the idea of using the bag-of-words (BoW) model popularly applied in partial-duplicate image search. Unlike the classic BoW model, for each plate character, we automatically discover the PVW characterized with geometric context. Given a new image, the license plates are extracted by matching local features with PVW. Besides license plate detection, our approach can also be extended to the detection of logos and trademarks. Due to the invariance virtue of scale-invariant feature transform feature, our method can adaptively deal with various changes in the license plates, such as rotation, scaling, illumination, etc. Promising results of the proposed approach are demonstrated with an experimental study in license plate detection.\"",
        "Document: \"Progressive distributed coding of multispectral images. We present in this paper a novel distributed coding scheme for lossless and progressive compression of multispectral images. The main strategy of this new scheme is to explore data redundancies at the decoder in order to design a lightweight yet very efficient encoder suitable for onboard applications during acquisition of multispectral image. A sequence of increasing resolution layers is encoded and transmitted successively until the original image can be losslessly reconstructed from all layers. We assume that the decoder with abundant resources is able to perform adaptive region-based predictor estimation to capture spatially varying spectral correlation with the knowledge of lower-resolution layers, thus generate high quality side information for decoding the higher-resolution layer. Progressive transmission enables the spectral correlation to be refined successively, resulting in gradually improved decoding performance of higher-resolution layers as more data are decoded. Simulations have been carried out to demonstrate that the proposed scheme, with innovative combination of low complexity encoding, lossless compression and progressive coding, can achieve competitive performance comparing with high complexity state-of-the-art 3-D DPCM technique.\"",
        "Document: \"Coding techniques in Multiview Video Coding and Joint Multiview Video Model. Since early 2006, Joint Video Team has been devoting on the development of Multiview Video Coding (MVC) standard as an extension of H.264/AVC. This MVC standard has been finalized in 2008. During the standardization of MVC, there was also a project namely Joint Multiview Video Model (JMVM), which focused on the advanced coding tools that are potentially useful. Those coding tools adopted into JMVM, including illumination compensation and motion skip, have not been added into MVC specification. In this paper, coding techniques in MVC as well as the tools in JMVM are described and discussed, focusing on the coding efficiency.\"",
        "Document: \"Robust Transmission of Scalable Video Coding Bitstream Over Heterogeneous Networks. Video transmission using Scalable Video Coding (SVC) enables the functionalities of adaptation for both channel bandwidth and terminal device. However, link adaptation, which requires the video bitstream to provide error resilient scalability (ERS), is currently not supported in SVC. In this paper, we propose a robust SVC bitstream transmission framework, which integrates both error resilient (ER) coding and error concealment (EC) to enable the desired link adaptability. First, at the encoder, we design an ER coding scheme for SVC using redundant pictures, in which redundant picture information (RPI) is generated under rate-distortion criteria and transmitted to the media gateway together with the SVC bitstream. Then, ERS can be effectively achieved at the media gateway by selectively adding or removing coded pictures of different video coding layers according to the RPI and current network link status. Finally, at the decoder, we take advantage of the unique characteristics of SVC to develop an EC scheme based on both the proposed Virtual-BLSkip method and Wiener filtering to recover the lost or removed enhancement layer pictures. The proposed framework has negligible impact on the coding efficiency because it transmits RPIs rather than directly adding redundancy into the original bitstream. More importantly, it successfully strikes a balance between coding efficiency, error resiliency, and operation complexity. It is capable of providing the desired ERS with extremely low complexity, and maintaining virtually the same bit rate as input video bitstream. The experimental results show that the proposed framework achieves significant performance gains over existing ER transmission schemes.\"",
        "1 is \"Low-delay multiview video coding for free-viewpoint video communication\", 2 is \"Real-Time Single Image And Video Super-Resolution Using An Efficient Sub-Pixel Convolutional Neural Network\"",
        "Given above information, for an author who has written the paper with the title \"Modeling spatial and semantic cues for large-scale near-duplicated image retrieval\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003195": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Energy-Efficient Virtualisation of Threads in a Server Cluster.':",
        "Document: \"Flexible Group Protocol for Distributed Systems. The group protocol has to support applications with enough quality (QoS) of service in change of QoS supported by the underlying network and QoS requirements of applications. A flexible group service is supported for applications by cooperation of multiple autonomous agents. Each agent dynamically and autonomously takes a class of each protocol function like retransmission in change of QoS supported by networks and required by applications.\"",
        "Document: \"Trustworthiness of acquaintances in Peer-to-Peer overlay networks. Various types of applications manipulate objects distributed in Peer-to-Peer overlay networks. An acquaintance peer of a peer p is a peer whose service the peer p knows and with which the peer p can directly communicate. We discuss types of acquaintance relations among peers and how much a peer trusts each acquaintance peer. We define the trustworthiness of each acquaintance peer in terms of the acquaintance relations. We discuss a Charge-Based Flooding (CBF) algorithm to find target peers so that more trustworthy areas in Peer-to-Peer overlay networks are more deeply searched. We evaluate the CBF algorithm compared with a TTL-based algorithm.\"",
        "Document: \"Multimedia Quorum-based Synchronization Protocols. Replicas of multimedia objects are distributed to peers in overlay networks. Multimedia objects are characterized in terms of not only data structure parameter but also quality of service (QoS) parameters like frame rate. Replicas have to be synchronized to be mutually consistent. Each parameter of a replica is read and written by transactions. Thus, the data structure and QoS parameters of a replica are manipulated. There are enriching and impoverishing types of write operations. Some data is added to a replica in an enriching operation and is removed in an impoverishing operation. In the MQB protocol, the physical state of each replica is not always updated. If a transaction issues an enriching operation, every replica in the quorum is updated in the same way as the QB protocol. On the other hand, if an impoverishing write operation is issued, every replica is not updated in the quorum. In order to reduce the processing overhead of each replica, we discuss an extended MQB (EMQB) protocol where every replica is not updated even in an enriching write operation. The EMQB protocol is evaluated in terms of the processing overhead of replicas. We show that the processing overhead of each replica can be reduced in the MQB protocol.\"",
        "Document: \"An Energy-Efficient Dynamic Live Migration of Multiple Virtual Machines. In this paper, we propose an algorithm to migrate virtual machines to reduce the total electric energy consumption of servers. Here, virtual machines are dynamically resumed and suspended so that the number of processes on each virtual machine can be kept fewer. In addition, multiple virtual machines migrate from a host server to a more energy-efficient guest server. In our previous studies, time to migrate virtual machines is assumed to be zero. The more often a virtual machine migrates, the longer time it takes to perform processes on the virtual machine. We propose a model to estimate the electric energy consumption of servers by considering the migration time of each virtual machine. By using the model, virtual machines to migrate and to perform processes are selected so that the total electric energy consumption can be reduced. In the evaluation, we show the total electric energy consumption of servers can be reduced compared with other algorithms.\"",
        "Document: \"A Multi-layered Model for Scalable Group Communication in P2P Overlay Networks. A group of n (1) peers are required to cooperate with each other in distributed applications. In this paper, we consider a distributed group of multiple peers in P2P overlay networks. A P2P group is distributed, i.e. no centralized controller and is scalable and heterogeneous since various types and huge number of computers are interconnected in types of networks. In group communication, messages have to be causally delivered to every peer. In order to realize a scalable group, messages are ordered by linear time (LT) and physical time (PT) since message length is O(1). In order to use the physical time, each peer has to hold information on the accuracy of physical clock of each peer and minimum delay time among every pair of peers. Hence, the size of the information is O (n^2). In this paper, we newly discuss a multi-layered hierarchical model to reduce the size of group information. We evaluate the hierarchical model in terms of the group information size and delay time compared with a flat group model.\"",
        "Document: \"Virtual Machine Migration Algorithms to Reduce Electric Energy Consumption of a Server Cluster. In this paper, we discuss a virtual machine migration approach to reducing the electric energy consumption of servers. In our previous algorithms, one virtual machine migrates from a host server to a guest server. While the electric energy consumption of servers can be reduced by migrating some number b of processes, there might not be a virtual machine with the same number b of processes on a host server. In this paper, we propose an ISEAM2T algorithm where multiple virtual machines can migrate from a host server to a guest server. Here, multiple virtual machines on a host server are selected so that the total number of processes on the virtual machines can be more easily adjusted to the best number b of processes. In the evaluation, we show the total electric energy consumption and active time of the servers and the average execution time of processes can be reduced in the proposed algorithm.\"",
        "Document: \"Serial and Parallel Transmission Models for Multi-source Streaming of Multimedia Objects. In peer-to-peer (P2P) overlay networks, source peers holding multimedia objects can transmit the multimedia objects to receiver peers. In this paper, we discuss parallel types of multi-source streaming (MSS) models where a receiver peer can receive primitive objects of a multimedia object from multiple source peers which can support enough. A multimedia object is realized in a sequence of primitive objects which are units of transmission at the P2P overlay layer. The receiver peer is required to receive primitive objects of the multimedia object with enough QoS, e.g. no primitive object loss. Multiple source peers in parallel send primitive objects to the receiver peer. In this paper, each primitive object is delivered to the receiver peer so that time to buffer the primitive object is reduced. In addition, primitive objects are duplicated by sending parity objects of the primitive objects. Here, the receiver peer can deliver every primitive object even if some primitive object is lost. We evaluate the model in terms of buffering time and delivery continuity in presence of network fault.\"",
        "Document: \"A purpose-based synchronisation protocol of multiple transactions in multi-agent systems. Multiple agents cooperate with each other through manipulating objects. A transaction is a unit of work issued by an agent. A transaction is assigned with a purpose which is a subfamily of roles granted to the agent. Even if transactions issue methods according to the purposes, illegal information flow might occur. We define legal, independent, illegal, and possibly illegal information flow relations among purposes. We discussed the purpose-based marking protocol to prevent illegal information flow. Then, we discussed the releasing mechanism of purpose marks to improve the throughput. Finally, we evaluated the Purpose-based Marking and Releasing (PMR) protocol.\"",
        "Document: \"A Quorum-Based Synchronization Protocol. Multimedia objects are distributed to peers through downloading and caching in peer-to-peer (P2P) overlay networks. In this paper, we consider multimedia objects which are characterized in terms of not only data structure but also quality of service (QoS). For example, there are a pair of replicas oi and oj of a movie object. Here, a content of a replica oi is changed while not in another replica oj. On the other hand, QoS of the replica oj is changed while not in oi. This means, oi is newer than oj with respect to the content but is older than oj with respect to QoS. Thus, replicas of a multimedia object are partially ordered in terms of newness of content and QoS. In traditional quorum-based (QB) protocols theories, replicas are totally ordered just in terms of newness of content. We discuss a multimedia quorum-based (MQB) protocol to synchronize multiple replicas to make consistent on the basis of the newness-precedent relation of replicas. We evaluate the MQB protocol in terms of communication overheads and show the communication overhead can be reduced in the MQB protocol compared with the traditional QB protocols.\"",
        "Document: \"Trustworthiness-Based Broadcast Algorithm in Scalable P2P Group. Nowadays information systems are being shifted to distributed architectures, i.e. Grid and P2P models to obtain the benefits like scalability, autonomy, and fault-tolerance. We consider the peer-to-peer (P2P) model as a fully distributed, scalable system, which is composed of peer processes (peers). A group of multiple peers cooperate with each other. Here, peers have to efficiently and flexibly deliver messages to every peer of the group in P2P overlay networks. In order to efficiently and reliably broadcast messages in a scalable group, we take advantage of the multipoint relaying (MPR) mechanism. Here, each peer sends messages to only a subset of its acquaintances. However, if a peer to forward messages to other peers fails to forward the messages, the peers cannot receive the messages. In this paper, we newly discuss a trustworthiness-based broadcast (TBB) algorithm where only trustworthy peers forward messages. Even if untrustworthy peers do not forward messages, messages can be delivered to every peer. Here, the transmission fault implied by faults of untrustworthy peers can be reduced. We evaluate the TBB algorithm in terms of the number of messages transmitted.\"",
        "1 is \"Federating Diverse Collections of Scientific Literature\", 2 is \"Quorum-Based Replication in Asynchronous Crash-Recovery Distributed Systems (Research Note)\"",
        "Given above information, for an author who has written the paper with the title \"Energy-Efficient Virtualisation of Threads in a Server Cluster.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003276": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Time-Critical Database Scheduling: A Framework For Integrating Real-Time Scheduling and Concurrency Control':",
        "Document: \"Performance evaluation of message-oriented middleware using the SPECjms2007 benchmark. Message-oriented middleware (MOM) is at the core of a vast number of financial services and telco applications, and is gaining increasing traction in other industries, such as manufacturing, transportation, health-care and supply chain management. Novel messaging applications, however, pose some serious performance and scalability challenges. In this paper, we present a methodology for performance evaluation of MOM platforms using the SPECjms2007 standard benchmark. SPECjms2007 is based on a novel application in the supply chain management domain, designed to stress MOM infrastructures in a manner representative of real-world applications. In addition to providing a standard workload and metrics for MOM performance, the benchmark provides a flexible performance analysis framework that allows users to tailor the workload to their requirements. The contributions of this paper are: (i) we present a detailed workload characterization of SPECjms2007 with the goal to help users understand the internal components of the workload and the way they are scaled, (ii) we show how the workload can be customized to exercise and evaluate selected aspects of MOM performance, (iii) we present a case study of a leading JMS platform, the BEA WebLogic server, conducting an in-depth performance analysis of the platform under a number of different workload and configuration scenarios. The methodology we propose is the first one that uses a standard benchmark, providing both a representative workload as well as the ability to customize it to evaluate the features of MOM platforms selectively.\"",
        "Document: \"Making cost-based query optimization asymmetry-aware. The architecture and algorithms of database systems have been built around the properties of existing hardware technologies. Many such elementary design assumptions are 20--30 years old. Over the last five years we witness multiple new I/O technologies (e.g. Flash SSDs, NV-Memories) that have the potential of changing these assumptions. Some of the key technological differences to traditional spinning disk storage are: (i) asymmetric read/write performance; (ii) low latencies; (iii) fast random reads; (iv) endurance issues. Cost functions used by traditional database query optimizers are directly influenced by these properties. Most cost functions estimate the cost of algorithms based on metrics such as sequential and random I/O costs besides CPU and memory consumption. These do not account for asymmetry or high random read and inferior random write performance, which represents a significant mismatch. In the present paper we show a new asymmetry-aware cost model for Flash SSDs with adapted cost functions for algorithms such as external sort, hash-join, sequential scan, index scan, etc. It has been implemented in PostgreSQL and tested with TPC-H. Additionally we describe a tool that automatically finds good settings for the base coefficients of cost models. After tuning the configuration of both the original and the asymmetry-aware cost model with that tool, the optimizer with the asymmetry-aware cost model selects faster execution plans for 14 out of the 22 TPC-H queries (the rest being the same or negligibly worse). We achieve an overall performance improvement of 48% on SSD.\"",
        "Document: \"Automating the Development of Web Service Compositions Using Templates. The development of Web Service compositions has not yet been automated. Web Service-based process definitions can be created automatically using Web Service compositions templates. Templates are units of code and functionality reuse. We argue that templates can be used to implement coordination protocols roles, design patterns, algorithms and domain specific business processes. We also discuss the implications of this approach. The success of this approach depends mainly on the existence of appropriate tools.\"",
        "Document: \"Extending BPEL for Run Time Adaptability. The existing Web service flow (WS-flow) technologies enable both static and dynamic binding of participating Web services (WSs) on the process model level. Adaptability on per-instance basis is not sufficiently supported and therefore must be addressed to improve process flexibility upon changes in the environment. Ad-hoc process instance changes can be enabled by swapping participating WS instances, by modifying port types of the partners to be invoked, and by changing process logic. In this work, we address the problem of dynamic binding of WSs to WS-flow instances at run time, i.e. the ability to exchange a WS instance participating in a WS-flow instance with an alternative one. The problem is additionally complicated by the fact that the execution of a process depends on its deployment. We describe the \"find and bind\" mechanism, and we show its representation as a BPEL extension. We discuss the benefits that could be gained and the disadvantages it brings in. The mechanism extends and improves the existing process technologies. It facilitates a precisely controlled policy-based selection of WSs at run time and also provides for process instance repair, while maintaining simplicity. We also discuss a prototypical implementation of the presented functionality.\"",
        "Document: \"SI-CV: snapshot isolation with co-located versions. Snapshot Isolation is an established concurrency control algorithm, where each transaction executes against its own version/snapshot of the database. Version management may produce unnecessary random writes. Compared to magnetic disks Flash storage offers fundamentally different IO characteristics, e.g. excellent random read, low random write performance and strong read/write asymmetry. Therefore the performance of snapshot isolation can be improved by minimizing the random writes. We propose a variant of snapshot isolation (called SI-CV) that collocates tuple versions created by a transaction in adjacent blocks and therefore minimizes random writes at the cost of random reads. Its performance, relative to the original algorithm, in overloaded systems under heavy transactional loads in TPC-C scenarios on Flash SSD storage increases significantly. At high loads that bring the original system into overload, the transactional throughput of SI-CV increases further, while maintaining response times that are multiple factors lower.\"",
        "Document: \"A Prototype for Metadata-Based Integration of Internet Sources. The combination of semistructured data from different sources on the Internet often fails because of syntactic and semantic differences. The resolution of these heterogeneities requires explicit context information in the form of metadata. We give a short overview of a representation model that is well suited for the explicit description of semistructured data, and show how it is used as the basis of a prototype for metadata-driven integration of heterogeneous data extracted from Web-pages.\"",
        "Document: \"Active Object-Relational Mediators. The paper describes an active abject-oriented mediator for the enforcement of global consistency between relational legacy databases. The authors discuss the problem of integrating several local relational systems into a federated system by the usage of an object-oriented mediator system. They explore how relational DBMSs can be enhanced to signal local updates that may violate global constraints without sacrificing too much autonomy and present a database gateway for detection, logging and signalling. They show how the gateway is embedded into the architecture of an object-relational mediator system. They give a solution to the problem of mapping SQL commands to method calls in a C++ based system using a so-called mediator generator. Furthermore, they discuss how the federated system can be enriched by rule mechanisms that make the mediator behave actively\"",
        "Document: \"Building a case for FIPA compliant multiagent approaches for wireless sensor networks. There have been some efforts to apply agent based approaches in Wireless Sensor Networks (WSNs) for different application scenarios in recent past. These efforts have met with varying levels of success so far. However, the efforts to assess the suitability of Foundation for Intelligent Physical Agents (FIPA) compliant multiagent solutions for WSNs are almost non-existent. In this paper we present a scenario from an industrial domain and asses the suitability of a FIPA compliant multiagent based solution for it. The scenario is based on a WSN deployment in an underground mine where it could be put to use for several purposes like hazardous gas plume detection and tracking, mine lighting control, Proximate Environment Monitoring (PEM), structural health monitoring of mine structure, to name a few. However, we only consider the PEM usage scenario in this paper.\"",
        "Document: \"An architecture and data model for CAD databases. An architecture for a process-plant CAD system is presented to serve as the framework for the discussion of a data model based on the notion of molecular aggregation. Structures, operations and constraint handling mechanisms are introduced.\"",
        "Document: \"Infrastructure for Smart Cities: The Killer Application for Event-Based Computing. Smart cities are part of the Ambient Intelligence vision that foresees the vanishing of computational devices into the fabric of society and the ubiquitous availability of intelligent services in support of our daily lives. In this vision we should not be burdened by conscious manipulation of devices and ever more powerful but also complex interfaces. Instead, devices should be able to communicate and cooperate and support us in a proactive manner. For this vision to become true, an infrastructure is needed that can provide for the seamless provisioning of intelligent, pervasive, context-aware services throughout all the domains of our daily lives. As we move in our daily activities from the home to means of transportation and public spaces, to spaces for work and leisure, the infrastructure must be capable of supporting us in a variety of places, situations and contexts. Accordingly, the notion of context must be very rich and must encompass a deep understanding of the user and his or her needs and preferences, as well as the availability of resources in a dynamically reconfiguring environment. The spontaneous interaction of devices and the wealth of sensors providing data and detecting events make the traditional computing paradigms based on simple request/reply or point to point messaging inadequate. Instead, event-driven processing is needed, in which producers and consumers of events are not aware of each other's existence and events are delivered to interested parties by a broker (network) and the corresponding notification mechanisms (1). Event services are a crucial part of the infrastructure for smart cities. The event composition and management service will be responsible for defining, detecting, composing and managing events. Simple events will be collected from sensors, positioning devices, and other event sources. Simple events may either be consumed directly by reactive components or may be filtered and composed. Event algebras are needed for defining valid event compositions. Both graph based and query based composition mechanisms will be used. The event management service is also responsible for storing and managing the event definitions and event state. Stateful event composition is needed, whenever complex event patterns for which events may have to be collected over longer time intervals must be analyzed. The complexity of the application stems from several factors that compound each other: \u2022 The rich set of events. Events range from simple sensor readings, different kinds of aggregations, to complex events, such as traffic patterns, temporal events (both physical and logical time), positioning and other types of context-sensitive events. \u2022 Extremely large numbers of sensors and mobile devices. Even for a medium sized city with only 100 000 inhabitants, each with his/her own positioning and authentication device, several personal computing and communication devices ranging from cell-phones to PDAs and laptops, multiple processors in such smart devices as glasses or health monitoring devices, multiple processors in each car interacting with the environment, etc., the total number of devices will be in the millions. \u2022 Heterogeneity of devices. The broad range of capabilities characterizing the devices, ranging from resource rich to resource frugal, from stationary to mobile, from high- bandwidth wired to low-bandwidth wireless communication, from devices with stationary power supply to power constrained battery-driven devices requires a variety\"",
        "1 is \"Presto: distributed machine learning and graph processing with sparse matrices\", 2 is \"Complex data types and a data manipulation language for scientific and statistical databases\"",
        "Given above information, for an author who has written the paper with the title \"Time-Critical Database Scheduling: A Framework For Integrating Real-Time Scheduling and Concurrency Control\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003277": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'An Optimal-Storage Approach to Semidefinite Programming Using Approximate Complementarity':",
        "Document: \"Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions. Low-rank matrix approximations, such as the truncated singular value decomposition and the rank-revealing QR decomposition, play a central role in data analysis and scientific computing. This work surveys and extends recent research which demonstrates that randomization offers a powerful tool for performing low-rank matrix approximation. These techniques exploit modern computational architectures more fully than classical methods and open the possibility of dealing with truly massive data sets. This paper presents a modular framework for constructing randomized algorithms that compute partial matrix decompositions. These methods use random sampling to identify a subspace that captures most of the action of a matrix. The input matrix is then compressed\u2014either explicitly or implicitly\u2014to this subspace, and the reduced matrix is manipulated deterministically to obtain the desired low-rank factorization. In many cases, this approach beats its classical competitors in terms of accuracy, robustness, and/or speed. These claims are supported by extensive numerical experiments and a detailed error analysis. The specific benefits of randomized techniques depend on the computational environment. Consider the model problem of finding the $k$ dominant components of the singular value decomposition of an $m \\times n$ matrix. (i) For a dense input matrix, randomized algorithms require $\\bigO(mn \\log(k))$ floating-point operations (flops) in contrast to $ \\bigO(mnk)$ for classical algorithms. (ii) For a sparse input matrix, the flop count matches classical Krylov subspace methods, but the randomized approach is more robust and can easily be reorganized to exploit multiprocessor architectures. (iii) For a matrix that is too large to fit in fast memory, the randomized techniques require only a constant number of passes over the data, as opposed to $\\bigO(k)$ passes for classical algorithms. In fact, it is sometimes possible to perform matrix approximation with a single pass over the data.\"",
        "Document: \"Random Filters For Compressive Sampling And Reconstruction. We propose and study a new technique for efficiently acquiring and reconstructing signals based on convolution with a fixed FIR filter having random taps. The method is designed for sparse and compressible signals, i.e., ones that are well approximated by a short linear combination of vectors from an orthonormal basis. Signal reconstruction involves a non-linear Orthogonal Matching Pursuit algorithm that we implement efficiently by exploiting the nonadaptive, time-invariant structure of the measurement process. While simpler and more efficient than other random acquisition techniques like Compressed Sensing, random filtering is sufficiently generic to summarize many types of compressible signals and generalizes to streaming and continuous-time signals. Extensive numerical experiments demonstrate its efficacy for acquiring and reconstructing signals sparse in the time, frequency, and wavelet domains, as well as piecewise smooth signals and Poisson processes.\"",
        "Document: \"Time-Data Tradeoffs by Aggressive Smoothing. This paper proposes a tradeoff between sample complexity and computation time that applies to statistical estimators based on convex optimization. As the amount of data increases, we can smooth optimization problems more and more aggressively to achieve accurate estimates more quickly. This work provides theoretical and experimental evidence of this tradeoff for a class of regularized linear inverse problems.\"",
        "Document: \"The achievable performance of convex demixing.   Demixing is the problem of identifying multiple structured signals from a superimposed, undersampled, and noisy observation. This work analyzes a general framework, based on convex optimization, for solving demixing problems. When the constituent signals follow a generic incoherence model, this analysis leads to precise recovery guarantees. These results admit an attractive interpretation: each signal possesses an intrinsic degrees-of-freedom parameter, and demixing can succeed if and only if the dimension of the observation exceeds the total degrees of freedom present in the observation. \"",
        "Document: \"Restricted Isometries for Partial Random Circulant Matrices. In the theory of compressed sensing, restricted isometry analysis has become a standard tool for studying how efficiently a measurement matrix acquires information about sparse and compressible signals. Many recovery algorithms are known to succeed when the restricted isometry constants of the sampling matrix are small. Many potential applications of compressed sensing involve a data-acquisition process that proceeds by convolution with a random pulse followed by (nonrandom) subsampling. At present, the theoretical analysis of this measurement technique is lacking. This paper demonstrates that the sth-order restricted isometry constant is small when the number m of samples satisfies m\u2273(slogn)3/2, where n is the length of the pulse. This bound improves on previous estimates, which exhibit quadratic scaling.\"",
        "Document: \"Algorithms for simultaneous sparse approximation: part I: Greedy pursuit. A simultaneous sparse approximation problem requests a good approximation of several input signals at once using different linear combinations of the same elementary signals. At the same time, the problem balances the error in approximation against the total number of elementary signals that participate. These elementary signals typically model coherent structures in the input signals, and they are chosen from a large, linearly dependent collection.The first part of this paper proposes a greedy pursuit algorithm, called simultaneous orthogonal matching pursuit (S-OMP), for simultaneous sparse approximation. Then it presents some numerical experiments that demonstrate how a sparse model for the input signals can be identified more reliably given several input signals. Afterward, the paper proves that the S-OMP algorithm can compute provably good solutions to several simultaneous sparse approximation problems.The second part of the paper develops another algorithmic approach called convex relaxation, and it provides theoretical results on the performance of convex relaxation for simultaneous sparse approximation.\"",
        "Document: \"Sparse Approximation Via Iterative Thresholding. The well-known shrinkage technique is still relevant for con- temporary signal processing problems over redundant dictio- naries. We present theoretical and empirical analyses for two iterative algorithms for sparse approximation that use shrink- age. The GENERAL IT algorithm amounts to a Landweber iteration with nonlinear shrinkage at each iteration step. The BLOCK IT algorithm arises in morphological components anal- ysis. A sufficient condition for which General IT exactly re- covers a sparse signal is presented, in which the cumulative coherence function naturally arises. This analysis extends previous results concerning the Orthogonal Matching Pursuit (OMP) and Basis Pursuit (BP) algorithms to IT algorithms. We provide a sufficient condition for which guarantees that GENERAL IT recovers exactly sparse signals. This suf- ficient condition matches the sufficient geometric conditions for the Orthogonal Matching Pursuit (OMP) and Basis Pur- suit (BP) algorithms. We also provide analysis of the fixed points of the BLOCK IT algorithm. In the following section we make rigorous the concepts that arise in sparse approxi- mation problems and define two iterative thresholding algo- rithms. We then provide the theoretical analysis of these al- gorithms, followed by a discussion of the main result of the article: a sufficient condition guaranteeing the recovery of ex- actly sparse signals. The paper concludes with a study of the empirical performance of each algorithm.\"",
        "Document: \"Concentration of the Intrinsic Volumes of a Convex Body. The intrinsic volumes are measures of the content of a convex body. This paper applies probabilistic and information-theoretic methods to study the sequence of intrinsic volumes. The main result states that the intrinsic volume sequence concentrates sharply around a specific index, called the central intrinsic volume. Furthermore, among all convex bodies whose central intrinsic volume is fixed, an appropriately scaled cube has the intrinsic volume sequence with maximum entropy.\"",
        "Document: \"An Introduction to Matrix Concentration Inequalities. AbstractRandom matrices now play a role in many areas of theoretical, applied,and computational mathematics. Therefore, it is desirable to have toolsfor studying random matrices that are flexible, easy to use, and powerful.Over the last fifteen years, researchers have developed a remarkablefamily of results, called matrix concentration inequalities, that achieveall of these goals.This monograph offers an invitation to the field of matrix concentrationinequalities. It begins with some history of random matrix theory;it describes a flexible model for random matrices that is suitablefor many problems; and it discusses the most important matrix concentrationresults. To demonstrate the value of these techniques, thepresentation includes examples drawn from statistics, machine learning,optimization, combinatorics, algorithms, scientific computing, andbeyond.\"",
        "Document: \"The sparsity gap: Uncertainty principles proportional to dimension. In an incoherent dictionary, most signals that admit a sparse representation admit a unique sparse representation. In other words, there is no way to express the signal without using strictly more atoms. This work demonstrates that sparse signals typically enjoy a higher privilege: each nonoptimal representation of the signal requires far more atoms than the sparsest representation-unless it contains many of the same atoms as the sparsest representation. One impact of this finding is to confer a certain degree of legitimacy on the particular atoms that appear in a sparse representation. This result can also be viewed as an uncertainty principle for random sparse signals over an incoherent dictionary.\"",
        "1 is \"Optimal Primal-Dual Methods for a Class of Saddle Point Problems.\", 2 is \"An equivalence between sparse approximation and support vector machines\"",
        "Given above information, for an author who has written the paper with the title \"An Optimal-Storage Approach to Semidefinite Programming Using Approximate Complementarity\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003283": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Ant-Based Clustering in Delta Episode Information Systems Based on Temporal Rough Set Flow Graphs':",
        "Document: \"A constructive feature induction mechanism founded on evolutionary strategies with fitness functions generated on the basis of decision trees. In the paper, we present a novel approach to calculating a new (extra) attribute (feature) using a constructive feature induction mechanism. The problem being solved is founded on coefficients for values of existing attributes determined empirically using evolutionary strategies with fitness functions based on parameters calculated from decision trees generated for extended decision tables.\"",
        "Document: \"Ant based clustering of MMPI data: an experimental study. Our research concerns psychometric data coming from the Minnesota Multiphasic Personality Inventory (MMPI) test. MMPI is one of the most frequently used in clinical mental health personality tests as well as psychopathology (mental and behavioral disorders). We are developing the Copernicus system. It is a tool for the analysis of MMPI profiles of patients with mental disorders. In this system, there have been selected and implemented different quantitative groups of methods useful for differential interprofile diagnosis. In the paper, we investigate clustering of MMPI data using one of nature-inspired heuristic approaches - ant based clustering. For this approach, we test usefulness of different dissimilarity measures of MMPI profiles both standard and those defined in the professional domain literature.\"",
        "Document: \"A Rough Set Approach to Information Systems Decomposition. The aim of this paper is to present the methods and algorithms of information systems decomposition. In the paper, decomposition with respect to reducts and the so-called global decomposition are considered. Moreover, coverings of information systems by components are discussed. An essential difference between two kinds of decomposition can be observed. In general, global decomposition can deliver more components of a given information system. This fact can be treated as some kind of additional knowledge about the system. The proposed approach is based on rough set theory. To demonstrate the usefulness of this approach, we present an illustrative example coming from the economy domain. The discussed decomposition methods can be applied e.g. for design and analysis of concurrent systems specified by information systems, for automatic feature extraction, as well as for control design of systems represented by experimental data tables.\"",
        "Document: \"On consistent and partially consistent extensions of information systems. Consistent extensions of information systems have been earlier considered in the literature. Informally, a consistent extension of a given information system includes only such objects corresponding to known attribute values which are consistent with the whole knowledge represented by rules extracted from the information system. This paper presents a new look at consistent extensions of information systems focusing mainly on partially consistent extensions and broadening the approach proposed earlier by Z. Suraj. To this end, a notion of a partially consistent extension of an information system is introduced. The meaning, properties and application examples of such extensions are given. In the approach presented, we admit the situation that some objects in an extension are consistent only with a part of the knowledge extracted from the information system. We show how a factor of consistency with the original knowledge for a given object from an extension can be computed. Some coefficients concerning rules in information systems are defined to compute a factor of consistency. The notions presented are crucial for solving different problems. An example is given in the paper to show an application of the proposed approach in modelling of concurrent systems described by information systems.\"",
        "Document: \"Some Remarks on Complex Information Systems over Ontological Graphs. In the paper, we consider information systems, from the Pawlak's perspective, as the knowledge representation systems. In our approach, we develop complex information systems over ontological graphs in which attribute values are local ontological graphs of ontologies assigned to attributes. Basic notions and some properties of such systems are considered analogously to those known from classic information systems in rough set theory.\"",
        "Document: \"Classification of speech signals through ant based clustering of time series. Classification of speech signals in a time domain can be made through a clustering process of time windows into which examined speech signals are divided. Disturbances in speech signals of patients having some problems with the voice organ cause some difficulties in formation of coherent clusters of similar time windows. A quality of a clustering process result can be used as an indicator of non-natural disturbances in articulation of selected phonemes by patients. In the paper, we describe a procedure based on this fact. A special ant based algorithm is used to cluster time windows being time series. In this algorithm, a new local function, formulas for picking and dropping decisions as well as some additional operations are implemented to adjust the clustering process to a classification ability.\"",
        "Document: \"Towards an Object-Oriented Programming Language for Physarum Polycephalum Computing: A Petri Net Model Approach. Our research is focused on creation of a new object-oriented programming language for Physarum polycephalum computing. Physarum polycephalum is a one-cell organism that can be used for developing a biological architecture of different abstract devices, among others, the digital ones. In the paper, we use an abstract graphical language in the form of Petri nets to describe the Physarum polycephalum behavior. Petri nets are a good formalism to assist designers and support hardware design tools, especially in developing concurrent systems. At the beginning stage considered in this paper, we show how to build Petri net models, and next implement them as Physarum polycephalum machines, of basic logic gates AND, OR, NOT, and simple combinational circuits on the example of the 1-to-2 demultiplexer.\"",
        "Document: \"Belief networks in classification of laryngopathies based on speech spectrum analysis. The paper is devoted to classification of laryngopathies on the basis of a family of coefficients reflecting spectrum disturbances around basic tones and their multiples in patients' speech signals. In experiments, a special computer tool called BeliefSEEKER is tested. BeliefSEEKER is capable to generate belief networks and also to generate sets of belief rules. The paper presents feature selection and classification mechanisms as well as the experiments carried out on real-life data.\"",
        "Document: \"A petri net system: an overview. Petri nets are one of well established tools in both theoretical analysis and practical modelling of concurrent systems as well as approximate reasoning. However, practical usage of Petri nets is limited by the lack of computer tools which would allow to handle large and complex nets in a comfortable way. Three things are essential for modelling and analyzing by means of Petri nets - a good editor, a simulator and a powerful analysis engine. Moreover, a program should have a graphical user interface providing an opportunity to work directly with the graphical representations of Petri nets and should be able to read and write data in formats of other popular simulators of Petri Nets.This paper presents a set of integrated graphical Petri net tools called Petri Net system (PN-system, in short).PN-system is a follow - up on PN-tools. This system can be used for constructing, editing and analyzing of different classes of Petri nets. PN-system is enhanced with fuzzy and adaptive fuzzy Petri nets' modules which allow to perform fuzzy reasoning automatically. It has got a graphical user interface. Moreover, PN-system can cooperate with the ROSECON system which is an original software tool for discovering concurrent models from data tables.PN-system runs on IBM PC platform under MS Windows operating system.\"",
        "Document: \"Experiments with hybridization and optimization of the rules knowledge base for classification of MMPI profiles. In the paper, we investigate a problem of hybridization and optimization of the knowledge base for the Copernicus system. Copernicus is a tool for computer-aided diagnosis of mental disorders based on personality inventories. Currently, Copernicus is used to analyze and classify patients' profiles obtained from the Minnesota Multiphasic Personality Inventory (MMPI) test. The knowledge base embodied in the Copernicus system consists of, among others, classification functions, classification rule sets as well as nosological category patterns. A special attention is focused on selection of a suitable set of rules classifying new cases. In experiments, rule sets have been generated by different data mining tools and have been optimized by generic operations implemented in the RuleSEEKER system.\"",
        "1 is \"Transformations and decompositions of nets\", 2 is \"Minimal shape and intensity cost path segmentation.\"",
        "Given above information, for an author who has written the paper with the title \"Ant-Based Clustering in Delta Episode Information Systems Based on Temporal Rough Set Flow Graphs\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003482": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Exploiting Regression Trees as User Models for Intent-Aware Multi-attribute Diversity.':",
        "Document: \"Location-Based Semantic Matchmaking in Ubiquitous Computing. Ever increasing efforts are spent in developing techniques and tools for a full exploitation of semantics in mobile environments, able to overcome volatility and resource limitations of mobile contexts. This paper presents a platform-independent mobile semantic discovery framework as well as a working prototypical implementation which enables advanced knowledge-based services taking into account user\u2019s location. The proposed approach is clarified and motivated in a ubiquitous tourism case study, where some evaluations are presented to prove its feasibility and usefulness.\"",
        "Document: \"Structured knowledge representation for image retrieval. We propose a structured approach to the problem of retrieval of images by content and present a description logic that has been devised for the semantic indexing and retrieval of images containing complex objects. As other approaches do, we start from low-level features extracted with image analysis to detect and characterize regions in an image. However, in contrast with feature-based approaches, we provide a syntax to describe segmented regions as basic objects and complex objects as compositions of basic ones. Then we introduce a companion extensional semantics for defining reasoning services, such as retrieval, classification, and subsumption. These services can be used for both exact and approximate matching, using similarity measures. Using our logical approach as a formal specification, we implemented a complete client-server image retrieval system, which allows a user to pose both queries by sketch and queries by example. A set of experiments has been carried out on a testbed of images to assess the retrieval capabilities of the system in comparison with expert users ranking. Results are presented adopting a well-established measure of quality borrowed from textual information retrieval.\"",
        "Document: \"Adaptive multi-attribute diversity for recommender systems. Providing very accurate recommendations to end users has been nowadays recognized to be just one of the tasks an effective recommender system should accomplish. While predicting relevant suggestions, attention needs to be paid also to their diversification in order to avoid monotony in the returned list of recommendations. In this paper we focus on modeling user propensity toward selecting diverse items, where diversity is computed by means of content-based item attributes. We then exploit such modeling to present a novel approach to re-arrange the list of Top-N items predicted by a recommendation algorithm, with the aim of fostering diversity in the final ranking. An extensive experimental evaluation proves the effectiveness of the proposed approach as well as its ability to improve also novelty and catalog coverage values.\"",
        "Document: \"A Tableaux-based Method for Computing Least Common Subsumers for Expressive Description Logics. Least Common Subsumers (LCS) have been pro- posed in Description Logics (DL) to capture the commonalities between two or more concepts. Since its introduction in 1992, LCS have been suc- cessfully employed as a logical tool for a variety of applications, spanning from inductive learning, to bottom-up construction of knowledge bases, infor- mation retrieval, to name a few. The best known algorithm for computing LCS uses structural com- parison on normal forms, and the most expressive DL it is applied to is ALEN . We provide a gen- eral tableau-based calculus for computing LCS, via substitutions on concept terms containing concept variables. We show the applicability of our method to an expressive DL (but without disjunction and full negation), discuss complexity issues, and show the generality of our proposal.\"",
        "Document: \"Ontology Driven Resource Discovery in Bluetooth Based M-Marketplace. We present a semantic-based approach to resource retrieval in an m-commerce scenario. We enhance the original Bluetooth Service Discovery Protocol by integrating a \"semantic layer\" within the application level of the standard. Given a user request, this layer makes possible a matchmaking process exploiting the semantics of the resources descriptions exposed by a hotspot. It computes the match degree between the request and the goods available i n the m-marketplace taking into account both their ontology-based descriptions and \"classical\" attributes such as price difference, availability, quantity.\"",
        "Document: \"Feature integration and relevance feedback analysis in image similarity evaluation. In this article we describe the results of a study on similarity evaluation in image retrieval using color, object orientation, and relative position as content features, in a framework oriented to image repositories where the semantics of stored images are limited to a specific domain. The focus is not on a complete description of image content, which is supposed to be known to some extent, but on the extraction of simple and immediate features that can assure, through their combination, automated image analysis and efficient retrieval. Relevance feedback is introduced as an effective way to improve retrieval accuracy. A simple prototype system is also introduced that competes feature descriptors and allows users to enter queries, browse the retrieved images, and refine the results through relevance feedback analysis. (C) 1998 SPIE and IS&T. [S1017-9909(98)00502-9].\"",
        "Document: \"A semantic-based registry enabling discovery, composition and substitution of pervasive services. In this paper we present a semantic-enhanced registry specifically devised for pervasive environments, able to cope with automated mobile service discovery and composition, compliant with OWL-S and with Semantic Web technologies. The proposed approach also deals with non-exact matches (computing an approximate result) and implements a dynamic substitution of services, especially useful in highly unpredictable contexts. It has been implemented and tested in a home/office automation case study, and we also report on experimental results.\"",
        "Document: \"Knowledge-Aware Autoencoders For Explainable Recommender Systems. Recommender Systems have been widely used to help users in finding what they are looking for thus tackling the information overload problem. After several years of research and industrial findings looking after better algorithms to improve accuracy and diversity metrics, explanation services for recommendation are gaining momentum as a tool to provide a human-understandable feedback to results computed, in most of the cases, by black-box machine learning techniques. As a matter of fact, explanations may guarantee users satisfaction, trust, and loyalty in a system. In this paper, we evaluate how different information encoded in a Knowledge Graph are perceived by users when they are adopted to show them an explanation. More precisely, we compare how the use of categorical information, factual one or a mixture of them both in building explanations, affect explanatory criteria for a recommender system. Experimental results are validated through an A/B testing platform which uses a recommendation engine based on a Semantics-Aware Autoencoder to build users profiles which are in turn exploited to compute recommendation lists and to provide an explanation.\"",
        "Document: \"I-Search: A System for Intelligent Information Search on the Web. Current Web search engines find new documents basically crawling the hyperlinks with the aid of spider agents. Nevertheless, when indexing newly discovered documents they revert to conventional information retrieval models and single-document indexing, thus neglecting the inherently hypertextual structure of Web documents. Therefore, it can happen that a query string, partially present in a document, with the remaining part available in a linked document on the same site, does not correspond to a hit. This considerably reduces retrieval effectiveness. To overcome this and other limits we propose an approach based on temporal logic that, starting with the modeling of a web site as a finite state graph, allows one to define complex queries over hyperlinks with the aid of Computation Tree Logic (CTL) operators. Query formulation is composed by two steps: the first one is user-oriented and provides a user with a friendly interface to pose queries. The second step is the query translation in CTL formulas. The formulation of the query is not visible to the user that simply expresses his/her requirements in natural language. We implemented the proposed approach in a prototype system. Results of experiments show an improvement in retrieval effectiveness.\"",
        "Document: \"Partial and Informative Common Subsumers in Description Logics. Least Common Subsumers in Description Logics have shown their usefulness for discovering commonalities among all concepts of a collection. Several applications are nevertheless focused on searching for properties shared by significant portions of a collection rather than by the collection as a whole. Actually, this is an issue we faced in a real case scenario that provided initial motivation for this study, namely the process of Core Competence extraction in knowledge intensive companies. The paper defines four reasoning services for the identification of meaningful common subsumers describing partial commonalities in a collection. In particular Common Sub-sumers adding informative content to the Least Common Subsumer are investigated, with reference to different DLs.\"",
        "1 is \"Using ontological and document similarity to estimate museum exhibit relatedness\", 2 is \"Decoding wikipedia categories for knowledge acquisition\"",
        "Given above information, for an author who has written the paper with the title \"Exploiting Regression Trees as User Models for Intent-Aware Multi-attribute Diversity.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003493": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Aggregation Time Control Algorithm for Time constrained Data Delivery in Wireless Sensor Networks':",
        "Document: \"Parameter Region for the Proper Operation of the IEEE 802.2 LLC Type 3 Protocol: A Petri Net Approach. This paper derives the parameter region of the IEEE 802.2 LLC type 3 protocol to guarantee a proper operation. The protocol\n is modeled by a time Petri net and investigated using reachability analysis. Three necessary conditions on the parameters\n are derived, and then a reduced reachability graph is obtained by applying the necessary conditions to lessen the combinatorial\n state explosion. From the reduced reachability graph, a necessary and sufficient condition on the parameters is derived to\n guarantee the proper operation of the LLC type 3 protocol for a link. By using the condition, a procedure to set the parameters\n of the LLC type 3 protocol is provided at each station in a network.\n \"",
        "Document: \"Packet Error Rate Analysis Of Ieee 802.11b Under Ieee 802.15.4 Interference. This paper presents an interference model of IEEE 802.11b wireless local area network (WLAN) affected by IEEE 802.15.4 wireless personal area network (WPAN). The packet error rate (PER) of the IEEE 802.11b under the interference of the IEEE 802.15.4 is analyzed, and is obtained by the bit error rate (BER) and the collision time. The safe distance ratio can be obtained from the PER. Further, this paper suggests a packet length to reduce the effect of the IEEE 802.15.4 interference and obtain a maximum throughput of the IEEE 802.11b. The analytic results are validated using the simulation.\"",
        "Document: \"A receding horizon Kalman FIR filter for discrete time-invariant systems. A receding horizon Kalman FIR filter is presented that combines the Kalman filter and the receding horizon strategy when the horizon initial state is assumed to be unknown. The suggested filter is a FIR filter form which has many good inherent properties. It can always be defined irrespective of singularity problems caused by unknown information about the horizon initial state. The suggested filter can be represented in either an iterative form or a standard FIR form. It is also shown that the suggested filter possesses the unbiasedness property and the remarkable deadbeat property irrespective of any horizon initial condition. The validity of the suggested filter is illustrated by numerical examples\"",
        "Document: \"Architectural design of an RISC processor for programmable logic controllers. In this paper, an architecture of the RISC processor for programmable logic controllers is proposed. Execution characteristics of relay ladder logic (RLL) are analyzed with various application programs in order to determine an optimal architecture for programmable logic controllers (PLCs). A conditional execution mechanism is developed to prevent pipeline hazards caused by the inherent execution behavior of RLL. Instruction sets of three different architectural models are defined. Translators, assemblers, and simulators are developed for three models to evaluate performance and to choose an optimal architecture for PLCs. The proposed processor, which has an accumulator architecture with a four-stage pipeline, exhibits desirable performance much higher than that of recent commercial PLCs.\"",
        "Document: \"Minimax FIR smoothers for deterministic continuous-time state space models. In order to design a smoother for a deterministic continuous-time state space model, a new performance criterion is proposed, which is given by a ratio of the current estimation error to the weighted energy of the deterministic disturbance applied during the recent finite horizon. Among smoothers with the deadbeat property and finite impulse response (FIR) structure, a minimax FIR smoother (MFS) is obtained to optimize the proposed performance criterion. To begin with, the functional optimization problem is formulated with respect to kernel functions of the MFS and then its solution is explicitly presented. The MFS depends only on inputs and outputs on the finite recent horizon, and is independent of any a priori state information. The MFS is first represented in an integral form for simple representation and then a differential form is introduced for efficient numerical computation. As in H\u221e IIR smoothing and H2 IIR filtering, it is shown that the proposed MFS for a deterministic system can be interpreted as the minimum variance unbiased FIR smoother for a stochastic system.\"",
        "Document: \"Dynamic delay-constrained minimum-energy dissemination in wireless sensor networks. Disseminating data generated by sensors to users is one of useful functions of sensor networks. In probable real-time applications of sensor networks, multiple mobile users should receive data within their end-to-end delay constraint. In this paper, we propose a dynamic DElay-constrained minimum-Energy Dissemination (DEED) scheme. A dissemination tree (d-tree) is updated in a distributed way without regenerating the tree from scratch, such that energy consumption of the tree is minimized while satisfying end-to-end delay constraints. The d-tree is adjusted using delay estimation based on geometric distance. DEED increases the probability that packets arrive at users within an upper-bound end-to-end delay (UBED) and minimizes energy consumption in both building the d-tree and disseminating data to mobile sinks. Evaluation results show that DEED makes each node consume small energy resources and maintains fewer UBED misses when compared to Directed Diffusion and other baselines for sensor networks.\"",
        "Document: \"Event-based modeling and control for the burnthrough point in sintering processes. This paper treats modeling and control for the burnthrough point in industrial sintering processes. First, a simple state-space model for event-time dynamics is derived for a complicated system by introducing events into a continuous variable system. For the control of the sintering process, a two-stage control policy is used. First, optimal interevent time is obtained from an output-constrained receding horizon control with a least-square prediction algorithm. Then the average strand speeds for the interevent times are obtained and applied as a command signal to a motor control system. It is proved that the proposed output-constrained receding horizon control law with a short horizon stabilizes the closed-loop system and the steady-state error for a set-point becomes zero. The real-time experiments are carried out in a POSCO (Pohang Steel Company, Korea) sintering plant and satisfactory results are presented in this paper. Also, computer simulations are carried out and compared with the real-time experiments.\"",
        "Document: \"LMS finite memory estimators for discrete-time state space models. In this paper, a least-mean-squares (LMS) finite memory (FM) estimator for a stochastic discrete-time state space model is obtained by taking the conditional expectation of the estimated state given a finite number of inputs and outputs measured on the recent finite horizon. Any a priori state information is not involved and any arbitrary constraints are not imposed. For a general discrete-time state space model with both system and measurement noises, the LMS FM estimator is represented in a closed-form. It turns out that the proposed LMS FM estimator has the unbiased property and the linear structure with respect to inputs and outputs on the recent finite horizon.\"",
        "Document: \"A Continuous-Time Recursive Fixed-Lag Smoother Converging in Finite Time. In this technical note, we propose a new fixed-lag smoother that estimates the fixed-delayed state for a stochastic continuous-time system. The estimation error variance of the proposed smoother is minimized under the constraint that the estimated state converges to the real state exactly in finite time after noises or uncertainties disappear. For numerical computing, the proposed smoother is represented in a recursive form. Unlike other approaches, any additional processes such as batch processing and sampling data through discrete-time techniques are not required to achieve the finite time convergence. A numerical example is presented to illustrate the finite time convergence of the proposed smoother in comparison with the asymptotic convergence of an optimal smoother.\"",
        "Document: \"Robust stabilization of uncertain input-delayed systems using reduction method. This paper concerns a robust stabilization problem that arises when applying the so-called reduction method to multiple input-delayed systems with parametric uncertainties. A robust stabilizing controller that can be constructed easily by solving convex problems in terms of linear matrix inequalities is presented. Numerical examples are provided to show that in many cases the proposed controller is less conservative and easier to obtain than the existing controllers.\"",
        "1 is \"Priority-based delay mitigation for event-monitoring IEEE 802.15.4 LR-WPANs\", 2 is \"Diglossia: detecting code injection attacks with precision and efficiency\"",
        "Given above information, for an author who has written the paper with the title \"Aggregation Time Control Algorithm for Time constrained Data Delivery in Wireless Sensor Networks\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003512": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Real-Time Multi-Criteria Social Graph Partitioning: A Game Theoretic Approach':",
        "Document: \"Blind evaluation of location based queries using space transformation to preserve location privacy. In this paper we propose a fundamental approach to perform the class of Range and Nearest Neighbor (NN) queries, the core class of spatial queries used in location-based services, without revealing any location information about the query in order to preserve users' private location information. The idea behind our approach is to utilize the power of one-way transformations to map the space of all objects and queries to another space and resolve spatial queries blindly in the transformed space. Traditional encryption based techniques, solutions based on the theory of private information retrieval, or the recently proposed anonymity and cloaking based approaches cannot provide stringent privacy guarantees without incurring costly computation and/or communication overhead. In contrast, we propose efficient algorithms to evaluate KNN and range queries privately in the Hilbert transformed space. We also propose a dual curve query resolution technique which further reduces the costs of performing range and KNN queries using a single Hilbert curve. We experimentally evaluate the performance of our proposed range and KNN query processing techniques and verify the strong level of privacy achieved with acceptable computation and communication overhead.\"",
        "Document: \"Utilizing Real-World Transportation Data for Accurate Traffic Prediction. For the first time, real-time high-fidelity spatiotemporal data on transportation networks of major cities have become available. This gold mine of data can be utilized to learn about traffic behavior at different times and locations, potentially resulting in major savings in time and fuel, the two important commodities of 21st century. As a first step towards the utilization of this data, in this paper, we study the real-world data collected from Los Angeles County transportation network in order to incorporate the data's intrinsic behavior into a time-series mining technique to enhance its accuracy for traffic prediction. In particular, we utilized the spatiotemporal behaviors of rush hours and events to perform a more accurate prediction of both short-term and long-term average speed on road-segments, even in the presence of infrequent events (e.g., accidents). Our result shows that taking historical rush-hour behavior we can improve the accuracy of traditional predictors by up to 67% and 78% in short-term and long-term predictions, respectively. Moreover, we can incorporate the impact of an accident to improve the prediction accuracy by up to 91%.\"",
        "Document: \"TSA-Tree: A Wavelet-Based Approach to Improve the Efficiency of Multi-Level Surprise and Trend Queries on Time-Series Data. We introduce a novel wavelet-based tree structure, termed TSA-tree, which improves the efficiency of multi-level trend and surprise queries on time sequence data. With the explosion of scientific observation data (some conceptualized as time-sequences), we are facing the challenge of efficiently storing, retrieving and analyzing this data. Frequent queries on this data set are to find trends (e.g., global warming) or surprises (e.g., undersea volcano eruption) within the original time-series. The challenge, however, is that these trend and surprise queries are needed at different levels of abstractions (e.g., within the last week, last month, last year or last decade). To support these multi-level trend and surprise queries, sometimes-huge subset of raw data needs to be retrieved and processed. To expedite this process, we utilize our TSA-tree. Each node of TSA-tree contains pre-computed trends and surprises at different levels. Wavelet transform is used recursively to construct TSA nodes. As a result, each node of TSA tree is readily available for visualization of trends and surprises. In addition, the size of each node is significantly smaller than that of the original time series, resulting in faster I/O operations. However, a limitation of TSA-tree is that its size is larger than the original time series. To address this shortcoming, first we prove that the storage space required to store the optimal subtree of TSA-tree (OTSA-tree) is no more than that required to store the original time-series without losing any information. Next, we propose two alternative techniques to reduce the size of OTSA-tree even further, while maintaining an acceptable query precision as compared to querying the original time sequences. Utilizing real and synthetic time-sequence databases, we compare our techniques with some well-known algorithms such as DFT and SVD in both performance and query precision. The results indicate the superiority of our approach. Finally, we show that our techniques are scalable as we increase either the database size or the length of time sequences.\"",
        "Document: \"Real-time Pattern Isolation and Recognition Over Immersive Sensor Data Streams. Data streams appear in many recent applications, where data are constantly changing or take the form of continuously arriving streams. We focus on data streams generated by sensors for monitoring users in immersive environments. To recognize users' interactions, we need to analyze the aggregation of several sensor data streams and match the result to a set of known actions. In addition, we need to separate a continuous series of actions into recognizable atomic actions. Hence, we first propose a distance metric, weighted-sum Singular Value Decomposition (SVD), suitable for similarity measurement of immersive data sequences. Subsequently, we propose a mutual information based heuristic for separation of the action sequences. Finally, we perform several empirical experiments using real-world virtual-reality devices to verify the effectiveness of our approach.\"",
        "Document: \"Efficient algorithms for answering reverse spatial-keyword nearest neighbor queries. With the proliferation of local services and GPS-enabled mobile phones, reverse spatial-keyword Nearest Neighbor queries are becoming an important type of query. Given a service object (e.g., shop) q as the query, which has a location and a text description, we return customers such that q is one of top-k spatial-keyword relevant service objects for each result customer. Existing algorithms for answering reverse nearest neighbor queries cannot be used for processing reverse spatial-keyword nearest neighbor queries due to the additional text information. To design efficient algorithms, for the first time we theoretically analyze an ideal case, which minimizes the object/index node accesses, for processing reverse spatial-keyword nearest neighbor queries. Under the derived theoretical guidelines, we design novel search algorithms for efficiently answering the queries. Empirical studies show that the proposed algorithms offer scalability and are orders of magnitude faster than existing methods for reverse spatial-keyword nearest neighbor queries.\"",
        "Document: \"An Adaptive Recommendation System without Explicit Acquisition of User Relevance Feedback. Recommendation systems are widely adopted in e-commerce businesses for helping customers locate products they would like to purchase. In an earlier work, we introduced a recommendation system, termed Yoda, which employs a hybrid approach that combines collaborative filtering (CF) and content-based querying to achieve higher accuracy for large-scale Web-based applications. To reduce the complexity of the hybrid approach, Yoda is structured as a tunable model that is trained off-line and employed for real-time recommendation on-line. The on-line process benefits from an optimized aggregation function with low complexity that allows the real-time aggregation based on confidence values of an active user to pre-defined sets of recommendations. In this paper, we extend Yoda to include more recommendation sets. The recommendation sets can be obtained from different sources, such as human experts, web navigation patterns, and clusters of user evaluations. Moreover, the extended Yoda can learn the confidence values automatically by utilizing implicit users' relevance feedback through web navigations using genetic algorithms (GA). Our end-to-end experiments show while Yoda's complexity is low and remains constant as the number of users and/or items grow, its accuracy surpasses that of the basic nearest-neighbor method by a wide margin (in most cases more than 100%). The experimental results also indicate that the retrieval accuracy is significantly increased by using the GA-based learning mechanism.\"",
        "Document: \"Automatically and Efficiently Matching Road Networks with Spatial Attributes in Unknown Geometry Systems. Vast amount of geospatial datasets are now available through numerous public and private organizations. These datasets usually cover different areas, have different accuracy and level of details, and are usually provided in the vector data format, where the latitude and longitude of each object is clearly specified. However, there are scenarios in which the spatial attributes of the objects are intentionally transformed to a different, and usually unknown, (alien) system. Moreover, it is possible that the datasets were generated from a legacy system or are represented in a native coordinate system. An example of this scenario is when a very accurate vector data representing the road network of a portion of a country is obtained with unknown coordinate. In this paper, we propose a solution that can efficiently and accurately find the area that is covered by this vector data simply by matching it with the (possibly inaccurate and abstract) data with known geocoordinates. In particular, we focus on vector datasets that represent road networks and our approach identifies the exact location of the vector dataset of alien system by comparing the distribution of the detected road intersection points between two datasets. Our experiment results show that our technique can match road vector datasets that are composed of thousands of arcs in a relatively short time with 91% precision and 92.5% recall for the matched road feature points.\"",
        "Document: \"Yima: real-time multimedia storage and retrieval. Yima is a scalable, real-time streaming architecture that enables applications such as video-on-demand and distance learning on a large scale. While Yima incorporates lessons learned from first generation research prototypes, it also complies with industry standards in content format (MPEG-4) and communication protocols (RTP/RTSP). Yima improves upon both research and commercial approaches by using a bipartite design and alternative approaches to handling variable-bit-rate (VBR) video. We also integrated a selective retransmission protocol into Yima's RTP server to recover from packet loss. Lastly, we tweaked available hardware and software to achieve certain objectives (e.g., playback of HDTV streams on an HDTV monitor). Yima is operational and supports a variety of display bandwidths.\"",
        "Document: \"SHIFT-SPLIT: I/O efficient maintenance of wavelet-transformed multidimensional data. The Discrete Wavelet Transform is a proven tool for a wide range of database applications. However, despite broad acceptance, some of its properties have not been fully explored and thus not exploited, particularly for two common forms of multidimensional decomposition. We introduce two novel operations for wavelet transformed data, termed SHIFT and SPLIT, based on the properties of wavelet trees, which work directly in the wavelet domain. We demonstrate their significance and usefulness by analytically proving six important results in four common data maintenance scenarios, i.e., transformation of massive datasets, appending data, approximation of data streams and partial data reconstruction, leading to significant I/O cost reduction in all cases. Furthermore, we show how these operations can be further improved in combination with the optimal coefficient-to-disk-block allocation strategy. Our exhaustive set of empirical experiments with real-world datasets verifies our claims.\"",
        "Document: \"Approximate voronoi cell computation on spatial data streams. Several studies have exploited the properties of Voronoi diagrams to improve the efficiency of variations of the nearest neighbor search on stored datasets. However, the significance of Voronoi diagrams and their basic building blocks, Voronoi cells, has been neglected when the geometry data is incrementally becoming available as a data stream. In this paper, we study the problem of Voronoi cell computation for fixed 2-d site points when the locations of the neighboring sites arrive as a spatial data stream. We show that the non-streaming solution to the problem does not meet the memory requirements of many realistic scenarios over a sliding window. Hence, we propose AVC-SW, an approximate streaming algorithm that computes (1 + \u03b5)-approximations to the actual exact Voronoi cell in O(\u03ba) where \u03ba is its sample size. With the sliding window model and random arrival of points, we show both analytically and experimentally that for given window size w and parameter k, AVC-SW reduces the expected memory requirements of the classic algorithm from O(w) to $$O(k \\log (\\frac{w}{k} + 1))$$ regardless of the distribution of the points in the 2-d space. This is a significant improvement for most of the real-world scenarios where w \u226b k.\"",
        "1 is \"Securing Web Servers against Insider Attack\", 2 is \"A comparison of selectivity estimators for range queries on metric attributes\"",
        "Given above information, for an author who has written the paper with the title \"Real-Time Multi-Criteria Social Graph Partitioning: A Game Theoretic Approach\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003571": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Toward a Learning/Instruction Process Model for Facilitating the Instructional Design Cycle':",
        "Document: \"Using erroneous examples to improve mathematics learning with a web-based tutoring system. \u2022Middle school students learned to solve decimal problems with a web-based tutoring system.\u2022ExErr group received erroneous examples to correct and explain.\u2022PS group received problems to solve and explain.\u2022ExErr group outperformed PS group on a delayed test and on judging answer correctness.\u2022PS group reported liking the lessons better than the ExErr group.\"",
        "Document: \"Steps Towards The Gamification Of Collaborative Learning Scenarios Supported By Ontologies. The Computer-Support Collaborative Learning (CSCL) script is an effective approach to support meaningful interactions and better learning. Unfortunately, in some situations, scripted collaboration demotivates students, which makes more difficult its use over time. To deal with this problem, we propose the use of gamification to positively change learners' motivation and engagement. Nevertheless, the adequate application of gamification is a complex task that requires deeper knowledge about game design and their impact on collaborative learning (CL). Thus, we develop an ontology called OntoGaCLeS to provide a formal systematization of the knowledge about gamification and its correct application. In this paper, we focus in the formalization of concepts relate to gamification as persuasive technology.\"",
        "Document: \"What do students do on-line? Modeling students' interactions to improve their learning experience. In this work, we present an approach to model and analyze students interactions, within a gamified on-line learning environment, in order to assist teachers and tutors (education professionals) decision-making, regarding their students learning experience. We noticed that asking students this information might not bring precise and dynamic results for all students. This way, we characterize the educational resources available in the studied environment, and collected data from students interactions with these resources. Our objective was to generate the students\u2019 interactional profile (a model of their interactions). The information, then, is presented to teachers and tutors, who should use it to guide their pedagogical decision-making process. In this study, the types of interactions were used to personalize gamification elements named missions. We experimented the approach with two groups of users from the studied environment (MeuTutor). The data analysis showed differences in the way these groups were performing, where group B was considerably above group A. We sent the personalized missions (following our approach) to every student from group A, and waited some time for them to interact with it. In the end we checked the effect of this treatment, which, according to the results, promoted relevant improvement in group A interactions.\"",
        "Document: \"AIMED: Agile, Integrative and Open Method for Open Educational Resources Development. This project is inserted in the research and practice areas that support the development of educational resources, such as, serious games and training simulation, to understand how they should be planned, developed, evaluated, validated and used for the learning and assessment purposes. In this study, we have proposed and specified an agile, integrative and open method for educational resources development, named AIMED, in a holistic and multidisciplinary approach. This approach integrates the areas of simulation modeling, game design, instructional design, principles of agile development methods, project management practices and content domain. AIMED is based on integration of DevJSTA methodology, for creation of serious games, and AM-OER method, for development of open educational resources. A feasibility study was done with serious game development, which should be used and evaluated by domain experts, as well as, the methodology will be used and validated in new case studies.\"",
        "Document: \"A semi-automatic system to evaluate the performance and scalability of ontology persistent APIs. The international Semantic Web community has produced several tools, known as API \u2013 application programming interfaces, to help development teams to create applications using semantic technologies. One important family of tools developed to date is related to the manipulation of instances in ontology-based applications. There are two main approaches for manipulating ontology instances in currently developed APIs: (a) using RDF (resource description framework) triples or (b) using classes in object-oriented programming (OOP). In APIs based on RDF triples the development teams need to understand how ontology works in the RDF layers in order to manipulate data for each triple in the application code. Such an approach is highly desired to develop clean applications based strongly on Semantic Web Technologies. Nevertheless, it requires development teams to change their programming paradigm and master those technologies. In APIs based on OOP developers can manipulate data at object level in order to make application development simple and flexible using the conventional programming paradigm. Because of that, several developers have shifted from RDF triples to OOP. Although several tools have been developed to manipulate ontologies at object level, many have not been adequately evaluated for performance and scalability. In effect, there is an urgent demand for effective research results regarding metrics and systems to evaluate the performance and scalability of ontology persistent APIs. As a result, these systems should facilitate the evaluation of persistent APIs to developers. This study thus aims to provide a system for developers to evaluate ontology persistent APIs at object level. In order to validate the feasibility of our system, we have conducted an experiment by considering two solutions used by community of ontology persistence.\"",
        "Document: \"A systematic mapping on gamification applied to education. Gamification is a term that refers to the use of game elements in non-game contexts with the goal of engaging people in a variety of tasks. There is a growing interest in gamification as well as its applications and implications in the field of Education since it provides an alternative to engage and motivate students during the process of learning. Despite this increasing interest, to the best of our knowledge, there are no studies that cover and classify the types of research being published and the most investigated topics in the area. As a first step towards bridging this gap, we carried out a systematic mapping to synthesize an overview of the area. We went through 357 papers on gamification. Among them, 48 were related to education and only 26 met the criteria for inclusion and exclusion of articles defined in this study. These 26 papers were selected and categorized according to their contribution. As a result, we provide an overview of the area. Such an overview suggests that most studies focus on investigating how gamification can be used to motivate students, improve their skills, and maximize learning.\"",
        "Document: \"Work in progress -- Enhancing Interactive Geometry Systems with intelligent tutoring features. There are different approaches that drive the development and use of educational software, such as Interactive Geometry Systems--IGS and Intelligent Tutoring Systems--ITS. Considering their benefits to teachers and students, these systems may be used to complement each other. The ongoing development of ITS features in an existing IGS called iGeom is presented. First, the limitations of both approaches are listed, describing possible benefits of using them together. Then, the resulting component architecture of the conducted analysis and software design is outlined. The ITS paradigm chosen was Example-tracing Tutors. The current state of research is the tutoring features implementation and planning for testing in classrooms and in distance learning courses.\"",
        "Document: \"A Systematic Review on the Use of Ontologies in Requirements Engineering. Requirements Engineering (RE) discipline deals with elicitation, analysis, specification, validation and management of requirements. Several ontology-driven approaches have been proposed to improve these RE activities. However, the requirements engineering community still lacks a comprehensive understanding on how ontologies are used in RE process. The objective of this work is to explore how ontologies are employed in requirements engineering, aiming to identify the main phases addressed, the languages that have been used, the types of existing contributions, as well as the requirements modeling styles have been used and the benefits of using ontology in RE. We conducted a systematic literature review to identify the primary studies on the use of ontologies in RE, following a pre-defined review protocol. Sixty-six papers were selected, covering the five main RE process phases. Moreover, we have identified thirteen ontology-related languages. Furthermore, twenty-six empirical studies have been identified which provided evidence of five group of benefits. The main findings of this review are: (1) there are empirical evidences to state that ontologies benefit RE activities in both academy and industry settings, helping to reduce ambiguity, inconsistency and incompleteness of requirements; (2) the vast majority of papers do not meet all RE phases; (3) nearly half of the papers use W3C recommended languages; (4) the majority of contributions are supported by a tool; and (5) there is a great diversity of requirements modeling styles supported by ontologies.\"",
        "Document: \"An Ontology-based Framework and its Application to Effective Collaboration. In the past few years Artificial Intelligence has been gradually introduced to enhance Education through technologies. However, usual approaches provide systems with a kind of expertise using a set of heuristics and domain theories built in the procedures. As a result they cannot justify their recommendations systematically and scientifically. To overcome such problems it is necessary to establish a common understanding of what a learning theory is and how to represent it adequately. In this work we present part of our ontological framework that allows for the partial representation of learning theories considering explicitness, formalism, concepts and vocabulary. Then, we propose sophisticated techniques to reasoning on these theories considering their semantics showing the use of this framework to build a system called CHOCOLATO to facilitate the effective design and analysis of collaborative learning activities.\"",
        "Document: \"The Use of Handwriting Input in Math Tutoring Systems: An Use Case with PAT2Math. Intelligent Tutoring Systems (ITSs) for Math still use traditional methods of input, such as computer keyboard and mouse. This strategy exposes students to a high extraneous cognitive load, due to the way the students are used to work, what could impair their learning. This paper presents a tool for Math ITSs that uses the student handwriting as data input. This tool was integrated into PAT2Math, an algebraic ITS, for an experimental evaluation in a public school with 48 students. It followed a design of control and experimental groups with pre and post-tests and a post questionnaire. Although our results have shown statistically significant improvement in student learning gains in both groups, no difference between groups was found. However, questionnaire responses suggest that handwriting provides a greater facility to the insertion of equations into ITSs for Math if compared with traditional methods (i.e., keyboard and mouse).\"",
        "1 is \"Towards a Framework for the Development of Adaptive Multimodal User Interfaces for Ambient Assisted Living Environments\", 2 is \"Negotiated learner modelling to maintain today's learner models.\"",
        "Given above information, for an author who has written the paper with the title \"Toward a Learning/Instruction Process Model for Facilitating the Instructional Design Cycle\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003597": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Adaptive interference suppression in CDMA systems by LCL-PTV filtering':",
        "Document: \"Widely linear equalization and blind channel identification for interference-contaminated multicarrier systems. This work addresses the problem of designing efficient detection techniques for multicarrier transmission systems operating in the presence of narrowband interference (NBI). In this case, conventional linear receivers, such as the zero-forcing (ZF) or the minimum-mean square error (MMSE) ones, usually perform poorly since they are not capable of suppressing satisfactorily the NBI. To synthesize interference-resistant detection algorithms, we resort to widely linear (WL) filtering, which allows one to exploit the noncircularity property of the desired signal constellation by jointly processing the received signal and its complex-conjugate version. In particular, we synthesize new WL-ZF receivers for multicarrier systems, which mitigate, in the minimum output-energy (MOE) sense, the NBI contribution at the receiver output, without requiring knowledge of the NBI statistics. By exploiting the noncircularity property, we also propose a new subspace-based blind channel identification algorithm and derive the channel identifiability condition. Blind identification can be performed satisfactorily also in the presence of NBI, requiring only an approximate rank determination of the NBI autocorrelation matrix. The performance analysis shows that the proposed MOE WL-ZF receiver, even when implemented blindly, assures a substantial improvement over the conventional linear ZF and MMSE ones, particularly when the NBI bandwidth is very small in comparison with the intercarrier spacing and the NBI is not exactly located on a subcarrier.\"",
        "Document: \"On Reliability of Dynamic Addressing Routing Protocols in Mobile Ad Hoc Networks. In this paper, a reliability analysis is carried out to state a performance comparison between two recently proposed proactive routing algorithms. These protocols are able to scale in ad hoc and sensor networks by resorting to dynamic addressing, to face with the topology variability, which is typical of ad hoc, and sensor networks. Numerical simulations are also carried out to corroborate the results of the analysis.\"",
        "Document: \"Bio-inspired link quality estimation for wireless mesh networks. In this paper, the problem of estimating the link quality, in mesh networks has been considered. Such a process is a major task to develop an efficient network, layer since it allows routing protocols to efficiently use neighbors as relays for multi-hop communications. In the last years, a number of link-quality aware routing metrics have been proposed and analyzed. However, such metrics usually adopt simple link-quality estimators based on moving average filters, which lead to poor performances due to their static nature. In this paper, we propose to improve the estimation of the link quality resorting to a bio-inspired estimator based on the neural network paradigm. The effectiveness of the proposal has been proved by means of a numerical performance comparison between the proposed estimator and the traditional ones under several environmental conditions.\"",
        "Document: \"Cyclostationarity: half a century of research. In this paper, a concise survey of the literature on cyclostationarity is presented and includes an extensive bibliography. The literature in all languages, in which a substantial amount of research has been published, is included. Seminal contributions are identified as such. Citations are classified into 22 categories and listed in chronological order. Both stochastic and nonstochastic approaches for signal analysis are treated. In the former, which is the classical one, signals are modelled as realizations of stochastic processes. In the latter, signals are modelled as single functions of time and statistical functions are defined through infinite-time averages instead of ensemble averages. Applications of cyclostationarity in communications, signal processing, and many other research areas are considered.\"",
        "Document: \"Blind periodically time-varying MMOE channel shortening for OFDM systems. In this paper, the problem of synthesizing a blind channel shortening algorithm for orthogonal frequency-division multiplexing (OFDM) systems is addressed. In particular, a commonly adopted assumption in the channel shortening framework is discussed, showing that, when it is violated, the data statistics usually needed for the synthesis of the shortening algorithms turn out to be periodically time varying (PTV) rather than time-invariant. Elaborating on this point, and considering in particular the recently proposed minimum-mean output-energy (MMOE) blind channel shortening algorithm, it is shown how its synthesis must be modified in order to account for the PTV nature of the data statistics. Numerical results assessing the performance of the proposed blind PTV-MMOE channel shortening algorithm are reported.\"",
        "Document: \"X-ray fluoroscopy noise modeling for filter design. Fluoroscopy is an invaluable tool in various medical practices such as catheterization or image-guided surgery. Patient's screen for prolonged time requires substantial reduction in X-ray exposure: The limited number of photons generates relevant quantum noise. Denoising is essential to enhance fluoroscopic image quality and can be considerably improved by considering the peculiar noise characteristics. This study presents analytical models of fluoroscopic noise to express the variance of noise as a function of gray level, a practical method to estimate the parameters of the models and a possible application to improve the performance of noise filtering.Quantum noise is modeled as a Poisson distribution and results strongly signal-dependent. However, fluoroscopic devices generally apply gray-level transformations (i.e., logarithmic-mapping, gamma-correction) for image enhancement. The resulting statistical transformations of the noise were analytically derived. In addition, a characterization of the statistics of noise for fluoroscopic image differences was offered by resorting to Skellam distribution. Real fluoroscopic sequences of a simple step-phantom were acquired by a conventional fluoroscopic device and were utilized as actual noise measurements to compare with. An adaptive spatio-temporal filter based on the local conditional average of similar pixels has been proposed. The gray-level differences between the local pixel and the neighboring pixels have been assumed as measure of similarity. Filter performance was evaluated by using real fluoroscopic images of a step phantom and acquired during a pacemaker implantation.The comparison between experimental data and the analytical derivation of the relationship between noise variance and mean pixel intensity (noise-parameter models) were presented relatively to raw-images, after applying logarithmic-mapping or gamma-correction and for difference images. Results have confirmed a great agreement (adjusted R-squared values >  0.8). Clipping effects of real sensors were also addressed. A fine image restoration has been obtained by using a conditioned spatio-temporal average filter based on the noise statistics previously estimated.Fluoroscopic noise modeling is useful to design effective procedures for noise estimation and image filtering. In particular, filter performance analysis has showed that the knowledge of the noise model and the accurate estimate of noise characteristics can significantly improve the image restoration, especially for edge preserving. Fluoroscopic image enhancement can support further X-ray exposure reduction, medical image analysis and automated object identification (i.e., surgery tools, anatomical structures).\"",
        "Document: \"M-DART: multi-path dynamic address routing. The paper proposes a Distributed Hash Table (DHT)-based multi-path routing protocol for scalable ad hoc networks. Specifically, we propose a multipath-based improvement to a recently proposed DHT-based shortest-path routing protocol, namely the Dynamic Address RouTing (DART). The resulting protocol, referred to as multi-path DART (M-DART), guarantees multi-path forwarding without introducing any additional communication or coordination overhead with respect to DART. The performances of M-DART have been evaluated by means of numerical simulations across a wide range of environments and workloads. The results show that M-DART performs the best or at least comparable with respect to widely adopted routing protocols in all the considered scenarios. Moreover, unlike these protocols, it is able to assure satisfactory performances for large networks by reducing the packet loss by up to 75%. Copyright \u00a9 2010 John Wiley & Sons, Ltd. (This work is partially supported by the Italian National Project \u2018Global & Reliable End to End e-Commerce & On Line Service Platform\u2019 (GRECO).)\"",
        "Document: \"Optimal Constrained Candidate Selection for Opportunistic Routing. In this paper we address the issue of the optimal candidate-set selection in the opportunistic routing paradigm. More specifically, although several algorithms for selecting the optimal candidate set have been proposed, to the best of our knowledge none of them has never considered the problem of selecting the optimal constrained candidate set, namely the optimal candidate set with a fixed maximum set size. In this paper we contribute to this problem by providing an analytical framework to model both the optimal constrained and unconstrained candidate-set selection. Moreover, we propose two algorithms for optimal candidate-set selection for distance vector routing, one for the constrained and one for the unconstrained case. Simulations based on experimental data validate our proposal.\"",
        "Document: \"Optimal Primary-User Mobility Aware Spectrum Sensing Design for Cognitive Radio Networks. A key issue of the spectrum sensing functionality in Cognitive Radio (CR) networks is the ability of tuning the sensing time parameters, i.e., the sensing time and the transmission time, according to the Primary User (PU) network dynamics. In fact, these parameters influence both the spectrum sensing efficiency and the PU interference avoidance. This issue becomes even more challenging in presence of PU mobility. In this paper, an optimal spectrum sensing design for mobile PU scenarios is proposed with the aim to achieve the following important features: i) to determine the optimal mobility-aware transmission time, i.e., the transmission time value that jointly maximizes the spectrum sensing efficiency and satisfies the PU interference constraint; ii) to determine the optimal mobility-aware sensing time threshold, i.e., the maximum sensing time value assuring efficient spectrum sensing. First, closed-form expressions of both the optimal transmission time and the optimal sensing time threshold are analytically derived for a general PU mobility model. Then, the derived expressions are specialized for two widely adopted mobility models, i.e., the Random Walk mobility Model with reflection and the Random Way-Point mobility Model. Practical rules for the sensing parameter tuning are provided with reference to the considered mobility models. The analytical results are finally validated through simulations.\"",
        "Document: \"Widely linear decision-feedback equalizer for time-dispersive linear MIMO channels. The paper deals with the transmission over multiple-input/multiple-output channels exhibiting time-dispersion. A minimum mean-square error equalizer based on widely linear processing combined with the decision-feedback (DF) strategy is implemented via finite-impulse-response filters. The proposed equalizer provides considerable performance gain at the expense of a limited increase in computational complexity. The performance analysis has been carried out accounting for mismatch conditions always present in practice. The results confirm the stronger sensitivity of the DF-based equalizers with respect to the feedforward-based ones when system parameters are not accurately known.\"",
        "1 is \"An adaptive semiparametric and context-based approach to unsupervised change detection in multitemporal remote-sensing images.\", 2 is \"Spectrum Aware Opportunistic Routing in Cognitive Radio Networks\"",
        "Given above information, for an author who has written the paper with the title \"Adaptive interference suppression in CDMA systems by LCL-PTV filtering\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003618": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Multi-way set enumeration in real-valued tensors':",
        "Document: \"Iterative Subgraph Mining for Principal Component Analysis. Graph mining methods enumerate frequent subgraphs efficiently, but they are not necessarily good features for machine learning due to high correlation among features. Thus it makes sense to perform principal component analysis to reducethe dimensionality and create decorrelated features. We present a novel iterative mining algorithm that captures informative patterns corresponding to major entries of top principal components. It repeatedly callsweighted substructure mining where example weights are updated in each iteration. The Lanczos algorithm, a standard algorithm of eigen decomposition, is employed to update the weights. In experiments, our patterns are shown to approximate the principal components obtained by frequent mining.\"",
        "Document: \"Significant Pattern Mining with Confounding Variables. Recent pattern mining algorithms such as LAMP allow us to compute statistical significance of patterns with respect to an outcome variable. Their p-values are adjusted to control the family-wise error rate, which is the probability of at least one false discovery occurring. However, they are a poor fit for medical applications, due to their inability to handle potential confounding variables such as age or gender. We propose a novel pattern mining algorithm that evaluates statistical significance under confounding variables. Using a new testability bound based on the exact logistic regression model, the algorithm can exclude a large quantity of combination without testing them, limiting the amount of correction required for multiple testing. Using synthetic data, we showed that our method could remove the bias introduced by confounding variables while still detecting true patterns correlated with the class. In addition, we demonstrated application of data integration using a confounding variable.\"",
        "Document: \"The em algorithm for kernel matrix completion with auxiliary data. In biological data, it is often the case that observed data are available only for a subset of samples. When a kernel matrix is derived from such data, we have to leave the entries for unavailable samples as missing. In this paper, the missing entries are completed by exploiting an auxiliary kernel matrix derived from another information source. The parametric model of kernel matrices is created as a set of spectral variants of the auxiliary kernel matrix, and the missing entries are estimated by fitting this model to the existing entries. For model fitting, we adopt the em algorithm (distinguished from the EM algorithm of Dempster et al., 1977) based on the information geometry of positive definite matrices. We will report promising results on bacteria clustering experiments using two marker sequences: 16S and gyrB.\"",
        "Document: \"Modeling splicing sites with pairwise correlations. Motivation: A new method for finding subtle patterns in sequences is introduced. It approximates the multiple correlations among residuals with pair-wise correlations, with the learning cost O(m(2)n) where n is the number of training sequences, each of length m. The method suits to model splicing sites in human DNA, which are reported to have higher-order dependencies. Results: By computational experiments, the prediction accuracy of our model was shown to surpass that of previously reported Markov models for the prediction of acceptor sites in human.\"",
        "Document: \"Entire regularization paths for graph data. Graph data such as chemical compounds and XML documents are getting more common in many application domains. A main difficulty of graph data processing lies in the intrinsic high dimensionality of graphs, namely, when a graph is represented as a binary feature vector of indicators of all possible subgraph patterns, the dimensionality gets too large for usual statistical methods. We propose an efficient method to select a small number of salient patterns by regularization path tracking. The generation of useless patterns is minimized by progressive extension of the search space. In experiments, it is shown that our technique is considerably more efficient than a simpler approach based on frequent substructure mining.\"",
        "Document: \"The Leave-One-Out Kernel. Recently, several attempts have been made for deriving datadependent kernels from distribution estimates withparametric models (e.g. the Fisher kernel). In this paper, we propose a new kernel derived from any distribution estimators, parametric or nonparametric. This kernel is called the Leave-one-out kernel (i.e. LOO kernel), because the leave-one-out process plays an important role to compute this kernel. We will show that, when applied to a parametric model, the LOO kernel converges to the Fisher kernel asymptotically as the number of samples goes to infinity.\"",
        "Document: \"Privacy-Preserving Statistical Analysis by Exact Logistic Regression. Logistic regression is the method of choice in most genome-wide association studies (GWAS). Due to the heavy cost of performing iterative parameter updates when training such a model, existing methods have prohibitive communication and computational complexities that make them unpractical for real-life usage. We propose a new sampling-based secure protocol to compute exact statistics, that requires a constant number of communication rounds and a much lower number of computations. The publicly available implementation of our protocol (and its many optional optimisations adapted to different security scenarios) can, in a matter of hours, perform statistical testing of over 600 SNP variables across thousands of patients while accounting for potential confounding factors in the clinical data.\"",
        "Document: \"Safe Pattern Pruning: An Efficient Approach for Predictive Pattern Mining. In this paper we study predictive pattern mining problems where the goal is to construct a predictive model based on a subset of predictive patterns in the database. Our main contribution is to introduce a novel method called safe pattern pruning (SPP) for a class of predictive pattern mining problems. The SPP method allows us to efficiently find a superset of all the predictive patterns in the database that are needed for the optimal predictive model. The advantage of the SPP method over existing boosting-type method is that the former can find the superset by a single search over the database, while the latter requires multiple searches. The SPP method is inspired by recent development of safe feature screening. In order to extend the idea of safe feature screening into predictive pattern mining, we derive a novel pruning rule called safe pattern pruning (SPP) rule that can be used for searching over the tree defined among patterns in the database. The SPP rule has a property that, if a node corresponding to a pattern in the database is pruned out by the SPP rule, then it is guaranteed that all the patterns corresponding to its descendant nodes are never needed for the optimal predictive model. We apply the SPP method to graph mining and item-set mining problems, and demonstrate its computational advantage.\"",
        "Document: \"An Attempt for Coloring Multichannel MR Imaging Data. This is an elementary research for assigning color values to voxels of multichannel Magnetic Resonance Imaging (MRI) volume data. The MRI volume data sets obtained under different scanning conditions are transformed to the components by independent component analysis (ICA), which enhances physical characteristics of the tissue. The transfer functions for generating color values from independent components are obtained using the radial basis function network, a kind of neural net, by training the network with sample data chosen from visible human female data set (VHF). The resultant color volume data sets correspond well with the full-color cross-sections of the visible human data sets.\"",
        "Document: \"Clustering with the Fisher Score. Abstract Recently the Fisher score (or the Fisher kernel) is increasingly used as a feature extractor for classification problems The Fisher score is a vector of parameter derivatives of loglikelihood of a probabilistic model This paper  gives  a  theoretical  analysis  about  how  class  information  is  pre - served in  the  space  of  the  Fisher  score,  which  turns  out  that  the  Fisher score consists of a few important dimensions with class information and many nuisance dimensions When we perform clustering with the Fisher score, K - Means type methods are obviously inappropriate because they make use of all dimensions So we will develop a novel but simple clus - tering algorithm specialized for the Fisher score, which can exploit im - portant dimensions This algorithm is successfully tested in experiments with artificial data and real data (amino acid sequences)\"",
        "1 is \"Gstring: A Novel Approach For Efficient Search In Graph Databases\", 2 is \"Process fault detection based on modeling and estimation methods-A survey\"",
        "Given above information, for an author who has written the paper with the title \"Multi-way set enumeration in real-valued tensors\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003622": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Outsourcing Search Services on Private Spatial Data':",
        "Document: \"A Lightweight Secure Provenance Scheme for Wireless Sensor Networks. Large-scale sensor networks are being deployed in numerous application domains, and often the data they collect are used in decision-making for critical infrastructures. Data are streamed from multiple sources through intermediate processing nodes that aggregate information. A malicious adversary may tamper with the data by introducing additional nodes in the network, or by compromising existing ones. Therefore, assuring high data trustworthiness in such a context is crucial for correct decision-making. Data provenance represents a key factor in evaluating the trustworthiness of sensor data. Provenance management for sensor networks introduces several challenging requirements, such as low energy and bandwidth consumption, efficient storage and secure transmission. In this paper, we propose a novel light-weight scheme to securely transmit provenance for sensor data. The proposed technique relies on in-packet Bloom filters to encode provenance. In addition, we introduce efficient mechanisms for provenance verification and reconstruction at the base station. We evaluate the proposed technique both analytically and empirically, and the results prove its effectiveness and efficiency for secure provenance encoding and decoding.\"",
        "Document: \"Privacy-Aware Location-Aided Routing in Mobile Ad Hoc Networks. Mobile Ad-hoc Networks (MANETs) enable users in physical proximity to each other to exchange data without the need for expensive communication infrastructures. Each user represents a node in the network, and executes a neighbor discovery Typically, nodes broadcast beacon messages that are received by other participants within the sender\u2019s communication range. Routing strategies are computed on-line based on the locations of nearby nodes, and geocasting is employed to deliver data packets to their destinations. However, mobile users may be reluctant to share their exact locations with other participants, since location can disclose private details about a person\u2019s lifestyle, religious or political affiliations, etc. A common approach to protect location privacy is to replace exact coordinates with coarser-grained regions, based on the privacy profile of each user. In this paper, we investigate protocols that support MANET routing without disclosing exact positions of nodes. Each node defines its own privacy profile, and reports a cloaked location information to its neighbors. We adopt a novel strategy to advertise beacons, to prevent inference of node locations. We also propose packet forwarding heuristics that rely on cloaking regions, rather than point locations. Our extensive experimental evaluation shows that the proposed routing scheme achieves low delays and high packet delivery ratios, without incurring significant overhead compared to conventional MANET routing protocols.\"",
        "Document: \"MOBIHIDE: a mobilea peer-to-peer system for anonymous location-based queries. Modern mobile phones and PDAs are equipped with positioning capabilities (e.g., GPS). Users can access public location-based services (e.g., Google Maps) and ask spatial queries. Although communication is encrypted, privacy and confidentiality remain major concerns, since the queries may disclose the location and identity of the user. Commonly, spatial K-anonymity is employed to hide the query initiator among a group of K users. However, existing work either fails to guarantee privacy, or exhibits unacceptably long response time. In this paper we propose MobiHide, a Peer-to-Peer system for anonymous location-based queries, which addresses these problems. MobiHide employs the Hilbert space-filling curve to map the 2-D locations of mobile users to 1-D space. The transformed locations are indexed by a Chord-based distributed hash table, which is formed by the mobile devices. The resulting Peer-to-Peer system is used to anonymize a query by mapping it to a random group of K users that are consecutive in the 1-D space. Compared to existing state-of-the-art, MobiHide does not provide theoretical anonymity guarantees for skewed query distributions. Nevertheless, it achieves strong anonymity in practice, and it eliminates system hotspots. Our experimental evaluation shows that MobiHide has good load balancing and fault tolerance properties, and is applicable to real-life scenarios with numerous mobile users.\"",
        "Document: \"Privacy-preserving publication of provenance workflows. Provenance workflows capture the data movement and the operations changing the data in complex applications such as scientific computations, document management in large organizations, content generation in social media, etc. Provenance is essential to understand the processes and operations that data undergo, and many research efforts focused on modeling, capturing and analyzing provenance information. Sharing provenance brings numerous benefits, but may also disclose sensitive information, such as secret processes of synthesizing chemical substances, confidential business practices and private details about social media participants' lives. In this paper, we study privacy-preserving provenance workflow publication using differential privacy. We adapt techniques designed for sanitization of multi-dimensional spatial data to the problem of provenance workflows. Experimental results show that such an approach is feasible to protect provenance workflows, while at the same time retaining a significant amount of utility for queries. In addition, we identify influential factors and trade-offs that emerge when sanitizing provenance workflows.\"",
        "Document: \"A Hybrid Approach to Private Record Matching. Real-world entities are not always represented by the same set of features in different data sets. Therefore, matching records of the same real-world entity distributed across these data sets is a challenging task. If the data sets contain private information, the problem becomes even more difficult. Existing solutions to this problem generally follow two approaches: sanitization techniques and cryptographic techniques. We propose a hybrid technique that combines these two approaches and enables users to trade off between privacy, accuracy, and cost. Our main contribution is the use of a blocking phase that operates over sanitized data to filter out in a privacy-preserving manner pairs of records that do not satisfy the matching condition. We also provide a formal definition of privacy and prove that the participants of our protocols learn nothing other than their share of the result and what can be inferred from their share of the result, their input and sanitized views of the input data sets (which are considered public information). Our method incurs considerably lower costs than cryptographic techniques and yields significantly more accurate matching results compared to sanitization techniques, even when privacy requirements are high.\"",
        "Document: \"Privacy-Preserving Detection of Anomalous Phenomena in Crowdsourced Environmental Sensing. Crowdsourced environmental sensing is made possible by the wide-spread availability of powerful mobile devices with a broad array of features, such as temperature, location, velocity, and acceleration sensors. Mobile users can contribute measured data for a variety of purposes, such as environmental monitoring, traffic analysis, or emergency response. One important application scenario is that of detecting anomalous phenomena, where sensed data is crucial to quickly acquire data about forest fires, environmental accidents or dangerous weather events. Such cases typically require the construction of a heatmap that captures the distribution of a certain parameter over a geospatial domain (e.g., temperature, CO2 concentration, water polluting agents, etc.). However, contributing data can leak sensitive private details about an individual, as an adversary may be able to infer the presence of a person in a certain location at a given time. In turn, such information may reveal information about an individual's health, lifestyle choices, and may even impact the physical safety of a person. In this paper, we propose a technique for privacy-preserving detection of anomalous phenomena, where the privacy of the individuals participating in collaborative environmental sensing is protected according to the powerful semantic model of differential privacy. Our techniques allow accurate detection of phenomena, without an adversary being able to infer whether an individual provided input data in the sensing process or not. We build a differentially-private index structure that is carefully customized to address the specific needs of anomalous phenomenon detection, and we derive privacy-preserving query strategies that judiciously allocate the privacy budget to maintain high data accuracy. Extensive experimental results show that the proposed approach achieves high precision of identifying anomalies, and incurs low computational overhead.\"",
        "Document: \"A Lightweight Secure Scheme for Detecting Provenance Forgery and Packet DropAttacks in Wireless Sensor Networks. Large-scale sensor networks are deployed in numerous application domains, and the data they collect are used in decision-making for critical infrastructures. Data are streamed from multiple sources through intermediate processing nodes that aggregate information. A malicious adversary may introduce additional nodes in the network or compromise existing ones. Therefore, assuring high data trustworthiness is crucial for correct decision-making. Data provenance represents a key factor in evaluating the trustworthiness of sensor data. Provenance management for sensor networks introduces several challenging requirements, such as low energy and bandwidth consumption, efficient storage and secure transmission. In this paper, we propose a novel lightweight scheme to securely transmit provenance for sensor data. The proposed technique relies on in-packet Bloom filters to encode provenance. We introduce efficient mechanisms for provenance verification and reconstruction at the base station. In addition, we extend the secure provenance scheme with functionality to detect packet drop attacks staged by malicious data forwarding nodes. We evaluate the proposed technique both analytically and empirically, and the results prove the effectiveness and efficiency of the lightweight secure provenance scheme in detecting packet forgery and loss attacks.\"",
        "Document: \"Fast data anonymization with low information loss. Recent research studied the problem of publishing microdata without revealing sensitive information, leading to the privacy preserving paradigms of k-anonymity and l-diversity. k-anonymity protects against the identification of an individual's record. l-diversity, in addition, safeguards against the association of an individual with specific sensitive information. However, existing approaches suffer from at least one of the following drawbacks: (i) The information loss metrics are counter-intuitive and fail to capture data inaccuracies inflicted for the sake of privacy. (ii) l-diversity is solved by techniques developed for the simpler k-anonymity problem, which introduces unnecessary inaccuracies. (iii) The anonymization process is inefficient in terms of computation and I/O cost. In this paper we propose a framework for efficient privacy preservation that addresses these deficiencies. First, we focus on one-dimensional (i.e., single attribute) quasi-identifiers, and study the properties of optimal solutions for k-anonymity and l-diversity, based on meaningful information loss metrics. Guided by these properties, we develop efficient heuristics to solve the one-dimensional problems in linear time. Finally, we generalize our solutions to multi-dimensional quasi-identifiers using space-mapping techniques. Extensive experimental evaluation shows that our techniques clearly outperform the state-of-the-art, in terms of execution time and information loss.\"",
        "Document: \"PrivGeoCrowd: A toolbox for studying private spatial Crowdsourcing. Spatial Crowdsourcing (SC) is a novel and transformative platform that engages individuals, groups and communities in the act of collecting, analyzing, and disseminating environmental, social and other spatio-temporal information. SC outsources a set of spatio-temporal tasks to a set of workers, i.e., individuals with mobile devices that perform the tasks by physically traveling to specified locations of interest. Protecting location privacy is an important concern in SC, as an adversary with access to individual whereabouts can infer sensitive details about a person (e.g., health status, political views). Due to the challenging nature of protecting worker privacy in SC, solutions for this problem are quite complex, and require tuning of several parameters to obtain satisfactory results. In this paper, we propose PrivGeoCrowd, a toolbox for interactive visualization and tuning of SC private task assignment methods. This toolbox is useful for several real-world entities that are involved in SC, such as: mobile phone operators that want to sanitize datasets with worker locations, spatial task requesters, and SC-service providers that match workers to tasks.\"",
        "Document: \"Privacy-preserving similarity measurement for access control policies. The emergence of global-scale infrastructures for outsourcing data and content to service providers (e.g., cloud computing) creates unprecedented opportunities for data owners to expand their operations and increase their customer base. On the other hand, each data owner (DO) has a certain set of access control policies, which may be different than those of the service providers (SP). Therefore, to enable effective outsourcing, it is important for the DOs to choose SPs with similar access control policies. Several techniques that measure policy similarity have been proposed in previous work, but they assume that policies are publicly accessible. However, in a global-scale environment without well-established relationships of trust, participants may not be willing to reveal their policies to every other stakeholder. Therefore, the need arises to perform policy similarity in a privacy-preserving manner. Specifically, we propose a technique that allows similarity evaluation of encrypted policies. Our technique relies on an existing encryption method for numerical data called asymmetric scalar product-preserving encryption (ASPE). ASPE allows answering of nearest-neighbor queries without the need to reveal the plaintext contents of either the query or the data. We adapt ASPE to support access control policies, and we present a case study of how private policy similarity evaluation is performed within our proposed framework.\"",
        "1 is \"Towards privacy-sensitive participatory sensing\", 2 is \"Approximate MaxRS in spatial databases\"",
        "Given above information, for an author who has written the paper with the title \"Outsourcing Search Services on Private Spatial Data\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003638": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Deep Recurrent Multi-instance Learning with Spatio-temporal Features for Engagement Intensity Prediction.':",
        "Document: \"A fuzzy-neural network approach to multisensor integration for obstacle avoidance of a mobile robot. In this paper, a novel fuzzy-neural network approach is proposed for obstacle avoidance of mobile robots through multisensor fusion. The proposed model is constituted with a precondition network and a conclusion network. In the precondition network, each rule matches the preconditions of fuzzy rules. The conclusion network generates conclusions of fuzzy rules. The total output is the weighted addition and the weight represents the applicability of each rule. The proposed model not only has the ability to handle vague information through the fuzzy logic but also has the ability to learn through the neural network. Multisensor integration based on the proposed fuzzy-neural networks is applied to obstacle avoidance of mobile robots, which adopt multiple ultrasonic sensors to detect the distance and direction of obstacles. The mobile robot can recognize the obstacles, the types of environment and generate collision-free motion by the fuzzy-neural network model. Simulation results show that the proposed model is capable of recognizing the environment and avoiding the obstacles and generating a collision-free path from the start point to the end point. \u00a9 2009, TSI\u00ae Press.\"",
        "Document: \"Experiment and analysis of active measurement for packet delay dynamics. Active measurements have formed the basis for much of our empirical efforts to understand Internet packet delay dynamics. Packet-average performance of user flow is getting more and more important for users, and especially for network service providers. But in network active measurement area, there are not empirical efforts to investigate the performance of active measurement for packet performance from user's standpoint. We quantitatively assess and compare the one-way delay statistics experienced by user flow and active probe flow based on simulation experiments, and find that: (1) Active measurement systematically underestimates statistics of packet delay experienced by user flow and the estimation error is far severe than can be ignored. (2) Increasing sampling frequency is almost helpless for the reducing of the estimation error. (3)The estimation error degrees of active measurement decrease as the increasing of the queue utilization. The above conclusions are based on active measurements using Poisson sampling and Periodic sampling. As they are mainly used sampling methods in active measurement area, so our conclusions indicate that current active method for measuring Internet packet delay suffer from system errors from user's standpoint.\"",
        "Document: \"Implantable CMOS neuro-stimulus chip for visual prosthesis. A prototype chip with 2\u00d72 pixels for implanting in blind patients affected by outer retinal degeneration is presented in this\n paper. This visual prosthesis chip imitates the degenerated photoreceptor cells, senses the incident light and stimulates\n the remaining healthy layers of retina or optic nerve. Each pixel integrates photodiode and stimulus pulse generator, converting\n the illumination on the eyes into 3-bit resolution bi-phasic current pulses. On-chip charge cancellation modules are used\n to discharge each electrode site for tissue safety. The prototype chip is designed and fabricated in 0.18-\u03bcm N-well CMOS (complementary\n metal oxide semiconductor) 1P6M Mix-signal process, with a \u00b12.5 V dual voltage supply. The functionality of the fabricated\n chip is demonstrated on anesthetized rabbits. Neural responses in visual cortex are successfully evoked by the neuro-stimulus\n chip through an on-board trigger interface and flexible electrode.\"",
        "Document: \"Sequential Point Cloud Upsampling by Exploiting Multi-Scale Temporal Dependency. In this work, we propose a new sequential point cloud upsampling method called SPU, which aims to upsample sparse, non-uniform, and orderless point cloud sequences by effectively exploiting rich and complementary temporal dependency from multiple inputs. Specifically, these inputs include a set of multi-scale short-term features from the 3D points in three consecutive frames (i.e., the previous/cu...\"",
        "Document: \"Stability of almost periodic solution for a generalized neutral-type neural networks with delays. In this paper, a generalized neutral-type neural networks with delays is studied. Some simple sufficient conditions are obtained for guaranteeing the existence, global asymptotic stability and exponential stability of the unique almost periodic solution for the system by using fixed point theorem, Lyapunov functional method and comparison theorem, respectively. It is interesting that, in some special cases, we show that the condition for guaranteeing the existence of almost periodic solution also yields the global exponential stability of it.\"",
        "Document: \"Prototype hierarchy based clustering for the categorization and navigation of web collections. This paper presents a novel prototype hierarchy based clustering (PHC) framework for the organization of web collections. It solves simultaneously the problem of categorizing web collections and interpreting the clustering results for navigation. By utilizing prototype hierarchies and the underlying topic structures of the collections, PHC is modeled as a multi-criterion optimization problem based on minimizing the hierarchy evolution, maximizing category cohesiveness and inter-hierarchy structural and semantic resemblance. The flexible design of metrics enables PHC to be a general framework for applications in various domains. In the experiments on categorizing 4 collections of distinct domains, PHC achieves 30% improvement in \u00bcF1 over the state-of-the-art techniques. Further experiments provide insights on performance variations with abstract and concrete domains, completeness of the prototype hierarchy, and effects of different combinations of optimization criteria.\"",
        "Document: \"Parallelization of a level set method for simulating dendritic growth. Processor virtualization is a parallelization technique that may be used to enhance the performance of parallel applications through the improvement of cache performance, overlapping of communication and computation. In this study, we use the processor virtualization technique to parallelize the level set method for solving solidification problems. Numerical results on a distributed memory machine are reported to show the performance of the resulting level set solver, and demonstrate the advantages of using processor virtualization.\"",
        "Document: \"Joint Feature Selection and Subspace Learning for Cross-modal Retrieval. Cross-modal retrieval has recently drawn much attention due to the widespread existence of multimodal data. It takes one type of data as the query to retrieve relevant data objects of another type, and generally involves two basic problems: the measure of relevance and coupled feature selection. Most previous methods just focus on solving the first problem. In this paper, we aim to deal with both problems in a novel joint learning framework. To address the first problem, we learn projection matrices to map multimodal data into a common subspace, in which the similarity between different modalities of data can be measured. In the learning procedure, the \u211321-norm penalties are imposed on the projection matrices separately to solve the second problem, which selects relevant and discriminative features from different feature spaces simultaneously. A multimodal graph regularization term is further imposed on the projected data, which preserves the inter-modality and intra-modality similarity relationships. An iterative algorithm is presented to solve the proposed joint learning problem, along with its convergence analysis. Experimental results on cross-modal retrieval tasks demonstrate that the proposed method outperforms the state-of-the-art subspace approaches.\"",
        "Document: \"Fast postplacement optimization using functional symmetries. The timing-convergence problem arises because estimations made during logic synthesis may not be met during physical design. In this paper, an efficient rewiring engine is proposed to explore maximal freedom after placement. The most important feature of this approach is that the existing placement solution is left intact throughout the optimization. A linear-time algorithm is proposed to detect functional symmetries in the Boolean network which are then used as the basis for rewiring. Integration with an existing gate-sizing algorithm further proves the effectiveness of our technique. Three applications are demonstrated: delay, power, and reliability optimization.\"",
        "Document: \"Ant colony optimization for symmetrical FPGA placement. Field programmable gate arrays (FPGAs) are becoming increasingly important implementation platforms for digital circuits. This paper presents a method for symmetrical FPGA placement based on ant colony optimization (ACO). Also, we take the routing congestion into consideration by introducing a congestion factor in our algorithm. Experimental results show that compared with the state-of-the-art FPGA place and route tool VPR, ACO algorithm achieves promising performance, in terms of routing channel density.\"",
        "1 is \"Unsupervised Visual Domain Adaptation Using Subspace Alignment\", 2 is \"Concept space connected to knowledge processing for supporting creative design\"",
        "Given above information, for an author who has written the paper with the title \"Deep Recurrent Multi-instance Learning with Spatio-temporal Features for Engagement Intensity Prediction.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003649": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'On the recursive sequence xn=axn\u22121+bxn\u22122c+dxn\u22121xn\u22122':",
        "Document: \"The Diagnosabilities of and Diagnosis Algorithms for Regular Networks under Two Three-Valued Models. The two three-valued test models of system-level diagnosis, t[x] model and t/x model, are of practical importance in some real situations. This paper addresses the diagnosabilities of regular networks under the two three-valued models as well as the design of the corresponding diagnosis algorithms. First, we derive sufficient conditions for t[x]-diagnosable and t/x-diagnosable regular networks, respectively. Then we propose efficient one-step diagnosis algorithms for diagnosable regular networks under the two three-valued models, respectively.\"",
        "Document: \"An epidemic model of computer viruses with vaccination and generalized nonlinear incidence rate. This paper aims to explore the influence of vaccination on the diffusion of computer viruses under more reasonable assumptions. By incorporating a vaccination probability in the SIRS model with generalized nonlinear incidence rate, a novel epidemic model of computer viruses is established. A thorough analysis of this model shows that, depending on the value of the basic reproduction number, either the virus-free equilibrium or the endemic equilibrium is globally asymptotically stable. Simulation results not only justify the proposed model, but also demonstrate the effectiveness of vaccination. Based on a parameter analysis for the model, some effective strategies for inhibiting the virus prevalence are posed.\"",
        "Document: \"Conditional Fault Tolerance Of Hypermesh Optical Interconnection Networks. Fault tolerance is especially important for interconnection networks, vast influencing the performance of the parallel processing systems underlying the corresponding networks. This paper studies the fault tolerance of radix-k n-dimensional hypermesh optical interconnection networks, determines the connectivity of partial hypermesh, and derives the conditional connectivity of hypermesh provided that each adjacent set cannot be faulty simultaneously. Under this condition, the hypermesh networks can tolerate up to 2n(k-1)-k-1 fault processors without being disrupted, implying that when the number of dimension n, (respectively, radix-k) is a fixed value in the hypermesh network, the larger the value of radix-k (respectively, dimension n) is, the higher the reliability and availability of the network becomes.\"",
        "Document: \"A comparison-based diagnosis algorithm tailored for crossed cube multiprocessor systems. Comparison-based diagnosis is an effective approach to system-level fault diagnosis. Under the Maeng\u2013Malek comparison model (MM* model), Sengupta and Dahbura proposed an O(N5) diagnosis algorithm for general diagnosable systems with N nodes. Thanks to lower diameter and better graph embedding capability as compared with a hypercube of the same size, the crossed cube has been a promising candidate for interconnection networks. In this paper, we propose a fault diagnosis algorithm tailored for crossed cube connected multicomputer systems under the MM* model. By introducing appropriate data structures, this algorithm runs in O(Nlog22N) time, which is linear in the size of the input. As a result, this algorithm is significantly superior to the Sengupta\u2013Dahbura's algorithm when applied to crossed cube systems.\"",
        "Document: \"Routing and wavelength assignment for 3-ary n-cube in array-based optical network. The k-ary n-cube, denoted Q\"n^k, is one of the popular communication patterns of parallel algorithms. This paper addresses the routing and wavelength assignment for Q\"n^3 communication pattern in array-based WDM optical network. By using congestion estimation and giving a routing and wavelength assignment strategy, we prove that the optimal number of wavelengths is 3^n-1.\"",
        "Document: \"Fault Tolerance Of Mobius Cubes Under Two Forbidden Fault Set Models. An n-dimensional Mobius cube, 0MQ(n) or 1MQ(n), is a variation of n-dimensional cube Q(n) which possesses many attractive properties such as significantly smaller communication delay and stronger graph-embedding capabilities. In some practical situations, the fault tolerance of a distributed memory multiprocessor system can be measured more precisely by the connectivity of the underlying graph under forbidden fault set models. This article addresses the connectivity of 0MQ(n)/1MQ(n), under two typical forbidden fault set models. We first prove that the connectivity of 0MQ(n)/1MQ(n) is 2n - 2 when the fault set does not contain the neighborhood of any vertex as a subset. We then prove that the connectivity of 0MQ(n)/1MQ(n) is 3n - 5 provided that the neighborhood of any vertex as well as that of any edge cannot fail simultaneously These results demonstrate that 0MQ(n)/1MQ(n) has the same connectivity as Q(n) under either of the previous assumptions.\"",
        "Document: \"Dynamic wavelength assignment for realizing hypercube-based Bitonic sorting on wavelength division multiplexing linear arrays. As compared to its static counterpart, the dynamic wavelength assignment (DWA) strategy can greatly reduce the number of wavelengths required in realizing parallel algorithms on wavelength division multiplexing (WDM) networks. This article addresses the DWA problem for realizing the hypercube-based Bitonic sorting on WDM linear arrays. For that purpose, the notions of dimension congestion and dimension embedding are introduced. Then, two novel dimension embedding schemes of hypercubes in linear arrays are developed. The wavelength resources consumed by either of the two schemes are much less than that required by an optimal static embedding scheme of the same description. As a result, the hypercube-based Bitonic sorting can be performed on WDM linear arrays with limited wavelength resources.\"",
        "Document: \"Part-metric and its applications in discrete systems. This paper applies the part-metric method to study some types of higher-order symmetric difference equations with several different exponential parameters. These difference equations are proved to have unique equilibria and some useful inequalities regarding the difference equation functions are formulated. By use of the part-metric and a result given by Kruse and Nesemann (1999) [8], some sufficient conditions on the parameters are given to guarantee the global asymptotic stability of the equilibria. Furthermore, by the part-metric defined on matrices, this approach is also applicable to show the global asymptotic stability of some cyclic discrete dynamic systems. The results of this paper are considered a big improvement over many existing results found in the literature.\"",
        "Document: \"Stability of a generalized Putnam equation. Consider the rational difference equation xn+1=xn+xn\u22121+Axn\u22122xn\u22123Axnxn\u22121+xn\u22122+xn\u22123,n=0,1,2,\u2026, with initial conditions x\u22123>0,x\u22122>0,x\u22121>0,x0>0. We prove that c=1 is a globally asymptotically stable equilibrium of this equation if 1\u2264A<2. This result extends a previously known result.\"",
        "Document: \"Optimal doublecast path in hexagonal honeycomb mesh. The performance of a multicomputer is greatly dependent on the multicast strategy, i.e., how to deliver the same message from a source node to a number of destination nodes. Building an optimal multicast path is suited for multicast wormhole routing. A doublecast path is a multicast path with exactly two destination nodes. Hexagonal honeycomb mesh is a promising candidate for interconnection networks. This paper addresses the building of an optimal doublecast path on hexagonal honeycomb mesh. A theorem concerned with the length of an optimal doublecast path is established, and a time-optimal algorithm for building an optimal doublecast path is proposed.\"",
        "1 is \"Fault-free Hamiltonian cycles in twisted cubes with conditional link faults\", 2 is \"Degrees of Freedom Region of the MIMO Interference Channel With Output Feedback and Delayed CSIT\"",
        "Given above information, for an author who has written the paper with the title \"On the recursive sequence xn=axn\u22121+bxn\u22122c+dxn\u22121xn\u22122\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003679": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Numerical Solution of Non-Homogeneous Markov Processes through Uniformization':",
        "Document: \"A survey of experimental evaluation in indoor localization research. During the last decade, research in indoor localization and navigation has focused on techniques, protocols, and algorithms. The first International Conference on Indoor Positioning and Indoor Navigation (IPIN) was held in 2010. Since then, this annual conference showed the progress of research and technology. The variations of evaluation methods are significant in this field: they range from none, to extensive simulations, and real-world experiments under non-lab conditions. We look at the articles published in the proceedings of IPIN by IEEE Xplore from 2010 to 2014, and analyze the development of evaluation methods. We categorized 183 randomly selected papers, in respect to five different aspects. Namely: (1) the underlying system/technology in use, (2) the evaluation method for the proposed technique, (3) the method of ground truth data gathering, (4) the applied metrics, and (5) whether the authors establish a baseline for their work.\"",
        "Document: \"Detection and Analysis of Performance Deterioration in Mobile Offloading System. Offloading is an advanced technique to improve the performance of mobile devices by migrating heavy computation to remote powerful servers. The completion of an offloading task requires a reliable network connection and a stable remote server. Therefore, the system performance is determined by the network quality and the server service rate. In this paper, we present an experiment to measure the system performance using task completion time as metric. We detect that system performance is not stable at some moments. For identifying the cause of performance deterioration, we analyse the impact of server and network. It is well known that software aging is a common phenomenon in computer systems, which may cause frequent software faults and finally result in system outage. As rejuvenation is used to counteract software aging, through our experiment, we observed the impact of periodical rejuvenation on the offloading system. Although the rejuvenation process impairs the system performance temporarily, long-term stability of the server is guaranteed. We further explore the impact of poor network quality, which is the main factor degrading the system performance. To improve the system performance, we proved that restart is an efficient mechanism in theory.\"",
        "Document: \"Reducing the cost of generating APH-Distributed random numbers. Phase-type (PH) distributions are proven to be very powerful tools in modelling and analysis of a wide range of phenomena in computer systems. The use of these distributions in simulation studies requires efficient methods for generating PH-distributed random numbers. In this work, we discuss algorithms for generating random numbers from PH distributions and propose two algorithms for reducing the cost associated with generating random numbers from Acyclic Phase-Type distributions (APH).\"",
        "Document: \"Analysis of the Energy-Performance Tradeoff for Delayed Mobile Offloading.   This paper has been withdrawn by the author \"",
        "Document: \"Tradeoff Analysis for Mobile Cloud Offloading Based on an Additive Energy-Performance Metric. Mobile offloading migrates heavy computation from mobile devices to powerful cloud servers. It is a promising technique that can save energy of the mobile device while keeping job completion time low when cloud servers are available and accessible. The benefit obtained by offloading greatly depends on whether it is applied at the right time in the right way. In this paper, we use queueing models to minimize a weighted sum of energy consumption and performance expressed in the Energy-Response time Weighted Sum (ERWS) metric. We consider different offloading policies (static and dynamic), where arriving jobs are processed either locally or remotely in the cloud. Offloading can be performed via WLAN or via a cellular network. The transmission techniques differ in energy requirement and speed. We find that the dynamic offloading policy derived from the tradeoff offloading policy (TOP) outperforms other policies like the random selection of transmission channel by a significant margin. This is because the dynamic offloading policy considers the increase in each queue and the change in metric that newly arriving jobs bring in should they be assigned to that queue. The ERWS metric can be reduced more by considering either energy consumption or response time and it is minimal when optimising only energy consumption.\"",
        "Document: \"Adaptivity metric and performance for restart strategies in web services reliable messaging. Adaptivity, the ability of a system to adapt itself to its environment, is a key property of autonomous systems. In his paper we propose a benefit-based framework for the efinition of metrics to measure adaptivity. We demonstrate the application of the framework in a case study of the adaptivity of restart strategies for Web Services Reliable Messaging (WSRM). Using the framework, we define two adaptivity metrics for a fault-injection-driven evaluation of the adaptivity of three restart strategies in aWSRM implementation. The adaptivity measurements are complemented by a thorough discussion of the performance of the restart strategies.\"",
        "Document: \"Micro and macro views of discrete-state markov models and their application to efficient simulation with phase-type distributions. No abstract available.\n\n\"",
        "Document: \"Reducing Task Completion Time in Mobile Offloading Systems through Online Adaptive Local Restart. Offloading is an advanced technique to improve the performance of mobile devices. In a mobile offloading system, heavy computations are migrated from resource constrained mobile devices to powerful cloud servers through a wireless network connection. The unreliable wireless network often disturbs system operation. Task completion can be delayed or interrupted by congestion or packet loss in the network. To deal with this problem the offloaded jobs can be locally restarted and completed in the mobile device itself. In this paper, we propose a dynamic scheme to determine whether and when to locally restart a task. First, we design an experiment to explore the impact of packet loss and delay in unreliable networks on the completion time of an offloading task. Then, we mathematically derive the prerequisites for local restart and selection of the optimal timeout. The analysis result confirms that local restart is beneficial when the distribution of task completion time has high variance. Further, a dynamic local restart scheme is proposed for mobile applications. This scheme keeps track of the variance of the probability density function of the distribution of task completion time. This is done using a dynamic histogram, which collects and updates data at run time. The efficiency of the local restart scheme is confirmed by experimental results. The experiment shows that local restart at the right time achieves better performance than always offloading.\"",
        "Document: \"Dynamic decision making for candidate access point selection. In this paper, we solve the problem of candidate access point selection in 802.11 networks, when there is more than one access point available to a station. We use the QBSS (quality of service enabled basic service set) Load Element of the new WLAN standard 802.11e as prior information and deploy a decision making algorithm based on reinforcement learning. We show that using reinforcement learning, wireless devices can reach more efficient decisions compared to static methods of decision making which opens the way to a more autonomic communication environment. We also present how the reinforcement learning algorithm reacts to changing situations enabling self adaptation.\"",
        "Document: \"Accelerating task completion in mobile offloading systems through adaptive restart. Mobile application offloading is an efficient technique to unload the burden of intensive computation from thin clients to powerful servers. In a mobile offloading system, cloud computing is utilized to complete some heavy tasks which are migrated from resource-constrained mobile devices to the Cloud. To assure system performance, the quality of the wireless network connection plays an important role. In previous work we experimentally explored the impact of packet loss and delay in wireless networks on the completion time of an offloading task. We investigated a local restart mechanism to mitigate these effects. In the presence of unreliable communication, once the waiting time for the response of a cloud server exceeds a given threshold, exploiting the local resources of a mobile client can accelerate the task completion.\"",
        "1 is \"Generic on-line discovery of quantitative models for service level management\", 2 is \"Performance Analysis of Offloading Systems in Mobile Wireless Environments\"",
        "Given above information, for an author who has written the paper with the title \"Numerical Solution of Non-Homogeneous Markov Processes through Uniformization\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003690": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Healing on the cloud: Secure cloud architecture for medical wireless sensor networks':",
        "Document: \"Key management with host mobility in dynamic groups. Key management, which is an important building bloc in securing group communications, has received a particular attention in both academic and industry research communities. This is due to the economical relevance of group-based applications. The key management concerns the distribution and updates of the key material each time a member joins or leaves the group. The dynamic aspect of group applications due to free membership joins and leaves in addition to members' mobility makes difficult the design of efficient and scalable key management protocols. In this paper, we propose a new key management protocol to secure group communications where we consider the mobility of nodes in a mobile environment with a null rekeying cost. Protocol simulations show that our protocol achieves better performance in terms of rekeying.\"",
        "Document: \"Adaptive Group Key Management Protocol for Wireless Communications. Group-oriented services and wireless communication networks are among the emerging technologies of the last few years. Group key management, which is an important building bloc in securing group communication, has received a particular attention in both academic and industry research communities. This is due to the economical relevance of group-based applications, such as video on demand, videoconferencing, collaborative work. The key management concerns the distribution and updates of the key material each time a member joins or leaves the group. The dynamic aspect of group applications due to free membership joins and leaves in addition to members' mobility makes difficult the design of efficient and scalable key management protocols. Indeed, to secure group communication in mobile environment, the protocol must deal not only with dynamic group membership but also with dynamic member location. Thus, the challenges in designing secure and scalable key management protocols are: the dynamic updates of the key caused by frequent joins and leaves, the large size of the group and the mobility of group members. Many researches have addressed the first and the third challenges. However, the mobility challenge has not been widely addressed. In this paper, we present our solution for group key management with a mobility support. Our protocol focuses on the above three challenges. It is highly scalable to dynamic groups and treats the nodes' mobility with a null re-keying cost and keeps perfect backward and forward secrecies. Our simulation studies show that our protocol makes better performance compared to other protocols while reducing the overall overhead and the number of re-keying messages and has no security failures.\"",
        "Document: \"SEDAN: Secure and Efficient protocol for Data Aggregation in wireless sensor Networks. Energy is a scarce resource in Wireless Sensor Networks. Some studies show that more than 70% of energy is consumed in data transmission. Since most of the time, the sensed information is redundant due to geographically collocated sensors, most of this energy can be saved through data aggregation. Furthermore, data aggregation improves bandwidth usage. Unfortunately, while aggregation eliminates redundancy, it makes data integrity verification more complicated since the received data is unique. In this paper, we present a new protocol that provides secure aggregation for wireless sensor networks. Our protocol is based on a two hops verification mechanism of data integrity. Our solution is essentially different from existing solutions in that it does not require referring to the base station for verifying and detecting faulty aggregated readings, thus providing a totally distributed scheme to guarantee data integrity. We carried out simulations using TinyOS environment. Simulation results show that the proposed protocol yields significant savings in energy consumption while preserving data integrity.\"",
        "Document: \"An Efficient and Privacy-Preserving Similarity Evaluation for Big Data Analytics. Big data systems are gathering more and more information in order to discover new values through data analytics and depth insights. However, mining sensitive personal information breaches privacy and degrades services' reputation. Accordingly, many research works have been proposed to address the privacy issues of data analytics, but almost seem to be not suitable in big data context either in data types they support or in computation time efficiency. In this paper we propose a novel privacy-preserving cosine similarity computation protocol that will support both binary and numerical data types within an efficient computation time, and we prove its adequacy for big data high volume, high variety and high velocity.\"",
        "Document: \"Batch-based CP-ABE with attribute revocation mechanism for the Internet of Things. Ciphertext-Policy Attribute-Based Encryption (CP-ABE) is an extremely powerful asymmetric encryption mechanism, it allows to achieve fine-grained access control. However, there is no solution to manage efficiently key/attribute revocation problem in CP-ABE scheme. Key revocation problem is very important in dynamic environment like Internet of Things (IoT), where billions of things are connected together and are cooperating without human intervention. Existing solutions are not efficient due to their overhead (traffic) and complexity (big access trees). Other solutions require the use of powerful semi-trusted proxies to re-encrypt data. The proposed solution in this paper called Batch-Based CP-ABE reduces the complexity and the overhead, and does not require extra nodes in the system. We propose to split time axis into intervals (time slots) and to send only the necessary key parts to allow refreshing the secrets keys. An analysis is conducted on the way to choose the best time slot duration in order to maximize system performances and minimize average waiting time.\"",
        "Document: \"BitTorrent Worm Sensor Network: P2P Worms Detection and Containment. Peer-to-peer (p2p) networking technology has gained popularity as an efficient mechanism for users to obtain free services without the need for centralized servers. Protecting these networks from intruders and attackers is a real challenge. One of the constant threats on P2P networks is the propagation of active worms. In 2007, Worms have caused damages worth the amount of 8,391,800 USD in the United States alone. Nowadays, BitTorrent is becoming more and more popular, mainly due to its fair load distribution mechanism. Unfortunately, BitTorrent is particularly vulnerable to active worms. In this paper, we propose a novel worm detection system in BitTorrent and evaluate it. We show that our solution can detect various worm scans before 1% of the vulnerable hosts are infected in worst case scenarios. Our solution, the BitTorrent Worm Sensor Network, is built over a network of immunized agents, which their main job is to efficiently stop worm spread in BitTorrent.\"",
        "Document: \"A New Scalable Key Pre-Distribution Scheme for WSN. Given the sensitivity of the potential WSN applications, security emerges as a challenging issue in these networks. Because of the resource limitations, symmetric key establishment is one favorite paradigm for securing WSN. One of the main concerns when designing a key management scheme for WSN is the network scalability. Indeed, the protocol should support a large number of nodes to enable a large scale deployment of the network. In this paper, we propose a new highly scalable key establishment scheme for WSN. For that purpose, we make use, for the first time, of the unital design theory. We show that the basic mapping from unitals to pairwise key establishment allows to achieve an extremely high network scalability while degrading the key sharing probability. Then, we propose a new unital-based key pre-distribution approach which provides high network scalability and good key sharing probability. We conduct analytical analysis to compare our solutions to existing ones, the obtained results show that our approach enhances the network scalability while providing good overall performances. Also, we show that our solutions reduce significantly the storage overhead at equal network size compared to existing solutions.\"",
        "Document: \"Secure and Scalable Cloud-Based Architecture for e-Health Wireless Sensor Networks. There has been a host of research works on wireless sensor networks for medical applications. However, the major shortcoming of these efforts is a lack of consideration of data management. Indeed, the huge amount of high sensitive data generated and collected by medical sensor networks introduces several challenges that existing architectures cannot solve. These challenges include scalability, availability and security. In this paper, we propose an innovative architecture for collecting and accessing large amount of data generated by medical sensor networks. Our architecture resolves all the aforementioned challenges and makes easy information sharing between healthcare professionals. Furthermore, we propose an effective and flexible security mechanism that guarantees confidentiality, integrity as well as fine grained access control to outsourced medical data. This mechanism combines several cryptographic schemes to achieve high flexibility and performance.\"",
        "Document: \"Security in device-to-device communications: a survey. Device-to-device (D2D) communication is a promising technology for the next generation mobile communication networks (5G). Indeed, it is expected to allow high throughput, reduce communication delays and reduce energy consumption and traffic load. D2D technology will enhance the capacity and the performance of traditional cellular networks. Security issues must be considered in all types of commun...\"",
        "Document: \"Mediator-Based Immediate Attribute Revocation Mechanism for CP-ABE in Multicast Group Communications. Attribute Based Encryption (ABE) scheme is a mechanism that allows implementing cryptographic fine grained access control to shared information. It achieves information sharing of type one-to-many users, without considering the number of users and their identities. However, original ABE systems presents some drawbacks, especially the non-efficiency of their attribute/key revocation mechanisms.Based on Ciphertext-Policy ABE (CP-ABE) scheme, we propose an efficient proxy-based immediate private key update for multicast group communications. Our solution does require neither re-encrypting cipher-texts, nor affecting other users (Updating secret keys).The proxy that has been introduced plays the role of a necessary semi-trusted assistant during the decryption process without taking decisions about who is eligible or not to decrypt data.Finally, we demonstrate that our scheme guarantees security requirements that we target and we also show through analysis that our scheme achieves effectively its goals.\"",
        "1 is \"Host multicast: a framework for delivering multicast to end users\", 2 is \"PowerTOSSIM z: realistic energy modelling for wireless sensor network environments\"",
        "Given above information, for an author who has written the paper with the title \"Healing on the cloud: Secure cloud architecture for medical wireless sensor networks\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003844": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Generalized Pipelined Tomlinson\u2013Harashima Precoder Design Methodology With Build-In Arbitrary Speed-Up Factors':",
        "Document: \"A novel trellis-based searching scheme for EEAS-based CORDIC algorithm. The CORDIC algorithm is a well-known iterative method for the computation of vector rotation. For applications that require forward rotation (or vector rotation) only, the extended elementary angle set (EEAS) Scheme provides a relaxed approach to speed up the operation of the CORDIC algorithm. When determining the parameters of EEAS-based CORDIC algorithm, two optimization problems are encountered. In the previous work, the greedy algorithm is suggested to solve these optimization problems. However, for an application that requires high-precision rotation operation, the results generated by the greedy algorithm may not be applicable. We propose a novel searching algorithm to overcome the aforementioned problem, called the trellis-based searching (TBS) algorithm. Compared with the greedy algorithm used in the conventional EEAS-based CORDIC algorithm, the proposed TBS algorithm yields apparent performance improvement. Moreover, the derivation of the error boundary as well as computer simulations are provided to support our arguments\"",
        "Document: \"A reduced-complexity fast algorithm for software implementation of the IFFT/FFT in DMT systems. The discrete multitone (DMT) modulation/demodulation scheme is the standard transmission technique in the application of asymmetric digital subscriber lines (ADSL) and very-high-speed digital subscriber lines (VDSL). Although the DMT can achieve higher data rate compared with other modulation/demodulation schemes, its computational complexity is too high for cost-efficient implementations. For example, it requires 512-point IFFT/FFT as the modulation/demodulation kernel in the ADSL systems and even higher in the VDSL systems. The large block size results in heavy computational load in running programmable digital signal processors (DSPs). In this paper, we derive computationally efficient fast algorithm for the IFFT/FFT. The proposed algorithm can avoid complex-domain operations that are inevitable in conventional IFFT/FFT computation. The resulting software function requires less computational complexity. We show that it acquires only 17% number of multiplications to compute the IFFT and FFT compared with the Cooly-Tukey algorithm. Hence, the proposed fast algorithm is very suitable for firmware development in reducing the MIPS count in programmable DSPs.\"",
        "Document: \"VLSI design of a variable-length FFT/IFFT processor for OFDM-based communication systems. The technique of orthogonal frequency division multiplexing (OFDM) is famous for its robustness against frequency-selective fading channel. This technique has been widely used in many wired and wireless communication systems. In general, the fast Fourier transform (FFT) and inverse FFT (IFFT) operations are used as the modulation/demodulation kernel in the OFDM systems, and the sizes of FFT/IFFT operations are varied in different applications of OFDM systems. In this paper, we design and implement a variable-length prototype FFT/IFFT processor to cover different specifications of OFDM applications. The cached-memory FFT architecture is our suggested VLSI system architecture to design the prototype FFT/IFFT processor for the consideration of low-power consumption. We also implement the twiddle factor butterfly processing element (PE) based on the coordinate rotation digital computer (CORDIC) algorithm, which avoids the use of conventional multiplication-and-accumulation unit, but evaluates the trigonometric functions using only add-and-shift operations. Finally, we implement a variable-length prototype FFT/IFFT processor with TSMC 0.35 \u00b5m 1P4M CMOS technology. The simulations results show that the chip can perform (64-2048)-point FFT/IFFT operations up to 80 MHz operating frequency which can meet the speed requirement of most OFDM standards such as WLAN, ADSL, VDSL (256 \u223c 2K), DAB, and 2k-mode DVB.\"",
        "Document: \"High-performance VLSI architecture of adaptive decision feedback equalizer based on predictive parallel branch slicer (PPBS) scheme. Among existing works of high-speed pipelined adaptive decision feedback equalizer (ADFE), the pipelined ADFE using relaxed look-ahead technique results in a substantial hardware saving than the parallel processing or Look-ahead approaches. However, it suffers from both the signal-to-noise ratio (SNR) degradation and slow convergence rate. In this paper, we employ the predictive parallel branch slicer (PPBS) to eliminate the dependencies of the present and past decisions so as to reduce the iteration bound of decision feedback loop of the ADFE. By adding negligible hardware complexity overheads, the proposed architecture can help to improve the output mean-square error (MSE) of the ADFE compared with the Relaxed Look-ahead ADFE architecture. Moreover, we show the superior performance of the proposed pipelined ADFE by using theoretical derivations and computer simulation results. A VLSI design example using Avant! 0.35-\u00b5m CMOS standard cell library is also illustrated. From the post-layout simulation results, we can see that the PPBS scheme requires only 38.4% gate count overhead, but it can help to reduce the critical path from 7.06 to 4.69 ns so as to meet very high-speed data transmission systems.\"",
        "Document: \"Parallel Architecture Core (PAC)--the First Multicore Application Processor SoC in Taiwan Part I: Hardware Architecture & Software Development Tools. In order to develop a low-power and high-performance SoC platform for multimedia applications, the Parallel Architecture Core (PAC) project was initiated in Taiwan in 2003. A VLIW digital signal processor (PACDSP) has been developed from a proprietary instruction set with multimedia-rich instructions, a complexity-effective microarchitecture with an innovative distributed & ping-pong register organization and variable-length VLIW encoding, to a highly-configurable soft IP with several successful silicon implementations. A complete toolchain with an optimizing C compiler has also been developed for PACDSP. A dual-core PAC SoC has been designed and fabricated, which consists of a PACDSP core, an ARM9 core, scratchpad memories, and various on-chip peripherals, to demonstrate the outstanding performance and energy efficiency for multimedia processing such as the real-time H.264 codec. The first part of the two introductory papers of PAC describes the hardware architecture of the PACDSP core, its software development tools, and the PAC SoC with dynamic voltage and frequency scaling (DVFS).\"",
        "Document: \"DSP engine design for LINC wireless transmitter systems. Linear amplification with nonlinear components (LINC) technique is a linearization technique for power amplifier designs. By using LINC, the nonlinear power amplifier with high power efficiency can be used to amplify the input signal in a transmitter system with high linearity performance. In this paper, we propose a DSP engine design for LINC wireless transmitter under the WCDMA specification. New algorithms and architectures are proposed, and they can reduce the hardware cost efficiently. We also utilize Agilent ADS to perform mixed-mode system simulation to verify our design. The DSP engine is implemented by Verilog and synthesized with UMC 0.18 mum process. The DSP engine area is 0.155 mm2 and the maximum clock frequency is 117 MHz. Finally, we use a FPGA to verify our design\"",
        "Document: \"Regional ACO-based routing for load-balancing in NoC systems. Ant Colony Optimization (ACO) is a problem-solving technique that was inspired by the related research on the behavior of real-world ant colony. In the domain of Network-on-chip (NoC), ACO-based adaptive routing has been applied to achieve load-balancing effectively with historical information. However, the cost of the ACO network pheromone table is too high, and this overhead grows fast with the scaling of NoC. In order to fix this problem, it is essential to model the ACO algorithm in more careful consideration of the system architecture, available hardware resource, and appropriate transformation from the ant colony metaphor. In this paper, we analyzed the NoC network characteristic and bring about the corresponding issues of implementing ACO on NoC. We proposed a Regional ACO-based routing (RACO) with static and dynamic regional table forming technique to reduce the cost of table, share pheromone information, and adopt look-ahead model for further load-balancing. The experimental results show that RACO can be implemented with less memory, less cost increase on scaling, and better performance of load-balancing compared to traditional ACO-based routing.\"",
        "Document: \"Parallel Architecture Core (PAC)--the First Multicore Application Processor SoC in Taiwan Part II: Application Programming. Two representative multimedia applications--AAC and H.264/AVC decoders on the parallel architecture core (PAC) SoC are introduced in the second part of the two introductory papers. The applications have been programmed on the PACDSP core and the PAC SoC to demonstrate the high-performance, low-power DSP computations and the effectiveness of the dynamic voltage and frequency scaling (DVFS) capability on the heterogeneous multicore SoC. First, techniques to exploit data- and instruction-level parallelisms existing in the application kernels are described for performance optimizations on the clustered VLIW architecture of PACDSP with the distributed register organization. Next, two variation techniques of asymmetric programming model are introduced by examples of decoders. Then, the energy efficiency of the programmable multimedia SoC is demonstrated using an innovative power-aware H.264/AVC decoder. Finally, a DVFS-aware framework for soft real-time video playback is provided by extending the power-aware decoding scheme. The work provides practical references of realizing multimedia applications on PAC SoC suitable for rich-function and resource constraint portable devices.\"",
        "Document: \"A stroke severity monitoring system based on quantitative modified multiscale entropy. Stroke is the leading cause of death and disability worldwide. This requires significant resources in health-care costs. In addition to physical examination and brain imaging, medical staff need a more quantitative and continuous method to reveal and monitor the severity of patients. This paper proposed a novel stroke severity monitoring system based on a nonlinear method-quantitative modified multiscale entropy (qmMSE). National Institutes of Health Stroke Scale (NIHSS) is adopted as reference of the severity of stroke patients. In the intensive care unit (ICU) admitted acute stroke patients, our proposed qmMSE feature has significant (p-value equals to 0.0026) difference between high NIHSS groups and low NIHSS groups. QmMSE not only highly correlates to NIHSS, but also consists with other clinical parameters, such as stroke volume and Glasgow Coma Scale (GCS). Beside, our proposed method has better significance in patient with ischemic stroke in cortical region. The p-value reaches 0.0008.\"",
        "Document: \"Cost-efficient parallel lattice VLSI architecture for the IFFT/FFT in DMT transceiver technology. The discrete multitone (DMT) modulation/demodulation scheme is the standard transmission technique in the application of asymmetric digital subscriber lines (ADSL). Although the DMT can achieve a higher data rate compared with other modulation/demodulation schemes, its computational complexity is too high for cost-efficient implementations. For example, it requires 512-point IFFT/FFT as the modulation/demodulation kernel. The large block size results in heavy computational load in running programmable DSP processors. It also makes VLSI implementation not feasible. We derive the parallel lattice structure for the IFFT/FFT based on the time-recursive approach. The resulting architectures are regular, modular, and without global communications so that they are very suitable for VLSI implementation. Also, the proposed structure requires only 11% of the multipliers and 9% of the adders compared with the direct implementation approach\"",
        "1 is \"Two simple stopping criteria for turbo decoding\", 2 is \"Frequent Subgraph Discovery\"",
        "Given above information, for an author who has written the paper with the title \"Generalized Pipelined Tomlinson\u2013Harashima Precoder Design Methodology With Build-In Arbitrary Speed-Up Factors\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003890": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Maximum Network Lifetime In Interference-Aware Wimax/802.16 Mesh Centralized Scheduling':",
        "Document: \"Generalized Convex Multiplicative Programming via Quasiconcave Minimization. We present a new method for minimizing the sum of a convex function and aproduct of k nonnegative convex functions over a convex set. This problem isreduced to a k-dimensional quasiconcave minimization problem which is solvedby a conical branch-and-bound algorithm. Comparative computational results areprovided on test problems from the literature.\"",
        "Document: \"Uniquely Solvable Quadratic Boolean Equations. Quadratic boolean equations with a unique solution are characterized. A linear-time algorithm is proposed to recognize them.\"",
        "Document: \"On the Relations between Probabilistic Logic and p-CMS. Abstract We discuss the relationship between,probabilis\u00ad tic logic and  CMS.,Given a set of logical sen\u00ad tences and their probabilities of being true, the outcome,of a probabilistic,logic system,con\u00ad sists of lower and upper bounds on the prob\u00ad ability of an additional,sentence,to be true. These bounds,are computed,using a linear pro\u00ad gramming formulation. In  -CMS systems, the outcome,is defined by the probabilities,of the support,and,the plausibility (with the assump\u00ad tions,being,independent),after a first phase which,consists of computing,the prime,implicants depending only on the variables,of the assumptions.,We propose,to reformulate,a CMS system without independence conditions on the assumptions, using the linear program\u00ad ming framework of probabilisticlogic, and show how,to exploit its particular structure,to solve it efficiently. When,an independence,condition is imposed on the assumptions,the two systems give different results. Comparisons,are made,on small problems,using the assumption-basedev\u00ad idential language,program,(ABEL) of [Anrig et a/., 1998] and the PSAT program of [Jaumard eta/., 1991].\"",
        "Document: \"A cross layer optimization scheme for WDM PON network design and dimensioning. We propose a novel cross layer optimization scheme for the design and dimensioning of greenfield WDM PON networks. For a given geographical location of the OLT, the ONUs and their corresponding aggregated traffic demand, we propose a generic integer linear programming (ILP) model which optimally and simultaneously: (i) congregates the ONUs into clusters, (ii) determines the type (splitter/AWG) and number of output ports of the passive switching equipment for all clusters so that all ONUs can be served, (iii) identifies the location of the switching equipment, (iv) determines the proper link dimensioning so as to allow the provisioning of the overall aggregated traffic demand destined to/from the ONUs.The ILP model not only includes the physical layer constraints of PON (i. e., power attenuation and splitting ratio), but the optical layer constraints as well (i. e., number wavelengths carried by the optical fibers depending on the traffic and on the selected switching equipment). The resulting model is therefore the most general model proposed so far, and it guarantees the optimal solution in terms of minimum deployment cost for greenfield WDM PON, with a two stage architecture. Computational results demonstrate the validation and effectiveness of the proposed solution scheme on various data sets with up to 128 ONUs.\"",
        "Document: \"Cluster analysis and mathematical programming. Abstract Given a set of entities, Cluster Analysis aims at finding subsets, called clusters, which are homogeneous and/or well separated. As many types of clustering and criteria for homogeneity or separation are of interest, this is a vast field. A survey is given from a mathematical programming,viewpoint. Steps of a clustering study, types of clustering and criteria are discussed. Then algorithms for hierarchical, partitioning, sequential, and additive clustering are studied. Emphasis is on solution methods, i.e., dynamic programming, graph theoretical algorithms, branch-and-bound, cutting planes, column generation and heuristics. R\u00b4esum\u00b4e\"",
        "Document: \"On the efficiency of stream line effect for contention avoidance in optical burst switching networks. The OBS paradigm is a very promising all-optical transmission paradigm; however, OBS still suffers from burst losses because of burst contentions. Consequently, several loss-less approaches were proposed by various authors. In this paper, we investigate further one of them, the CAROBS framework, originally proposed by Coutelen et al. (2010) 15. CAROBS uses electronic buffering for contention resolution that requires careful routing strategies in order not to entail too many new contentions, and then buffering as a result of these contentions. This led us to look at the stream-line effect in order to devise efficient routing. The streamline effect has not yet been much exploited except for its straightforward implementation. Firstly, we empirically verify the impact of buffering within a stream-line effect framework, and discuss the significance of the results. Secondly, we devise an optimized request provisioning taking advantage of the stream-line effect with a compact integer linear programming (ILP) model, and then with a decomposition ILP model. Thirdly, we proceed with extensive numerical experiments. We run a time domain analysis of the routing obtained from the last ILP model for four referential network topologies. Results show very appealing properties of the CAROBS framework, and in particular, a maximal wavelength efficiency higher than 70%.\"",
        "Document: \"Equivalence of some LP-based lower bounds for the Golomb ruler problem. The Golomb Ruler problem consists in finding n integers such that all possible differences are distinct and such that the largest difference is minimum. We review three lower bounds based on linear programming that have been proposed in the literature for this problem, and propose a new one. We then show that these 4 lower bounds are equal. Finally we discuss some computational experience aiming at identifying the easiest lower bound to compute in practice.\"",
        "Document: \"Robust FIPP p-cycles against dual link failures. We propose a new generic flow formulation for Failure-Independent Path-Protecting (FIPP) p -cycles subject to multiple failures. While our new model resembles the decomposition model formulation proposed by Orlowski and Pioro (Networks, 2011 ) in the case of classical shared path protection, its originality lies in its adaptation to FIPP p -cycles. When adapted to that last pre-configured pre-cross connected protection scheme, the bandwidth sharing constraints must be handled in a different way in order to take care of the sharing along the FIPP p -cycles. It follows that, instead of a polynomial-time solvable pricing problem as in the model of Orlowski and Pioro (Networks, 2011 ), we end up with a much more complex pricing problem, which has an exponential number of constraints due to some subtour elimination constraints. Consequently, in order to efficiently solve the pricing problem, we consider: (i) a hierarchical decomposition of the original pricing problem; (ii) heuristics in order to go around the large number of constraints in the pricing problem.Performance evaluation is made in the case of FIPP p -cycles subject to dual failures. For small to medium size networks, the proposed model remains fairly scalable for increasing percentages of dual failures, and requires much less bandwidth than p -cycle protection schemes (ratio varies from 2 to 4). For larger networks, heuristics are required in order to keep computing times reasonable. In the particular case of single link failures, it compares very favorably (5 to 10 % of bandwidth saving) to the previously proposed column generation ILP model of Rocha, Jaumard and Stidsen (Telecommun. Syst., 2012 ).\"",
        "Document: \"On Timonov's algorithm for global optimization of univariate Lipschitz functions. Timonov proposes an algorithm for global maximization of univariate Lipschitz functions in which successive evaluation points are chosen in order to ensure at each iteration a maximal expected reduction of the \u201cregion of indeterminacy\u201d, which contains all globally optimal points. It is shown that such an algorithm does not necessarily converge to a global optimum.\"",
        "Document: \"Optimized dimensioning of resilient optical grids with respect to grade of services. n optical grid network geographically integrates distributed computing/information resources with high speed communications. Network dimensioning, maximization of services, and job scheduling are some of today key arising issues in optical grids. Since the last decade, many projects have been conducted in order to provide computational and information facilities in the academic as well as in the business communities. In this paper, we study the network dimensioning and the maximization of IT services in optical grids.We propose a scalable optimization model for maximizing IT services under link transport capacities. We assume the use of the anycast routing principle to identify the server nodes for executing the jobs, and a shared path protection mechanism in order to offer protection against single link/node failures. We also investigate different calculation methods of the link transport capacities in order to maximize the grade of services, while taking into account the bandwidth requirements.Computational results are presented on different traffic distributions. They show that the proposed link dimensioning can save more than 35 % bandwidth in optical grid networks, in comparison with the classical link dimensioning strategies. We also investigate the different protection schemes against single link failures, single node failures, single node and server node failures, and compare their bandwidth requirements, as well as their impact on the grade of services (GoS). Results show that there is no significant increase of the bandwidth requirements and no meaningful impact on the GoS when moving from a single link protection scheme to a single node (including server nodes) protection scheme.\"",
        "1 is \"Role-based multicast in highly mobile but sparsely connected ad hoc networks\", 2 is \"Computational study of a column generation algorithm for bin packing and cutting stock problems\"",
        "Given above information, for an author who has written the paper with the title \"Maximum Network Lifetime In Interference-Aware Wimax/802.16 Mesh Centralized Scheduling\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003928": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'MultiGreen: cost-minimizing multi-source datacenter power supply with online control':",
        "Document: \"Dynamic Packet Length Control in Wireless Sensor Networks. Previous packet length optimizations for sensor networks often employ a fixed optimal length scheme, while in this study we present DPLC, a Dynamic Packet Length Control scheme. To make DPLC more efficient in terms of channel utilization, we incorporate a lightweight and accurate link estimation method. We further provide two easy-to-use services, i.e., small message aggregation and large message fragmentation, to facilitate upper-layer application programming. The implementation of DPLC based on TinyOS 2.1 is lightweight, with respect to computation, memory, and header overhead. Our experiments using a real indoor testbed running CTP show that DPLC achieves the best performance compared with previous works.\"",
        "Document: \"Optimal real-time sampling rate assignment for wireless sensor networks. How to allocate computing and communication resources in a way that maximizes the effectiveness of control and signal processing, has been an important area of research. The characteristic of a multi-hop Real-Time Wireless Sensor Network raises new challenges. First, the constraints are more complicated and a new solution method is needed. Second, a distributed solution is needed to achieve scalability. This article presents solutions to both of the new challenges. The first solution to the optimal rate allocation is a centralized solution that can handle the more general form of constraints as compared with prior research. The second solution is a distributed version for large sensor networks using a pricing scheme. It is capable of incremental adjustment when utility functions change. This article also presents a new sensor device/network backbone architecture---Real-time Independent CHannels (RICH), which can easily realize multi-hop real-time wireless sensor networking.\"",
        "Document: \"Reconfigurable Battery Systems: A Survey on Hardware Architecture and Research Challenges. In a reconfigurable battery pack, the connections among cells can be changed during operation to form different configurations. This can lead a battery, a passive two-terminal device, to a smart battery that can reconfigure itself according to the requirement to enhance operational performance. Several hardware architectures with different levels of complexities have been proposed. Some researchers have used existing hardware and demonstrated improved performance on the basis of novel optimization and scheduling algorithms. The possibility of software techniques to benefit the energy storage systems is exciting, and it is the perfect time for such methods as the need for high-performance and long-lasting batteries is on the rise. This novel field requires new understanding, principles, and evaluation metrics of proposed schemes. In this article, we systematically discuss and critically review the state of the art. This is the first effort to compare the existing hardware topologies in terms of flexibility and functionality. We provide a comprehensive review that encompasses all existing research works, starting from the details of the individual battery including modeling and properties as well as fixed-topology traditional battery packs. To stimulate further research in this area, we highlight key challenges and open problems in this domain.\n\n\"",
        "Document: \"A highly scalable bandwidth estimation of commercial hotspot access points. WiFi access points that provide Internet access to users have been steadily increasing in urban areas. Different access points differ from one another in terms of services that they provide, including available upstream and downstream bandwidths, overall network capacity, open/blocked ports, security features, and so on. However, there is no reliable service available at present that can aid a user in selecting an access point from the many that are available. The primary research challenge is how to accurately estimate the current backhaul bandwidth of different access points in an efficient manner without requiring any installation of special software on the access points, and not burdening the WiFi subscribers to perform any communication or computation intensive task. This paper presents a new highly scalable bandwidth estimation technique that is suitable for efficiently estimating the backhaul bandwidth of a large number of APs. This technique has been extensively evaluated via a prototype implementation in an indoor testbed and in the Amazon EC2 platform. The evaluation demonstrates that the proposed technique exhibits high measurement accuracy, low latency, high scalability, and minimal intrusiveness.\"",
        "Document: \"Improving BitTorrent Traffic Performance by Exploiting Geographic Locality. Current implementations of BitTorrent-like P2P applications ignore the underlying Internet topology hence incur a large amount of traffic both inside an Internet Service Provider (ISP)' national backbone networks and over cross-ISP internetworking links. These traffics not only occupy costly bandwidth, but also increase user perceived response latency. ISP-Biased Neighbor Selection proposes to exploit peers' topological locality by biased neighbor selection, in which a peer chooses the majority of its neighbors from peers within the same ISP. In this paper, we propose to further exploit peers' geographic locality. First we improved ISP-Biased Neighbor Selection (ISP-Biased+) to take into consideration network locality (or, city locations) within the same ISP. When required neighbor number is relatively much less than seeds available, ISP-Biased Neighbor Selection+ performs much better than original approach, proved by simulations. Next, we propose that a peer could also choose its neighbors from peers of different ISPs within the same city with priority: assist by a well-know Chinese operator's unique ISP-internetworking content distribution network (CDN), these local cross-ISP traffics can be routed through local CDN cite. Using simulations, we show that cross-ISP traffic burden can be completely shifted to CDN local links and backbone traffic. At the same time, user perceived delay can be significantly reduced.\"",
        "Document: \"Adaptive Calibration for Fusion-based Wireless Sensor Networks. Wireless sensor networks (WSNs) are typically composed of low-cost sensors that are deeply integrated with physical environments. As a result, the sensing performance of a WSN is inevitably undermined by various physical uncertainties, which include stochastic sensor noises, unpredictable environment changes and dynamics of the monitored phenomenon. Traditional solutions (e.g., sensor calibration and collaborative signal processing) work in an open-loop fashion and hence fail to adapt to these uncertainties after system deployment. In this paper, we propose an adaptive system-level calibration approach for a class of sensor networks that employ data fusion to improve system sensing performance. Our approach features a feedback control loop that exploits sensor heterogeneity to deal with the aforementioned uncertainties in calibrating system performance. In contrast to existing heuristic based solutions, our control-theoretical calibration algorithm can ensure provable system stability and convergence. We also systematically analyze the impacts of communication reliability and delay, and propose an optimal routing algorithm that minimizes the impact of packet loss on system stability. Our approach is evaluated by both experiments on a testbed of Tmotes as well as extensive simulations based on data traces gathered from a real vehicle detection experiment. The results demonstrate that our calibration algorithm enables a network to maintain the optimal detection performance in the presence of various system and environmental dynamics.\"",
        "Document: \"Distributed Deadline and Renewable Aware Electric Vehicle Demand Response in the Smart Grid. Demand response is an important feature and functionality of the future smart grid. Electric vehicles are recognized as a particularly promising resource for demand response given their high charging demand and flexibility in demand management. Recently, researchers begun to apply market-based solutions to electric vehicle demand response. A clear vision, however, remains elusive because existing works overlook three key issues. (i) The hierarchy among electric vehicles (EVs), charging stations, and electric power companies (EPCs). Previous works assume direct interaction between EVs and EPCs and thus confine to single-level market designs. The designed mechanisms are inapplicable here due to ignoring the role of charging stations in the hierarchy. (ii) Temporal aspects of charging loads. Solely focusing on economic aspects makes significant demand reduction, but electric vehicles would end up with little allocated power due to overlooking their temporal constraints. (iii) Renewable generation co-located with charging stations. Market mechanisms that overlook the uncertainty of renewable would cause much inefficiency in terms of both the economic and temporal aspects. To address these issues, we study a new demand response scheme, i.e, hierarchical demand response for electric vehicles via charging stations. We propose that two-level marketing is suitable to this hierarchical scheme, and design a distributed market mechanism that is compatible with both the economic and temporal aspects of electric vehicle demand response. The market mechanism has a hierarchical decision-making structure by which the charging station leads the market and electric vehicles follow and respond to its actions. An appealing feature of the mechanism is the provable convergence to a unique equilibrium solution. At the equilibrium, neither the charging station or electric vehicles can improve their individual economic and/or temporal performance by changing their own strategies. Furthermore, we present a stochastic optimization based algorithm to optimize economic performance for the charging station at the equilibrium, given the predictions of the co-located renewable generation. The algorithm has provable robust performance guarantee in terms of the variance of the prediction errors. We finally evaluate the designed mechanism via detailed simulations. The results show the efficacy and validate the theoretical analysis for the mechanism.\"",
        "Document: \"Delay analysis in practical wireless network coding. AbstractNetwork coding provides a powerful mechanism for improving performance of wireless networks. In this paper, we present an analytical approach for end-to-end delay analysis in wireless networks that employs inter-session network coding. Prior work on performance analysis in wireless network coding mainly focuses on the throughput of the overall network. Our approach aims to analyze the delay of each flow in the network. The theoretical basis of our approach is network calculus. In order to use network calculus to analyze the performance of traffic flows in the network, we have to address three specific problems: identifying traffic flows, characterizing broadcast links, and measuring coding opportunities. We propose solutions for these problems and discuss the practical issues when applying the approach in practice. We make three main contributions. First, we obtain theoretical formulations for computing the queueing delay bounds of traffic flows in wireless networks with network coding. Second, with the formulations, we figure out the factors that affect the queueing delay of a flow and find that first-in first-out scheduling cannot fully exploit the benefit of network coding. Third, in order to exploit our findings, we introduce a new scheduling scheme that can improve the performance of current practical wireless network coding. Copyright \u00a9 2012 John Wiley & Sons, Ltd.\"",
        "Document: \"When Online Dating Meets Nash Social Welfare: Achieving Efficiency and Fairness. Mobile dating applications such as Coffee Meets Bagel, Tantan, and Tinder, have become significant for young adults to meet new friends and discover romantic relationships. From a system designer's perspective, in order to achieve better user experience in these applications, we should take both the efficiency and fairness of a dating market into consideration, so as to increase the overall satisfaction for all users. Towards this goal, we investigate the nature of diminishing marginal returns for online dating markets (i.e., captured by the submodularity), and trade-off between the efficiency and fairness of the market with Nash social welfare. We further design effective online algorithms to the apps. We verify our models and algorithms through sound theoretical analysis and empirical studies by using real data and show that our algorithms can significantly improve the ecosystems of the online dating applications.\n\n\"",
        "Document: \"How Cars Talk Louder, Clearer And Fairer: Optimizing The Communication Performance Of Connected Vehicles Via Online Synchronous Control. The connected vehicles have been considered as a remedy for modern traffic issues, potentially saving hundreds of thousands of lives every year worldwide. The Dedicated Short-Range Communications (DSRC) technology is an essential building block of this promising vision. DSRC faces volatile vehicular environments, where not only wireless propagation channels but also network topologies vary rapidly. Moreover, traffic congestions during rush hours may lead to an unprecedentedly high density of broadcasting radios, resulting in compromised reliability, efficiency and fairness of DSRC.In order to optimize the performance of DSRC, we develop a novel Online Control Approach of power and Rates (OnCAR). Supported by systematic control theories, OnCAR performs stably even in the dynamic and unpredictable vehicular environments. To the best of our knowledge, OnCAR is the first solution to address the strong coupling between communication variables. It adopts a multi-variable control model to synchronously adjust transmission power and data rates, which are two major variables determining the performance of DSRC. In addition, OnCAR leverages receiver-side measurements of performance metrics to strike a balance between overall performance and fairness. Compared with the state of the art, OnCAR enhances the overall reliability and efficiency of DSRC by 23.7% and 30.1%, respectively. Meanwhile, these numbers are achieved with a 40.1% improvement in fairness.\"",
        "1 is \"Lock-Free Cuckoo Hashing\", 2 is \"Using Sorted Switching Median Filter to remove high-density impulse noises\"",
        "Given above information, for an author who has written the paper with the title \"MultiGreen: cost-minimizing multi-source datacenter power supply with online control\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003971": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Controllability/Observability Measures for Multiple-Valued Test Generation Based on D-Algorithm':",
        "Document: \"Fuzzy Logic Approach to 3D Magnetic Resonance Image Segmentation. This paper proposes an approach of fuzzy logic to 3D MR image segmentation. We show a fuzzy knowledge representation method to represent the knowledge needed to segment the target portions, and apply our method to 3D MR human brain image segmentation. In it we consider position knowledge, boundary surface knowledge and intensity knowledge. They are expressed by fuzzy if-then rules and compiled to a total degree as the measure of segmentation. The degree is evaluated in region growing technique and which segments the whole brain region into the left cerebral hemisphere, the right cerebral hemisphere, the cerebellum and the brain stem. The experimental result on 36 MR voxel data shows that our method extracted the portions precisely.\"",
        "Document: \"An ultrasonic thickness and wave speed determination system aided by fuzzy logic. This paper proposes an ultrasonic system to determine thickness and wave speed of an object. In it, the authors newly develop a triangle probe system composed of three probes: a central probe and two side probes. The central probe straight transmits/receives ultrasonic waves and the right probe receives ultrasonic waves transmitted from the left one. In the authors' method, the triangle probe is moved to the depth direction of an object. Then, two peak intensities are revealed at the focal points of the surface and at the bottom of the object. The authors show a method to calculate the wave speed and the thickness of the objects from two focal points. In the experiment, the authors extract the two focal points aided by fuzzy logic. Then, they determine the wave speed and thickness of the object from these focal points. The experimental results on two hard phantoms show that the method can successfully determine the wave speed and the thickness of the object with high accuracy.\"",
        "Document: \"A Rough Set-Based Clustering Method with Modification of Equivalence Relations. This paper presents a clustering method for nominal and numerical data based on rough set theory. We represent relative similarity between objects as a weighted sum of two types of distances: the Hamming distance for nominal data and the Mahalanobis distance for numerical data. On assigning initial equivalence relations to every object, modification of slightly different equivalence relations is performed to suppress excessive generation of categories. The optimal clustering result can be obtained by evaluating the cluster validity over all clusters generated with various values of similarity thresholds. After classification has been performed, features of each class are extracted based on the concept of value reduct. Experimental results on artificial data and amino acid data show that this method can deal well with both types of attributes.\"",
        "Document: \"Yuragi Synthesis For Ultrasonic Human Brain Imaging. This paper proposes YURAGI synthesis for brain imaging under the skull. The advantage of the proposed method over conventional methods is that, using YURACI synthesis, it is possible to obtain the effective results without image registration. Image registration is generally needed when more than two images are to be synthesized into one image. YURAGI synthesis does not need image registration; thus, its method is simpler than other methods that need image synthesis. The effectiveness of the proposed method was confirmed by comparing its error rate and accuracy with those of other methods. YURAGI leads the simple and energy-saving system with performing autoregulation. Autoregulation is utilized in many biological systems. In this study, YURAGI was applied to an ultrasound-based diagnostic medical imaging technique. The experimental results using YURAGI were superior to those using other methods. Thus, YURAGI is useful for visualizing the human brain.(1)\"",
        "Document: \"Activation Function Manipulation for Fault Tolerant Feedforward Neural Networks. We propose a learning algorithm to enhance the fault tolerance of feedforward neural networks (NNs for short) by manipulating the gradient of sigmoid activation function of the neuron. For the output layer, we employ the function with the relatively gentle gradient. For the hidden layer, we steepen the gradient of function after convergence. The experimental results show that our NNs are superior to NNs trained with other algorithms employing fault injection and the calculation of relevance of each weight to the output error in fault tolerance, learning cycles and time. Besides our gradient manipulation never spoils the generalization ability.\"",
        "Document: \"Computer Aided Diagnosis System of Meniscal Tears with T1 and T2 Weighted MR Images Based on Fuzzy Inference. This paper proposes a computer aided diagnosis system of meniscal tears from 3D human knee MR image with T1-weighted and T2-weighted MR images. The first step of our method is a 3D image registration between both images on the computer display by manual. The second step determines the candidate region of the menisci from T2-weighted MR image aided by fuzzy if-then rules with respect to the ltoiocna and the intensity. The final step determines the voxels in the menisci from the candidate region by using T1- weighted and T2-weighted MR images. We applied this method to several subjects. All voxels in the menisci of each subject were successfully identified and their 3D surfaces were displayed. Thus, our developed system would improve to diagnose the meniscal tears.\"",
        "Document: \"Fully Automated Segmentation of Cerebral Ventricles from 3-D SPGR MR Images using Fuzzy Representative Line. Generating surface shaded display images and measuring the volumes of cerebral ventricles using 3-D SPGR MR images will help to diagnose many types of cerebral diseases with quantitatively and qualitatively. However, manual segmentation of cerebral ventricles is time-consuming and is subject to inter- and intra-operator variation. This article proposes a fully automated method for segmenting cerebrospinal fluid (CSF) and cerebral ventricles from MR images. Our method segments the cerebral ventricles by using a representative line (RL), which can represent the abstract of the shape and position of the cerebral ventricles. The RL is found by fuzzy If-Then rules that can implement physicians\u2019 knowledge on the cerebral ventricles. The proposed method was applied to MR volumes of 20 normal subjects, 20 Alzheimer disease (AD) and 20 normal pressure hydrocephalus (NPH) patients. The segmentation error ratio of the lateral ventricles was 1.98% in comparison with the volumes of manually delineated region by a physician. Using the proposed method, we found that patients of NPH significantly increased the ratio of volume of the lateral ventricles to the total CSF volume in comparison with that of AD (significance level\"",
        "Document: \"Gate model networks for minimization of multiple-valued logic functions. The use of gate model networks as a logic minimization method for multiple-valued logic functions is proposed. The gate model network is a kind of neural network constructed like and AND-OR two-level circuits using two gate models: an AND type gate model and an OR type gate model. The backpropagation (BP) method is used to train the network until it realizes the minimal solution. A solution is derived from the weights and thresholds. The gate model networks are applied to binary AND-OR circuit minimization and to the multiple-value max-of-min's expression minimization. It is shown that the gate model network is also applicable to minimize multiple-valued sum-of-products expressions, where sum refers to TSUM\"",
        "Document: \"Fuzzy ultrasonic array system for locating screw holes of intramedullary nail. In this paper, we describe an ultrasonography system for locating screw holes of intramedullary nail by one-direction freehand scanning using an ultrasonic array probe. Although conventional X-ray method can visualize the nail in the femur, it has serious problem of X-ray exposure. We propose a locating method of the nail screw holes by an ultrasonic array probe. We extract screw hole regions by calculating two fuzzy degrees: average of the intensity and variance of the intensity using fuzzy inference. Next, we do a registration between the obtained image with the true image, where the true image is the nail image obtained by scanning an array probe to the nail exactly. As the result, we could calculate the center distance of two screw holes within an error of 1.0 mm.\"",
        "Document: \"Stem Cell Quantity Determination In Artificial Culture Bone By Ultrasonic Testing. This paper describes an ultrasonic system that estimates the cell quantity of an artificial culture bone, which is effective for appropriate treat with a composite of this material and Bone Marrow Stromal Cells. For this system, we examine two approaches for analyzing the ultrasound waves transmitted through the cultured bone, including stem cells to estimate cell quantity: multiple regression and fuzzy inference. We employ two characteristics from the obtained wave for applying each method. These features are the amplitude and the frequency; the amplitude is measured from the obtained wave, and the frequency is calculated by the cross-spectrum method. The results confirmed that the fuzzy inference method yields the accurate estimates of cell quantity in artificial culture bone. Using this ultrasonic estimation system, the orthopaedic surgeons can choose the composites that contain favorable number of cells before the implantation.\"",
        "1 is \"On implementing large binary tree architectures in VLSI and WSI\", 2 is \"Chaos in Neural Networks\"",
        "Given above information, for an author who has written the paper with the title \"Controllability/Observability Measures for Multiple-Valued Test Generation Based on D-Algorithm\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "003972": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A Test Suite For Parallel Performance Analysis Tools':",
        "Document: \"Specification And Detection Of Performance Problems With Asl. Performance analysis is an important step in tuning performance-critical applications. It is a cyclic process of measuring and analyzing performance data, driven by the programmer's hypotheses on potential performance problems. Currently this process is controlled manually by the programmer. The goal of the work described in this article is to automate the performance analysis process based on a formal specification of performance properties. One result of the APART project is the APART Specification Language (ASL) for the formal specification of performance properties. Performance bottlenecks can then be identified based on the specification, since bottlenecks are viewed as performance properties with a large negative impact. We also present the overall design and an initial evaluation of the Periscope system which utilizes ASL specifications to automatically search for performance bottlenecks in a distributed manner. Copyright (c) 2006 John Wiley & Sons, Ltd.\"",
        "Document: \"A Data Structure Oriented Monitoring Environment for Fortran OpenMP Programs. This paper describes a monitoring environment that enables the analysis of memory access behavior of applications in a selective way with a potentially very high degree of detail. It is based on a novel hardware monitor design that employs an associative counter array to measure data structure related information at runtime. A simulator for this hardware monitor is implemented, providing the capability of on-the-fly simulation targeting shared memory systems. Layers of software are constructed to operate and utilize the underlying hardware monitor, thus forming a complete monitoring environment. This environment is useful to help users to reason about optimizations based on data reorganization as well as on standard loop transformations.\"",
        "Document: \"Work distribution in parallel programs for distributed memory multiprocessors. Abstract\"",
        "Document: \"The READEX Project for Dynamic Energy Efficiency Tuning. High Performance Computing (HPC) systems consume a lot of energy. The overall energy consumption is one of the biggest challenges on the way towards exascale computers. Therefore, energy reduction techniques have to be applied on all levels from the basic chip technology up to the data center infrastructure. The READEX project explores the potential of dynamically switching application and system parameters, such as the clock frequency of the cores, to reduce the overall energy consumption of applications. An analysis is performed during application design time to precompute a tuning model that is then input to the runtime tuning library. This library switches the application and system configuration at runtime to adapt to varying application characteristics.\"",
        "Document: \"Evaluating OpenMP Performance Analysis Tools with the APART Test Suite. We outline the design of ATS, the APART Test Suite, for evaluating (automatic) performance analysis tools with respect to their correctness and effectiveness in detecting actual performance problems, with focus on the ATS test programs related to OpenMP. We report on results from applying two OpenMP performance analysis tools to the test cases generated from ATS.\"",
        "Document: \"Automatic MPI-IO Tuning with the Periscope Tuning Framework. The performance of I/O operations on HPC systems is a bottleneck in many parallel applications. The MPI Forum defined the MPI-IO programming interface for parallel I/O as part of the MPI-2 standard. With MPIIO, parallel applications can overcome the performance and portability limitations of existing parallel I/O interfaces. MPIIO performance analysis and tuning are important for parallel software development. Some analysis tools have been designed to help developers understand MPI-IO performance. However, there are no recommendations given by those tools on how to tune the MPI-IO code to obtain better performance. In this paper, we introduce the design of an MPI-IO automatic tuner. The automatic tuner relies on the Periscope Tuning Framework (PTF) for performance data collection and for passing hints to the MPI-IO library. The goal is to produce tuning recommendations that will help applications achieve efficient execution of MPI-IO operations in production runs.\"",
        "Document: \"A multi-aspect online tuning framework for HPC applications. Developing software applications for high-performance computing (HPC) requires careful optimizations targeting a myriad of increasingly complex, highly interrelated software, hardware and system components. The demands placed on minimizing energy consumption on extreme-scale HPC systems and the associated shift towards hete rogeneous architectures add yet another level of complexity to program development and optimization. As a result, the software optimization process is often seen as daunting, cumbersome and time-consuming by software developers wishing to fully exploit HPC resources. To address these challenges, we have developed the Periscope Tuning Framework (PTF), an online automatic integrated tuning framework that combines both performance analysis and performance tuning with respect to the myriad of tuning parameters available to today\u2019s software developer on modern HPC systems. This work introduces the architecture, tuning model and main infrastructure components of PTF as well as the main tuning plugins of PTF and their evaluation.\"",
        "Document: \"Performance cockpit: an extensible GUI platform for performance tools. Within the EP-Cache project, the Performance Cockpit has been developed to provide a unified GUI for a series of performance tools. This is achieved through the establishment of a general extensible architecture and the application of standardized intermediate representations of program structures. This paper describes the design and implementation of this platform, and discusses the future evolvement into a universal GUI platform for performance tools independent of programming language and programming paradigms.\"",
        "Document: \"Support Tools and Environments. Parallel computing is a key technology for many areas in science and industry. Outstanding examples are the ASCI and Blue\n Gene programs that target only very few but critical applications. A much broader spectrum of applications can be found on\n any of the machines of supercomputing centers all over the world.\n \"",
        "Document: \"Automatic performance analysis of large scale simulations. Developing efficient parallel programs for supercomputers is a challenging task. It requires insight into the application, the parallelization concepts, as well as the parallel architectures. Performance analysis tools such as Periscope, an automatic performance analysis tool currently under development at Technische Universit\u00e4t M\u00fcnchen, help the programmer in detecting performance bottlenecks. The goal of the ISAR project is to enhance the existing Periscope research prototype and deliver a production version. This paper focuses on the evaluation of Periscope's main features based on two large scale simulation codes.\"",
        "1 is \"Design and Prototype of a Performance Tool Interface for OpenMP\", 2 is \"The Repeat Offender Problem: A Mechanism for Supporting Dynamic-Sized, Lock-Free Data Structures\"",
        "Given above information, for an author who has written the paper with the title \"A Test Suite For Parallel Performance Analysis Tools\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004008": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Finding objects in ':",
        "Document: \"Slant menu: novel GUI widget with ergonomic design. In this paper, we propose a new GUI design method based on ergonomics and describe our new menu widget named Slant Menu. Natural human hand movements on a table are reflected in this menu, which appears in an inclined direction with a curved form, rather than a conventional vertical, linear GUI menu. We have developed the prototype and conducted usability testing.\"",
        "Document: \"SyncDecor: appliances for sharing mutual awareness between lovers separated by distance. Many lovers separated by distance worry about their relationships, despite the fact that the use of various means of communication such as mobile phones and e-mail is now widespread. We interviewed some such couples, who expressed the desire to feel a sense of connection and synchronization with their partners. They also expressed the desire to have devices that provide awareness about their partners. For this purpose, we propose \"SyncDecor\" devices, which are pairs of remotely installed appliances that synchronize each other.\"",
        "Document: \"iMake: eye makeup design generator. Many women enjoy applying makeup. Eye makeup is especially important for face makeup, because eyeshadow color and eyeline shape can dramatically change a person's impression on others. In addition to standard eye makeup, there is \\\"artistic eye makeup,\\\" which tends to have a greater variety of designs and is more ostentatious than standard eye makeup. Artistic eye makeup often has a motif of characters or symbols, such as a butterfly, heart or rose. Needless to say, it is often difficult for non-artistic people to apply this type of eye makeup. Artistic eye makeup requires a special technique; therefore, we propose and implement a computer-aided eye makeup design system called \\\"iMake.\\\" This system generates artistic eye makeup designs from the colors and shapes of a favorite character selected by a user. Once the user has selected the desired eye makeup pattern, an ink-jet color printer prints it on a transfer sheet that the user can apply to his/her eyelids. The user can design any type of eye makeup with a simple operation, and then apply the transfer sheet makeup without any special techniques. The usability evaluation provided by eight participants has shown that our system is sufficiently useful for practical eye makeup.\"",
        "Document: \"Smart makeup system: supporting makeup using lifelog sharing. Although many women wear makeup every day, they often have difficulty in varying their makeup. In this paper, we propose \"Smart Makeup System\" that helps users find new makeup methods for use with their daily cosmetics by sharing makeup logs (makeup pictures and cosmetics usages) on the web.\"",
        "Document: \"InPhase: a communication system focused on \"happy coincidences\" of daily behaviors. To supplement existing forms of communication such as telephone and e-mail, this research proposes a new method of communicating \"awareness\" between people who are separated by long distances. In this paper, we investigate cases where coincidences in daily behaviors lead to casual conversation and thus intimacy and togetherness. We propose a new method of communicating these \"happy coincidences\" between a pair of remotely located houses. By equipping furniture and appliances such as doors, sofas, refrigerators and televisions with sensors, we developed a system where these items are connected to remote equivalents and their near simultaneous use is communicated.\"",
        "Document: \"Flip-Flop Sticker: Force-to-Motion Type 3DoF Input Device for Capacitive Touch Surface. No abstract available.\n\n\"",
        "Document: \"MagNail: user interaction with smart device through magnet attached to fingernail. Nowadays, people can walk around sporting wearable devices owing to the advancements in wearable computing. Our objective is to develop wearable computing technology that allows a user to sport a device in a natural manner at all times. We then focus on the fingernail, which is accepted as a body part that is decorated, and a magnet that does not require batteries. In this study, we use MagNail, which is nail art that uses a magnet as the decorative material. It is attached to the user's nail and detects the status of the user's finger while operating a smartphone, via the magnetic sensor inherent in the device.\"",
        "Document: \"BookAidee: managing evacuees from natural disaster by RFID tagged library books. BookAidee is a system to manage people who evacuate into public school buildings from disaster. The system identifies people by using already implemented structure of school library books that have RFID tags. These tags are used for connecting the person into the system database. We have implemented the system in server and client applications and tested the feasibility.\"",
        "Document: \"iMake: computer-aided eye makeup. Many women enjoy applying makeup. Eye makeup is especially important for face makeup, because eyeshadow color and eye line shape can dramatically change a person's impression given to others. In addition to standard eye makeup, there is \\\"artistic eye makeup,\\\" which tends to have a greater variety of designs and is more ostentatious than standard eye makeup. Artistic eye makeup often has a motif of characters or symbols, such as a butterfly or a musical note. Needless to say, it is often difficult for non-artistic people to apply this type of eye makeup. Artistic eye makeup requires a special technique; therefore, we propose and implement a computer-aided eye makeup design system called \\\"iMake.\\\" This system generates eye makeup designs from the colors and shapes of a favorite characters selected by a user. Once the user has selected the desired eye makeup pattern, an ink-jet color printer prints it on a transfer sheet that the user can apply to his/her eyelids. The user can design any type of eye makeup with a simple operation, and then apply the transfer sheet makeup without any special techniques.\"",
        "Document: \"Remote skincare advice system using life logs. Many women find it difficult to maintain beautiful skin since different skincare approaches require different amounts of effort, time, and special knowledge. Women often ask experts in cosmetic stores for skincare advice. However, this approach has the limitations of time, place, and personal information. To solve these problems, we propose a remote skincare advice system that uses life logs. This system helps users to automatically log information related to their skin condition and share these data with skincare experts in order to obtain appropriate advice.\"",
        "1 is \"\", 2 is \"An experimental evaluation of transparent user interface tools and information content\"",
        "Given above information, for an author who has written the paper with the title \"Finding objects in \", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004031": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'New shortest-path approaches to visual servoing':",
        "Document: \"New shortest-path approaches to visual servoing. In recent years, a number of visual servo control algorithms have been proposed. Most approaches try to solve the inherent problems of image-based and position- based servoing by partitioning the control between image and Cartesian spaces. However, partitioning of the control often causes the Cartesian path to become more complex, which might result in operation close to the joint limits. A solution to avoid the joint limits is to use a shortest-path approach, which avoids the limits in most cases. In this paper, two new shortest-path approaches to visual servoing are presented. First, a position-based approach is proposed that guarantees both shortest Cartesian trajectory and object visibility. Then, a variant is presented, which avoids the use of a 3D model of the target object by using homography based partial pose estimation. I. INTRODUCTION\"",
        "Document: \"Three-dimensional object reconstruction of symmetric objects by fusing visual and tactile sensing. In this work, we propose to reconstruct a complete three-dimensional (3-D) model of an unknown object by fusion of visual and tactile information while the object is grasped. Assuming the object is symmetric, a first hypothesis of its complete 3-D shape is generated. A grasp is executed on the object with a robotic manipulator equipped with tactile sensors. Given the detected contacts between the fingers and the object, the initial full object model including the symmetry parameters can be refined. This refined model will then allow the planning of more complex manipulation tasks. The main contribution of this work is an optimal estimation approach for the fusion of visual and tactile data applying the constraint of object symmetry. The fusion is formulated as a state estimation problem and solved with an iterated extended Kalman filter. The approach is validated experimentally using both artificial and real data from two different robotic platforms.\"",
        "Document: \"A framework for generating tunable test functions for multimodal optimization. Multimodal function optimization, where the aim is to locate more than one solution, has attracted growing interest especially in the evolutionary computing research community. To evaluate experimentally the strengths and weaknesses of multimodal optimization algorithms, it is important to use test functions representing different characteristics and various levels of difficulty. The available selection of multimodal test problems is, however, rather limited and no general framework exists. This paper describes an attempt to construct a software framework which includes a variety of easily tunable test functions. The aim is to provide a general and easily expandable environment for testing different methods of multimodal optimization. Several function families with different characteristics are included. The framework implements new parameterizable function families for generating desired landscapes. Additionally the framework implements a selection of well known test functions from the literature, which can be rotated and stretched. The software module can easily be imported to any optimization algorithm implementation compatible with the C programming language. As an application example, 8 optimization approaches are compared by their ability to locate several global optima over a set of 16 functions with different properties generated by the proposed module. The effects of function regularity, dimensionality and number of local optima on the performance of different algorithms are studied.\"",
        "Document: \"Manipulation primitives: A paradigm for abstraction and execution of grasping and manipulation tasks. Sensor-based reactive and hybrid approaches have proven a promising line of study to address imperfect knowledge in grasping and manipulation. However the reactive approaches are usually tightly coupled to a particular embodiment making transfer of knowledge difficult. This paper proposes a paradigm for modeling and execution of reactive manipulation actions, which makes knowledge transfer to different embodiments possible while retaining the reactive capabilities of the embodiments. The proposed approach extends the idea of control primitives coordinated by a state machine by introducing an embodiment independent layer of abstraction. Abstract manipulation primitives constitute a vocabulary of atomic, embodiment independent actions, which can be coordinated using state machines to describe complex actions. To obtain embodiment specific models, the abstract state machines are automatically translated to embodiment specific models, such that full capabilities of each platform can be utilized. The strength of the manipulation primitives paradigm is demonstrated by developing a set of corresponding embodiment specific primitives for object transport, including a complex reactive grasping primitive. The robustness of the approach is experimentally studied in emptying of a box filled with several unknown objects. The embodiment independence is studied by performing a manipulation task on two different platforms using the same abstract description.\"",
        "Document: \"Affordance Learning For End-To-End Visuomotor Robot Control. Training end-to-end deep robot policies requires a lot of domain-, task-, and hardware-specific data, which is often costly to provide. In this work, we propose to tackle this issue by employing a deep neural network with a modular architecture, consisting of separate perception, policy, and trajectory parts. Each part of the system is trained fully on synthetic data or in simulation. The data is exchanged between parts of the system as low-dimensional latent representations of affordances and trajectories. The performance is then evaluated in a zero-shot transfer scenario using Franka Panda robot arm. Results demonstrate that a low-dimensional representation of scene affordances extracted from an RGB image is sufficient to successfully train manipulator policies. We also introduce a method for affordance dataset generation, which is easily generalizable to new tasks, objects and environments, and requires no manual pixel labeling.\"",
        "Document: \"Reinforcement learning for improving imitated in-contact skills. Although motor primitives (MPs) for trajectory-based skills have been studied extensively, much less attention has been devoted to studying in-contact tasks. With robots becoming more commonplace, it is both economical and convenient to have a mechanism for learning an in-contact task from demonstration. However, transferring an in-contact skill such as wood planing from a human to a robot is significantly more challenging than transferring a trajectory-based skill; it requires a simultaneous control of both pose and force. Furthermore, some in-contact tasks have extremely complex contact environments. We present a framework for imitating an in-contact skill from a human demonstration and automatically enhancing the imitated force profile using a policy search method. The framework encodes both the the demonstrated trajectory and the normal contact force using Dynamic Movement Primitives (DMPs). In experiments, we utilize Policy Improvement with Path Integral (PI\n<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sup>\n) algorithm for updating the imitated force policy. Our results demonstrate the effectiveness of this approach in improving the performance of a wood planing task. After only two update rounds, all the updated policies have outperformed the imitated policy at a significance level of P <; 0.001.\"",
        "Document: \"Hallucinating Robots: Inferring Obstacle Distances From Partial Laser Measurements. Many mobile robots rely on 2D laser scanners for localization, mapping, and navigation. However, those sensors are unable to correctly provide distance to obstacles such as glass panels and tables whose actual occupancy is invisible at the height the sensor is measuring. In this work, instead of estimating the distance to obstacles from richer sensor readings such as 3D lasers or RGBD sensors, we present a method to estimate the distance directly from raw 2D laser data. To learn a mapping from raw 2D laser distances to obstacle distances we frame the problem as a learning task and train a neural network formed as an autoencoder. A novel configuration of network hyperparameters is proposed for the task at hand and is quantitatively validated on a test set. Finally, we qualitatively demonstrate in real time on a Care-O-bot 4 that the trained network can successfully infer obstacle distances from partial 2D laser readings.\"",
        "Document: \"Optimal Reconstruction of Approximate Planar Surfaces Using Photometric Stereo. Photometric stereo can be used to obtain a fast and noncontact surface reconstruction of Lambertian surfaces. Despite several published works concerning the uncertainties and optimal light configurations of photometric stereo, no solutions for optimal surface reconstruction from noisy real images have been proposed. In this paper, optimal surface reconstruction methods for approximate planar textured surfaces using photometric stereo are derived, given that the statistics of imaging errors are measurable. Simulated and real surfaces are experimentally studied, and the results validate that the proposed approaches improve the surface reconstruction especially for the high-frequency height variations.\"",
        "Document: \"Combination of local and global line extraction. In this paper we study how to combine local and global line extraction. The Hough transform is usually used to detect line segments in an image. However, the standard Hough transform (SHT) suffers from time and storage complexity, and it is incapable to utilize local line extraction. Recently an approach, called the connective randomized Hough transform (CRHT), has been proposed to take advantage of local detection, such as the connectivity of neighboring edge points and the line fitting of these detected points. However, this approach contains problems with images with many distorted lines. We suggest a new line detection approach, called the extended connective randomized Hough transform (ECRHT), to alleviate these problems. The use of gradient information of edge points is also discussed. Experiments with simulated and real-world data demonstrate the benefits of the ECRHT, as compared to the SHT and the CRHT.\"",
        "Document: \"Localization in ambiguous environments using multiple weak cues. This paper presents a probabilistic approach for sensor-based localization with weak sensor data. Wireless received signal\n strength measurements are used to disambiguate sonar measurements in symmetric environments. Particle filters are used to\n model the multi-hypothesis estimation problem. Experiments indicate that multiple weak cues can provide robust position estimates\n and that multiple sensors also aid in solving the kidnapped robot problem.\"",
        "1 is \"Object manipulation coordinating multiple primitive motions\", 2 is \"SLAM++: Simultaneous Localisation and Mapping at the Level of Objects\"",
        "Given above information, for an author who has written the paper with the title \"New shortest-path approaches to visual servoing\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004087": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Distributed Throughput Optimization for ZigBee Cluster-Tree Networks':",
        "Document: \"vGPRS: a mechanism for voice over GPRS. This paper proposes vGPRS, a voice over IP (VoIP) mechanism for general packet radio service (GPRS) network. In this approach, a new network element called VoIP mobile switching center (VMSC) is introduced to replace standard GSM MSC. Both standard GSM and GPRS mobile stations can be used to receive real-time VoIP service, which need not be equipped with the VoIP (i.e., H.323) terminal capabilities. The vGPRS approach is implemented using standard H.323, GPRS, and GSM protocols. Thus, existing GPRS and H.323 network elements are not modified. Furthermore, the message flows for vGPRS registration, call origination, call release and call termination procedures are described to show the feasibility of our vGPRS system.\"",
        "Document: \"An IPv4-IPv6 translation mechanism for SIP overlay network in UMTS all-IP environment. Both IPv6 and session initiation protocol (SIP) are default protocols for Universal Mobile Telecommunications System (UMTS) all-Internet protocol (IP) network. In the existing mobile telecommunications environments, an IPv6-based UMTS all-IP network needs to interwork with other Internet protocol version 4 (IPv4)-based SIP networks. Therefore, mobile SIP applications are typically offered through an overlay structure over the IPv4-Internet protocol version 6 (IPv6) interworking environments. Based on 3GPP 23.228, we propose an IPv4-IPv6 translation mechanism (i.e., SIPv6 translator) that integrates different IP infrastructures (i.e., IPv4 and IPv6) to provide an overlay network for transparent SIP application deployment. In this paper, we present the architecture and the call flows of the SIPv6 translator. An analytic model is proposed to investigate the fault tolerance issue of our approach. Our study provides guidelines to select appropriate number of processors for fault tolerance.\"",
        "Document: \"A comprehensive analysis of low-power operation for beacon-enabled IEEE 802.15.4 wireless networks. ZigBee, a unique communication standard designed for low-rate wireless personal area networks, has extremely low complexity, cost, and power consumption for wireless connectivity of inexpensive, portable, and moving devices. ZigBee uses the IEEE 802.15.4 standard as its communication protocol for medium access control (MAC) layer and physical (PHY) layer. The IEEE 802.15.4 MAC layer achieves duty-cycle operations by setting two system parameters, macBeaconOrder (BO) and macSuper FrameOrder (SO), to achieve low power consumption for ZigBee devices. This study comprehensively analyzes IEEE 802.15.4 duty-cycle operation. Specifically, a novel analytical model that accommodates a general traffic distribution is developed. An NS-2 based simulation model, which is validated by the developed analytical model is also proposed. Through the experiments conducted by the analytical and simulation models, some important performance-evaluation insights are gained that can be used as guidelines for future low-power ZigBee network deployment.\"",
        "Document: \"Energy-Efficient Video Multicast in 4G Wireless Systems. Layer-based video coding, together with adaptive modulation and coding, is a promising technique for providing real-time video multicast services on heterogeneous mobile devices. With the rapid growth of data communications for emerging applications, reducing the energy consumption of mobile devices is a major challenge. This paper addresses the problem of resource allocation for video multicast in fourth-generation wireless systems, with the objective of minimizing the total energy consumption for data reception. First, we consider the problem when scalable video coding is applied. We prove that the problem is NP-hard and propose a 2-approximation algorithm to solve it. Then, we investigate the problem under multiple description coding, and show that it is also NP-hard and cannot be approximated in polynomial time with a ratio better than 2, unless P = NP. To solve this case, we develop a pseudopolynomial time 2-approximation algorithm. The results of simulations conducted to compare the proposed algorithms with a brute-force optimal algorithm and a conventional approach are very encouraging.\"",
        "Document: \"An Adaptive Contention Control Strategy for IEEE 802.15.4-Based Wireless Sensor Networks. The IEEE 802.15.4 standard is able to achieve low-power transmissions in low-rate and short-distance wireless personal area networks (WPANs). Due to the constitutional design of the sensor node and the transmission architecture (client-server model), any data communication between two sensor nodes will involve the coordinator. One shortcoming of redundant channel-access steps will result in excess...\"",
        "Document: \"Enabling Low-Latency Applications in Fog-Radio Access Networks. The ultra low-latency operations of communications and computing enable many potential IoT applications, and thus have gained widespread attention recently. Existing mobile devices and telecommunication systems may not be able to provide the highly desired low-latency computing and communications services. To meet the needs of those applications, we introduce the Fog-Radio Access Network (F-RAN) architecture, which brings the efficient computing capability of the cloud to the edge of the network. By distributing computing-intensive tasks to multiple F-RAN nodes, F-RAN has the potential to meet the requirements of those ultra low-latency applications. In this article, we first introduce the F-RAN and its rationale in serving ultra low-latency applications. Then we discuss the need for a service framework for F-RAN to cope with the complex tradeoff among performance, computing cost, and communication cost. Finally, we illustrate the mobile AR service as an exemplary scenario to provide insights for the design of the framework. Examples and numerical results show that ultra low-latency services can be achieved by the F-RAN by properly handling the tradeoff.\"",
        "Document: \"A half-key key management scheme for wireless sensor networks. Wireless Sensor Networks (WSN) have recently been in the limelight for many domains. The characteristics of sensor networks have imposed various restrictions on their system designs. Since sensor nodes usually are developed by low-cost hardware, one major challenge in the development of many sensor-network applications is to provide high-security features with limited resources. In this paper, we propose a half-key scheme based on the well-known random key pre-distribution scheme and DDHV-D deployment knowledge to provide resource-efficient key management in wireless sensor networks with reduced memory space requirements and better security enforcement. The capability of the proposed approach is evaluated by an analytical model and a series of experiments.\"",
        "Document: \"Mobility and session management: UMTS vs. cdma2000. This article describes the mobility and session management mechanisms for UMTS and cdma2000 packet-switched (PS) service domains, and compares the design guidelines for these two third-generation technologies. We first introduce the network architectures and protocols for UMTS and cdma2000, and then elaborate on the PS service domain's mobility management, session management, and IP-level mobility mechanisms. Based on the mobility and session management mechanisms of the UMTS and cdma2000 PS service domains, an integrated architecture and intersystem roaming procedures are proposed to show the implementation feasibility of UMTS-cdma2000 IP-level interworking.\"",
        "Document: \"Impact of mobility on mobile telecommunications networks. This paper describes the mobility management mechanisms for mobile telecommunications networks. There are two major types of mobility: radio network mobility and core network mobility. Radio network mobility supports radio link switching of a mobile user during conversation, and core network mobility provides roaming and tunnel-related management for packet re-routing due to user movement. Impact of mobility on both the radio and the core networks is addressed in this paper. Also, potential research issues on these topics are discussed. Copyright (C) 2005 John Wiley & Sons, Ltd.\"",
        "Document: \"Another Real-Time Operating System and Unified MAC Protocol for Home Controlling and Monitoring. A real-time kernel is an essential component for embedded systems. This research designs a real-time operating system for the realization of home monitoring and controlling via power lines or infrared media. We also design a unified medium access control (MAC) protocol so that it will be easy for swapping and including different hardware transceivers, such as Bluetooth. The designed platform is low cost, low power, flexible, and easy for construction and maintenance.\"",
        "1 is \"Providing multiple data rates in infrastructure wireless networks\", 2 is \"YAPPERS: a peer-to-peer lookup service over arbitrary topology\"",
        "Given above information, for an author who has written the paper with the title \"Distributed Throughput Optimization for ZigBee Cluster-Tree Networks\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004159": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A 3D Modeling System for Creative Design':",
        "Document: \"A Method Of Image Edition With Haptic Devices. Advanced force feedback devices can realize tactile sensation with the current virtual reality techniquies. These devices are expected to bring new type of interactions between human and computers. In this paper an advanced image edition tool with these force feedback interfaces is proposed. Users can utilize haptic information for image edition additionally We discuss about a method of haptic rendering and display for image features by using a prototype of this tool.\"",
        "Document: \"A Handy Image Explorer with Tactile Feedback for Diagnostic Imaging. Diagnostic imaging with tracing a series of CT scan images is an important way for doctors to detect lesions. The doctors, however, need to view a huge number of high resolution images in a day, therefore, excessive burdens on their vision become a very serious problem. We propose a new method of assisting the diagnostic imaging with a multimodal interaction technique. We designed and prototyped a system allowing users (doctors) to stably exploring and finding critical features observed in the medical images even if they get eye strain. The system supports sound and hap tic modalities for assisting the users' vision to find important parts that cannot be overlooked in the images. It extracts some visual features observed in the images and transforms them to other sensory parameters. Then, the system notifies the doctors for possible existence of lesions in the images with multiple sensory outputs. This paper especially presents a method to use hap tic modality, the sense of touch, as a sensory channel for calling the users' attention to focus on some specific parts in an image. We also show mobile tablet personal computers (PCs) are promising hardware platform to implement the system.\"",
        "Document: \"Foot-controlled interaction assistant based on visual tracking. Whereas a mouse and a keyboard are indispensable devices for computer operation, they are totally useless for users who have upper limb disabilities. Customized devices to support these users tend to be expensive and have some functional limitations. We propose a method to effectively implement the system for assisting them based on visual tracking technology with readily-accessible consumer electronics such as a web camera and a low-cost motion sensor. In this paper, we describe the implementation method and a preliminary experiment conducted to verify the effectiveness of the proposed system.\"",
        "Document: \"Development and evaluation of a hybrid shared tele-haptic system. A constitution method of a distributed virtual environment using haptic devices is shown. In recent years, the force-feedback devices have been developed and they have gotten a lot of attentions to realize a new interaction modality between human begins and computer systems. The haptic channels, however, are more sensitive to the network QoS (quality of service) such as delays and packet losses than the visual and acoustic channels. Therefore, we propose a new constitution method for realizing an effective virtual environment even in the low-quality network. The proposed method adopts a hybrid approach using the client-server and peer-to-peer models. In this paper, we describe our proposed method, an implemented system, and the results of a preliminary experiment to evaluate the scalability of the proposed architecture.\"",
        "Document: \"A distributed virtual reality framework for Korea-Japan high-speed network test bed. A framework to practically constitute a Distributed Virtual Reality (DVR) system on a heterogeneous high-speed network environment is shown. The proposed approach essentially integrates a 'haptic' channel into traditional vision and acoustic modalities. The haptic technology empowers the reality of the DVR system by allowing users to touch virtual objects. The human touch sensation, however, is very sensitive to delays and jitters; therefore, we propose a hybrid DVR architecture to realise both data consistency by client-server and scalability by peer-to-peer models on a long-haul network. Some experiments using a Korea-Japan high-speed research network are described to validate the proposed method.\"",
        "Document: \"A Method to Improve Usability of Image Editing Operation by Haptic Interaction. \u2014Nowadays, advanced virtual reality technology realize to generate tactile sensations for user interface with haptic information, such as force feedback and tactile sensation. Haptic devices are expected to achieve new type of interaction between human and computer. We have proposed an advanced framework of image editing operations with haptic interaction. Users can utilize haptic information to edit images additionally. It can improve the usability and user-friendliness. In this paper, especially, we discuss using force feedback as supporting operation for image edition by using a prototype of this tool.\"",
        "Document: \"A Web-Based Artwork Editing System Empowered by Neural Style Transfer. A technique called neural style transfer is an effective method for generating artistic images based on a deep learning technique. It can extract a mood of a specific painting and blends it with a different image. The original method, however, needs a high-performance computer to get an output image within a practical response time since the neural style transfer involves heavily-loaded processing. To solve the problem, we develop a web-based image editing system enabling users to readily access the function only by using a mobile device with a standard web browser and a network connection. The proposed system allows the users to easily generating a wide variety of artistic images like logos and image clips using the neural style transfer anywhere they have a connection to the Internet. We implement the system as a web application and conduct some experiments to verify the effectiveness of the system. We elaborate the implementation method, experimental results, and observations in this paper.\"",
        "Document: \"A Device Identification Method for AR-based Network Topology Visualization. It is a crucial issue for a network administrator to quickly and accurately acquire up-to-data network topology information for installing new devices, changing existing routes, and fixing communication failures. In this paper, we propose a network topology visualization system based on mobile AR (Augmented Reality) for assisting the administrator in real working environments. A focus to effectively support him/her is to provide a way for automatically identifying a specific network device he/she is working on and present its latest topology information without interrupting his/her administration tasks. The proposed device identification method estimates the administrator's physical location using the signal strength of wireless APs (access points) receivable at the location and makes a guess for candidate target devices such as a group of routers and switches. Next, it narrows down to a specific device among the candidates using a vision-based object identification technique. Then, the system graphically visualizes the network topology information of the identified device through an HMD (Head Mounted Display) device worn by the administrator. The system supports not only senior administrators to efficiently performing complex management tasks, but also juniors and trainees to acquire practical knowledge and skills on network administration.\"",
        "Document: \"A Touch Screen Interface Design with Tactile Feedback. Various information displays are becoming available for implementing new kinds of human computer interaction (HCI) methods. Touch screen devices become the most popular choice among many types and models. They have been used in wide range of applications and are proven to be a useful infrastructure for creating intuitive HCI. In spite of their popularity, there are some weak points. The most serious drawback is their hardness for operation especially for the weak in information technology such as elderly and blind users. A tactile feedback function has a potential ability for enabling them to make full use of the devices. We consider the tactile interaction as communication modality for complementing other channels such as visual and auditory senses and improving intuitiveness for various operations. To make the tactile interface a practical communication channel, a design principle for implementing mutually discriminable tactile stimuli is required. The principle should define multiple stimulus patterns giving users distinctive tactile impressions. Our goal is to empirically work out the principle through developing an experiment system for checking varied tactile effects and discovering good solutions. In this paper, we elaborate the system implemented by using a type of touch screen tactile display and some experiments conducted for exploring the principle.\"",
        "Document: \"Development of Medical Imaging Diagnosis Support System with Sound Effects. Recently, Computer Aided Diagnosis (CAD) has become one of the most important for medical activity. The more exact and various CAD become, the larger amount of medical images are provided. Furthermore, these images becomes high definition. Radiologists have to cost their time and efforts to investigate these medical images. It is strongly required to reduce their burden without debasing the quality of imaging diagnosis. In this paper, we propose the technique to generate sound information based on the image features and discuss their effects for diagnosis. Generating sound effects helps for attention rousing and the fatigue reduction for medical imaging diagnosis.\"",
        "1 is \"Decoupled simulation in virtual reality with the MR toolkit\", 2 is \"Learning hierarchical bayesian networks for large-scale data analysis\"",
        "Given above information, for an author who has written the paper with the title \"A 3D Modeling System for Creative Design\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004170": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Automatically discovering word senses':",
        "Document: \"Word-for-word glossing with contextually similar words. Many corpus-based machine translation systems require parallel corpora. In this paper, we present a word-for-word glossing algorithm that requires only a source language corpus. To gloss a word, we first identify its similar words that occurred in the same context in a large corpus. We then determine the gloss by maximizing the similarity between the set of contextually similar words and the different translations of the word in a bilingual thesaurus.\"",
        "Document: \"Discriminative learning of selectional preference from unlabeled text. We present a discriminative method for learning selectional preferences from unlabeled text. Positive examples are taken from observed predicate-argument pairs, while negatives are constructed from unobserved combinations. We train a Support Vector Machine classifier to distinguish the positive from the negative instances. We show how to partition the examples for efficient training with 57 thousand features and 6.5 million training instances. The model outperforms other recent approaches, achieving excellent correlation with human plausibility judgments. Compared to Mutual Information, it identifies 66% more verb-object pairs in unseen text, and resolves 37% more pronouns correctly in a pronoun resolution experiment.\"",
        "Document: \"Induction of semantic classes from natural language text. Many applications dealing with textual information require classification of words into semantic classes (or concepts). However, manually constructing semantic classes is a tedious task. In this paper, we present an algorithm, UNICON, for UNsupervised Induction of CONcepts. Some advantages of UNICON over previous approaches include the ability to classify words with low frequency counts, the ability to cluster a large number of elements in a high-dimensional space, and the ability to classify previously unknown words into existing clusters. Furthermore, since the algorithm is unsupervised, a set of concepts may be constructed for any corpus.\"",
        "Document: \"A probability model to improve word alignment. Word alignment plays a crucial role in statistical machine translation. Word-aligned corpora have been found to be an excellent source of translation-related knowledge. We present a statistical model for computing the probability of an alignment given a sentence pair. This model allows easy integration of context-specific features. Our experiments show that this model can be an effective tool for improving an existing word alignment.\"",
        "Document: \"University of Manitoba: description of the NUBA system as used for MUC-5. Abduction is the inference to the best explanation. Many tasks in natural language understanding such as word-sense disambiguity [1], local pragmatics [4], metaphor interpretation [3], and plan recognition [5, 8], can be viewed as abduction.\"",
        "Document: \"An Information-Theoretic Definition of Similarity. Similarity is an important and widely used con- cept. Previous definitions of similarity are tied to a particular application or a form of knowl- edge representation. We present an information- theoretic definition of similarity that is applica- ble as long as there is a probabilistic model. We demonstrate how our definition can be used to measure the similarity in a number of different domains.\"",
        "Document: \"Integrating probabilistic, taxonomic and causal knowledge in abductive diagnosis.   We propose an abductive diagnosis theory that integrates probabilistic, causal and taxonomic knowledge. Probabilistic knowledge allows us to select the most likely explanation; causal knowledge allows us to make reasonable independence assumptions; taxonomic knowledge allows causation to be modeled at different levels of detail, and allows observations be described in different levels of precision. Unlike most other approaches where a causal explanation is a hypothesis that one or more causative events occurred, we define an explanation of a set of observations to be an occurrence of a chain of causation events. These causation events constitute a scenario where all the observations are true. We show that the probabilities of the scenarios can be computed from the conditional probabilities of the causation events. Abductive reasoning is inherently complex even if only modest expressive power is allowed. However, our abduction algorithm is exponential only in the number of observations to be explained, and is polynomial in the size of the knowledge base. This contrasts with many other abduction procedures that are exponential in the size of the knowledge base. \"",
        "Document: \"DIRT @SBT@discovery of inference rules from text. In this paper, we propose an unsupervised method for discovering inference rules from text, such as \"X is author of Y &ap; X wrote Y\", \"X solved Y &ap; X found a solution to Y\", and \"X caused Y &ap; Y is triggered by X\". Inference rules are extremely important in many fields such as natural language processing, information retrieval, and artificial intelligence in general. Our algorithm is based on an extended version of Harris' Distributional Hypothesis, which states that words that occurred in the same contexts tend to be similar. Instead of using this hypothesis on words, we apply it to paths in the dependency trees of a parsed corpus.\"",
        "Document: \"Parsing semantic dependencies in associative networks. Associative networks are commonly used as a represents tion for domain knowledge. One of the important kinds of reasoning in the networks is marker passing. During the marker passing process, activation markers are first assigned to two nodes in the network and then passed to the neighbouring nodes. When two markers meet, a path linking the two starting nodes can be found. Each of these paths identifies a relationship between the two starting nodes connected by the path [Charniak, 1986; Quillian, 1968; Norvig, 19871. Often, we need to find the relationships between\"",
        "Document: \"Unsupervised translation sense clustering. We propose an unsupervised method for clustering the translations of a word, such that the translations in each cluster share a common semantic sense. Words are assigned to clusters based on their usage distribution in large monolingual and parallel corpora using the soft K-Means algorithm. In addition to describing our approach, we formalize the task of translation sense clustering and describe a procedure that leverages WordNet for evaluation. By comparing our induced clusters to reference clusters generated from WordNet, we demonstrate that our method effectively identifies sense-based translation clusters and benefits from both monolingual and parallel corpora. Finally, we describe a method for annotating clusters with usage examples.\"",
        "1 is \"Extracting Patterns and Relations from the World Wide Web\", 2 is \"Authoritative sources in a hyperlinked environment\"",
        "Given above information, for an author who has written the paper with the title \"Automatically discovering word senses\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004173": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Performance analysis and adaptive call admission control in cellular mobile networks with time-varying traffic':",
        "Document: \"Optimal and Fair Rate Adaptation in Wireless Mesh Networks Based on Mathematical Programming and Game Theory. The authors have proposed a fair solution to optimal rate adaptation. The problem has been described in terms of the objective function, decision variables and constraints. Furthermore, using this problem definition, a rate adaptation heuristic was developed and divided into two sub-problems; part one requires finding the optimal rate allocation within the network, part two continues by finding a fair solution whilst still keeping an optimal rate allocation. The heuristic relies on cooperation in the network, and information regarding selected rates and loss due to interference, is distributed between neighbouring nodes. Furthermore, the heuristic is modelled as a repeated game with infinite horizon and it is shown how cooperation can be enforced. A Stack topology has been used for analysing and comparing performance and OMNeT++ 4.2.2 has been selected as simulation platform. The authors have shown that the heuristic effectively reduces the packet loss ratio (PLR). Thereafter, it was shown that the solution is both fair and optimal in terms of data rate.\"",
        "Document: \"Phase-type models for cellular networks supporting voice, video and data traffic. This paper presents an analytical model for cellular networks supporting voice, video and data traffic. Self-similar and bursty nature of the incoming traffic causes correlation in inter-arrival times of the incoming traffic. Therefore, arrival of calls is modeled with Markovian arrival process as it allows for the correlation. Call holding times, cell residence times and retrial times are modeled as phase-type distributions. We consider that the cells in a cellular network are statistically homogeneous, so it is enough to investigate a single cell for the performance analysis of the entire networks. With appropriate assumptions, the stochastic process that describes the state of a cell is a Quasi-birth-death (QBD) process. We derive explicit expressions for the infinitesimal generator matrix of this QBD process. Also, expressions for performance measures are obtained. Further, complexity involved in computing the steady-state probabilities is discussed. Finally, queueing examples are provided that can be obtained as particular cases of the proposed analytical model.\"",
        "Document: \"Adaptive dual-radio spectrum-sensing scheme in cognitive radio networks. In this paper, a novel spectrum-sensing scheme, called adaptive dual-radio spectrum-sensing scheme (ADRSS), is proposed for cognitive radio networks. In ADRSS, each secondary user (SU) is equipped with a dual radio. During the data transmission, with the received signal-to-noise ratio of primary user (PU) signal, the SU transmitter (SUT) and the SU receiver (SUR) are selected adaptively to sense one channel by one radio while communicating with each other by the other one. The sensing results of the SUR are sent to the SUT through feedback channels (e.g., ACK). After that, with the sensing results from the SUT or the SUR, the SUT can decide whether the channel switching should be carried out. The theoretical analysis and simulation results indicate that the normalized channel efficiency, defined as the expected ratio of time duration without interference to PUs in data transmission to the whole frame length, can be improved while satisfying the interference constraint to PUs. After that, an enhanced ADRSS is designed by integrating ADRSS with cooperative spectrum sensing, and the performance of ADRSS under imperfect feedback channel is also discussed. Copyright (c) 2011 John Wiley & Sons, Ltd.\"",
        "Document: \"An Analytical Model For Prioritized Contention Access In Ecma-368 Mac Protocol. The European Computer Manufacturers Association (ECMA) International recently defined the ECMA-368 standard, which specifies the physical and media access control (MAC) layers for Ultra Wideband (UWB) based wireless personal area networks (WPANs). The MAC protocol in ECMA-368 has a superframe structure. Each superframe is divided into three different time periods. One of them is the prioritized contention access (PCA) period which supports content ion-based access between different traffic classes. In this paper, we propose an analytical model to evaluate the performance of PCA in ECMA-368 MAC protocol. We assume that packets follow the Markovian Arrival Process (MAP) and various service times can be modeled by different phase type distributions (PHs). We apply the Matrix Geometric Method (MGM) technique and model the system as a MAP/PH/1 queueing system. We derive the probability mass function for the number of the packets in the queue. and the cumulative distribution function for the packets' waiting time. The correctness of our proposed analytical model is validated via OPNET simulations.\"",
        "Document: \"A vacation model for the non-saturated readers and writers system with a threshold policy. The non-saturated Writers and Readers system with threshold service policy is a general situation of the classical Writers and Readers problem. It considers the case where both Writers and Readers have thresholds for beginning of service. This paper sets up two vacation models for the Writers and Readers, respectively. Since the dependency between Writers and Readers is approximated, this model system is an approximation. The numerical results compared with the simulation results show that this vacation model system approximates the non-saturated Writers and Readers problem very well.\"",
        "Document: \"A Survey on an Energy-Efficient and Energy-Balanced Routing Protocol for Wireless Sensor Networks. Wireless sensor networks (WSNs) form an important part of industrial application. There has been growing interest in the potential use of WSNs in applications such as environment monitoring, disaster management, health care monitoring, intelligence surveillance and defence reconnaissance. In these applications, the sensor nodes (SNs) are envisaged to be deployed in sizeable numbers in an outlying area, and it is quite difficult to replace these SNs after complete deployment in many scenarios. Therefore, as SNs are predominantly battery powered devices, the energy consumption of the nodes must be properly managed in order to prolong the network lifetime and functionality to a rational time. Different energy-efficient and energy-balanced routing protocols have been proposed in literature over the years. The energy-efficient routing protocols strive to increase the network lifetime by minimizing the energy consumption in each SN. On the other hand, the energy-balanced routing protocols protract the network lifetime by uniformly balancing the energy consumption among the nodes in the network. There have been various survey papers put forward by researchers to review the performance and classify the different energy-efficient routing protocols for WSNs. However, there seems to be no clear survey emphasizing the importance, concepts, and principles of load-balanced energy routing protocols for WSNs. In this paper, we provide a clear picture of both the energy-efficient and energy-balanced routing protocols for WSNs. More importantly, this paper presents an extensive survey of the different state-of-the-art energy-efficient and energy-balanced routing protocols. A taxonomy is introduced in this paper to classify the surveyed energy-efficient and energy-balanced routing protocols based on their proposed mode of communication towards the base station (BS). In addition, we classified these routing protocols based on the solution types or algorithms, and the input decision variables defined in the routing algorithm. The strengths and weaknesses of the choice of the decision variables used in the design of these energy-efficient and energy-balanced routing protocols are emphasised. Finally, we suggest possible research directions in order to optimize the energy consumption in sensor networks.\"",
        "Document: \"Cooperative sensing with transmit diversity based on randomised STBC in CR networks. In this paper, a cognitive radio (CR) network composed of K secondary users who cooperatively sense a channel using the k-out-of-K fusion rule to determine the presence of the primary user is studied. The sensing-throughput tradeoff problem is investigated in a realistic environment where both the sensing channels and reporting channels are characterized by fading channels. It is observed that taking into consideration the probability of reporting error in the CR network increases the sensing time and reduces the maximum average throughput of the secondary users. To mitigate the effect of the probability of reporting error, a transmit diversity based cooperative spectrum sensing method using randomized space-time block coding (RSTBC) is proposed. Simulations results show that the spatial diversity gain induced by RSTBC significantly decreases the sensing time and improves the throughput of the secondary users.\"",
        "Document: \"Dynamic Load-Balancing Spectrum Decision for Heterogeneous Services Provisioning in Multi-Channel Cognitive Radio Networks. In this paper, we study dynamic load-balancing spectrum decision for a cognitive radio network (CRN) that dynamically distributes packets from the secondary user (SU) to different available primary channels. We consider two different classes of services at the SU, i.e., delay sensitive (DS) and best effort (BE) services, and assign a higher priority to the DS services. We apply priority queuing mo...\"",
        "Document: \"Combined Elapsed Time and Matrix-Analytic Method for the Discrete Time GI/G/1 and GIX/G/1 Systems. In this paper, we show that the discrete GI/G/1 system can be easily analysed as a QBD process with infinite blocks by using the elapsed time approach in conjunction with the Matrix-geometric approach. The positive recurrence of the resulting Markov chain is more easily established when compared with the remaining time approach. The G-measure associated with this Markov chain has a special structure which is usefully exploited. Most importantly, we show that this approach can be extended to the analysis of the GIX/G/1 system. We also obtain the distributions of the queue length, busy period and waiting times under the FIFO rule. Exact results, based on computational approach, are obtained for the cases of input parameters with finite support \u2013 these situations are more commonly encountered in practical problems.\"",
        "Document: \"Entrywise perturbation theory for diagonally dominant M-matrices with applications. Summary.\u00a0\u00a0 This paper introduces a new perspective on the study of computational problems related to diagonally dominant M-matrices\n by establishing some new entrywise perturbation results. If a diagonally dominant M-matrix is perturbed in a way that each\n off-diagonal entry and its row sums (i.e. the quantities of diagonal dominance) have small relative errors, we show that its\n determinant, cofactors, each entry of the inverse and the smallest eigenvalue all have small relative errors. The error bounds\n are given and they do not depend on any condition number. Applying this result to the studies of electrical circuits and tail\n probabilities of a queue whose embedded Markov chains is of GI/M/1 type, we discuss the relative sensitivity of the operating\n speed of circuits and of the percentile of the queue length, respectively.\n \"",
        "1 is \"Reality mining: sensing complex social systems\", 2 is \"Ally Friendly Jamming: How to Jam Your Enemy and Maintain Your Own Wireless Connectivity at the Same Time\"",
        "Given above information, for an author who has written the paper with the title \"Performance analysis and adaptive call admission control in cellular mobile networks with time-varying traffic\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004203": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Lexical classification in a speech understanding system using fuzzy relations.':",
        "Document: \"Towards the Integration of Different Knowledge Sources in Model-Based Diagnosis. Over the last few years several attempts have been made to design diagnostic systems which combine more than one model of the system to be diagnosed or more than one reasoning mechanism. In this paper we analyze the integration of different knowledge sources in model-based diagnosis, and, in particular, abductive diagnosis. We consider behavioral models in which the interaction between processes can be represented, enriched with constraints and taxonomic relationships among diagnostic hypotheses. We provide several insights into the role of such forms of knowledge in model-based diagnosis, showing how they can be accommodated both in a meta-level definition of abduction with constraints and in an object-level framework for abduction.\"",
        "Document: \"Monitoring the Execution of a Multi-Agent Plan: Dealing with Partial Observability. The paper addresses the task of monitoring and diagnosing the execution of a multi-agent plan (MAP) which involves actions concurrently executed by a team of cooperating agents. The paper describes a weak commitment strategy to deal with cases where observability is only partial and it is not sufficient for inferring the outcome of all the actions executed so far. The paper discusses the role of target actions in providing sufficient conditions for inferring the pending outcomes in a finite time window. The action outcome provides the basis for computing plan diagnosis and for singling out the goals which will not be achieved because of an action failure.\"",
        "Document: \"Ubiquitous User Assistance in a Tourist Information Server. This paper discusses the provision of ubiquitous services for desktop and handheld interfaces. The focus of the paper is on the interactive agenda offered by INTRIGUE, a tourist information server which assists the user in the organization of a tour by providing personalized recommendations of tourist attractions and helping the user to schedule her itinerary. The interactive agenda can be accessed both remotely, by interacting with the central server, and locally to the user's device.\"",
        "Document: \"On the role of abduction.  point of view, this correspondsto the unsound inference concluding A from A ! B and B. Abduction is thusa form of non-classical inference whose properties (e.g., non-monotonicity) aresimilar to those of other non-classical logics proposed and studied in the AIliterature.Given such premises, one would expect that abduction played a central rolein the logical approaches to AI. In fact, however, many researchers are quitediffident about such a form of reasoning and some criticisms have... \"",
        "Document: \"Towards An Integration Of Time And Causation In A Hybrid Knowledge Representation Formalism. The article focuses on the relations between time and causation and proposes a general framework in which a strict integration is achieved both in the representation formalism and in the reasoning process. By taking advantage of the basic capabilities of a Hybrid Knowledge Representation formalism (in particular, we use BACK), we provide an explicit and integrated representation of the basic entities in the temporal and causal ontology, and propose a taxonomy in which different types of causal relations found in the literature are classified, depending on the temporal relations they impose between causes and effects. Moreover, we develop a specialized causal-temporal reasoner which operates on causal nets built upon the basic causal-temporal representation. Such a reasoner is flexible (in the sense that different definitions of causation may be considered in the same reasoning process) and integrated (since the mutual effects of time and causation are coped with), and takes advantage of a specialized temporal reasoner. (C) 1994 John Wiley & Sons, Inc.\"",
        "Document: \"Decomposing and Distributing Configuration Problems. In the present work the issue of decomposing and distributing a configuration problem is approached in a framework where the domain knowledge is represented in a structured way by using a KL-One like language, where whole-part relations play a major role in defining the structure of the configurable objects. The representation formalism provides also a constraint language for expressing complex relations among components and subcomponents.The paper presents a notion of boundness among constraints which specifies when two components can be independently configured. Boundness is the basis for partitioning constraints and such a partitioning induces a decomposition of the configuration problem into independent subproblems that are distributed to a pool of configurators to be solved in parallel.Preliminary experimental results in the domain of PC configuration showing the effectiveness of the decomposition technique in a sequential approach to configuration are also presented.\"",
        "Document: \"Dealing with uncertain knowledge in medical decision-making: A case study in hepatology. It has widely been recognized that knowledge-based expert systems need efficient mechanisms to model the uncertainty associated with many decision-making activities. Such a need is particularly urgent in medicine. In this paper, we present an approach based on fuzzy logic to give a possible solution to this problem; its pros and cons are discussed by taking into account the experience gained in developing LITO1 and LITO2, two expert systems devoted to the assessment of the liver function and to the diagnosis of hepatic diseases. The advantages of mixing fuzzy production rules with frame-like structures (introduced for representing the clinical data) are discussed. In particular, the use of fuzzy linguistic variables for modeling the possible values of the clinical data is described: this allows, for a clear and perspicuous description of the correspondence between quantitative and qualitative expressions. Furthermore, different alternatives for evaluating and combining evidence are reviewed. Finally, the need of introducing frame structures also for representing diagnostic hypotheses is discussed, together with the problem of evaluating the fuzzy match between the prototypical description of a diagnostic hypothesis and the data describing the status of the specific patient under examination.\"",
        "Document: \"Dynamic User Modeling in a Web Store Shell. We describe a framework for the dynamic revision of user models in an adaptive Web store shell, which tailors the suggestion of goods, as well as the interaction style, to the characteristics and interests of the individual user. The behavior of the user is unobtrusively monitored, by segmenting it on the basis of the focus spaces explored in the browsing activity, and the actions performed by the user are summarized into abstract facts. These facts are used for revising the user model via the interpretation process performed by a Bayesian Network which relates user features to user behavior.\"",
        "Document: \"On-line monitoring and diagnosis of a team of service robots: A model-based approach. The paper presents an approach for the on-line monitoring and diagnosis of multi-robot systems where services are provided by a team of robots and the environment is only partially observable via a net of fixed sensors. This kind of systems exhibits complex dynamics where weakly predictable interactions among robots may occur. To face this problem, a model-based approach is adopted: in particular, the paper discusses how to build a system model by aggregating a convenient set of basic system components, which are modeled via communicating automata. Since the dynamics of a multi-robot system depend on the actions performed by the robots (and actions change over time), the global system model is partitioned into a number of submodels, each one describing the dynamics of a single action.The paper introduces the architecture of the Supervisor which has to track the actions progress and to infer an explanation when an action is completed with delay or fails. The Supervisor includes two main modules: the On-line Monitoring Module (OMM) tracks the status of the system by exploiting the (partial) observations provided by sensors and robots. When the monitor detects failures in the actions execution, the Diagnostic Interpretation Module (DIM) is triggered for explaining the failure in terms of faults in the robots and/or troublesome interactions among them.The RoboCare domain has been selected as a test bed of the approach. The paper discusses experimental results collected in such a domain with particular focus on the competence and the efficiency of both the OMM and the DIM.\"",
        "Document: \"Supervision and diagnosis of joint actions in multi-agent plans. The paper formalizes a distributed approach to the problem of supervising the execution of a multi-agent plan where (possibly joint) actions are executed concurrently by a team of cooperating agents in a partially observable environment. The notions of plan and agent diagnosis are introduced and discussed.\"",
        "1 is \"Assessing the relevance of identifier names in a legacy software system\", 2 is \"Multi-UAV Cooperation and Control for Load Transportation and Deployment\"",
        "Given above information, for an author who has written the paper with the title \"Lexical classification in a speech understanding system using fuzzy relations.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004276": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Performance Analysis Of Flow-Based Traffic Splitting Strategy On Cluster-Mesh Sensor Networks':",
        "Document: \"An Efficient Multi-fidelity Bayesian Optimization Approach for Analog Circuit Synthesis. This paper presents an efficient multi-fidelity Bayesian optimization approach for analog circuit synthesis. The proposed method can significantly reduce the overall computational cost by fusing the simple but potentially inaccurate low-fidelity model and a few accurate but expensive high-fidelity data. Gaussian Process (GP) models are employed to model the low- and high-fidelity black-box functions separately. The nonlinear map between the low-fidelity model and high-fidelity model is also modelled as a Gaussian process. A fusing GP model which combines the low- and high-fidelity models can thus be built. An acquisition function based on the fusing GP model is used to balance the exploitation and exploration. The fusing GP model is evolved gradually as new data points are selected sequentially by maximizing the acquisition function. Experimental results show that our proposed method reduces up to 65.5% of the simulation time compared with the state-of-the-art single-fidelity Bayesian optimization method, while exhibiting more stable performance and a more promising practical prospect.\n\n\"",
        "Document: \"An Efficient Optimization Based Method to Evaluate the DRV of SRAM Cells. To reduce the substantial leakage current, the supply voltage of SRAM cells has being scaled down towards its lower limit, which is called the data Retention Voltage (DRV). Although the power consumption is largely reduced, this down-scaling trend, however, impacts the stability of the SRAM cell due to the unpredictable process or device parameter variations. In this work, we propose a novel method to evaluate the DRV of SRAM cells at the presence of variations. The DRV issue is first formulated as a time domain worst performance bound problem. To accurately and efficiently evaluate the DRV, a multi-start point (MSP) optimization strategy is then studied and developed with the use of practical circuit simulator. One feature of the proposed method is that it can efficiently evaluate the DRV without suffering from any process/model accuracy. Experiment results show that it achieves a speedup of 3 and 5-7 order over the Importance Sampling (IS) and Monte Carlo (MC) method respectively under the context of the DRV evaluation in this paper. The proposed method can serve as an efficient DRV evaluation tool on any specific technology process or in-house circuit simulator. In this work, the DRVs at the technology node from 130 nm to 45 nm under the influence of different variation sources are also presented and analyzed.\"",
        "Document: \"Direct Nonlinear Order Reduction with Variational Analysis. The variational analysis [11] has been employed in [7] for order reduction of weakly nonlinear systems. For a relatively strong nonlinear system, this method will mostly lose efficiency because of the exponentially increased number of inputs in higher order variational equations caused by the individual reduction process of the variational systems. Moreover, the inexact inputs into the higher order variational equations indispensably introduce extra errors in theorder reduction process. Inspired by the variational analysis, we propose a direct model order reduction method. The order of the approximate polynomial system of the original nonlinear system is directly reduced by one project space. The proposed direct reduction technique can easily avoid the errors brought by inexact inputs and the exponentially increased inputs. We show theoretically and experimentally that the proposed method can achieve much more accurate reduced system with smaller order size than the conventional variational equation order reduction method.\"",
        "Document: \"Analog circuit performance bound estimation based on extreme value theory. Estimation of performance bound with respect to design parameters play an important role in analog circuit design. The challenge is that circuit performance among design parameter space is normally non-parametric distributed, which is difficult to model or fit with known probability distributions. This paper present a novel performance bound estimation method based on extreme value theory. The method first generates generalized extreme value (GEV) distribution of performance parameter through sampling. Then the performance bound is estimated on the GEV distribution. Experimental results show that this method achieves less than 5% estimation error within 1,000 circuit simulations.\"",
        "Document: \"C-YES: An Efficient Parametric Yield Estimation Approach for Analog and Mixed-Signal Circuits Based on Multicorner-Multiperformance Correlations. Parametric yield estimation is a critical task for design and validation of analog and mixed-signal (AMS) circuits. However, the computational cost for yield estimation based on Monte Carlo (MC) analysis is often prohibitively high, especially when multiple circuit performances and/or environmental corners (e.g., voltage and temperature corners) are considered. In this paper, a novel statistical method named correlation-aided yield estimation (C-YES) is proposed to reduce the computational cost for parametric yield estimation. Our proposed approach exploits the fact that multiple circuit performances over different environmental corners are often correlated. Hence, we can accurately predict the performance value at one corner from the simulation results for other performances and/or corners. Based upon this observation, instead of running a large number of MC simulations to cover all performances and corners, an efficient algorithm is developed to select a small set of the most \u201cinformative\u201d simulations that should be performed for yield estimation. Our numerical experiments show that for parametric yield estimation with multiple circuit performances and environmental corners, C-YES achieves 6.5\u2013 $9.3{ \\\\boldsymbol {\\\\times } }$ runtime speedups over other conventional methods.\"",
        "Document: \"Layout decomposition co-optimization for hybrid e-beam and multiple patterning lithography. As the feature size keeps scaling down and the circuit complexity increases rapidly, a more advanced hybrid lithography, which combines multiple patterning and e-beam lithography (EBL), is promising to further enhance the pattern resolution. In this paper, we formulate the layout decomposition problem for this hybrid lithography as a minimum vertex deletion K-partition problem, where K is the number of masks in multiple patterning. Stitch minimization and EBL throughput are considered uniformly by adding a virtual vertex between two feature vertices for each stitch candidate during the conflict graph construction phase. For K = 2, we propose a primal-dual method for solving the underlying minimum odd-cycle cover problem efficiently. In addition, a chain decomposition algorithm is employed for removing all \u201cnon-cyclable\u201d edges. For K > 2, we propose a random-initialized local search method that iteratively applies the primal-dual solver. Experimental results show that compared with a two-stage method, our proposed methods reduce the EBL usage by 64.4% with double patterning and 38.7% with triple patterning on average for the benchmarks.\"",
        "Document: \"Stochastic coverage in event-driven sensor networks. One of the primary tasks of sensor networks is to detect events in a field of interest (FoI). To quantify how well events are detected in such networks, coverage of events is a fundamental problem to be studied. However, traditional studies mostly focus on analyzing the coverage of the FoI, which is usually called area coverage. In this paper, we propose an analytic method to evaluate the performance of event coverage in sensor networks with randomly deployed sensor nodes and stochastic event occurrences. We provide formulas to calculate the probabilities of event coverage and event missing. The numerical results show how these two probabilities change with the sensor and event densities. Moreover, simulations are conducted to validate the analytic method. This method can provide guidelines for determining the amount of sensor nodes to achieve a certain level of coverage in event-driven sensor networks.\"",
        "Document: \"A novel wavelet method for noise analysis of nonlinear circuits. In this paper, a novel wavelet method is proposed for noise analysis of nonlinear circuits. Compared with the existing algorithms capable of accessing circuit performance in the present of noise, the proposed method presents several merits. First, it fully accounts for nonlinearities. Second, it can handle signals with continuous frequency spectra. Third, by taking advantage of the properties of the wavelet bases, such as local compactness and multi-resolution, it holds high simulation speed and high accuracy. Furthermore, an adaptive scheme exists to automatically select the wavelet basis functions for a desired accuracy. All these merits make the novel wavelet method outperforms its previous techniques.\"",
        "Document: \"Implementations Of Fft And Stbd For Mimo-Ofdm On A Reconfigurable Baseband Platform. MIMO-OFDM systems aim to improve transmission quality and/or throughput but require significant signal processing capability and flexibility at reasonable cost This paper proposes a reconfigurable architecture and associated algorithm optimizations for these types of systems based on the IEEE 802 11n and IEEE 802 16e standards In particular, we describe the implementation Of two key computations onto this architecture. namely Fast Fourier Transform (FFT) and Space-Time Block Decoding (STBD) The design is post-layout using a UMC 0 18 micron technology at a clock rate of 100 MHz Performance comparisons with Other optimization met hock and hardware implementations are given.\"",
        "Document: \"LVS verification across multiple power domains for a quad-core microprocessor. A unique LVS (layout-versus-schematic) methodology has been developed for the verification of a four-core microprocessor with multiple power domains using a triple-well 90-nm CMOS technology. The chip is migrated from its previous generation that is for a twin-well process. Due to the design reuse, VDD and GND are designed as global nets but they are not globally connected across the entire chip. The standard LVS flow is unable to handle the additional design complexity and there seems to be no published literature tackling the problem. This paper presents a two-phase LVS methodology: a standard LVS phase where power and ground nets are defined as global nets and a multi-power-domain LVS phase where power and ground nets are treated as local nets. The first phase involves verifying LVS at the block level as well as the full-chip level. The second phase aims at verifying the integrity of the multi-power-domain power grid that is not covered in the first phase LVS. The proposed LVS methodology was successfully verified by real silicon.\"",
        "1 is \"Fault-tolerant tree-based multicasting in mesh multicomputers\", 2 is \"FAR-DS: full-plane AWE routing with driver sizing\"",
        "Given above information, for an author who has written the paper with the title \"Performance Analysis Of Flow-Based Traffic Splitting Strategy On Cluster-Mesh Sensor Networks\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004290": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Cooperative cognitive radio networking using quadrature signaling':",
        "Document: \"Performance analysis of burst switching for integrated voice/data services. The voice cutoff fraction and mean data waiting time of an integrated burst-switched system are analyzed. Several approximate approaches are proposed for the analysis. Comparison between simulation and approximate results shows that the accuracy of the approximations depends on the relationship between mu /sub d/ and mu /sub v/ where 1/ mu /sub d/ and 1/ mu /sub v/ are the mean lengths of data and voice packets (bursts), respectively. In general, the approximations give good accuracy for mu /sub d/ mu /sub v/, the approximations for the mean data waiting time give less accurate but adequate results. From the analysis, it is found that a burst-switched system can achieve a higher bandwidth efficiency when data traffic has a shorter burst length (e.g. interactive data). Furthermore, it is also observed that the relative burst lengths between voice and data can significantly affect the characteristic of the voice clipping process. The analysis shows that, under normal operating condition where voice traffic is predominant and the constraint that the voice cutoff fraction is less than 0.5%, the data queue is always stable. >\"",
        "Document: \"Integrated power control and rate allocation for radio resource management in uplink wideband CDMA systems. Power control and rate allocation are two radio resource management functions that can be used to enhance system utilization. In a time-varying environment, these functions need to be made adaptive. We consider certain implementation restrictions which cause deviations from ideal conditions in adaptive processing. An integrated power control and rate allocation approach to enhance system throughput is proposed. Simulation results show that by using closed-loop power control in conjunction with rate allocation in the presence of the Doppler effect and delay in feeding back channel state information (CSI), significant throughput and BER performance improvement over rate adaptation alone is obtained.\"",
        "Document: \"Air interface switching and performance analysis for fast vertical handoff in cellular network and WLAN interworking. The integration of wireless local area network (WLAN) hotspot and the 3G cellular networks is imminently the future mode of public access networks. One of the key elements for the successful integration is vertical handoff between the two heterogeneous networks. Service disruption may occur during the vertical handoff because of the IP layer handoff activities, such as registration, binding update, routing table update, etc. In this paper, the network interface switching and registration process are proposed for the integrated WLAN/cellular network. Two types of fast vertical handoff protocols based on bicasting and non-bicasting supporting real-time traffic, such as voice over IP, are modeled. The performance of a bicasting based handoff scheme is analyzed and compared with that of fast handoff without bicasting. Numerical results and the simulation are given to show that packet loss rate can be reduced by the bicasting during handoff scheme without increasing bandwidth on both wireless interfaces. Copyright (c) 2006 John Wiley & Sons, Ltd.\"",
        "Document: \"Downlink resource management for packet transmission in OFDM wireless communication systems. In this paper, an optimal downlink resource man- agement scheme for heterogeneous packet transmission in orthogonal frequency-divisi on multiplexing (OFDM) wireless communication systems is proposed. By making use of the channel impulse response and the properties of the OFDM physical layer, a resource management scheme is developed by integrating power distribution, subcarrier allocation, and the generalized processor sharing (GPS) scheduling. The scheme can: 1) maximize system throughput; 2) guarantee the required signal-to-noise ratio for het- erogeneous traffic; 3) provide fairness to all the traffic admitted in the system; and 4) satisfy the total transmission power constraint. For practical implementation, a simplified power and subcarrier allocation algorithm, a robust H \u221e channel estimation algorithm, and a truncated GPS (TGPS) scheduling scheme are introduced. Simulation results show that the proposed resource management scheme exhibits good throughput performance. Index Terms \u2014Generalized processor sharing, multiuser diver- sity, orthogonal frequency-divisi on multiplexing, power distribu- tion, subcarrier allocation, wireless communications. I.\"",
        "Document: \"An Innovations Approach to Adaptive Data Compression in Data Transmission. An innovations approach to data compression as a viable method to improve the efficiency of data transmission is proposed. The correlated source is modeled by a ratio of polynomials with slowly time-varying coefficients. Data compression is achieved in three steps by: 1) dithering the source signal with a zero mean white noise process to enable the application of the innovations concept, 2) transforming the dithered signal to an innovations process (correlation removal), and 3) quantizing the resultant innovations process to remove unnecessary fidelity. An adaptive implementation of the data compression system using tapped-delay lines (TDL's) is obtained using the steepest descent method. Preliminary computer simulation demonstrates that, for binary data transmission, a reduction in mean square signal value of approximately 3 is attained with no observable error at a 31-dB signal-to-noise ratio (SNR).\"",
        "Document: \"Trellis source codes designed by conjugate gradient optimization. Time-invariant trellis codes for stationary, ergodic, discrete-time sources are designed by unconstrained, nonlinear optimization of the performance in a simulated source encoding with the Viterbi algorithm. A nonderivative conjugate directions algorithm and a conjugate gradient algorithm with restarts are applied to design low-constraint-length, unit-rate, binary codes for the memoryless Gaussian source. The latter algorithm is also used to design codes for the memoryless Laplacian source and a third-order autoregressive model for speech. Good codes are tabulated and compared to other known results on a performance versus complexity basis. Those for the Gaussian source are tested in a joint (tandem) trellis-coding system with known convolutional channel codes. >\"",
        "Document: \"Credit-Based User Authentication For Delay Tolerant Mobile Wireless Networks. In this paper, a credit-based user authentication scheme is proposed for delay tolerant mobile wireless networks. The proposed authentication scheme isolates the uncertain network condition in the high-delay wireless backhaul with high error rate, and accelerates the overall authentication process when the mobile terminal roams in the visited network. The performance evaluation demonstrates that the proposed credit-based authentication scheme is secure and effectively reduces the overall delay and overhead in user authentication for delay tolerant mobile wireless networks.\"",
        "Document: \"A quadrature signaling based cooperative scheme for Cognitive Radio Networks. A two-phase cooperative framework in Cognitive Radio Networks (CRNs), whereby the secondary users (SUs) can fully exploit the transmission opportunities through cooperation with primary users (PUs), is proposed. Specifically, the SU cooperates with the active PU to improve the PU's utility. As a reward, the period of time when the PU is inactive, is allocated to the cooperating SU for its own transmissions. During the cooperation, the PU and the cooperating SU use quadrature amplitude modulation (QAM) to attain orthogonal signaling to cooperate efficiently, while the SU selects the optimal power allocation coefficient to maximize the performance of the PU, when its own transmission requirement is satisfied. The SU selection and power allocation determination procedure is formulated as a nonlinear optimization problem. The closed-form solution is derived. Numerical results demonstrate that, with the proposed cooperative strategy, the PU can achieve optimal performance and the SU can gain transmission opportunities through cooperation.\"",
        "Document: \"A Cross-Layer Path Selection Scheme For Video Streaming Over Vehicular Ad-Hoc Networks. A new cross layer path selection scheme with quality of service (QoS) support along urban areas is proposed. First, we introduce a cross-layer approach where the routing decision takes explicit consideration of the application layer objective function. Second, the queueing based mobility model, spatial traffic distribution and probability of connectivity for sparse and dense Vehicular Ad-Hoc Networks (VANETs) are taken into consideration for developing the cross-layer routing protocol. We focus on the video streaming application and optimize application layer performance metric, i.e., Peak Signal to Noise Ratio (PSNR). Video stream is downloaded from a Road Side Unit (RSU) deployed along the road and is transmitted to the destination vehicle via multi-hop communication. Simulation results show that the cross layer path selection scheme can achieve results close to the upper analytical bound.\"",
        "Document: \"Supporting voice and video applications over IEEE 802.11n WLANs. In this paper, an analytical model is developed for the performance study of an IEEE 802.11n wireless local area network (WLAN) supporting voice and video services, considering the new features of the medium access control (MAC) protocol proposed in IEEE 802.11n, i.e., frame aggregation and bidirectional transmission. We show that these enhanced MAC mechanisms can effectively improve the network capacity by not only reducing the protocol overheads, but also smoothing the AP-bottleneck effect in an infrastructure-based WLAN. Voice and video capacity under various MAC mechanisms are compared as well.\"",
        "1 is \"Gaussian Interference Networks: Sum Capacity in the Low-Interference Regime and New Outer Bounds on the Capacity Region\", 2 is \"Optimal Base-Station Locations in Two-Tiered Wireless Sensor Networks\"",
        "Given above information, for an author who has written the paper with the title \"Cooperative cognitive radio networking using quadrature signaling\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004335": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'BGRP plus: quiet grafting mechanisms for providing a scalable end-to-end QoS solution':",
        "Document: \"Mobile agent standards and available platforms. This paper examines the current status of standardization efforts concerning mobile agent technology and presents five Java-based mobile agent platforms. Standards directly relating to mobile agent technology are presented first, followed by an overview of other standardization efforts that while not directly relating to mobile agent technology, can still contribute significantly to its success, This is followed by a detailed presentation of five Java-based mobile agent platforms. The description of each platform examines its communication mechanisms, its architecture and the services that it offers to a developer. The presentation of the platforms ends with a comparative overview of their features accompanied by a brief presentation of some performance results. The paper concludes with some general remarks on the future of this technology. (C) 1999 Elsevier Science B.V. All rights reserved.\"",
        "Document: \"A signaling architecture for wireless ATM access networks. A multiservice wireless Asynchronous Transfer Mode &lpar;ATM&rpar; access system is considered from a signaling protocol viewpoint. In an attempt to generalize and extend results and experiences obtained from the specification, design, and implementation of fixed ATM&dash;based access networks, we extend the concept of the broadband V interface &lpar;referred to as VB&rpar; for application to wireless ATM access networks. The proposed architecture follows the signaling structure of Broadband ISDN &lpar;B&dash;ISDN&rpar; User\u2013Network Interface &lpar;UNI&rpar;, thus offering the possibility for integration of the wireless ATM access system into fixed B&dash;ISDN. It is shown that the use of the proposed access signaling architecture provides cost effective implementations without degrading the agreed Quality of Service &lpar;QoS&rpar;, and simplifies call/connection and handover control. The evaluation of the proposed access signaling protocol structure yields results that fall within acceptable ATM signaling performance measures. A performance comparison of our approach with an alternative access signaling configuration is also carried out to quantify the relative gains.\"",
        "Document: \"Privacy-Aware Access Control and Authorization in Passive Network Monitoring Infrastructures. Despite the usefulness of passive network monitoring for the operation, maintenance, control and protection of communication networks, as well as law enforcement, network monitoring activities are surrounded by serious privacy implications. In this paper, an innovative approach for privacy-preserving authorization and access control to data originating from passive network monitoring is described. The proposed framework relies on an ontological model for the specification of the access control policies, which are evaluated and enforced on a two-phase and two-stage basis by a system that intercedes between the network link and the monitoring applications. The two stages refer to controlled access regarding both the data that are disclosed to the monitoring application from the mediating system and the raw data that the mediator retrieves from the network link. On the other hand, the two phases concern respectively the execution of \u201cstatic\u201d and \u201cdynamic\u201d control; the former enforces the rules that are a priori applicable, grounded on the data, role and purpose semantics, while the latter evaluates the real-time contextual parameters for the adaptation of the access control procedures to the particular conditions underlying a request.\"",
        "Document: \"ICALB: an integrated congestion avoidance and load balancing algorithm for distributed intelligent networks. Part II: Performance evaluation of ICALB. This paper evaluates the Integrated Congestion Avoidance and Load Balancing (ICALB) algorithm [ICALB: an integrated congestion avoidance and load balancing algorithm for distributed intelligent networks-Part I. Description of ICALB, 2002]. We test its performance by applying it in a simulated Distributed Intelligent Network (D-IN). We evaluate the algorithm ability to protect the service execution nodes from congestion, to maintain a high throughput, and to ensure a fair distribution of the resource's capacity among services taking into account service priorities and quality of service requirements. The algorithm's operation mainly exploits the advantages of Common Object Request Broker Architecture (CORBA) Object Request Broker (ORB) middleware that allow the invocation of operations on distributed objects without concern for object location, programming language, OS platform, communication protocols and interconnects, and hardware. Hence, all necessary operations of the algorithm are assumed to be executed over the CORBA ORB.\"",
        "Document: \"Definition and Performance Evaluation of Network Services Deployed Over a Differentiated Services Network. A vital requirement for next generation IP networks is the provision of services with differentiated behavior and characteristics. The basic reason for that is the need to provide Quality of Service (QoS) to the different types of user traffic produced by applications that are different in nature and behavior, analogously to the IP network services. The Differentiated Services (DiffServ) paradigm is still one of the major outcomes of the research community toward the provision of QoS to individual customer needs and applications. This paper addresses the definition and deployment of specific network services in a DiffServ environment. We reuse and extend the fundamental concepts of the Expedited Forwarding and Assured Forwarding per hop behaviors in order to define four new network services, apart from the well known Best Effort one, which introduce a specific traffic handling implementation along with an Admission Control methodology. These are analyzed and simulated in the paper in order to evaluate their performance and confirm the correctness of their fundamental principles.\"",
        "Document: \"Heuristics for the multi-level capacitated minimum spanning tree problem. The capacitated minimum spanning tree (CMST) problem is fundamental to the design of centralized communication networks. In this paper we consider the multi-level capacitated minimum spanning tree problem, a generalization of the well-known CMST problem. Based on work previously done in the field, three heuristics are presented, addressing unit and non-unit demand cases. The proposed heuristics have been also integrated into a mixed integer programming solver. Evaluation results are presented, for an extensive set of experiments, indicating the improvements that the heuristics bring to the particular problem.\"",
        "Document: \"Label Management in Broadband Internetworks. The interworking of LANs, MANs and ATM are investigated and classified in an increasing order of label use. It is shown that efficient relay of information among diverse networks is achieved through protocols utilizing the label resources of the lowest possible layer. Through a label processing complexity taxonomy this study is aimed at assisting potential Interworking Unit (IWU) designers and implementers to decide on key options in this versatile and evolving environment.\"",
        "Document: \"On the use of Attribute-Based Encryption for multimedia content protection over Information-Centric Networks. AbstractVideo streaming dominates the Internet traffic, a trend that is expected to increase in the next years. End users expect to access video content, regardless of their device, connection type and location. On the other hand, content providers set strict security requirements regarding the protection of their content, making up an overall difficult and challenging context for Over-the-Top service providers. A miscellany of proprietary approaches tries to address these issues, providing, however, only partial solutions, and composing a domain, highly fragmented by non-interoperable products. In this paper, we propose an architecture that utilises Information-Centric Networking ICN in order to provide scalable and efficient video distribution, and Attribute-Based Encryption ABE to securely distribute this content in the ICN environment. ABE is incorporated into Digital Rights Management DRM schemes. We present the video license acquisition process, with a focus on orchestrating multiple authorities; subsequently, the video distribution over ICN is demonstrated, and finally, we investigate user revocation techniques in the ABE and ICN context and adopt the most appropriate one. Copyright \u00a9 2013 John Wiley & Sons, Ltd.\"",
        "Document: \"Privacy Protection in Passive Network Monitoring: An Access Control Approach. \"On the Internet, nobody knows you are a dog\" according to the famous Pat Steiner cartoon in The New Yorker, which has been very frequently cited in order to emphasize the potential for anonymity and privacy that the Internet was supposed to offer. However, the reality seems to be rather different. Among the several threats to personal privacy caused by the emerging Information and Communication Technologies, activities related to passive network monitoring hold an outstanding position. This paper discusses from an access control point of view the issue of privacy protection regarding data originating from passive network monitoring. It proposes a semantic information model, along with the underlying enforcement framework, for the real-time determination of access control provisions as stem from the personal data protection legislation and depending on the particular characteristics of every request for monitoring data disclosure.\"",
        "Document: \"Towards Inherent Privacy Awareness In Workflows. This paper presents a holistic approach to the realisation of Privacy by Design in workflow environments, ensuring that workflow models are rendered privacy-aware already at their specification phase. In this direction, the proposed framework, considering the particular technical requirements stemming from data protection principles, is centred around the following features: a novel, ontology-based approach to workflow modelling, which manages, unlike all other existing technologies, to adequately capture privacy aspects pertaining to workflow execution; the appropriate codification of privacy requirements into compliance rules and directives; an automated procedure for the verification of workflow models and their subsequent transformation, if needed, so that they become inherently privacy-aware before being deployed for execution.\"",
        "1 is \"Benchmarking of Signaling Based Resource Reservation in the Internet\", 2 is \"A caching strategy to reduce network impacts of PCS\"",
        "Given above information, for an author who has written the paper with the title \"BGRP plus: quiet grafting mechanisms for providing a scalable end-to-end QoS solution\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004364": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Pad minimization for planar routing of multiple power nets':",
        "Document: \"On the signal bounding problem in timing analysis. In this paper, we study the propagation of slew dependent bounding signals and the corresponding slew problem in static timing analysis. The selection of slew from the latest arriving signal, a commonly used strategy, may violate the rule of monotonic delay. Several methods for generating bounding signals to overcome this difficulty are described. The accuracy and monotonicity of each method is analyzed. These methods can be easily implemented in a static timer to improve the accuracy.\"",
        "Document: \"On Logarithmic Simulated Annealing. We perform a convergence analysis of simulated annealing for the special case of logarithmic cooling schedules. For this class of simulated annealing algorithms, B. HAJEK proved that the convergence to optimum solutions requires the lower bound \u0393/ln (k + 2) on the cooling schedule, where k is the number of transitions and \u0393 denotes the maximum value of the escape depth from local minima. Let n be a uniform upper bound for the number of neighbours in the underlying configuration space. Under some natural assumptions, we prove the following convergence rate: After k \u2265 nO(\u0393) + logO(1) (1/\u0190) transitions the probability to be in an optimum solution is larger than (1 - \u0190). The result can be applied, for example, to the average case analysis of stochastic algorithms when estimations of the corresponding values \u0393 are known.\"",
        "Document: \"Floating Steiner trees. We study the reproducing placement problem, which finds application in layout-driven logic synthesis. In each phase, a module (or gate) is decomposed into two (or more) simpler modules. The goal is to find a \u201cgood\u201d placement in each phase. The problem, being iterative in nature, requires an iterative algorithm. In solving the RPP, we introduce the notion of minimum floating Steiner trees (MFST). We employ an MFST algorithm as a central step in solving the RPP. A Hanan-like theorem is established for the MFST problem, and two approximation algorithms are proposed. Experiments on commonly employed benchmarks verify the effectiveness of the proposed technique\"",
        "Document: \"The Effect of a Capacity Constraint on the Minimal Cost of a Partition. A recent paper by Chandra and Wong considered the problem of finding a partition of nonnegatlve numbers into m groups to minimize a certain cost, the sum of the squares of the group sums. One apphcatlon of thin problem is m allocation of data records to disks to minimize arm con- tention, under certain assumptions about record accessing behavior In the paper by Chandra and Wong it was assumed that the dmk capacltms were so large that capacity constraints could be ig- nored Consideration of the effect of such constraints, assuming equal-razed data records and equal- sized dmks, leads to the problem of partitioning numbers (which represent access probabihtles) into m groups of at most k numbers each A practical method for partltmmng is shown to ymld a cost no more than } of the minimal cost without the constraint on group size (Cases are constructed that approach this limit asymptotically ) Therefore, within the context of the model, increasing the disk capacity (keeping the number of arms fixed) and arbitrarily changing the partitmn cannot reduce the arm contention cost below 75 percent of that achieved on the existing system with the suggested partition The result also shows that the proposed partition has a cost for the constrained problem at most of the minimal cost for the constrained problem However, the exact worst-case performance is not yet known except in the case when the group size is 2. In that case, the proposed partition is actually optimal\"",
        "Document: \"Single-layer global routing. We introduce the single-layer global routing problem (SLGRP), also called homotopic routing or rubber-band-equivalent routing, and propose a technique for solving it. Given a set of nets, the proposed technique first determines the routing sequence based on the estimated congestion, the bounding-box length and priority. Then, it finds a routing path, being a sequence of tiles, for each net (one net at a time), avoiding \u201ccongested\u201d areas. The overall goal of the algorithm is to maximize the number of routed nets. The proposed global router is the first true single-layer global router ever reported in the literature. The size of tiles, w\u00d7w, is an input parameter in our algorithm. For w=1, the proposed global router serves as an effective detailed router. An optimal postprocessing algorithm, minimizing wire length and number of bends, under homotopic transformation, is presented. The technique has been implemented and tried out for randomly generated data. The algorithm is very efficient and produces good results\"",
        "Document: \"A Simulated Annealing and Resampling Method for Training Perceptrons to Classify Gene-Expression Data. We investigate the use of perceptrons for classification of microarray data. Small round blue cell tumours of childhood are difficult to classify both clinically and via routine histology. Khan et al. [10] showed that a system of artificial neural networks can utilize gene expression measurements from microarrays and classify these tumours into four different categories. We used a simulated annealing-based method in learning a system of perceptrons, each obtained by resampling of the training set. Our results are comparable to those of Khan et al., indicating that there is a role for perceptrons in the classification of tumours based on gene expression data. We also show that it is critical to perform feature selection in this type of models.\"",
        "Document: \"A Polynomial-Time Algorithm for the Knapsack Problem with Two Variables. The general knapsack problem is known to be NP-complete. In this paper a very special knapsack problem ia studied, namely, one with only two variables. A polynomial-time algorithm is presented and analyzed. However, it remains an open problem that for any fixed n 2, the knapsack problem with n variables can be solved in polynomial time.\"",
        "Document: \"Finding intersection of rectangles by range search. Three related rectangle intersection problems in k-dimensional space are considered: (1) find the intersections of a rectangle with a given set of rectangles, (2) find the intersecting pairs of rectangles as they are inserted into or deleted from an existing set of rectangles, and (3) find the intersecting pairs of a given set of rectangles. By transforming these problems into range search problems, one need not divide the intersection problem into two subproblems, namely, the edge-intersecting problem and the containment problem, as done by many previous studies. Furthermore, this approach can also solve these subproblems separately, if required. For the first problem the running time is O((log n)2k\u22121 + s), where s is the number of intersecting pairs of rectangles. For the second problem the time needed to generate and maintain n rectangles is O(n(log n)2k) and the time for each query is O((log n)2k\u22121 + s). For the third problem the total time is O(n log n + n(log n)2(k\u22121) + s) for k \u2265 1.\"",
        "Document: \"Approximate algorithms for some generalized knapsack problems. In this paper we construct approximate algorithms for the following problems: integer multiple-choice knapsack problem, binary multiple-choice knapsack problem and multi-dimensional knapsack problem. The main result can be described as follows: for every \u03b5 0 one can construct a polynomial-time algorithm for each of the above problems such that the ratio of the value of the objective function by this algorithm and the optimal value is bounded below by 1 - \u03b5.\"",
        "Document: \"Convergence Analysis of Simulated Annealing-Based Algorithms Solving Flow Shop Scheduling Problems. In the paper, we apply logarithmic cooling schedules of inhomogeneous Markov chains to the flow shop scheduling problem with the objective to minimize the makespan. In our detailed convergence analysis, we prove a lower bound of the number of steps which are sufficient to approach an optimum solution with a certain probability. The result is related to the maximum escape depth \u0393 from local minima of the underlying energy landscape. The number of steps k which are required to approach with probability 1 - \u03b4 the minimum value of the makespan is lower bounded by nO(\u0393) \u010b logO(1)(1/\u03b4). The auxiliary computations are of polynomial complexity. Since the model cannot be approximated arbitrarily closely in the general case (unless P = NP), the approach might be used to obtain approximation algorithms that work well for the average case.\"",
        "1 is \"Exploiting the hierarchical structure for link analysis\", 2 is \"A stochastic model to predict the routability of field-programmable gate arrays\"",
        "Given above information, for an author who has written the paper with the title \"Pad minimization for planar routing of multiple power nets\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004384": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A parallel computational model for integrated speech and natural language understanding':",
        "Document: \"Enhancing Wlan Performance With Rate Adaptation Considering Hidden Node Effect. In IEEE 802.11 WLANs, many rate adaptation studies have proposed rate adaptation schemes to differentiate channel-related loss and collision loss in order to enhance WLAN performance. Most of these studies focus on rate adaptation schemes on the client side when many clients transmit uplink traffic to an access point (AP). However, considering the high proportion of downlink traffic in WLANs, rate adaptation on the AP side has a greater impact on WLAN performance and is more influenced by the downlink traffic on neighboring APs. In addition, considering that many APs are deployed in the real world, some APs use the same channel and are hidden from each other. In this paper, we first analyze the impact of hidden nodes on the rate adaptation scheme. We then propose a new rate adaptation scheme, called Hidden node Effect aware Rate Adaptation (HERA). HERA optimizes the RTS exchange by utilizing RTS transmission success/failure and makes rate decrease decisions based on frame error rate (FER) in order to enhance WLAN performance. We evaluate the performance of HERA through extensive simulation, comparing it with other well-known rate adaptation schemes. Simulation results show that HERA outperforms other rate adaptation schemes by up to 161% in terms of downlink throughput in hidden node environments.\"",
        "Document: \"Implementation and performance study of a hardware-VIA-based network adapter on gigabit ethernet. This paper presents the implementation and performance of a hardware-VIA-based network adapter on Gigabit Ethernet. VIA is a useer-level communication interface for high performance PC clustering. The network adapter is a 64-bit/66 MHz PCI plug-in card containing an FPGA for the VIA Protocol Engine and a Gigabit Ethernet chip to construct a high performance system area network. The network adapter performs virtual-to-physical address translation, doorbell, RDMA write, and send/receive completion operations in hardware without kernel intervention. In particular, the Address Translation Table (ATT) is stored on the local memory of the network adapter, and the VIA Protocol Engine efficiently controls the address translation process by directly accessing the ATT. In addition, Address Prefetch Buffer is used to reduce the time of address translation process in the receiver. As a result, the communication overhead during send/receive transactions is greatly reduced. Our experimental results show a minimum latency of 8.2 \u00b5s, and a maximum bandwidth of 112.1 MB/s. In terms of minimum latency, the hardware-VIA-based network adapter performs 2.8 times and 3.3 times faster than M-VIA, which is a software implementation of VIA, and TCP/IP, respectively, over Gigabit Ethernet. In addition, the maximum bandwidth of the hardware-VIA-based network adapter is 24% and 55% higher than M-VIA and TCP/IP, respectively. These results show that the performance of HVIA-GE is far better than that of ServerNet II, which is a hardware version of VIA developed by Tandem/Compaq.\"",
        "Document: \"HWMP+: An Improved Traffic Load Sheme for Wireless Mesh Networks. Wireless Mesh Networks (WMNs) are broadly used recently, due to its flexibility, reliability and robust services. IEEE 802.11s defines Hybrid Wireless Mesh Protocol (HWMP) as the default routing protocol for WMNs and airtime link metric as the default path selection metric. Airtime metric can collect the channel information from both PHY and MAC layers to estimate average latency per packet transmission through a link, but it typically suffers from several shortcomings, such as ignoring the influence of traffic flow and inefficient usage of the shared wireless resources. In this paper, we propose a modified HWMP protocol with an enhanced airtime metric called as HWMP+, which can estimate the link quality in combination with traffic flow information and allocate the network resources in an efficient way. The proposed metric computation procedure has both historical and real-time perspectives of the link quality and is more sensitive to the link quality variations, especially in a dynamic topology. We also improve the routing scheme and optimally manage the shared channel resources according to the traffic flow information. Simulation results using Qualnet 5.0 show that, the proposed schemes can considerably improve the system performance, in terms of network throughput and average end-to-end delay.\"",
        "Document: \"A 32-Gb MLC NAND Flash Memory With Vth Endurance Enhancing Schemes in 32 nm CMOS. Novel program and read schemes are presented to break barriers in scaling of NAND flash memory such as threshold voltage endurance from floating gate interference, and charge loss tolerance. To enhance threshold voltage endurance and charge loss tolerance, we introduced three schemes; MSB Re-PGM scheme, Moving Read scheme and Adaptive Code Selection scheme. Using the MSB Re-PGM scheme, threshold voltage distribution width is improved about 200 mV. The PGM throughput is enhanced from 1500 \u03bcs to 1250 \u03bcs. With the Moving Read scheme about half order of UBER is improved with 10 bit ECC. Also, Adaptive Code Selection scheme are used to decrease a current consumption. There is 5.5% current reduction. With these techniques, 32-Gb MLC NAND flash memory has been fabricated using a 32 nm CMOS process technology. Its program throughput reaches 13.0 MB/s at a multi-plane program operation with cache operation keeping a desirable threshold voltage distribution.\"",
        "Document: \"Comparison of End-to-End Loss Differentiation Algorithms in Multi-hop Wireless Networks. When TCP operates in wireless networks, it suffers from severe performance degradation. This is because TCP reacts to wireless packet losses by unnecessarily slowing down its sending rate. Although several loss differentiation schemes are proposed to avoid such performance reduction, their accuracies were not evaluated in multi-hop wireless networks because these schemes are designed for only the last-hop wireless networks. In this paper, not only we observe how the accuracies of these schemes vary in multi-hop wireless networks, but also we suggest a new end-to-end loss differentiation scheme. Our scheme estimates the rate of queue usage using information available to TCP. If the estimated queue usage is larger than 50% when a packet is lost, our scheme diagnoses the packet loss as congestion losses. Otherwise, it diagnoses the packet loss as wireless losses. Through extensive simulations, we show that the accuracies of previous schemes tend to decrease as the number of hops increases, and also show that our scheme has the highest accuracy in multi-hop wireless networks.\"",
        "Document: \"Improving Load Balance and Fault Tolerance for PC Cluster-Based Parallel Information Retrieval. \n Information service providers and companies have typically been using expensive mid-range or mainframe computers when they\n need a high performance information retrieval system for massive data sources such as the Internet. In recent years, companies\n have begun considering the PC cluster system as an alternative solution because of its cost-effectiveness as well as its high\n scalability. However, if some of the cluster nodes break down, users may have to wait for a long time or even may not be able\n to get any result in the worst case. This paper presents a duplicated data declustering method for PC cluster-based parallel\n information retrieval in order to achieve fault tolerance and to improve load balance in an efficient manner at low cost.\n The effectiveness of our method has been confirmed by experiments with a corpus of two million newspaper articles on an 8-node\n PC cluster.\n \n \"",
        "Document: \"An Efficient Tag Collection Algorithm Utilizing Empty Time Slots In Active Rfid Systems. We propose an efficient tag collection algorithm utilizing empty time slots in active RFID systems. In the proposed tag collection algorithm, the reader recognizes the existence of empty time slots via carrier sensing, and utilizes the redundant empty time slots to transmit sleep commands to the tags collected, resulting in performance improvement for tag collection. The simulation results show that the proposed tag collection algorithm can reduce the average tag collection time by 12.28%, 12.30%, and 13.31%, for the framed slotted aloha with the fixed 128 time slots and 256 time slots, and the dynamic framed slotted aloha anticollision protocols, respectively.\"",
        "Document: \"An Efficient Load Balancing Scheme For Multi-Gateways In Wireless Mesh Networks. In Wireless Mesh Networks (WMNs), we usually deploy multiple Internet Gateways (IGWs) to improve the capacity of WMNs. As most of the traffic is oriented towards the Internet and may not be distributed evenly among different IGWs, some IGWs may suffer from bottleneck problem. To solve the IGW bottleneck problem, we propose an efficient scheme to balance the load among different IGWs within a WMN. Our proposed load-balancing scheme consists of two parts: a traffic load calculation module and a traffic load migration algorithm. The IGW can judge whether the congestion has occurred or will occur by using a linear smoothing forecasting method. When the IGW detects that the congestion has occurred or will occur, it will firstly select another available IGW that has the lightest traffic load as the secondary IGW and then inform some mesh routers (MPs) which have been selected by using the Knapsack Algorithm to change to the secondary IGW. The MPs can return to their primary IGW by using a regression algorithm. Our Qualnet 5.0 experiment results show that our proposed scheme gives up to 18% end-to-end delay improvement compared with the existing schemes.\"",
        "Document: \"An M-VIA-Based channel bonding mechanism on gigabit ethernet. This paper proposes an M-VIA-based channel bonding mechanism on Gigabit Ethernet to improve network bandwidths of cluster systems. M-VIA is a software implementation of a user-level communication protocol that replaces the time-consuming TCP/IP protocol in cluster systems. Channel bonding techniques manage multiple network cards as a virtual single network card and expand network bandwidth by sending data concurrently through the multiple network cards and their associated networks. According to experiments with two Gigabit Ethernet adapters, the M-VIA-based channel bonding mechanism on Gigabit Ethernet showed a bandwidth improvement of 29% over an M-VIA on a single channel.\"",
        "Document: \"Seamless Handoff and Performance Anomaly Reduction Schemes Based on OpenFlow Access Points. The enterprise WLANs that are composed of multiple access points (APs) and AP controller, provide wireless backbone networks in the building and campus. In this paper, we propose the Open Flow AP system that is a software-defined enterprise WLAN system based on the Open Flow-based AP and the Open Flow controller. Also, we propose the seamless handoff and performance anomaly reduction schemes based on the Open Flow AP system in order to improve total throughput of the networks while user experience is guaranteed. We deployed the Open Flow AP system in the lab building and performed several experiments to evaluate the efficiency of the proposed system and applications. When the proposed system is compared with the existing WLANs, it performs handoff seamlessly without reassociation and increases total throughput of the network by 26.7% when client is located in overlapped service area.\"",
        "1 is \"Performance comparison of LAM/MPI, MPICH, and MVICH on a linux cluster connected by a gigabit ethernet network\", 2 is \"Bio-Inspired Imprecise Computational Blocks for Efficient VLSI Implementation of Soft-Computing Applications\"",
        "Given above information, for an author who has written the paper with the title \"A parallel computational model for integrated speech and natural language understanding\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004415": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Collective communication on architectures that support simultaneous communication over multiple links':",
        "Document: \"A case study in mechanically deriving dense linear algebra code. Design by Transformation (DxT) is a top-down approach to mechanically derive high-performance algorithms for dense linear algebra. We use DxT to derive the implementation of a representative matrix operation, two- sided Trmm. We start with a knowledge base of transformations that were encoded for a simpler set of operations, the level-3 BLAS, and add only a few transformations to accommodate the more complex two- sided Trmm. These additions explode the search space of our prototype system, DxTer, requiring the novel techniques defined in this paper to eliminate large segments of the search space that contain suboptimal algorithms. Performance results for the mechanically optimized implementations on 8192 cores of a BlueGene/P architecture are given.\"",
        "Document: \"Exploiting Symmetry in Tensors for High Performance: Multiplication with Symmetric Tensors. Symmetric tensor operations arise in a wide variety of computations. However, the benefits of exploiting symmetry in order to reduce storage and computation is in conflict with a desire to simplify memory access patterns. In this paper, we propose a blocked data structure (blocked compact symmetric storage) wherein we consider the tensor by blocks and store only the unique blocks of a symmetric tensor. We propose an algorithm by blocks, already shown of benefit for matrix computations, that exploits this storage format by utilizing a series of temporary tensors to avoid redundant computation. Further, partial symmetry within temporaries is exploited to further avoid redundant storage and redundant computation. A detailed analysis shows that, relative to storing and computing with tensors without taking advantage of symmetry and partial symmetry, storage requirements are reduced by a factor of O(m!) and computational requirements by a factor of O((m + 1)!/2(m)), where m is the order of the tensor. However, as the analysis shows, care must be taken in choosing the correct block size to ensure these storage and computational benefits are achieved (particularly for low-order tensors). An implementation demonstrates that storage is greatly reduced and the complexity introduced by storing and computing with tensors by blocks is manageable. Preliminary results demonstrate that computational time is also reduced. The paper concludes with a discussion of how insights in this paper point to opportunities for generalizing recent advances in the domain of linear algebra libraries to the field of multilinear computation.\"",
        "Document: \"Two dimensional basic linear algebra communication subprograms.  this paper, we describe extensions to a proposed set of linear algebra communicationroutines for communicating and manipulating data structures that are distributed among thememories of a distributed memory MIMD computer. In particular, recent experience shows thathigher performance can be attained on such architectures when parallel dense matrix algorithmsutilize a data distribution that views the computational nodes as a logical two dimensional mesh.The motivation for the BLACS continues ... \"",
        "Document: \"Solving \u201clarge\u201d dense matrix problems on multi-core processors. Few realize that for large matrices dense matrix computations achieve nearly the same performance when the matrices are stored on disk as when they are stored in a very large main memory. Similarly, few realize that, given the right programming abstractions, coding Out-of-Core (OOC) implementations of dense linear algebra operations (where data resides on disk and has to be explicitly moved in and out of main memory) is no more difficult than programming high-performance implementations for the case where the matrix is in memory. Finally, few realize that on a contemporary eight core architecture one can solve a 100,000 times 100,000 dense symmetric positive definite linear system in about an hour. Thus, for problems that used to be considered large, it is not necessary to utilize distributed-memory architectures with massive memories if one is willing to wait longer for the solution to be computed on a fast multithreaded architecture like an SMP or multi-core computer. This paper provides evidence in support of these claims.\"",
        "Document: \"Parallelizing the QR algorithm for the unsymmetric algebraic eigenvalue problem: myths and reality. Over the last few years, it has been suggested that the popular QR algorithm for the unsymmetric Schur decomposition does not parallelize. In this paper, we present both positive and negative results on this subject. In theory, asymptotically perfect speedup can be obtained. In practice, reasonable speedup can be obtained on an MIMD distributed memory computer for a relatively small number of processors. However, we also show theoretically that it is impossible for the standard QR algorithm to be scalable. performance of a parallel implementation of the LAPACK DLAHQR routine on the Intel Paragon(TM) system is reported.\"",
        "Document: \"Solving dense linear systems on platforms with multiple hardware accelerators. In a previous PPoPP paper we showed how the FLAME methodology, combined with the SuperMatrix runtime system, yields a simple yet powerful solution for programming dense linear algebra operations on multicore platforms. In this paper we provide further evidence that this approach solves the programmability problem for this domain by targeting a more complex architecture, composed of a multicore processor and multiple hardware accelerators (GPUs, Cell B.E., etc.), each with its own local memory, resulting in a platform more reminiscent of a heterogeneous distributed-memory system. In particular, we show that the FLAME programming model accommodates this new situation effortlessly so that no significant change needs to be made to the codebase. All complexity is hidden inside the SuperMatrix runtime scheduling mechanism, which incorporates software implementations of standard cache/memory coherence techniques in computer architecture to improve the performance. Our experimental evaluation on a Intel Xeon 8-core host linked to an NVIDIA Tesla S870 platform with four GPUs delivers peak performances around 550 and 450 (single-precision) GFLOPS for the matrix-matrix product and the Cholesky factorization, respectively, which we believe to be the best performance numbers posted on this new architecture for such operations.\"",
        "Document: \"Retargeting PLAPACK to clusters with hardware accelerators. Hardware accelerators are becoming a highly appealing approach to boost the raw performance as well as the price-performance and power-performance ratios of current clusters. In this paper we present a strategy to retarget PLAPACK, a library initially designed for clusters of nodes equipped with general- purpose processors and a single address space per node, to clusters equipped with graphics processors (GPUs). In our approach data are kept in the device memory and only retrieved to main memory when they have to be communicated to a dierent node. Here we benet from the object-based orientation of PLAPACK which allows all communication between host and device to be embedded within a pair of routines, providing a clean abstraction that enables an ecient and direct port of all the contents of the library. Our experiments in a cluster consisting of 16 nodes with two NVIDIA Quadro FX5800 GPUs each show the performance of our approach.\"",
        "Document: \"Out-of-Core Computation of the QR Factorization on Multi-core Processors. We target the development of high-performance algorithms for dense matrix operations where data resides on disk and has to be explicitly moved in and out of the main memory. We provide strong evidence that, even for a complex operation like the QR factorization, the use of a run-time system creates a separation of concerns between the matrix computations and I/O operations with the result that no significant changes need to be introduced to existing in-core algorithms. The library developer can thus focus on the design of algorithms-by-blocks, addressing disk memory as just another level of the memory hierarchy. Experimental results for the out-of-core computation of the QR factorization on a multi-core processor reveal the potential of this approach.\"",
        "Document: \"Improving the performance of reduction to Hessenberg form. In this article, a modification of the blocked algorithm for reduction to Hessenberg form is presented that improves performance by shifting more computation from less efficient matrix-vector operations to highly efficient matrix-matrix operations. Significant performance improvements are reported relative to the performance achieved by the current LAPACK implementation.\"",
        "Document: \"Parallel out-of-core computation and updating of the QR factorization. This article discusses the high-performance parallel implementation of the computation and updating of QR factorizations of dense matrices, including problems large enough to require out-of-core computation, where the matrix is stored on disk. The algorithms presented here are scalable both in problem size and as the number of processors increases. Implementation using the Parallel Linear Algebra Package (PLAPACK) and the Parallel Out-of-Core Linear Algebra Package (POOCLAPACK) is discussed. The methods are shown to attain excellent performance, in some cases attaining roughly 80&percent; of the \u201crealizable\u201d peak of the architectures on which the experiments were performed.\"",
        "1 is \"Synthesis of an Optimal Family of Matrix Multiplication Algorithms on Linear Arrays\", 2 is \"Applying MPI derived datatypes to the NAS benchmarks: A case study\"",
        "Given above information, for an author who has written the paper with the title \"Collective communication on architectures that support simultaneous communication over multiple links\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004418": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Real World Video Avatar: Transmission and Presentation of Human Figure':",
        "Document: \"Dexterous object manipulation based on collision response. We propose an approach to object manipulation. In the approach, the user's hand or fingers are represented by a set of interface points, the interaction force on each interface point is computed by the collision response computation algorithm, and the motion of the object is simulated based on the interaction force. We employed a fast collision response computation method to accommodate a large number of interaction points. Also, we devised a method for stable computation of object motion. We carried out experiments on interaction with models of varying complexity and demonstrated the feasibility of our approach for dexterous manipulations.\"",
        "Document: \"Five senses theatre project: Sharing experiences through bodily ultra-reality. The Five Senses Theatre project was established for the development of a basic technology that enables the user to relive a spatial motion of other persons as if the user him/her-self experienced it in person. This technology aims to duplicate a bodily experience performed in the real space and to pass it to the other person. In other words, the user experiences another person's body that moved in a real space as if the user moved in that real space. The spatial motion may be a walking tour to the world heritage, a legend run of a top athlete, and etc. More specifically, the system creates the sensation of a self-body motion without a voluntary motion of the user by providing physical motion to the real body. The sensation of self-body motion is generated by not only a visually induced vection but proprioceptive and tactile sensations of the body passively evoked. The Five Senses Theatre provides multisensory stimuli to the user to make use of the user's body as medium to project the valuable experience from others to the user's cognition. The research issues include the following: 1) Cognitive mechanism of passive stimulation perceived as an active motion sensation (pseudo agency), and body ownership transfer (virtual body). 2) Device development of the mutisensory display system, rendering/projection algorithms, and a lifelog data system.\"",
        "Document: \"Walking experience by real-scene optic flow with synchronized vibrations on feet. We developed a walking recording and experiencing system. For the recording we captured stereo motion images from two cameras attached to a person's forehead with synchronized data of ankles' accelerations. For the experiencing we presented 3-D motion images with binocular disparity on a head-mounted display and vibrations to user's feet. The vibration was made from a sound of shoes when a person walked. We found that users subjectively reported the 3-D motion images with synchronized foot vibrations elicited stronger feelings of walking, leg motion, footsteps, and tele-existence than without vibrations in Experiment 1. In Experiment 2, participants' self-localization drifted in the direction of virtual walking after experiencing other walker's visual sight with the synchronized foot vibrations. These results suggest that our walking experiencing system gave users somewhat active walking feelings.\"",
        "Document: \"A method for transformation of 3D space into Ukiyo-e composition. In 1739, Western perspective drawing reached Japan via China. Before then, Japanese drawing, known as Yamato-e, had depicted architectural space through parallel projection. This was true for the ukiyo-e compositions that were popular among the general public during the Edo Era as well (see Figure 1.a). For some time after perspective drawing reached Japan in 1739, ukiyo-e artists created ukiyo-e compositions that incorporated perspective drawing called uki-e (see Figure 1.b). However, this movement was short lived, and after 1800, ukiyo-e artists created compositions using their own type of structure that did not conform to perspective drawing. [Kuroda 17] [Oka 92] [Kishi 94] [Yokochi 95] Works from this time by artists such as Katsushika Hokusai and Utagawa Hiroshige also became influential in the West through the Japonism movement of the 1860s. Figure 2 typifies the style of ukiyo-e composition from the 1800s.\"",
        "Document: \"Cluster overlap distribution map: visualization for gene expression analysis using immersive projection technology. In this paper, we discuss possible applications of virtual reality technologies, such as immersive projection technology (IPT), in the field of genome science, and propose cluster-oriented visualization that attaches importance to data separation of large gene data sets with multiple variables. Based on these strategies, we developed the cluster overlap distribution map (CDCM), which is a visualization methodology using IPT for pairwise comparison between cluster sets generated from different gene expression data sets. This methodology effectively provides the user with indications of gene clusters that are worth a close examination. In addition, by using the plate window manager system, which enables the user to manipulate existing 2D GUI applications in the virtual 3D space, we developed the virtual environment for the comprehensive analysis from providing the indications to further examination by referring to the database on Web sites. Our system was applied in the comparison between the gene expression data sets of hepatocellular carcinomas and hepatoblastomas, and the effectiveness of the system was confirmed.\"",
        "Document: \"Inertial force display to represent content inside the box. By shaking a box, we can estimate content inside. Relationship between force that is applied to the box and resulting motion of the box is considered to be a clue to the estimation. In this paper, we implement 4 physical models consist of a box and contents, and evaluate users' discrimination ability between different models and parameter of each models.\"",
        "Document: \"Position tracking using infra-red signals for museum guiding system. In this paper, an indoor positioning system has been presented. The proposed system is set up by using infra-red transmitters. The developed system will be utilized as the museum guiding system for the next-generation at the National Science Museum of Japan in this summer. Plenty of research works have been performed for the development of positioning systems or mobile devices of the museum guiding systems. However, user-friendly and more flexible guiding systems are still required for both exhibitors and visitors to the museums. The developed positioning system is a simple system with inexpensive components. In addition, the museum guiding devices should be simple and portable especially for the visitors. Each infra-red(IR) transmitter, which is set on the ceiling of the exhibition hall, transmits its own signal for the identification of the coordinate value of the hall area. Unlike the common IR beacon usage as a part of the museum guiding system, all the IR transmitters are set to have overlap areas for the precise positioning of the visitors with limited number of transmitters.\"",
        "Document: \"CAGRA: Optimal parameter setting and rendering method for occlusion-capable automultiscopic three-dimensional display. We have developed a novel automultiscopic display, CAGRA (Computer-Aided Graphical Real-world Avatar), which can provide full parallax both horizontally and vertically without imposing any other additional equipment such as goggles on users. CAGRA adopts two axes of rotation so that it can distribute reflected light all around. In our previous work, it was proved that the display can present a black and white image of a 3D object at 1 Hz with both horizontal and vertical parallax. Here, we discuss two important problems that remain unsolved in our previous work: optimal parameter setting and the rendering method. As for finding optimal parameters, the relationship among the rotation speed of two axes and the diffusion angle of the holographic diffuser is discussed. The rendering method was implemented so that accurate view is presented in spite of the unique mechanical structure of CAGRA. Furthermore, the synchronization mechanism between projection of images and rotation of the mirror, which was also a remaining problem, is also implemented. Findings obtained through solving these problems clarify the characteristics of this novel display system, CAGRA, and lead to its further development.\"",
        "Document: \"Topographic Surface Perception Modulated By Pitch Rotation Of Motion Chair. The paper investigates multimodal perception of a topographic surface induced by visual and vestibular stimuli. Using an experimental system consisting of a motion chair and optic flow on a wide screen, we conducted a user study to assess how congruence or incongruence of visual and vestibular shape cues influence the perception of a topographic surface. Experimental results show that the vestibular shape cue contributed to making the shape perception larger than the visual one. Finally, the results of a linear regression analysis showed that performance with visual unimodal and vestibular unimodal cues could account for that with visuo-vestibular multimodal cues.\"",
        "Document: \"Spatial constraint method: a new approach to real-time haptic interaction in virtual environments. In this paper, we propose an approach to real-time haptic interaction based on the concept of simulating the constraining propertes of space. Research on haptic interaction has been conducted from the points of view of both surface and volume rendering. Most approaches to surface rendering--such as the constraint-based god-object method, the point-based approach, and the virtual proxy approach-- have dealt only with the interaction with an object surface. Whereas, in volume rendering approaches, algorithms for representing volume data through interactions in space have been investigated. Our approach provides a framework for the representation of haptic interaction with both surface and space. We discretize the space using a tetrahedral cell mesh and associate a constraining property with each cell. The interaction of the haptic interface points with a volume is simulated using the constraining properties of the cells occupied by this volume. We implemented a fast computation algorithm that works at a haptic rate. The algorithm is robust in that any sudden or quick motion of the user does not disturb the computation, and the computation time for each cycle is independent of the complexity of the model as a whole. To demonstrate the performance of the proposed method, we present experimental results on the interaction with models of varying complexity. Also, we discuss some problems that need to be solved in future work.\"",
        "1 is \"TeleHuman: effects of 3d perspective on gaze and pose estimation with a life-size cylindrical telepresence pod\", 2 is \"Cellular texture generation\"",
        "Given above information, for an author who has written the paper with the title \"Real World Video Avatar: Transmission and Presentation of Human Figure\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004434": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A Fully Consistent Hidden Semi-Markov Model-Based Speech Recognition System':",
        "Document: \"A New Robust Two Dimensional Spectral Estimation Based on an AR Model Excited by a t Distribution Process and its QR-Decomposition Recursive Algorithm. In this paper a novel robust two dimensional spectral estimation based on an AR model is proposed. The robustness of the method is obtained by assuming that the output or the residual signals are independently and identically distributed (IID) t-processes with small alpha degrees of freedom. By doing so, the effect of large amplitude residuals is reduced. To reduce the calculation burden, the optimal solution is recursively calculated by incorporating the QR-decomposition method. The optimal solution is updated each time the number of input samples grows. Simulation results show that when the excitation is Gaussian, the obtained estimate by using large and small alpha are comparable. On the other hand, when the excitation is impulsive, the obtained estimate after 100 by 100 pixels iterations by using small alpha; i.e. alpha = 3; is more accurate than that obtained by using alpha = infinity which is applied in the conventional least square approach. The plots of the mean square error (MSE) show that by using small alpha we can achieve smaller MSE's than that by using large alpha.\"",
        "Document: \"Estimating Trajectory Hmm Parameters Using Monte Carlo Em With Gibbs Sampler. In the present paper, the Monte Carlo EM (MCEM) al- gorithm with a Gibbs sampler is applied for estimating pa- rameters of a trajectory HMM, which has been derived from an HMM by imposing explicit relationships between static and dynamic features. The trajectory HMM can alleviate two limitations of the HMM, which are i) constant statistics within a state, and ii) conditional independence of state out- put probabilities, without increasing the number of model parameters. Ina speaker-dependent continuous speechrecog- nition experiment, trajectory HMMsestimated by the MCEM algorithm achieved significant improvements over the cor- responding HMMs trained by the EM (Baum-Welch) algo- rithm.\"",
        "Document: \"Gmm-Based Missing-Feature Reconstruction On Multi-Frame Windows. Methods for missing-feature reconstruction substitute noise-corrupted features with clean-speech estimates calculated based on reliable information found in the noisy speech signal. Gaussian mixture model (GMM) based reconstruction has conventionally focussed on reliable information present in a single frame. In this work, GMM-based reconstruction is applied on windows that span several time frames. Mixtures of factor analysers (MFA) are used to limit the number of model parameters needed to describe the feature distribution as window width increases. Using the window-based MFA in noisy speech recognition task resulted in relative error reductions up to 52 % compared to frame-based GMM.\"",
        "Document: \"Celp Coding System Based On Mel-Generalized Cepstral Analysis. This paper presents a CELP speech coding system based on mel-generalized cepstral analysis. In the mel-generalized cepstral analysis, we can vary the modal spectrum continuously from AR to cepstral modeling by changing the value of a parameter gamma and we can choose an appropriate modal spectrum. Furthermore, the spectrum represented by mel-generalized cepstrum has frequency resolution similar to that of human ear. Since the perceptual weighting and postfiltering are carried out through the mel-generalized cepstrum, we expect the perceptual performance of the proposed coder to be improved. The subjective performance test indicates that the quality of the proposed CELP coder is about 2 dB higher than that of the conventional one.\"",
        "Document: \"Minimum Classification Error Interactive Training For Speaker Identification. This paper describes an online discriminative training algorithm aiming at achieving speaker identification on interactive robots. A robot incrementally acquires speakers' voice characteristics during the interaction with the speakers. We simulate the situation that the speakers never give their IDs and the robot can only know whether the identification decision was correct or not from the speaker's positive or negative behavioral reaction. The speaker models are adjusted based on this limited information using minimum classification error (MCE) training consisting of positive and negative adaptation. In cases of correct identification, the conventional NICE training algorithm can be used. We compare three kinds of negative adaptation algorithms for the cases of incorrect identification. Experimental results show that the combination of the positive and negative adaptation achieves faster convergence, and negative adaptation which adjusts only a misclassified speaker model reaches an identification rate of 80% four times faster than the positive adaptation alone.\"",
        "Document: \"Parameter sharing and minimum classification error training of mixtures of factor analyzers for speaker identification. This paper investigates the parameter tying strategies of mix- tures of factor analyzers (MFA) and discriminative training of MFA for speaker identification. The parameters of factor load- ing matrices or diagonal matrices are shared in different mix- tures of MFA. The minimum classification error (MCE) training is applied to the MFA parameters to enhance the discrimination abilities. The results of text-independent speaker identification experiments show that MFA outperforms the conventional Gaus- sian mixture models (GMMs) with diagonal or full covariance matrices and achieve the best performance when sharing the diag- onal matrices, resulting in a relative gain of 26% over the GMM with diagonal covariance matrices. The recognition performance is further improved by the MCE training with an additional 3% error reduction.\"",
        "Document: \"Continuous Stochastic Feature Mapping Based on Trajectory HMMs. This paper proposes a technique of continuous stochastic feature mapping based on trajectory hidden Markov models (HMMs), which have been derived from HMMs by imposing explicit relationships between static and dynamic features. Although Gaussian mixture model (GMM)- or HMM-based feature-mapping techniques work effectively, their accuracy occasionally degrades due to inappropriate dynamic characteristics caused by frame-by-frame mapping. While the use of dynamic-feature constraints at the mapping stage can alleviate this problem, it also introduces inconsistencies between training and mapping. The technique we propose can eliminate these inconsistencies while retaining the benefits of using dynamic-feature constraints, and it offers entire sequence-level transformation rather than frame-by-frame mapping. The results obtained from speaker-conversion, acoustic-to-articulatory inversion-mapping, and noise-compensation experiments demonstrated that our new approach outperformed the conventional one.\"",
        "Document: \"Constructing emotional speech synthesizers with limited speech database. This paper describes an emotional speech synthesis system based on HMMs and related modeling techniques. For con- catenative speech synthesis, we require all of the concatena- tion units that will be used to be recorded beforehand and made available at synthesis time. To adopt this approach for synthe- sizing the wide variety of human emotions possible in speech, implies that this process should be repeated for every targeted emotion making this task challenging and time consuming. In this paper, we propose an emotional speech synthesis technique based on HMMs, especially for the case where only limited amount of training data is available, directly incorporating sub- jective evaluation results performed on the training data. Lis- tening results performed on the synthesized speech suggest that the proposed technique helps to improve the emotional content of synthesized speech.\"",
        "Document: \"Realizing Tibetan speech synthesis by speaker adaptive training. This paper presents a method to realize HMM-based Tibetan speech synthesis using a Mandarin speech synthesis framework. A Mandarin context-dependent label format is adopted to label Tibetan sentences. A Mandarin question set is also extended for Tibetan by adding language-specific questions. A Mandarin speech synthesis framework is utilized to train an average mixed-lingual model from a large Mandarin multi-speaker-based corpus and a small Tibetan one-speaker-based corpus using the speaker adaptive training. Then the speaker adaptation transformation is applied to the average mixed-lingual model to obtain a speaker adapted Tibetan model. Experimental results show that this method outperforms the method using speaker dependent Tibetan model when only a small amount of training Tibetan utterances are available. When the number of training Tibetan utterances is increased, the performances of the two methods tend to be the same. \u00a9 2013 APSIPA.\"",
        "Document: \"Image modeling using two dimensional exponential systems. In this paper a new image modeling is proposed. We propose the usage of the exponential (EXP) model as an alternative for the existing AR models which have stability problem. Since the EXP model is always stable, the obtained model can always be utilized to synthesize the original image. Since the EXP systems have an infinite impulse response, it is suitable to model images which contain not only poles, but also images with poles and zeroes. Therefore, it is expected that the EXP system is more suitable to model more variations of images than that of AR system. The simulation results show the the power gain (PG) of proposed EXP and the conventional AR model are comparable\"",
        "1 is \"DIPPER: Description and formalisation of an information-state update dialogue system architecture\", 2 is \"Deep mixture density networks for acoustic modeling in statistical parametric speech synthesis\"",
        "Given above information, for an author who has written the paper with the title \"A Fully Consistent Hidden Semi-Markov Model-Based Speech Recognition System\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004467": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A nonlinear entropic variational model for image filtering':",
        "Document: \"A nonlinear entropic variational model for image filtering. We propose an information-theoretic variational filter for image denoising. It is a result of minimizing a functional subject to some noise constraints, and takes a hybrid form of a negentropy variational integral for small gradient magnitudes and a total variational integral for large gradient magnitudes. The core idea behind this approach is to use geometric insight in helping to construct regularizing functionals and avoiding a subjective choice of a prior in maximum a posteriori estimation. Illustrative experimental results demonstrate a much improved performance of the approach in the presence of Gaussian and heavy-tailed noise.\"",
        "Document: \"Computing persistent features in big data: A distributed dimension reduction approach. Persistent homology has become one of the most popular tools used in topological data analysis for analyzing big data sets. In an effort to minimize the computational complexity of finding the persistent homology of a data set, we develop a simplicial collapse algorithm called the selective collapse. This algorithm works by representing the previously developed strong collapse as a forest and uses that forest data to improve the speed of both the strong collapse and of persistent homology. Finally, we demonstrate the savings in computational complexity using geometric random graphs.\"",
        "Document: \"Human activity modeling as brownian motion on shape manifold. In this paper we propose a stochastic modeling of human activity on a shape manifold. From a video sequence, human activity is extracted as a sequence of shape. Such a sequence is considered as one realization of a random process on shape manifold. Then Different activities are modeled by manifold valued random processes with different distributions. To solve the problem of stochastic modeling on a manifold, we first regress a manifold values process to a Euclidean process. The resulted process then could be modeled by linear models such as a stationary incremental process and a piecewise stationary incremental process. The mapping from manifold to Euclidean space is known as a stochastic development. The idea is to parallelly transport the tangent along curve on manifold to a single tangent space. The advantage of such technique is the one to one correspondence between the process in Euclidean space and the one on manifold. The proposed algorithm is tested on database [5] and compared with the related work in [5]. The result demonstrate the high accuracy of our modeling in characterizing different activities.\"",
        "Document: \"A stochastic flow for feature extraction. Over the years the evolution of level sets of two-dimensional functions or images in time through a partial differential equation has emerged as an important tool in image processing. Curve evolution, which may be viewed as an evolution of a single level curve, has been applied to a wide variety of problems such as smoothing of shapes, shape analysis and shape recovery. We give a stochastic interpretation of the basic curve smoothing equation, the so called geometric heat equation, and show that this evolution amounts to a rotational diffusion movement of the particles along the contour. Moreover, assuming that a priori information about the orientation of objects to be preserved is known, we present new flows which amount to weighting the geometric heat equation nonlinearly as a function of the angle of the normal to the curve at each point.\"",
        "Document: \"Activity Video Analysis via Operator-Based Local Embedding. High dimensional data sequences, such as video clips, can be modeled as trajectories in a high dimensional space, and usually exhibit a low dimensional structure intrinsic to each distinct class of data sequence [1]. In this paper, we proposed a novel geometric framework to investigate the temporal relations as well as spatial features in a video sequence. Important visual features are preserved by mapping a high dimensional video sequence to operators in a circulant operator space (image operator space). The corresponding operator sequence is subsequently embedded into a low dimensional space, in which the temporal dynamics of each sequence is well preserved. In addition, an algorithm for human activity video classification is implemented by employing Markov models in the low dimensional embedding space, and illustrating examples and classification performance are presented.\"",
        "Document: \"Stochastic differential equations and geometric flows. In previous years, curve evolution, applied to a single contour or to the level sets of an image via partial differential equations, has emerged as an important tool in image processing and computer vision. Curve evolution techniques have been utilized in problems such as image smoothing, segmentation, and shape analysis. We give a local stochastic interpretation of the basic curve smoothing equation, the so called geometric heat equation, and show that this evolution amounts to a tangential diffusion movement of the particles along the contour. Moreover, assuming that a priori information about the shapes of objects in an image is known, we present modifications of the geometric heat equation designed to preserve certain features in these shapes while removing noise. We also show how these new flows may be applied to smooth noisy curves without destroying their larger scale features, in contrast to the original geometric heat flow which tends to circularize any closed curve.\"",
        "Document: \"Distributed Localization of Coverage Holes Using Topological Persistence. We develop distributed algorithms to detect and localize coverage holes in sensor networks. We neither assume coordinate information of the nodes, neither any distances between the nodes. We use algebraic topological methods to define a coverage hole, and develop provably correct algorithm to detect a hole. We then partition the network into smaller subnetworks, while ensuring that the holes are preserved, and checking for holes in each. We show that repeating this process leads to localizing the coverage holes. We demonstrate the improved complexity of our algorithm using simulations.\"",
        "Document: \"Nonlinear diffusion: A probabilistic view. A probabilistic view of diffusion is presented. A discrete symmetric random walk is shown to be equivalent to a heat equation evolution, and an extension to nonlinear evolutions including Perona-Malik equation is shown to be of utmost importance for analysis. Upon unraveling the limitations as well as the advantages of such an equation, we are able to propose a new approach which is demonstrated to outperform existing approaches, and to lift the long-standing problem of when to stop the evolution. Substantiating examples of image enhancement and segmentation are provided.\"",
        "Document: \"Object recognition through topo-geometric shape models using error-tolerant subgraph isomorphisms. We propose a method for 3-D shape recognition based on inexact subgraph isomorphisms, by extracting topological and geometric properties of a shape in the form of a shape model, referred to as topo-geometric shape model (TGSM). In a nutshell, TGSM captures topological information through a rigid transformation invariant skeletal graph that is constructed in a Morse theoretic framework with distance function as the Morse function. Geometric information is then retained by analyzing the geometric profile as viewed through the distance function. Modeling the geometric profile through elastic yields a weighted skeletal representation, which leads to a complete shape signature. Shape recognition is carried out through inexact subgraph isomorphisms by determining a sequence of graph edit operations on model graphs to establish subgraph isomorphisms with a test graph. Test graph is recognized as a shape that yields the largest subgraph isomorphism with minimal cost of edit operations. In this paper, we propose various cost assignments for graph edit operations for error correction that takes into account any shape variations arising from noise and measurement errors.\"",
        "Document: \"Rotation invariant topology coding of 2D and 3D objects using Morse theory. In this paper, we propose a numerical algorithm for extracting the topology of a three-dimensional object (2 dimensional surface) embedded in a three-dimensional space R3. The method is based on capturing the topology of a modified Reeb graph by tracking the critical points of a distance function. As such, the approach employs Morse theory in the study of translation, rotation, and scale invariant skeletal graphs. The latter are useful in the representation and classification of objects in R3.\"",
        "1 is \"Fuzzy Markovian segmentation in application of magnetic resonance images\", 2 is \"Group Actions, Homeomorphisms, and Matching: A General Framework\"",
        "Given above information, for an author who has written the paper with the title \"A nonlinear entropic variational model for image filtering\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004476": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'On difference of two monotone operators':",
        "Document: \"General Wiener\u2013Hopf equation technique for nonexpansive mappings and general variational inequalities in Hilbert spaces. In this paper, we show the general variational inequality problems are equivalent to solving the general Wiener\u2013Hopf equations. By using the equivalence, we establish a general iterative algorithm for finding the solution of general variational inequalities, general Wiener\u2013Hopf and the fixed point of nonexpansive mappings. Our results extend and improve the recent corresponding results announced by many others.\"",
        "Document: \"Self-adaptive projection algorithms for general variational inequalities. In this paper, we consider and analyze a new class of self-adaptive projection algorithms for solving general variational inequalities by using the technique of updating the solution. We prove that the convergence of these new methods only requires the pseudomonotonicity, which is a weaker condition than monotonicity. These new methods differ from the previously known splitting methods for solving variational inequalities and related complementarity problems. Proof of convergence is very simple. As special cases, we can obtain a number of four-step forward-backward splitting methods of Noor for solving variational inequalities.\"",
        "Document: \"Iterative methods for solving extended general mixed variational inequalities. In this paper, we introduce and consider a new class of mixed variational inequalities involving four operators, which are called extended general mixed variational inequalities. Using the resolvent operator technique, we establish the equivalence between the extended general mixed variational inequalities and fixed point problems as well as resolvent equations. We use this alternative equivalent formulation to suggest and analyze some iterative methods for solving general mixed variational inequalities. We study the convergence criteria for the suggested iterative methods under suitable conditions. Our methods of proof are very simple as compared with other techniques. The results proved in this paper may be viewed as refinements and important generalizations of the previous known results.\"",
        "Document: \"On an iterative algorithm for general variational inequalities. In this paper, we suggest and analyze a new three-step iterative algorithm for solving the general variational inequalities. We also study the global convergence of the proposed method under some mild conditions. Since the general variational inequalities include variational inequalities, quasi variational inequalities and quasi complementarity problems as special cases, one can deduce similar results for these problems. Our results can be viewed as an improvement and refinement of the previously known and new results. Preliminary numerical experiments are included to illustrate the advantage and efficiency of the proposed method.\"",
        "Document: \"On the convergence rate of the over-relaxed proximal point algorithm. This paper is to illustrate that the main result of the paper [R.U. Verma, Generalized over-relaxed proximal algorithm based on A-maximal monotonicity framework and applications to inclusion problems, Mathematical and Computer Modelling 49 (2009) 1587\u20131594] is incorrect. The convergence rate of the over-relaxed proximal point algorithm should be greater than 1. Moreover, the strong convergence and the unique solution may not be proved accordingly in the paper by Verma.\"",
        "Document: \"Splitting Algorithms for General Pseudomonotone Mixed Variational Inequalities. In this paper, we suggest and analyze a number of resolvent-splitting algorithms for solving general mixed variational inequalities by using the updating technique of the solution. The convergence of these new methods requires either monotonicity or pseudomonotonicity of the operator. Proof of convergence is very simple. Our new methods differ from the existing splitting methods for solving variational inequalities and complementarity problems. The new results are versatile and are easy to implement.\"",
        "Document: \"Some Iterative Methods for Solving Nonconvex Bifunction Equilibrium Variational Inequalities. We introduce and consider a new class of equilibrium problems and variational inequalities involving bifunction, which is called the nonconvex bifunction equilibrium variational inequality. We suggest and analyze some iterative methods for solving the nonconvex bifunction equilibrium variational inequalities using the auxiliary principle technique. We prove that the convergence of implicit method requires only monotonicity. Some special cases are also considered. Our proof of convergence is very simple. Results proved in this paper may stimulate further research in this dynamic field.\"",
        "Document: \"On the equivalence of the convergence criteria between modified Mann\u2013Ishikawa and multi-step iterations with errors for successively strongly pseudo-contractive operators. In this paper, the equivalence of the convergence between the modified Mann\u2013Ishikawa and multi-step Noor iterations with errors is proven for the successively strongly pseudo-contractive operators without Lipschitzian assumption. Our results generalize the recent results of the paper [B.E. Rhoades, S.M. Soltuz, The equivalence between Mann\u2013Ishikawa iterations and multi-step iteration, Nonlinear Anal. 58 (2004) 219\u2013228; B.E. Rhoades, S.M. Soltuz, The equivalence between the convergences of Ishikawa and Mann iterations for an asymptotically nonexpansive in the intermediate sense and strongly successively pseudo-contractive maps, J. Math. Anal. Appl. 289 (2004) 266\u2013278] by extending to the more generalized multi-step iterations with errors and hence improve the corresponding results of all the references in bibliography by providing the equivalences of convergence between all of these up-to-date iteration schemes.\"",
        "Document: \"Mixed quasi complementarity problems in topological vector spaces. In this paper, we introduce and consider a new class of complementarity problems, which is called the mixed quasi complementarity problems in a topological vector space. We show that the mixed quasi complementarity problems are equivalent to the mixed quasi variational inequalities. Using the KKM mapping theorem, we study the existence of a solution of the mixed quasi variational inequalities and mixed quasi complementarity problems. Several special cases are also discussed. Results obtained in this paper can be viewed as extension and generalization of the previously known results.\"",
        "Document: \"A new iterative method for variational inequalities. It is well known that the variational inequalities are equivalent to the fixed point problems. Using this equivalence, we suggest and consider a new three-step iterative method for solving variational inequalities. The new iterative method is obtained by using three steps under suitable conditions. We prove that the new method is globally convergent. Our results can be viewed as significant extensions of the previously known results for variational inequalities. Preliminary numerical experiments are included to illustrate the advantage and efficiency of the proposed method.\"",
        "1 is \"Convergence of Proximal-Like Algorithms\", 2 is \"On k-uniformly close-to-convex functions of complex order\"",
        "Given above information, for an author who has written the paper with the title \"On difference of two monotone operators\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004506": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Boundary estimation of fiber bundles derived from diffusion tensor images.':",
        "Document: \"Fast and accurate identification of fat droplets in histological images. HighlightsA new method for identifying fat droplets in histological images is presented.Adjacency statistics are utilized as shape features.Fat droplets are identified with high sensitivity and specificity.Adjacency statistics greatly improve the identification of clustered fat droplets.The method can be quickly executed on standard computers. Background and objectiveThe accurate identification of fat droplets is a prerequisite for the automatic quantification of steatosis in histological images. A major challenge in this regard is the distinction between clustered fat droplets and vessels or tissue cracks. MethodsWe present a new method for the identification of fat droplets that utilizes adjacency statistics as shape features. Adjacency statistics are simple statistics on neighbor pixels. ResultsThe method accurately identified fat droplets with sensitivity and specificity values above 90%. Compared with commonly-used shape features, adjacency statistics greatly improved the sensitivity toward clustered fat droplets by 29% and the specificity by 17%. On a standard personal computer, megapixel images were processed in less than 0.05s. ConclusionsThe presented method is simple to implement and can provide the basis for the fast and accurate quantification of steatosis.\"",
        "Document: \"MRI-based volumetry of intra- and extracerebral liquor spaces. We describe a hybrid approach to the volumetry of intracerebral and entire intracranial cerebrospinal fluid (CSF) spaces, combining two imaging techniques. Ultra-fast and reproducible quantification of intracranial liquor volumes is carried out based on T2-weighted magnetic resonance (MR) images. Isotropic T1-weighted data are used as a basis for robust semi-automatic three-dimensional segmentation procedures. Variability of the proposed method, including image acquisition, is evaluated on volunteers and phantoms to be well below 2%. The total time required for image analysis including interaction is below 5 min.\"",
        "Document: \"Breast segmentation in MRI: quantitative evaluation of three methods. A precise segmentation of breast tissue is often required for computer-aided diagnosis (CAD) of breast MRI. Only a few methods have been proposed to automatically segment breast in MRI. Authors reported satisfactory performance, but a fair comparison has not been done yet as all breast segmentation methods were evaluated on their own data sets with different manual annotations. Moreover, breast volume overlap measures, which were commonly used for evaluations, do not seem to be adequate to accurately quantify the segmentation qualities. Breast volume overlap measures are not sensitive to small errors, such as local misalignments, because the breast appears to be much larger than other structures. In this work, two atlas-based approaches and a breast segmentation method based on Hessian sheetness filter are exhaustively evaluated and benchmarked on a data set of 52 manually annotated breast MR images. Three quantitative measures including dense tissue error, pectoral muscle error and pectoral surface distance are defined to objectively reflect the practical use of breast segmentation in CAD methods. The evaluation measures provide important evidence to conclude that the three evaluated techniques perform accurate breast segmentations. More specifically, the atlas-based methods appear to be more precise, but require larger computation time than the sheetness-based breast segmentation approach.\"",
        "Document: \"GPU implementations of a relaxation scheme for image partitioning: GLSL versus CUDA. The GPU programmability opens a new perspective for algorithms that have not been studied and used for real applications on commodity state-of-the-art hardware due to their computational expenses. In this paper, we present three implementations of a partitioning algorithm for multi-channel images, which extends an original algorithm for single-channel images presented in the early 1990\u2019s. The segmentation algorithm is based on the information theory concept of minimum description length, which leads to the formulation of an energy functional. The optimal solution is obtained by minimizing the functional. The minimization approach follows a graduated non-convexity approach, which leads to a fully explicit scheme. As the scheme is applied to all pixels of the image simultaneously, it is naturally parallelizable. Besides the optimized sequential implementation in C++ we developed a GLSL version of the algorithm using vertex and fragment shaders as well as a CUDA version using global memory, shared memory, and texture memory. We compare the performance of the implementations, discuss the implementation details, and show that suitability of this algorithm for GPU allows it to become a comparable alternative to the modern partitioning algorithm (multi-label Graph-Cuts).\"",
        "Document: \"How Accurate Is Brain Volumetry? A Methodological Evaluation. We evaluate the accuracy and precision of different techniques for measuring brain volumes based on MRI. We compare two established software packages that offer an automated image analysis, EMS and SIENAX, and a third method, which we present. The latter is based on the Interactive Watershed Transform and a model based histogram analysis. All methods are evaluated with respect to noise, image inhomogeneity, and resolution as well as inter-examination and inter-scanner characteristics on 66 phantom and volunteer images. Furthermore, we evaluate the N3 nonuniformity correction for improving robustness and reproducibility. Despite the conceptual similarity of SIENAX and EMS, important differences are revealed. Finally, the volumetric accuracy of the methods is investigated using the ground truth of the BrainWeb phantom.\"",
        "Document: \"Analysis Of Variability In Manual Liver Tumor Delineation In Ct Scans. Manual delineations by experts are often used as reference standards for validating segmentation algorithms, although it is well known that they always show some degree of variability. Our goal is to estimate the effects of using a limited number of expert segmentations. Given ten manual delineations of 13 liver tumors, we analyzed the volume error made by randomly selecting subsets of the ten segmentations compared to the complete set. We found that when using just one segmentation the expected error was 17% with a maximum of 35%. This means that it is questionable whether a comparison with a single reference allows a reliable validation. When three segmentations are chosen, the error is halved, so this might be a reasonable compromise between accuracy and viability of evaluation studies.\"",
        "Document: \"Boundary estimation of fiber bundles derived from diffusion tensor images. Purpose\u00a0\u00a0Diffusion tensor imaging (DTI) is a non-invasive imaging technique that allows estimating the location of white matter tracts\n based on the measurement of water diffusion properties. Using DTI data, the fiber bundle boundary can be determined to gain\n information about eloquent structures, which is of major interest for neurosurgical interventions. In this paper, a novel\n approach for boundary estimation is presented.\n \n \n \n \n Methods\u00a0\u00a0DTI in combination with diverse segmentation algorithms allows estimating the position and course of fiber tracts in the human\n brain. For additional information about the expansion of the fiber bundle, the introduced iterative approach uses the centerline\n of a tracked fiber bundle between two regions of interest (ROI). After sampling along this centerline, rays are sent out radially,\n discrete 2D contours are calculated, and the fiber bundle boundary is estimated in a stepwise manner. For this purpose, each\n ray is analyzed using several criteria, including anisotropy parameters and angle parameters, to find the boundary point.\n \n \n \n \n Results\u00a0\u00a0The novel method for automatically calculating the boundaries has been applied to several artificially generated DTI datasets.\n Multiple parameters were varied: number of rays per plane, sampling rate and sampled points along the rays. For the DTI data\n used in the experiments, the method yielded a dice similarity coefficient (DSC) between 74.7 and 91.5%.\n \n \n \n \n Conclusions\u00a0\u00a0In this paper, a novel approach to retrieve significant information about the fiber bundle boundary from DTI data is presented.\n The method is a contribution to gather important knowledge about high-risk structures in neurosurgical interventions.\"",
        "Document: \"Fast Automated Segmentation Of Femoral Heads In Fluoroscopic X-Ray Images. In fluoroscopic tracking for fractured femur bone fixation, a precise identification of the femoral head provides valuable guidance for positioning the implant instruments such as nails and screws. Noise, occlusions and weak edges challenge the task of automatically segmenting the femoral head. In this paper, a fast and fully automated method to precisely delineate the femoral head in fluoroscopic X-ray images is presented. The proposed method comprises two stages: First, a candidate circle detection stage using a set of curved Gabor filters and a Gabor-based Hough transform is applied to estimate a few candidate circles approximating the femoral head. Second, a fine circle determination stage extracts the target circle from the candidates by analyzing the anatomical features of the femoral head and its spatial relation to the acetabulum. The validity and robustness of the method were tested on a set of 1184 fluoroscopic images from different vendors.\"",
        "Document: \"3D-Lungenlappen-Segmentierung durch Kombination von Region Growing, Distanz- und Wasserscheiden-Transformation. Zusammenfassung. Die Lungenlappen spielen als ann\u00a8ahernd unab-h\u00a8angige anatomische Komponenten der Lunge eine wesentliche Rolle bei Diagnose und Therapie von Lungenerkrankungen. Eine Detektion der d\u00a8 unnen Lappengrenzen, der sogenannten Fissuren ist jedoch schwie-rig, da diese in vielen F\u00a8allen aufgrund pathologischer Ver\u00a8anderungen nur unvollst\u00a8andig im CT-Bild erscheinen. Daher bestimmt unser An-satz die Lappengrenzen im Wesentlichen auf Basis der lappenspezifischen Gef\u00a8a?systeme und verwendet die eventuell vorhandene Repr\u00a8asentation der Fissuren in den Daten lediglich als Zusatzinformation. Die Metho-de ben\u00a8otigt dabei minimale und intuitive Interaktion und erlaubt eine robuste Dekomposition der Lunge in ihre Lappen, welche vor allem zur Bestimmung lappenspezifischer CT-Parameter verwendet werden kann. \"",
        "Document: \"Ground Truth in MS Lesion Volumetry - A Phantom Study. A quantitative analysis of small structures such as focal legions in patients suffering from multiple sclerosis (MS) is an important issue in both diagnosis and therapy monitoring. In order to reach clinical relevance, the reproducibility and especially the accuracy of a proposed method has to be validated. We propose a framework for the generation of realistic digital phantoms of MS lesions of known volumes and their incorporation into an MR dataset of a healthy volunteer. Due to the absence of a \"ground truth\" for lesions in general and MS lesions in particular, phantom data are a commonly used validation method for quantitative image analysis methods. However, currently available lesion phantoms suffer from the fact that the embedding structures are only simplifications of the real organs. We generated 54 datasets from a multispectral MR scan with incorporated MS lesion phantoms. The lesion phantoms were created using various shapes (3), sizes (6) and orientations (3). Since the common gold standard, iin clinical lesion volumetry is based on manual volume tracing, an evaluation is carried out from both a manual analysis of three human experts and a semi-automated approach based on regional histogram analysis. Additionally, an intra-observer study is performed. Our results clearly demonstrate the importance of an improved gold, standard in lesion volumetry beyond manual tracing and voxel counting.\"",
        "1 is \"Automated Segmentation of Multiple Sclerosis Lesions by Model Outlier Detection.\", 2 is \"Sophia: A novel approach for Textual Case-based Reasoning\"",
        "Given above information, for an author who has written the paper with the title \"Boundary estimation of fiber bundles derived from diffusion tensor images.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004571": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Waterloo Exploration Database: New Challenges for Image Quality Assessment Models':",
        "Document: \"Polyview fusion: a strategy to enhance video-denoising algorithms. We propose a simple but effective strategy that aims to enhance the performance of existing video denoising algorithms, i.e., polyview fusion (PVF). The idea is to denoise the noisy video as a 3-D volume using a given base 2-D denoising algorithm but applied from multiple views (front, top, and side views). A fusion algorithm is then designed to merge the resulting multiple denoised videos into one, so that the visual quality of the fused video is improved. Extensive tests using a variety of base video-denoising algorithms show that the proposed PVF method leads to surprisingly significant and consistent gain in terms of both peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) performance, particularly at high noise levels, where the improvement over state-of-the-art denoising algorithms is often more than 2 dB in PSNR.\"",
        "Document: \"SSIM-inspired image restoration using sparse representation. Recently, sparse representation based methods have proven to be successful towards solving image restoration problems. The objective of these methods is to use sparsity prior of the underlying signal in terms of some dictionary and achieve optimal performance in terms of mean-squared error, a metric that has been widely criticized in the literature due to its poor performance as a visual quality predictor. In this work, we make one of the first attempts to employ structural similarity (SSIM) index, a more accurate perceptual image measure, by incorporating it into the framework of sparse signal representation and approximation. Specifically, the proposed optimization problem solves for coefficients with minimum \n\n\n\u2112\n\n\n0\n\n\nOpen image in new window norm and maximum SSIM index value. Furthermore, a gradient descent algorithm is developed to achieve SSIM-optimal compromise in combining the input and sparse dictionary reconstructed images. We demonstrate the performance of the proposed method by using image denoising and super-resolution methods as examples. Our experimental results show that the proposed SSIM-based sparse representation algorithm achieves better SSIM performance and better visual quality than the corresponding least square-based method.\"",
        "Document: \"Cooperation Enhancement for Message Transmission in VANETs. As one special case of the Mobile Ad Hoc Networks (MANET), vehicular ad-hoc networking (VANET) is featured by its high mobility and constantly changing topology. In VANET, nodes can work properly only if the participating vehicles cooperate with each other during communications. However, as a distributed network, individual vehicles might be non-cooperative for their own benefits. In order to prevent non-cooperative vehicles from tampering packet relaying in the network, we propose a cooperation enhancement mechanism using \"Neighborhood WatchDog\" to generate \"Trust Token\" based on the first-hand observation. Therefore, trust relationships and packet-acceptance decisions of the receiving nodes are based on the instant observation and the token-proved relaying behavior of the benign neighboring vehicles. With the inherit mapping between the Electronic ID of one vehicle and its public key, keys can be distributed on-the-fly. As a network layer solution, the cooperation enhancement mechanism proposed in this paper is built on the top of our previous proposed Media Access Control (MAC) protocol: Relative Position Based-MAC (RPB-MAC).\"",
        "Document: \"A Patch-Structure Representation Method for Quality Assessment of Contrast Changed Images. Contrast is a fundamental attribute of images that plays an important role in human visual perception of image quality. With numerous approaches proposed to enhance image contrast, much less work has been dedicated to automatic quality assessment of contrast changed images. Existing approaches rely on global statistics to estimate contrast quality. Here we propose a novel local patch-based objecti...\"",
        "Document: \"An Adaptive Linear System Framework For Image Distortion Analysis. We describe a framework for decomposing the distortion between two images into a linear combination of components. Unlike conventional linear bases such as those in Fourier or wavelet decompositions, a subset of the components in our representation are not fixed, but are adaptively computed from the input images. We show that this framework is a generalization of a number of existing image comparison approaches. As an example of a specific implementation, we select the components based on the structural similarity principle, separating the overall image distortions into non-structural distortions (those that do not change the structures of the objects in the scene) and the remaining structural distortions. We demonstrate that the resulting measure is effective in predicting image distortions as perceived by human observers.\"",
        "Document: \"Flexible Mode Selection and Complexity Allocation in High Efficiency Video Coding. To improve compression performance, High Efficiency Video Coding (HEVC) employs a quad-tree based block representation, namely Coding Tree Unit (CTU), which can support larger partitions and more coding modes than a traditional macroblock. Despite its high compression efficiency, the number of combinations of coding modes increases dramatically, which results in high computational complexity at the encoder. Here we propose a flexible framework for HEVC coding mode selection, with a user-defined global complexity factor. Based on linear programming, a hierarchical complexity allocation scheme is developed to allocate computational complexities among frames and Coding Units (CUs) to maximize the overall Rate-Distortion (RD) performance. In each CU, with the allocated complexity factor, a mode mapping based approach is employed for coding mode selection. Extensive experiments demonstrate that, with a series of global complexity factors, the proposed model can achieve good trade-offs between computational complexity and RD performance.\"",
        "Document: \"Why is image quality assessment so difficult?. ABSTRACT Image quality assessment plays an important role in various image processing applications. A great deal of effort has been made in recent years to develop objective image quality metrics that correlate with perceived quality measurement. Unfortunately, only limited success has been achieved. In this paper, we provide some insights on why image quality assessment is so difficult by pointing out the weaknesses of the error sensitivity based framework, which has been used by most image quality assessment approaches in the literature. Furthermore, we propose a new philosophy in designing image quality metrics: The main function of the human,eyes is to extract structural information from the viewing field, and the human visual system is highly adapted for this purpose. Therefore, a measurement of structural distortion should be a good approximation of perceived image distortion. Based on the new philosophy, we implemented,a simple but effective image quality indexing algorithm, which is very promising as shown by our current results.\"",
        "Document: \"Deep Blur Mapping: Exploiting High-Level Semantics by Deep Neural Networks. The human visual system excels at detecting the local blur of visual images, but the underlying mechanism is not well understood. Traditional views of blur such as the reduction in energy at high frequencies and loss of phase coherence at localized features have fundamental limitations. For example, they cannot well discriminate flat regions from blurred ones. Here, we propose that the high-level ...\"",
        "Document: \"Multi-sensor image registration based-on local phase coherence. The major challenges in automatic multi-sensor image registration are the inconsistency in intensity or contrast patterns, and the existence of partial or missing information between images. Here we propose a novel image registration method based on local phase coherence features, which are insensitive to changes in intensity or contrast. Furthermore, a new objective function based on weighted mutual information is proposed, where less weight is given to the objects that have no correspondence between images. The proposed method has been tested on both synthetic and medical images and evaluated based on registration accuracy. Our experiments demonstrate good performance of the proposed approach with missing or partial data, with significant changes in contrast, and with the presence of noise.\"",
        "Document: \"Quality-of-Experience for Adaptive Streaming Videos: An Expectation Confirmation Theory Motivated Approach. The dynamic adaptive streaming over HTTP provides an inter-operable solution to overcome volatile network conditions, but how the human visual quality of experience (QoE) changes with time-varying video quality is not well-understood. Here, we build a large-scale video database of time-varying quality and design a series of subjective experiments to investigate how humans respond to compression le...\"",
        "1 is \"Channel estimation techniques based on pilot arrangement in OFDM systems\", 2 is \"Interactive local adjustment of tonal values\"",
        "Given above information, for an author who has written the paper with the title \"Waterloo Exploration Database: New Challenges for Image Quality Assessment Models\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004651": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A new, general method of 3D model generation for active shape image segmentation':",
        "Document: \"Interactive segmentation and boundary surface formation for 3-D digital images. In visualizing objects in 3-D digital images based on shaded-surface displays, segmentation of the 3-D regions is a basic processing operation. When object and nonobject regions touch or overlap and have identical features, automatic segmentation methods fail. We present an interactive technique which permits isolation of arbitrary subregions of the 3-D image by a region filling procedure. We illustrate the usefulness of this capability, through computerized tomography data, in preoperative surgical planning and for producing arbitrary fragmentations of objects in 3-D images. The techniques are applicable in other fields as well, where independent display of overlapping structures is important. When the object regions are specified through a set of contours in a sequence of slices, we show that the boundary surfaces of the object are produceable in an efficient way using the cuberille-based discrete representation of the object surfaces. The algorithms we present for forming boundary surfaces do not impose restrictions on the shape and pattern of distribution of contours in the slices as do the surface tiling schemes reported in the literature. We assume that a contour is made up of at least three points.\"",
        "Document: \"Shape modeling via local curvature scale. Segmentation and modeling of organs using model-based approaches require a priori information which is often given by manually tagging landmarks on a training set of shapes. This is a tedious, time-consuming, and error prone task. To overcome some of these drawbacks, focusing on 2D shapes, we devised an automatic method based on the notion of curvature scale - a new local scale concept. This shape descriptor is used to automatically locate mathematical landmarks on the mean of the shapes in the training set, which are then propagated to the training shapes. Altogether 12 different strategies are described and are evaluated in different combinations in terms of compactness on two data sets - 40 CT images of the liver and 40 MR images of the talus bone of the foot. The results show that, for the same number of landmarks, the proposed methods are more compact than manual and equally spaced annotations.\"",
        "Document: \"Scale-based fuzzy connected image segmentation: theory, algorithms, and validation. This paper extends a previously reported theory and algorithms for object definition based on fuzzy connectedness. In this approach, a strength of connectedness is determined between every pair of image elements. This is done by considering all possible connecting paths between the two elements in each pair. The strength assigned to a particular path is defined as the weakest affinity between successive pairs of elements along the path. Affinity specifies the degree to which elements hang together locally in the image. Although the theory allowed any neighborhood size for affinity definition, it did not indicate how this was to be selected. By bringing object scale into the framework in this paper, not only the size of the neighborhood is specified but also it is allowed to change in different parts of the image. This paper argues that scale-based affinity, and hence connectedness, is natural in object definition and demonstrates that this leads to more effective object segmentation. The approach presented here considers affinity to consist of two components. The homogeneity-based component indicates the degree of affinity between image elements based on the homogeneity of their intensity properties. The object-feature-based component captures the degree of closeness of their intensity properties to some expected values of those properties for the object. A family of non-scale-based and scale-based affinity relations are constructed dictated by how we envisage the two components to characterize objects. A simple and effective method for giving a rough estimate of scale at different locations in the image is presented. The original theoretical and algorithmic framework remains more-or-less the same but considerably improved segmentations result. The method has been tested in several applications qualitatively. A quantitative statistical comparison between the non-scale-based and the scale-based methods was made based on 250 phantom images. These were generated from 10 patient MR brain studies by first segmenting the objects, then setting up appropriate intensity levels for the object and the background, and then by adding five different levels for each of noise and blurring and a fixed slow varying background component. Both the statistical and the subjective tests clearly indicate that the scale-based method is superior to the non-scale-based method in capturing details and in robustness to noise. It is also shown, based on these phantom images, that any (global) optimum threshold selection method will perform inferior to the fuzzy connectedness methods described in this paper.\"",
        "Document: \"Co-segmentation of functional and anatomical images. This paper presents a novel method for segmenting functional and anatomical structures simultaneously. The proposed method unifies domains of anatomical and functional images (PET-CT), represents them in a product lattice, and performs simultaneous delineation of regions based on a random walk image segmentation. In addition, we propose a simple yet efficient object/background seed localization method, where background and foreground object cues are automatically obtained from PET images and propagated onto the corresponding anatomical images (CT). In our experiments, abnormal anatomies on PET-CT images from human subjects are segmented synergistically by the proposed fully automatic co-segmentation method with high precision (mean DSC of 91.44%) in seconds (avg. 40 seconds).\"",
        "Document: \"GC-ASM: Synergistic Integration of Graph-Cut and Active Shape Model Strategies for Medical Image Segmentation. Image segmentation methods may be classified into two categories: purely image based and model based. Each of these two classes has its own advantages and disadvantages. In this paper, we propose a novel synergistic combination of the image based graph-cut (GC) method with the model based ASM method to arrive at the GC-ASM method for medical image segmentation. A multi-object GC cost function is proposed which effectively integrates the ASM shape information into the GC framework. The proposed method consists of two phases: model building and segmentation. In the model building phase, the ASM model is built and the parameters of the GC are estimated. The segmentation phase consists of two main steps: initialization (recognition) and delineation. For initialization, an automatic method is proposed which estimates the pose (translation, orientation, and scale) of the model, and obtains a rough segmentation result which also provides the shape information for the GC method. For delineation, an iterative GC-ASM algorithm is proposed which performs finer delineation based on the initialization results. The proposed methods are implemented to operate on 2D images and evaluated on clinical chest CT, abdominal CT, and foot MRI data sets. The results show the following: (a) An overall delineation accuracy of TPVF > 96%, FPVF < 0.6% can be achieved via GC-ASM for different objects, modalities, and body regions. (b) GC-ASM improves over ASM in its accuracy and precision to search region. (c) GC-ASM requires far fewer landmarks (about 1/3 of ASM) than ASM. (d) GC-ASM achieves full automation in the segmentation step compared to GC which requires seed specification and improves on the accuracy of GC. (e) One disadvantage of GC-ASM is its increased computational expense owing to the iterative nature of the algorithm.\"",
        "Document: \"Fuzzy connectedness and object definition: theory, algorithms, and applications in image segmentation. Images are by nature fuzzy. Approaches to object information extraction from images should attempt to use this fact and retain fuzziness as realistically as possible. In past image segmentation research, the notion of \u201changing togetherness\u201d of image elements specified by their fuzzy connectedness has been lacking. We present a theory of fuzzy objects for n -dimensional digital spaces based on a notion of fuzzy connectedness of image elements. Although our definitions lead to problems of enormous combinatorial complexity, the theoretical results allow us to reduce this dramatically, leading us to practical algorithms for fuzzy object extraction. We present algorithms for extracting a specified fuzzy object and for identifying all fuzzy objects present in the image data. We demonstrate the utility of the theory and algorithms in image segmentation based on several practical examples all drawn from medical imaging.\"",
        "Document: \"Intensity Standardization Simplifies Brain MR Image Segmentation. Typically, brain MR images present significant intensity variation across patients and scanners. Consequently, training a classifier on a set of images and using it subsequently for brain segmentation may yield poor results. Adaptive iterative methods usually need to be employed to account for the variations of the particular scan. These methods are complicated, difficult to implement and often involve significant computational costs. In this paper, a simple, non-iterative method is proposed for brain MR image segmentation. Two preprocessing techniques, namely intensity-inhomogeneity-correction, and more importantly MR image intensity standardization, used prior to segmentation, play a vital role in making the MR image intensities have a tissue-specific numeric meaning, which leads us to a very simple brain tissue segmentation strategy.\"",
        "Document: \"User-steered image segmentation paradigms: live wire and live lane. In multidimensional image analysis, there are, and will continue to be, situations wherein automatic image segmentation methods fail, calling for considerable user assistance in the process. The main goals of segmentation research for such situations ought to be (i) to provide effective control to the user on the segmentation process while it is being executed, and (ii) to minimize the total user's time required in the process. With these goals in mind, we present in this paper two paradigms, referred to as live wire and live lane , for practical image segmentation in large applications. For both approaches, we think of the pixel vertices and oriented edges as forming a graph, assign a set of features to each oriented edge to characterize its ``boundariness,'' and transform feature values to costs. We provide training facilities and automatic optimal feature and transform selection methods so that these assignments can be made with consistent effectiveness in any application. In live wire, the user first selects an initial point on the boundary. For any subsequent point indicated by the cursor, an optimal path from the initial point to the current point is found and displayed in real time. The user thus has a live wire on hand which is moved by moving the cursor. If the cursor goes close to the boundary, the live wire snaps onto the boundary. At this point, if the live wire describes the boundary appropriately, the user deposits the cursor which now becomes the new starting point and the process continues. A few points (live-wire segments) are usually adequate to segment the whole 2D boundary. In live lane, the user selects only the initial point. Subsequent points are selected automatically as the cursor is moved within a lane surrounding the boundary whose width changes as a function of the speed and acceleration of cursor motion. Live-wire segments are generated and displayed in real time between successive points. The users get the feeling that the curve snaps onto the boundary as and while they roughly mark in the vicinity of the boundary. We describe formal evaluation studies to compare the utility of the new methods with that of manual tracing based on speed and repeatability of tracing and on data taken from a large ongoing application. The studies indicate that the new methods are statistically significantly more repeatable and 1.5\u20132.5 times faster than manual tracing.\"",
        "Document: \"Local curvature scale: a new concept of shape description. Shape description plays a fundamental role in computer vision and pattern recognition, especially in the fields of shape analysis, image segmentation, and registration. Shape representations must be unique, complete and should be able to reflect the differences between similar objects while abstracting from detail and keeping the basic features. Although many methods for shape description exist, they are usually application dependent. The proposed method of boundary shape description is based on the notion of curvature-scale, which is a new local scale concept, defined at each boundary element. From this representation, we can extract special points of interest such as convex and concave corners, straight lines, circular segments, and inflection points. This method is different from existing methods of curvature estimation and can be directly applied to digital boundaries without requiring prior approximation of the boundary. The results show that it produces a complete boundary shape description capable of handling different levels of shape detail. It also has numerous potential applications such as automatic landmark tagging which becomes necessary to build model-based approaches toward the goal of organ modeling and segmentation. The method is applicable to spaces of any dimensionality, although we have focused in this paper on 2D shapes.\"",
        "Document: \"Affinity functions in fuzzy connectedness based image segmentation II: Defining and recognizing truly novel affinities. Affinity functions - the measure of how strongly pairs of adjacent spels in the image hang together - represent the core aspect (main variability parameter) of the fuzzy connectedness (FC) algorithms, an important class of image segmentation schemas. In this paper, we present the first ever theoretical analysis of the two standard affinities, homogeneity and object-feature, the way they can be combined, and which combined versions are truly distinct from each other. The analysis is based on the notion of equivalent affinities, the theory of which comes from a companion Part I of this paper (Ciesielski and Udupa, in this issue) [11]. We demonstrate that the homogeneity based and object feature based affinities are equivalent, respectively, to the difference quotient of the intensity function and Rosenfeld's degree of connectivity. We also show that many parameters used in the definitions of these two affinities are redundant in the sense that changing their values lead to equivalent affinities. We finish with an analysis of possible ways of combining different component affinities that result in non-equivalent affinities. In particular, we investigate which of these methods, when applied to homogeneity based and object-feature based components lead to truly novel (non-equivalent) affinities, and how this is affected by different choices of parameters. Since the main goal of the paper is to identify, by formal mathematical arguments, the affinity functions that are equivalent, extensive experimental confirmations are not needed - they show completely identical FC segmentations - and as such, only relevant examples of the theoretical results are provided. Instead, we focus mainly on theoretical results within a perspective of the fuzzy connectedness segmentation literature.\"",
        "1 is \"A benchmark for RGB-D visual odometry, 3D reconstruction and SLAM\", 2 is \"Elastic model-based segmentation of 3-D neuroradiological data sets.\"",
        "Given above information, for an author who has written the paper with the title \"A new, general method of 3D model generation for active shape image segmentation\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004710": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Pattern tool support to guide interface design':",
        "Document: \"Model transformation from OWL-S to BPEL via SiTra. Although there are a large number of academic and industrial model transformation frameworks available, allowing specification, implementation, maintenance and documentation of model transformations which provide a rich set of functionalities, such tools are inherently complex. In particular, for a newcomer to the field of model transformation and for researchers who are only interested in experimentation and creation of prototypes, the steep learning curve is a significant hurdle. There is thus a clear scope for the creation of model transformation frameworks that are both easy to use and able to conduct complex transformations. Simple Transformer (SiTra) is a model transformation framework, which was originally designed to be a \"way in\" for the experienced programmer, to start using the concepts of model transformation, and for academic researchers to experiment with the creation of prototypes of implementation of their transformations. The underlying idea of SiTra is to put less focus on the specification language, maintenance and documentation aspects of transformation, by focusing on the implementation of transformations. SiTra makes use of Java for the specification of transformations. This alleviates the need to learn a new specification language or get to grips with a new tool and development environment. SiTra is equipped with a strong transformation engine to execute the transformation behind the scenes. This paper reports on a case study involving transformations from Ontology Web Language-Service (OWL-S) to Business Process Execution Language (BPEL), demonstrating that SiTra can also be used to handle complex and large transformations.\"",
        "Document: \"A Framework for the Analysis of Process Mining Algorithms. There are many process mining algorithms and representations, making it difficult to choose which algorithm to use or compare results. Process mining is essentially a machine learning task, but little work has been done on systematically analyzing algorithms to understand their fundamental properties, such as how much data are needed for confidence in mining. We propose a framework for analyzing process mining algorithms. Processes are viewed as distributions over traces of activities and mining algorithms as learning these distributions. We use probabilistic automata as a unifying representation to which other representation languages can be converted. We present an analysis of the Alpha algorithm under this framework and experimental results, which show that from the substructures in a model and behavior of the algorithm, the amount of data needed for mining can be predicted. This allows efficient use of data and quantification of the confidence which can be placed in the results.\"",
        "Document: \"A principled approach to the analysis of process mining algorithms. Process mining uses event logs to learn and reason about business process models. Existing algorithms for mining the control-flow of processes in general do not take into account the probabilistic nature of the underlying process, which affects the behaviour of algorithms and the amount of data needed for confidence in mining. We contribute a first step towards a novel probabilistic framework within which to talk about approaches to process mining, and apply it to the well-known Alpha Algorithm. We show that knowledge of model structures and algorithm behaviour can be used to predict the number of traces needed for mining.\"",
        "Document: \"Efficient Retrieval of Key Material for Inspecting Potentially Malicious Traffic in the Cloud. Cloud providers must detect malicious traffic in and out of their network, virtual or otherwise. The use of Intrusion Detection Systems (IDS) has been hampered by the encryption of network communication. The result is that current signatures cannot match potentially malicious requests. A method to acquire the encryption keys is Virtual Machine Introspection (VMI). VMI is a technique to view the internal, and yet raw, representation of a Virtual Machine (VM). Current methods to find keys are expensive and use sliding windows or entropy. This inevitably requires reading the memory space of the entire process, or worse the OS, in a live environment where performance is paramount. This paper describes a structured walk of memory to find keys, particularly RSA, using as fewer reads from the VM as possible. In doing this we create a scalable mechanism to populate an IDS with keys to analyse traffic.\"",
        "Document: \"A Simplified Method of Measurement of Energy Consumption in Cloud and Virtualized Environment. Measuring energy consumption is an essential step in the development of policies for the management of energy in every IT system. There is a wide range of methods using both hardware and software for measuring energy consumed by the system accurately. However, most of these methods measure energy consumed by a machine or a cluster of machines. In environments such as Cloud that an application can be built from components with comparable characteristics, measuring energy consumed by a single component can be extremely beneficial. For example, if we can measure energy consumed by different HTTP servers, then we can establish which one consumes less energy performing a given task. As a result, the Cloud provider can provide incentives, so that, application developers use the HTTP server that consume less energy. Indeed, considering size of the Cloud, even a small amount of saving per Virtual Machine can add up to a substantial saving. In this paper, we propose a technique to measure energy consumed by an application via measuring energy consumed by the individual processes of the application. We shall deal with applications that run in a virtualized environment such as Cloud. We present two implementations of our idea to demonstrate the feasibility of the approach. Firstly, a method of measurement with the help of Kernel-Based Virtual Machine running on a typical laptop is presented. Secondly, in a commercial Cloud such as Elastic host, we describe a method of measuring energy consumed by processes such as HTTP servers. This will allow commercial providers to identify which product consumes less energy on their platform.\"",
        "Document: \"An architecture for automated QoS resolution in wireless systems. The pervasive nature of mobile and wireless systems has led to increased concerns over quality of service (QoS). In the prevailing models for QoS management, QoS resolution is achieved by table look-up, a feature that makes table access the focal point of activity. This approach suffers from two limitations, namely, an inability to deal with unexpected QoS requests, and a reliance on human intervention for update of information. This paper is concerned with the presentation of an architecture for supporting automated QoS resolution through verification. The architecture is modular and the QoS resolution function is performed by a subsidiary component, which combines knowledge base with resolution mechanism. This separation of concerns and the support for flexible QoS management has the advantage of accommodating new forms of QoS requests, and of minimising human intervention. An implementation of the QoS resolution architecture is presented in terms of timed automata. In addition, a framework is also introduced for extending QoS architectures such as ITSUMO.\"",
        "Document: \"An Evaluation Mechanism for QoS Management in Wireless Systems. The evaluation of QoS requirements is one of the critical functions that span both the design and the run-time phases of QoS management. This paper presents an architecture for QoS evaluation and admission control, based on the modelling of both system behaviour and QoS requirements. Two aspects are considered. The first refers to QoS management, and to the component-based architecture for QoS evaluation. The second relates to the approach and its illustration by a case study, based on a Personal Area Network. The proposed approach relies on the instantiation of models for representing both the behaviour and the QoS aspects of the system in terms of Timed Automata. The compatibility of the evaluation mechanism with architectures with a defined role for a QoS manager, such as ITSUMO, is also highlighted.\"",
        "Document: \"Stateless data concealment for distributed systems. With the growing number of Web applications and their variety, the need to prevent unauthorised access to data and to ensure data integrity in distributed systems has led to an increasing reliance on encryption. Within a node, a typical encryption process operates at file or directory level and applies indiscriminately one algorithm to its data. In this paper, a scheme is proposed whereby secrecy is achieved through file data and file location concealment, within a client-server distributed system. This involves the division of a file into fragments, their encryption and compression, the random allocation of these fragments to the nodes, the generation and transcription of metadata for reconstructing the original file, and finally the deletion of both the original file and its metadata from the local node. A prototype of the scheme was implemented and evaluated in terms of the performance of the distribution and recovery process.\"",
        "Document: \"Original Article: Translating natural language constraints to OCL. Object Constraint Language (OCL) is the only available language to annotate the Unified Modeling Language (UML) based conceptual schema (CS) of a software application. In practice, the constraints are captured in a natural language (NL) such as English and then an OCL expert manually transforms the NL expressions to OCL syntax. However, it is a common knowledge that OCL is difficult to write specifically for the novel users with little or no prior knowledge of OCL. In recent times, model transformation technology has made transformation of one language to another simpler and easier. In this paper we present a novel approach to automatically transform NL specification of software constraints to OCL constraints. In NL to OCL transformation, Semantics of Business Vocabulary and Rules (SBVR) standard is used as an intermediate representation due to a couple of reasons: first of all, SBVR is based on higher order logic that simplifies the transformation of SBVR to other formal languages such as OCL. Moreover, SBVR used syntax of natural language and thus is close to human beings. The presented NL to OCL transformation via SBVR will not only simplify the process of generating OCL constraints but also generate accurate models in less time.\"",
        "Document: \"Weaving True-Concurrent Aspects Using Constraint Solvers. Large system models usually consist of several simpler models that can be understood more easily. Making changes to the behaviour of a component will likely affect several models and could introduce accidental errors. Aspects address this by modelling new functionality required in several places as an advice, which can be integrated with the original base models by specifying a pointcut. Before checking that the overall outcome is correct, we need to weave the cross-cutting advice into the base models, and obtain new augmented models. Although considerable research has been done to weave models, many such approaches are not fully automated. This paper looks at aspect weaving of scenario-based models, where aspects are given a true-concurrent semantics based on event structures. Our contribution is a novel formal automated technique for weaving aspects using the Z3-SMT solver. We compare the performance of Alloy and Z3 to justify our choice.\"",
        "1 is \"Extending UML to Visualize Design Patterns In Class Diagrams\", 2 is \"Don't Kick the Habit: The Role of Dependency in Habit Formation Apps.\"",
        "Given above information, for an author who has written the paper with the title \"Pattern tool support to guide interface design\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004732": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Self-optimization of large scale wildfire simulations':",
        "Document: \"Application of autonomic agents for global information grid management and security. In the near future, the U.S. DoD will activate a newly created Global Information Grid (GIG) providing an agile, robust, interoperable and collaborative communication network. This GIG is viewed as the single most important contributor to combat power and protection [1]. The GIG can be characterized as a globally interconnected, end-to-end set of disparate information and processing capabilities available on-demand to warfighters, policymakers and support personnel. Management of the GIG resources and operation will be, of necessity, intrinsic to the architecture and transparent to the user. The development an architecture that is secure against malicious exploitation, data or service denial and data corruption is a principal focus. Introduction of mobile, adhoc network elements having intermittent network connectivity and limited life exacerbates this problem. In this paper we explore the GIG and present an innovative application of Autonomic Agent technology currently in development that has the potential to automate many of the obligatory network management tasks. Additionally, we introduce a GIG Simulation Testbed that, utilizing Discrete Event System Specification (DEVS), is proving useful in simulating GIG operation at the node level for the analysis of agent-based detection and neutralization of network cyber attacks by malicious parties and programs.\"",
        "Document: \"Physics aware programming paradigm: approach and evaluation. Large scale scientific applications generally experience different execution phases at runtime and each phase has different computational and communication requirements. An optimal solution or numerical scheme for one execution phase might not be appropriate for the next phase of the application execution. In this paper we present Physics Aware Programming (PAP) paradigm that supports dynamic changes of the application solution if it optimizes the application performance at runtime. In the PAP approach, the application execution state is periodically monitored and analyzed to identify its current execution phase (state). For each change in the application execution phase, we will exploit the spatial and temporal attributes of the application physics to select the numerical algorithms/solvers that optimize its performance. We have applied our approach to a Ten-Tusscher's model of human ventricular epicardia myocyte paced at a varied cycle length (1000 to 50 ms). At runtime, we recognize the current phase of the heart simulation and based on the detected phase, we adopt the simulation \u0394t that maximizes the performance and maintains the required accuracy. Our experimental results show that we can achieve a speedup of three orders of magnitude while maintaining the required accuracy.\"",
        "Document: \"Communication-System For High-Performance Distributed Computing. With the current advances in computer and networking technology coupled with the availability of software tools for parallel and distributed computing, there has been increased interest in high-performance distributed computing (HPDC). We envision that HPDC environments with supercomputing capabilities will be available in the near future. However, a number of issues have to be resolved before future network-based applications can fully exploit the potential of the HPDC environment. In the paper we present an architecture for a high-speed local area network and a communication system that provides HPDC applications with high bandwidth and low latency. We also characterize the message-passing primitives required in HPDC applications and develop a communication protocol that implements these primitives efficiently.\"",
        "Document: \"ExNet: An Intelligent Network Management System. In this paper, we investigate the use of artificial Intelligence techniques in the management of large- scale high-speed networks. We present a design of an intelligent network management system (ExNet) that efficiently and effectively monitors and controls the resources of a large scale computer network. The ExNet provides World Wide Web graphical user interface. We have implemented a rule-based prototype of ExNet and have incorporated ExNet modules with IBM NetView network management system.\"",
        "Document: \"The design and evaluation of a virtual distributed computing environment. Current advances in high-speed networks such as ATM and fiber-optics, and software technologies such as the JAVA programming language and WWW tools, have made network-based computing a cost-effective, high-performance distributed computing environment. Metacomputing, a special subset of network-based computing, is a well-integrated execution environment derived by combining diverse and distributed resources such as MPPs, workstations, mass storage, and databases that show a heterogeneous nature in terms of hardware, software, and organization. In this paper we present the Virtual Distributed Computing Environment (VDCE), a metacomputing environment currently being developed at Syracuse University. VDCE provides an efficient web-based approach for developing, evaluating, and visualizing large-scale distributed applications that are based on predefined task libraries on diverse platforms. The VDCE task libraries relieve end-users of tedious task implementations and also support reusability. The VDCE software architecture is described in terms of three modules: (a) the Application Editor, a user-friendly application development environment that generates the Application Flow Graph (AFG) of an application&semi; (b) the Application Scheduler, which provides an efficient task-to-resource mapping of AFG&semi; and (c) the VDCE Runtime System, which is responsible for running and managing application execution and for monitoring the VDCE resources. We present experimental results of an application execution on the VDCE prototype for evaluating the performance of different machine and network configurations. We also show how the VDCE can be used as a problem-solving environment on which large-scale, network-centric applications can be developed by a novice programmer rather than by an expert in low-level details of parallel programming languages.\"",
        "Document: \"Resilient And Trustworthy Dynamic Data-Driven Application Systems (Dddas) Services For Crisis Management Environments. Future crisis management systems need resilient and trustworthy infrastructures to quickly develop reliable applications and processes, and ensure end-to-end security, trust, and privacy. Due to the multiplicity and diversity of involved actors, volumes of data, and heterogeneity of shared information; crisis management systems tend to be highly vulnerable and subject to unforeseen incidents. As a result, the dependability of crisis management systems can be at risk. This paper presents a cloud-based resilient and trustworthy infrastructure (known as rDaaS) to quickly develop secure crisis management systems. The rDaaS integrates the Dynamic Data-Driven Application Systems (DDDAS) paradigm into a service-oriented architecture over cloud technology and provides a set of resilient DDDAS-As-A Service (rDaaS) components to build secure and trusted adaptable crisis processes. The rDaaS also ensures resilience and security by obfuscating the execution environment and applying Behavior Software Encryption and Moving Technique Defense. A simulation environment for a nuclear plant crisis management case study is illustrated to build resilient and trusted crisis response processes.\"",
        "Document: \"Pragma: An Infrastructure for Runtime Management of Grid Applications. This paper presents an overview of Pragma, an adaptive runtime infrastructure capable of reactively and proactively managing and optimizing application execution using current system and application state, predictive models for system behavior and application performance, and an agent based control network. The overarching motivation for this research is to enable the next generation of very large-scale dynamically adaptive scientific and engineering simulations on widely distributed and highly heterogeneous and dynamic execution environments such as the computational \"grid\". Pragma combines 4 key components: system characterization and abstraction component, application characterization component, active network control, and policybase. The design of Pragma is driven by three astrophysical simulations chosen to be representative of a wide variety of important simulations and to expose many of the problems presently encountered (and currently unsolved) by computational physicists. The design, prototype implementation, and preliminary evaluations of Pragma components are presented.\"",
        "Document: \"A Network Protection Framework for DNP3 over TCP/IP protocol. The pervasive deployment of intelligent devices in the critical infrastructures sector and the high dependency of these devices on the Internet motivated attackers to target the communication and control protocols of these devices. DNP3 over TCP/IP is among those protocols that are widely used as communication and control protocols in critical infrastructures. Due to the facts that security was not part of the goals for designing the DNP3 and the incompetent of current protection systems, adversary can easily succeed in attacking DNP3 devices and network. In this paper, we present an Autonomic Network Protection Framework for DNP3 over TCP/IP that detects old attacks that cannot be prevented by the legacy DNP3 security devices as well as new attacks. The system's detection module is based on rule-based anomaly intrusion detection. We evaluated the effectiveness of the generated rules in detecting anomalies through both offline and online testing. Both the false positive and the false negative rates of our approach are quite low. In addition, we present a classification technique and an access control mechanism to provide autonomic network protection.\"",
        "Document: \"Anomaly-based Fault Detection System in Distributed System. One of the important design criteria for distributed systems and their applications is their reliability and robustness to hardware and software failures. The increase in complexity, interconnectedness, dependency and the asynchronous interactions between the components that include hardware resources (computers, servers, network devices), and software (application services, middleware, web services, etc.) makes the fault detection and tolerance a challenging research problem. In this paper, we present an innovative approach based on statistical and data mining techniques to detect faults (hardware or software) and also identify the source of the fault. In our approach, we monitor and analyze in realtime all the interactions between all the components of a distributed system. We used data mining and supervised learning techniques to obtain the rules that can accurately model the normal interactions among these components. Our anomaly analysis engine will immediately produce an alert whenever one or more of the interaction rules that capture normal operations is violated due to a software or hardware failure. We evaluate the effectiveness of our approach and its performance to detect software faults that we inject asynchronously, and compare the results for different noise level.\"",
        "Document: \"Resilient Dynamic Data Driven Application Systems (Rdddas). There is a growing interest in Cloud Computing for delivering computing as a utility. Security in Cloud Computing is a challenging research problem because it involves many interdependent tasks including vulnerability scanning, application layer firewalls, configuration management, alert monitoring and analysis, source code analysis, and user identity management. It is widely accepted that we cannot build software and computing systems that are free from vulnerabilities and cannot be penetrated or attacked. Consequently, there is a strong interest in resilience approach because of its potential to address the cybersecurity challenges. Our is based on using the Dynamic Data Driven Application System (DDDAS) and Moving Target Defence (MTD) strategies to develop resilient DDDAS. The Resilient Applications utilize the following capabilities: Software Behaviour Encryption (SBE), Replication, Diversity, Automated Checkpointing and Recovery. Software Behaviour Encryption employs spatiotemporal behaviour encryption and a moving target defence to make active software components change their implementations and their resources randomly and consequently evade attackers. Diversity and random execution is achieved by \"hot\" shuffling multiple functionality-equivalent, behaviourally-different software versions at runtime (This encryption of the execution environment will make it extremely difficult for an attack to disrupt the normal operations of a cloud application. Also, the dynamic change in the execution environment will hide the software flaws that would otherwise be exploited by a cyberattacker. Checkpointing is used to save the current state of the task to a reliable storage and thus enabling rollback recovery if it is required to tolerate cyberattacks and mitigate their impacts.We use the Compiler for Portable Checkpointing (CPPC), a tool for automatically inserting portable checkpoints into the code. We also evaluate the performance and overhead of running three applications in our rDDDAS environment. Our experimental results show that the rDDDAS environment can be used to develop resilient cloud applications are resilient against attacks with around 7% in execution time overhead. (C) 2013 The Authors. Published by Elsevier B.V.\"",
        "1 is \"Simplifying the Development, Use and Sustainability of HPC Software.\", 2 is \"A compile-time scheduling heuristic for interconnection-constrained heterogeneous processor architectures\"",
        "Given above information, for an author who has written the paper with the title \"Self-optimization of large scale wildfire simulations\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004780": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Using the OPC Standard for Real-Time Process Monitoring and Control':",
        "Document: \"Volume morphing and rendering\u2014an integrated approach. In this paper, we first introduce a 3D morphing method for landmark-based volume deformation, using various scattered data interpolation schemes. Qualitative and speed comparisons are also made for different interpolation schemes. To efficiently render the volume morphing process, a new deformable volume rendering algorithm is presented. The algorithm renders the deformed volume directly without going through the expensive volume construction process. Piecewise linear approximation of the deformation function by adaptive space subdivision and template-based block projection are used to speed up the rendering process. While the resultant timings is slower than real time, it is much faster than existing volume morphing/rendering pipelines.\"",
        "Document: \"Image-based Metrology of Porous Tissue Engineering Scaffolds. Tissue engineering is an interdisciplinary effort aimed at the repair and regeneration of biological tissues through the application and control of cells, porous scaffolds and growth factors. The regeneration of specific tissues guided by tissue analogous substrates is dependent on diverse scaffold architectural indices that can be derived quantitatively from the microCT and microMR images of the scaffolds. However, the randomness of pore-solid distributions in conventional stochastic scaffolds presents unique computational challenges. As a result, image-based characterization of scaffolds has been predominantly qualitative. In this paper, we discuss quantitative image-based techniques that can be used to compute the metrological indices of porous tissue engineering scaffolds. While bulk averaged quantities such as porosity and surface are derived directly from the optimal pore-solid delineations, the spatially distributed geometric indices are derived from the medial axis representations of the pore network. The computational framework proposed (to the best of our knowledge for the first time in tissue engineering) in this paper might have profound implications towards unraveling the symbiotic structure-function relationship of porous tissue engineering scaffolds.\"",
        "Document: \"A model-based rescheduling framework for managing abnormal supply chain events. Enterprises today have realized the importance of supply chain management to achieve operational efficiency, cut costs, and maintain quality. Uncertainties in supply, demand, transportation, market conditions, and many other factors can interrupt supply chain operations, causing significant adverse effects. These uncertainties motivate the development of decision support systems for managing disruptions in the supply chain. In this paper, we propose a model-based framework for rescheduling operations in the face of supply chain disruptions. A causal model, called the composite-operations graph, captures the cause-and-effect among all the variables in supply chain operation. Its subgraph, called scheduled-operations graph, captures the causal relationships in a schedule and is used for identifying the consequences of a disruption. Rescheduling is done by searching a rectifications-graph, which captures all possible options to overcome the disruption effects, based on a user-specified utility function. In contrast to heuristic approaches, the main advantages of the proposed model-based rescheduling method are the completeness of solution search and flexibility of the utility function. The proposed framework is illustrated using a refinery supply chain example.\"",
        "Document: \"Automating the expert consensus paradigm for robust lung tissue classification. Clinicians confirm the efficacy of dynamic multidisciplinary interactions in diagnosing Lung disease/wellness from CT scans. However, routine clinical practice cannot readily accomodate such interactions. Current schemes for automating lung tissue classification are based on a single elusive disease differentiating metric; this undermines their reliability in routine diagnosis. We propose a computational workflow that uses a collection (#: 15) of probability density functions (pdf)-based similarity metrics to automatically cluster pattern-specific (# patterns: 5) volumes of interest (#VOI: 976) extracted from the lung CT scans of 14 patients. The resultant clusters are refined for intra-partition compactness and subsequently aggregated into a super cluster using a cluster ensemble technique. The super clusters were validated against the consensus agreement of four clinical experts. The aggregations correlated strongly with expert consensus. By effectively mimicking the expertise of physicians, the proposed workflow could make automation of lung tissue classification a clinical reality.\"",
        "Document: \"Efficient bulk maritime logistics for the supply and delivery of multiple chemicals. Many multinational chemical companies (MNCs) manage the inventories of several raw materials at their worldwide sites. Maritime transportation plays a key role in this chemical logistics. In this paper, we address an inventory service problem in which a chemical MNC uses a fleet of multi-parcel ships with dedicated compartments to move multiple chemicals continually among its internal and external production and consumption sites. The objective is to ensure continuity of operation at all sites by maintaining adequate inventory levels of all raw materials. We develop a novel multi-grid continuous-time mixed-integer linear programming (MILP) formulation based (Susarla, Li, & Karimi, 2010) for this chemical logistics problem. Our model allows limited jetties at each site, non-zero transfer times, variable load/unload quantities, transfer task sequencing, etc. In contrast to the literature, it needs no separate estimates for arrivals at each site. Several examples are solved to illustrate the efficiency of our proposed formulation.\"",
        "Document: \"Implementation of multi agents based system for process supervision in large-scale chemical plants. \u2022Modern-day chemical plants exhibit scale-, structure- and dynamic-complexity.\u2022Process supervision systems for such processes should be flexible, cooperative, collaborative, extensible, and scalable.\u2022In this paper, we propose an agent-based architecture for developing and deploying such process supervision systems.\u2022The key insight is that the process descriptors used for developing the supervision models are also dynamic.\u2022The implementation of the agent-based system is described and its benefits demonstrated using an oil & gas case study.\"",
        "Document: \"Shortest Path Adjusted Similarity Metrics for Resolving Boundary Perturbations in Scaffold Images for Tissue Engineering. The degree of match between the delineation result produced by a segmentation technique and the ground truth can be assessed using robust \"presence-absence\" resemblance measures. Previously [1], we had investigated and introduced an exhaustive list of similarity indices for assessing multiple segmentation techniques. However, these measures are highly sensitive to even minor boundary perturbations which imminently manifest in the segmentations of random biphasic spaces reminiscent of the stochastic pore-solid distributions in the tissue engineering scaffolds. This paper investigates the ideas adapted from ecology to emphasize global resemblances and ignore minor local dissimilarities. It uses concepts from graph theory to perform controlled local mutations in order to maximize the similarities. The effect of this adjustment is investigated on a comprehensive list (forty nine) of similarity indices sensitive to the over- and underestimation errors associated with image delineation tasks.\"",
        "Document: \"Evaluating refinery supply chain policies and investment decisions through simulation-optimization. The dynamic, non-linear, and complex nature of a supply chain with numerous interactions among its entities are best evaluated using simulation models. The optimization of such system is not amenable to mathematical programming approaches. The simulation-optimization method seems to be the most promising. In this paper, we look at a refinery supply chain simulation and attempt to optimize the refinery operating policies and capacity investments by employing a genetic algorithm. The refinery supply chain is complex with multiple, distributed, and disparate entities which operate their functions based on certain policies. Policy and investment decisions have significant impact on the refinery bottom line. To optimize them, we develop a simple simulation-optimization framework by combining the refinery supply chain simulator called Integrated Refinery In Silico (IRIS) and genetic algorithm. Results indicate that the proposed framework works well for optimization of supply chain policy and investment decisions.\"",
        "Document: \"Multi-agent based collaborative fault detection and identification in chemical processes. Fault detection and identification (FDI) has received significant attention in literature. Popular methods for FDI include principal component analysis, neural-networks, and signal processing methods. However, each of these methods inherit certain strengths and shortcomings. A method that works well under one circumstance might not work well under another when different features of the underlying process come to the fore. In this paper, we show that a collaborative FDI approach that combines the strengths of various heterogeneous FDI methods is able to maximize diagnostic performance. A multi-agent framework is proposed to realize such collaboration in practice where different FDI methods, i.e: principal component analysis, self-organizing maps, non-parametric approaches, or neural-networks are combined. Since the results produced by different FDI agents might be in conflict, we use decision fusion methods to combine FDI results. Two different methods - voting-based fusion and Bayesian probability fusion are studied here. Most monitoring and fault diagnosis algorithms are computationally complex, but their results are often needed in real-time. One advantage of the multi-agent framework is that it provides an efficient means for speeding up the execution time of the various FDI methods through seamless deployment in a large-scale grid. The proposed multi-agent approach is illustrated through fault diagnosis of the startup of a lab-scale distillation unit and the Tennessee Eastman Challenge problem. Extensive testing of the proposed method shows that combining diagnostic classifiers of different types can significantly improve diagnostic performance.\"",
        "Document: \"A framework for managing transitions in chemical plants. Chemical processes operate at a multitude of steady states and frequently undergo transitions between them. Alarm management, fault diagnosis, and other automation systems are however, usually configured assuming a single state of operation. When the plant moves out of that state, these applications signal false alarms even when a desired change is occurring. In this paper, we propose a framework that would enable these applications to be state-conscious and reconfigure themselves to remain relevant in any process state. Process states are demarcated into modes and transitions corresponding to quasi-steady state and transient operations, respectively. Modes are characterized using variable ranges and transitions using key variables and their trends. A trend analysis-based approach for locating and characterizing the modes and transitions in historical data is proposed. A supervisory system that tracks the state of the process units online is also developed. The application to two case studies \u2013 startup and grade change transitions in a pilot scale distillation column and startup and shutdown of a refinery catalytic cracking unit simulation \u2013 is reported. The benefits of the framework are illustrated through dynamic state-specific alarm reconfiguration during a startup.\"",
        "1 is \"An Evaluation of Over-Fit Control Strategies for Multi-Objective Evolutionary Optimization\", 2 is \"Multi-technology distributed obbjects and their integration\"",
        "Given above information, for an author who has written the paper with the title \"Using the OPC Standard for Real-Time Process Monitoring and Control\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004844": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Head detection for video surveillance based on categorical hair and skin colour models':",
        "Document: \"Fully Automatic Analysis of Engagement and Its Relationship to Personality in Human-Robot Interactions. Engagement is crucial to designing intelligent systems that can adapt to the characteristics of their users. This paper focuses on the automatic analysis and classification of engagement based on humans&#39; and robot&#39;s personality profiles in a triadic human-human-robot interaction setting. More explicitly, we present a study that involves two participants interacting with a humanoid robot, and inves...\"",
        "Document: \"Mixtures of Normalized Linear Projections. High dimensional spaces pose a challenge to any classification task. In fact, these spaces contain much redundancy and it becomes crucial to reduce the dimensionality of the data to improve analysis, density modeling, and classification. In this paper, we present a method for dimensionality reduction in mixture models and its use in classification. For each component of the mixture, the data are projected by a linear transformation onto a lower-dimensional space. Subsequently, the projection matrices and the densities in such compressed spaces are learned by means of an Expectation Maximization (EM) algorithm. However, two main issues arise as a result of implementing tins approach, namely: 1) the scale of the densities can be different across the mixture components and 2) a. singularity problem may occur. We suggest solutions to these problems and validate the proposed method on three image data sets from the UCI Machine Learning Repository. The classification performance is compared with that of a mixture of probabilistic principal component analysers (MPPCA). Across the three data sets, our accuracy always compares favourably, with improvements ranging from 2.5% to 35.4%.\"",
        "Document: \"Automatic analysis of facial attractiveness from video. There has been a growing interest in the computer science field for automatic analysis and recognition of facial beauty and attractiveness. Most of the proposed studies attempt to model and predict facial attractiveness using a single static facial image. While a static image provides limited information about facial attractiveness, using a video clip that contains information about the motion and the dynamic behaviour of the face provides a richer understanding and valuable insights into analysing facial attractiveness. With this motivation, we propose to use dynamic features obtained from video clips along with static features obtained from static frames for automatic analysis of facial attractiveness. Support Vector Machine (SVM) and Random Forest (RF) are utilised to create and train models of attractiveness using the features extracted. Experimental results show that combining static and dynamic features improve performance over using either of these feature sets alone, and SVM provides the best prediction performance.\"",
        "Document: \"Automatic Recognition of Emotions and Membership in Group Videos. Automatic affect analysis and understanding has become a well established research area in the last two decades. However, little attention has been paid to the analysis of the affect expressed in group settings, either in the form of affect expressed by the whole group collectively or affect expressed by each individual member of the group. This paper presents a framework which, in group settings automatically classifies the affect expressed by each individual group member along both arousal and valence dimensions. We first introduce a novel Volume Quantised Local Zernike Moments Fisher Vectors (vQLZM-FV) descriptor to represent the facial behaviours of individuals in the spatio-temporal domain and then propose a method to recognize the group membership of each individual (i.e., which group the individual in question is part of) by using their face and body behavioural cues. We conduct a set of experiments on a newly collected dataset that contains fourteen recordings of four groups, each consisting of four people watching affective movie stimuli. Our experimental results show that (1) the proposed vQLZM-FV outperforms the other feature representations in affect recognition, and (2) group membership can be recognized using the non-verbal face and body features, indicating that individuals influence each other's behaviours within a group setting.\"",
        "Document: \"Measuring Affect for the Study and Enhancement of Co-present Creative Collaboration. Affective computing research has tended to focus on the recognition of emotional states in individuals, with the intention of enhancing human-computer interaction. In this paper we advocate the need for a shift of attention towards emotional communication between people. To contextualise our views we discuss the ways in which rapid technological advances have impacted society and human psychology over the last decade. By outlining our doctoral research topic, we then highlight how affective computing based research could help us understand and enhance co-present human-human interactions. We are especially interested in studying situations where the interaction is directed towards collaborative creativity, as there is little existing work in this area and we see great potential for real-world applications to stem from our research.\"",
        "Document: \"Automatic Visual Recognition of Face and Body Action Units. Expressive face and body gestures are among the main non-verbal communication channels in human-human interaction. Understanding human emotions through these nonverbal means is one of the necessary skills both for humans and also for the computers to interact intelligently and effectively with their human counterparts. Much progress has been achieved in affect assessment using a single measure type; however, reliable assessment typically requires the concurrent use of multiple modalities. Accordingly in this paper, we present preliminary results of automatic visual recognition of expressive face and upper-body action units (FAUs and BAUs) suitable for use in a vision-based affective multimodal framework.\"",
        "Document: \"MAPTRAITS 2014: The First Audio/Visual Mapping Personality Traits Challenge. The Audio/Visual Mapping Personality Challenge and Workshop (MAPTRAITS) is a competition event aimed at the comparison of signal processing and machine learning methods for automatic visual, vocal and/or audio-visual analysis of personality traits and social dimensions, namely, extroversion, agreeableness, conscientiousness, neuroticism, openness, engagement, facial attractiveness, vocal attractiveness, and likability. The MAPTRAITS Challenge aims to bring forth existing efforts and major accomplishments in modelling and analysis of personality and social dimensions in both quantised and continuous time and space. This paper provides the details of the two Sub-Challenges, their conditions, datasets, labels and baseline features that made available to the researchers who are interested in taking part in this challenge and workshop.\"",
        "Document: \"Categorical and dimensional affect analysis in continuous input: Current trends and future directions. In the context of affective human behavior analysis, we use the term continuous input to refer to naturalistic settings where explicit or implicit input from the subject is continuously available, where in a human-human or human-computer interaction setting, the subject plays the role of a producer of the communicative behavior or the role of a recipient of the communicative behavior. As a result, the analysis and the response provided by the automatic system are also envisioned to be continuous over the course of time, within the boundaries of digital machine output. The term continuous affect analysis is used as analysis that is continuous in time as well as analysis that uses affect phenomenon represented in dimensional space. The former refers to acquiring and processing long unsegmented recordings for detection of an affective state or event (e.g., nod, laughter, pain), and the latter refers to prediction of an affect dimension (e.g., valence, arousal, power). In line with the Special Issue on Affect Analysis in Continuous Input, this survey paper aims to put the continuity aspect of affect under the spotlight by investigating the current trends and provide guidance towards possible future directions.\"",
        "Document: \"Audio-Visual Classification and Fusion of Spontaneous Affective Data in Likelihood Space. This paper focuses on audio-visual (using facial expression, shoulder and audio cues) classification of spontaneous affect, utilising generative models for classification (i) in terms of Maximum Likelihood Classification with the assumption that the generative model structure in the classifier is correct, and (ii) Likelihood Space Classification with the assumption that the generative model structure in the classifier may be incorrect, and therefore, the classification performance can be improved by projecting the results of generative classifiers onto likelihood space, and then using discriminative classifiers. Experiments are conducted by utilising Hidden Markov Models for single cue classification, and 2 and 3-chain coupled Hidden Markov Models for fusing multiple cues and modalities. For discriminative classification, we utilise Support Vector Machines. Results show that Likelihood Space Classification improves the performance (91.76%) of Maximum Likelihood Classification (79.1%). Thereafter, we introduce the concept of fusion in the likelihood space, which is shown to outperform the typically used model-level fusion, attaining a classification accuracy of 94.01% and further improving all previous results.\"",
        "Document: \"Continuous Prediction of Spontaneous Affect from Multiple Cues and Modalities in Valence-Arousal Space. Past research in analysis of human affect has focused on recognition of prototypic expressions of six basic emotions based on posed data acquired in laboratory settings. Recently, there has been a shift toward subtle, continuous, and context-specific interpretations of affective displays recorded in naturalistic and real-world settings, and toward multimodal analysis and recognition of human affect. Converging with this shift, this paper presents, to the best of our knowledge, the first approach in the literature that: 1) fuses facial expression, shoulder gesture, and audio cues for dimensional and continuous prediction of emotions in valence and arousal space, 2) compares the performance of two state-of-the-art machine learning techniques applied to the target problem, the bidirectional Long Short-Term Memory neural networks (BLSTM-NNs), and Support Vector Machines for Regression (SVR), and 3) proposes an output-associative fusion framework that incorporates correlations and covariances between the emotion dimensions. Evaluation of the proposed approach has been done using the spontaneous SAL data from four subjects and subject-dependent leave-one-sequence-out cross validation. The experimental results obtained show that: 1) on average, BLSTM-NNs outperform SVR due to their ability to learn past and future context, 2) the proposed output-associative fusion framework outperforms feature-level and model-level fusion by modeling and learning correlations and patterns between the valence and arousal dimensions, and 3) the proposed system is well able to reproduce the valence and arousal ground truth obtained from human coders.\"",
        "1 is \"Detecting Human Behavior Models From Multimodal Observation in a Smart Home\", 2 is \"Local binary patterns for multi-view facial expression recognition\"",
        "Given above information, for an author who has written the paper with the title \"Head detection for video surveillance based on categorical hair and skin colour models\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004858": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Airborne vision-based mapping and classification of large farmland environments':",
        "Document: \"Learning to close loops from range data. In this paper we address the loop closure detection problem in simultaneous localization and mapping ( slam ), and present a method for solving the problem using pairwise comparison of point clouds in both two and three dimensions. The point clouds are mathematically described using features that capture important geometric and statistical properties. The features are used as input to the machine learning algorithm AdaBoost, which is used to build a non-linear classifier capable of detecting loop closure from pairs of point clouds. Vantage point dependency in the detection process is eliminated by only using rotation invariant features, thus loop closure can be detected from an arbitrary direction. The classifier is evaluated using publicly available data, and is shown to generalize well between environments. Detection rates of 66%, 63% and 53% for 0% false alarm rate are achieved for 2D outdoor data, 3D outdoor data and 3D indoor data, respectively. In both two and three dimensions, experiments are performed using publicly available data, showing that the proposed algorithm compares favourably with related work.\"",
        "Document: \"Shared environment representation for a human-robot team performing information fusion. This paper addresses the problem of building a shared environment representation by a human-robot team. Rich environment models are required in real applications for both autonomous operation of robots and to support human decision-making. Two probabilistic models are used to describe outdoor environment features such as trees: geometric (position in the world) and visual. The visual representation is used to improve data association and to classify features. Both models are able to incorporate observations from robotic platforms and human operators. Physically, humans and robots form a heterogeneous sensor network. In our experiments, the human-robot team consists of an unmanned air vehicle, a ground vehicle, and two human operators. They are deployed for an information gathering task and perform information fusion cooperatively. All aspects of the system including the fusion algorithms are fully decentralized. Experimental results are presented in form of the acquired multi-attribute feature map, information exchange patterns demonstrating human-robot information fusion, and quantitative model evaluation. Learned lessons from deploying the system in the field are also presented. \u00a9 2007 Wiley Periodicals, Inc.\"",
        "Document: \"Learning highly dynamic environments with stochastic variational inference. Understanding the dynamics of urban environments is crucial for path planning and safe navigation. However, the dynamics might be extremely complex making learning the environment an unfathomable task. Within the methods available for learning dynamic environments, dynamic Gaussian process occupancy maps (DGPOM) are very attractive because they can produce spatially-continuous occupancy maps taking into account neighborhood information, and provide probabilistic estimates, naturally inferring the uncertainty of predictions. Despite these properties, they are extremely slow, especially in dynamic mapping where the parameters of the map have to be updated as new data arrive from range sensors such as LiDARs. In this work, we leverage recent advancements in stochastic variational inference (SVI) to quickly learn dynamic areas in an online fashion. Further, we propose an information-driven technique to \u201cintelligently\u201d select inducing points required for SVI without relying on any object trackers which essentially improves computational time as well as robustness. These long-term occupancy maps entertain all attractive properties of DGPOM while the learning process is significantly faster, yet accurate. Our experiments with both simulation and real robot data on road intersections show a significant improvement in speed while maintaining a comparable or better accuracy.\"",
        "Document: \"Expected Similarity Estimation for Large-Scale Batch and Streaming Anomaly Detection. We present a novel algorithm for anomaly detection on very large datasets and data streams. The method, named EXPected Similarity Estimation (expose), is kernel-based and able to efficiently compute the similarity between new data points and the distribution of regular data. The estimator is formulated as an inner product with a reproducing kernel Hilbert space embedding and makes no assumption about the type or shape of the underlying data distribution. We show that offline (batch) learning with exposecan be done in linear time and online (incremental) learning takes constant time per instance and model update. Furthermore, exposecan make predictions in constant time, while it requires only constant memory. In addition, we propose different methodologies for concept drift adaptation on evolving data streams. On several real datasets we demonstrate that our approach can compete with state of the art algorithms for anomaly detection while being an order of magnitude faster than most other approaches.\"",
        "Document: \"Hyperparameter Learning for Conditional Mean Embeddings with Rademacher Complexity Bounds. Conditional kernel mean embeddings are nonparametric models that encode conditional expectations in a reproducing kernel Hilbert space. While they provide a flexible and powerful framework for probabilistic inference, their performance is highly dependent on the choice of kernel and regularization hyperparameters. Nevertheless, current hyperparameter tuning methods predominantly rely on expensive cross validation or heuristics that is not optimized for the inference task. For conditional kernel mean embeddings with categorical targets and arbitrary inputs, we propose a hyperparameter learning framework based on Rademacher complexity bounds to prevent overfitting by balancing data fit against model complexity. Our approach only requires batch updates, allowing scalable kernel hyperparameter tuning without invoking kernel approximations. Experiments demonstrate that our learning framework outperforms competing methods, and can be further extended to incorporate and learn deep neural network weights to improve generalization. (Source code available at: https://github.com/Kelvin-Hsu/cake).\"",
        "Document: \"CRF-Matching: Conditional Random Fields for Feature-Based Scan Matching. Matching laser range scans observed at different points in time is a crucial component of many robotics tasks, including mobile robot localization and mapping. While existing techniques such as the Iterative Closest Point (ICP) algorithm perform well under many circumstances, they often fail when the initial estimate of the offset between scans is highly uncertain. This paper presents a novel approach to 2D laser scan matching. CRF-Matching generates a Condition Random Field (CRF) to reason about the joint association between the measurements of the two scans. The approach is able to consider arbitrary shape and appearance features in order to match laser scans. The model parameters are learned from labeled training data. Inference is performed efficiently using loopy belief propagation. Experiments using data collected by a car navigating through urban environments show that CRF-Matching is able to reliably and efficiently match laser scans even when no a priori knowledge about their offset is given. They additionally demonstrate that our approach can seamlessly integrate camera information, thereby further improving performance.\"",
        "Document: \"An integrated probabilistic model for scan-matching, moving object detection and motion estimation. This paper presents a novel framework for integrating fundamental tasks in robotic navigation through a statistical inference procedure. A probabilistic model that jointly reasons about scan-matching, moving object detection and their motion estimation is developed. Scan-matching and moving object detection are two important problems for full autonomy of robotic systems in complex dynamic environments. Popular techniques for solving these problems usually address each task in turn disregarding important dependencies. The model developed here jointly reasons about these tasks by performing inference in a probabilistic graphical model. It allows different but related problems to be expressed in a single framework. The experiments demonstrate that jointly reasoning results in better estimates for both tasks compared to solving the tasks individually.\"",
        "Document: \"Multi-kernel Gaussian processes. Multi-task learning remains a difficult yet important problem in machine learning. In Gaussian processes the main challenge is the definition of valid kernels (covariance functions) able to capture the relationships between different tasks. This paper presents a novel methodology to construct valid multi-task covariance functions (Mercer kernels) for Gaussian processes allowing for a combination of kernels with different forms. The method is based on Fourier analysis and is general for arbitrary stationary covariance functions. Analytical solutions for cross covariance terms between popular forms are provided including Mat\u00e9rn, squared exponential and sparse covariance functions. Experiments are conducted with both artificial and real datasets demonstrating the benefits of the approach.\"",
        "Document: \"Recognising and Modelling Landmarks to Close Loops in Outdoor SLAM. In this paper, simultaneous localisation and mapping (SLAM) is combined with landmark recognition to close large loops in unstructured, outdoor environments. Camera and laser information are fused to recognise and create appearance models for landmarks. The representation is obtained through a non-linear probabilistic regression model encoding a neighbourhood preserving dimensionality reduction. A new data association algorithm is proposed where landmarks are associated based on both position and appearance. The resulting system is more robust and able to recover from possible misassociations. Experiments demonstrate the benefits of this approach in challenging problems involving mapping with large loop closings in irregular terrain, and with dynamic objects.\"",
        "Document: \"Contextual occupancy maps incorporating sensor and location uncertainty. This paper describes a method of incorporating sensor and localisation uncertainty into contextual occupancy maps to provide for robust mapping. This paper builds on a recently proposed application of the Gaussian process (GP) to occupancy mapping. An extension of GPs is employed which incorporates uncertain inputs into the covariance function. In turn, this allows statistically consistent, multi-resolution maps to be constructed which exploit the spatial inference properties of GPs while correctly accounting for sensor and localisation errors. Experiments are described, with both synthetic and real data, which show the benefits of complete uncertainty modeling and how contextual occupancy maps may be constructed by fusing data from different sensors on different robots in a common probabilistic representation.\"",
        "1 is \"Adaptive bayesian filtering for vibration-based terrain classification\", 2 is \"Real-Time Vision-Based Relative Aircraft Navigation.\"",
        "Given above information, for an author who has written the paper with the title \"Airborne vision-based mapping and classification of large farmland environments\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004962": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Alternating, pattern-avoiding permutations':",
        "Document: \"The homogeneous q-difference operator. We introduce a q-differential operator D\"x\"y on functions in two variables which turns out to be suitable for dealing with the homogeneous form of the q-binomial theorem as studied by Andrews, Goldman, and Rota, Roman, Ihrig, and Ismail, et al. The homogeneous versions of the q-binomial theorem and the Cauchy identity are often useful for their specializations of the two parameters. Using this operator, we derive an equivalent form of the Goldman-Rota binomial identity and show that it is a homogeneous generalization of the q-Vandermonde identity. Moreover, the inverse identity of Goldman and Rota also follows from our unified identity. We also obtain the q-Leibniz formula for this operator. In the last section, we introduce the homogeneous Rogers-Szego polynomials and derive their generating function by using the homogeneous q-shift operator.\"",
        "Document: \"Equivalence classes of matchings and lattice-square designs. We enumerate nonisomorphic lattice-square designs yielded by a conventional construction. Constructed designs are specified by words composed from finite-field elements. These words are permuted by the isomorphism group in question. The latter group contains a direct-product subgroup, acting, respectively, upon the positions and identities of the finite-field elements. We review enumeration theory for such direct-product groups. This subgroup is a direct product of a hyperoctahedral and a dihedral group, with the orbits of the hyperoctahedral group, acting on the positions of the field elements, interpretable as perfect matchings. Thus, the enumeration of dihedral equivalence classes of perfect matchings provides an upper bound on the number of nonisomorphic, constructed designs. The full isomorphism group also contains non-direct-product elements, and the isomorphism classes are enumerated using Burnside's Lemma: counting the number of orbits of a normal subgroup fixed by the quotient group. This approach is applied to constructed lattice-square designs of odd, prime-power order \u226413.\"",
        "Document: \"Noncrossing linked partitions and large (3,2) -Motzkin paths. Noncrossing linked partitions arise in the study of certain transforms in free probability theory. We explore the connection between noncrossing linked partitions and (3,2)-Motzkin paths, where a (3,2)-Motzkin path can be viewed as a Motzkin path for which there are three kinds of horizontal steps and two kinds of down steps. A large (3,2)-Motzkin path is a (3,2)-Motzkin path for which there are only two kinds of horizontal steps on the x-axis. We establish a one-to-one correspondence between the set of noncrossing linked partitions of {1,...,n+1} and the set of large (3,2)-Motzkin paths of length n, which leads to a simple explanation of the well-known relation between the large and the little Schroder numbers.\"",
        "Document: \"Derangement Polynomials and Excedances of Type B. Based on the notion of excedances of type B introduced by Brenti, we give a type B analogue of the derangement polynomials. The connection between the derangement polynomials and Eulerian polynomials naturally extends to the type B case. Using this relation, we derive some basic properties of the derangement polynomials of type B, including the generating function formula, the Sturm sequence property, and the asymptotic normal distribution. We also show that the derangement polynomials are almost symmetric in the sense that the coefficients possess the spiral property.\"",
        "Document: \"q-Hook length formulas for signed labeled forests. A signed labeled forest is defined as a (plane) forest labeled by 1,2,...,n along with minus signs associated with some vertices. Signed labeled forests can be viewed as an extension of signed permutations. We define the inversion number, the flag major index and the R-major index of a signed labeled forest, which can be considered as type B analogues of the statistics for a labeled forest introduced by Bjorner and Wachs. The flag major index of a signed labeled forest is based on the flag major index of a signed permutation defined by Adin and Roichman. We introduce the R-major index of a signed permutation based on the natural order, and we show that it can be expressed by the major index defined by Reiner via a bijection. Then we define the R-major index of a signed labeled forest. We obtain q-hook length formulas by q-counting signed labelings of a given forest with respect to the above three indices and we show that these statistics are equidistributed over signed labeled forests. Our formulas can be viewed as type B analogues of the formulas due to Bjorner and Wachs. We also give a type D analogue with respect to the inversion number of an even-signed labeled forest.\"",
        "Document: \"Labeled partitions with colored permutations. In this paper, we extend the notion of labeled partitions with ordinary permutations to colored permutations. We use this structure to derive the generating function of the fmaj\"k indices of colored permutations. We further give a combinatorial treatment of a relation on the q-derangement numbers with respect to colored permutations. Based on labeled partitions, we provide an involution that implies the generating function formula due to Gessel and Simon for signed q-counting of the major indices. This involution can be extended to signed permutations. This gives a combinatorial interpretation of a formula of Adin, Gessel and Roichman.\"",
        "Document: \"Converging to Gosper's algorithm. Given two polynomials, we find a convergence property of the GCD of the rising factorial and the falling factorial. Based on this property, we present a unified approach to computing the universal denominators as given by Gosper's algorithm and Abramov's algorithm for finding rational solutions to linear difference equations with polynomial coefficients. Our approach easily extends to the q-analogues.\"",
        "Document: \"On the Positive Moments of Ranks of Partitions. By introducing k-marked Durfee symbols, Andrews found a combinatorial interpretation of the 2k-th symmetrized moment n(2k)(n)of ranks of partitions of n in terms of (k + 1)-marked Durfee symbols of n. In this paper, we consider the k-th symmetrized positive moment (n) over bar (k)(n) of ranks of partitions of n which is defined as the truncated sum over positive ranks of partitions of n. As combinatorial interpretations of (n) over bar (2k)(n) and (n) over bar (2k-1)(n), we show that for given k and i with 1 <= k + 1, (n) over bar (2k-1)(n) equals the number of (k + 1)-marked Durfee symbols of n with the i-th rank being zero and (n) over bar (2k)(n) equals the number of (k + 1)-marked Durfee symbols of n with the i-th rank being positive. The interpretations of (n) over bar (2k-1)(n) and (n) over bar (2k)(n) are independent of i, and they imply the interpretation of (n) over bar (2k)(n) given by Andrews since n(2k)(n) equals (n) over bar (2k-1)(n) plus twice of (n) over bar (2k)(n). Moreover, we obtain the generating functions of (n) over bar (2k)(n) and (n) over bar (2k-1)(n).\"",
        "Document: \"Combinatorial telescoping for an identity of Andrews on parity in partitions. Recently Andrews proposed a problem of finding a combinatorial proof of an identity on the q-little Jacobi polynomials. We give a classification of certain triples of partitions and find bijections based on this classification. By the method of combinatorial telescoping for identities on sums of positive terms, we establish a recurrence relation that leads to the identity of Andrews.\"",
        "Document: \"Linked partitions and linked cycles. The notion of noncrossing linked partition arose from the study of certain transforms in free probability theory. It is known that the number of noncrossing linked partitions of [n+1] is equal to the n-th large Schroder number r\"n, which counts the number of Schroder paths. In this paper we give a bijective proof of this result. Then we introduce the structures of linked partitions and linked cycles. We present various combinatorial properties of noncrossing linked partitions, linked partitions, and linked cycles, and connect them to other combinatorial structures and results, including increasing trees, partial matchings, k-Stirling numbers of the second kind, and the symmetry between crossings and nestings over certain linear graphs.\"",
        "1 is \"Application of active contours for photochromic tracer flow extraction.\", 2 is \"Growth diagrams, and increasing and decreasing chains in fillings of Ferrers shapes\"",
        "Given above information, for an author who has written the paper with the title \"Alternating, pattern-avoiding permutations\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "004997": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Dynamic Slip-Ratio Estimation and Control of Antilock Braking Systems Using an Observer-Based Direct Adaptive Fuzzy\u2013Neural Controller':",
        "Document: \"Robust adaptive controller design for a class of uncertain nonlinear systems using online T-S fuzzy-neural modeling approach. This paper proposes a novel method of online modeling and control via the Takagi-Sugeno (T-S) fuzzy-neural model for a class of uncertain nonlinear systems with some kinds of outputs. Although studies about adaptive T-S fuzzy-neural controllers have been made on some nonaffine nonlinear systems, little is known about the more complicated uncertain nonlinear systems. Because the nonlinear functions of the systems are uncertain, traditional T-S fuzzy control methods can model and control them only with great difficulty, if at all. Instead of modeling these uncertain functions directly, we propose that a T-S fuzzy-neural model approximates a so-called virtual linearized system (VLS) of the system, which includes modeling errors and external disturbances. We also propose an online identification algorithm for the VLS and put significant emphasis on robust tracking controller design using an adaptive scheme for the uncertain systems. Moreover, the stability of the closed-loop systems is proven by using strictly positive real Lyapunov theory. The proposed overall scheme guarantees that the outputs of the closed-loop systems asymptotically track the desired output trajectories. To illustrate the effectiveness and applicability of the proposed method, simulation results are given in this paper.\"",
        "Document: \"Hierarchical T-S fuzzy-neural control of anti-lock braking system and active suspension in a vehicle. This paper proposes a novel method for identification and robust adaptive control of an anti-lock braking system with an active suspension system by using the hierarchical Takagi\u2013Sugeno ( T \u2013 S ) fuzzy-neural model. The goal of a conventional ABS control system is to rapidly eliminate tracking error between the actual slip ratio and a set reference value in order to bring the vehicle to a stop in the shortest time possible. However, braking time and stopping distance can be reduced even further if the same control system also simultaneously considers the state of the active suspension system. The structure learning capability of the proposed hierarchical  T \u2013 S  fuzzy-neural network is exploited to reduce computational time, and the number of fuzzy rules. Thus, this proposed controller is applied to achieve integrated control over the anti-lock braking system (ABS) with the active suspension system. Our simulation results, presented at the end of this paper, show that the proposed controller is extremely effective in integrated control over the ABS and the active suspension system.\"",
        "Document: \"On-Line Hybrid Intelligent Tracking Control for a Class of Nonaffine Multivariable Systems. A novel B-spline neural backstepping controller design with mean-value approximation and first-order filters is proposed for a class of uncertain multiple-input\u2013multiple-output nonaffine nonlinear systems. By combining the proposed systematic backstepping design technique with B-spline neural network structure, one not only has the improved tracking performance but also reduces the computation time. Moreover, using the proposed control scheme, the problems of higher-order derivative and complexity explosion can be solved. According to the stability analysis, it is proven that the tracking errors can be made small by tuning adjustable parameters appropriately. Finally, simulation results are provided to confirm the effectiveness and applicability of the proposed control scheme.\"",
        "Document: \"Soft Computing for Battery State-of-Charge (BSOC) Estimation in Battery String Systems. In this paper, a soft computing technique for estimating battery state-of-charge of individual batteries in a battery string is proposed. The soft computing approach uses a fusion of a fuzzy neural network (FNN) with B-spline membership functions (BMFs) and a reduced-form genetic algorithm (RGA). The algorithm is employed to tune both control points of the BMFs and the weights of the FNNs. The tra...\"",
        "Document: \"Predictor-Based Fuzzy Adaptive Containment Control for Nonlinear Multiagent Systems With Actuator Nonlinearity and Unmeasurable States. This article investigates the containment control problem of nonlinear multiagent systems subject to actuator nonlinearity, unmeasured states, and unknown external disturbances. The predictor technique is employed for each subsystem, and the prediction error is utilized to update the adaptive law. The problem of actuator nonlinearity with dead zone and saturation input is solved by introducing an auxiliary control signal. Incorporating the state observer and disturbance ones, an extended state observer is proposed to compensate the effect of unmeasurable and unknown disturbances, simultaneously. Based on Lyapunov theory and graph theory, it is shown that the devised control scheme guarantees that all the signals in the closed-loop system are semi-globally uniformly ultimately bounded. Moreover, the followers can enter the dynamic convex hull spanned by the dynamic leaders. A simulation example with comparison analysis is provided to illustrate the effectiveness of the theoretical results.\"",
        "Document: \"Design of sliding mode controllers for bilinear systems with time varying uncertainties. Sliding mode controllers for the bilinear systems with time varying uncertainties are developed in this paper. The bilinear coefficient matching condition which is similar to the traditional matching condition for linear system is defined for the homogeneous bilinear systems. It can be seen that the bilinear coefficient matching condition is very limited and is not generally applicable to the nonhomogeneous bilinear system. Thus, the sliding coefficient matching condition is also considered for the bilinear systems with time varying uncertainties. Then, the sufficient conditions are provided for the reaching mode of the time varying uncertain bilinear systems to be guaranteed by the designed sliding mode controllers. Moreover, the stability of the uncertain bilinear systems with the sliding mode controller is discussed. Simulation results are included to illustrate the effectiveness of the proposed sliding mode controllers.\"",
        "Document: \"Genetic fuzzy system for servo motors with a buck converter. The paper proposes a fuzzy control method with a real-time genetic algorithm for an uncertain DC server motor with a Buck converter. The parameters of the fuzzy system are online adjusted by the real-time genetic algorithm in order to generate appropriate control input. For the purpose of on-line evaluating the stability of the closed-loop system, an energy fitness function derived from backstepping technique is involved in the genetic algorithm. According to the experimental results, the genetic fuzzy control scheme performs on-line tracking successfully.\"",
        "Document: \"Function approximation using fuzzy neural networks with robust learning algorithm. The paper describes a novel application of the B-spline membership functions (BMF's) and the fuzzy neural network to the function approximation with outliers in training data. According to the robust objective function, we use gradient descent method to derive the new learning rules of the weighting values and BMF's of the fuzzy neural network for robust function approximation. In this paper, the robust learning algorithm is derived. During the learning process, the robust objective function comes into effect and the approximated function will gradually be unaffected by the erroneous training data. As a result, the robust function approximation can rapidly converge to the desired tolerable error scope. In other words, the learning iterations will decrease greatly. We realize the function approximation not only in one dimension (curves), but also in two dimension (surfaces). Several examples are simulated in order to confirm the efficiency and feasibility of the proposed approach in this paper\"",
        "Document: \"Time-efficient structure for observer-based direct/indirect fuzzy-neural controller. This paper proposes a novel method for robust adaptive control of a class of nonlinear systems by using the proposed observer-based direct/indirect fuzzy-neural controller. Using a conventional fuzzy-neural network (FNN) encounters a seriously problem of the computation parameter explosion which leads to a huge computation time during the process of a high dimensional system. The proposed adaptive control scheme using the hybrid direct/indirect fuzzy-neural controller guarantees that the outputs of the closed-loop system asymptotically track the desired output trajectories. Finally, simulation results are presented to confirm the effectiveness and applicability of the proposed method.\"",
        "Document: \"Observer-Based T\u2013S Fuzzy Control for a Class of General Nonaffine Nonlinear Systems Using Generalized Projection-Update Laws. In this paper, we propose an online observer-based Takagi-Sugeno (T-S) fuzzy-output tracking-control technique and an improved generalized projection-update law for a class of general nonaffine nonlinear systems with unknown functions and external disturbances. First, a T-S fuzzy model and a mean-value estimation technique are adopted to approximate a so-called virtual linearized system (VLS) of a real system and avoiding a high-order derivative problem, respectively. Second, a novel design concept combining the T-S fuzzy controller, observer, and tuning algorithm by neural networks is proposed to improve system performance. After that, we also use improved generalized projection-update laws, which prevent parameters drift and confine adjustable parameters to the specified regions, to tune adjustable parameters. As a result, both the stability guarantee based on strictly positive real (SPR) Lyapunov theory and Barbalat's lemma and the better tracking performance are concluded. To illustrate the effectiveness of the proposed T-S fuzzy controller and observer-design methodology, numerical simulation results are given in this paper.\"",
        "1 is \"The adaptive control of nonlinear systems using the Sugeno-type of fuzzy logic\", 2 is \"Synthesis Of Fuzzy Model-Based Designs To Synchronization And Secure Communications For Chaotic Systems\"",
        "Given above information, for an author who has written the paper with the title \"Dynamic Slip-Ratio Estimation and Control of Antilock Braking Systems Using an Observer-Based Direct Adaptive Fuzzy\u2013Neural Controller\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005042": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Optimal PN sequence design for quasi-synchronous CDMA communication systems':",
        "Document: \"Intraprediction scheme for pipelined processing in high-efficiency video coding encoder. We propose an intraprediction scheme that enables the pipelined processing to reduce processing time in the high-efficiency video coding (HEVC) encoder. In the standard HEVC encoder, a rough mode decision (RMD) is used to reduce the complexity and processing time for intraprediction. However, because the RMD of the current block cannot be performed before the reconstruction of the previous block, intraprediction cannot be pipelined to reduce processing time. To remove this dependency, our proposed scheme uses the original data instead of the reconstructed data of the previous block in the RMD processing. By removing the data dependency, our proposed scheme allows block processing to be pipelined while maintaining consistency with the standard. Performance analysis results show that our proposed scheme reduces the processing time by up to 49.8%, with almost no loss in coding performance, compared to the HEVC test model 10.0. (C) 2014 SPIE and IS&T\"",
        "Document: \"Low-complexity multi-way and reconfigurable cyclic shift network of QC-LDPC decoder for Wi-Fi/WIMAX applications. This paper proposes a cyclic shift decomposition (CSD) algorithm to perform multi-way cyclic shifts with low complexity in the quasi-cyclic low-density parity-check (QCLDPC) decoder. The proposed algorithm decomposes the cyclic shift into a common cyclic shift and a private cyclic shift. Based on the proposed CSD algorithm, a low-complexity multi-way and reconfigurable cyclic shift network (CSN) for QC-LDPC codes is proposed. The proposed CSN is composed of the shared component, which performs the common cyclic shift, and the repeated component, which performs the private cyclic shift. Each component can support reconfigurability for given QCLDPC codes. By introducing the single-path shared component, only the complexity of the multi-path repeated component increases linearly as the number of multi-way paths increases. A complexity analysis of each component is also proposed. Based on the complexity analysis, the proposed CSN can perform multi-way and reconfigurable cyclic shifts with low complexity in the QC-LDPC decoder. The implementation results show that the areas of the proposed four-way CSN are 0.227 mm2 and 0.276 mm2 for the IEEE 802.11n/ac and IEEE 802.16e QC-LDPC codes, respectively, with 130 nm CMOS technology. The area saving per each-way is from 13.8% to 86.5% compared with previously presented works.\"",
        "Document: \"Efficient List Extension Algorithm Using Multiple Detection Orders For Soft-Output Mimo Detection. This paper proposes an efficient list extension algorithm for soft-output multiple-input-multiple-output (soft-MIMO) detection. This algorithm extends the list of candidate vectors based on the vector selected by initial detection, in order to solve the empty-set problem, while reducing the number of additional vectors. The additional vectors are obtained from multiple detection orders, from which high-quality soft-output can be generated. Furthermore, a method to reduce the complexity of the determination of the multiple detection orders is described. From simulation results for a 4 x 4 system with 16- and 64-quadrature amplitude modulations (QAM) and rate 1/2 and 5/6 duo-binary convolutional turbo code (CTC), the soft-MIMO detection to which the proposed list extension was applied showed a performance degradation of less than 0.5 dB at bit error rate (BER) of 10(-5), compared to that of the soft-output maximum-likelihood detection (soft-MLD) for all code rate and modulation pairs, while the complexity of the proposed list extension was approximately 38% and 17% of that of an existing algorithm with similar performance in a 4x4 system using 16- and 64-QAM, respectively.\"",
        "Document: \"Optimal PN sequence design for quasi-synchronous CDMA communication systems. In this paper, the problem of optimal PN sequence design for quasi-synchronous (QS) DS-CDMA communication systems is formulated, and classes of optimal PN sequences which have good pseudorandomness and small cross-correlation values for some range of offsets are designed. The new results are compared with the commonly used PN sequences, and the system performances of the QS CDMA systems using different PN sequences are also evaluated\"",
        "Document: \"Cost-efffective color filter array demosaicing using spatial correlation. In this paper, we propose a cost-effective color filter array (CFA) demosaicing method for digital still cameras in which a single CCD or CMOS image sensor is used. Since a CFA is adopted, we must interpolate missing color values in the red, green and blue channels at each pixel location. While most state-of-the-art algorithms invest a great deal of computational effort in the enhancement of the reconstructed image to overcome the color artifacts, we focus on eliminating the color artifacts with low computational complexity. Using spatial correlation of the adjacent pixels, the edge-directional information of the neighbor pixels is used for determining the edge direction of the current pixel. We apply our method to the state-of-the-art algorithms which use edge-directed methods to interpolate the missing color channels. The experiment results show that the proposed method enhances the demosaiced image quality, especially visually, by removing most of the color artifacts.\"",
        "Document: \"A frequency domain coherent detection scheme to realize DAA technique for WiMedia UWB systems. In this paper, a low complexity detection scheme is proposed to detect so-called victim systems in the WiMedia UWB environment. According to regulation of DAA in world-wide radio frequency, many detection schemes have been studied and time-domain coherent detection (TDCD) scheme is known as the best approach with respect to the detection performance. However, this scheme requires the additional filter and down-sampler to reconstruct victim signal from the received signal and these requirements lead to increase of hardware complexity. The proposed frequency-domain coherent detection (FDCD) scheme is based on the cross-correlation with frequency-domain reference sequences (FDRS), which are derived from the preamble sequence of the victim system. The detection performance is almost the same as that of the TDCD scheme and the computational complexity is about half.\"",
        "Document: \"Low-power low-complexity MIMO-OFDM baseband processor for wireless LANs. In this paper, we propose an efficient design and implementation results of a high speed 2TX-2RX multiple-input multiple-output orthogonal frequency division multiplexing (MIMO-OFDM) wireless LAN (WLAN) baseband processor. The proposed processor includes bit-parallel processing transmitter physical layer convergence procedure (TX-PLCP) processor and space-division multiplexing (SDM) symbol detector, which have been optimized for low power consumption and low hardware overhead. It was implemented using 0.18-mum CMOS technology. The proposed architecture can operate at a 40-MHz clock frequency and supports the maximum data rate of 130 Mbps. The logic gate count for the processor is 978 K and the power consumption is 62/284 mW (TX / RX), respectively.\"",
        "Document: \"Highly optimized intra prediction architecture for high resolution application. This paper proposes a new intra prediction architecture for high resolution applications. The standard intra prediction has a data dependency for the pipelined processing. To enable the pipelined intra prediction architecture, processing order changing methods and additional processing schedulers are proposed. However, previous methods are not considered for the parallel processing which is a key issue of high resolution applications, such as High Definition videos or Ultra High Definition videos. The proposed intra prediction architecture has a new scheduler and two difference calculation processing units for the high performance parallel processing in intra 4\u00d74 luminance prediction and intra 8x8 luminance prediction. The proposed architecture reduces processing time by about 43.40% compared with the standard and by about 20.10% compared with previous architecture.\"",
        "Document: \"Efficient Motion Vector Prediction Scheme For High Speed Motion Estimation In H.264/Avc. This paper proposes an efficient motion vector prediction (MVP) scheme for high speed motion estimation (ME) in H.264/AVC. Though MVP is essential for high coding efficiency, ME with MVP cannot be processed in parallel for high speed implementation due to the dependency of MVP. To eliminate this dependency, the proposed MVP uses the motion vectors in the neighboring macro blocks instead of those in the near sub-macro blocks which are used in the standard. Consequently, high speed ME is possible by using the proposed MVP and the processing time is reduced by up to 88% with insignificant coding quality degradation. The proposed MVP is verified with ME architecture which can encode SD (720 x 480) resolution video sequence in real-time at 54MHz operation clock with 284 K logic gates.\"",
        "Document: \"Exact ML Criterion Based on Semidefinite Relaxation for MIMO Systems. In this letter, we propose an exact maximum likelihood (ML) criterion based on semidefinite relaxation (SDR) in multiple-input multiple-output systems. Although a conventional SDR criterion for determining whether a symbol is the ML solution exists, its results cannot be guaranteed when noise is present. In place of the conventional criterion's positive semidefinite (PSD) discriminant, we propose a new, exact ML criterion based on the condition that all diagonal values are positive (PDV), a simple characteristic and necessary condition of PSD. The proposed criterion has a lower calculation complexity for testing than does a PSD and can ensure that the ML solution is always satisfactory.\"",
        "1 is \"Enhancing cell-edge performance: a downlink dynamic interference avoidance scheme with inter-cell coordination\", 2 is \"Fast hypergraph partition\"",
        "Given above information, for an author who has written the paper with the title \"Optimal PN sequence design for quasi-synchronous CDMA communication systems\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005203": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Utility-based QoS optimisation strategy for multi-criteria scheduling on the grid':",
        "Document: \"Load-Balancing Based Cross-Layer Elastic Resource Allocation in Mobile Cloud. The paper proposes a hybrid mobile cloud computing system, in which mobile applications can use different resources or services in local cloud and remote public cloud such as computation, storage and bandwidth. The cross-layer load-balancing based mobile cloud resource allocation optimization is proposed. The proposed approach augments local cloud service pools with public cloud to increase the probability of meeting the service level agreements. Our problem is divided by public cloud service allocation and local cloud service allocation, which is achieved by public cloud supplier, local cloud agent and the mobile user. The system status information is used in the hybrid mobile cloud computing system such as the preferences of mobile applications, energy, server load in cloud datacenter to improve resource utilization and quality of experience of mobile user. Therefore, the system status of hybrid mobile cloud is monitored continuously. The mathematical model of the system and optimization problem is given. The system design of load-balancing based cross-layer mobile cloud resource allocation is also proposed. Through extensive experiments, this paper evaluates our algorithm and other approaches from the literature under different conditions. The results of the experiments show a performance improvement when compared to the approaches from the literature.\"",
        "Document: \"A Multipath Routing for Mobile Ad Hoc Networks. With the development of mobile Ad Hoc networks, it is more important to provide QoS guarantee for multimedia application. QoS-based routing is an available method to do that, whereas most of the QoS-based routing are based on single path, which do not take full advantage of network resources. This paper puts forward a QoS-based multipath routing called QMPSR. QMPSR takes bandwidth and delay constraint into account, it can find several paths to provide QoS guarantee. Also, it can simultaneously use the node disjoint paths to transmit application data stream. The results of simulation show QMPSR routing is better than QoS-MSR.\"",
        "Document: \"A global optimization approach for three layers of computational grid stack. This paper presents a global optimization approach for three layers of grid stack. In grid environment, the Quality of Service (QoS) needs to be supported at every layer of the grid architecture. However, less attention has been paid to incorporating QoS at multiple layers of the grid architecture. The primary objective of most existing scheduling mechanisms is to improve QoS at single grid layer, while the QoS optimization at multiple grid layers is seldom considered. In the paper, we consider the requirements of fabric layer, collective layer and application layer at the same time, and propose a global optimization approach for three layers of grid stack. The paper deals with global optimization as optimization decomposition. The global optimization for three layers of grid stack can be decomposed into three sub-problems at different grid layers: resource allocation at the fabric layer, service integration at the collective layer, and application QoS optimization problem at the application layer. A distributed iterative algorithm for three-layer optimization is proposed. The convergence of the iterative algorithm is proved. The simulations are conducted to test Three-Layer Optimization Algorithm.\"",
        "Document: \"Agent based sensors resource allocation in sensor grid. Sensor enabled grid may combine real time data about physical environment with vast computational resources derived from the grid architecture. One of the major challenges of designing a sensor enabled grid is how to efficiently schedule sensor resource to user jobs across the collection of sensor resources. The paper presents an agent based scheme for assigning sensor resources to appropriate sensor grid users on the basis of negotiation results among agents. The proposed model consists of two types of agents: the sensor resource agents that represent the economic interests of the underlying sensor resource providers of the sensor grid and the sensor user agents that represent the interests of grid user application using the grid to achieve goals. Interactions between the two agent types are mediated by means of market mechanisms. We model sensor allocation problems by introducing the sensor utility function. The goal is to find a sensor resource allocation that maximizes the total profit. This paper proposes a distributed optimal sensor resource allocation algorithm. The performance evaluation of proposed algorithm is evaluated and compared with other resource allocation algorithms for sensor grid. The paper also gives the application example of proposed approach.\"",
        "Document: \"Fabric Level and Application Level QoS Guarantees in Grid Computing. In the paper, a cross-layer optimization between application layer and fabric layer is proposed. The aim is to optimize the end-to-end quality of the dynamic grid application as well as efficiently utilizing the grid resources. The application layer QoS and fabric layer QoS are closely interrelated in Grids since the upper layer service is based on the lower level's capabilities. A fabric level and application level QoS scheduling algorithm is proposed. We formulate the integrated design of resource allocation and user QoS satisfaction control into a constrained optimization problem. The optimization framework provides a layered approach to the sum utility maximization problem. The application layer adaptively adjusts user's resource demand based on the current resource conditions, while the fabric layer adaptively allocates CPU, storage and bandwidth required by the upper layer.\"",
        "Document: \"The use of economic agents under price driven mechanism in grid resource management. This paper presents multi-economic agent for grid resource management. A system model is described that allows agents representing various grid resources and grid users to interact without assuming priori cooperation. The system model consists of three layers. The lower layer is the underlying grid resource. The middle layer is the agent-based grid resource management system. It consists of three types of agent and market institution that allocates resources. The grid task agents buy resources to complete tasks. Grid resource agents charge the task agents for the amount of resource capacity allocated. Grid resource agents are registered with a Grid Manager. The third layer is the user layer at which grid request agents provide interfaces to the grid user' request. The three processes involved in grid resource management are given. A price-directed algorithm for solving the grid task agent resource allocation problem is presented. A basic performance evaluation is given. Finally, some conclusions are given.\"",
        "Document: \"Hierarchical control policy for dynamic resource management in grid virtual organization. This paper proposes a hierarchical control system in grid virtual organization. The hierarchical system can be decomposed into multiple application groups, which can be further decomposed into multiple applications. At the top of the hierarchy, the global controller controls the gross allocation of resources to the groups. At the next level down, the group controller coordinates the local deployments of all applications that consume the local allocation of resources. At the lowest level, the local controllers adjust the local resource usages to optimize the utility of single application. The hierarchical control system considers all applications and coordinates all layers of grid architecture upon any changes. According to different time granularity, we adopt a different control scheme. The global control considers all applications and coordinates three layers of grid architecture in response to large system changes at coarse time granularity, while local control adapts a single application to small changes at fine granularity. This paper adopts utility-driven cross layer optimization for grid applications to find a system wide optimization and solves the cross-layer optimization by using pricing based decomposition. A set of hierarchical utility functions is used to measure the performance of the grid system that follows the system, group and application hierarchy. This paper uses total utility to measure the overall quality of grid system. The experiments are conducted to test the performance of the hierarchical control algorithms.\"",
        "Document: \"A market-based mechanism for integration of mobile devices into mobile grids. In a mobile grid, energy resources distribution and computation workloads are not balanced within mobile devices. Some mobile devices have spare energy; some mobile devices are energy exhausted. In this paper, we present a market-based mechanism for efficient integration of mobile devices into mobile grids to optimise the system performance. All mobile devices in a mobile grid can be classified into different roles, such as buyers (consumers) and sellers (providers). Using market-based cooperation among devices, an energy saving scheme is proposed for a mobile grid. The paper is targeted to solve energy allocation of mobile devices by using the utility-based scheme. The system utility of a mobile grid is maximised when the equilibrium prices are obtained through the device market optimisation. A market-based algorithm for integration of mobile devices into a mobile grid is proposed. In order to test the performance of the algorithm, simulation is conducted by comparing with other power-aware scheduling in mobile grids.\"",
        "Document: \"Multi-Layer Resource Management in Cloud Computing. The paper studies multi-layer optimization in service oriented cloud computing to optimize the utility function of cloud computing, subject to resource constraints of an IaaS provider at the resource layer, service provisioning constraints of a SaaS provider at the service layer, and user QoS (quality of service) constraints of cloud users at application layer, respectively. The multi-layer optimization problem can be decomposed into three subproblems: cloud computing resource allocation problem, SaaS service provisioning problem, and user QoS maximization problem. The proposed algorithm decomposes the global optimization problem of cloud computing into three sub-problems via an iterative algorithm. The experiments are conducted to test the efficiency of the proposed algorithm with varying environmental parameters. The experiments also compare the performance of the proposed approach with other related work.\"",
        "Document: \"Hybrid Cloud Scheduling Method for Cloud Bursting. In the paper, we consider the hybrid cloud model used for cloud bursting, when the computational capacity of the private cloud provider is insufficient to deal with the peak number of customers' applications, the private cloud will rely on the resources leased from public cloud providers for the execution of private cloud applications. The paper proposes the model and algorithm of hybrid cloud scheduling optimization for cloud bursting. Public cloud providers and private cloud users communicate by the hybrid cloud marketplace. The hybrid cloud scheduling optimization is conducted at different levels. According to the model formulation and mathematic solutions of hybrid cloud scheduling, hybrid cloud scheduling algorithms for cloud bursting are proposed, which includes the routines of public cloud optimization, private cloud application optimization and private cloud job optimization. In the simulations, compared with other related algorithm, our proposed hybrid cloud scheduling algorithms achieve the better performance.\"",
        "1 is \"QoE-aware optimization of multimedia flow scheduling\", 2 is \"A privacy aware media gateway for connecting private multimedia clouds to limited devices\"",
        "Given above information, for an author who has written the paper with the title \"Utility-based QoS optimisation strategy for multi-criteria scheduling on the grid\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005245": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Modeling and Dimensioning Hierarchical Storage Systems for Low-Delay Video Services':",
        "Document: \"Throughput Analysis Of A Path In An Ieee 802.11 Multihop Wireless Network. In this paper, we present a methodology to analytically compute the end-to-end throughput that can be achieved on a path between two nodes in an IEEE 802.11 multihop wireless network with homogenous data transmission rates. Knowledge of achievable end-to-end throughput is important for making routing decisions and performing admission control. Unlike a wired network where the throughput of a path after accounting for overheads is roughly equal to the lowest bandwidth along the path, the throughput of a wireless multihop path is much lower than the minimum data rate along the path because of sharing of the medium by nodes within carrier sensing range of each other. Our analysis of the achievable throughput takes into account this effect of sharing of the medium in the form of Blocking and Interference. We apply our analysis to several randomly generated paths, and compare the analytical results with those from simulations to validate the analysis.\"",
        "Document: \"A novel efficient technique for traffic grooming in WDM SONET with multiple line speeds. SONET rings remain the dominant optical transport architecture in the metropolitan area. To support traffic growth on these rings, WDM technology is used to provide multiple SONET rings on the same fiber, each SONET ring running on a separate wavelength. Traffic grooming refers to intelligently arranging low speed traffic streams onto different SONET rings and selecting the proper line speed for each wavelength so as to minimize certain cost objective, such as the total cost of electronic Add-Drop Multiplexer (ADM) equipments used. The problem can be formulated as an Integer Linear Programming (ILP) problem by generalizing the formulation in [1] [2] to support multiple line speeds. Unfortunately, solving the ILP directly could be very computation intensive. In this paper, we propose a non-linear formulation instead, and then solve it by decomposing it into several smaller ILP subproblems, each can be solved separately by an ILP solver. Decomposition allows us to exploit the symmetry in the problem structure, and cut down the solution space dramatically, therefore, reducing the computation time to solve a problem. Even if we may have to terminate the computation early for large size problems, decomposition allows us to explore a larger portion of the solution space in a given amount of time, therefore, obtain better results.\"",
        "Document: \"Power-Law Tradeoffs Between Optical and Electronic Switching. Designing a transport network requires finding the most cost-effective combination of electronic and optical switching to support the given demands. This task is not trivial because the solution space is so large. Furthermore, there are many interacting design variables. In electronic switching, the cost is a function of the number of packets that are switched in the routers, which directly corresponds to the average hop count ( \u00af H). The dominant cost of optical switching is a function of the total number of wavelengths used in the network (S) or the maximum number of wavelengths used on any fiber ( W ) and the number of transceivers (L) used. In this paper, we show that there is a power-law relationship between each pair of these variables, and we quantify the power-law exponent for many different types of topologies. Our results allow a network designer to choose not only the most cost-effective combination of electronic and optical switching, but also the best fiber topology to achieve the min imal total cost.\"",
        "Document: \"Physical topology design for all-optical networks. There are many advantages of deploying an all- optical network. Unfortunately, there are still few guidelines on how to properly design the physical topology for such a network. We propose an efficient physical topology design algorithm a nd we use the asymptotic growth rate of the provisioned capacity as a metric to compare various design alternatives. A higher asymptotic growth rate translates directly into higher deployment cost for large networks. Our study shows that taking fiber len gth into consideration can lead to lower capacity requirement. We also find that a sufficiently large fiber-to-node ratio is nece ssary in order to minimize the asymptotic growth in the provisioned capacity, increase capacity utilization and minimize the need for wavelength conversion. We study a real network and find that its fiber-to-node ratio is too low. As a result, large provisi oned capacity is required and less than 55% of the capacity is usable. By increasing the ratio, we can reduce the provisioned capacity and achieve close to 80% utilization. I. I NTRODUCTION Compared to the broadcast-based optical network architec- tures (1)(2)(3)(4)(5), Wavelength Routed All-optical Network (WRAN) utilizing the WDM technology promises to greatly increase the transport capacity at much reduced cost. A connection, also known as a lightpath (6), only occupies one wavelength on each fiber link along the physical route used to connect the two end nodes. Thus, the same wavelength on other fiber links could be reused for other lightpaths to incr ease the utilization of the provisioned wavelengths. Besides the increased utilization, WRAN has many other advantages. Since a lightpath is routed transparently thro ugh the WRAN, that is, bypassing intermediate nodes without packet processing or costly opto-electronic conversion, much of the queuing delay and electronic equipment cost can be eliminated. The cost savings in electronic equipment will be significant (7), especially when the line speed is very high. Furthermore, WRAN can be easily and cheaply upgraded when the interface speed is increased. This is because optical switching is agnostic to the underlying data rate of an optic al channel (up to a certain limit because the channel bandwidth limits the maximum data rate), and thus, the intermediate optical switches do not have to be upgraded when the line speed increases. Given the high cost of deploying a WRAN, it is important to design the physical (fiber) topology to minimize the total capital investment. The total cost of deploying a WRAN is the sum of two cost components\u2014the link cost and the node cost. The link cost, i.e., the cost of laying down fibers to interconnect nodes, is a function of the total fiber length L. The node cost, i.e., the cost of the all-optical wavelength swit ch is a function of the number of wavelengths W that are provisioned on each fiber link. In general, there is a tradeoff between L and W \u2014more L will translate into less W and vice versa. One can design the physical topology to use the minimum amount of fiber by connecting the nodes using a minimum spanning tree. Even though the link (fiber) cost is at the minimum, the node (wavelength) cost will be very high. Alternatively, one can connect all node pairs using direct fibers. The node (wavelength) cost is at its minimum since W = 1. However, the link (fiber) cost will be very high. The optimum design with the minimum total (link and node) cost will be between these two extreme solutions. To pick the best topology with the minimum total cost, one has to solve the problem of designing a physical topology to minimize the number of wavelengths W required given a budget on L. We will present a comprehensive treatment of the design problem, including both a mathematical problem formulation and an efficient heuristic algorithm. Using the design algorithm we proposed, one can design the topology with the minimum total (link and node) cost by repeatedly run the algorithm for different L, comparing the resulting solutions based on the actual cost functions, and then picking the one with the lowest total cost. In order to be independent of the actual cost functions, we study the tradeoff between L and W. In addition, we also derive design principles and guidelines, which are unfortunately nonexistent as of now.\"",
        "Document: \"Optimizing Mobility Support In Large Switched Lans. With the recent explosion in wireless mobile communication, a great deal of work is taking place to solve the problem of handling mobile users in the Internet. This problem encompasses the design of schemes for tracking and routing to mobile users in various networks and subnetworks that make up the Internet infrastructure.Switched LANs today employ the transparent learning protocol which allows switches to learn about the location of users by promiscuously listening to the traffic emitted by the users. Unfortunately, as this paper will show, fast moving users render the transparent learning protocol inefficient so that high bandwidth consumption and even packet loss may occur. To prevent packets form being mis-routed or even lost, one can employ a lightweight control-based mechanisms for user tracking, based on an existing LAN-protocol known as the Generic Attribute Registration Protocol (GARP). However, this solution can lead to large databases at the switches, which can in turn significantly increase the cost of the Internet infrastructure.In this paper, we propose to combine and tune the transparent learning protocol and the GARP protocol in an efficient way as to jointly minimize packet loss, bandwidth consumption and the size of the tracking databases in a large switched network. Our optimization is shown to be effective for a wide range of mobility speeds and application characteristics, and consistent with the state of technology.\"",
        "Document: \"On Traffic Types And Service Classes In The Internet. In today's Internet, various traffic types having different characteristics and requirements (e.g., voice, video, best effort) share the same resources. In order to provide services that are appropriate to cache both the IETF and IEEE 802 have proposed a support for traffic differentiation. In both cases service classes corresponding to separate queues are identified, and packets am marked according to their class, which in turn defines the treatment they will get at each hop in the network. Using realistic models to represent each or the carious traffic types, we identify those that can be mixed in the same queue without hearing a significant loss in through-put; correspondingly, we make recommendations on how to map different traffic types to the available service classes.\"",
        "Document: \"IP Routing and Mobility. The original design of the Internet and its underlying protocols did not anticipate users to be mobile. With the growing interest in supporting mobile users and mobile computing, a great deal of work is taking place to solve this problem. For a solution to be practical, it has to integrate easily with existing Internet infrastructure and protocols, and offer an adequate migration path toward what might represent the ultimate solution. In that respect, the solution has to be incrementally scalable to handle a large number of mobile users and wide geographical scopes, and well performing so as to support all application requirements including voice and video communications and a wide range of mobility speeds. In this paper, we present a survey of the state-of-the- art and propose a multi-layer architecture for mobility in IP networks. In particular, we propose the use of extended local area networks and protocols for efficient and scalable mobility support in the Internet.\"",
        "Document: \"Packet Switching in Radio Channels: Part I--Carrier Sense Multiple-Access Modes and Their Throughput-Delay Characteristics. Absfract-Radio communication is considered as a method for providing remote terminal access to computers. Digital byte streams from each terminal are partitioned into packets (blocks) and trans- mitted in a burst mode over a shared radio channel. When many terminals operate in this fashion, transmissions may conflict with and destroy each other. A means for controlling this is for the termi- nal to sense the presence of other transmissions; this leads to a new method for multiplexing in a packet radio environment: carrier sense multiple access (CSMA). Two protocols are described for CSMA and their throughput-delay characteristics are given. These results show the large advantage CSMA provides as compared to the random ALOHA access modes.\"",
        "Document: \"Performance evaluation of ATM networks carrying constant and variable bit-rate video traffic. In this paper, we present the performance of asynchronous transfer mode (ATM) networks supporting audio/video traffic. The performance evaluation is done by means of a computer simulation model driven by real video traffic generated by encoding video sequences. We examine the glitching that occurs when the video information is not delivered on time at the receiver; we characterize various glitching quantities, such as the glitch duration, total number of macroblocks unavailable per glitch, and the maximum unavailable area per glitch. For various types of video contents, we compare the maximum number of constant bit-rate (CBR) and constant-quality variable bit-rate (CQ-VBR) video streams that can be supported by the network while meeting the same end-to-end delay constraint, the same level of encoded video quality, and the same glitch rate constraint. We show that when the video content is highly variable, many more CQ-VBR streams than CBR streams can be supported under given quality and delay constraints, while for relatively uniform video contents (as in a videoconferencing session), the number of CBR and CQ-VBR streams supportable is about the same. We also compare the results with those obtained for a 100Base-T Ethernet segment. We then consider heterogeneous traffic scenarios, and show that when video streams with different content, encoding scheme, and encoder control schemes are mixed, the results are at intermediate points compared to the homogeneous cases, and the maximum number of supportable streams of a given type can be determined in the presence of other types of video traffic by considering an \u201ceffective bandwidth\u201d for each of the stream types. We consider multihop ATM network scenarios as well, and show that the number of video streams that can be supported on a given network node is very weakly dependent on the number of hops that the video streams traverse. Finally, we also consider scenarios with mixtures of video streams and bursty traffic, and determine the effect of bursty traffic load and burst size on the video performance\"",
        "Document: \"On measurement facilities in packet radio systems. The growth of computer networks has proven both the need for and the success of resource sharing technology. A new resource sharing technique, utilizing broadcast channels, has been under development as a Packet Radio system and will shortly undergo testing. In this paper, we consider that Packet Radio system, and examine the measurement tasks necessary to support such important measurement goals as the validation of mathematical models, the evaluation of system protocols and the detection of design flaws. We describe the data necessary to measure the many aspects of network behavior, the tools needed to gather this data and the means of collecting it at a central location; all in a fashion consistent with the system protocols and hardware constraints, and with minimal impact on the system operation itself.\"",
        "1 is \"Capacity of Ad Hoc wireless networks\", 2 is \"Madf: a novel approach to add an ad-hoc overlay on a fixed cellular infrastructure\"",
        "Given above information, for an author who has written the paper with the title \"Modeling and Dimensioning Hierarchical Storage Systems for Low-Delay Video Services\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005265": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'An approach to specify and analyze goal model families':",
        "Document: \"From business models to service-oriented design: a reference catalog approach. Service-oriented architecture (SOA) is rapidly becoming the dominant paradigm for next generation information systems. It has been recognized, however, that the full benefits of SOA would not be realized unless its capabilities are exploited at the business level. In the business arena, innovations in e-business have led to the identification and classification of business models and analysis of their properties. To ease the transition from business design to service-oriented system design, we propose a reference catalog approach. Recurring business designs are collected, pre-analyzed, and documented as a set of reference business models, following a standardized template. Each reference business model is realized through a set of service-oriented design patterns. The i* framework is the basis for modeling and analysis at both the business and service design level, taking advantage of its agent orientation for modeling service relationships, and its goal orientation to facilitate adaptation from generic patterns to specific needs.\"",
        "Document: \"Towards a service requirements modelling ontology based on agent knowledge and intentions. In this paper, we propose a formalism for service requirements and capability modelling. It adopts concepts from the agent-oriented requirements modelling framework i*, which can be used as a means of studying the requirements and architecture for distributed agent systems. We argue that a social modelling framework such as i*, extended with the necessary service-related concepts and formal reasoning mechanisms, offers a better understanding of the social/organisational relationship in an open services world. By explicitly representing the underlying assumptions and the essential factors of services, a semiformal requirements model in i* can automatically evolve and be refined into a service requirements and capability reasoning framework. Eventually, it will assist intelligent agents with certain knowledge and intentions to make intelligent, rational decisions during service discovery, publication, selection and binding within an open services community.\"",
        "Document: \"e-Service Design Using i* and e3value Modeling. e-Services are intangible products, provisioned via the Internet. Examples include Internet access and Internet radio. Because most e-services involve multiple enterprises, creating a shared understanding of the service under development is an issue. Such an e-service is more difficult to understand than a proposition just consisting of goods because services lack easily observable physical properties. Consequently, software engineers must first understand the e-service itself before they can build effective systems and support for these services. The authors present the complementary use of two requirements engineering techniques. Using i* modeling, they explore strategic goals that enterprises have, and using e3value modeling, they understand how these goals can result in profitable services for enterprises. They demonstrate their approach using a case study on Internet radio.This article is part of a special issue on requirements engineering.\"",
        "Document: \"A repository of agile method fragments. Despite the prevalence of agile methods, few software companies adopt a prescribed agile process in its entirety. For many practitioners, agile software development is about picking up fragments from various agile methods, assembling them as a light-weight process, and deploying them in their software projects. Given the growing number of published empirical studies about using agile in different project situations, it is now possible to gain a more realistic view of what each agile method fragment can accomplish, and the necessary requisites for its successful deployment. With the aim of making this knowledge more accessible, this paper introduces a repository of agile method fragments, which organizes the evidential knowledge according to their objectives and requisites. The knowledge is gathered through systematic review of empirical studies which investigated the enactment of agile methods in various project situations. In addition, the paper proposes a modeling paradigm for visualizing the stored knowledge of method fragments, to facilitate the subsequent assembly task.\"",
        "Document: \"A goal oriented approach for modeling and analyzing security trade-offs. In designing software systems, security is typically only one design objective among many. It may compete with other objectives such as functionality, usability, and performance. Too often, security mechanisms such as firewalls, access control, or encryption are adopted without explicit recognition of competing design objectives and their origins in stakeholder interests. Recently, there is increasing acknowledgement that security is ultimately about trade-offs. One can only aim for \"good enough\" security, given the competing demands from many parties. In this paper, we examine how conceptual modeling can provide explicit and systematic support for analyzing security trade-offs. After considering the desirable criteria for conceptual modeling methods, we examine several existing approaches for dealing with security trade-offs. From analyzing the limitations of existing methods, we propose an extension to the i* framework for security trade-off analysis, taking advantage of its multi-agent and goal orientation. The method was applied to several case studies used to exemplify existing approaches.\"",
        "Document: \"Intentional modeling of social media design knowledge for government-citizen communication. Social media can be employed as powerful tools for enabling broad participation in public policy making. However, variations in the design of a social media technology system can lead to different levels or kinds of engagement, including low participation or polarized interchanges. An effective means toward learning of and analyzing the complex motivations, expectations, and actions among various actors in political communication can help designers create satisfactory social media systems. This paper uses the i* modeling framework to analyze the impact that alternative configurations of a social media technology can have on the goals and relationships of the actors involved. In doing so, we demonstrate and provide preliminary validation for a research-informed model creation and analysis approach to assessing competing design alternatives in an online climate change debate community.\"",
        "Document: \"A Semi-automated Decision Support Tool for Requirements Trade-Off Analysis. System designers and requirements analysts face many competing requirements, such as performance, usability, security, cost, and so forth. To make trade-offs among requirements, ideally analysts would like to quantitatively measure consequences of alternative solutions on requirements. However, during the early stages of requirements and system design, it is hard to quantitatively measure all factors and quantify stakeholders' preferences. The Even Swaps method is a technique developed in management science to assist in multi-criteria decision making which allows the use of available but potentially incomplete quantitative and qualitative measures. It teases out the need to elicit importance weights of requirements. Instead, stakeholders are asked how much they would relax one objective to better achieve another. We apply the Even Swaps technique to requirements trade-offs, and supplement it with an algorithm that automates the decision analysis process. The algorithm fins the most distinguishable pair of alternatives and suggests the next requirements to be swapped to stakeholders.\"",
        "Document: \"Understanding the diversity of services based on users' identities. Internet services involve complex networks of relationships among users and providers - human and automated - acting in many different capacities under interconnected and dynamic contexts. Due to the vast amount of information and services available, it is not easy for novice users to identify the right services that fit his purposes and preferences best. At the same time, it is not easy for service providers to build a service with a customizable set of features that satisfies the most people. This paper proposes to further extend the strategic actors modeling framework i* to analyze the diverse needs of users by modeling explicitly the personal characteristics, organizational positions, and service related roles. We assume that service users' needs and preferences are determined by their personal background, organizational roles, and the immediate operational context in combination. In this way, the origin of the diversity of service needs, quality preferences, and usage constraints, can be ascribed and used as a basis to make rationale selection from currently available types of services, and to reconfigure service interfaces and structures. Example usage scenarios of web services are used to illustrate the proposed approach.\"",
        "Document: \"Security and Privacy Requirements Analysis within a Social Setting. Security issues for software systems ultimately concern relationships among social actors - stakeholders, system users, potential attackers - and the software acting on their behalf. This paper proposes a methodological framework for dealing with security and privacy requirements based on i*, an agent-oriented requirements modeling language. The framework supports a set of analysis techniques. In particular, attacker analysis helps identify potential system abusers and their malicious intents. Dependency vulnerability analysis helps detect vulnerabilities in terms of organizational relationships amongstakeholders. Countermeasure analysis supports the dynamic decision-making process of defensive system players in addressing vulnerabilities and threats. Finally, access control analysis bridges the gap between security requirement models and security implementation models. The framework is illustrated with an example involving security and privacy concerns in the design of agent-based health information systems. In addition, we discuss model evaluation techniques, including qualitative goal model analysis and property verification techniques based on model checking.\"",
        "Document: \"Analyzing Knowledge Transfer Effectiveness--An Agent-Oriented Modeling Approach. Facilitating the transfer of knowledge between knowledge workers represents one of the main challenges of knowledge management. Knowledge transfer instruments, such as the experience factory concept, represent means for facilitating knowledge transfer in organizations. As past research has shown, effectiveness of knowledge transfer instruments strongly depends on their situational context, on the stakeholders involved in knowledge transfer, and on their acceptance, motivation and goals. In this paper, we introduce an agent-oriented modeling approach for analyzing the effectiveness of knowledge transfer instruments in the light of (potentially conflicting) stakeholders' goals. We apply this intentional approach to the experience factory concept and analyze under which conditions it can fail, and how adaptations to the Experience Factory can be explored in a structured way.\"",
        "1 is \"Using Abuse Case Models for Security Requirements Analysis\", 2 is \"Using VCL as an aspect-oriented approach to requirements modelling\"",
        "Given above information, for an author who has written the paper with the title \"An approach to specify and analyze goal model families\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005316": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of '@tComment: Testing Javadoc Comments to Detect Comment-Code Inconsistencies':",
        "Document: \"Temporal code completion and navigation. Modern IDEs make many software engineering tasks easier by automating functionality such as code completion and navigation. However, this functionality operates on one version of the code at a time. We envision a new approach that makes code completion and navigation aware of code evolution and enables them to operate on multiple versions at a time, without having to manually switch across these versions. We illustrate our approach on several example scenarios. We also describe a prototype Eclipse plugin that embodies our approach for code completion and navigation for Java code. We believe our approach opens a new line of research that adds a novel, temporal dimension for treating code in IDEs in the context of tasks that previously required manual switching across different code versions.\"",
        "Document: \"An Empirical Comparison of Automated Generation and Classification Techniques for Object-Oriented Unit Testing. Testing involves two major activities: generating test inputs and determining whether they reveal faults. Automated test generation techniques include random generation and symbolic execution. Automated test classification techniques include ones based on uncaught exceptions and violations of operational models inferred from manually provided tests. Previous research on unit testing for object-oriented programs developed three pairs of these techniques: model-based random testing, exception-based random testing, and exception-based symbolic testing. We develop a novel pair, model-based symbolic testing. We also empirically compare all four pairs of these generation and classification techniques. The results show that the pairs are complementary (i.e., reveal faults differently), with their respective strengths and weaknesses.\"",
        "Document: \"Optimized execution of deterministic blocks in java pathfinder. Java PathFinder (JPF) is an explicit-state model checker for Java programs. It explores all executions that a given program can have due to different thread interleavings and nondeterministic choices. JPF implements a backtracking Java Virtual Machine (JVM) that executes Java bytecodes using a special representation of JVM states. This special representation enables JPF to quickly store, restore, and compare states; it is crucial for making the overall state exploration efficient. However, this special representation creates overhead for each execution, even execution of deterministic blocks that have no thread interleavings or nondeterministic choices. We propose mixed execution, a technique that reduces execution time of deterministic blocks in JPF. JPF is written in Java as a special JVM that runs on top of a regular, host JVM. mixed execution works by translating the state between the special JPF representation and the host JVM representation. We also present lazy translation, an optimization that speeds up mixed execution by translating only the parts of the state that a specific execution dynamically depends on. We evaluate mixed execution on six programs that use JPF for generating tests for data structures and on one case study for verifying a network protocol. The results show that mixed execution can improve the overall time for state exploration up to 36.98%, while improving the execution time of deterministic blocks up to 69.15%. Although we present mixed execution in the context of JPF and Java, it generalizes to any model checker that uses a special state representation.\"",
        "Document: \"ReAssert: a tool for repairing broken unit tests. Successful software systems continuously change their requirements and thus code. When this happens, some existing tests get broken because they no longer reflect the intended behavior, and thus they need to be updated. Repairing broken tests can be time-consuming and difficult. We present ReAssert, a tool that can automatically suggest repairs for broken unit tests. Examples include replacing literal values in tests, changing assertion methods, or replacing one assertion with several. Our experiments show that ReAssert can repair many common test failures and that its suggested repairs match developers' expectations.\"",
        "Document: \"Assertion Checking in J-Sim Simulation Models of Network Protocols. Verification and validation (V&V) is a critically important phase in the development life cycle of a simulation model. In the context of network simulation, traditional network simulators perform well in using a simulation model for evaluating the performance of a network protocol but lack the capability to check the \u8292\u8059\u8079correctness\u8292\u8059\u807a of the simulation model being used. To address this problem, we have extended J-Sim\u8292\u8059\u8070an open-source component-based network simulator written entirely in Java\u8292\u8059\u8070with a state space exploration (SSE) capability that explores the state space created by a network simulation model, up to a configurable maximum depth, in order to find an execution (if any) that violates an assertion, i.e. a property specifying an invariant that must always hold true in all states. In this paper, we elaborate on the SSE framework in J-Sim and present one of our fairly complex case studies, namely verifying the simulation model of the Ad-hoc On-demand Distance Vector (AODV) routing protocol for wireless ad-hoc networks. The SSE framework makes use of protocol-specific properties along two orthogonal dimensions: state similarity and state ranking. State similarity determines whether a state is \u8292\u8059\u8079similar to\u8292\u8059\u807a another in order to enable the implementation of stateful search. State ranking determines whether a state is \u8292\u8059\u8079better than\u8292\u8059\u807a another in order to enable the implementation of best-first search (BeFS). Specifically, we develop protocol-specific search heuristics to guide SSE towards finding assertion violations in less time. We evaluate the efficiency of our SSE framework by comparing its performance with that of a state-of-the-art model checker for Java programs, namely Java PathFinder (JPF). The results of the comparison show that the time needed to find an assertion violation by our SSE framework in J-Sim can be significantly less than that in JPF unless a substantial amount of programming effort is spent in JPF to make its performance close to that of our SSE framework.\"",
        "Document: \"Using coverage criteria on RepOK to reduce bounded-exhaustive test suites. Bounded-exhaustive exploration of test case candidates is a commonly employed approach for test generation in some contexts. Even when small bounds are used for test generation, executing the obtained tests may become prohibitive, despite the time for test generation not being prohibitive. In this paper, we propose a technique for reducing the size of bounded-exhaustive test suites. This technique is based on the application of coverage criteria on the representation invariant of the structure for which the suite was produced. More precisely, the representation invariant (which is often implemented as a repOK routine) is executed to determine how its code is exercised by (valid) test inputs. Different valid test inputs are deemed equivalent if they exercise the repOK code in a similar way according to a white-box testing criterion. These equivalences between test cases are exploited for reducing test suites by removing from the suite those tests that are equivalent to some test already present in the suite. We present case studies that evaluate the effectiveness of our technique. The results show that by reducing the size of bounded-exhaustive test suites up to two orders of magnitude, we obtain test suites whose efficacy measured as their mutant-killing ability is comparable to that of bounded-exhaustive test suites.\"",
        "Document: \"Korat: automated testing based on Java predicates. This paper presents Korat, a novel framework for automated testing of Java programs. Given a formal specification for a method, Korat uses the method precondition to automatically generate all (nonisomorphic) test cases up to a given small size. Korat then executes the method on each test case, and uses the method postcondition as a test oracle to check the correctness of each output.To generate test cases for a method, Korat constructs a Java predicate (i.e., a method that returns a boolean) from the method's pre-condition. The heart of Korat is a technique for automatic test case generation: given a predicate and a bound on the size of its inputs, Korat generates all (nonisomorphic) inputs for which the predicate returns true. Korat exhaustively explores the bounded input space of the predicate but does so efficiently by monitoring the predicate's executions and pruning large portions of the search space.This paper illustrates the use of Korat for testing several data structures, including some from the Java Collections Framework. The experimental results show that it is feasible to generate test cases from Java predicates, even when the search space for inputs is very large. This paper also compares Korat with a testing framework based on declarative specifications. Contrary to our initial expectation, the experiments show that Korat generates test cases much faster than the declarative framework.\"",
        "Document: \"Detecting Redundant Unit Tests for AspectJ Programs. Aspect-oriented software development is gaining popularity with the adoption of languages such as AspectJ. Testing is an important part in any software development, including aspect-oriented development. To automate generation of unit tests for AspectJ programs, we can apply the existing tools that automate generation of unit tests for Java programs. However, these tools can generate a large number of test inputs, and manually inspecting the behavior of the software on all these inputs is time consuming. We propose Raspect, a framework for detecting redundant unit tests for AspectJ programs. We introduce three levels of units in AspectJ programs: advised methods, advice, and intertype methods. We show how to detect at each level redundant test that does not exercise new behavior. Our approach selects only non-redundant tests from the automatically generated test suites, thus allowing the developer to spend less time in inspecting this reduced set of tests. We have implemented Raspect and applied it on 12 subjects taken from a variety of sources; our experience shows that Raspect can effectively reduce the size of generated test suites for inspecting AspectJ programs\"",
        "Document: \"Symstra: a framework for generating object-oriented unit tests using symbolic execution. Object-oriented unit tests consist of sequences of method invocations. Behavior of an invocation depends on the method's arguments and the state of the receiver at the beginning of the invocation. Correspondingly, generating unit tests involves two tasks: generating method sequences that build relevant receiver-object states and generating relevant method arguments. This paper proposes Symstra, a framework that achieves both test generation tasks using symbolic execution of method sequences with symbolic arguments. The paper defines symbolic states of object-oriented programs and novel comparisons of states. Given a set of methods from the class under test and a bound on the length of sequences, Symstra systematically explores the object-state space of the class and prunes this exploration based on the state comparisons. Experimental results show that Symstra generates unit tests that achieve higher branch coverage faster than the existing test-generation techniques based on concrete method arguments.\"",
        "Document: \"Reducing the Costs of Bounded-Exhaustive Testing. Bounded-exhaustive testing is an automated testing methodology that checks the code under test for all inputs within given bounds: first the user describes a set of test inputs and provides test oracles that check test outputs; then the tool generates all the inputs, executes them on the code under test, and checks the outputs; and finally the user inspects failing tests to submit bug reports. The costs of bounded-exhaustive testing include machine time for test generation and execution (which translates into human time waiting for these results) and human time for inspection of results. This paper proposes three techniques that reduce these costs. Sparse Test Generation skips some tests to reduce the time to the first failing test. Structural Test Merging generates a smaller number of larger test inputs (rather than a larger number of smaller test inputs) to reduce test generation and execution time. Oracle-based Test Clustering groups failing tests to reduce the inspection time. Results obtained from the bounded-exhaustive testing of the Eclipse refactoring engine show that these three techniques can substantially reduce the costs while mostly preserving fault-detection capability.\"",
        "1 is \"Integrated program debugging, verification, and optimization using abstract interpretation (and the Ciao system preprocessor)\", 2 is \"First I \"",
        "Given above information, for an author who has written the paper with the title \"@tComment: Testing Javadoc Comments to Detect Comment-Code Inconsistencies\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005335": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Nonparametric Identification Of Linearizations And Uncertainty Using Gaussian Process Models - Application To Robust Wheel Slip Control':",
        "Document: \"Constrained Iterative Speech Enhancement Using Phonetic Classes. The degree of influence of noise over phonemes is not uniform since it is dependent on their distinct acoustic properties. In this study, the problem of selectively enhancing speech based on broad phoneme classes is addressed using Auto-(LSP), a constrained iterative speech enhancement algorithm. Multiple enhanced utterances are generated for every noisy utterance by varying the Auto-LSP parameters. The noisy utterance is then partitioned into segments based on broad level phoneme classes, and constraints are applied on each segment using a hard decision solution. To alleviate the effect of hard decision errors, a Gaussian mixture model (GMM)-based maximum-likelihood (ML) soft decision solution is also presented. The resulting utterances are evaluated over the TIMIT speech corpus using the Itakura\u2013Saito, segmental signal-to-noise ratio (SNR) and perceptual evaluation of speech quality (PESQ) metrics over four noise types at three SNR levels. Comparative assessment over baseline enhancement algorithms like Auto-LSP, log-minimum mean squared error (log-MMSE), and log-MMSE with speech presence uncertainty (log-MMSE-SPU) demonstrate that the proposed solution exhibits greater consistency in improving speech quality over most phoneme classes and noise types considered in this study.\"",
        "Document: \"Automatic Beamforming for Blind Extraction of Speech From Music Environment Using Variance of Spectral Flux-Inspired Criterion. This paper addresses the problem of automatic beamforming for blind extraction of speech in a music environment, using multiple microphones. A new criterion is proposed based on the variance of the spectral flux (VSF), which is shown to be a compound measure of the Kurtosis and across-time correlation for the time-frequency domain signals. Spectral flux (SF) had been adopted as a feature that distinguishes speech from other acoustic noises and the VSF of speech tends to be larger than that of other acoustic sounds. Henceforth, maximization of VSF can be employed as one potential criterion to identify the speech direction-of-arrival (DOA), in order to extract speech from the noisy observations. We construct a VSF-inspired cost function and develop a complex-value fixed-point algorithm for the optimization. Then, the stability of the proposed algorithm is analyzed based on the second-order Taylor series expansion. Rather than the DOA identification ambiguity caused by subspace decomposition-based methods or maximization of non-Gaussianity-based approaches, both real and simulated evaluations indicate that the VSF-inspired criterion can effectively extract speech from a music diffuse noise field or a musical interference noise field. A key feature of the proposed approach is that it can operate blindly, i.e., it does not require a priori knowledge about the array geometry, the noise covariance matrix, or the geometrical knowledge of the location of desired speech. Therefore, this study offers a potential perspective for blindly extracting speech from a music environment.\"",
        "Document: \"Text-directed speech enhancement using phoneme classification and feature map constrained vector quantization. This paper presents and evaluates a novel text-directed speech enhancement algorithm for usage in non-real-time applications. In our approach, the text of the intended dialogue is used to partition noisy speech into regions of broad phoneme classifications. Classes considered include stops, fricatives, affricates, nasals, vowels, semivowels, diphthongs and silence. These partitions are then used to direct a new vector quantizer based enhancement scheme in which class directed constraints are applied to improve speech quality. Objective enhancement evaluations conducted across 100 sentences of the TIMIT database indicate consistent improvement in speech quality for actual helicopter fly-by noise, aircraft cockpit noise, and automobile highway noise at signal-to-noise ratios ranging from -5 to 10 dB. Subjective quality assessment was conducted in the form of an A-B comparison test. Results of these evaluations demonstrate that, for wideband noise distortion, the proposed algorithm is preferred over unprocessed noisy speech more than 2 to 1, while the proposed algorithm is preferred over spectral subtraction processed speech by more than 3 to 1.\"",
        "Document: \"Speech enhancement using a constrained iterative sinusoidal model. This paper presents a sinusoidal model based algorithm for enhancement of speech degraded by additive broad-band noise. In order to ensure speech-like characteristics observed in clean speech, smoothness constraints are imposed on the model parameters using a spectral envelope surface (SES) smoothing procedure. Algorithm evaluation is performed using speech signals degraded by additive white Gaussian noise. Distortion as measured by objective speech quality scores showed a 34%-41% reduction over a SNR range of 5-to-20 dB. Objective and subjective evaluations also show considerable improvement over traditional spectral subtraction and Wiener filtering based schemes. Finally, in a subjective AB preference test, where enhanced signals were coded with the G729 codec, the proposed scheme was preferred over the traditional enhancement schemes tested for SNRs in the range of 5 to 20 dB\"",
        "Document: \"Analysis and compensation of speech under stress and noise for environmental robustness in speech recognition. Il est connu que la distorsion acoustique introduite par l'environnement ambiant ainsi que la variabilit\u00e9 r\u00e9sultant du stress induit d\u00e9t\u00e9riorent \u00e9norm\u00e9ment les performances des algorithmes de reconnaissance. Dans cet article, on explore les diverses causes de d\u00e9gradation de ces performances. On sugg\u00e8re que les \u00e9tudes r\u00e9centes effectu\u00e9es sur l'approche appel\u00e9e Source Generator Framework produisent un fondement viable pour d\u00e9velopper des techniques robustes de reconnaissance de la parole. L'\u00e9tude d\u00e9crite s'articule autour de trois axes corr\u00e9l\u00e9s: (i) l'analyse et la mod\u00e9lisation de la parole produite soit sous l'effet de stress du \u00e0 la charge de travail et/ou \u00e0 l'\u00e9motion, soit dans le bruit, (ii) les m\u00e9thodes de traitement adaptatif du signal pour le d\u00e9bruitage de la parole et la r\u00e9duction de l'effet du stress, et (iii) la formulation de nouveaux algorithmes robustes de reconnaissance. Une analyse statistique d'une base de donn\u00e9es (SUSAS) de parole sous stress simul\u00e9 et r\u00e9el est pr\u00e9sent\u00e9e. Cette analyse a \u00e9t\u00e9 men\u00e9e sur plus de 200 param\u00e8tres relatifs au pitch, \u00e0 la dur\u00e9e, \u00e0 l'intensit\u00e9, \u00e0 la source glottique et aux variations des spectres du conduit vocal. Ces \u00e9tudes ont motiv\u00e9 le d\u00e9veloppement de l'approche appel\u00e9e Source Generator Framework qui permet de mod\u00e9liser la dynamique de la parole sous stress. Ce cadre offre des moyens int\u00e9ressants pour effectuer l'\u00e9galisation des param\u00e8tres de la parole sous stress. Dans la seconde moiti\u00e9 de l'article, trois nouvelles approches pour le d\u00e9bruitage de la parole et la r\u00e9duction de l'effet du stress sont consid\u00e9r\u00e9es. La premi\u00e8re m\u00e9thode utilise la technique it\u00e9rative contrainte (Auto:I,LSP:T) de d\u00e9bruitage et une \u00e9galisation par maximum de vraisemblance de la parole \u00e0 travers la localisation des formants et leurs bandes passantes. Pour la reconnaissance de mots cl\u00e9s, la seconde m\u00e9thode utilise un r\u00e9seau de neurones qui transforme les vecteurs de param\u00e8tres de la parole sous stress pendant la phase de param\u00e9trisation. La derni\u00e8re m\u00e9thode applique une technique de rehaussement des param\u00e8tres bas\u00e9e sur des contraintes morphologiques pour effectuer le d\u00e9bruitage et utilise un algorithme adaptatif sur les cepstres-Mel pour \u00e9galiser les effets du stress. Les performances de reconnaissance sont donn\u00e9es pour la parole produite dans plusieurs conditions de stress, avec plusieurs rapports signal/bruit, et pour diff\u00e9rents types de bruit ambiant.\"",
        "Document: \"Prof-Life-Log: Personal Interaction Analysis For Naturalistic Audio Streams. Analysis of personal audio recordings is a challenging and interesting subject. Using contemporary speech and language processing techniques, it is possible to mine personal audio recordings for a wealth of information that can be used to measure a person's engagement with their environment as well as other people. In this study, we propose an analysis system that uses personal audio recordings to automatically estimate the number of unique people and environments which encompass the total engagement within the recording. The proposed system uses speech activity detection (SAD), speaker diarization and environmental sniffing techniques, and is evaluated on naturalistic audio streams from the Prof-Life-Log corpus. We also report performance of the individual systems, and also present a combined analysis which reveals the interaction of the subject with both people and environment. Hence, this study establishes the efficacy and novelty of using contemporary speech technology for life logging applications.\"",
        "Document: \"An investigation into back-end advancements for speaker recognition in multi-session and noisy enrollment scenarios. This study aims to explore the case of robust speaker recognition with multi-session enrollments and noise, with an emphasis on optimal organization and utilization of speaker information presented in the enrollment and development data. This study has two core objectives. First, we investigate more robust back-ends to address noisy multi-session enrollment data for speaker recognition. This task is achieved by proposing novel back-end algorithms. Second, we construct a highly discriminative speaker verification framework. This task is achieved through intrinsic and extrinsic back-end algorithm modification, resulting in complementary sub-systems. Evaluation of the proposed framework is performed on the NIST SRE2012 corpus. Results not only confirm individual sub-system advancements over an established baseline, the final grand fusion solution also represents a comprehensive overall advancement for the NIST SRE2012 core tasks. Compared with state-of-the-art SID systems on the NIST SRE2012, the novel parts of this study are: 1) exploring a more diverse set of solutions for low-dimensional i-Vector based modeling; and 2) diversifying the information configuration before modeling. All these two parts work together, resulting in very competitive performance with reasonable computational cost.\"",
        "Document: \"Speech Enhancement Based on Generalized Minimum Mean Square Error Estimators and Masking Properties of the Auditory System. In this paper, the family of conditional minimum mean square error (MMSE) spectral estimators is studied which take on the form (E(XEalphap/|Xp + Dp|))1alpha/, where Xp is the clean speech spectrum, and Dp is the noise spectrum, resulting in a generalized MMSE estimator (GMMSE). The degree of noise suppression versus musical tone artifacts of these estimators is studied. The tradeoffs in selection of (alpha), across noise spectral structure and signal-to-noise ratio (SNR) level, are also considered. Members of this family of estimators include the Ephraim-Malah (EM) amplitude estimator and, for high SNRs, the Wiener Filter. It is shown that the colorless residual noise observed in the EM estimator is a characteristic of this general family of estimators. An application of these estimators in an auditory enhancement scheme using the masking threshold of the human auditory system is formulated, resulting in the GMMSE-auditory masking threshold (AMT) enhancement method. Finally, a detailed evaluation of the proposed algorithms is performed over the phonetically balanced TIMIT database and the National Gallery of the Spoken Word (NGSW) audio archive using subjective and objective speech quality measures. Results show that the proposed GMMSE-AMT outperforms MMSE and log-MMSE enhancement methods using a detailed phoneme-based objective quality analysis\"",
        "Document: \"A source generator based modeling framework for synthesis of speech under stress. The objective of this paper is to formulate an algorithm to generate stressed synthetic speech from neutral speech using a source generator framework previously employed for stressed speech recognition. The following goals are addressed (i) identify the most visible indicators of stress as perceived by the listener in stressed speaking styles such as loud, Lombard effect and angry, (ii) develop a mathematical model for representing speech production under stressed conditions, and (iii) employ the above model to produce emotional/stressed synthetic speech from neutral speech. The stress modeling scheme is applied to an existing low-bit rate CELP speech coder in order to investigate (i) the coder's ability and limitations reproducing stressed synthetic speech, and (ii) our ability to perturb coded neutral speech parameters at the synthesis stage so that resulting speech is perceived as being under stress. Two stress perturbation algorithms are proposed and evaluated. Results from formal listener evaluations show that 87% of neutral perturbed speech was indeed perceived as stressed\"",
        "Document: \"Missing-Feature Method For Speaker Recognition In Band-Restricted Conditions. In this study, the missing-feature method is considered to address band-limited speech for speaker recognition. In an effort to mitigate possible degradation due to the general speaker independent model, a two-step reconstruction scheme is developed, where speaker class independent/dependent models are used separately. An advanced marginalization in the cepstral domain is proposed employing a high order extension method in order to address loss of model accuracy in the conventional method due to cepstrum truncation. To detect the cut-off regions from incoming speech, a blind mask estimation scheme is employed which uses a synthesized band-limited speech model. Experimental results on band-limited conditions indicate that our two-step reconstruction scheme with missing-feature processing is effective in improving in-set/out-of-set speaker recognition performance for band-limited speech, particularly in severely band-restricted conditions (i.e., 4.72% EER improvement in 2, 3, and 4kHz band-limited conditions over a conventional data-driven method). The improvement of the proposed marginalization method proves its effectiveness for acoustic model conversion by employing high order extension, showing 0.57% EER improvement over conventional marginalization.\"",
        "1 is \"Maximum a posteriori estimation for multivariate Gaussian mixture observations of Markov chains\", 2 is \"Understanding and constructing shared spaces with mixed-reality boundaries\"",
        "Given above information, for an author who has written the paper with the title \"Nonparametric Identification Of Linearizations And Uncertainty Using Gaussian Process Models - Application To Robust Wheel Slip Control\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005371": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Discovery is never by chance: designing for (un)serendipity':",
        "Document: \"Network Analysis of Third Party Tracking: User Exposure to Tracking Cookies through Search. Internet advertisers reach millions of customers through practices that real time tracking of users' online activities. The tracking is conducted by third party ad services engaged by the Web sites to facilitate marketing campaigns. Previous research has investigated tracking practices and tracking agencies associated with popular Web sites. Here we investigate the network properties of the third party referral structures that facilitate gathering of user information for the delivery of personalized ads. By considering third party domains associated with the top ten search results for a diverse set of queries, we arrived at the networks of third party domains in four search markets. We show a consistent network structure across markets, with a dominant connected component that, on average, includes 92.8% of network vertices and 99.8% of the connecting edges. There is 99.5% chance that a user will become tracked by all top 10 trackers within 30 clicks on search results. Finally, the third party networks exhibit properties of the small world networks. This implies a high-level global and local efficiency in spreading the user information and delivering targeted ads.\"",
        "Document: \"The Body as Starting Point: Exploring Inside and Around Body Boundaries for Body-Centric Computing Design. More HCI designs and devices are embracing what is being dubbed \"body centric computing,\" where designs both deliberately engage the body as the locus of interest, whether to move the body into play or relaxation, or to track and monitor its performance, or to use it as a surface for interaction. Most HCI researchers are engaging in these designs, however, with little direct knowledge of how the body itself works either as a set of complex internal systems or as sets of internal and external systems that interact dynamically. The science of how our body interacts with the microbiome around us also increasingly demonstrates that our presumed boundaries between what is inside and outside us may be misleading if not considered harmful. Developing both (1) introductory knowledge and (2) design practice of how these in-bodied and circumbodied systems work with our understanding of the em-bodied self, and how this gnosis/praxis may lead to innovative new body-centric computing designs is the topic of this workshop.\n\n\"",
        "Document: \"Finders/keepers: a longitudinal study of people managing information scraps in a micro-note tool. Mainstream PIM tools capture only a portion of the information that people need to manage. Many information scraps seem to exist that don't make their way into these tools, instead being relegated to sticky notes, text files, and other makeshift storage, or simply being lost. In an effort to understand the role of these information scraps, the underlying needs they reflect, and the way PIM tools must be modified to support those needs, we created List-it, a micronote tool for quick and simple capture of information scraps. In this article, we analyze the notes and interaction logs of 420 volunteer users of List-it over a two-year period of study (August 2008-August 2010). We contextualize our analysis with results of two surveys and an e-mail interview we conducted in October 2009. We find that people are drawn to List-it by the ease and speed of note capture and by the ability to record scraps with arbitrary content that blends or completely escapes the types and roles imposed by our rigid PIM tools. Notes are taken to serve a variety of needs -- reminding, reference, journaling/activity logging, brainstorming, and to indefinitely archive information of sentimental or personal value. Finally, while people differ considerably in the ways they keep information, our findings suggest such differences can be described as a combination of four distinct strategies, enriching the Filer/Piler distinction identified for classic document management.\"",
        "Document: \"Bringing communities to the semantic web and the semantic web to communities. In this paper we consider the types of community networks that are most often codified within the Semantic Web. We propose the recognition of a new structure which fulfils the definition of community used outside the Semantic Web. We argue that the properties inherent in a community allow additional processing to be done with the described relationships existing between entities within the community network. Taking an existing online community as a case study we describe the ontologies and applications that we developed to support this community in the Semantic Web environment and discuss what lessons can be learnt from this exercise and applied in more general settings.\"",
        "Document: \"Usability research challenges for cyberinfrastructure and tools. We summarize the motivation and aims for this workshop on usability research challenges for cyberinfrastructure and tools, and outline workshop preparations and program.\"",
        "Document: \"Towards an Evaluation Methodology for the Development of Research-Oriented Virtual Communities. This paper is part a case study of collaborative environment use and parts a proposition for a methodological approach for evaluating virtual environments based on the findings from that case study. We review the literature on evaluating collaborative environments and we report on a usability study carried out on a research oriented virtual community. The report includes a cluster analysis of the questionnaire results and the use of those results in profiling user attitudes. We propose that this type of differential cluster analysis should be used in the future to identify people who are generally better able to make discriminating judgments, and to identify people who are able to analyze the usability of different components of an environment rather than just the overall environment.\"",
        "Document: \"Twiage: a game for finding good advice on twitter. Millions of recommendations, opinions and experiences are shared across popular microblogging platforms and services each day. Yet much of this content becomes quickly lost in the stream shortly after being posted. This paper looks at the feasibility of identifying useful content in microblog streams so that it might be archived to facilitate wider access and reference. Towards this goal, we present an experiment with a game-with-a-purpose called Twiage that we designed to determine how well the deluge of content in \"raw\" microblog streams could be turned into filtered and ranked collections using ratings from players. Experiments with Twiage validate the feasibility of applying human-computation to this problem, finding strong agreement about what constitutes the \"most useful\" content in our test dataset. Second, we compare the effectiveness of various methods of eliciting such ratings, finding that a \"choose-best\" interface and Elo rating ranking scheme yield the greatest agreement in the fewest rounds. External validation of resulting top-rated twitter content with a domain expert found that while the top Twiage-ranked \"tweets\" were among the best of the set, there was a tendency for players to also select what we term \"weak spam\" - e.g., promotional content disguised as articles or reviews, indicating a need for more stringent content filtering.\"",
        "Document: \"Keep Calm and Carry On: Exploring the Social Determinants of Indoor Environment Quality. Poor Indoor Environment Quality (IEQ) in office environments leads to worker discomfort and lost productivity. This paper provides a unique perspective into the specifically social determinants of IEQ in naturally ventilated offices and our work toward designing technology that might improve it. Based on 15 qualitative interviews we explore the rituals, practices and negotiations involved in opening windows and thermostat adjustment. We find that a wish to maintain status quo results in workers putting up with discomfort with IEQ factors well before requesting a change. In closing, we introduce our future design work aimed at drawing attention to existing office practices and encouraging a broader participation in negotiations around IEQ factors in the workplace.\n\n\"",
        "Document: \"Walking through CS AKTive Space: a demonstration of an integrated Semantic Web application. We describe CS AKTive Space, an integrated semantic web application and winner of the 2003 Semantic Web Challenge [http://www.challenge.semanticweb.org/]. A demonstration of the application is available at http://cs.aktivespace.org/. CS AKTive Space represents and integrates a wide range of heterogenous resources representing the computer science domain in the UK; it supports the exploration of patterns and implications inherent in the content and exploits a variety of services, visualisations and multidimensional representations to support questions like who is working with whom, where are there geographical concentrations in funding or research area, who are the most significant researchers in an area. We briefly show how this demonstration illustrates a number of substantial challenges for the Semantic Web. These include problems of referential integrity, tractable inference and interaction support. We review our approaches to these issues and discuss relevant related work.\"",
        "Document: \"TouchViz: a case study comparing two interfaces for data analytics on tablets. As more applications move from the desktop to touch devices like tablets, designers must wrestle with the costs of porting a design with as little revision of the UI as possible from one device to the other, or of optimizing the interaction per device. We consider the tradeoffs between two versions of a UI for working with data on a touch tablet. One interface is based on using the conventional desktop metaphor (WIMP) with a control panel, push buttons, and checkboxes -- where the mouse click is effectively replaced by a finger tap. The other interface (which we call FLUID) eliminates the control panel and focuses touch actions on the data visualization itself. We describe our design process and evaluation of each interface. We discuss the significantly better task performance and preference for the FLUID interface, in particular how touch design may challenge certain assumptions about the performance benefits of WIMP interfaces that do not hold on touch devices, such as the superiority of gestural vs. control panel based interaction.\"",
        "1 is \"Question Answering: CNLP at the TREC 2002 Question Answering Track\", 2 is \"The universal labeler: Plan the project and let your information follow\"",
        "Given above information, for an author who has written the paper with the title \"Discovery is never by chance: designing for (un)serendipity\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005381": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Linear receiver based high-rate space-time block codes':",
        "Document: \"On the Achievable Throughput of MIMO Broadcast Channels with Finite Rate Feedback. We consider multiuser scheduling with limited feedback of partial channel state information in MIMO broadcast channels. By using spatial multiplexing at the base station (BS) and antenna selection for each user, we propose a multiuser scheduling method that allocates independent information streams from all M transmit antennas to the M most favorable users with the highest signal-to-interference-plus-noise ratio (SINR). A close approximation of the achievable sum-rate throughput for the proposed method is obtained and shown to match the simulation results very well. Moreover, a scheduling algorithm with finite rate feedback of SINR is proposed and the achievable throughput is derived. Performance analysis will show that even with only 1-bit quantization of SINR, the proposed scheduling approach can exploit multiuser diversity at an expense of slight decrease of throughput.\"",
        "Document: \"Degrees of freedom of relay-assisted MIMO interfering broadcast channels. This work studies achievable degrees of freedom (DoF) of two-cell two-hop interfering broadcast channels. In the two cells, base stations (BS) equipped with Mi and M2 antennas intend to communicate with M1 and M2 single-antenna users, respectively. The messages sent from each BS are first received by two shared full-duplex multiantenna relays simultaneously in the first hop and then are forwarded to the intended users from the relays in the second hop. We study the achievable DoF of the network with various channel state information (CSI) available at the BSs and the users while the relays have CSI of the two-hop channels. We first consider the scenario when the BSs have the CSI of the first hop and the users have the CSI of the second hop, respectively. A linear scheme with precoding at the BSs and interference neutralization at the relays is designed to achieve the cut-set DoF of M1 + M2 if both relays have M1 + M2 - 1 antennas. We then consider the scenario when the BSs do not have any CSI but the users have the full CSI of both hops. A relay beamforming scheme is proposed to achieve the DoF of M1 + M2 when both relays have M1 + M2 - 1 antennas.\"",
        "Document: \"Full-Diversity and Fast ML Decoding Properties of General Orthogonal Space-Time Block Codes for MIMO-OFDM Systems. In this letter we apply the general orthogonal space-time block codes (OSTBC) to MIMO-OFDM systems over frequency-selective fading channels and aim to exploit the potential multipath diversity. By replacing the scalar entry of an OSTBC matrix with the vector of repeated symbols, we obtain a new OSTBC which can achieve both spatial diversity and multipath diversity. Moreover, a fast maximum likelihood (ML) decoding is admitted. Simulation results show that the proposed OSTBC, for two transmit antennas, can obtain a higher diversity gain than the Alamouti code at the same ML decoding complexity\"",
        "Document: \"Transport Capacity of Distributed Wireless CSMA Networks. In this paper, we study the transport capacity of large multi-hop wireless CSMA networks. Different from previous studies that rely on the use of a centralized scheduling algorithm and/or a centralized routing algorithm to achieve the optimal capacity scaling law, we show that the optimal capacity scaling law can be achieved using entirely distributed routing and scheduling algorithms. Specificall...\"",
        "Document: \"Deep Learning for Channel Tracking in IRS-Assisted UAV Communication Systems. To boost the performance of wireless communication networks, unmanned aerial vehicles (UAVs) aided communications have drawn dramatically attention due to their flexibility in establishing the line of sight (LoS) communications. However, with the blockage in the complex urban environment, and due to the movement of UAVs and mobile users, the directional paths can be occasionally blocked by trees and high-rise buildings. Intelligent reflection surfaces (IRSs) that can reflect signals to generate virtual LoS paths are capable of providing stable communications and serving wider coverage. This is the first paper that exploits a three-dimensional geometry dynamic channel model in IRS- assisted UAV-enabled communication system. Moreover, we develop a novel deep learning based channel tracking algorithm consisting of two modules: channel pre-estimation and channel tracking. A deep neural network with off-line training is designed for denoising in the pre-estimation module. Moreover, for channel tracking, a stacked bi-directional long short term memory (Stacked Bi-LSTM) is developed based on a framework that can trace back historical time sequence together with bidirectional structure over multiple stacked layers. Simulations have shown that the proposed channel tracking algorithm requires fewer epochs to convergence compared to benchmark algorithms. It also demonstrates that the proposed algorithm is superior to different benchmarks with small pilot overheads and comparable computation complexity.\"",
        "Document: \"A Systematic Design of Full Diversity Multiuser Space-Frequency Codes. In this paper, we propose a systematic design of full-diversity space-frequency (SF) codes for multiuser MIMO-OFDM systems. With joint maximum likelihood detection, the proposed codes can obtain full diversity over selective-fading multiple access channels. The proposed coding scheme does not require the cooperation of multiple transmitters, nor the instantaneous channel side information at the transmitters but the channel statistics. Moreover, the proposed coding scheme is bandwidth efficient in that all users send their data streams through all OFDM subchannels simultaneously to make full use of the available bandwidth.\"",
        "Document: \"On achievable degrees of freedom of 3-user MIMO interference channels. This paper studies the achievable degrees of freedom (DoF) of 3-user MIMO interference channels based on linear spatial beamforming scheme. Previous work mainly focus on the scenario where each user must have the same DoF and the achievable sum DoF is 3\u230aDout over/3\u230b, where Douter denotes the outer bound of sum DoF. In this paper, we investigate the scenarios where three users are allowed to have different DoF. Three interference alignment based signal transmission strategies are proposed and the achievable DoF is derived. We show that the achievable DoF of the proposed interference alignment scheme achieves the DoF of \u230aDout\u230b in some cases, which is equal to or higher than 3\u230aDout over 3\u230b.\"",
        "Document: \"Opportunistic Spectrum Sharing Based on Exploiting ARQ Retransmission in Cognitive Radio Networks. In this paper, we consider a pair of cognitive radio (CR) users co-existing with a pair of ARQ- based primary users (PU). The secondary user (SU) overhears the ACK/NACK feedback sent from the receiver of the primary system, and then decides to access the spectrum or not. An opportunistic sharing scheme, referred to as Spectrum sHaring with ARQ Retransmission and Probing timeslots (SHARP), is proposed to exploit spectrum opportunities based on the ACK/NACK of PU only. Numerical results show that the analytical outcomes perfectly match those from the Monte Carlo simulation. Moreover, the goodput of SU increases dramatically while the outage probability of the primary remains small.\"",
        "Document: \"Spectrum sharing between random geometric networks. In this paper a spectrum sharing scheme is proposed to maximize the successful transmission probability of a single-hop cognitive network coexisting with a random primary network. Both cognitive and primary networks exhibit randomness in topologies and endure imperfect wireless channel conditions. For a given primary outage probability bound, the maximum secondary transmit power is determined and then the maximum transmission capacity of the cognitive user is derived. Numerical results show that the proposed spectrum sharing scheme indeed boosts the transmission capacity of the cognitive network dramatically whilst having little performance loss of the primary network.\"",
        "Document: \"Opportunistic Spectrum Sharing With Limited Feedback in Poisson Cognitive Radio Networks. In this paper, we propose two limited feedback based underlay spectrum sharing schemes in Poisson cognitive radio networks without and with primary exclusive regions, respectively. Both primary and secondary transmitters are Poisson distributed nodes with limited feedback of channel quality information from their local receivers. The primary transmitters are active if their local primary channels are above a certain threshold. In the opportunistic spectrum sharing scheme without primary exclusive region, the secondary transmitters are elected to transmit if their local channels are above the required threshold. In the opportunistic spectrum sharing scheme with a primary exclusive region, the secondary transmitters transmit if their local channels are above the threshold and they are outside the exclusive regions of the active primary receivers. For both schemes, the optimal secondary node density is analytically derived by maximizing the secondary area spectral efficiency subject to the secondary outage constraint and the primary efficiency loss constraint. Numerical results show that, for a tight secondary outage constraint, it is more beneficial to use the scheme with no primary exclusive region. For relatively loose secondary outage constraint, the scheme with primary exclusive regions is recommended.\"",
        "1 is \"Confidential Broadcasting via Linear Precoding in Non-Homogeneous MIMO Multiuser Networks.\", 2 is \"Optimization of transport capacity in wireless multihop networks.\"",
        "Given above information, for an author who has written the paper with the title \"Linear receiver based high-rate space-time block codes\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005416": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Face and eye detection on hard datasets':",
        "Document: \"A Silhouette-Contour Based 3-D Registration Methodology As A Pre-Evaluation Step Of 3-D Reconstruction Techniques. Data registration is a crucial step in performance evaluation procedures that aim at localizing errors in given measured data. Under the performance evaluation topic, the probably corrupted measured data should be accurately registered/aligned to the ground truth data such that the registration process would not affect the accuracy of the consequent evaluation steps. To cope with this problem, another registration methodology that can go beyond the conventional ones should be used. Here we propose a novel approach for 3-D data registration. The performance of the approach is totally independent of the measured data set. The key advantage of this approach is that it keeps the registration process away from being affected by the probably corrupted data sets, hence it permits confidential evaluation results.\"",
        "Document: \"On the Extraction of Curve Skeletons using Gradient Vector Flow. In this paper, we propose a new variational framework for computing continuous curve skeletons from discrete ob- jects that are suitable for structural shape representation. We have derived a new energy function, which is propor- tional to some medialness function, such that the minimum cost path between any two medial voxels in the shape is a curve skeleton. We have employed two different medial- ness functions; the Euclidean distance field and a variant of the magnitude of the gradient vector flow (GVF), resulting in two different energy functions. The first energy controls the identificationof the shape topologicalnodes from which curve skeletons start, while the second one controls the ex- traction of curve skeletons. The accuracy and robustness of the proposed framework are validated both quantitatively and qualitatively against competing techniques as well as several 3D shapes of different complexity.\"",
        "Document: \"Surface modeling of the corpus callosum from MRI scans. In this paper, the Bezier curve and surface are used to model the shape of the Corpus Callosum (CC) region from T1-weighted clinical MRI scans. We drive a closed form solution for the Bezier coefficients in 2D and 3D Euclidean spaces. The coefficients of the models are used for reconstruction of the CC contours and surfaces with varying degrees of accuracy, and constitute basis for discrimination between populations, and ways to enhance elastic registration of the CC. The discrimination ability of the Bezier curves and surfaces are evaluated against the Fourier Descriptors (FD) and Spherical Harmonics (SH) approaches.\"",
        "Document: \"A Fuzzy Framework With Prior Information Unifying Registration, Segmentation, And Bias Field Correction Of Brain Mri. This paper introduces a fuzzy framework for the simultaneous segmentation and registration in addition to bias field correction of MRI datasets. The framework utilizes prior information which may be available about the tissues' mean intensities and tissues' distribution through the datasets. Moreover it works on the given, original image intensities without any logarithmic transformation and thus produces more accurate results and faster performance. The algorithm is evaluated using simulated and real brain MRI data. The results show that the algorithm has indeed improved the segmentation accuracy.\"",
        "Document: \"Nonmetric Lens Distortion Calibration: Closed-form Solutions, Robust Estimation and Model Selection. We address the problem of calibrating camera lens distortion, which can be significant in medium to wide angle lenses. While almost all existing nonmetric distortion calibration methods need user involvement in one form or another, we present an automatic approach based on the robust the-least-median-of-squares (LMedS) estimator. Our approach is thus less sensitive to erroneous input data such as image curves that are mistakenly considered as projections of 3D linear segments. Our approach uniquely uses fast, closed-form solutions to the distortion coefficients, which serve as an initial point for a nonlinear optimization algorithm to straighten imaged lines. Moreover we propose a method for distortion model selection based on geometrical inference. Successful experiments to evaluate the performance of this approach on synthetic and real data are reported.\"",
        "Document: \"A Variational Approach for Shapes Registration Using Vector Maps. The main focus of this paper is the shape representation and registration using vector level set functions. This powerful representation is more flexible than conventional signed distance level sets since it enables us to control the shapes registration process by using more complicated transforms. Based on this model, a variational frame work is proposed for the rigid and non-rigid registration of shapes which can be extended to the higher dimensional case in a straightforward manner and also does not need any point correspondences. The optimization criterion presented can handle efficiently both the rigid and the non-rigid operations together. Experimental results in 2D for real shapes registration are discussed to show the efficiency of the approach with small and large global deformations of shapes.\"",
        "Document: \"Nonmetric calibration of camera lens distortion: differential methods and robust estimation. This paper addresses the problem of calibrating camera lens distortion, which can be significant in medium to wide angle lenses. Our approach is based on the analysis of distorted images of straight lines. We derive new distortion measures that can be optimized using nonlinear search techniques to find the best distortion parameters that straighten these lines. Unlike the other existing approaches, we also provide fast, closed-form solutions to the distortion coefficients. We prove that including both the distortion center and the decentering coefficients in the nonlinear optimization step may lead to instability of the estimation algorithm. Our approach provides a way to get around this, and, at the same time, it reduces the search space of the calibration problem without sacrificing the accuracy and produces more stable and noise-robust results. In addition, while almost all existing nonmetric distortion calibration methods needs user involvement in one form or another, we present a robust approach to distortion calibration based on the least-median-of-squares estimator. Our approach is, thus, able to proceed in a fully automatic manner while being less sensitive to erroneous input data such as image curves that are mistakenly considered projections of three-dimensional linear segments. Experiments to evaluate the performance of this approach on synthetic and real data are reported.\"",
        "Document: \"Data-Driven Lung Nodule Models for Robust Nodule Detection in Chest CT. The quality of the lung nodule models determines the success of lung nodule detection. This paper describes aspects of our data-driven approach for modeling lung nodules using the texture and shape properties of real nodules to form an average model template per nodule type. The ELCAP low dose CT (LDCT) scans database is used to create the required statistics for the models based on modern computer vision techniques. These models suit various machine learning approaches for nodule detection including Bayesian methods, SVM and Neural Networks, and computations may be enhanced through genetic algorithms and Adaboost. The eminence of the new nodule models are studied with respect to parametric models showing significant improvements in both sensitivity and specificity.\"",
        "Document: \"Curve/Surface Representation and Evolution Using Vector Level Sets with Application to the Shape-Based Segmentation Problem. In this paper, we revisit the implicit front representation and evolution using the vector level set function (VLSF) proposed in [1]. Unlike conventional scalar level sets, this function is designed to have a vector form. The distance from any point to the nearest point on the front has components (projections) in the coordinate directions included in the vector function. This kind of representation is used to evolve closed planar curves and 3D surfaces as well. Maintaining the VLSF property as the distance projections through evolution will be considered together with a detailed derivation of the vector partial differential equation (PDE) for such evolution. A shape-based segmentation framework will be demonstrated as an application of the given implicit representation. The proposed level set function system will be used to represent shapes to give a dissimilarity measure in a variational object registration process. This kind of formulation permits us to better control the process of shape registration, which is an important part in the shape-based segmentation framework. The method depends on a set of training shapes used to build a parametric shape model. The color is taken into consideration besides the shape prior information. The shape model is fitted to the image volume by registration through an energy minimization problem. The approach overcomes the conventional methods problems like point correspondences and weighing coefficients tuning of the evolution (PDEs). It is also suitable for multidimensional data and computationally efficient. Results in 2D and 3D of real and synthetic data will demonstrate the efficiency of the framework.\"",
        "Document: \"Towards accurate and efficient representation of image irradiance of convex-Lambertian objects under unknown near lighting. Surface irradiance signals are turned into outgoing radiance through the surface reflectance function, which can be significantly perturbed by the illumination conditions. Due to their low-frequency nature, irradiance signals can be represented using low-order basis functions, where spherical harmonics (SH) have been extensively used to provide such basis. When capturing image irradiance from a single viewpoint, the visible part of the object's surface constructs the upper hemisphere of the surface normals where the SH are no longer orthonormal. This reduced domain paves the way for even lower-dimensional approximation since full spherical representation is not needed. While harmonic basis are known to be optimal under distant light, light coming from near-by objects and indoor environments are common near light scenarios; it is essential to relax distant light assumption. Considering light source(s) distributed uniformly over the upper hemisphere, we propose the use of hemispherical harmonics (HSH) to model image irradiance of convex Lambertian objects perceived from single viewpoint under unknown near illumination. We prove analytically, and experimentally validated, that the Lambertian kernel has a more compact harmonic expansion in the hemispherical domain when compared to its spherical counterpart. We illustrate that HSH provide an efficient and accurate low-dimensional representation of image irradiance of Lambertian objects under near lighting conditions in contrast to SH.\"",
        "1 is \"Locating Facial Features with an Extended Active Shape Model\", 2 is \"Synthetic speech detection using temporal modulation feature\"",
        "Given above information, for an author who has written the paper with the title \"Face and eye detection on hard datasets\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005466": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Multi-sphere Support Vector Data Description for Outliers Detection on Multi-distribution Data':",
        "Document: \"A primal method for multiple kernel learning. The canonical support vector machines (SVMs) are based on a single kernel, recent publications have shown that using multiple kernels instead of a single one can enhance interpretability of the decision function and promote classification accuracy. However, most of existing approaches mainly reformulate the multiple kernel learning as a saddle point optimization problem which concentrates on solving the dual. In this paper, we show that the multiple kernel learning (MKL) problem can be reformulated as a BiConvex optimization and can also be solved in the primal. While the saddle point method still lacks convergence results, our proposed method exhibits strong optimization convergence properties. To solve the MKL problem, a two-stage algorithm that optimizes canonical SVMs and kernel weights alternately is proposed. Since standard Newton and gradient methods are too time-consuming, we employ the truncated-Newton method to optimize the canonical SVMs. The Hessian matrix need not be stored explicitly, and the Newton direction can be computed using several Preconditioned Conjugate Gradient steps on the Hessian operator equation, the algorithm is shown more efficient than the current primal approaches in this MKL setting. Furthermore, we use the Nesterov's optimal gradient method to optimize the kernel weights. One remarkable advantage of solving in the primal is that it achieves much faster convergence rate than solving in the dual and does not require a two-stage algorithm even for the single kernel LapSVM. Introducing the Laplacian regularizer, we also extend our primal method to semi-supervised scenario. Extensive experiments on some UCI benchmarks have shown that the proposed algorithm converges rapidly and achieves competitive accuracy. \u00a9 2012 Springer-Verlag London Limited.\"",
        "Document: \"Particle swarm optimization with convergence speed controller for large-scale numerical optimization. Particle swarm optimization (PSO) has high convergence speed yet with its major drawback of premature convergence when solving large-scale optimization problems. We argue that it can be empowered by adaptively adjusting its convergence speed for the problems. In this paper, a convergence speed controller is proposed to improve the performance of PSO for large-scale optimization. As an additional operator of PSO, the controller is applied periodically and independently. It has two conditions and rules for adjusting the convergence speed of PSO, one for premature convergence and the other for slow convergence. The effectiveness of the PSO with convergence speed controller is evaluated by calculating the benchmark functions of CEC\u20192010. The numerical results indicate that the proposed controller helps PSO to keep a balance between convergence speed and swarm diversity during the optimization process. The results also support our argument that PSO can on average outperform other PSOs and cooperative coevolution methods for large-scale optimization when working with the convergence speed controller.\"",
        "Document: \"Mining hidden non-redundant causal relationships in online social networks. Causal discovery is crucial to obtain a deep understanding of the actual mechanism behind the online social network, e.g., identifying the influential individuals and understanding the interaction among user behavior sequences. However, detecting causal directions and pruning causal redundancy of online social networks are still the great challenge of existing research. This paper proposed a constraint-based approach, minimal causal network (MCN), to mine hidden non-redundant causal relationships behind user behavior sequences. Under the MCN, the transfer entropy with the adaptive causal time lag is used to detect causal directions and find causal time lags, while a permutation-based significance test is proposed to prune redundant edges. Experiments on simulated data verify the effectiveness of our proposed method. We also apply our approach to real-world data from Sina Weibo and reveal some interesting discoveries.\"",
        "Document: \"An Efficient Entropy-Based Causal Discovery Method for Linear Structural Equation Models With IID Noise Variables. The discovery of causal relationships from the observational data is an important task. To identify the unique causal structure belonging to a Markov equivalence class, a number of algorithms, such as the linear non-Gaussian acyclic model (LiNGAM), have been proposed. However, two challenges remain to be met: 1) these algorithms fail to work on the data which follow linear structural equation model with Gaussian noise and 2) they misjudge the causal direction when the data contain additional measurement errors. In this paper, we propose an entropy-based two-phase iterative algorithm for arbitrary distribution data with additional measurement errors under some mild assumptions. In the first phase of the algorithm, based on the property that entropy can measure the amount of information behind the data with arbitrary distribution, we design a general approach for the identification of exogenous variable on both Gaussian and non-Gaussian data, and we give the corresponding theoretical derivation. In the second phase, to eliminate the effects of measurement errors, we revise the value of the exogenous variable by removing its measurement error and further use the revised value to remove its effect on the remaining variables. Experimental results on real-world causal structures are presented to demonstrate the effectiveness and stability of our method. We also apply the proposed algorithm on the mobile-base-station data with measurement errors, and the results further prove the effectiveness of our algorithm.\"",
        "Document: \"Binary tree support vector machine based on kernel fisher discriminant for multi-classification. In order to improve the accuracy of the conventional algorithms for multi-classifications, we propose a binary tree support vector machine based on Kernel Fisher Discriminant in this paper. To examine the training accuracy and the generalization performance of the proposed algorithm, One-against-All, One-against-One and the proposed algorithms are applied to five UCI data sets. The experimental results show that in general, the training and the testing accuracy of the proposed algorithm is the best one, and there exist no unclassifiable regions in the proposed algorithm.\"",
        "Document: \"An adaptive support vector machine learning algorithm for large classification problem. Based on the incremental and decremental learning strategies, an adaptive support vector machine learning algorithm (ASVM) is presented for large classification problems in this paper. In the proposed algorithm, the incremental and decremental procedures are performed alternatively, and a small scale working set, which can cover most of the information in the training set and overcome the drawback of losing the sparseness in least squares support vector machine (LS-SVM), can be formed adaptively. The classifier can be constructed by using this working set. In general, the number of the elements in the working set is much smaller than that in the training set. Therefore the proposed algorithm can be used not only to train the data sets quickly but also to test them effectively with losing little accuracy. In order to examine the training speed and the generalization performance of the proposed algorithm, we apply both ASVM and LS-SVM to seven UCI datasets and a benchmark problem. Experimental results show that the novel algorithm is very faster than LS-SVM and loses little accuracy in solving large classification problems.\"",
        "Document: \"Similarity-based approach for positive and unlabelled learning. Positive and unlabelled learning (PU learning) has been investigated to deal with the situation where only the positive examples and the unlabelled examples are available. Most of the previous works focus on identifying some negative examples from the unlabelled data, so that the supervised learning methods can be applied to build a classifier. However, for the remaining unlabelled data, which can not be explicitly identified as positive or negative (we call them ambiguous examples), they either exclude them from the training phase or simply enforce them to either class. Consequently, their performance may be constrained. This paper proposes a novel approach, called similarity-based PU learning (SPUL) method, by associating the ambiguous examples with two similarity weights, which indicate the similarity of an ambiguous example towards the positive class and the negative class, respectively. The local similarity-based and global similarity-based mechanisms are proposed to generate the similarity weights. The ambiguous examples and their similarity-weights are thereafter incorporated into an SVM-based learning phase to build a more accurate classifier. Extensive experiments on real-world datasets have shown that SPUL outperforms state-of-the-art PU learning methods.\"",
        "Document: \"Convergence Time Analysis Of Ant System Algorithm. Ant colony optimisation (ACO) which is one of the most popular algorithms in machine learning has been used widely to solve combinatorial optimisation problems. However, there are few studies for its runtime analysis which can reflect the computational complexity of ACO algorithm. The presented paper proposes a method for analysing the convergence time of ant system algorithm with pheromone rate. The analysis is a process of estimating the iteration time that pheromone rate attains the objective value and the mean convergence time based on the objective pheromone rate in expectation. The proposed method can be used to analyse the computational complexity of other ACO algorithms. Finally, a brief ant system algorithm is analysed as an example of using the method.\"",
        "Document: \"Gaussian kernel-based fuzzy inference systems for high dimensional regression. We propose a novel architecture for a higher order fuzzy inference system (FIS) and develop a learning algorithm to build the FIS. The consequent part of the proposed FIS is expressed as a nonlinear combination of the input variables, which can be obtained by introducing an implicit mapping from the input space to a high dimensional feature space. The proposed learning algorithm consists of two phases. In the first phase, the antecedent fuzzy sets are estimated by the kernel-based fuzzy c-means clustering. In the second phase, the consequent parameters are identified by support vector machine whose kernel function is constructed by fuzzy membership functions and the Gaussian kernel. The performance of the proposed model is verified through several numerical examples generally used in fuzzy modeling. Comparative analysis shows that, compared with the zero-order fuzzy model, first-order fuzzy model, and polynomial fuzzy model, the proposed model exhibits higher accuracy, better generalization performance, and satisfactory robustness.\"",
        "Document: \"Discovering Many-to-One Causality in Software Project Risk Analysis. Many risk factors affect software development and risk management has become one of the major activities in software development. Discovering causal directions among risk factors and project performance are important support for risk management. The Additive Noise Model (ANM) is an effective algorithm for discovering the direction on one-to-one causalities, but ineffective on many-to-one causalities which are frequent in software project risk analysis (SPRA) process. Thus we proposed a modified ANM with Conditional Probability Table (ANMCPT) to discover the causal direction among risk factors and project performance. The experimental results show our proposed algorithm is effective to discover the many-to-one causalities in SPRM on 498 collected software project data, and it performs better than other algorithms in the prediction with discovered causes of project performance, such as logistic regression, C4.5, Na\u00efve Bayes, and general BNs. This study firstly presents an approach using ANM for many-to-one causality discovery in SPRA and then proves that it is an effective algorithm for analyzing the risk in software project.\"",
        "1 is \"Eddies: continuously adaptive query processing\", 2 is \"Differential privacy via wavelet transforms\"",
        "Given above information, for an author who has written the paper with the title \"Multi-sphere Support Vector Data Description for Outliers Detection on Multi-distribution Data\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005495": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Energy efficiency of an enhanced DCF access method using bidirectional communications for infrastructure-based IEEE 802.11 WLANs':",
        "Document: \"Modeling and analysis of reservation frame slotted-ALOHA in wireless machine-to-machine area networks for data collection. Reservation frame slotted-ALOHA (RFSA) was proposed in the past to manage the access to the wireless channel when devices generate long messages fragmented into small packets. In this paper, we consider an M2M area network composed of end-devices that periodically respond to the requests from a gateway with the transmission of fragmented messages. The idle network is suddenly set into saturation, having all end-devices attempting to get access to the channel simultaneously. This has been referred to as delta traffic. While previous works analyze the throughput of RFSA in steady-state conditions, assuming that traffic is generated following random distributions, the performance of RFSA under delta traffic has never received attention. In this paper, we propose a theoretical model to calculate the average delay and energy consumption required to resolve the contention under delta traffic using RFSA. We have carried out computer-based simulations to validate the accuracy of the theoretical model and to compare the performance for RFSA and FSA. Results show that there is an optimal frame length that minimizes delay and energy consumption and which depends on the number of end-devices. In addition, it is shown that RFSA reduces the energy consumed per end-device by more than 50% with respect to FSA under delta traffic.\"",
        "Document: \"Testing Cooperative Communication Schemes in a Virtual Distributed Testbed of Wireless Networks. It is expected that Next Generation Networks (NGNs) will offer seamless interoperability among heterogeneous access technologies in order to provide ubiquitous access. In such settings, short range technologies may be used in order to extend the coverage area of cellular systems while cooperative diversity can improve the efficiency of the wireless systems. An advanced, backward compatible, with the 802.11 standard, MAC protocol for cooperative ARQ scenarios in NGNs sets the research framework for this work. The functionalities of the RCSMA protocol 1 will be enhanced and the derived analytical models will be validated at the UNITE Virtual Distributed Testbed (VDT) (2).\"",
        "Document: \"Performance analysis of a cluster-based MAC protocol for wireless ad hoc networks. An analytical model to evaluate the non-saturated performance of the Distributed Queuing Medium Access Control Protocol for Ad Hoc Networks (DQMANs) in single-hop networks is presented in this paper. DQMAN is comprised of a spontaneous, temporary, and dynamic clustering mechanism integrated with a near-optimum distributed queuing Medium Access Control (MAC) protocol. Clustering is executed in a distributed manner using a mechanism inspired by the Distributed Coordination Function (DCF) of the IEEE 802.11. Once a station seizes the channel, it becomes the temporary clusterhead of a spontaneous cluster and it coordinates the peer-to-peer communications between the clustermembers. Within each cluster, a near-optimum distributed queuing MAC protocol is executed. The theoretical performance analysis of DQMAN in single-hop networks under non-saturation conditions is presented in this paper. The approach integrates the analysis of the clustering mechanism into the MAC layer model. Up to the knowledge of the authors, this approach is novel in the literature. In addition, the performance of an ad hoc network using DQMAN is compared to that obtained when using the DCF of the IEEE 802.11, as a benchmark reference.\"",
        "Document: \"On the Impact of Repeaters Deployment on WCDMA, Networks Planning. This paper addresses the analysis of WCDMA systems with repeaters deployment. The real different path delays, taking into account the repeaters presence and the finite nature of the time window of Rake receivers are considered. This allows an enhanced analysis with respect to classical approaches from a system level viewpoint. Results show that not taking into account these effects imply erroneous metrics. Indeed, some of the most relevant parameters in network performance evaluation are clearly affected.\"",
        "Document: \"RLNC-Aided Cooperative Compressed Sensing for Energy Efficient Vital Signal Telemonitoring. Wireless Body Area Networks (WBANs) are composed of sensors that either monitor and trans- mit vital signals or act as relays that forward the received data to a Body Node Coordinator (BNC). In this paper, we introduce an energy efficient vital signal telemonitoring scheme, which exploits Com- pressed Sensing (CS) for low-complexity signal com- pression/reconstruction and distributed cooperation for reliable data transmission to the BNC. More specif- ically, we introduce a Cooperative Compressed Sensing (CCS) approach, which increases the energy efficiency of WBANs by exploiting the benefits of Random Linear Network Coding (RLNC). We study the energy effi- ciency of RLNC and compare it with the Store-and- Forward (FW) protocol. Our mathematical analysis shows that the gain introduced by RLNC increases as the link failure rate increases, especially in practical scenarios with a limited number of relays. Furthermore, we propose a reconstruction algorithm that further enhances the benefits of RLNC by exploiting key characteristics of vital signals. With the aid of elec- trocardiographic (ECG) and electroencephalographic (EEG) data available in medical databases, extensive simulation results are illustrated, which validate our theoretical findings and show that the proposed re- covery algorithm increases the energy efficiency of the body sensor nodes by 40% compared to conventional CS-based reconstruction methods.\"",
        "Document: \"A cloud-assisted random linear network coding medium access control protocol for healthcare applications. Relay sensor networks are often employed in end-to-end healthcare applications to facilitate the information flow between patient worn sensors and the medical data center. Medium access control (MAC) protocols, based on random linear network coding (RLNC), are a novel and suitable approach to efficiently handle data dissemination. However, several challenges arise, such as additional delays introduced by the intermediate relay nodes and decoding failures, due to channel errors. In this paper, we tackle these issues by adopting a cloud architecture where the set of relays is connected to a coordinating entity, called cloud manager. We propose a cloud-assisted RLNC-based MAC protocol (CLNC-MAC) and develop a mathematical model for the calculation of the key performance metrics, namely the system throughput, the mean completion time for data delivery and the energy efficiency. We show the importance of central coordination in fully exploiting the gain of RLNC under error-prone channels.\"",
        "Document: \"Energy efficiency of an enhanced DCF access method using bidirectional communications for infrastructure-based IEEE 802.11 WLANs. The Distributed Coordination Function (DCF) is the fundamental access method defined in the IEEE 802.11 Standard for Wireless Local Area Networks (WLANs). With this standard, the Access Point (AP) and the mobile stations consume a significant amount of energy to contend for access to the shared wireless channel. In order to improve the energy efficiency of WLANs, we investigate in this paper a simple and backwards compatible mechanism, called Bi-Directional DCF (BD-DCF), that enables bidirectional communications between wireless stations with a single channel access invocation. The key idea is to let the AP or any mobile station transmit a data packet together with the acknowledgement upon the successful reception of a data packet. This approach reduces the communication overhead, the channel contention, and better balances uplink and downlink transmission opportunities. We evaluate the performance of the proposed BD-DCF by means of computer-based simulation considering different traffic loads, degrees of traffic symmetry, data packet sizes, and data rates. The results presented in this paper show that the BD-DCF protocol can improve the energy efficiency of DCF up to 50%.\"",
        "Document: \"Communication recovery with emergency aerial networks. In spite of the significant advancements in wireless connectivity, the static form of the network infrastructure cannot guarantee an uninterrupted operation of the ever-growing wireless consumer electronics in emergency situations such as natural disasters. In such occasions, employing flexible aerial nodes can tackle this issue by recovering the communication rapidly, when the need for connectivity is of utmost importance. In this paper, we study the use of aerial nodes for communication recovery after a communication breakdown. We provide an analytical model of the recovery probability that demonstrates the capabilities of such networks. In the performance evaluation, we show the effects of the altitude and the distance between the aerial nodes on the recovery probability and verify them with simulations. Moreover, we introduce our testbed and preliminary experimental work that shows promising results for aerial networks. Finally, we discuss useful insights for the network design and present some open issues that exist in this field.\"",
        "Document: \"Performance Analysis of a Cognitive Radio Contention-Aware Channel Selection Algorithm. In cognitive radio (CR) networks, due to the ever increasing traffic demands and the limited spectrum resources, it is very likely for several secondary networks (SNs) to coexist and opportunistically use the same primary user (PU) resources. In such scenarios, the ability to distinguish whether a licensed channel is occupied by a PU or by other SNs can significantly improve the spectrum efficienc...\"",
        "Document: \"Reliable MAC design for ambient assisted living: moving the coordination to the cloud. AAL technologies constitute a new paradigm that promises quality of life enhancements in chronic care patients and elderly people. From a communication perspective, they involve heterogeneous deployments of body and ambient sensors in complex multihop topologies. Such networks can significantly benefit from the application of cooperative schemes based on network coding, where random linear combinations of the original data packets are transmitted in order to exploit diversity. Nevertheless, network coordination is sometimes required to obtain the full potential of these schemes, especially in the presence of channel errors, requiring the design of efficient, reliable, and versatile MAC protocols. Motivated by the recent advances in cloud computing, we investigate the possibility of transferring the network coordination to the cloud while maintaining the data exchange and storage at a local data plane. Hence, we design a general framework for the development of cloudassisted protocols for AAL applications and propose a high-performance and error-resilient MAC scheme with cloud capabilities.\"",
        "1 is \"A New Genetic Algorithm for Scheduling for Large Communication Delays\", 2 is \"Advanced prototype platform for a wireless multimedia local area network\"",
        "Given above information, for an author who has written the paper with the title \"Energy efficiency of an enhanced DCF access method using bidirectional communications for infrastructure-based IEEE 802.11 WLANs\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005604": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Relative reducts in consistent and inconsistent decision tables of the Pawlak rough set model':",
        "Document: \"Evaluation of post-fault restoration strategies in multi-domain networks. Although multi-domain survivability is a major concern for operators, few studies have considered the design of post-fault restoration schemes. This paper proposes two such strategies, based upon hierarchical routing and signaling crankback, to handle single and multi-link failure events in multi-domain IP/MPLS networks (also extendible to optical DWDM networks). Specifically, the hierarchical routing strategy relies upon abstracted domain information to compute inter-domain loose routes, whereas the crankback scheme applies signaling re-tries to restore paths in a domain-by-domain manner. The performance of these proposed solutions is then analyzed and compared via simulation.\"",
        "Document: \"Dual Sentiment Analysis: Considering Two Sides Of One Review. Bag-of-words (BOW) is now the most popular way to model text in statistical machine learning approaches in sentiment analysis. However, the performance of BOW sometimes remains limited due to some fundamental deficiencies in handling the polarity shift problem. We propose a model called dual sentiment analysis (DSA), to address this problem for sentiment classification. We first propose a novel data expansion technique by creating a sentiment-reversed review for each training and test review. On this basis, we propose a dual training algorithm to make use of original and reversed training reviews in pairs for learning a sentiment classifier, and a dual prediction algorithm to classify the test reviews by considering two sides of one review. We also extend the DSA framework from polarity (positive-negative) classification to 3-class (positive-negative-neutral) classification, by taking the neutral reviews into consideration. Finally, we develop a corpus-based method to construct a pseudo-antonym dictionary, which removes DSA's dependency on an external antonym dictionary for review reversion. We conduct a wide range of experiments including two tasks, nine datasets, two antonym dictionaries, three classification algorithms, and two types of features. The results demonstrate the effectiveness of DSA in supervised sentiment classification.\"",
        "Document: \"Delay-Bounded Data Forwarding In Low-Duty-Cycle Sensor Networks. In many sensor network applications, sink node needs to actively communicate with other sensor nodes in order to perform data forwarding operations. For those applications, there is usually a delay-bounded associated with them and require the messages sent to be received within a designated time bound. In energy harvesting sensor networks, limited energy from environment necessitates sensor nodes to operate at a low-duty-cycle. Sensor nodes work active briefly and stay asleep most of time. Such low-duty-cycle operation leads to communication delays in comparison with the always-active networks. In this paper, we address the data forwarding problems in an energy harvesting sensor network where energy efficiency and data freshness need to be balanced. To solve this problem, we propose a utility-based delay bounded scheme for data forwarding, MaxOpUtility-based scheme. MaxOpUtility scheme offers the ability to increase reliability through relay nodes selection as well as to ensure the timeliness. In addition, we show how nodes in the network can cooperatively bound end-to-end delay with maximum utility. Extensive simulations are conducted to verify the effectiveness of our approach compared with the optimal one. Meanwhile, we demonstrate that our solution is able to effectively provide delay bounded guarantee in energy harvesting networks.\"",
        "Document: \"Cryptographic Cloud Storage With Public Verifiability: Ensuring Data Security Of The Yml Framework. YML framework is a well-adapted advanced tool to support designing and executing portable parallel applications over large scale peer to peer and grid middlewares. This work studies the problem of ensuring data security of the YML framework. We define and construct a mechanism that enables us to move the data repository to a public cloud infrastructure where the service provider is not completely trustworthy. To achieve confidentiality, we encrypt the data using the encryption algorithm in our prior work before uploading to the cloud, and then attach pre-classified keywords to them for ciphertext-searching, which are generated by a statistically consistent public-key encryption with keyword search (PEKS) scheme, so the service provider can use the corresponding trapdoor to identify all data containing some specific keywords without learning anything else. To ensure integrity, an elegant verification scheme is proposed, enabling a third party auditor (TPA), on behalf of data owner, to verify the integrity of the (encrypted) data stored in the cloud. The introduction of TPA eliminates the involvement of client through the auditing of whether his data stored in the cloud is indeed intact, which can be important in achieving economies of scale for cloud computing.\"",
        "Document: \"Construction of Barycentric Blending Rational Interpolation Over the Triangular Grids. Laying down the foundation for the basic function of the barycentric rational interpolation, some rational interpolations over all kinds of triangle grids were constructed, and duality theorems and characterization theorems were given, some significative characters are obtained. Compared with the traditional rational interpolation based on continued fraction, the barycentric blending interpolation inherited the advantages of the simple expressions, has many advantages such as small calculation quantity, good numerical stability, no poles and unattainable points, etc. The barycentric blending interpolation can also be extended to both higher dimensions, vector-valued case and matrix-valued case. \u00a9 Springer-Verlag Berlin Heidelberg 2013.\"",
        "Document: \"Research And Application Of A New Rectangle Intersection Algorithm. Rectangles' intersecting includes several situations like intersecting, containing, enclosing and crossing. Rectangle intersection algorithm is a more complex algorithm. Some of traditional algorithms are incomplete, for example, vertex detection algorithm ignores the situation of crossing and edges crossing algorithm ignores the situation of containing and enclosing. Some solved the problem successfully. But they separated the problem into several sub cases which needs to judge all kinds of conditions with complex formulas. In this paper, we design a new rectangle intersection algorithm based on ISO-oriented rectangles which successfully solves the rectangle intersection problem using one simple and perfect judgment formula. We also use this algorithm combined with the Google map technology to achieve a regional retrieval system. It is verified to be simple, comprehensive and practical through the tests.\"",
        "Document: \"Solution of Large Scale Matrix Inversion on Cluster and Grid. Large scale matrix inversion has been used in many domains and block-based Gauss-Jordan (G-J) algorithm as a classical method of large matrix inversion has become the focus of many researchers. Many people show us their parallel version of G-J. But the large parallel granularity in those algorithms restricts the performance of parallel block-based G-J algorithm, especially in the cluster environment consisting of PCs or workstations. This paper presents a fine-grained parallel G-J algorithm to settle the problem presented above. Experiments are made based on YML a framework which enables using different middleware to make large scale parallel computing for its feathers of components reuse, easy programmability for noncomputer professionals. Cluster and Grid environments are based on Grid'5000 platform, France. Experiments show us that the better performance of fine-grained parallel G-J algorithm and YML though overhead existing is a good solution for large scale parallel computing.\"",
        "Document: \"A Novel Cloud Load Balancing Mechanism In Premise Of Ensuring Qos. In premise of ensuring users' QoS, the paper proposes a novel load balancing mechanism in cloud computing environment. The idea takes into account not only the energy efficiency of cloud providers, also the needs of users' QoS. The main work includes: construct QoS model, resource model of cloud infrastructure and the mapping between low-level resource metrics and QoS attributes; model load status of the virtual machine instance, and estimate the resource utilization ratio of the virtual machine cluster quantitatively; design the task scheduling algorithm and elastic scaling algorithm to achieve the tasks distribution and the elastic scaling of the virtual machines cluster respectively. By simulation using CloudSim platform, the result shows that this novel method has better load balance degree and complement time compared with other common algorithms such as round robin and green load, achieving the purpose of optimal load balancing in premise of ensuring QoS.\"",
        "Document: \"Enhanced Crankback Signaling for Multi-Domain Traffic Engineering. Multi-domain traffic engineering is a major focus area for carriers today and crankback signaling offers a very promising and viable alternative here. Although some initial crankback studies have been done, there is still significant latitude for improving multi-domain crankback performance. To address these concerns, this paper studies realistic IP/MPLS multi-domain networks and proposes a novel solution for joint intra/inter-domain signaling crankback. Namely, dynamic intra-domain link-state routing information is coupled with inter-domain path/distance-vector routing state to improve the overall search process. Mechanisms are also introduced to limit setup signaling overheads/delays. The performance of the proposed solution is then analyzed using simulation and compared against other crankback techniques as well as hierarchical inter-domain routing strategies. \u00a92010 IEEE.\"",
        "Document: \"Clustering-weighted SIFT-based classification method via sparse representation. In recent years, sparse representation-based classification (SRC) has received significant attention due to its high recognition rate. However, the original SRC method requires a rigid alignment, which is crucial for its application. Therefore, features such as SIFT descriptors are introduced into the SRC method, resulting in an alignment-free method. However, a feature-based dictionary always contains considerable useful information for recognition. We explore the relationship of the similarity of the SIFT descriptors to multitask recognition and propose a clustering-weighted SIFT-based SRC method (CWS-SRC). The proposed approach is considerably more suitable for multitask recognition with sufficient samples. Using two public face databases (AR and Yale face) and a self-built car-model database, the performance of the proposed method is evaluated and compared to that of the SRC, SIFT matching, and MKD-SRC methods. Experimental results indicate that the proposed method exhibits better performance in the alignment-free scenario with sufficient samples. (C) The Authors. Published by SPIE under a Creative Commons Attribution 3.0 Unported License.\"",
        "1 is \"Convergence and stability results of Zhang neural network solving systems of time-varying nonlinear equations.\", 2 is \"Local reduction of decision system with fuzzy rough sets\"",
        "Given above information, for an author who has written the paper with the title \"Relative reducts in consistent and inconsistent decision tables of the Pawlak rough set model\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005627": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Distributed secret sharing scheme based on personalized spherical coordinates space.':",
        "Document: \"Energy-Balanced Density Control To Avoid Energy Hole For Wireless Sensor Networks. Density control is of great relevance for wireless sensor networks monitoring hazardous applications where sensors are deployed with high density. Due to the multihop relay communication and many-to-one traffic characters in wireless sensor networks, the nodes closer to the sink tend to die faster, causing a bottleneck for improving the network lifetime. In this paper, the theoretical aspects of the network load and the node density are investigated systematically. And then, the accessibility condition to satisfy that all the working sensors exhaust their energy with the same ratio is proved. By introducing the concept of the equivalent sensing radius, a novel algorithm for density control to achieve balanced energy consumption per node is thus proposed. Different from other methods in the literature, a new pixel-based transmission mechanism is adopted, to reduce the duplication of the same messages. Combined with the accessibility condition, nodes on different energy layers are activated with a nonuniform distribution, so as to balance the energy depletion and enhance the survival of the network effectively. Extensive simulation results are presented to demonstrate the effectiveness of our algorithm.\"",
        "Document: \"An evolutionary game-based spectrum sharing scheme. Spectrum shortage and low spectrum utilization have attracted much attention. In order to meet the communication demands of users, the dynamic and efficient spectrum management scheme with cognitive radio technology has been used. Spectrum share is the key component in the dynamic spectrum management. To enable cognitive user (CU) to share spectrum fairly, an evolutionary game-based spectrum sharing scheme is designed based on the results of spectrum sensing. The knowledge of fuzzy mathematic is introduced to characterize the parameters related to cognitive user satisfaction degree (CUSD). Gas Brownian motion optimization (GBMO) is used to solve the problem of spectrum access cost of CU, the fairness of spectrum sharing is considered in utility function, the dynamic equilibrium is solved according to replicator dynamic equation (RDE), finally, the detailed implementation steps of the scheme are given. Simulation results have shown that the proposed scheme is both feasible and effective.\"",
        "Document: \"An Intelligent Economic Approach for Dynamic Resource Allocation in Cloud Services. With Inter-Cloud, distributed cloud and Open Cloud Exchange (OCX) emerging, a comprehensive resource allocation approach is fundamental to highly competitive cloud market. Oriented to Infrastructure as a Service (IaaS), an Intelligent Economic approach for Dynamic resource Allocation (IEDA) is proposed with the improved combinatorial double auction protocol devised to enable various kinds of resources traded among multiple consumers and multiple providers at the same time enable task partitioning among multiple providers. To make bidding and asking reasonable in each round of the auction and determine eligible transaction relationship among providers and consumers, a price formation mechanism is proposed, which is consisted of a Back Propagation Neural Network (BPNN) based price prediction algorithm and a price matching algorithm. A reputation system is proposed and integrated to exclude dishonest participants from the cloud market. The Winner Determination Problem (WDP) is solved by the improved Paddy Field Algorithm (PFA). Simulation results have shown that IEDA can not only help maximize market surplus and surplus strength but also encourage participants to be honest.\"",
        "Document: \"ACO-inspired ICN Routing Scheme with Density-Based Spatial Clustering. This paper proposes a new Ant Colony Optimization (ACO)-inspired Information-Centric Networking (ICN) routing scheme with Density-based Spatial Clustering (DSC). At first, the content concentration model is established with network load to address interest routing; in particular, we investigate the failed situation of content retrieval. Then, the dot product method is used to compute similarity relation between two routers, which is considered as clustering reference attribute. In addition, DSC is exploited to detect core nodes which are used to cache contents during the data routing process. Finally, the proposed scheme is simulated over Mini-NDN, and the results show that it has better performance than the benchmark scheme.\"",
        "Document: \"Survivable Routing with Risk-Level Disjoint in WDM Optical Networks. This paper investigates the survivable routing in backbone optical transport networks with shared-risk link groups (SRLG) constraints, and proposes a new risk-level disjoint protection (RLDP) algorithm for provide different service level for different connection request. Compared with the conventional SRLG- disjoint protection (SDP) algorithm, RLDP performs better resource utilization and blocking probability. Simulation results are shown to be promising.\"",
        "Document: \"Modelling, evaluating, designing and maximising resource allocation revenue by an auction mechanism in cloud computing environments. Cloud computing is still in its infancy in spite of gaining significant momentum recently. Allocating cloud resources and taking both optimisation and fairness into account is one of the major challenges. In this paper, a novel cloud resource allocation algorithm through an auction mechanism to maximise resource revenue is put forward with a brief survey of resource allocation suitable for large-scale distributed computing environments. It includes: a) modelling a m*n type of resource allocation for cloud system; b) evaluating user bid price, deciding resource request price, and establishing auction price by a proposed auction mechanism; c) designing a cloud resource allocation algorithm in a cloud system. Experimental results conclusively demonstrate that the algorithm with the auction mechanism takes both optimisation and fairness into account, and maximises resource allocation revenue by the proposed auction mechanism in cloud computing environments.\"",
        "Document: \"Multi-layer survivable routing mechanism in GMPLS based optical networks. The structure of IP/MPLS over optical networks has become the trend for new generation backbone transport networks since the technology of general multi-protocol label switching (GMPLS) can realize the seamless converage between IP and optical networks. At the same time, due to the high-speed transmission rate of each wavelength in optical networks, survivability is a very important issue and has been studied all through these years. Therefore, in new generation GMPLS based networks, it is very necessary to investigate the multi-layer survivable algorithms. In this paper, we comprehensively review the existing survivable algorithms in multi-layer networks and analyze the shortages of current researches. Based on previous studies, we prospect challenges and propose new solutions of designing efficient survivable algorithms to well guide the future work of researchers in multi-layer networks.\"",
        "Document: \"An Auction and League Championship Algorithm Based Resource Allocation Mechanism for Distributed Cloud. In cloud computing, all kinds of idle resources can be pooled to establish a resource pool, and different kinds of resources combined as a service is provided to users through virtualization. Therefore, an effective mechanism is necessary for managing and allocating the resources. In this paper, we propose a double combinatorial auction based allocation mechanism based on the characteristics of cloud resources and inspired by the flexibility and effectiveness of microeconomic methods. The feedback evaluation based reputation system with attenuation coefficient of time and the hierarchy of users introduced is implemented to avoid malicious behavior. In order to make decisions scientifically, we propose a price decision mechanism based on a BP back propagation neural network, in which various factors are taken into account, so the bidding/asking prices can adapt to the changing supply-demand relation in the market. Since the winner determination is an NP hard problem, a league championship algorithm is introduced to achieve optimal allocation with the optimization goals being market surplus and total reputation. We also conduct empirical studies to demonstrate the feasibility and effectiveness of the proposed mechanism.\"",
        "Document: \"Performance Analysis of Hybrid Control Serial Production Lines. This paper studied a multiple-stage tandem production system. The control strategy is Hybrid which combines Kanban and CONWIP strategy together. We focus on Work-In-Process (WIP) and service lost rate of the system. Thus we propose some approximation methods here. We consider a simple situation and formulate this production control as a Markov chain process. Then we derive the steady-state probability of the system for two and three machines case, obtain the WIP and the service lost rate. The formulations proposed show the connection of control cards and performance measure. We also present the results of a simulation study that tests the performance of our approach.\"",
        "Document: \"A new multi-granularity grooming algorithm based on traffic partition in IP over WDM networks. Currently, the required bandwidth of IP-level users tends to be diversity, i.e., coarse-granularity demands and small-granularity demands. Employing traffic grooming or waveband switching (WS) only may results in several traffic-diversity problems in wavelength routed networks (WRNs) or WaveBand Switching (WBS) networks. In order to solve the traffic-diversity problems in IP over WDM networks, an identified multi-granularity optical cross-connect (MG-OXC) with supporting traffic partition is devised in this paper. In this node structure, one traffic partition module is used to check the type of demand. Followed by our proposed traffic partition based grooming policy (TPGP), if the currently-arriving IP-level user is small-granularity demand, it will be processed by traffic grooming, otherwise processed by waveband switching. Then, we propose a new multi-granularity grooming algorithm based on integrated grooming auxiliary graph (MG-IGAG). The IGAG includes a wavelength integrated auxiliary graph (WIAG) and a waveband integrated auxiliary graph (BIAG) to groom small-granularity and coarse-granularity demands, respectively. Simulation results show that compared to previous WS algorithms and multi-granularity grooming algorithms followed by integrated grooming policy (IGP), MG-IGAG not only performs traffic grooming and waveband switching together, but also solves the traffic-diversity problems effectively, especially for port savings.\"",
        "1 is \"A Multicast Routing Algorithm Using Multiobjective Optimization\", 2 is \"A Two-Stage Approach to Harmonic Rejection Mixing Using Blind Interference Cancellation\"",
        "Given above information, for an author who has written the paper with the title \"Distributed secret sharing scheme based on personalized spherical coordinates space.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005687": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'An abstraction-based approach to measuring the structural similarity between two unordered XML documents':",
        "Document: \"Dynamic reasoning on XML updates. In many emerging XML application contexts and distributed execution environments (like disconnected and cloud computing, collaborative editing and document versioning) the server that determines the updates to be performed on a document, by evaluating an XQuery Update expression, is not always the same that actually makes such updates -represented as Pending Update Lists (PULs)-effective. The process of generating the PUL is thus decoupled from that of making its effect persistent on the document. The PUL executor needs to manage several PULs, that, depending on the application context, are to be executed as sequential or parallel update requests, possibly relying on application-specific policies. This requires some capabilities of dynamic reasoning on updates. In the paper, we state the most relevant properties to reason on, develop the corresponding algorithms and present a PUL handling system, providing an experimental evaluation of this system.\"",
        "Document: \"GOssTo: a stand-alone application and a web tool for calculating semantic similarities on the Gene Ontology. We present GOssTo, the Gene Ontology semantic similarity Tool, a user-friendly software system for calculating semantic similarities between gene products according to the Gene Ontology. GOssTo is bundled with six semantic similarity measures, including both term-and graph-based measures, and has extension capabilities to allow the user to add new similarities. Importantly, for any measure, GOssTo can also calculate the Random Walk Contribution that has been shown to greatly improve the accuracy of similarity measures. GOssTo is very fast, easy to use, and it allows the calculation of similarities on a genomic scale in a few minutes on a regular desktop machine.\"",
        "Document: \"Handling Semi-Structured Data through an Extended Object-Oriented Data Model.  In traditional database applications the structure of data ispre-defined, and data are entered into the database specifying the schemaelement (relation or class, depending on the paradigm) they belong to.New emerging database applications, expecially those related to the Web,are characterized by data that have an irregular, heterogeneous, partialstructure that quickly evolves. In this paper we adapt an object-orienteddata model to this kind of data, by providing it with more... \"",
        "Document: \"A Trigger-Based Approach for Communication Personalization. Communication personalization is assuming more and more relevance nowadays. The possibility to specify policies for being reached from other people as well as policies for specifying means by which reaching other people is considered from the research community as well as the Telecommunication Companies. In this paper we present a policy language and its engine for specifying and enforcing policies for routing messages arriving at a subscribed user. These policies are on the characteristics of the actors involved in the communication process: the users, the devices they hold, the message/call, and the environment in which the communication occurs.\"",
        "Document: \"An abstraction-based approach to measuring the structural similarity between two unordered XML documents. Measuring the structural similarity between two document-centric XML documents is a well-known problem. Many approaches have been proposed that, by fixing constraints in the evaluation of similarity, compute the similarity with polynomial time complexity. Despite this huge activity for document-centric XML documents, no approaches have been presented so far for data-centric XML documents (i.e., XML documents for which order among elements is not relevant) mainly because this problem is, in general, NP-complete. In this paper we propose an approach for measuring the structural similarity between two data-centric XML documents that, when the structure of elements with the same label is not heterogenous, is polynomial.\"",
        "Document: \"Navigational Path Expressions on XML Schemas. XML Schema is employed for describing the type and structure of information contained in valid XML documents. As for a document, a schema can be navigated and its components can be identified through a path language. In this paper we discuss the drawbacks of using XPath for this purpose and present XSPath, a language tailored for specifying path expressions on schemas.\"",
        "Document: \"Reverting the effects of XQuery update expressions. The need of reverting the effects of updates on the affected documents arises in many contexts, ranging from undos in transactional applications to versioning systems. In this paper, we investigate this issue for XQuery Update expressions, relying on the Pending Update List (PUL) obtained from the evaluation of an expression on a document. Specifically, we introduce an inversion operator, that, given a PUL to be applied on a document, allows to determine a corresponding inverted PUL that, applied on the modified document, produces the original document. Moreover, an alternative approach for enriching a PUL with additional information, so that it can be inversely applied, is proposed and the two approaches are experimentally compared.\"",
        "Document: \"A semantic information retrieval advertisement and policy based system for a P2P network. In this paper we propose a semantic based P2P system that incorporates peer sharing policies, which allow a peer to state, for each of the concepts it deals with, the conditions under which it is available to process requests related to that concept. The semantic routing approach, based on advertisements and peer behavior in answering previous requests, takes also into account sharing policies.\"",
        "Document: \"Time-completeness trade-offs in record linkage using adaptive query processing. Applications that involve data integration among multiple sources often require a preliminary step of data reconciliation in order to ensure that tuples match correctly across the sources. In dynamic settings such as data mashups, however, traditional offline data reconciliation techniques that require prior availability of the data may not be applicable. The alternative, performing similarity joins at query time, is computationally expensive, while ignoring the mismatch problem altogether leads to an incomplete integration. In this paper we make the assumption that, in some dynamic integration scenarios, users may agree to trade the completeness of a join result in return for a faster computation. We explore the consequences of this assumption by proposing a novel, hybrid join algorithm that involves a combination of exact and approximate join operators, managed using adaptive query processing techniques. The algorithm is optimistic: it can switch between physical join operators multiple times throughout query processing, but it only resorts to approximate join operators when there is statistical evidence that result completeness is compromised. Our experiments show that sensible savings in join execution time can be achieved in practice, at the expense of a modest reduction in result completeness.\"",
        "Document: \"XML data clustering: An overview. In the last few years we have observed a proliferation of approaches for clustering XML documents and schemas based on their structure and content. The presence of such a huge amount of approaches is due to the different applications requiring the clustering of XML data. These applications need data in the form of similar contents, tags, paths, structures, and semantics. In this article, we first outline the application contexts in which clustering is useful, then we survey approaches so far proposed relying on the abstract representation of data (instances or schema), on the identified similarity measure, and on the clustering algorithm. In this presentation, we aim to draw a taxonomy in which the current approaches can be classified and compared. We aim at introducing an integrated view that is useful when comparing XML data clustering approaches, when developing a new clustering algorithm, and when implementing an XML clustering component. Finally, the article moves into the description of future trends and research issues that still need to be faced.\"",
        "1 is \"You are who you know: inferring user profiles in online social networks\", 2 is \"Modeling and Analysis of Workflows Using Petri Nets\"",
        "Given above information, for an author who has written the paper with the title \"An abstraction-based approach to measuring the structural similarity between two unordered XML documents\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005753": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Inferring (Biological) Signal Transduction Networks via Transitive Reductions of Directed Graphs':",
        "Document: \"Parking slot assignment games. With the proliferation of location-based services, mobile devices, and embedded wireless sensors, more and more applications are being developed to improve the efficiency of the transportation system. In particular, new applications are arising to help vehicles locate open parking spaces. Nevertheless, while engaged in driving, travelers are better suited being guided to a particular and ideal parking slot, than looking at a map and choosing which spot to go to. Then the question of how an application should choose this ideal parking spot becomes relevant. Vehicular parking can be viewed as vehicles (players) competing for parking slots (resources with different costs). Based on this competition, we present a game-theoretic framework to analyze parking situations. We introduce and analyze Parking Slot Assignment Games (Psag) in complete and incomplete information contexts. For both models we present algorithms for individual players to choose parking spaces ideally. To evaluate the more realistic incomplete information Psag, simulations were performed to test the performance of various proposed algorithms.\"",
        "Document: \"On approximate learning by multi-layered feedforward circuits. We deal with the problem of efficient learning of feedforward neural networks. First, we consider the objective to maximize the ratio of correctly classified points compared to the size of the training set. We show that it is NP-hard to approximate the ratio within some constant relative error if architectures with varying input dimension, one hidden layer, and two hidden neurons are considered where the activation function in the hidden layer is the sigmoid function, and the situation of epsilon-separation is assumed, or the activation function is the semilinear function. For single hidden layer threshold networks with varying input dimension and n hidden neurons, approximation within a relative error depending on n is NP-hard even if restricted to situations where the number of examples is limited with respect to n.Afterwards, we consider the objective to minimize the failure ratio in the presence of misclassification errors. We show that it is NP-hard to approximate the failure ratio within any positive constant for a multilayered threshold network with varying input dimension and a fixed number of neurons in the hidden layer if the thresholds of the neurons in the first hidden layer are zero. Furthermore, even obtaining weak approximations is almost NP-hard in the same situation.\"",
        "Document: \"Merging Query Results From Local Search Engines for Georeferenced Objects. The emergence of numerous online sources about local services presents a need for more automatic yet accurate data integration techniques. Local services are georeferenced objects and can be queried by their locations on a map, for instance, neighborhoods. Typical local service queries (e.g., \u201cFrench Restaurant in The Loop\u201d) include not only information about \u201cwhat\u201d (\u201cFrench Restaurant\u201d) a user is searching for (such as cuisine) but also \u201cwhere\u201d information, such as neighborhood (\u201cThe Loop\u201d). In this article, we address three key problems: query translation, result merging and ranking. Most local search engines provide a (hierarchical) organization of (large) cities into neighborhoods. A neighborhood in one local search engine may correspond to sets of neighborhoods in other local search engines. These make the query translation challenging. To provide an integrated access to the query results returned by the local search engines, we need to combine the results into a single list of results. Our contributions include: (1) An integration algorithm for neighborhoods. (2) A very effective business listing resolution algorithm. (3) A ranking algorithm that takes into consideration the user criteria, user ratings and rankings. We have created a prototype system, Yumi, over local search engines in the restaurant domain. The restaurant domain is a representative case study for the local services. We conducted a comprehensive experimental study to evaluate Yumi. A prototype version of Yumi is available online.\"",
        "Document: \"On the Computational Complexity of Measuring Global Stability of Banking Networks. Threats on the stability of a financial system may severely affect the functioning of the entire economy, and thus considerable emphasis is placed on the analyzing the cause and effect of such threats. The financial crisis in the current and past decade has shown that one important cause of instability in global markets is the so-called financial contagion , namely the spreadings of instabilities or failures of individual components of the network to other, perhaps healthier, components. This leads to a natural question of whether the regulatory authorities could have predicted and perhaps mitigated the current economic crisis by effective computations of some stability measure of the banking networks. Motivated by such observations, we consider the problem of defining and evaluating stabilities of both homogeneous and heterogeneous banking networks against propagation of synchronous idiosyncratic shocks given to a subset of banks. We formalize the homogeneous banking network model of Nier et al. (J. Econ. Dyn. Control 31:2033---2060, 2007 ) and its corresponding heterogeneous version, formalize the synchronous shock propagation procedures outlined in (Nier et al. J. Econ. Dyn. Control 31:2033---2060, 2007 ; M. Eboli Mimeo, 2004 ), define two appropriate stability measures and investigate the computational complexities of evaluating these measures for various network topologies and parameters of interest. Our results and proofs also shed some light on the properties of topologies and parameters of the network that may lead to higher or lower stabilities.\"",
        "Document: \"On communication protocols that compute almost privately. A traditionally desired goal when designing auction mechanisms is incentive compatibility, i.e., ensuring that bidders fare best by truthfully reporting their preferences. A complementary goal, which has, thus far, received significantly less attention, is to preserve privacy, i.e., to ensure that bidders reveal no more information than necessary. We further investigate and generalize the approximate privacy model for two-party communication recently introduced by Feigenbaum et al. [8]. We explore the privacy properties of a natural class of communication protocols that we refer to as \"dissection protocols\". Dissection protocols include, among others, the bisection auction in [9,10] and the bisection protocol for the millionaires problem in [8]. Informally, in a dissection protocol the communicating parties are restricted to answering simple questions of the form \"Is your input between the values \u03b1 and \u03b2 (under a pre-defined order over the possible inputs)?\". We prove that for a large class of functions called tiling functions, which include the 2nd-price Vickrey auction, there always exists a dissection protocol that provides a constant average-case privacy approximation ratio for uniform or \"almost uniform\" probability distributions over inputs. To establish this result we present an interesting connection between the approximate privacy framework and basic concepts in computational geometry. We show that such a good privacy approximation ratio for tiling functions does not, in general, exist in the worst case. We also discuss extensions of the basic setup to more than two parties and to non-tiling functions, and provide calculations of privacy approximation ratios for two functions of interest.\"",
        "Document: \"Locating a Tree in a Reticulation-Visible Network in Cubic Time.   In this work, we answer an open problem in the study of phylogenetic networks. Phylogenetic trees are rooted binary trees in which all edges are directed away from the root, whereas phylogenetic networks are rooted acyclic digraphs. For the purpose of evolutionary model validation, biologists often want to know whether or not a phylogenetic tree is contained in a phylogenetic network. The tree containment problem is NP-complete even for very restricted classes of networks such as tree-sibling phylogenetic networks. We prove that this problem is solvable in cubic time for stable phylogenetic networks. A linear time algorithm is also presented for the cluster containment problem. \"",
        "Document: \"Parking in Competitive Settings: A Gravitational Approach. With the proliferation of location-based services, mobile devices, and embedded wireless sensors, more and more applications are being developed to improve the efficiency of the transportation system. In particular, new applications are arising to help vehicles locate open parking slots. Nevertheless, while engaged in driving, travelers are better suited being guided to an ideal parking slot, than looking at a map and choosing which slot to go to. Then the question of how an application should choose this ideal parking slot becomes relevant. Vehicular parking can be viewed as vehicles (players) competing for parking slots (resources with different costs). Based on this competition, we present a game-theoretic framework to analyze parking situations. We introduce and analyze parking slot assignment games and present algorithms that choose parking slots ideally in competitive parking simulations. We also present algorithms for incomplete information contexts and show how these algorithms outperform even algorithms with complete information in some cases.\"",
        "Document: \"Spatio-temporal matching algorithms for road networks. In this paper we present a model of spatially located mobile agents and static resources, in which the agents are looking to obtain one of the resources while minimizing their costs to obtain the resource. The proliferation of mobile devices, location-based services and embedded wireless sensors has given rise to applications that could help the mobile agents have updated information of the location of the resources they are looking for. Nevertheless, while engaged in driving, travelers are better suited being guided to an ideal resource, rather than looking at a map and deciding which available resource to visit. Then the question of how an application should choose this ideal resource, to guide the agent towards it, becomes relevant. In this work we develop algorithms that are designed to guide users to these resources. They use a gravitational approach to guide a mobile agent through a road network in order to find this ideal resource. The performance of the algorithms is evaluated through simulations.\"",
        "Document: \"Analog versus discrete neural networks. We show that neural networks with three-times continuously differentiable activation functions are capable of computing a certain family of n-bit boolean functions with two gates, whereas networks composed of binary threshold functions require at least omega(log n) gates. Thus, for a large class of activation functions, analog neural networks can be more powerful than discrete neural networks, even when computing Boolean functions.\"",
        "Document: \"Classifying trend movements in the MSCI U.S.A. capital market index\u2014a comparison of regression, ARIMA and neural network methods. This paper describes our initial results in applying neural networks to forecast the MSCI U.S.A. Capital Market Index. The objective is to test the ability of an non-parametric learning network to provide valuable information to a global portfolio manager, who needs to assess investment opportunities in equity markets in order to shape a one month ahead asset allocation. Primarily, the objective is to test the directional classification properties of the method with secondary objectives of higher magnitude prediction and lower RMS error. The system achieved fairly good results on the directional classification criteria as well as the other criteria mentioned both in absolute terms and in comparison with multiple linear regression and two ARIMA models.\"",
        "1 is \"A maximum profit coverage algorithm with application to small molecules cluster identification\", 2 is \"HMM Logos for visualization of protein families.\"",
        "Given above information, for an author who has written the paper with the title \"Inferring (Biological) Signal Transduction Networks via Transitive Reductions of Directed Graphs\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005759": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Impact of estimated CSI quantization on secrecy rate loss in pilot-aided MIMO systems':",
        "Document: \"Peak-to-average power ratio and power efficiency considerations in MIMO-OFDM systems. Peak-to-average power ratio (PAR) reduction is a powerful tool that can increase power efficiency and reduce distortion noise in MIMO-OFDM systems. Various PAR metrics have been used in the MIMO-OFDM literature without justifications regarding the physical mechanisms that support those PAR definitions. In this paper, we show that in order to deliver high power efficiency, two MIMO-OFDM linear-scal...\"",
        "Document: \"Parallel Weighted Bit-Flipping Decoding. A parallel weighted bit-flipping (PWBF) decoding algorithm for low-density parity-check (LDPC) codes is proposed. Compared to the best known serial weighted bit-flipping decoding, the PWBF decoding converges significantly faster but with little performance penalty. For decoding of finite-geometry LDPC codes, we demonstrate through examples that the proposed PWBF decoding converges in about 5 iterations with performance very close to that of the standard belief-propagation decoding.\"",
        "Document: \"Generation-Based Network Coding over Networks with Delay. The generation-based network coding has been successfully employed in overlay networks. The key idea is the generation-marking for packets along with the suitable control of buffering and transmission opportunity at nodes in network. Building on recent work by Chou et al., who present the generation-based network coding for multicast networks, we extend this framework to arbitrary network coding problems over networks with delay. It is shown that generation-based technique can transform the nonzero-delay network into the zero-delay network and however, inappropriate transmission opportunity or short of memory at nodes may change the topology of the original network.\"",
        "Document: \"On the BCJR algorithm for asynchronous physical-layer network coding. In practical asynchronous bi-directional relaying, symbols transmitted by two source nodes cannot arrive at the re- lay with perfect symbol alignment and the symbol-asynchronous multiple-access channel (MAC) should be seriously considered. Recently, Lu et al. proposed a Tanner-graph representation of symbol-asynchronous MAC with rectangular-pulse shaping and further developed the message-passing algorithm for optimal decoding of the asynchronous physical-layer network coding. In this paper, we present a general channel model for the asynchronous multiple-access channel with arbitrary pulse-shaping. Then, the Bahl, Cocke, Jelinek, and Raviv (BCJR) algorithm is developed for optimal decoding of asynchronous MAC channel. This formulation can be well employed to develop various low- complexity algorithms, such as Log-MAP algorithm, Max-Log- MAP algorithm, which are favorable in practice.\"",
        "Document: \"Constrained Ls Algorithm For Channel Vector Estimation In 2-D Rake Receiver. In conventional 2-D RAKE receiver, channel vector is usually estimated based on LS criterion only using pilot. When the power of the pilot is low or the variation of the channel is fast, the error of the channel estimation becomes large. We propose an improved algorithm in this paper. The steering vector of the desired signal is estimated firstly using subspace decomposition method and a constrained condition is then configured employing this estimated steering vector. Channel vector is estimated based on constrained LS criterion applying traffic signals. The pilot is only used to get the initial value of the estimation. The estimation accuracy of channel vector can progressively increase through iteration. Simulation results demonstrate that the improved algorithm upgrades the estimation accuracy effectively.\"",
        "Document: \"Transmission capacity maximization for LED array-assisted multiuser VLC systems. In this paper, we present a light-emitting diode (LED) array-assisted visible light communication (VLC) system. Considering the requirements of illumination, the human eye safety, and the energy saving, we optimize the optical power allocation (PA) to maximize the transmission capacity of the overall system. Specifically, we discuss two typical scenarios where the LED array in each room serves multiple users either simultaneously or in a round-robin manner. The problem corresponding to the first case turns out to be nonconvex whose globally optimal solution is hard to obtain. To address this difficult problem, we first adopt the famous gradient projection algorithm to obtain a locally optimal solution, then we employ powerful convex relaxation techniques, and achieve a high-quality solution. While for the round-robin case, we are able to derive an optimal solution in closed form by virtue of Karush-Kuhn-Tucker conditions. Interestingly, the derived closed-form solution follows a water-filling pattern with both square root and two-side clipping operations, which differs from the conventional water-filling-like PA in radiofrequency channels. Numerical results depict the performance advantage provided by the proposed PA strategies under various settings.\"",
        "Document: \"Closed-Form Expressions for Secrecy Capacity over Correlated Rayleigh Fading Channels. We investigate the secure communications over correlated wiretap Rayleigh\nfading channels assuming the full channel state information (CSI) available.\nBased on the information theoretic formulation, we derive closed-form\nexpressions for the average secrecy capacity and the outage probability.\nSimulation results confirm our analytical expressions.\"",
        "Document: \"Magnitude-Scaled Selected Mapping: A Crest Factor Reduction Scheme for OFDM Without Side-Information Transmission. Selected mapping (SLM) is a distortionless crest factor reduction (CFR) method for orthogonal frequency division multiplexing (OFDM) transmission. With SLM, it is possible to reduce the peak-to-average power ratio (PAR) of an OFDM symbol by several decibels. In this paper, we propose a method for SLM phase sequence detection that does not require side information transmission. We refer to this method as magnitude-scaled SLM, in the sense that it scales the frequency-domain power profile of the OFDM symbol with an envelope function from a set of pre-determined envelope functions. From the envelope of the received symbol, the receiver can detect which envelope and thus which phase sequence was used in the transmission. Also presented in this paper are the theoretical characterizations of the detection error rate (DER) and symbol error rate (SER) in a magnitude-scaled SLM system. Compared with ordinary OFDM without CFR, magnitude-scaled SLM can achieve an order of magnitude SER improvement in a peak-power-limited channel.\"",
        "Document: \"An efficient adaptive receiver for MIMO-OFDM systems. In this paper, we present an adaptive receiver for multiple-input multiple-output orthogonal frequency-division multiplexing (MIMO-OFDM) systems. The adaptive algorithm switches between the maximum likelihood detection (MLD) and the minimum mean square error (MMSE) detection according to the channel condition. By exploiting the correlation in both time and frequency domains of OFDM systems, we reduce the complexity of the algorithm further. Simulation in the long term evolution (LTE) system shows that the adaptive detection can achieve high performance with low complexity.\"",
        "Document: \"An Improvement on LDPC Coded Queued Codes. The queued-code based on low-density parity-check (LDPC) codes, which exploit both the near Shannon-limit performance LDPC codes and the instantaneous channel state information (CSI), can provide excellent performance in the block fading channels. The existing work utilized the conventional random puncturing and repetition operation, which may lead to considerable performance loss due to the changed intrinsic connections of parity-check matrix (PCM) for the receiver. In this paper, we present a reversed sequential puncturing (RSP) scheme and a construction method with modified dual-diagonal structure for the LDPC coded queued-code. The consecutive puncturing pattern in reversed direction can ensure the uniform row-weight distribution of the punctured PCM. The modified dual-diagonal form and the RSP scheme can ensure no more than one punctured symbol involved in each check-sum equation. The simulation results demonstrate that our proposed scheme can achieve noticeable performance gains.\"",
        "1 is \"LDPC Code Design for Half-Duplex Cooperative Relay\", 2 is \"On Performance of Quantized Transceiver in Multiuser Massive MIMO Downlinks.\"",
        "Given above information, for an author who has written the paper with the title \"Impact of estimated CSI quantization on secrecy rate loss in pilot-aided MIMO systems\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005769": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Improving alignment in Tract-based spatial statistics: Evaluation and optimization of image registration.':",
        "Document: \"Segmentation of brain MR images through a hidden Markov random field model and the expectation-maximization algorithm. The finite mixture (FM) model is the most commonly used model for statistical segmentation of brain magnetic resonance (MR) images because of its simple mathematical form and the piecewise constant nature of ideal brain MR images. However, being a histogram-based model, the FM has an intrinsic limitation--no spatial information is taken into account. This causes the FM model to work only on well-defined images with low levels of noise; unfortunately, this is often not the the case due to artifacts such as partial volume effect and bias field distortion. Under these conditions, FM model-based methods produce unreliable results. In this paper, we propose a novel hidden Markov random field (HMRF) model, which is a stochastic process generated by a MRF whose state sequence cannot be observed directly but which can be indirectly estimated through observations. Mathematically, it can be shown that the FM model is a degenerate version of the HMRF model. The advantage of the HMRF model derives from the way in which the spatial information is encoded through the mutual influences of neighboring sites. Although MRF modeling has been employed in MR image segmentation by other researchers, most reported methods are limited to using MRF as a general prior in an FM model-based approach. To fit the HMRF model, an EM algorithm is used. We show that by incorporating both the HMRF model and the EM algorithm into a HMRF-EM framework, an accurate and robust segmentation can be achieved. More importantly, the HMRF-EM framework can easily be combined with other techniques. As an example, we show how the bias field correction algorithm of Guillemaud and Brady (1997) can be incorporated into this framework to achieve a three-dimensional fully automated approach for brain MR image segmentation.\"",
        "Document: \"Signal and noise characteristics of SSFP FMRI: A comparison with GRE at multiple field strengths. Recent work has proposed the use of steady-state free precession (SSFP) as an alternative to the conventional methods for obtaining functional MRI (FMRI) data. The contrast mechanism in SSFP is likely to be related to conventional FMRI signals, but the details of the signal changes may differ in important ways. Functional contrast in SSFP has been proposed to result from several different mechanisms, which are likely to contribute in varying degrees depending on the specific parameters used in the experiment. In particular, the signal dynamics are likely to differ depending on whether the sequence is configured to scan in the SSFP transition band or passband. This work describes experiments that explore the source of SSFP FMRI signal changes by comparing SSFP data to conventional gradient-recalled echo (GRE) data. Data were acquired at a range of magnetic field strengths and repetition times, for both transition band and passband methods. The signal properties of SSFP and GRE differ significantly, confirming a different source of functional contrast in SSFP. In addition, the temporal noise properties are significantly different, with important implications for SSFP FMRI sequence optimisation.\"",
        "Document: \"Functional Connectivity under Anticipation of Shock: Correlates of Trait Anxious Affect versus Induced Anxiety. Sustained anxiety about potential future negative events is an important feature of anxiety disorders. In this study, we used a novel anticipation of shock paradigm to investigate individual differences in functional connectivity during prolonged threat of shock. We examined the correlates of between-participant differences in trait anxious affect and induced anxiety, where the latter reflects changes in self-reported anxiety resulting from the shock manipulation. Dissociable effects of trait anxious affect and induced anxiety were observed. Participants with high scores on a latent dimension of anxious affect showed less increase in ventromedial pFC-amygdala connectivity between periods of safety and shock anticipation. Meanwhile, lower levels of induced anxiety were linked to greater augmentation of dorsolateral pFC-anterior insula connectivity during shock anticipation. These findings suggest that ventromedial pFC-amygdala and dorsolateral pFC-insula networks might both contribute to regulation of sustained fear responses, with their recruitment varying independently across participants. The former might reflect an evolutionarily old mechanism for reducing fear or anxiety, whereas the latter might reflect a complementary mechanism by which cognitive control can be implemented to diminish fear responses generated due to anticipation of aversive stimuli or events. These two circuits might provide complementary, alternate, targets for exploration in future pharmacological and cognitive intervention studies.\"",
        "Document: \"Changes in white matter microstructure during adolescence. Postmortem histological studies have demonstrated that myelination in human brain white matter (WM) continues throughout adolescence and well into adulthood. We used in vivo diffusion-weighted magnetic resonance imaging to test for age-related WM changes in 42 adolescents and 20 young adults. Tract-Based Spatial Statistics (TBSS) analysis of the adolescent data identified widespread age-related increases in fractional anisotropy (FA) that were most significant in clusters including the body of the corpus callosum and right superior corona radiata. These changes were driven by changes in perpendicular, rather than parallel, diffusivity. These WM clusters were used as seeds for probabilistic tractography, allowing us to identify the regions as belonging to callosal, corticospinal, and prefrontal tracts. We also performed voxel-based morphometry-style analysis of conventional T1-weighted images to test for age-related changes in grey matter (GM). We identified a cluster including right middle frontal and precentral gyri that showed an age-related decrease in GM density through adolescence and connected with the tracts showing age-related WM FA increases. The GM density decrease was highly significantly correlated with the WM FA increase in the connected cluster. Age-related changes in FA were much less prominent in the young adult group, but we did find a significant age-related increase in FA in the right superior longitudinal fascicle, suggesting that structural development of this pathway continues into adulthood. Our results suggest that significant microstructural changes in WM continue throughout adolescence and are associated with corresponding age-related changes in cortical GM regions.\"",
        "Document: \"Regional brain atrophy development is related to specific aspects of clinical dysfunction in multiple sclerosis. Brain atrophy in multiple sclerosis (MS) is thought to reflect irreversible tissue damage leading to persistent clinical deficit. Little is known about the rate of atrophy in specific brain regions in relation to specific clinical deficits.\"",
        "Document: \"Mixture models with adaptive spatial regularization for segmentation with an application to FMRI data. Mixture models are often used in the statistical seg- mentation of medical images. For example, they can be used for the segmentation of structural images into different matter types or of functional statistical parametric maps (SPMs) into activations and nonactivations. Nonspatial mixture models segment using models of just the histogram of intensity values. Spatial mixture models have also been developed which augment this histogram information with spatial regularization using Markov random fields. However, these techniques have control parameters, such as the strength of spatial regularization, which need to be tuned heuristically to particular datasets. We present a novel spatial mixture model within a fully Bayesian framework with the ability to perform fully adaptive spatial regularization using Markov random fields. This means that the amount of spatial regular- ization does not have to be tuned heuristically but is adaptively determined from the data. We examine the behavior of this model when applied to artificial data with different spatial characteris- tics, and to functional magnetic resonance imaging SPMs.\"",
        "Document: \"Heterogeneous fractionation profiles of meta-analytic coactivation networks. Computational cognitive neuroimaging approaches can be leveraged to characterize the hierarchical organization of distributed, functionally specialized networks in the human brain. To this end, we performed large-scale mining across the BrainMap database of coordinate-based activation locations from over 10,000 task-based experiments. Meta-analytic coactivation networks were identified by jointly applying independent component analysis (ICA) and meta-analytic connectivity modeling (MACM) across a wide range of model orders (i.e., d=20\u2013300). We then iteratively computed pairwise correlation coefficients for consecutive model orders to compare spatial network topologies, ultimately yielding fractionation profiles delineating how \u201cparent\u201d functional brain systems decompose into constituent \u201cchild\u201d sub-networks. Fractionation profiles differed dramatically across canonical networks: some exhibited complex and extensive fractionation into a large number of sub-networks across the full range of model orders, whereas others exhibited little to no decomposition as model order increased. Hierarchical clustering was applied to evaluate this heterogeneity, yielding three distinct groups of network fractionation profiles: high, moderate, and low fractionation. BrainMap-based functional decoding of resultant coactivation networks revealed a multi-domain association regardless of fractionation complexity. Rather than emphasize a cognitive-motor-perceptual gradient, these outcomes suggest the importance of inter-lobar connectivity in functional brain organization. We conclude that high fractionation networks are complex and comprised of many constituent sub-networks reflecting long-range, inter-lobar connectivity, particularly in fronto-parietal regions. In contrast, low fractionation networks may reflect persistent and stable networks that are more internally coherent and exhibit reduced inter-lobar communication.\"",
        "Document: \"Tradeoffs in pushing the spatial resolution of fMRI for the 7T Human Connectome Project. Whole-brain functional magnetic resonance imaging (fMRI), in conjunction with multiband acceleration, has played an important role in mapping the functional connectivity throughout the entire brain with both high temporal and spatial resolution. Ultrahigh magnetic field strengths (7T and above) allow functional imaging with even higher functional contrast-to-noise ratios for improved spatial resolution and specificity compared to traditional field strengths (1.5T and 3T). High-resolution 7T fMRI, however, has primarily been constrained to smaller brain regions given the amount of time it takes to acquire the number of slices necessary for high resolution whole brain imaging. Here we evaluate a range of whole-brain high-resolution resting state fMRI protocols (0.9, 1.25, 1.5, 1.6 and 2mm isotropic voxels) at 7T, obtained with both in-plane and slice acceleration parallel imaging techniques to maintain the temporal resolution and brain coverage typically acquired at 3T. Using the processing pipeline developed by the Human Connectome Project, we demonstrate that high resolution images acquired at 7T provide increased functional contrast to noise ratios with significantly less partial volume effects and more distinct spatial features, potentially allowing for robust individual subject parcellations and descriptions of fine-scaled patterns, such as visuotopic organization.\"",
        "Document: \"Benefits of multi-modal fusion analysis on a large-scale dataset: Life-span patterns of inter-subject variability in cortical morphometry and white matter microstructure. Neuroimaging studies have become increasingly multimodal in recent years, with researchers typically acquiring several different types of MRI data and processing them along separate pipelines that provide a set of complementary windows into each subject's brain. However, few attempts have been made to integrate the various modalities in the same analysis. Linked ICA is a robust data fusion model that takes multi-modal data and characterizes inter-subject variability in terms of a set of multi-modal components. This paper examines the types of components found when running Linked ICA on a large magnetic resonance imaging (MRI) morphometric and diffusion tensor imaging (DTI) data set comprising 484 healthy subjects ranging from 8 to 85years of age. We find several strong global features related to age, sex, and intracranial volume; in particular, one component predicts age to a high accuracy (r=0.95). Most of the remaining components describe spatially localized modes of variability in white or gray matter, with many components including both tissue types. The multimodal components tend to be located in anatomically-related brain areas, suggesting a morphological and possibly functional relationship. The local components show relationships between surface-based cortical thickness and arealization, voxel-based morphometry (VBM), and between three different DTI measures. Further, we report components related to artifacts (e.g. scanner software upgrades) which would be expected in a dataset of this size. Most of the 100 extracted components showed interpretable spatial patterns and were found to be reliable using split-half validation. This work provides novel information about normal inter-subject variability in brain structure, and demonstrates the potential of Linked ICA as a feature-extracting data fusion approach across modalities. This exploratory approach automatically generates models to explain structure in the data, and may prove especially powerful for large-scale studies, where the population variability can be explored in increased detail.\"",
        "Document: \"Meta-analysis of neuroimaging data: A comparison of image-based and coordinate-based pooling of studies. With the rapid growth of neuroimaging research and accumulation of neuroinformatic databases the synthesis of consensus findings using meta-analysis is becoming increasingly important. Meta-analyses pool data across many studies to identify reliable experimental effects and characterize the degree of agreement across studies. Coordinate-based meta-analysis (CBMA) methods are the standard approach, where each study entered into the meta-analysis has been summarized using only the (x, y, z) locations of peak activations (with or without activation magnitude) reported in published reports. Image-based meta-analysis (IBMA) methods use the full statistic images, and allow the use of hierarchical mixed effects models that account for differing intra-study variance and modeling of random inter-study variation. The purpose of this work is to compare image-based and coordinate-based meta-analysis methods applied to the same dataset, a group of 15 fMRI studies of pain, and to quantify the information lost by working only with the coordinates of peak activations instead of the full statistic images. We apply a 3-level IBMA mixed model for a \"mega-analysis\", and highlight important considerations in the specification of each model and contrast. We compare the IBMA result to three CBMA methods: ALE (activation likelihood estimation), KDA (kernel density analysis) and MKDA (multi-level kernel density analysis), for various CBMA smoothing parameters. For the datasets considered, we find that ALE at \u03c3=15\u00a0mm, KDA at \u03c1=25\u201330\u00a0mm and MKDA at \u03c1=15\u00a0mm give the greatest similarity to the IBMA result, and that ALE was the most similar for this particular dataset, though only with a Dice similarity coefficient of 0.45 (Dice measure ranges from 0 to 1). Based on this poor similarity, and the greater modeling flexibility afforded by hierarchical mixed models, we suggest that IBMA is preferred over CBMA. To make IBMA analyses practical, however, the neuroimaging field needs to develop an effective mechanism for sharing image data, including whole-brain images of both effect estimates and their standard errors.\"",
        "1 is \"Tensor-Based Morphometry with Mappings Parameterized by Stationary Velocity Fields in Alzheimer's Disease Neuroimaging Initiative\", 2 is \"Depth judgment measures and occluding surfaces in near-field augmented reality\"",
        "Given above information, for an author who has written the paper with the title \"Improving alignment in Tract-based spatial statistics: Evaluation and optimization of image registration.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005921": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Randomized Nonlinear Component Analysis.':",
        "Document: \"Entropic metric alignment for correspondence problems. Many shape and image processing tools rely on computation of correspondences between geometric domains. Efficient methods that stably extract \\\"soft\\\" matches in the presence of diverse geometric structures have proven to be valuable for shape retrieval and transfer of labels or semantic information. With these applications in mind, we present an algorithm for probabilistic correspondence that optimizes an entropy-regularized Gromov-Wasserstein (GW) objective. Built upon recent developments in numerical optimal transportation, our algorithm is compact, provably convergent, and applicable to any geometric domain expressible as a metric measure matrix. We provide comprehensive experiments illustrating the convergence and applicability of our algorithm to a variety of graphics tasks. Furthermore, we expand entropic GW correspondence to a framework for other matching problems, incorporating partial distance matrices, user guidance, shape exploration, symmetry detection, and joint analysis of more than two domains. These applications expand the scope of entropic GW correspondence to major shape analysis problems and are stable to distortion and noise.\"",
        "Document: \"Fast Projection-Based Methods for the Least Squares Nonnegative Matrix Approximation Problem. Nonnegative matrix approximation (NNMA) is a popular matrix decomposition technique that has proven to be useful across a diverse variety of fields with applications ranging from document analysis and image processing to bioinformatics and signal processing. Over the years, several algorithms for NNMA have been proposed, e.g. Lee and Seung's multiplicative updates, alternating least squares (ALS), and gradient descent-based procedures. However, most of these procedures suffer from either slow convergence, numerical instability, or at worst, serious theoretical drawbacks. In this paper, we develop a new and improved algorithmic framework for the least-squares NNMA problem, which is not only theoretically well-founded, but also overcomes many deficiencies of other methods. Our framework readily admits powerful optimization techniques and as concrete realizations we present implementations based on the Newton, BFGS and conjugate gradient methods. Our algorithms provide numerical results superior to both Lee and Seung's method as well as to the alternating least squares heuristic, which was reported to work well in some situations but has no theoretical guarantees [1]. Our approach extends naturally to include regularization and box-constraints without sacrificing convergence guarantees. We present experimental results on both synthetic and real-world datasets that demonstrate the superiority of our methods, both in terms of better approximations as well as computational efficiency. Copyright \u00a9 2007 Wiley Periodicals, Inc., A Wiley Company Statistical Analy Data Mining 1: 000-000, 2007\"",
        "Document: \"A new metric on the manifold of kernel matrices with application to matrix geometric means. Use the \"Report an Issue\" link to request a name change.\"",
        "Document: \"Reflection methods for user-friendly submodular optimization.   Recently, it has become evident that submodularity naturally captures widely occurring concepts in machine learning, signal processing and computer vision. Consequently, there is need for efficient optimization procedures for submodular functions, especially for minimization problems. While general submodular minimization is challenging, we propose a new method that exploits existing decomposability of submodular functions. In contrast to previous approaches, our method is neither approximate, nor impractical, nor does it need any cumbersome parameter tuning. Moreover, it is easy to implement and parallelize. A key component of our method is a formulation of the discrete submodular minimization problem as a continuous best approximation problem that is solved through a sequence of reflections, and its solution can be easily thresholded to obtain an optimal discrete solution. This method solves both the continuous and discrete formulations of the problem, and therefore has applications in learning, inference, and reconstruction. In our experiments, we illustrate the benefits of our method on two image segmentation tasks. \"",
        "Document: \"Efficient Structured Matrix Rank Minimization. We study the problem of finding structured low-rank matrices using nuclear norm regularization where the structure is encoded by a linear map. In contrast to most known approaches for linearly structured rank minimization, we do not (a) use the full SVD; nor (b) resort to augmented Lagrangian techniques; nor (c) solve linear systems per iteration. Instead, we formulate the problem differently so that it is amenable to a generalized conditional gradient method, which results in a practical improvement with low per iteration computational cost. Numerical results show that our approach significantly outperforms state-of-the-art competitors in terms of running time, while effectively recovering low rank solutions in stochastic system realization and spectral compressed sensing problems.\"",
        "Document: \"Denoising sparse noise via online dictionary learning. The idea of learning overcomplete dictionaries based on the paradigm of compressive sensing has found numerous applications, among which image denoising is considered one of the most successful. But many state-of-the-art denoising techniques inherently assume that the signal noise is Gaussian. We instead propose to learn overcomplete dictionaries where the signal is allowed to have both Gaussian and (sparse) Laplacian noise. Dictionary learning in this setting leads to a difficult non-convex optimization problem, which is further exacerbated by large input datasets. We tackle these difficulties by developing an efficient online algorithm that scales to data size. To assess the efficacy of our model, we apply it to dictionary learning for data that naturally satisfy our noise model, namely, Scale Invariant Feature Transform (SIFT) descriptors. For these data, we measure performance of the learned dictionary on the task of nearest-neighbor retrieval: compared to methods that do not explicitly model sparse noise our method exhibits superior performance.\"",
        "Document: \"Generative model-based clustering of directional data. High dimensional directional data is becoming increasingly important in contemporary applications such as analysis of text and gene-expression data. A natural model for multi-variate directional data is provided by the von Mises-Fisher (vMF) distribution on the unit hypersphere that is analogous to the multi-variate Gaussian distribution in Rd. In this paper, we propose modeling complex directional data as a mixture of vMF distributions. We derive and analyze two variants of the Expectation Maximization (EM) framework for estimating the parameters of this mixture. We also propose two clustering algorithms corresponding to these variants. An interesting aspect of our methodology is that the spherical kmeans algorithm (kmeans with cosine similarity) can be shown to be a special case of both our algorithms. Thus, modeling text data by vMF distributions lends theoretical validity to the use of cosine similarity which has been widely used by the information retrieval community. As part of experimental validation, we present results on modeling high-dimensional text and gene-expression data as a mixture of vMF distributions. The results indicate that our approach yields superior clusterings especially for difficult clustering tasks in high-dimensional spaces.\"",
        "Document: \"Row-Action Methods for Compressed Sensing. Compressed sensing uses a small number of random, linear measurements to acquire a sparse signal. Nonlinear algorithms, such as 11 minimization, are used to reconstruct the signal from the measured data. This paper proposes row-action methods as a computational approach to solving the 11 optimization problem. This paper presents a specific row-action method and provides extensive empirical evidence that it is an effective technique for signal reconstruction. This approach offers several advantages over interior-point methods, including minimal storage and computational requirements, scalability, and robustness\"",
        "Document: \"Geometric Mean Metric Learning. We revisit the task of learning a Euclidean metric from data. We approach this problem from first principles and formulate it as a surprisingly simple optimization problem. Indeed, our formulation even admits a closed form solution. This solution possesses several very attractive properties: (i) an innate geometric appeal through the Riemannian geometry of positive definite matrices; (ii) ease of interpretability; and (iii) computational speed several orders of magnitude faster than the widely used LMNN and ITML methods. Furthermore, on standard benchmark datasets, our closed-form solution consistently attains higher classification accuracy.\"",
        "Document: \"A short note on parameter approximation for von Mises-Fisher distributions: and a fast implementation of I s (x). In high-dimensional directional statistics one of the most basic probability distributions is the von Mises-Fisher (vMF) distribution. Maximum likelihood estimation for the vMF distribution turns out to be surprisingly hard because of a\n difficult transcendental equation that needs to be solved for computing the concentration parameter \n \u03ba. This paper is a followup to the recent paper of Tanabe et\u00a0al. (Comput Stat 22(1):145\u2013157, 2007), who exploited inequalities about Bessel function ratios to obtain an interval in which the parameter estimate for \u03ba should lie; their observation lends theoretical validity to the heuristic approximation of Banerjee et\u00a0al. (JMLR 6:1345\u20131382,\n 2005). Tanabe et\u00a0al. (Comput Stat 22(1):145\u2013157, 2007) also presented a fixed-point algorithm for computing improved approximations for \u03ba. However, their approximations require (potentially significant) additional computation, and in this short paper we show\n that given the same amount of computation as their method, one can achieve more accurate approximations using a truncated Newton method. A more\n interesting contribution of this paper is a simple algorithm for computing I\n \n s\n (x): the modified Bessel function of the first kind. Surprisingly, our na\u00efve implementation turns out to be several orders of\n magnitude faster for large arguments common to high-dimensional data, than the standard implementations in well-established\n software such as Mathematica\n \u00a9, Maple\n \u00a9, and Gp/Pari.\"",
        "1 is \"On the Minimizing Property of a Second Order Dissipative System in Hilbert Spaces\", 2 is \"An information-maximization approach to blind separation and blind deconvolution\"",
        "Given above information, for an author who has written the paper with the title \"Randomized Nonlinear Component Analysis.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "005983": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Black penguin: On the feasibility of detecting intrusion with homogeneous memory.':",
        "Document: \"A Formal Framework for Program Anomaly Detection. Program anomaly detection analyzes normal program behaviors and discovers aberrant executions caused by attacks, misconfigurations, program bugs, and unusual usage patterns. The merit of program anomaly detection is its independence from attack signatures, which enables proactive defense against new and unknown attacks. In this paper, we formalize the general program anomaly detection problem and point out two of its key properties. We present a unified framework to present any program anomaly detection method in terms of its detection capability. We prove the theoretical accuracy limit for program anomaly detection with an abstract detection machine. We show how existing solutions are positioned in our framework and illustrate the gap between state-of-the-art methods and the theoretical accuracy limit. We also point out some potential modeling features for future program anomaly detection evolution.\"",
        "Document: \"Probabilistic Program Modeling for High-Precision Anomaly Classification. The trend constantly being observed in the evolution of advanced modern exploits is their growing sophistication in stealthy attacks. Code-reuse attacks such as return-oriented programming allow intruders to execute mal-intended instruction sequences on a victim machine without injecting external code. We introduce a new anomaly-based detection technique that probabilistically models and learns a program's control flows for high-precision behavioral reasoning and monitoring. Our prototype in Linux is named STILO, which stands for STatically InitiaLized markOv. Experimental evaluation involves real-world code-reuse exploits and over 4,000 testcases from server and utility programs. STILO achieves up to 28-fold of improvement in detection accuracy over the state-of-the-art HMM-based anomaly detection. Our findings suggest that the probabilistic modeling of program dependences provides a significant source of behavior information for building high-precision models for real-time system monitoring.\"",
        "Document: \"Detection of Repackaged Android Malware with Code-Heterogeneity Features. During repackaging, malware writers statically inject malcode and modify the control flow to ensure its execution. Repackaged malware is difficult to detect by existing classification techniques, partly because of their behavioral similarities to benign apps. By exploring the app's internal different behaviors, we propose a new Android repackaged malware detection technique based on \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">code heterogeneity analysis</italic>\n. Our solution strategically partitions the code structure of an app into multiple dependence-based regions (subsets of the code). Each region is independently classified on its behavioral features. We point out the security challenges and design choices for partitioning code structures at the class and method level graphs, and present a solution based on multiple dependence relations. We have performed experimental evaluation with over 7,542 Android apps. For repackaged malware, our partition-based detection reduces false negatives (i.e., missed detection) by 30-fold, when compared to the non-partition-based approach. Overall, our approach achieves a false negative rate of 0.35 percent and a false positive rate of 2.97 percent.\"",
        "Document: \"Fast Detection of Transformed Data Leaks. The leak of sensitive data on computer systems poses a serious threat to organizational security. Statistics show that the lack of proper encryption on files and communications due to human errors is one of the leading causes of data loss. Organizations need tools to identify the exposure of sensitive data by screening the content in storage and transmission, i.e., to detect sensitive information being stored or transmitted in the clear. However, detecting the exposure of sensitive information is challenging due to data transformation in the content. Transformations (such as insertion and deletion) result in highly unpredictable leak patterns. In this paper, we utilize sequence alignment techniques for detecting complex data-leak patterns. Our algorithm is designed for detecting long and inexact sensitive data patterns. This detection is paired with a comparable sampling algorithm, which allows one to compare the similarity of two separately sampled sequences. Our system achieves good detection accuracy in recognizing transformed leaks. We implement a parallelized version of our algorithms in graphics processing unit that achieves high analysis throughput. We demonstrate the high multithreading scalability of our data leak detection method required by a sizable organization.\"",
        "Document: \"Towards automatic privacy management in Web 2.0 with semantic analysis on annotations. Sharing personal information and documents is pervasive in Web 2.0 environments, which creates the need for properly controlling shared data. Most existing authorization and policy management systems are for organizational use by IT professionals. Average Web users, however, do not have the sophistication to specify and maintain privacy policies for their shared content. In this paper, we aim to utilize personal and social annotations to develop automatic tools for managing content sharing, and demonstrate a new application of social annotations in access control. We use annotation data to predict privacy preferences of users and automatically derive policies for shared content. We carry out a series of user studies to evaluate the accuracy of our predicted techniques. We also perform extensive analysis on static and dynamic approaches of analyzing semantic similarities of tags, which is of independent interest. Our analysis gives encouraging results on the feasibility of using annotations for privacy management in Web 2.0.\"",
        "Document: \"User-Assisted host-based detection of outbound malware traffic. Conventional network security solutions are performed on network-layer packets using statistical measures. These types of traffic analysis may not catch stealthy attacks carried out by today's malware. We aim to develop a host-based security tool that identifies suspicious outbound network connections through analyzing the user's surfing activities. Specifically, our solution for Web applications predicts user's network connections by analyzing Web content; unpredicted traffic is further investigated with the user's help. We describe our method and implementation as well as the experimental results in evaluating its efficiency and effectiveness. We describe how our studies can be applied to detecting bot infection. In order to assess the workload of our host-based traffic-analysis tool, we also perform a large-scale characterization study on 500 university-users' wireless network traces for 4-month period. We study both the statistical and temporal patterns of individuals' web usage behaviors from collected wireless network traces. Users are classified into different profiles based on their web usage patterns. Our results show that users have regularities in their Web activities and the expected workload of our traffic-analysis solution is low.\"",
        "Document: \"POSTER: Detection of CPS Program Anomalies by Enforcing Cyber-Physical Execution Semantics. In this work, we present a new program behavior model, i.e., the event-aware finite-state automaton ( eFSA ), which takes advantage of the event-driven nature of control programs in cyber-physical systems (CPS) and incorporates event checking in anomaly detection. eFSA provides new detection capabilities to detect data-oriented attacks in CPS control programs, including attacks on control intensity (i.e., hijacked for/while-loops) and attacks on control branch (i.e., conditional branches). We implement a prototype of our approach on Raspberry Pi and evaluate eFSA 's performance by conducting CPS case studies. Results show that it is able to effectively detect different CPS attacks in our experiments.\n\n\"",
        "Document: \"Efficient signature schemes supporting redaction, pseudonymization, and data deidentification. In this paper we give a new signature algorithm that allows for controlled changes to the signed data. The change operations we study are removal of subdocuments (redaction), pseudonymization, and gradual deidentification of hierarchically structured data. These operations are applicable in a number of practically relevant application scenarios, including the release of previously classified government documents, privacy-aware management of audit-log data, and the release of tables of health records. When applied directly to redaction, our algorithm improves on [18] by reducing significantly the overhead of cryptographic information that has to be stored with the original data.\"",
        "Document: \"Program Anomaly Detection: Methodology and Practices. This tutorial will present an overview of program anomaly detection, which analyzes normal program behaviors and discovers aberrant executions caused by attacks, misconfigurations, program bugs, and unusual usage patterns. It was first introduced as an analogy between intrusion detection for programs and the immune mechanism in biology. Advanced models have been developed in the last decade and comprehensive techniques have been adopted such as hidden Markov model and machine learning. We will introduce the audience to the problem of program attacks and the anomaly detection approach against threats. We will give a general definition for program anomaly detection and derive model abstractions from the definition. The audience will be walked through the development of program anomaly detection methods from early-age n-gram approaches to complicated pushdown automata and probabilistic models. Some lab tools will be provided to help understand primitive detection models. This procedure will help the audience understand the objectives and challenges in designing program anomaly detection models. We will discuss the attacks that subvert anomaly detection mechanisms. The field map of program anomaly detection will be presented. We will also briefly discuss the applications of program anomaly detection in Internet of Things security. We expect the audience to get an idea of unsolved challenges in the field and develop a sense of future program anomaly detection directions after attending the tutorial.\"",
        "Document: \"Unearthing Stealthy Program Attacks Buried in Extremely Long Execution Paths. Modern stealthy exploits can achieve attack goals without introducing illegal control flows, e.g., tampering with non-control data and waiting for the modified data to propagate and alter the control flow legally. Existing program anomaly detection systems focusing on legal control flow attestation and short call sequence verification are inadequate to detect such stealthy attacks. In this paper, we point out the need to analyze program execution paths and discover event correlations in large-scale execution windows among millions of instructions. We propose an anomaly detection approach with two-stage machine learning algorithms to recognize diverse normal call-correlation patterns and detect program attacks at both inter- and intra-cluster levels. We implement a prototype of our approach and demonstrate its effectiveness against three real-world attacks and four synthetic anomalies with less than 0.01% false positive rates and 0.1~1.3 ms analysis overhead per behavior instance (1k to 50k function or system calls).\"",
        "1 is \"Scalable analysis of attack scenarios\", 2 is \"Security Flaws and Improvements to a Direct Anonymous Attestation Scheme for Mobile Computing Platforms\"",
        "Given above information, for an author who has written the paper with the title \"Black penguin: On the feasibility of detecting intrusion with homogeneous memory.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006004": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Can developer-module networks predict failures?':",
        "Document: \"Usage and Perceptions of Agile Software Development in an Industrial Context: An Exploratory Study. Agile development methodologies have been gaining acceptance in the mainstream software development community. While there are numerous studies of Agile development in academic and educational settings, there has been little detailed reporting of the usage, penetration and success of Agile methodologies in traditional, professional software development organizations. We report on the results of an empirical study conducted at Microsoft to learn about Agile development and its perception by people in development, testing, and management. We found that one-third of the study respondents use Agile methodologies to varying degrees, and most view it favorably due to improved communication between team members, quick releases and the increased flexibility of Agile designs. The Scrum variant of Agile methodologies is by far the most popular at Microsoft. Our findings also indicate that developers are most worried about scaling Agile to larger projects (greater than twenty members), attending too many meetings and the coordinating Agile and non-Agile teams.\"",
        "Document: \"Mining energy traces to aid in software development: an empirical case study. Context: With the advent of increased computing on mobile devices such as phones and tablets, it has become crucial to pay attention to the energy consumption of mobile applications. Goal: The software engineering field is now faced with a whole new spectrum of energy-related challenges, ranging from power budgeting to testing and debugging the energy consumption, for which exists only limited tool support. The goal of this work is to provide techniques to engineers to analyze power consumption and detect anomalies. Method: In this paper, we present our work on analyzing energy patterns for the Windows Phone platform. We first describe the data that is collected for testing (power traces and execution logs). We then present several approaches for describing power consumption and detecting anomalous energy patterns and potential energy defects. Finally we show prediction models based on usage of individual modules that can estimate the overall energy consumption with high accuracy. Results: The techniques in this paper were successful in modeling and estimating power consumption and in detecting anomalies. Conclusions: The techniques presented in the paper allow assessing the individual impact of modules on the overall energy consumption and support overall energy planning.\"",
        "Document: \"Characterizing and predicting which bugs get fixed: an empirical study of Microsoft Windows. We performed an empirical study to characterize factors that affect which bugs get fixed in Windows Vista and Windows 7, focusing on factors related to bug report edits and relationships between people involved in handling the bug. We found that bugs reported by people with better reputations were more likely to get fixed, as were bugs handled by people on the same team and working in geographical proximity. We reinforce these quantitative results with survey feedback from 358 Microsoft employees who were involved in Windows bugs. Survey respondents also mentioned additional qualitative influences on bug fixing, such as the importance of seniority and interpersonal skills of the bug reporter. Informed by these findings, we built a statistical model to predict the probability that a new bug will be fixed (the first known one, to the best of our knowledge). We trained it on Windows Vista bugs and got a precision of 68% and recall of 64% when predicting Windows 7 bug fixes. Engineers could use such a model to prioritize bugs during triage, to estimate developer workloads, and to decide which bugs should be closed or migrated to future product versions.\"",
        "Document: \"Providing test quality feedback using static source code and automatic test suite metrics. A classic question in software development is \"How much testing is enough?\" Aside from dynamic coverage-based metrics, there are few measures that can be used to provide guidance on the quality of an automatic test suite as development proceeds. This paper utilizes the software testing and reliability early warning (STREW) static metric suite to provide a developer with indications of changes and additions to their automated unit test suite and code for added confidence that product quality will be high. Retrospective case studies to assess the utility of using the STREW metrics as a feedback mechanism were performed in academic, open source and industrial environments. The results indicate at statistically significant levels the ability of the STREW metrics to provide feedback on important attributes of an automatic test suite and corresponding code\"",
        "Document: \"Toward a Software Testing and Reliability Early Warning Metric Suite. The field reliability is measured too late for affordablyguiding corrective action to improve the quality of thesoftware. Software developers can benefit from an earlywarning of their reliability while they can still affordablyreact. This early warning can be built from a collection ofinternal metrics. An internal metric, such as the numberof lines of code, is a measure derived from the productitself [15]. An external measure is a measure of a productderived from assessment of the behavior of the system[15]. For example, the number of defects found in test isan external measure. The ISO/IEC standard [15] statesthat \"[i]nternal metrics are of little value unless there isevidence that they are related to external quality.\"Internal metrics can be collected in-process and moreeasily than external metrics. Additionally, internalmetrics have been shown to be useful as early indicatorsof externally-visible product quality [1]. For these earlyindicators to be meaningful, they must be related (in astatistically significant and stable way) to the fieldquality/reliability of the product. The validation of suchmetrics requires the convincing demonstration that (1) themetric measures what it purports to measure and (2) themetric is associated with an important external metric,such as field reliability, maintainability, or fault-proneness[12].Software metrics have been used as indicators ofsoftware quality [1, 19-21, 23] and fault proneness [8-10,24]. There is a growing body of empirical results thatsupports the theoretical validity of the use of higher-orderearly metrics, such as OO metrics [1] defined byChidamber-Kemerer (CK) [6] and the MOOD [5] OOmetric suites as predictors of field quality. However,general validity of these metrics (which are oftenunrelated to the actual operational profile of the product)is still open to criticism [7].\"",
        "Document: \"Building Pair Programming Knowledge through a Family of Experiments. Pair programming is a practice in which two programmers work collaboratively at one computer on the same design, algorithm, code, or test. Pair programming is becoming increasingly popular in industry and in university curricula. A family of experiments was run with over 1200 students at two US universities, North Carolina State University and the University of California Santa Cruz, to assess the efficacy of pair programming as an alternative learning technique in introductory programming courses. Students who used the pair programming technique were at least as likely to complete the introductory course with a grade of C or better when compared with students who used the solo programming technique. Paired students earned exam and project scores equal to or better than solo students. Paired students had a positive attitude toward collaboration and were significantly more likely to be registered as computer science-related majors one year later. Our findings also suggest that students in paired classes continue to be successful in subsequent programming classes continue to be successful in subsequent programming classes that require solo programming.\"",
        "Document: \"Using In-Process Testing Metrics to Estimate Post-Release Field Quality. In industrial practice, information on the software field quality of a product is available too late in the software lifecycle to guide affordable corrective action. An important step towards remediation of this problem lies in the ability to provide an early estimation of post-release field quality. This paper evaluates the Software Testing and Reliability Early Warning for Java (STREW-J) metric suite leveraging the software testing effort to predict post-release field quality early in the software development phases. The metric suite is applicable for software products implemented in Java for which an extensive suite of automated unit test cases are incrementally created as development proceeds. We validated the prediction model using the STREW-J metrics via a two-phase case study approach which involved 27 medium-sized open source projects, and five industrial projects. The error in estimation and the sensitivity of the predictions indicate the STREW-J metric suite can be used effectively to predict post-release software field quality.\"",
        "Document: \"The effect of the number of inspectors on the defect estimates produced by capture-recapture models. Inspections can be made more cost-effective by using capture-recapture methods to estimate post-inspection defects. Previous capture-recapture studies of inspections used relatively small data sets compared with those used in biology and wildlife research (the origin of the models). A common belief is that capture-recapture models underestimate the number of defects but their performance can be improved with data from more inspectors. This increase has not been evaluated in detail. This paper evaluates new estimators from biology not been previously applied to inspections. Using a data from seventy-three inspectors, we analyze the effect of the number of inspectors on the quality of estimates. Contrary to previous findings indicating that Jackknife is the best estimator, our results show that the SC estimators are better suited to software inspections. Our results also provide a detailed analysis of the number of inspectors necessary to obtain estimates within 5% to 20% of the actual.\"",
        "Document: \"Myths in software engineering: from the other side. An important component of Empirical Software Engineering (ESE) research involves the measurement, observation, analysis and understanding of software engineering in practice. Results analyzed without understanding the contexts in which they were obtained can lead to wrong and potentially harmful interpretation. There exist several myths in software engineering, most of which have been accepted for years as being conventional wisdom without having been questioned. In this talk we will deal briefly with a few popular myths in software engineering ranging from testing and static analysis to distributed development and highlight the importance of context and generalization.\"",
        "Document: \"Ramp-up Journey of New Hires: Do strategic practices of software companies influence productivity?. Software companies regularly recruit skilled and talented employees to meet evolving business requirements. Although companies expect early contributions, new hires often take several weeks to reach the same productivity level as existing employees. We refer to this transition of new hires from novices to experts as ramp-up journey. There can be various factors such as lack of technical skills or lack of familiarity with the process that influence the ramp-up journey of new hires. The goal of our work is to identify those factors and study their influence on the ramp-up journey. We expect the results from this study to help identify the need of various types of assistance to new hires to ramp-up faster. As a first step towards our goal, this paper explores the impact of two strategic practices, namely distributed development and internship on the ramp-up journey of new hires. Our results show that new hires in proximity to the core development team and new hires with prior internship experience perform better than others in the beginning. In the overall ramp-up journey, the effect of the two factors attenuates, yet nevertheless better compared to their counterparts. Product teams can use this information to pay special attention to non-interns and use better tools for distributed, cooperative work to help new hires ramp-up faster.\"",
        "1 is \"Quantifying developers' adoption of security tools\", 2 is \"On the Automatic Modularization of Software Systems Using the Bunch Tool\"",
        "Given above information, for an author who has written the paper with the title \"Can developer-module networks predict failures?\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006030": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Towards self-organising personal networks':",
        "Document: \"Constrained Pricing for Cloud Resource Allocation. Constrained pricing in a cloud computing environment is addressed using game theory. The objective of the model and the game is to enable cloud providers to maximize their revenue while satisfying users by maximizing their utilities. The users net utility is modeled as a function of resource demand with a corresponding price. The game consists in the cloud provider suggesting differentiated prices according to demand and users updating their requests in view of the proposed price. The objective is to determine the optimal suggested prices by the cloud-provider and the optimal user demands. A theoretical model based on Stackelberg game is proposed and a Stackelberg/Nash equilibrium solution is found. Performance results show that rejecting a certain number of users is not the best decision to select. Cloud provider should deploy the right amount of resources to maintain good reputation while satisfying users' requirements.\"",
        "Document: \"Stitch-N-Sync: Discreetly Disclosing Topology Information Using Logically Centralized Controllers. Competitive Mobile Network Operators (MNOs) are typically long-known for their shrewdness to conceal their underlying network topology information. Having said this, in this article, we propose a quasi-distributed topology information sharing framework for network operators based on logically centralized controllers. Through our approach, we present a topology information sharing scheme in which two or more MNOs can cooperatively and more importantly, discreetly reveal their topology information for the sake of utilizing the unused available resources of each other, at times of network failure situations. Our approach has been formulated and developed based on a novel key metric to 'tune' the amount of information sharing. Based on extensive simulations, we then investigate the impacts of network topology information sharing on the network capacity. The overall feasibility is illustrated through significant numerical results.\"",
        "Document: \"Trust management system design for the Internet of Things: A context-aware and multi-service approach. This work proposes a new trust management system (TMS) for the Internet of Things (IoT). The wide majority of these systems are today bound to the assessment of trustworthiness with respect to a single function. As such, they cannot use past experiences related to other functions. Even those that support multiple functions hide this heterogeneity by regrouping all past experiences into a single metric. These restrictions are detrimental to the adaptation of TMSs to today's emerging M2M and IoT architectures, which are characterized with heterogeneity in nodes, capabilities and services. To overcome these limitations, we design a context-aware and multi-service trust management system fitting the new requirements of the IoT. Simulation results show the good performance of the proposed system and especially highlight its ability to deter a class of common attacks designed to target trust management systems.\"",
        "Document: \"Lightweight collaborative key establishment scheme for the Internet of Things. This work addresses new security issues in the Internet of Things (IoT). The heterogeneous nature of IoT communications and imbalance in resource capabilities between IoT entities make it challenging to provide the required end-to-end secured connections. Clarifying how existing security protocols can be adapted to fulfill these new challenges still has to be improved. A direct use of existing key exchange schemes between two IoT entities may be unfeasible unless both entities be able to run the resource consuming cryptographic primitives required to bootstrap them - thus leaving aside a whole class of resource-constrained devices. In this paper, we revisit existing end-to-end security standards and key establishment schemes and discuss their limitations considering the specific scenarios of the IoT. Later, we propose novel collaborative approaches for key establishment designed to reduce the requirements of these existing security protocols. A constrained device may delegate its heavy cryptographic load to less constrained nodes in neighborhood exploiting the spatial heterogeneity of IoT environment. We demonstrate through a performance analysis that our collaborative key establishment solution allows for a reduction in energy consumption at the constrained device by up to 80% in comparison with existing key establishment schemes.\"",
        "Document: \"Distributed transmit beamforming with 1-bit feedback for LoS-MISO channels. In this paper we develop a distributed transmit beamforming method for a Multiple Input Single Output (MISO) line of sight (LoS) channel requiring only one bit in feedback. We consider the case where several non cooperating transmitters are using the same frequency band without exchanging any information between each others. Each transmitter receives only 1-bit feedback from its own receiver. We develop a Nash seeking based precoder that adapts the Direction of Transmission (DoT) in such a way to maximize the probability of successful transmission of each transmitter. We also present numerical results which corroborate our claim.\"",
        "Document: \"SFC Provisioning over NFV Enabled Clouds. This paper deals with virtualised Network Function Placement and chaining over cloud and NFV environments. We propose a polynomial-time algorithm for joint VNF chain placement that is based on a recursive optimization approach. The key idea is to benefit from the robustness of the recursion paradigm that was used to solve many optimization problems. We demonstrate the potential of our solution by comparing it to previous work. We conduct performance evaluation first through extensive simulations and then using a real cloud testbed. The obtained results show that our proposed algorithm achieves better performance in terms of resource usage, execution time, acceptance rate and revenue of the cloud provider.\"",
        "Document: \"Dynamic service deployment in a distributed heterogeneous cluster based router (DHCR). This paper presents the design, implementation and evaluation of an extensible, scalable and distributed heterogeneous cluster based programmable router, called DHCR (Distributed Heterogeneous Cluster based Router), capable of supporting and deploying network services at run time. DHCR is a software IP router relying on heterogeneous cluster composed of separated computers with different hardware and software architecture capabilities, running different operating systems and interconnected through a high speed network connection. The DHCR ensures dynamic deployment of services and distributed control of router components (forwarding and routing elements) over heterogeneous system environments. The DHCR combines the IETF ForCES (Forwarding and Control Element Separation) architecture with software component technologies to meet the requirements of the next generation software routers. To ensure reliable and transparent communication between separated, decentralized and heterogeneous router components, the CORBA based middleware technology is used to support the DHCR internal communication. The paper also explores the use of the CORBA Component Model (CCM) to design and implement a modular, distributed and heterogeneous forwarding path for the DHCR router architecture. The CCM based forwarding plane ensures dynamic reconfiguration of the data path topology needed for low-level service deployment. Results on achievable performance using the proposed DHCR router are reported.\"",
        "Document: \"Tensor-Based Link Prediction in Intermittently Connected Wireless Networks.   Through several studies, it has been highlighted that mobility patterns in mobile networks are driven by human behaviors. This effect has been particularly observed in intermittently connected networks like DTN (Delay Tolerant Networks). Given that common social intentions generate similar human behavior, it is relevant to exploit this knowledge in the network protocols design, e.g. to identify the closeness degree between two nodes. In this paper, we propose a temporal link prediction technique for DTN which quantifies the behavior similarity between each pair of nodes and makes use of it to predict future links. Our prediction method keeps track of the spatio-temporal aspects of nodes behaviors organized as a third-order tensor that aims to records the evolution of the network topology. After collapsing the tensor information, we compute the degree of similarity for each pair of nodes using the Katz measure. This metric gives us an indication on the link occurrence between two nodes relying on their closeness. We show the efficiency of this method by applying it on three mobility traces: two real traces and one synthetic trace. Through several simulations, we demonstrate the effectiveness of the technique regarding another approach based on a similarity metric used in DTN. The validity of this method is proven when the computation of score is made in a distributed way (i.e. with local information). We attest that the tensor-based technique is effective for temporal link prediction applied to the intermittently connected networks. Furthermore, we think that this technique can go beyond the realm of DTN and we believe this can be further applied on every case of figure in which there is a need to derive the underlying social structure of a network of mobile users. \"",
        "Document: \"Collaborative home media community with semantic support. The great success of social technologies such as media sharing, blogs and wikis, is transforming the Internet into a collaborative community. This paper presents our research towards the exploitation of P2P networks, semantic metadata and social tagging for home media sharing, with a vision of P2P-based Collaborative Home Media Community (CHMC). The goal of the proposed CHMC is to enable sharing, searching and tagging of multimedia contents based on semantic metadata within home networks as well as between homes connected by broadband networks. We will firstly present a hierarchical P2P network architecture where the super-peer model is applied. The super peers act as a gateway bridging the inner home network with the external Internet. We further propose the Resource Description Framework (RDF) triple-based metadata indexing, retrieving and tagging algorithms on structured P2P networks. Finally, a prototype is developed for the purposes of system validation and performance evaluation.\"",
        "Document: \"A link failure recovery algorithm for Virtual Network Function chaining. This paper addresses Virtual Network Functions (VNFs) placement and chaining in the presence of physical link failures. A decision tree approach to the NP-Hard VNF placement and chaining problem is used to minimize the penalties induced by service interruptions due to link outages. Formulating the problem as decision tree reduces the complexity significantly and leads to a new reliable algorithm, named R-SFC-MCTS, that builds incrementally the decision tree to efficiently search for good placement and chaining solutions. Execution time is improved thanks to the Monte-Carlo Tree Search strategy. The proposed link failure recovery algorithm selects and assigns reliable paths to prevent and avoid the negative effects of link failures and reactively re-maps impacted virtual links in safer physical paths once an outage occurs in the infrastructure. The performance of R-SFC-MCTS is compared via extensive simulations with a baseline and a reactive solution in terms of: i) acceptance rate, ii) induced penalties iii) provider revenue loss and iv) the final provider's profit.\"",
        "1 is \"Joint optimal power control and beamforming in wireless networks using antenna arrays\", 2 is \"ns-3 RPL module: IPv6 routing protocol for low power and lossy networks\"",
        "Given above information, for an author who has written the paper with the title \"Towards self-organising personal networks\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006071": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Modelling and developing conflict-aware scheduling on large-scale data centres.':",
        "Document: \"Protection of Mammograms Using Blind Steganography and Watermarking. As medical image databases are connected together through PACS, those medical images may suffer from security breaches if not protected. If a medical image is illegally obtained or if its content is malevolently changed, the patient's privacy or health care to be provided will certainly be undermined. To solve this problem, this study proposes a steganographic method for mammograms, which hides patients' information in mammograms without changing their important details. In particular, the proposed method can extract the hidden information from stego-mammograms without the aid of the original images. This study also utilizes a watermarking technique, which masks the contents of the mammogram, for providing mammograms protection against illegal access and malevolent modifications. This watermark can be removed to reveal the masked mammogram when authorization for viewing is given.\"",
        "Document: \"Audio forensic authentication based on MOCC between ENF and reference signals. This paper proposes a new audio authenticity detection algorithm based on the max offset for cross correlation (MOCC) between the extracted ENF (Electric Network Frequency) signal and the reference signal. We first extract the ENF signal from a query audio signal. And then we partition it into overlapping blocks for forgery detection. The MOCC between the extracted ENF and the reference signal is calculated block by block. We also introduce an enhancement scheme to improve the quality of the ENF signal before the calculation of the MOCC. Our proposed method can detect not only audio forgery but also the edited region and audio forgery type. The effectiveness of our method has been verified by experiments on digitally edited audio signals. \u00a9 2013 IEEE.\"",
        "Document: \"Color-Decoupled Photo Response Non-Uniformity for Digital Image Forensics. The last few years have seen the use of photo response non-uniformity noise (PRNU), a unique fingerprint of imaging sensors, in various digital forensic applications such as source device identification, content integrity verification, and authentication. However, the use of a color filter array for capturing only one of the three color components per pixel introduces color interpolation noise, while the existing methods for extracting PRNU provide no effective means for addressing this issue. Because the artificial colors obtained through the color interpolation process are not directly acquired from the scene by physical hardware, we expect that the PRNU extracted from the physical components, which are free from interpolation noise, should be more reliable than that from the artificial channels, which carry interpolation noise. Based on this assumption we propose a couple-decoupled PRNU (CD-PRNU) extraction method, which first decomposes each color channel into four sub-images and then extracts the PRNU noise from each sub-image. The PRNU noise patterns of the sub-images are then assembled to get the CD-PRNU. This new method can prevent the interpolation noise from propagating into the physical components, thus improving the accuracy of device identification and image content integrity verification.\"",
        "Document: \"Inferring Causal Relations from Multivariate Time Series: A Fast Method for Large-Scale Gene Expression Data. Various multivariate time series analysis techniques have been developed with the aim of inferring causal relations between time series. Previously, these techniques have proved their effectiveness on economic and neurophysiological data, which normally consist of hundreds of samples. However, in their applications to gene regulatory inference, the small sample size of gene expression time series poses an obstacle. In this paper, we describe some of the most commonly used multivariate inference techniques and show the potential challenge related to gene expression analysis. In response, we propose a directed partial correlation (DPC) algorithm as an efficient and effective solution to causal/regulatory relations inference on small sample gene expression data. Comparative evaluations on the existing techniques and the proposed method are presented. To draw reliable conclusions, a comprehensive benchmarking on data sets of various setups is essential. Three experiments are designed to assess these methods in a coherent manner. Detailed analysis of experimental results not only reveals good accuracy of the proposed DPC method in large-scale prediction, but also gives much insight into all methods under evaluation.\"",
        "Document: \"Removable visible image watermarking algorithm in the discrete cosine transform domain. A removable visible watermarking scheme, which operates in the discrete cosine transform (DCT) domain, is proposed for combating copyright piracy. First, the original watermark image is divided into 16 x 16 blocks and the preprocessed watermark to be embedded is generated by performing element-by-element matrix multiplication on the DCT coefficient matrix of each block and a key-based matrix. The intention of generating the preprocessed watermark is to guarantee the infeasibility of the illegal removal of the embedded watermark by the unauthorized users. Then, adaptive scaling and embedding factors are computed for each block of the host image and the preprocessed watermark according to the features of the corresponding blocks to better match the human visual system characteristics. Finally, the significant DCT coefficients of the preprocessed watermark are adaptively added to those of the host image to yield the watermarked image. The watermarking system is robust against compression to some extent. The performance of the proposed method is verified, and the test results show that the introduced scheme succeeds in preventing the embedded watermark from illegal removal. Moreover, experimental results demonstrate that legally recovered images can achieve superior visual effects, and peak signal-to-noise ratio values of these images are >50 dB. (C) 2008 SPIE and IS&T. [DOI: 10.1117/1.2952843]\"",
        "Document: \"Wavelet-based fragile watermarking scheme for image authentication. We propose a fragile watermarking scheme in the wavelet transform domain that is sensitive to all kinds of manipulations and has the ability to localize the tampered regions. To achieve high transparency (i.e., low embedding distortion) while providing protection to all coefficients, the embedder involves all the coefficients within a hierarchical neighborhood of each sparsely selected watermarkable coefficient during the watermark embedding process. The way the nonwatermarkable coefficients are involved in the embedding process is content-dependent and nondeterministic, which allows the proposed scheme to put up resistance to the so-called vector quantization attack, Holliman-Memon attack, collage attack, and transplantation attack. (c) 2007 SPIE and IS&T.\"",
        "Document: \"A Class of Discrete Multiresolution Random Fields and Its Application to Image Segmentation. In this paper, a class of Random Field model, defined on a multiresolution array is used in the segmentation of gray level and textured images. The novel feature of one form of the model is that it is able to segment images containing unknown numbers of regions, where there may be significant variation of properties within each region. The estimation algorithms used are stochastic, but because of the multiresolution representation, are fast computationally, requiring only a few iterations per pixel to converge to accurate results, with error rates of 1-2 percent across a range of image structures and textures. The addition of a simple boundary process gives accurate results even at low resolutions, and consequently at very low computational cost.\"",
        "Document: \"Fragile watermarking scheme based on the block-wise dependence in the wavelet domain. In transform-domain fragile watermarking schemes for authentication purposes, a common practice is to watermark some selected transform coefficients in order to achieve low embedding distortion. However, we point out in this work that leaving most of the coefficients, usually the low frequency and zero-valued ones, unmarked opens wide security gap for attacks to be mounted on them. In this work, a fragile watermarking scheme is proposed to implicitly watermark all the coefficients by registering the zero-valued coefficients with a key-generated binary sequence so as to create the watermark and involving the unwatermarkable coefficients within a hierarchical neigh-bourhood of the watermarkable coefficients in a random manner during the embedding process. The benefit of this approach is twofold: Firstly, only a small amount of coefficients are marked while all the coefficients are protected, so that high transparency is achieved. Secondly, when the relationship established in a random manner within a hierarchical neighbourhood is disturbed, the watermark detector would fail to extract the watermarking. This puts up resistance to the vector quantisation attack and transplantation attack.\"",
        "Document: \"Skin Colour-Based Face Detection in Colour Images. We propose in this work a method for detecting faces in colour images with complex backgrounds. The approach starts with the transformation of the image pixels from the RGB colour space to the chrominance space (YCbCr). Secondly, a Gaussian model is fitted on the transformed image in order to calculate the likelihood of skin for each pixel and to create a likelihood image. Thirdly, by thresholding the likelihood image, skin pixels are segmented to form a binary skin map, which contains the candidate face regions. Finally, a verification process is carried out to determine whether these candidate face regions are real faces or not.\"",
        "Document: \"Multiresolution genetic clustering algorithm for texture segmentation. This work plans to approach the texture segmentation problem by incorporating genetic algorithm and K-means clustering method within a multiresolution structure. As the algorithm descends the multiresolution structure, the coarse segmentation results are propagated down to the lower levels so as to reduce the inherent class\u2013position uncertainty and to improve the segmentation accuracy. The procedure is described as follows. In the first step, a quad-tree structure of multiple resolutions is constructed. Sampling windows of different sizes are utilized to partition the underlying image into blocks at different resolution levels and texture features are extracted from each block. Based on the texture features, a hybrid genetic algorithm is employed to perform the segmentation. While the select and mutate operators of the traditional genetic algorithm are adopted in this work, the crossover operator is replaced with K-means clustering method. In the final step, the boundaries and the segmentation result of the current resolution level are propagated down to the next level to act as contextual constraints and the initial configuration of the next level, respectively.\"",
        "1 is \"A Robust Content Based Digital Signature For Image Authentication\", 2 is \"Information measures in scale-spaces\"",
        "Given above information, for an author who has written the paper with the title \"Modelling and developing conflict-aware scheduling on large-scale data centres.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006159": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Anatomical augmented reality with 3D commodity tracking and image-space alignment.':",
        "Document: \"Tracking Of The Mitral Valve Leaflet In Echocardiography Images. This paper presents a semi-automatic method for the segmentation and the tracking of the mitral valve leaflet in transesophageal echocardiography. We use two connected active contours in order to track efficiently the mitral leaflet. Segmentation of very fast and non rigid motions involved are realized in two steps. In the first step we use transformation fitting to provide a rough segmentation that will be refined in a second step using snakes with Dynamic Programming minimization. The method successfully tracks the inner cardiac muscle and the mitral valve leaflet axis.\"",
        "Document: \"Combining vision based information and partial geometric models in automatic grasping. The problem of making sensing and acting techniques cooperate in order to achieve a given manipulation task in a partially structured environment is treated in the context of automatic grasping by guiding the decisional process using a combination of partial geometric models and a vision data. The geometric models represent the known information concerning the robot workspace and the object to be grasped. The vision-based information is collected at execution time using a 2D camera and a 3D vision sensor, both located on the robot end effector. This means that robot motions and sensing operations have to be combined for the purpose of both acquiring the missing information and guiding the grasping movements. This is achieved by applying three processing phases respectively aimed at selecting a viewpoint avoiding occlusions, modeling the local environment of the object to be grasped, and determining the grasping parameters\"",
        "Document: \"The use of localizers, robots and synergistic devices in CAS. There are many roles for electromechanical devices in image guided surgery. One is to help a surgeon accurately follow a preoperative plan. Devices for this purpose may be localizers, robots, or recently, synergistic systems in which surgeon and mechanism physically share control of the surgical tool. This paper discusses available technologies, and some emerging technologies, for guiding a surgical tool. Characteristics of each technology are discussed, and related to the needs which arise in surgical procedures. Three different approaches to synergistic systems, under study by the authors (PADyC, ACROBOT, and Cobots), are highlighted. An electromechanical device of some sort is needed in image-guided surgery, in order to connect the \"information world\" of images, plans, and computers, to the physical world of surgeons, patients, and tools. That is the situation in which a surgical plan has been created based on diagnostic images, and it is the job of the surgical system to guide the surgeon in the accurate execution of his own preoperative plan. The surgeon is again in direct contact with the surgical tool, but an interface device must also be connected to that tool, so that the computer may in some way provide guidance. Thought of as human interfaces, the perceptual quality of such a device is often the most prominent factor in the performance of surgical systems. We appreciate a quality that is sometimes called transparency - the quality of being perceptually absent. One purpose of this paper to describe the measures of interface device performance which determine their suitability for use in various surgical situations. We give examples of surgical situations that particularly depend on one or another of these measures. Another purpose is to describe several classes of interface devices, with examples. Previous descriptions of such devices relied on a decomposition in passive, active and semi-active systems (1) in which the degree of passivity was often associated with a type of technology. We prefer to define a new classification based on function rather than mechanism including localizers, robots, and also a new class which we call synergistic devices. Synergistic devices are intended for direct physical guidance of a surgical tool which is also held and controlled directly by a surgeon. Each of the authors is pursuing a different approach to synergistic devices, and these approaches are outlined. The paper concludes with a discussion of the applicability of the technologies to various surgical purposes.\"",
        "Document: \"Planning and Executing Sensory Based Grasping Operation in a Partially Known Environment. Making sensing and acting techniques to cooperate in order to achieve a given manipulation task in a partially structured environment, is one of the major issues towards autonomous robotics. In this paper, we describe the way we contributed to solve this problem in the context of automatic grasping. The method we have developed for that purpose, leads to combine commands for moving the robot end-effector towards some selected positions, with several sensing operations aimed at acquiring the missing information. It operates in three phases leading to (1) select a relatively uncluttured region from which it is theoritically possible to see the chosen features of the object to be grasped, (2) construct a model of the local environment of these features using the 3D vision sensor located on the robot end-effector, and (3) determine the grasping parameters and the required robot motions by reasoning on the gripper configuration space. The current implementation of the system and some experimental results are also presented in the paper.\"",
        "Document: \"Development of a Compact Cable-Driven Laparoscopic Endoscope Manipulator. This report describes continuing development of a novel compact surgical assistant robot to control the orientation and insertion depth of a laparoscopic endoscope during abdomenal surgery. The sterilizable manipulator is sufficiently small and lightweight that it can be placed directly on the body of the patient without interfering with other handheld instruments during abdomenal surgery. It consists of a round base, a clamp to hold the endoscope trocar, and two joints which enable azimuth rotation and inclination of the endoscope pivoting about the insertion point. The robot is actuated by thin cables inside flexible sleeves which are pulled by electric motors with a rack-and-pinion drive. Operation of the robot is demonstrated using an abdomenal surgical simulation trainer and experimental results are given.\"",
        "Document: \"Fusion d\u2019images : application au contr\u00f4le de la distribution des biopsies prostatiques. Les biopsies de prostate sont r\u00e9alis\u00e9es le plus souvent sous guidage \u00e9chographique 2D en suivant un planning pr\u00e9d\u00e9fini. Les outils modernes de traitement d\u2019images permettent actuellement de contr\u00f4ler la distribution des biopsies r\u00e9alis\u00e9es au sein de la glande. Nous avons \u00e9valu\u00e9 les capacit\u00e9s d\u2019un op\u00e9rateur \u00e0 r\u00e9aliser un planning pr\u00e9d\u00e9fini de 12 biopsies en fusionnant les trajets des 12 biopsies dans un seul et m\u00eame volume \u00e9chographique. En moyenne, l\u2019op\u00e9rateur atteint sa cible dans seulement 60 % des cas. Cette \u00e9tude permet donc de mettre en \u00e9vidence qu\u2019il est difficile d\u2019atteindre avec pr\u00e9cision une cible dans la prostate avec un guidage \u00e9chographique 2D. Dans le futur proche, la fusion en temps r\u00e9el d\u2019images \u00e9chographiques et IRM devrait permettre de s\u00e9lectionner une cible dans une image IRM acquise ant\u00e9rieurement et de l\u2019atteindre avec un guidage \u00e9chographique.\"",
        "Document: \"3-D ultrasound probe calibration for computer-guided diagnosis and therapy. With the emergence of swept-volume ultrasound (US) probes, precise and almost real-time US volume imaging has become available. This offers many new opportunities for computer guided diagnosis and therapy, 3-D images containing significantly more information than 2-D slices. However, computer guidance often requires knowledge about the exact position of US voxels relative to a tracking reference, which can only be achieved through probe calibration. In this paper we present a 3-D US probe calibration system based on a membrane phantom. The calibration matrix is retrieved by detection of a membrane plane in a dozen of US acquisitions of the phantom. Plane detection is robustly performed with the 2-D Hough transformation. The feature extraction process is fully automated, calibration requires about 20 minutes and the calibration system can be used in a clinical context. The precision of the system was evaluated to a root mean square (RMS) distance error of 1.15mm and to an RMS angular error of 0.61\u00b0. The point reconstruction accuracy was evaluated to 0.9mm and the angular reconstruction accuracy to 1.79\u00b0.\"",
        "Document: \"Hybrid 2D\u20133D ultrasound registration for navigated prostate biopsy. We present a hybrid 2D\u20133D ultrasound (US) rigid registration method for navigated prostate biopsy that enables continuous localization of the biopsy trajectory during the exam.\"",
        "Document: \"Atlas-Based Prostate Segmentation Using an Hybrid Registration. Purpose: This paper presents the preliminary results of a semi-automatic method for prostate segmentation of Magnetic Resonance Images (MRI) which aims to be incorporated in a navigation system for prostate brachyther- apy. Methods: The method is based on the registration of an anatomical atlas computed from a population of 18 MRI exams onto a patient im- age. An hybrid registration framework which couples an intensity-based registration with a robust point-matching algorithm is used for both atlas building and atlas registration. Results: The method has been validated on the same dataset that the one used to construct the atlas using the leave-one-out method. Results gives a mean error of 3.39 mm and a standard deviation of 1.95 mm with respect to expert segmentations. Conclusions: We think that this segmentation tool may be a very valuable help to the clinician for routine quantitative image exploitation.\"",
        "Document: \"Percutaneous Computer Assisted Iliosacral Screwing: Clinical Validation. \n This paper describes the clinical validation of an image-guided system for the percutaneous placement of iliosacral screws.\n The goals of the approach are to decrease surgical complications, with a percutaneous technique, and to increase the accuracy\n and security of screw positioning thanks to a computer assisted system. Pre-operative planning is performed on CT-scan images\n and a 3D model is built. During surgery, tools are tracked with an optical localizer. An ultrasound acquisition is performed\n and images are segmented to obtain 3D intra-operative data that are registered with the CT-scan 3D model. The surgeon is assisted\n during drilling and screwing processes with re-sliced CT-scan images displayed on the computer screen and comparison between\n pre-operative planning and tools position. The system was validated in a cadaver study [1]. The clinical validation has then\n started and four patients have been successfully instrumented.\n \n \"",
        "1 is \"Ultrasound-fluoroscopy registration for prostate brachytherapy dosimetry.\", 2 is \"A 3D Graphical User Interface for Resonance Modeling\"",
        "Given above information, for an author who has written the paper with the title \"Anatomical augmented reality with 3D commodity tracking and image-space alignment.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006172": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'SIA: A Supervised Inductive Algorithm with Genetic Search for Learning Attributes based Concepts':",
        "Document: \"An interactive visualization environment for data exploration using points of interest. We present in this paper an interactive method for numeric or symbolic data visualization that allows a domain expert to extract useful knowledge and information. We propose a new approach based on points of interest (POI) but in the context of visual data mining. POIs are located on a circle, and data are displayed within this circle according to their similarities to these POI. Interactive actions are possible: selection, zoom, dynamical change of POI. We evaluate the properties of such visualization with standard data with known characteristics. We describe an industrial application which explores results from satisfaction inquiries.\"",
        "Document: \"A new approach of data clustering using a flock of agents. This paper presents a new bio-inspired algorithm (FClust) that dynamically creates and visualizes groups of data. This algorithm uses the concepts of a flock of agents that move together in a complex manner with simple local rules. Each agent represents one data. The agents move together in a 2D environment with the aim of creating homogeneous groups of data. These groups are visualized in real time, and help the domain expert to understand the underlying structure of the data set, like for example a realistic number of classes, clusters of similar data, isolated data. We also present several extensions of this algorithm, which reduce its computational cost, and make use of a 3D display. This algorithm is then tested on artificial and real-world data, and a heuristic algorithm is used to evaluate the relevance of the obtained partitioning.\"",
        "Document: \"Visualisation interactive de donn\u00e9es avec des points d'int\u00e9r\u00eat. We present in this paper an interactive method for data visualization based on points of interest but in the context of visual data mining. Points of interest are located on a circle, and data are displayed within this circle according to their similarities to these points of interest. We evaluate the properties of such a visualization with artificial and standard data.\"",
        "Document: \"Visual Clustering with Artificial Ants Colonies. In this paper, we propose a new model of the chemical recognition system of ants to solve the unsupervised clustering problem. The colonial closure mechanism allows ants to discriminate between nest-mates and intruders by the mean of a colonial odour that is shared by every nestmate. In our model we associate each object of the data set to the odour of an artificial ant. Each odour is defined as a real vector with two components, that can be represented in a 2D-space of odours. Our method simulates meetings between ants according to pre-established behavioural rules, to ensure the convergence of similar odours (i.e. similar objects) in the same portion of the 2D-space. This provides the expected partition of the objects. We test our method against other well-known clustering method and show that it can perform well. Furthermore, our approach can handle every type of data (from numerical to symbolic attibutes, since there exists an adapted similarity measure) and allows one to visualize the dynamic creation of the nests. We plan to use this algorithm as a basis for a more sophisticated interactive clustering tool.\"",
        "Document: \"Incremental Delaunay Triangulation Construction for Clustering. In this paper, we propose an original solution to the problem of point cloud clustering. The proposed technique is based on a d-dimensional formulated Delaunay Triangulation (DT) construction algorithm and adapts it to the problem of cluster detection. The introduced algorithm allows this detection as along with the DT construction. Precisely, a criterion that detects occurrences of gaps in the simplex perimeter distribution is added during the incremental DT construction. This detection allows to label simplices as being inter -- or intra cluster. Experimental results on 2D shape datasets are presented and discussed in terms of cluster detection and topological relationship preservation.\"",
        "Document: \"AGIL: solving the exploration versus Exploitation dilemma in a simple classifier system applied to simulated robotics. This paper describes an adaptive genetic learning system called AGIL that solves control problems by learning conditions-actions rules (classifiers). For this purpose, the exploration versus exploitation dilemma (choosing between using learned knowledge or learning new knowledge) is solved less sensitively to rule strength and in a more explicit and controllable way than standard genetic techniques used in simple classifier systems. AGIL can be compared favorably with other learning systems on the multiplexor function learning task. It solves a simulated task of an autonomous moving robot that must reach a target in an unknown environment with obstacles.\"",
        "Document: \"A Viewable Indexing Structure for the Interactive Exploration of Dynamic and Large Image Collections. Thanks to the capturing devices cost reduction and the advent of social networks, the size of image collections is becoming extremely huge. Many works in the literature have addressed the indexing of large image collections for search purposes. However, there is a lack of support for exploratory data mining. One may want to wander around the images and experience serendipity in the exploration process. Thus, effective paradigms not only for organising, but also visualising these image collections become necessary. In this article, we present a study to jointly index and visualise large image collections. The work focuses on satisfying three constraints. First, large image collections, up to million of images, shall be handled. Second, dynamic collections, such as ever-growing collections, shall be processed in an incremental way, without reprocessing the whole collection at each modification. Finally, an intuitive and interactive exploration system shall be provided to the user to allow him to easily mine image collections. To this end, a data partitioning algorithm has been modified and proximity graphs have been used to fit the visualisation purpose. A custom web platform has been implemented to visualise the hierarchical and graph-based hybrid structure. The results of a user evaluation we have conducted show that the exploration of the collections is intuitive and smooth thanks to the proposed structure. Furthermore, the scalability of the proposed indexing method is proved using large public image collections.\n\n\"",
        "Document: \"Interactive Design of Web Sites with a Genetic Algorithm.  We deal in this paper with the problem of automatically generating the style and the layout of web pages and web sites ina real world application where many web sites are considered. One of the main difficulty is to take into account the userpreferences which are crucial in web sites design. We propose the use of an interactive genetic algorithm which generatessolutions (styles, layouts) and which lets the user select the solutions that he favors based on their graphicalrepresentation. Two... \"",
        "Document: \"Towards a Genetic Theory of Easy and Hard Functions. According to the literature that deals with the difficulty of functions with respect to genetic algorithms (GA), the so-called GA-hard functions are usually hard for other methods. In this paper, we firstly show that a gradient easy function can be fully deceptive, and thus hard for a GA to optimize while being unimodal. More generally, we show that the global search method introduced by (Das and Whitley 1991) to optimize GA-easy functions can be simply adapted to solve GA-hard functions. The resulting algorithm, called GSC1, generates a set of binary strings and outputs the string that wins the first order schemas competitions as well as its binary complement. According to the theory of deceptiveness in GAs, this method solves GA-easy and GA-hard problems efficiently, as shown effectively in the reported experiments. This method is however only well suited for these functions, and does not deal with partially deceptive functions. It is then shown how it could be combined with a GA.\"",
        "Document: \"Fast Unsupervised Clustering with Artificial Ants. AntClust is a clustering algorithm that is inspired by the chemical recognition system of real ants. It associates the genome of each artificial ant to an object of the initial data set and simulates meetings between ants to create nests of individuals that share a similar genome. Thus, the nests realize a partition of the original data set with no hypothesis concerning the output clusters (number, shape, size...) and with minimum input parameters. Due to an internal mechanism of nest selection and finalization, AntClust runs in the worst case in quadratic time complexity with the number of ants. In this paper, we evaluate new heuristics for nest selection and finalization that allows AntClust to run on linear time complexity with the number of ants.\"",
        "1 is \"The anatomy of a large-scale hypertextual Web search engine\", 2 is \"Topological analysis of an online social network for older adults\"",
        "Given above information, for an author who has written the paper with the title \"SIA: A Supervised Inductive Algorithm with Genetic Search for Learning Attributes based Concepts\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006192": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Energy efficient cellular networks with CoMP communications and smart grid':",
        "Document: \"Internet roundtrip delay prediction using the maximum entropy principle. Internet roundtrip delay/time (RTT) prediction plays an important role in detecting packet losses in reliable transport protocols for traditional web applications and determining proper transmission rates in many rate-based TCP-friendly protocols for Internet-based real-time applications. The widely adopted autoregressive and moving average (ARMA) model with fixed-parameters is shown to be insufficient for all scenarios due to its intrinsic limitation that it filters out all high-frequency components of RTT dynamics. In this paper, we introduce a novel parameter-varying RTT model for Internet roundtrip time prediction based on the information theory and the maximum entropy principle (MEP). Since the coefficients of the proposed RTT model are updated dynamically, the model is adaptive and it tracks RTT dynamics rapidly. The results of our experiments show that the MEP algorithm works better than the ARMA method in both RTT prediction and RTO estimation.\"",
        "Document: \"A stochastic game approach to the security issue of networked control systems under jamming attacks. Securing networked control systems (NCSs) from cyber attacks has been a very important issue to keep NCSs reliable and stable. Most existing efforts tackling this issue treat cyber attacks as model-based disturbances to NCSs. But the reality is that intelligent attackers will not follow any prescribed models and in fact they are able to change their attack strategies dynamically and randomly. In this paper, we address this problem and present an optimal defense mechanism for the NCS under jamming attacks based on the stochastic game theory. A two-player zero-sum stochastic game is formulated to model the dynamic interactions between a jammer (attacker) and a sensor transmitter (defender) in the NCS. In this stochastic game, the cost function includes not only the resource costs used to conduct cyber-layer defense and attack actions, but also the possible degraded dynamic performance (indexed by a quadratic state error) of the NCS. With this cost function, the impacts of the interactions between the attacker and the defender on the dynamic performance of the NCS are taken into account when the two players design/change their cyber-layer strategies. The optimal defense mechanism is obtained by solving a stochastic dynamic programming (SDP) problem. Simulation and comparison studies show that the packet-loss rate of the communication channel of the NCS has been greatly reduced and the dynamic performance of the NCS being attacked by an intelligent jammer is much improved when the proposed defense mechanism is deployed.\"",
        "Document: \"Backstepping Control for Nonlinear Systems With Time Delays and Applications to Chemical Reactor Systems. The state feedback control problem is addressed for a class of nonlinear time-delay systems. The time delays appear in all state variables of the nonlinear system, which brings a challenging issue for controller design. With an introduced new Lyapunov-Krasovskii functional, we develop a novel control strategy. With the help of a backstepping method, we design a memoryless state feedback controller...\"",
        "Document: \"Fixed-time almost disturbance decoupling of nonlinear time-varying systems with multiple disturbances and dead-zone input. \u2022This paper is the first to involve the fixed-time almost disturbance decoupling problem of nonlinear time-varying systems with multiple disturbances and dead-zone input.\u2022The assumption made on the nonlinear terms is further relaxed and a fixed-time state feedback controller is designed by the recursive approach similar to the backstepping procedure, which attenuates the influence of the disturbance on the output with a given degree.\u2022Under the designed controller, the fixed-time stability of the closed-loop system shows that the state variables tend to zero in a faster convergent speed than the finite-time scheme and the upper bound of the settling time is independent of the initial values.\"",
        "Document: \"Multi-innovation stochastic gradient algorithm for output error systems based on the auxiliary model. This paper combines the multi-innovation theory with the auxiliary model identification idea to present the auxiliary model based multi-innovation stochastic gradient algorithm by expanding the scalar innovation to an innovation vector and introducing the innovation length. Convergence analysis in the stochastic framework indicates that the parameter estimation error consistently converges to zero under certain excitation condition. Finally, we illustrate and test the proposed algorithm with an example.\"",
        "Document: \"Dynamic operation of BSs in green wireless cellular networks powered by the smart grid. There is great interest in considering the energy efficiency aspect of wireless cellular networks. When a wireless cellular network is powered by the smart grid, only considering energy efficiency in the cellular network is not enough. In this paper, we consider not only energy-efficient communications but also the dynamics of the smart grid in operating green wireless cellular networks. We formulate the system as a two-level Stackelberg game. A backward induction method is used to analyze the proposed scheme. we prove that the Stackelberg equilibrium (SE) of the proposed game exists and is unique. An iteration algorithm is proposed to obtain the SE. Simulation results show that the smart grid has a significant impact on green wireless cellular networks, and our proposed scheme can significantly reduce operational expenditure and CO2 emissions in green wireless cellular networks.\"",
        "Document: \"A novel sub-band adaptive filtering for acoustic echo cancellation based on empirical mode decomposition algorithm. Acoustic echo cancellation is one of the most severe requirements in hands-free telephone and teleconference communication. This paper proposes an Empirical Mode Decomposition (EMD)-based sub-band adaptive filtering structure, which applies the EMD-based algorithm dealing with the far-end speech signal and the microphone output to obtain two sets of intrinsic mode functions (IMFs). In addition, each IMF set is separated into different bands based on the power spectral density (PSD) of every IMF. Experiment signals were collected from a medium-size office room and simulations were taken under different conditions by three types of EMD-based algorithms. Results show that the proposed structure is able to model the transfer function of the unknown environment and track the change of the room much faster than the normalized adaptive filtering structure. The ensemble EMD (EEMD) algorithm and the noise-modulated EMD (NEMD) are proved to have better performance than the EMD algorithm in terms of echo return loss enhancement.\"",
        "Document: \"Effects of cyber attacks on islanded microgrid frequency control. This work investigates the impact of the communication-channel cyber attacks on the dynamic performance of the islanded microgrid secondary frequency control. The cyber-physical system structure of the secondary frequency control is described. The secondary frequency control is introduced. A set of cyber attacks including denial of service (DoS) and false data attacks are then modeled. The Canadian urban benchmark distribution system has been built for testing the impact of cyber attacks. The testing results show that both DoS and false data attacks could result in the performance degradation and even the instable operation of the islanded microgrid.\"",
        "Document: \"Improving Haptic Feedback Fidelity in Wave-Variable-Based Teleoperation Orientated to Telemedical Applications. In wave-variable-based teleoperation systems, under the condition of time delays, the perceived haptic feedback by the user is deteriorated by a bias term due to the communication nature of wave variables. Orientated to telemedical applications, this paper proposes a new method that can partially cancel the bias portion, so that the fidelity of haptic feedback is improved. Specifically, the return...\"",
        "Document: \"Output feedback sliding mode control for robot manipulators. In this work, we propose an output feedback sliding mode control (SMC) method for trajectory tracking of robotic manipulators. The design process has two steps. First, we design a stable SMC controller by assuming that all state variables are available. Then, an output feedback version of this SMC design is presented, which incorporates a model-free linear observer to estimate unknown velocity signals. We then show that the tracking performance under the output feedback design can asymptotically converge to the performance achieved under state-feedback-based SMC design. A detailed stability analysis is given, which shows semi-global uniform ultimate boundedness property of all the closed-loop signals. The proposed method is implemented and evaluated on a robotic system to illustrate the effectiveness of the theoretical development.\"",
        "1 is \"Extended stochastic gradient identification algorithms for Hammerstein-Wiener ARMAX systems\", 2 is \"Joint Optimal Cooperative Sensing and Resource Allocation in Multichannel Cognitive Radio Networks\"",
        "Given above information, for an author who has written the paper with the title \"Energy efficient cellular networks with CoMP communications and smart grid\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006197": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Near optimality of greedy strategies for string submodular functions with forward and backward curvature constraints':",
        "Document: \"Error probability bounds for binary relay trees with unreliable communication links. We study the decentralized detection problem in the context of balanced binary relay trees. We assume that the communication links in the tree network fail with certain probabilities. Not surprisingly, the step-wise reduction of the total detection error probability is slower than the case where the network has no communication link failures. We show that, under the assumption of identical communication link failure probability in the tree, the exponent of the total error probability at the fusion center is o(\u221aN) in the asymptotic regime. In addition, if the given communication link failure probabilities decrease to 0 as communications get closer to the fusion center, then the decay exponent of the total error probability is \u0398(\u221aN), provided that the decay of the failure probabilities is sufficiently fast.\"",
        "Document: \"Constrained Quadratic Minimizations For Signal Processing And Communications. Constrained minimization problems considered here arise in the design of multi-dimensional subspace beamformers for radar, sonar, seismology, and wireless communications, and in the design of precoders and equalizers for digital communications. The problem is to minimize a quadratic form, under a set of linear or quadratic constraints. We derive the solutions to these problems and establish connections between them. We show that the quadratically-constrained problem can be solved by solving a set of linearly-constrained problems and then using a majorization argument and Poincare's separation theorem to determine which linearly-constrained problem solves the quadratically-constrained one. In addition, we present illuminating circuit diagrams for our solutions, called generalized sidelobe canceller (GSC) diagrams, which allow us to tie our constrained minimizations to linear minimum mean-squared error (LA/IMSE) estimations.\"",
        "Document: \"Decentralized rate control for tracking and surveillance networks. We present a duality-based method for decentralized communication resource management in the context of networked tracking and surveillance systems. This method shares communication resources according to the joint utility provided by information of particular types. By formulating the problem as an optimization of the joint utility of information flow rates, the dual of the problem can be understood to provide prices for particular routes. Decentralized rate control is accomplished using primal-dual iterations in combination with communication of these route prices and marginal utility values. We build on previous work on the subject in a few important ways. First, we consider utility functions that are jointly dependent on flow rates, to properly account for geometric synergy that can occur in sensor fusion problems. Second, we do not require that the rate-update algorithms have explicit knowledge of utility functions. Instead, our update algorithms only require transmission of marginal utility values. We provide a proof of the convergence of the algorithm, and present simulation results to demonstrate its effectiveness.\"",
        "Document: \"Improved bounds for the greedy strategy in optimization problems with curvature. Consider the problem of choosing a set of actions to optimize a real-valued polymatroid function subject to matroid constraints. The greedy strategy, an approximate solution, is known to satisfy some bounds in terms of the total curvature. The total curvature depends on function values on sets outside the constraint matroid. If the function is defined only on the matroid, the problem still makes sense, but the existing bounds involving the total curvature do not apply, which is puzzling. This motivates an alternative formulation of such bounds. The first question we address is whether it is possible to extend a polymatroid function defined on a matroid to one on the entire power set. This was recently shown to be negative in general. Here, we provide necessary and sufficient conditions for the existence of an incremental extension of a polymatroid function defined on the uniform matroid of rank k to one with rank \\(k+1\\), together with an algorithm for constructing the extension. Whenever a polymatroid function defined on a matroid can be extended to the entire power set, the bounds involving the total curvature of the extension apply. However, these bounds still depend on sets outside the constraint matroid. Motivated by this, we define a new notion of curvature called partial curvature, involving only sets in the matroid. We derive necessary and sufficient conditions for an extension to have a total curvature equal to the partial curvature. Moreover, we prove that the bounds in terms of the partial curvature are in general improved over the previous ones. We illustrate our results with two contrasting examples motivated by practical problems.\"",
        "Document: \"Survivable Multipath Routing Using Link Penalization. Most previous schemes for survivable routing assume that the maximum number of simultaneous link failures is known. In this paper, we take an alternative approach by introducing link failure probabilities to the routing problem, and allowing each link to be used for several channels. Based on these assumptions, we formulate a survivable multipath routing problem for point-to-point communications. We then develop two heuristic multipath routing schemes for this problem: CPMR (conditional-penalization multipath routing) and SPMR (successive-penalization multipath routing). To deal with the difficulty that link-sharing causes, our schemes use \"link penalization\" methods to control (but not prohibit) link-sharing. We show via simulation that our schemes have significantly higher routing success rates than a routing scheme that searches for disjoint paths.\"",
        "Document: \"A Group Theoretic Model for Information. In this paper we formalize the notions of information elements and information lattices, first proposed by Shannon. Exploiting this formalization, we identify a comprehensive parallelism between information lattices and subgroup lattices. Qualitativel y, we demonstrate isomorphisms between informa- tion lattices and subgroup lattices. Quantitatively, we es tablish a decisive approximation relation between the entropy structures of information lattices and the log- index structures of the corresponding subgroup lattices. This approximation extends the approximation for joint entropies carried out previously by Chan and Yeung. As a consequence of our approximation result, we show that any continuous law holds in general for the entropies of information elements if and only if the same law holds in general for the log-indices of subgroups. As an application, by constructing subgroup counterexamples we find surprisingly that common information, unlike joint information, obeys neither the submodularity nor the supermodularity law. We emphasize that the notion of information elements is conceptually significant\u2014 formalizing it helps to reveal the deep connection between information theory and group theory. The parallelism established in this paper admits an appealing group-action explanation and provides useful insights into the intrinsic structure among information el ements from a group-theoretic perspective.\"",
        "Document: \"Opportunistic scheduling for OFDM systems with fairness constraints. We consider the problem of downlink scheduling for multiuser orthogonal frequency-division multiplexing (OFDM) systems. Opportunistic scheduling exploits the time-varying, location-dependent channel conditions to achieve multiuser diversity. Previous work in this area has focused on single-channel systems. Multiuser OFDM allows multiple users to transmit simultaneously over multiple channels. In this paper, we develop a rigorous framework to study opportunistic scheduling in multiuser OFDM systems. We derive optimal opportunistic scheduling policies under three QoS/fairness constraints formultiuserOFDM systems--temporal fairness, utilitarian fairness, and minimum-performance guarantees. Our scheduler decides not only which time slot, but also which subcarrier to allocate to each user. Implementing these optimal policies involves solving a maximal bipartite matching problem at each scheduling time. To solve this problem efficiently, we apply a modified Hungarian algorithm and a simple suboptimal algorithm. Numerical results demonstrate that our schemes achieve significant improvement in system performance compared with nonopportunistic schemes.\"",
        "Document: \"Rate of Learning in Hierarchical Social Networks. We study a social network consisting of agents organized as a hierarchical M-ary rooted tree, common in enterprise and military organizational structures. The goal is to aggregate information to solve a binary hypothesis testing problem. Each agent at a leaf of the tree, and only such an agent, makes a direct measurement of the underlying true hypothesis. The leaf agent then generates a binary message and sends it to its supervising agent, at the next level of the tree. Each supervising agent aggregates the messages from the M members of its group, produces a summary message, and sends it to its supervisor at the next level, and so on. Ultimately, the agent at the root of the tree makes an overall decision. We derive upper and lower bounds for the Type I and Type II error probabilities associated with this decision with respect to the number of leaf agents, which in turn characterize the converge rates of the Type I, Type II, and total error probabilities.\"",
        "Document: \"Ranking and Selection as Stochastic Control. Under a Bayesian framework, we formulate the fully sequential sampling and selection decision in statistical ranking and selection as a stochastic control problem, and derive the associated Bellman equation. Using a value function approximation, we derive an approximately optimal allocation policy. We show that this policy is not only computationally efficient but also possesses both one-step-ahea...\"",
        "Document: \"String Submodular Functions with Curvature Constraints. The problem of choosing a string of actions to optimize an objective function that is string submodular has been considered in [1]. There it is shown that the greedy strategy, consisting of a string of actions that only locally maximizes the step-wise gain in the objective function, achieves at least a (1 \udbc0\udc00 e \udbc0\udc001)-approximation to the optimal strategy. This paper improves this approximation by introducing additional constraints on curvature, namely, total backward curvature, total forward curvature, and elemental forward curvature. We show that if the objective function has total backward curvature \u03c3, then the greedy strategy achieves at least a 1 (1\udbc0\udc00e \udbc0\udc00)-approximation of the optimal strategy. If the objective function has total forward curvature ??, then the greedy strategy achieves at least a (1\udbc0\udc00??)- approximation of the optimal strategy. Moreover, we consider a generalization of the diminishing-return property by dening the elemental forward curvature. We also introduce the notion of string-matroid and consider the problem of maximizing the objective function subject to a string-matroid constraint. We investigate two applications of string submodular functions with curvature constraints: 1) choosing a string of actions to maximize the expected fraction of accomplished tasks; and 2) designing a string of measurement matrices such that the information gain is maximized.\"",
        "1 is \"Learning multi-label scene classification\", 2 is \"Fluid and diffusion approximations of a two-station mixed queueing network\"",
        "Given above information, for an author who has written the paper with the title \"Near optimality of greedy strategies for string submodular functions with forward and backward curvature constraints\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006212": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Utilization and fairness in spectrum assignment for opportunistic spectrum access':",
        "Document: \"Efficient Batched Synchronization in Dropbox-Like Cloud Storage Services. As tools for personal storage, file synchronization and data sharing, cloud storage services such as Dropbox have quickly gained popularity. These services provide users with ubiquitous, reliable data storage that can be automatically synced across multiple devices, and also shared among a group of users. To minimize the network overhead, cloud storage services employ binary diff, data compression, and other mechanisms when transferring updates among users. However, despite these optimizations, we observe that in the presence of frequent, short updates to user data, the network traffic generated by cloud storage services often exhibits pathological inefficiencies. Through comprehensive measurements and detailed analysis, we demonstrate that many cloud storage applications generate session maintenance traffic that far exceeds the useful update traffic. We refer to this behavior as the traffic overuse problem. To address this problem, we propose the update-batched delayed synchronization (UDS) mechanism. Acting as a middleware between the user's file storage system and a cloud storage application, UDS batches updates from clients to significantly reduce the overhead caused by session maintenance traffic, while preserving the rapid file synchronization that users expect from cloud storage services. Furthermore, we extend UDS with a backwards compatible Linux kernel modification that further improves the performance of cloud storage applications by reducing the CPU usage. \u00a9 IFIP International Federation for Information Processing 2013.\"",
        "Document: \"Choosing an Accurate Network Model using Domain Analysis. Network link simulation is perhaps the most common method for evaluating application and network protocol de-signs. Simulation enables researchers to quickly and repeat-ably explore the behavior of a protocol under a variety of network conditions (e. g., varying loss, delay, and error). However, accurate results are highly dependent on realistic network conditions being simulated. Previous work [4] ar-gues that the use of inaccurate models leads to flaws in net-working research. We also demonstrated the importance of model accuracy by observing that a naive error model used in simulation during protocol design led to a poor choice of a protocol parameter [7]. For example, a detailed under-standing of the packet loss process and burstiness of errors is necessary for the proper design of error control protocols such as Automatic Repeat reQuest (ARQ) protocols. In modeling realistic networks, researchers face measurements whose characteristics experience non-stationarity (time variability) and complex patterns due to a number of factors, including both internal network ele-ments and external events. While classical models such as Bernoulli, Gilbert, high-order Discrete Time Markov Chain (DTMC), or Hidden Markov Models (HMM) have worked surprisingly well in modeling events in traditional net-works, they are ill-suited for handling traces from some of today's networks ( e. g., lossy wireless channels). For exam-ple, the Bernoulli model is a memory-less process, where each value is generated statistically independent of pre-vious outputs. Thus it is unlikely to produce accurate models of networks exhibiting bursty losses such as wire-less links. To address this, we introduce a data precon-ditioning technique that extracts and models the station-ary components of non-stationary datasets. We describe the original data preconditioning model the MTA [8], and in-troduce the Multiple states MTA (MMTA) model. \"",
        "Document: \"Supporting demanding wireless applications with frequency-agile radios. With the advent of new FCC policies on spectrum allocation for next generation wireless devices, we have a rare opportunity to redesign spectrum access protocols to support demanding, latency-sensitive applications such as high-def media streaming in home networks. Given their low tolerance for traffic delays and disruptions, these applications are ill-suited for traditional, contention-based CSMA protocols. In this paper, we explore an alternative approach to spectrum access that relies on frequency-agile radios to performinterference-free transmission across orthogonal frequencies. We describe Jello, a MAC overlay where devices sense and occupy unused spectrum without central coordination or dedicated radio for control. We show that over time, spectrum fragmentation can significantly reduce usable spectrum in the system. Jello addresses this using two complementary techniques: online spectrum defragmentation, where active devices periodically migrate spectrum usage, and non-contiguous access, which allows a single flow to utilize multiple spectrum fragments. Our prototype on an 8-node GNU radio testbed shows that Jello significantly reduces spectrum fragmentation and provides high utilization while adapting to client flows' changing traffic demands.\"",
        "Document: \"Value and Misinformation in Collaborative Investing Platforms. It is often difficult to separate the highly capable \u201cexperts\u201d from the average worker in crowdsourced systems. This is especially true for challenge application domains that require extensive domain knowledge. The problem of stock analysis is one such domain, where even the highly paid, well-educated domain experts are prone to make mistakes. As an extremely challenging problem space, the \u201cwisdom of the crowds\u201d property that many crowdsourced applications rely on may not hold. In this article, we study the problem of evaluating and identifying experts in the context of SeekingAlpha and StockTwits, two crowdsourced investment services that have recently begun to encroach on a space dominated for decades by large investment banks. We seek to understand the quality and impact of content on collaborative investment platforms, by empirically analyzing complete datasets of SeekingAlpha articles (9 years) and StockTwits messages (4 years). We develop sentiment analysis tools and correlate contributed content to the historical performance of relevant stocks. While SeekingAlpha articles and StockTwits messages provide minimal correlation to stock performance in aggregate, a subset of experts contribute more valuable (predictive) content. We show that these authors can be easily identified by user interactions, and investments based on their analysis significantly outperform broader markets. This effectively shows that even in challenging application domains, there is a secondary or indirect wisdom of the crowds. Finally, we conduct a user survey that sheds light on users\u2019 views of SeekingAlpha content and stock manipulation. We also devote efforts to identify potential manipulation of stocks by detecting authors controlling multiple identities.\"",
        "Document: \"Wisdom in the social crowd: an analysis of quora. Efforts such as Wikipedia have shown the ability of user communities to collect, organize and curate information on the Internet. Recently, a number of question and answer (Q&A) sites have successfully built large growing knowledge repositories, each driven by a wide range of questions and answers from its users community. While sites like Yahoo Answers have stalled and begun to shrink, one site still going strong is Quora, a rapidly growing service that augments a regular Q&A system with social links between users. Despite its success, however, little is known about what drives Quora's growth, and how it continues to connect visitors and experts to the right questions as it grows. In this paper, we present results of a detailed analysis of Quora using measurements. We shed light on the impact of three different connection networks (or graphs) inside Quora, a graph connecting topics to users, a social graph connecting users, and a graph connecting related questions. Our results show that heterogeneity in the user and question graphs are significant contributors to the quality of Quora's knowledge base. One drives the attention and activity of users, and the other directs them to a small set of popular and interesting questions.\"",
        "Document: \"Measurement-calibrated conflict graphs for dynamic spectrum distribution. Building accurate interference maps is critical for performing reliable and efficient spectrum allocation. In this work, we use empirical data to explore the feasibility of using measurement-calibrated propagation models to build accurate interference models. Our work shows that calibrated propagation models generate location-dependent signal prediction errors. Such error pattern leads to conservative conflict graphs that actually improve the reliability of spectrum allocations by reducing the impact of unpredicted accumulative interference.\"",
        "Document: \"Hybrid overlay structure based on random walks. Application-level multicast on structured overlays often suffer several drawbacks: 1) The regularity of the architecture makes it difficult to adapt to topology changes; 2) the uniformity of the protocol generally does not consider node heterogeneity. It would be ideal to combine the scalability of these overlays with the flexibility of an unstructured topology. In this paper, we propose a locality-aware hybrid overlay that combines the scalability and interface of a structured network with the connection flexibility of an unstructured network. Nodes self-organize into structured clusters based on network locality, while connections between clusters are created adaptively through random walks. Simulations show that this structure is efficient in terms of both delay and bandwidth. The network also supports the scalable fast rendezvous interface provided by structured overlays, resulting in fast membership operations.\"",
        "Document: \"The effectiveness of opportunistic spectrum access: a measurement study. Dynamic spectrum access networks are designed to allow today's bandwidth-hungry \"secondary devices\" to share spectrum allocated to legacy devices, or \"primary users.\" The success of this wireless communication model relies on the availability of unused spectrum and the ability of secondary devices to utilize spectrum without disrupting transmissions of primary users. While recent measurement studies have shown that there is sufficient underutilized spectrum available, little is known about whether secondary devices can efficiently make use of available spectrum while minimizing disruptions to primary users. In this paper, we present the first comprehensive study on the presence of \"usable\" spectrum in opportunistic spectrum access systems, and whether sufficient spectrum can be extracted by secondary devices to support traditional networking applications. We use for our study fine-grain usage traces of a wide spectrum range (20 MHz-6 GHz) taken at four locations in Germany, the Netherlands, and Santa Barbara, CA. Our study shows that on average, 54% of spectrum is never used and 26% is only partially used. Surprisingly, in this 26% of partially used spectrum, secondary devices can utilize very little spectrum using conservative access policies to minimize interference with primary users. Even assuming an optimal access scheme and extensive statistical knowledge of primary-user access patterns, a user can only extract between 20%-30% of the total available spectrum. To provide better spectrum availability, we propose frequency bundling, where secondary devices build reliable channels by combining multiple unreliable frequencies into virtual frequency bundles. Analyzing our traces, we find that there is little correlation of spectrum availability across channels, and that bundling random channels together can provide sustained periods of reliable transmission with only short interruptions.\"",
        "Document: \"Object Recognition and Navigation using a Single Networking Device. Tomorrow's autonomous mobile devices need accurate, robust and real-time sensing of their operating environment. Today's solutions fall short. Vision or acoustic-based techniques are vulnerable against challenging lighting conditions or background noise, while more robust laser or RF solutions require either bulky expensive hardware or tight coordination between multiple devices. This paper describes the design, implementation and evaluation of Ulysses, a practical environmental imaging system using colocated 60GHz radios on a single mobile device. Unlike alternatives that require specialized hardware, Ulysses reuses low-cost commodity networking chipsets available today. Ulysses' new imaging approach leverages RF beamforming, operates on specular (direct) reflection, and integrates the device's movement trajectory with sensing. Ulysses also includes a navigation component that uses the same 60GHz radios to compute \\\"safety regions\\\" where devices can move freely without collision, and to compute optimal paths for imaging within safety regions. Using our implementation of a small robotic car prototype, our experimental results show that Ulysses images objects meters away with cm-level precision, and provides accurate estimates of objects' surface materials.\"",
        "Document: \"Packet-Level Telemetry in Large Datacenter Networks. Debugging faults in complex networks often requires capturing and analyzing traffic at the packet level. In this task, datacenter networks (DCNs) present unique challenges with their scale, traffic volume, and diversity of faults. To troubleshoot faults in a timely manner, DCN administrators must a) identify affected packets inside large volume of traffic; b) track them across multiple network components; c) analyze traffic traces for fault patterns; and d) test or confirm potential causes. To our knowledge, no tool today can achieve both the specificity and scale required for this task. We present Everflow, a packet-level network telemetry system for large DCNs. Everflow traces specific packets by implementing a powerful packet filter on top of \\\"match and mirror\\\" functionality of commodity switches. It shuffles captured packets to multiple analysis servers using load balancers built on switch ASICs, and it sends \\\"guided probes\\\" to test or confirm potential faults. We present experiments that demonstrate Everflow's scalability, and share experiences of troubleshooting network faults gathered from running it for over 6 months in Microsoft's DCNs.\"",
        "1 is \"Multi-dimensional Conflict Graph Based Computing for Optimal Capacity in MR-MC Wireless Networks\", 2 is \"Soft Handoffs In Cdma Mobile Systems\"",
        "Given above information, for an author who has written the paper with the title \"Utilization and fairness in spectrum assignment for opportunistic spectrum access\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006256": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Navigation in GPS-denied environments':",
        "Document: \"Secure control systems with application to cyber-physical systems. Control systems are computer-based systems with networked units consisting of sensors, actuators, control processing units, and communication devices. The role of control system is to interact, monitor, and control physical processes. Reactive power control is a fundamental issue in ensuring the security of the power network. It is claimed that Synchronous Condensers (SC) have been used at both distribution and transmission voltage levels to improve stability and to maintain voltages within desired limits under changing load conditions and contingency situations. Performance of PI controller corresponding to various tripping faults are analyzed for SC systems. Most of the effort in protecting these systems has been in protection against random failures or reliability. However, besides failures these systems are subject to various signal attacks for which new analysis are discussed here. When a breach does occur, it is necessary to react in a time commensurate with the physical dynamics of the system as it responds to the attack. Failure to act swiftly enough may result in undesirable, and possibly irreversible, physical effects. Therefore, it is meaningful to evaluate the security of a cyber-physical system, especially to protect it from cyber-attack. Illustrative numerical examples are provided together with an application to the SC systems.\"",
        "Document: \"Combating False Reports for Secure Networked Control in Smart Grid via Trustiness Evaluation. Smart grid, equipped with modern communication infrastructures, is subject to possible cyber attacks. Particularly, false report attacks which replace the sensor reports with fraud ones may cause the instability of the whole power grid or even result in a large area blackout. In this paper, a trustiness system is introduced to the controller, who computes the trustiness of different sensors by comparing its prediction, obtained from Kalman filtering, on the system state with the reports from sensor. The trustiness mechanism is discussed and analyzed for the Linear Quadratic Regulation (LQR) controller. Numerical simulations show that the trustiness system can effectively combat the cyber attacks to smart grid.\"",
        "Document: \"Security of Classic PN-Spreading Codes for Hybrid DS/FH Spread-Spectrum Systems. Hybrid direct sequence/frequency hopping (DS/FH) spread-spectrum communication systems have recently received considerable interest in commercial applications in addition to their use in military communications because they accommodate high data rates with high link integrity, even in the presence of significant multipath effects and interfering signals. The security of hybrid DS/FH systems strongly depends on the choice of PN-spreading code employed. In this paper, we examine the security, in terms of unicity distance, of linear maximal-length, Gold, and Kasami PN-spreading codes for DS, FH, and hybrid DS/FH spread-spectrum systems without additional encryption methods. The unicity distance is a measure of the minimum amount of ciphertext required by an eavesdropper to uniquely determine the specific key used in a cryptosystem and hence break the cipher. Numerical results are presented to compare the security of the considered PN-spreading codes under known-ciphertext attacks.\"",
        "Document: \"Comments on \"Is the Frobenius Matrix Norm Induced?\" [with reply]. In \"Is the Frobenius matrix norm induced?\", the authors ask whether the Frobenius and the H/sup 2/ norms are induced. There, they claimed that the Frobenius norm is not induced and, consequently, conjectured that the H/sup 2/ norm may not be induced. In this paper, it is shown that the Frobenius norm is induced on particular matrix spaces. It is then shown that the H/sup 2/ norm is in fact induced on a particular matrix-valued L/sup /spl infin// space. A reply is provided to the comments.\"",
        "Document: \"Nonlinear model reduction using Space Vectors Clustering POD with application to the Burgers' equation. Proper Orthogonal Decomposition (POD) fails to capture the nonlinear degrees of freedom in large scale highly nonlinear systems because it assumes that data belongs to a linear space. In this paper we develop a new method that makes POD more accurate in reducing the order of large scale nonlinear systems. The solution space is grouped into clusters where the behavior has significantly different features. Although the clustering idea is not new, it has been implemented only on snapshots clustering where a snapshot is the solution over the whole space at a particular time. We show that clustering the spatial domain into the same number of clusters is more efficient. We call it Space Vector Clustering (SVC) POD where a space vector is the solution over all times at a particular space location. This is consistent with the fact that for infinite dimensional systems described by partial differential equations (PDEs), the large number of states comes from the discretization of the spatial domain of the PDE, not the time domain. We apply our method to reduce a nonlinear convective PDE system governed by the Burgers' equation over 1D and 2D domains and show a significant improvement over conventional POD.\"",
        "Document: \"Commutant lifting for linear time-varying systems. In this paper, we study two robust control problems for possibly infinite dimensional (i.e., systems with an infinite number of states) linear time-varying (LTV) systems using a framework based on a version of the commutant lifting theorem developed for nest algebras. The approach is purely operator theoretic and does not use any state space representation. The two problems studied include the optimal disturbance attenuation and the optimal mixed sensitivity problems for LTV systems. The proposed solutions are given in terms of projections of time-varying multiplication operators. The latter are computed explicitly.\"",
        "Document: \"Stochastic Control Of Energy Efficient Buildings: A Semidefinite Programming Approach. The key goal in energy efficient buildings is to reduce energy consumption of Heating, Ventilation, and Air-Conditioning (HVAC) systems while maintaining a comfortable temperature and humidity in the building. This paper proposes a novel stochastic control approach for achieving joint performance and power control of HVAC. We employ a constrained Stochastic Linear Quadratic Control (cSLQC) by minimizing a quadratic cost function with a disturbance assumed to be Gaussian. The problem is formulated to minimize the expected cost subject to a linear constraint and a probabilistic constraint. By using cSLQC, the problem is reduced to a semidefinite optimization problem, where the optimal control can be computed efficiently by Semidefinite programming (SDP). Simulation results are provided to demonstrate the effectiveness and power efficiency by utilizing the proposed control approach.\"",
        "Document: \"Operator theoretic approach to the optimal distributed control problem for spatially invariant systems. This paper considers the problem of optimal distributed control of spatially invariant systems. The Banach space duality structure of the problem is characterized in terms of tensor product spaces. This complements the prior study undertaken by the authors, where the dual and pre-dual formulations were in terms of abstract spaces. Here, we show that these spaces together with the pre-annihilator and annihilator subspaces can be realized explicitly as specific tensor spaces and subspaces, respectively. The tensor space formulation leads to a solution in terms of an operator given by a tensor product. Specifically, the optimal distributed control performance for spatially invariant systems is equal to the operator induced norm of this operator. The results obtained in this paper bridge the gap between control theory and the metric theory of tensor product spaces.\"",
        "Document: \"Home Energy Management Based On Optimal Production Control Scheduling With Unknown Regime Switching. We propose a novel home energy management framework to intelligently schedule the distributed energy storage (DES) for the cost reduction of customers in this paper. The proposed optimal production control technique determines the action policy (e.g., charging or discharging) and the power allocation policy of the DES to provide DES power at proper time with lower price than that of the utility grid, resulting in the reduction of the long term financial cost. Specifically, we first formulate the optimal decision problem for home energy systems with solar and energy storage devices, when the demand, renewable energy, electricity purchase from grid are all subject to Brownian motions. Both drift and variance parameters are modulated by a continuous-time Markov chain that represents the regime of electricity price. In particular, we set up a mean-variance problem where the cost function is both the running cost of diesel generator and deviation from the target State of Charge (SOC) of batteries. We assume the regime information follows a Hidden Markov Model (HMM), and then estimate the state by change of measure based on the Girsanov's theorem. Finally, the problem boils down to solving a stochastic differential equation (SDE), which we provide both the explicit and numerical solutions to this specific SDE. An example is provided to illustrate the effectiveness of our proposed approach. Moreover, we compare it with the traditional Model Predictive Control (MPC) technique, and show it outperforms MPC.\"",
        "Document: \"Statistical analysis of multipath fading channels using generalizations of shot noise. This paper provides a connection between the shot-noise analysis of Rice and the statistical analysis of multipath fading wireless channels when the received signals are a low-pass signal and a bandpass signal. Under certain conditions, explicit expressions are obtained for autocorrelation functions, power spectral densities, and moment-generating functions. In addition, a central limit theorem is derived identifying the mean and covariance of the received signals, which is a generalization of Campbell's theorem. The results are easily applicable to transmitted signals which are random and to CDMA signals.\"",
        "1 is \"On topology and dynamics of consensus among linear high-order agents\", 2 is \"Flexible learning of problem solving heuristics through adaptive search\"",
        "Given above information, for an author who has written the paper with the title \"Navigation in GPS-denied environments\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006304": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Semantic specification and evaluation of bids in web-based markets':",
        "Document: \"Evaluating the STORE Reputation System in Multi-Agent Simulations. In recent global business environments, collaborations among organisations raise an increased demand for swift establishment. Such collaborations are formed between organisations entering Virtual Organizations (VOs), crossing geographic borders and frequently without prior experience of the other partner's previous performance. In VOs, every participant risks engaging with partners who may exhibit unexpected fraudulent or otherwise untrusted behaviour. In order to cope with this risk, the STochastic REputation system (STORE) was designed to provide swift, automated decision support for selecting partner organisations in the early stages of the VO's formation. The contribution of this paper first consists of a multi-agent simulation framework design and implementation to evaluate the STORE reputation system. This framework is able to simulate dynamic agent behaviour, agents hereby representing organisations, and to capture the business context of different VO application scenarios. A configuration of agent classes is a powerful tool to obtain not only well or badly performing agents for simulation scenarios, but also agents which are specialized in particular VO application domains or even malicious agents, attacking the VO community. The second contribution comprises of STORE's evaluation in two simulation scenarios, set in the VO application domains of Collaborative Engineering and Ad-hoc Service provisioning. Besides the ability to clearly distinguish between agents of different classes according to their reputation, the results prove STORE's ability to take an agent's dynamic behaviour into account. The simulation results show, that STORE solves the difficult task of selecting the most trustworthy partner for a particular VO application domain from a set of honest agents that are specialized in a wide spread of VO application domains.\"",
        "Document: \"Market-Based EV Charging Coordination. Electric vehicle (EV) charging loads may challenge grid stability due to a combination of high charging power and temporal clustering of charging activity. Hence, EV charging needs to be coordinated appropriately. Prior work addressing this challenge focused on static charging strategies responding to exogenous price vectors. We extend this work in two directions: To achieve an endogenous resource pricing, we substitute exogenous pricing for a local market platform which allocates available charging capacity to demand from EVs. To achieve meaningful interaction with this market, we model the bidding behavior of EVs by means of a Q-learning approach. Using an integrated trip-based state space representation spanned by required battery level and time to departure, we moderate between bidding aggressiveness and mobility requirements. For appropriate learning parameters, bidding behavior converges and the market achieves significant load shifting.\"",
        "Document: \"Foundations of Trust: Contextualising Trust in Social Clouds. In this paper, we lay the foundations for a contextualisation of trust, the role it plays, and its different layers within the context of a novel paradigm: Social Cloud Computing. In a Social Cloud, trust plays a vital role as a collaboration enabler. However, trust is not trivial to define, observe, represent and analyse as precursors to understand exactly what role it plays in the enablement of collaboration. We do this through the definition of structure of a Social Cloud as a sequence of social and cognitive processes. We then survey research from the domains of computer science, economics and sociology that consider trust in online communities and exchange scenarios to illustrate the complexity of modelling trust in our scenario. Finally, we define trust within the context of a Social Cloud and identify the core components of trust to facilitate its understanding.\"",
        "Document: \"AKX - An exchange for predicting water dam levels in Australia. The Australian population in rural and urban areas is heavily influenced and affected by such water shortages, either economically or in their life style. Managing water resources is therefore seen as a critical environmental, social and economic issue. Good forecasts can provide better understanding for the current situation (e.g. drought severity) and consequently improve decision making. Prediction markets have long proved to successfully forecast events in a wide range of applications. They seem to be a promising tool for aggregating and at the same time publishing information about water availability. With the Australian Knowledge Exchange (AKX) we launched a prediction market, in which people were invited to trade their expectations about future dam levels. Our results show that traders are able to forecast water dam levels quite accurately. Nevertheless, a simple self-developed model based on historic data beats the market forecast in half of the cases. Experts seem reluctant - due to various reasons - to join and participate in (water related) prediction markets. In summary, our first experiment results show that markets are a promising approach to forecast Natural Resource Management related figures. Further improvements are discussed which may help to increase prediction accuracy in future applications.\"",
        "Document: \"Elicitation of user preferences for multi-attribute negotiation. Agents that act on behalf of users in electronic negotiations need to elicit the required information about their users' preference structures. Based on a multi-attri\\-bute utility theoretic model of user preferences, we propose an algorithm that enables an agent to learn the utility function with flexibility to accept several types of information for learning. The method combines an evolutionary learning with the application of external knowledge and local search. Empirical tests show that the algorithm provides a good learning performance.\"",
        "Document: \"Portfolio and contract design for demand response resources. \u2022We present a two-stage model of an aggregator portfolio and contract design problem.\u2022The bi-level model includes an aggregator and customer perspective.\u2022We explore three distinct contracting regimes.\u2022The interplay between flexibility contracting and renewable generation is analyzed.\u2022We investigate the impact of flexible supply cost on demand flexibility contracting.\"",
        "Document: \"Towards Financial Planning as a Service. The financial crisis has recalled the importance of proper financial planning. Companies which are organized as a multitude of legal entities are in particular affected by planning irregularities. To optimize the financial planning process and to cope with the challenges, we propose our service model \"Financial Planning as a Service\" (FiPlaaS). This approach allows companies to redesign their planning processes according to SOA principles and to achieve substantial improvements in performance.\"",
        "Document: \"Semantic specification and evaluation of bids in web-based markets. In recent years, electronic markets have gained much attention as institutions to allocate goods and services efficiently between buyers and sellers. By leveraging the Web as a global communication medium, electronic markets provide a platform that allows participants throughout the world to spontaneously exchange products in a flexible manner. However, ensuring interoperability and mutual understanding in such a highly dynamic and heterogenous environment can easily become very tricky, particularly if the services and goods involved are complex and described by multiple attributes. In this paper, we present a comprehensive ontology framework that allows the specification of bids for Web-based markets. By expressing utility function policies with a logic-based and standardized formalism, the framework enables a compact bid representation particularly for highly configurable goods and services while ensuring a high degree of interoperability. To facilitate matchmaking of offers and requests in the market, a method for evaluating bids based on logical reasoning is presented. In addition, as proof of concept, we show how the framework can be applied in a Web service selection scenario.\"",
        "Document: \"Views on the Past, Present, and Future of Business and Information Systems Engineering. \u2018\u2018The times they are a-changin,\u2019\u2019 a famous song title by Bob Dylan, also applies to our profession and our subject of study. Information technology has always been a driver for innovation. The recent years, however, have seen IT-based innovations that truly impact everybody\u2019s lives. Everything that can be digitized will be digitized, and this trend is continuing at an amazing speed. For a discipline that looks at the design and utilization ofinformation systems these are exciting times. Yet, it is also a time full of challenges. While our discipline has much to contribute, it competes with other disciplines for topics and ideas. Also, the scope of topics studied has become broader and broader, and so have our methods. While initial work in Business and Information Systems Engineering (BISE) was often rooted in artificial intelligence, database systems, or operations research, the community has adopted new approaches to address new types of problems. Nowadays, we also have a strong group of academics working primarily with empirical methods or methods from microeconomics, to name just a few. This development towards a more multiparadigmatic discipline also had its challenges and there were controversial discussions along the way.\"",
        "Document: \"Excitement Up! Price Down! Measuring Emotions In Dutch Auctions. Recently, Internet auction sites have begun to compete in emotions by advertising the fun and excitement that bidders may experience during auction participation. The implicit conjecture inherent in these advertisements is that consumers derive a hedonic value from auction participation that makes exciting auctions more attractive and possibly induces bidders to stay active longer in the auction. We test this conjecture in a controlled economic laboratory experiment in which we also measure bidders' physiological arousal in terms of heart rate and skin conductivity as proxies for their excitement during auction participation. In particular, we conduct a series of descending price (Dutch) auctions with different clock speeds and find that in fast Dutch auctions bidders (1) are more excited and (2) stay longer in the bidding process. Moreover, the unpleasant event of losing a Dutch auction is experienced more strongly than the rewarding event of winning. Our results show that bidders' excitement is directly reflected in final prices, indicating that Internet auction site sponsors should carefully consider how the emotions elicited on their platforms affect bidding behavior.\"",
        "1 is \"Learning-based negotiation strategies for grid scheduling\", 2 is \"A survey and classification of semantic search approaches\"",
        "Given above information, for an author who has written the paper with the title \"Semantic specification and evaluation of bids in web-based markets\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006390": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'On a class of left-continuous t-norms':",
        "Document: \"The consequence relation in the logic of commutative GBL-algebras is PSPACE-complete. Commutative, integral and bounded GBL-algebras form a subvariety of residuated lattices which provides the algebraic semantics of an interesting common fragment of intuitionistic logic and of several fuzzy logics. It is known that both the equational theory and the quasiequational theory of commutative GBL-algebras are decidable (in contrast to the noncommutative case), but their complexity has not been studied yet. In this paper, we prove that both theories are in PSPACE, and that the quasiequational theory is PSPACE-hard.\"",
        "Document: \"Basic Hoops: an Algebraic Study of Continuous -norms. A continuous t-norm is a continuous map * from [0, 1]2 into [0, 1] such that $$\\langle [0, 1], *, 1 \\rangle$$ is a commutative totally ordered monoid. Since the natural ordering on [0, 1] is a complete lattice ordering, each continuous\n t-norm induces naturally a residuation \u2192 and $$\\langle [0, 1], *,\\rightarrow, 1\\rangle$$ becomes a commutative naturally ordered residuated monoid, also called a hoop. The variety of basic hoops is precisely the\n variety generated by all algebras $$\\langle [0, 1], *,\\rightarrow, 1\\rangle$$ , where * is a continuous t-norm. In this paper we investigate the structure of the variety of basic hoops and some of its subvarieties. In particular\n we provide a complete description of the finite subdirectly irreducible basic hoops, and we show that the variety of basic\n hoops is generated as a quasivariety by its finite algebras. We extend these results to H\u00e1jek\u2019s BL-algebras, and we give an alternative proof of the fact that the variety of BL-algebras is generated by all algebras arising from continuous t-norms on [0, 1] and their residua. The last part of the paper is devoted to the investigation of the subreducts of BL-algebras, of G\u00f6del algebras and of product algebras.\"",
        "Document: \"Investigations on Fragments of First Order Branching Temporal Logic. We investigate axiomatizability of various fragments of first order computational tree logic (FOCTL) showing that the fragments with the modal operator F (H, respectively) are non axiomatizable. These results shows that the only axiomatizable fragment is the one with the modal operator next (X) only.\"",
        "Document: \"Archimedean classes in integral commutative residuated chains. This paper investigates a quasi-variety of representable integral commutative residuated lattices axiomatized by the quasi-identity resulting from the well-known Wajsberg identity (p -> q) -> q <= (q -> p) P if it is written as a quasi-identity, i.e., (p -> q) -> q approximate to 1 double right arrow (q -> p) -> p approximate to 1. We prove that this quasi-identity is strictly weaker than the corresponding identity. On the other hand, we show that the resulting quasi-variety is in fact a variety and provide an axiomatization. The obtained results shed some light on the structure of Archimedean integral commutative residuated chains. Further, they can be applied to various subvarieties of MTL-algebras, for instance we answer negatively Hajek's question asking whether the variety of Pi MTL-algebras is generated by its Archimedean members. (C) 2009 WILEY-VCH Verlag GmbH & Co. KGaA, Weinheim\"",
        "Document: \"A logical and algebraic treatment of conditional probability. This paper is devoted to a logical and algebraic treatment of condi- tional probability. The main ideas are the use of non-standard probabili- ties and of some kind of standard part function in order to deal with the case where the conditioning event has probability zero, and the use of a many-valued modal logic in order to deal probability of an event ' as the truth value of the sentence ' is probable, along the lines of Hajek's book (H98) and of (EGH). To this purpose, we introduce a probabilistic many- valued logic, called FP (S L), which is sound and complete with respect a class of structures having a non-standard extension (0, 1)? of (0, 1) as set of truth values. We also prove that the coherence of an assessment of con- ditional probabilities is equivalent to the coherence of a suitably defined theory over FP (S L) whose proper axioms reflect the assessment itself.\"",
        "Document: \"A Note On Relative Efficiency Of Axiom Systems. We introduce a notion of relative efficiency for axiom systems. Given an axiom system A(beta) for a theory T consistent with S2(1), we show that the problem of deciding whether an axiom system A(alpha) for the same theory is more efficient than A(beta) is pi2-hard. Several possibilities of speed-up of proofs are examined in relation to pairs of axiom systems A(alpha), A(beta), with A(alpha) Superset-or-equal-to A(beta), both in the case of A(alpha), A(beta) having the same language, and in the case of the language of A(alpha) extending that of A(beta): in the latter case, letting Pr(alpha), Pr(beta) denote the theories axiomatized by A(alpha), A(beta), respectively, and assuming Pr(alpha) to be a conservative extension of Pr(beta), we show that if A(alpha)-A(beta) contains no nonlogical axioms, then A(alpha) can only be a linear speed-up of A(beta); otherwise, given any recursive function g and any A(beta), there exists a finite extension A(alpha) of A(beta) such that A(alpha) is a speed-up of A(beta) with respect to g.\"",
        "Document: \"On the predicate logics of continuous t-norm BL-algebras. Given a class C of t-norm BL-algebras, one may wonder which is the complexity of the set Taut( C\u2200) of predicate formulas which are valid in any algebra in C. We first characterize the classes C for which Taut( C\u2200) is recursively axiomatizable, and we show that this is the case iff C only consists of the G\u00f6del algebra on [0,1]. We then prove that in all cases except from a finite number Taut( C\u2200) is not even arithmetical. Finally we consider predicate monadic logics Taut M( C\u2200) of classes C of t-norm BL-algebras, and we prove that (possibly with finitely many exceptions) they are undecidable.\"",
        "Document: \"Interpolation and Beth\u2019s property in propositional many-valued logics: A semantic investigation. In this paper we give a rather detailed algebraic investigation of interpolation and Beth\u2019s property in propositional many-valued logics extending H\u00e1jek\u2019s Basic Logic BL [P. H\u00e1jek, Metamathematics of Fuzzy Logic, Kluwer, 1998], and we connect such properties with amalgamation and strong amalgamation in the corresponding varieties of algebras. It turns out that, while the most interesting extensions of BL in the language of BL have deductive interpolation, very few of them have Beth\u2019s property or Craig interpolation. Thus in the last part of the paper we look for conservative extensions of BL having such properties.\"",
        "Document: \"Strongly involutive uninorm algebras. We investigate uninorm algebras satisfying a strong version of involutiveness. More precisely, we require that negation is an order reversing monoid isomorphism between the positive cone and the negative cone. A rather surprising consequence of this property is that the negative cones of these algebras are BL-algebras which do not admit MV-components with more than two elements. Among other things, we prove standard completeness and co-NP completeness of the logic corresponding to these algebras.\"",
        "Document: \"PAC learning of probability distributions over a discrete domain. We investigate learning of classes of distributions over a discrete domain in a PAC context. We introduce two paradigms of PAC learning, namely absolute PAC learning, which is independent of the representation of the class of hypotheses, and PAC learning wrt the indexes, which heavily depends on such representations. We characterize non-computable learnability in both contexts. Then we investigate efficient learning strategies which are simulated by a polynomial-time Turing machine. One strategy is the frequentist one. According to this strategy, the learner conjectures a hypothesis which is as close as possible to the distribution given by the frequency relative to the examples. We characterize the classes of distributions which are absolutely PAC learnable by means of this strategy, and we relate frequentist learning wrt the indexes to the NP=RP problem. Finally, we present another strategy for learning wrt the indexes, namely learning by tests.\"",
        "1 is \"Argumentation and the Dynamics of Warranted Beliefs in Changing Environments\", 2 is \"Strong non-standard completeness for fuzzy logics\"",
        "Given above information, for an author who has written the paper with the title \"On a class of left-continuous t-norms\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006411": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Deep Reinforcement Learning with a Combinatorial Action Space for Predicting and Tracking Popular Discussion Threads.':",
        "Document: \"Learning deep structured semantic models for web search using clickthrough data. Latent semantic models, such as LSA, intend to map a query to its relevant documents at the semantic level where keyword-based matching often fails. In this study we strive to develop a series of new latent semantic models with a deep structure that project queries and documents into a common low-dimensional space where the relevance of a document given a query is readily computed as the distance between them. The proposed deep structured semantic models are discriminatively trained by maximizing the conditional likelihood of the clicked documents given a query using the clickthrough data. To make our models applicable to large-scale Web search applications, we also use a technique called word hashing, which is shown to effectively scale up our semantic models to handle large vocabularies which are common in such tasks. The new models are evaluated on a Web document ranking task using a real-world data set. Results show that our best model significantly outperforms other latent semantic models, which were considered state-of-the-art in the performance prior to the work presented in this paper.\"",
        "Document: \"Reinforced Cross-Modal Matching And Self-Supervised Imitation Learning For Vision-Language Navigation. Vision-language navigation (VLN) is the task of navigating an embodied agent to carry out natural language instructions inside real 3D environments. In this paper, we study how to address three critical challenges for this task: the cross-modal grounding, the ill-posed feedback, and the generalization problems. First, we propose a novel Reinforced Cross-Modal Matching (RCM) approach that enforces cross-modal grounding both locally and globally via reinforcement learning (RL). Particularly, a matching critic is used to provide an intrinsic reward to encourage global matching between instructions and tV rajectories, and a reasoning navigator is employed to perform cross-modal grounding in the local visual scene. Evaluation on a VLN benchmark dataset shows that our RCM model significantly outperforms previous methods by 10% on SPL and achieves the new state-of-the-art performance. To improve the generalizability of the learned policy, we further introduce a Self-Supervised Imitation Learning (SIL) method to explore unseen environments by imitating its own past, good decisions. We demonstrate that SIL can approximate a better and more efficient policy, which tremendously minimizes the success rate performance gap between seen and unseen environments (from 30.7% to 11.7%).\"",
        "Document: \"Exploiting headword dependency and predictive clustering for language modeling. This paper presents several practical ways of incorporating linguistic structure into language models. A headword detector is first applied to detect the headword of each phrase in a sentence. A permuted headword trigram model (PHTM) is then generated from the annotated corpus. Finally, PHTM is extended to a cluster PHTM (C-PHTM) by defining clusters for similar words in the corpus. We evaluated the proposed models on the realistic application of Japanese Kana-Kanji conversion. Experiments show that C-PHTM achieves 15% error rate reduction over the word trigram model. This demonstrates that the use of simple methods such as the headword trigram and predictive clustering can effectively capture long distance word dependency, and substantially outperform a word trigram model.\"",
        "Document: \"A Comparative Study Of Discriminative Methods For Reranking Lvcsr N-Best Hypotheses In Domain Adaption And Generalization. This paper is an empirical study on the performance of different discriminative approaches to reranking the N-best hypotheses output from a large vocabulary continuous speech recognizer (LVCSR). Four algorithms, namely perceptron, boosting, ranking support vector machine (SVM) and minimum sample risk (MSR), are compared in terms of domain adaptation, generalization and time efficiency. In our experiments on Mandarin dictation speech, we found that for domain adaptation, perceptron performs the best; for generalization, boosting performs the best. The best result on a domain-specific test set is achieved by the perceptron algorithm. A relative character error rate (CER) reduction of 11 % over the baseline was obtained. The best result on a general test set is 3.4% CER reduction over the baseline, achieved by the boosting algorithm.\"",
        "Document: \"Improving Encarta Search Engine Performance By Mining User Logs. We propose a data-mining approach that produces generalized query patterns (with generalized keywords) from the raw user logs of the Microsoft Encarta search engine (http://encarta.msn.com). Those query patterns can act as cache of the search engine, improving its performance. The cache of the generalized query patterns is more advantageous than the cache of the most frequent user queries since our patterns are generalized, covering more queries and future queries - even those not previously asked. Our method is unique since query patterns discovered reflect the actual dynamic usage and user feedbacks of the search engine, rather than the syntactic linkage structure of web pages (as Google does). Simulation shows that such generalized query patterns improve search engine's overall speed considerably. The generalized query patterns, when viewed with a graphical user interface, are also helpful to web editors, who can easily discover topics in which users are mostly interested.\"",
        "Document: \"The Use of Clustering Techniques for Language Modeling-Application to Asian Languages. Cluster-based n-gram modeling is a variant of normal word-based n-gram modeling. It attempts to make use of the similarities between words. In this paper, we present an empirical study of clustering techniques for Asian language modeling. Clustering is used to improve the performance (i. e. perplexity) of language models as well as to compress language models. Experimental tests are presented for cluster-based trigram models on a Japanese newspaper corpus, and on a Chinese heterogeneous corpus. While the majority of previous research on word clustering has focused on how to get the best clusters, we have concentrated our research on the best way to use the clusters. Experimental results show that some novel techniques we present work much better than previous methods, and achieve up to more than 40% size reduction at the same perplexity \"",
        "Document: \"Approximation lasso methods for language modeling. Lasso is a regularization method for parameter estimation in linear models. It optimizes the model parameters with respect to a loss function subject to model complexities. This paper explores the use of lasso for statistical language modeling for text input. Owing to the very large number of parameters, directly optimizing the penalized lasso loss function is impossible. Therefore, we investigate two approximation methods, the boosted lasso (BLasso) and the forward stagewise linear regression (FSLR). Both methods, when used with the exponential loss function, bear strong resemblance to the boosting algorithm which has been used as a discriminative training method for language modeling. Evaluations on the task of Japanese text input show that BLasso is able to produce the best approximation to the lasso solution, and leads to a significant improvement, in terms of character error rate, over boosting and the traditional maximum likelihood estimation.\"",
        "Document: \"Deep stacking networks for information retrieval. Deep stacking networks (DSN) are a special type of deep model equipped with parallel and scalable learning. We report successful applications of DSN to an information retrieval (IR) task pertaining to relevance prediction for sponsor search after careful regularization methods are incorporated to the previous DSN methods developed for speech and image classification tasks. The DSN-based system significantly outperforms the LambdaRank-based system which represents a recent state-of-the-art for IR in normalized discounted cumulative gain (NDCG) measures, despite the use of mean square error as DSN's training objective. We demonstrate desirable monotonic correlation between NDCG and classification rate in a wide range of IR quality. The weaker correlation and more flat relationship in the high IR-quality region suggest the need for developing new learning objectives and optimization methods.\"",
        "Document: \"Jointly Optimizing Diversity and Relevance in Neural Response Generation. Although recent neural conversation models have shown great potential, they often generate bland and generic responses. While various approaches have been explored to diversify the output of the conversation model, the improvement often comes at the cost of decreased relevance. In this paper, we propose a method to jointly optimize diversity and relevance that essentially fuses the latent space of a sequence-to-sequence model and that of an autoencoder model by leveraging novel regularization terms. As a result, our approach induces a latent space in which the distance and direction from the predicted response vector roughly match the relevance and diversity, respectively. This property also lends itself well to an intuitive visualization of the latent space. Both automatic and human evaluation results demonstrate that the proposed approach brings significant improvement compared to strong baselines in both diversity and relevance.\"",
        "Document: \"Smoothing clickthrough data for web search ranking. Incorporating features extracted from clickthrough data (called clickthrough features) has been demonstrated to significantly improve the performance of ranking models for Web search applications. Such benefits, however, are severely limited by the data sparseness problem, i.e., many queries and documents have no or very few clicks. The ranker thus cannot rely strongly on clickthrough features for document ranking. This paper presents two smoothing methods to expand clickthrough data: query clustering via Random Walk on click graphs and a discounting method inspired by the Good-Turing estimator. Both methods are evaluated on real-world data in three Web search domains. Experimental results show that the ranking models trained on smoothed clickthrough features consistently outperform those trained on unsmoothed features. This study demonstrates both the importance and the benefits of dealing with the sparseness problem in clickthrough data.\"",
        "1 is \"First steps towards statistical modeling of dialogue to predict the speech act type of the next utterance\", 2 is \"Adversarial Learning of Task-Oriented Neural Dialog Models.\"",
        "Given above information, for an author who has written the paper with the title \"Deep Reinforcement Learning with a Combinatorial Action Space for Predicting and Tracking Popular Discussion Threads.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006422": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Video Content Manipulation by Means of Content Annotation and Nonsymbolic Gestural Interfaces':",
        "Document: \"Paraphrasing Out-of-Vocabulary Words with Word Embeddings and Semantic Lexicons for Low Resource Statistical Machine Translation. Out-of-vocabulary (OOV) word is a crucial problem in statistical machine translation (SMT) with low resources. OOV paraphrasing that augments the translation model for the OOV words by using the translation knowledge of their paraphrases has been proposed to address the OOV problem. In this paper, we propose using word embeddings and semantic lexicons for OOV paraphrasing. Experiments conducted on a low resource setting of the OLYMPICS task of IWSLT 2012 verify the effectiveness of our proposed method.\"",
        "Document: \"Construction of domain dictionary for fundamental vocabulary. For natural language understanding, it is essential to reveal semantic relations between words. To date, only the IS-A relation has been publicly available. Toward deeper natural language understanding, we semi-automatically constructed the domain dictionary that represents the domain relation between Japanese fundamental words. This is the first Japanese domain resource that is fully available. Besides, our method does not require a document collection, which is indispensable for keyword extraction techniques but is hard to obtain. As a task-based evaluation, we performed blog categorization. Also, we developed a technique for estimating the domain of unknown words.\"",
        "Document: \"Neural Network-Based Model For Japanese Predicate Argument Structure Analysis. This paper presents a novel model for Japanese predicate argument structure (PAS) analysis based on a neural network framework. Japanese PAS analysis is challenging due to the tangled characteristics of the Japanese language, such as case disappearance and argument omission. To unravel this problem, we learn selectional preferences from a large raw corpus, and incorporate them into a SOTA PAS analysis model, which considers the consistency of all PASs in a given sentence. We demonstrate that the proposed PAS analysis model significantly outperforms the base SOTA system.\"",
        "Document: \"Discourse structure analysis for news video. Various kinds of video recordings have discourse structures. Therefore, it is important to determine how video segments are combined and what kind of coherence relations they are connected with. In this paper, we propose a method for estimating the discourse structure of video news reports by analyzing the discourse structure of their transcripts.\"",
        "Document: \"Neural Joint Model For Transition-Based Chinese Syntactic Analysis. We present neural network-based joint models for Chinese word segmentation, POS tagging and dependency parsing. Our models are the first neural approaches for fully joint Chinese analysis that is known to prevent the error propagation problem of pipeline models. Although word embeddings play a key role in dependency parsing, they cannot be applied directly to the joint task in the previous work. To address this problem, we propose embeddings of character strings, in addition to words. Experiments show that our models outperform existing systems in Chinese word segmentation and POS tagging, and perform preferable accuracies in dependency parsing. We also explore bi-LSTM models with fewer features.\"",
        "Document: \"Text understanding for conversational agent. This paper describes a text understanding system for conversational agents. The system resolves zero, direct and indirect anaphors in Japanese texts by integrating two sorts of linguistic resources: a hand-annotated corpus with various relations and automatically constructed case frames. The corpus has relevance tags which consist of predicate-argument relations, relations between nouns and coreferences, and is utilised for learning parameters of the system and testing it. The case frames are indispensable knowledge both for detecting zero/indirect anaphors and estimating appropriate antecedents. Our preliminary experiments showed promising results.\"",
        "Document: \"Zero pronoun resolution based on automatically constructed case frames and structural preference of antecedents. This paper describes a method to detect and resolve zero pronouns in Japanese text. We detect zero pronouns by case analysis based on automatically constructed case frames, and select their appropriate antecedents based on similarity to examples in the case frames. We also introduce structural preference of antecedents to precisely capture the tendency that a zero pronoun has its antecedent in its close position. Experimental results on 100 articles indicated that the precision and recall of zero pronoun detection is 87.1% and 74.8% respectively and the accuracy of antecedent estimation is 61.8%.\"",
        "Document: \"Improving coreference resolution using bridging reference resolution and automatically acquired synonyms. We present a knowledge-rich approach to Japanese coreference resolution. In Japanese, proper noun coreference and common noun coreference occupy a central position in coreference relations. To improve coreference resolution for such language, wide-coverage knowledge of synonyms is required. We first acquire knowledge of synonyms from large raw corpus and dictionary definition sentences, and resolve coreference relations based on the knowledge. Furthermore, to boost the performance of coreference resolution, we integrate bridging reference resolution system into coreference resolver.\"",
        "Document: \"Kyoto-U: Syntactical EBMT System for NTCIR-7 Patent Translation Task. This paper describes \"Kyoto-U\" MT system that attended the patent translation task at NTCIR-7. Example-based machine translation is applied in this system to integrate our study on both structural NLP and machine translation. In the alignment step, con- sistency criteria are applied to solve the alignment ambiguities and to discard incorrect alignment candi- dates. In the translation step, translation examples are combined using \"bond\" information, which can han- dle the word ordering without any statistics. Keywords: EBMT, structural NLP, consistency.\"",
        "Document: \"Example-Based machine translation without saying inferable predicate. For natural translations, a human being does not express predicates that are inferable from the context in a target language. This paper proposes a method of machine translation which handles these predicates. First, to investigate how to translate them, we build a corpus in which predicate correspondences are annotated manually. Then, we observe the corpus, and find alignment patterns including these predicates. In our experimental results, the machine translation system using the patterns demonstrated the basic feasibility of our approach.\"",
        "1 is \"Unsupervised morphological segmentation with log-linear models\", 2 is \"Tracking and recognizing rigid and non-rigid facial motions using local parametric models of image motion\"",
        "Given above information, for an author who has written the paper with the title \"Video Content Manipulation by Means of Content Annotation and Nonsymbolic Gestural Interfaces\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006436": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Block Markov Superposition Transmission of Short Codes for Indoor Visible Light Communications':",
        "Document: \"Performance Analysis of Block Markov Superposition Transmission of Short Codes. In this paper, we consider the asymptotic and finite-length performance of block Markov superposition transmission (BMST) of short codes, which can be viewed as a new class of spatially coupled (SC) codes where the generator matrices of short codes (referred to as basic codes) are coupled. A modified extrinsic information transfer (EXIT) chart analysis that takes into account the relation between ...\"",
        "Document: \"Unequal error protection by partial superposition transmission using low-density parity-check codes. In this study, the authors consider designing low-density parity-check (LDPC) coded modulation systems to achieve unequal error protection (UEP). They propose a new UEP approach by partial superposition transmission (PST) called UEP-by-PST. In the UEP-by-PST system, the information sequence is distinguished as two parts, the more important data (MID) and the less important data (LID), both of which are coded with LDPC codes. The codeword that corresponds to the MID is superimposed on the codeword that corresponds to the LID. The system performance can be analysed by using discretised density evolution. Also proposed in this study is a criterion from a practical point of view to compare the efficiencies of different UEP approaches. Numerical results show that, over both additive white Gaussian noise channels and uncorrelated Rayleigh fading channels, (i) UEP-by-PST provides higher coding gain for the MID compared with the traditional equal error protection approach, but with negligible performance loss for the LID; and (ii) UEP-by-PST is more efficient with the proposed practical criterion than the UEP approach in the digital video broadcasting system.\"",
        "Document: \"Joint timing recovery and decoding algorithms for non-binary LDPC coded systems. In this paper, we present two joint timing recovery and decoding algorithms for non-binary low-density parity-check (LDPC) coded systems. To estimate the timing offset, the first algorithm utilizes the percentage of the satisfied check nodes (SCNs), while the second algorithm utilizes the soft decision metrics (SDMs). Both SCNs and SDMs are fed back from the LDPC decoder, which can be implemented by either the well-known q-ary sum-product algorithm (QSPA) or other low complexity algorithms, such as the X-EMS algorithms. Simulation results show that the proposed algorithms suffer from a little performance degradation compared with the perfect timing system. Simulation results also show that X-EMS algorithms aided by the timing recovery algorithm using SDMs outperform QSPA aided by the timing recovery algorithm using SCNs, but with a much lower complexity. This implies that the performance is mainly affected by the timing recovery algorithms.\"",
        "Document: \"Adaptive decoding algorithms for LDPC codes with redundant check nodes. This paper is concerned with decoding of algebraic low-density parity-check (LDPC) codes that are constructed based on finite fields and finite geometries. The parity-check matrices of such codes usually have redundant rows. Equivalently, their Tanner graphs have redundant check nodes. Based on this property, we propose an adaptive decoding algorithm. In the adaptive decoding algorithm, all the check nodes are classified into active nodes and silent nodes according to certain criteria, and only active check nodes are involved in the iterative procession. That is, each variable node collects messages from active check nodes and passes the extrinsic messages to its active neighbors. Two approaches to select the active check nodes are presented. Simulation results show that, the adaptive decoding algorithm can make a trade-off between the complexity and the performance. \u00a9 2012 IEEE.\"",
        "Document: \"New techniques for upper-bounding the MLD performance of binary linear codes. In this paper, two techniques are presented to either simplify or improve most of the existing upper bounds on the maximum-likelihood decoding (MLD) performance of the binary linear codes over additive white Gaussian noise (AWGN) channels. Firstly, the recently proposed union bound using truncated weight spectra by Ma et al is re-derived in a detailed way based on Gallager's first bounding technique (GFBT). Secondly, we propose using triplet-wise error probabilities instead of pair-wise error probabilities to improve the union bound. In doing so, we prove that any three codewords form a non-obtuse triangle, which can be utilized to upper-bound the triplet-wise error probability. The proposed bounds improve the conventional union bounds but have a similar complexity since they involve only the Q-function. The proposed bounds can also be adapted to bit-error probabilities.\"",
        "Document: \"Upper Bounds on the Capacities of Noncontrollable Finite-State Channels With/Without Feedback. Noncontrollable finite-state channels (FSCs) are FSCs in which the channel inputs have no influence on the channel states, i.e., the channel states evolve freely. Since single-letter formulas for the channel capacities are rarely available for general noncontrollable FSCs, computable bounds are usually utilized to numerically bound the capacities. In this paper, we take the delayed channel state as part of the channel input and then define the directed information rate from the new channel input (including the source and the delayed channel state) sequence to the channel output sequence. With this technique, we derive a series of upper bounds on the capacities of noncontrollable FSCs with/without feedback. These upper bounds can be achieved by conditional Markov sources and computed by solving an average reward per stage stochastic control problem (ARSCP) with a compact state space and a compact action space. By showing that the ARSCP has a uniformly continuous reward function, we transform the original ARSCP into a finite-state and finite-action ARSCP that can be solved by a value iteration method. Under a mild assumption, the value iteration algorithm is convergent and delivers a near-optimal stationary policy and a numerical upper bound.\"",
        "Document: \"Accessible Capacity of Secondary Users. A new problem formulation is presented for the Gaussian interference channels\n(GIFC) with two pairs of users, which are distinguished as primary users and\nsecondary users, respectively. The primary users employ a pair of encoder and\ndecoder that were originally designed to satisfy a given error performance\nrequirement under the assumption that no interference exists from other users.\nIn the case when the secondary users attempt to access the same medium, we are\ninterested in the maximum transmission rate (defined as {\\em accessible\ncapacity}) at which secondary users can communicate reliably without affecting\nthe error performance requirement by the primary users under the constraint\nthat the primary encoder (not the decoder) is kept unchanged. By modeling the\nprimary encoder as a generalized trellis code (GTC), we are then able to treat\nthe secondary link as a finite state channel (FSC). The relation of the\naccessible capacity to the capacity region of the GIFC is revealed. Upper and\nlower bounds on the accessible capacity are derived. For some special cases,\nthese bounds can be computed numerically by using the BCJR algorithm. The\nnumerical results show us, as expected, that primary users with lower\ntransmission rates may allow higher accessible rates, and that better primary\nencoders guarantee not only higher quality of the primary link but also higher\naccessible rates of the secondary users. More interestingly, the numerical\nresults show that the accessible capacity does not always increase with the\ntransmission power of the secondary transmitter.\"",
        "Document: \"A Variant of the EMS Decoding Algorithm for Nonbinary LDPC Codes. This letter is concerned with low-complexity decoding algorithms for nonbinary low-density parity-check (NB-LDPC) codes. A new truncation rule based on an adaptive threshold \u03bc is proposed for the extended min-sum (EMS) algorithm. The threshold \u03bc is determined by the mean of the message vector. Therefore, it can be matched to channel observation and decoding iteration. The resulting algorithm is referred to as \u03bc-EMS algorithm. Simulation results show that the \u03bc-EMS algorithm performs almost as well as the Q-ary sum-product algorithm (QSPA). Simulation results also show that the \u03bc-EMS algorithm is simpler than the other X-EMS algorithms for NB-LDPC over high order finite fields. \u00a9 1997-2012 IEEE.\"",
        "Document: \"Superposition coded modulation with peak-power limitation. We apply clipping to superposition coded modulation (SCM) systems to reduce the peak-to-average power ratio (PAPR) of the transmitted signal. The impact on performance is investigated by evaluating the mutual information driven by the induced peak-power-limited input signals. It is shown that the rate loss is marginal for moderate clipping thresholds if optimal encoding/decoding is used. This fact is confirmed in examples where capacity-approaching component codes are used together with the maximum a posteriori probability (MAP) detection. In order to reduce the detection complexity of SCM with a large number of layers, we develop a suboptimal soft compensation (SC) method that is combined with soft-input soft-output (SISO) decoding algorithms in an iterative manner. A variety of simulation results for additive white Gaussian noise (AWGN) and fading channels are presented. It is shown that with the proposed method, the effect of clipping can be efficiently compensated and a good tradeoff between PAPR and bit-error rate (BER) can be achieved. Comparisons with other coded modulation schemes demonstrate that SCM offers significant advantages for high-rate transmissions over fading channels.\"",
        "Document: \"Decoding and Computing Algorithms for Linear Superposition LDPC Coded Systems.   This paper is concerned with linear superposition systems in which all components of the superimposed signal are coded with an identical binary low-density parity-check (LDPC) code. We focus on the design of decoding and computing algorithms. The main contributions of this paper include: 1) we present three types of iterative multistage decoding/computing algorithms, which are referred to as decoding-computing (DC) type, computing-decoding (CD) type and computing-decoding computing (CDC) type, respectively; 2) we propose a joint decoding/computing algorithm by treating the system as a nonbinary LDPC (NB-LDPC) coded system; 3) we propose a time-varying signaling scheme for multi-user communication channels. The proposed algorithms may find applications in superposition modulation (SM), multiple-access channels (MAC), Gaussian interference channels (GIFC) and two-way relay channels (TWRC). For SM system, numerical results show that 1) the proposed CDC type iterative multistage algorithm performs better than the standard DC type iterative multistage algorithm, and 2) the joint decoding/computing algorithm performs better than the proposed iterative multistage algorithms in high spectral efficiency regime. For GIFC, numerical results show that, from moderate to strong interference, the time-varying signaling scheme significantly outperforms the constant signaling scheme when decoded with the joint decoding/computing algorithm (about 8.5 dB for strong interference). For TWRC, numerical results show that the joint decoding/computing algorithm performs better than the CD type algorithm. \"",
        "1 is \"Improved low-density parity-check codes using irregular graphs\", 2 is \"Covert Communication in the Presence of an Uninformed Jammer.\"",
        "Given above information, for an author who has written the paper with the title \"Block Markov Superposition Transmission of Short Codes for Indoor Visible Light Communications\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006446": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'On the Designs of Variable Fractional Hilbert Transformers':",
        "Document: \"Secure Single-Side-Band Signal Generation Using Two Fractional Hilbert Transformers. In this paper, a secure single-side-band (SSB) generation using two fractional Hilbert transformers (FHT) is presented. First, the SSB signal generation methods using conventional Hilbert transformer (HT) and one FHT are reviewed. Then, to improve the security, a more secure SSB generation method using two FHTs is proposed. The phase angles of two FHTs can be used as the secure keys for construction and reconstruction. Finally, a numerical example is illustrated to show the effectiveness of the proposed method.\"",
        "Document: \"Digital Image Sharpening Using Fractional Derivative And Mach Band Effect. In this paper, a new digital image sharpening method is presented by using fractional derivative and Mach band effect. First, fractional order digital differentiator is designed by using the Grunwald-Letnikov derivative. Next, the Mach band effect and fractional order digital differentiator are used to construct an image sharpening algorithm. Finally, the numerical examples are shown to demonstrate the effectiveness of the proposed method.\"",
        "Document: \"Neural networks for routing of communication networks with unreliable components. A new neural network model, Routron, which can handle dependent component failures of communication networks, is proposed. We prove that the proposed Routron has a stable solution. Moreover, useful upper and lower bounds for the design parameters are derived to help select them in implementations. Simulation results are included to illustrate the effectiveness of the algorithm\"",
        "Document: \"Design of linear phase FIR filters using fractional derivative constraints. In this paper, the designs of linear phase FIR filters using fractional derivative constraints are investigated. First, the definition of fractional derivative is reviewed briefly. Then, the linear phase FIR filters are designed by minimizing integral squares error under the constraint that the ideal response and actual response have several same fractional derivatives at the prescribed frequency point. Next, the fractional maximally flat FIR filters are designed by letting the number of fractional derivative constraints be equal to the number of filter coefficients. Finally, numerical examples are demonstrated to show that the proposed method has larger design flexibility than the conventional integer derivative constrained methods.\"",
        "Document: \"Designs of Discrete-Time Generalized Fractional Order Differentiator, Integrator and Hilbert Transformer. In this paper, the design of a generalized fractional order differentiator (FOD) whose magnitude and phase responses can be controlled independently is investigated. First, a relation between conventional FOD and generalized FOD is studied such that the design tools of conventional FOD in the literature can be used to design variable generalized FOD directly. Then, the similar method is applied to design generalized fractional order integrator (FOI). Next, the proposed generalized FOD and FOI are used to generate a secure single side band (SSB) signal for saving the transmission bandwidth. The parameters of variable generalized FOD and FOI can be used as the secure keys for construction and reconstruction. Finally, the relation between fractional Hilbert transformer and generalized FOD is studied and the edge detection application is demonstrated to show the flexibility and effectiveness of the proposed generalized FOD.\"",
        "Document: \"Closed-form design of fixed fractional hubert transformer using discrete sine transform. In this paper, the closed-form design of fixed fractional Hilbert transformer (FHT) using discrete sine transform (DST) is presented. First, the DST-based interpolation method is applied to reconstruct the continuous-time signal from the given discrete-time signal. Then, the filter coefficients of the transfer function of fixed FHT are obtained from the DST reconstruction results by using suitable index mapping. The main feature of the proposed method is that the closed-form design can be obtained without performing any optimization procedure. Finally, several numerical examples are demonstrated to show the effectiveness of the proposed design method.\"",
        "Document: \"Elimination of power line interference and noise in electrocardiogram using constrained eigenfilter. In this paper, an eigenfilter is presented to eliminate 60-Hz power line interference and noise in the electrocardiogram. The filter is designed by maximizing the signal to noise ratio under the constraint that 60-Hz interference is zero-valued at filter output. The optimal filter coefficients are obtained from the eigenvector associated with maximum eigenvalue of a correlation matrix. Several experiments are made to demonstrate the performance of our method\"",
        "Document: \"A Low-Light Color Image Enhancement Method on CIELAB Space. A low-light color image enhancement method on CIELAB uniform color space is presented in this paper. First, the RGB color space of the input image is transformed into the CIELAB color space. Then, the intensity component on CIELAB space is amplified by using the power law transform (PLT). Next, an image fusion method is used to combine several PLT-based enhanced images to obtain the final enhanced image. Finally, a back-lighting image is used to evaluate the performance of the proposed enhancement method and make comparisons with traditional methods.\"",
        "Document: \"Design Of Graph Filter In Ill-Posed Condition Using Tikhonov Regularization. In this paper, the design of graph filter in ill-posed condition using Tikhonov regularization method is presented. First, the design problem of graph filter is described. Then, the Tikhonov regularization method is used to determine the filter coefficients. Because the explicit solution is obtained, it is easily computed and used in the applications. Finally, a numerical example for graph filter design of Markov chain data is illustrated to show the effectiveness of the proposed method.\"",
        "Document: \"Design of digital IIR integrator using B-spline interpolation and Gauss-Legendre integration rule. In this paper, the design of digital IIR integrator is investigated. First, the B-spline interpolation method is described. Then, non-integer delay sample estimation of discrete-time sequence is derived by using B-spline interpolation approach. Next, the Gauss-Legendre integration rule and noninteger delay sample estimation are applied to obtain the transfer function of digital integrator. Finally, some numerical comparisons with conventional digital integrators are made to demonstrate the effectiveness of this new design approach.\"",
        "1 is \"Predictive Coding Based on Efficient Motion Estimation\", 2 is \"Design of fractional order digital FIR differentiators\"",
        "Given above information, for an author who has written the paper with the title \"On the Designs of Variable Fractional Hilbert Transformers\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006450": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Adaptive frame and QP selection for temporally super-resolved full-exposure-time video':",
        "Document: \"Rate-Distortion Optimized Joint Source/Channel Coding of WWAN Multicast Video for a Cooperative Peer-to-Peer Collective. Because of unavoidable wireless packet losses and inapplicability of retransmission-based schemes due to the well-known negative acknowledgment implosion problem, providing high quality video multicast over wireless wide area networks (WWAN) remains difficult. Traditional joint source/channel coding (JSCC) schemes for video multicast target a chosen th-percentile WWAN user. Users with poorer reception than th-percentile user (poor users) suffer substantial channel losses, while users with better reception (rich users) have more channel coding than necessary, resulting in sub-optimal video quality. In this paper, we recast the WWAN JSCC problem in a new setting called cooperative peer-to-peer repair (CPR), where users have both WWAN and wireless local area network (WLAN) interfaces and use the latter to exchange received WWAN packets locally. Given CPR can mitigate some WWAN losses via cooperative peer exchanges, a CPR-aware JSCC scheme can now allocate more bits to source coding to minimize source quantization noise without suffering more packet losses, leading to smaller overall visual distortion. Through CPR, this quality improvement is in fact reaped by all peers in the collective, not just a targeted th-percentile user. To efficiently implement both WWAN forward error correction and WLAN CPR repairs, we propose to use network coding for this dual purpose to reduce decoding complexity and maximize packet recovery at the peers. We show that a CPR-aware JSCC scheme dramatically improves video quality: by up to 8.7 dB in peak signal-to-noise ratio for the entire peer group over JSCC scheme without CPR, and by up to 6.0 dB over a CPR-ignorant JSCC scheme with CPR.\"",
        "Document: \"Energy-Aware Multi-Source Video Streaming. In a multi-source video streaming system, premature draining of low-power nodes can cause sudden failures of peer connections and degrade streaming performance. To solve this problem, we propose an energy-aware scheduling (EAS) scheme to better distribute the streaming load among different peers by jointly considering network conditions and node energy levels. We model the proposed scheme using a rate/energy-distortion optimization framework and heuristically solve it using the concept of asynchronous clocks. Simulation studies show that the proposed EAS scheme can achieve comparable streaming quality while consuming less energy\"",
        "Document: \"Context Tree based Image Contour Coding using A Geometric Prior. Efficient encoding of object contours in images can facilitate advanced image/video compression techniques, such as shape-adaptive transform coding or motion prediction of arbitrarily shaped pixel blocks. We study the problem of lossless and lossy compression of detected contours in images. Specifically, we first convert a detected object contour into a sequence of directional symbols drawn from a...\"",
        "Document: \"Redefining self-similarity in natural images for denoising using graph signal gradient. Image denoising is the most basic inverse imaging problem. As an under-determined problem, appropriate definition of image priors to regularize the problem is crucial. Among recent proposed priors for image denoising are: i) graph Laplacian regularizer where a given pixel patch is assumed to be smooth in the graph-signal domain; and ii) self-similarity prior where image patches are assumed to recur throughout a natural image in non-local spatial regions. In our first contribution, we demonstrate that the graph Laplacian regularizer converges to a continuous time functional counterpart, and careful selection of its features can lead to a discriminant signal prior. In our second contribution, we redefine patch self-similarity in terms of patch gradients and argue that the new definition results in a more accurate estimate of the graph Laplacian matrix, and thus better image denoising performance. Experiments show that our designed algorithm based on graph Laplacian regularizer and gradient-based self-similarity can outperform non-local means (NLM) denoising by up to 1.4 dB in PSNR.\"",
        "Document: \"Joint Source/Channel Coding Of Scalable Video Over Noisy Channels.  We propose an optimal bit allocation strategy for ajoint source/channel video codec over noisy channelwhen the channel state is assumed to be known. Ourapproach is to partition source and channel coding bitsin such a way that the expected distortion is minimized.The particular source coding algorithm we useis rate scalable and is based on 3D subband codingwith multi-rate quantization. We show that using thisstrategy, transmission of video over very noisy channelsstill renders... \"",
        "Document: \"Low-Latency Error Control of H.264 Using SP-Frames and Streaming Agent Over Wireless Networks. A key challenge to low-latency wireless video streaming, where persistent retransmission is impractical, is error control. While SP-frame adaptation of H.264 has potential to mitigate error propagation, streaming server is often either situated too far to react in a timely fashion to client feedbacks, or too computationally constrained to perform the necessarily complex adaptation simultaneously for multiple clients in different sessions. In this paper, we present an innovative error control mechanism using SP-frames of H.264 and performed by a network intermediary for video streaming to a wireless client. Using an intermediary means it is more responsive to client feedbacks due to its close proximity, and it can offload computation complexity from the streaming server. Simulation shows that about 2 dB improvement in PSNR is achievable for video streaming with low latency requirements over traditional schemes using I and P-frames only.\"",
        "Document: \"Unified distributed source coding frames for interactive multiview video streaming. Because of differential coding used in standard video compression algorithms to exploit temporal correlation in adjacent frames for coding gain a frame lost in network will cause error propagation in subsequent frames at the decoder Previously proposed distributed source coding (DSC) frames can be periodically inserted to halt this error propagation by overcoming the uncertainty at encoder of which frames will be correctly received at decoder without resorting to large intra-coded I-frames In the case of interactive multiview video streaming (IMVS) where a user watches one of M available captured views at a time but can periodically select and switch to a neighboring view the encoder must encode multiview video to enable this view-switching interactivity without knowing the exact view trajectories taken by viewers at stream time In this paper we propose a unified DSC frame construction for IMVS so that the encoder can overcome both types of uncertainty in a coding-efficient manner; ie halt error propagation in differentially coded multiview video and facilitate periodic interactive view-switching at the same time Having the additional unified DSC frames we design a multiview frame structure to maximize the expected number of correctly decoded frames at decoder for a given bandwidth constraint We develop a fast algorithm to find locally optimal structure parameters and packetization and packet reordering strategies for transmission Experimental results show that our optimized frame structures using unified DSC frames outperform nai\u0308ve structures using I- and P-frames only by up to 49% in fraction of correctly decoded frames under typical network condition.\"",
        "Document: \"Deterministic Structured Network Coding For Wwan Video Broadcast With Cooperative Peer-To-Peer Repair. Recent research has exploited the multi-homing property (one terminal with multiple network interfaces) of modern devices to improve communication performance in wireless networks. Cooperative Peer-to-peer Repair (CPR) is one example where given simultaneous connections to both a Wireless Wide Area Network (WWAN) and an ad-hoc Wireless Local Area Network (WLAN), peers receiving different subsets of WWAN broadcast packets can exchange received WWAN packets with their ad-hoc WLAN peers for local recovery. In our previous work, we have shown that by using Network Coding (NC) to linearly combine received packets into new CPR packets for local exchanges, packet recovery can be improved. Moreover, by imposing Structure on Network Coding (SNC) when encoding a CPR packet, decoding of at least the important packets becomes possible in the event when insufficient number of CPR packets were received for full recovery.Given SNC is used during CPR, the key decision for each peer is to determine which SNC type to encode a repair packet at each WLAN transmission opportunity. The decision is further complicated by the observation that peers in general receive different numbers of CPR packets from neighbors due to varying amount of WLAN link contentions and interference experienced. In this paper, we propose a novel counter-based deterministic SNC type selection scheme. Using this approach, we show that a simple local optimization procedure, taking advantage of available neighbors' state information, can be easily implemented to further improved CPR performance. Simulation results show that our proposed scheme outperformed our previous randomized SNC type selection scheme by up to 1.87dB.\"",
        "Document: \"Optimizing Landmark Insertions For Interactive Light Field Streaming. Light field imaging enables a user to navigate and observe a static 3D scene from different viewpoints. Downloading the entire data prior to navigation would incur a large startup delay. Instead, previous works propose an interactive light field streaming (ILFS) framework, where a user periodically requests a viewpoint, and in response the server transmits a pre synthesized and encoded viewpoint image. Using I-frame, P-frame and previously proposed merge frame that facilitates view-switches, the challenge is how to design and pre-encode a storage-constrained frame structure to enable efficient view navigation. In this paper, we initialize \"landmarks\" into a structure to improve ILFS performance. A landmark is a designated view with P-frames to/from each neighborhood view, so that any viewpoint image can transition to any other viewpoint image by first visiting a landmark, and then from the landmark to the destination view. This results in a transmission cost of only two P-frames. Using a Lloyd's algorithm variant, we first incrementally insert into a frame structure landmarks one at a time at locally optimal locations. We then employ a greedy algorithm to add / subtract P-frames based on a rate-storage criterion. Experimental results show that our proposed structures have noticeably lower expected transmission cost for the same storage than structures generated by a previous greedy algorithm.\"",
        "Document: \"Gaze-Driven video streaming with saliency-based dual-stream switching. The ability of a person to perceive image details falls precipitously with larger angle away from his visual focus. At any given bitrate, perceived visual quality can be improved by employing region-of-interest (ROI) coding, where higher encoding quality is judiciously applied only to regions close to a viewer's focal point. Straight-forward matching of viewer's focal point with ROI coding using a live encoder, however, is computation-intensive. In this paper, we propose a system that supports ROI coding without the need of a live encoder. The system is based on dynamic switching between two pre-encoded streams of the same content: one at high quality (HQ), and the other at mixed quality (MQ), where quality of a spatial region depends on its pre-computed visual saliency values. Distributed source coding (DSC) frames are periodically inserted to facilitate switching. Using a Hidden Markov Model (HMM) to model a viewer's temporal gaze movement, MQ stream is pre-encoded based on ROI coding to minimize the expected streaming rate, while keeping the probability of a viewer observing low quality (LQ) spatial regions below an application-specific \u03f5. At stream time, the viewer's gaze locations are collected and transmitted to server for intelligent stream switching. In particular, server employs MQ stream only if: i) viewer's tracked gaze location falls inside the high-saliency regions, and ii) the probability that a viewer's gaze point will soon move outside high-saliency regions, computed using tracked gaze data and updated saliency values, is below \u03f5. Experiments showed that video streaming rate can be reduced by up to 44%, and subjective quality is noticeably better than a competing scheme at the same rate where the entire video is encoded using equal quantization.\"",
        "1 is \"Layered coding vs. multiple descriptions for video streaming over multiple paths\", 2 is \"What is the set of images of an object under all possible lighting conditions?\"",
        "Given above information, for an author who has written the paper with the title \"Adaptive frame and QP selection for temporally super-resolved full-exposure-time video\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006599": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Learning to Measure Influence in a Scientific Social Network.':",
        "Document: \"Evaluation of emotional response to non-photorealistic images. Non-photorealistic rendering (NPR) algorithms are used to produce stylized images, and have been evaluated on the aesthetic qualities of the resulting images. NPR-produced images have been used for aesthetic and practical reasons in media intended to produce an emotional reaction in a consumer (e.g., computer games, films, advertisements, and websites); however, it is not understood how the use of these algorithms affects the emotion portrayed in an image. We conducted a study of subjective emotional response to five common NPR approaches, two blurring techniques, and the original image with 42 participants, and found that the NPR algorithms dampened participants' emotional responses in terms of arousal (activation) and valence (pleasure).\"",
        "Document: \"Useful junk?: the effects of visual embellishment on comprehension and memorability of charts. Guidelines for designing information charts (such as bar charts) often state that the presentation should reduce or remove 'chart junk' - visual embellishments that are not essential to understanding the data. In contrast, some popular chart designers wrap the presented data in detailed and elaborate imagery, raising the questions of whether this imagery is really as detrimental to understanding as has been proposed, and whether the visual embellishment may have other benefits. To investigate these issues, we conducted an experiment that compared embellished charts with plain ones, and measured both interpretation accuracy and long-term recall. We found that people's accuracy in describing the embellished charts was no worse than for plain charts, and that their recall after a two-to-three-week gap was significantly better. Although we are cautious about recommending that all charts be produced in this style, our results question some of the premises of the minimalist approach to chart design.\"",
        "Document: \"Age-Based Preferences And Player Experience: A Crowdsourced Cross-Sectional Study. We tend to treat the 18-55 demographic of gamers as a monolithic and homogenous group, even though the older ones witnessed the entire rise of the videogame and the younger ones were born into a world with MMORPGs. We present a cross-sectional study of 2747 crowdsourced players aged 18-55 and conduct linear regressions of age on several measures of play habits, preferences, and play experiences. Our results show a consistent pattern that with increasing age, preferences, play motive, play style, identification as a gamer, and player experience shift away from a focus on performance and towards a focus on completion, choice, and enjoyment. We situate our results in developmental psychology, which suggests that as we age, we exhibit an increased focus on goals that prioritize emotional regulation and relationships and less on the acquisitions of new skills. Our work provides new insights into the large and core demographic of gamers.\"",
        "Document: \"Biofeedback game design: using direct and indirect physiological control to enhance game interaction. Prior work on physiological game interaction has focused on dynamically adapting games using physiological sensors. In this paper, we propose a classification of direct and indirect physiological sensor input to augment traditional game control. To find out which sensors work best for which game mechanics, we conducted a mixed-methods study using different sensor mappings. Our results show participants have a preference for direct physiological control in games. This has two major design implications for physiologically controlled games: (1) Direct physiological sensors should be mapped intuitively to reflect an action in the virtual world; (2) Indirect physiological input is best used as a dramatic device in games to influence features altering the game world.\"",
        "Document: \"Modeling the efficacy of persuasive strategies for different gamer types in serious games for health. Persuasive games for health are designed to alter human behavior or attitude using various Persuasive Technology (PT) strategies. Recent years have witnessed an increasing number of such games, which treat players as a monolithic group by adopting a one-size-fits-all design approach. Studies of gameplay motivation have shown that this is a bad approach because a motivational approach that works for one individual may actually demotivate behavior in others. In an attempt to resolve this weakness, we conducted a large-scale study on 1,108 gamers to examine the persuasiveness of ten PT strategies that are commonly employed in persuasive game design, and the receptiveness of seven gamer personalities (gamer types identified by BrianHex) to the ten PT strategies. We developed models showing the receptiveness of the gamer types to the PT strategies and created persuasive profiles, which are lists of strategies that can be employed to motivate behavior for each gamer type. We then explored the differences between the models and, based on the results, proposed two approaches for data-driven persuasive game design. The first is the one-size-fits-all approach that will motivate a majority of gamers, while not demotivating any player. The second is the personalized approach that will best persuade a particular type of gamer. We also compiled a list of the best and the worst strategies for each gamer type. Finally, to bridge the gap between game design and PT researchers, we map common game mechanics to the persuasive system design strategies.\"",
        "Document: \"Effects of balancing for physical abilities on player performance, experience and self-esteem in exergames. Game balancing can help players with different skill levels play multiplayer games together; however, little is known about how the balancing approach affects performance, experience, and self-esteem'especially when differences in player strength result from given abilities, rather than learned skill. We explore three balancing approaches in a dance game and show that the explicit approach commonly used in commercial games reduces self-esteem and feelings of relatedness in dyads, whereas hidden balancing improves self-esteem and reduces score differential without affecting game outcome. We apply our results in a second study with dyads where one player had a mobility disability and used a wheelchair. By making motion-based games accessible for people with different physical abilities, and by enabling people with mobility disabilities to compete on a par with able-bodied peers, we show how to provide empowering experiences through enjoyable games that have the potential to increase physical activity and self-esteem.\"",
        "Document: \"Is movement better?: comparing sedentary and motion-based game controls for older adults. Providing cognitive and physical stimulation for older adults is critical for their well-being. Video games offer the opportunity of engaging seniors, and research has shown a variety of positive effects of motion-based video games for older adults. However, little is known about the suitability of motion-based game controls for older adults and how their use is affected by age-related changes. In this paper, we present a study evaluating sedentary and motion-based game controls with a focus on differences between younger and older adults. Our results show that older adults can apply motion-based game controls efficiently, and that they enjoy motion-based interaction. We present design implications based on our study, and demonstrate how our findings can be applied both to motion-based game design and to general interaction design for older adults.\"",
        "Document: \"Extracting Heart Rate from Videos of Online Participants. Crowdsourcing experiments online allows for low-cost data gathering with large participant pools; however, collecting data online does not give researchers access to certain metrics. For example, physiological measures such as heart rate (HR) can provide high-resolution data about the physical, emotional, and mental state of the participant. We investigate and characterize the feasibility of gathering HR from videos of online participants engaged in single user and social tasks. We show that room lighting, head motion, and network bandwidth influence measurement quality, but that instructing participants in good practices substantially improves measurement quality. Our work takes a step towards online physiological data collection.\n\n\"",
        "Document: \"Mobile Gamification for Crowdsourcing Data Collection: Leveraging the Freemium Model. Classic ways of gathering data on human behaviour are time-consuming, costly and are subject to limited participant pools. Crowdsourcing offers a reduction in operating costs and access to a diverse and large participant pool; however issues arise concerning low worker pay and questions about data quality. Gamification provides a motivation to participate, but also requires the development of specialized, research-question specific games that can be costly to produce. Our solution combines gamification and crowdsourcing in a smartphone-based system that emulates the popular Freemium model of play to motivate voluntary participation through in-game rewards, using a robust framework to study multiple unrelated research questions within the same system. We deployed our game on the Android store and compared it to a gamified laboratory version and a non-gamified laboratory version, and found that players who used the in-game rewards were motivated to do experimental tasks. There was no difference between the systems for performance on a motor task; however, performance on the cognitive task was worse for the crowdsourced game. We discuss options for improving performance on tasks requiring attention.\"",
        "Document: \"Does Helping Hurt?: Aiming Assistance and Skill Development in a First-Person Shooter Game. In multiplayer First-Person Shooter (FPS) games, experience can suffer if players have different skill levels -- novices can become frustrated, and experts can become bored. An effective solution to this problem is aiming-assistance-based player balancing, which gives weaker players assistance to bring them up to the level of stronger players. However, it is unknown how assistance affects skill development. The guidance hypothesis suggests that players will become overly reliant on the assistance and will not learn aiming skills as well as they would without it. In order to determine whether aiming assistance hinders FPS skill development, we carried out a study that compared performance gains and experiential measures for an assisted group and an unassisted group, over 14 game sessions over five days. Our results show that although aim assistance did significantly improve performance and perceived competence when it was present, there were no significant differences in performance gains or experiential changes between the assisted and unassisted groups (and on one measure, assisted players improved significantly more). These results go against the prediction of the guidance hypothesis, and suggest instead that the value of aiming assistance outweighs concerns about skill development -- removing one of the remaining barriers that designers may see in using player balancing techniques.\"",
        "1 is \"PROUST: Knowledge-Based Program Understanding\", 2 is \"Using frustration in the design of adaptive videogames\"",
        "Given above information, for an author who has written the paper with the title \"Learning to Measure Influence in a Scientific Social Network.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006606": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'An Integrated Framework For Energy Optimization Of Embedded Real-Time Applications':",
        "Document: \"Variation-aware Flip-Flop energy optimization for ultra low voltage operation. This paper presents an energy optimization method for a Flip-Flop (FF) circuit in a presence of manufacturing process variation. The optimal FF circuit can be obtained by simultaneously scaling the supply voltage and the transistor size with achieving a specific high yield of the circuit. Lowering the supply voltage is one of the most effective approaches for decreasing the energy consumption of the circuit. However, the increased variation in nano scale semiconductor devices causes a malfunction of FFs especially for the very low voltage operation. Therefore, it is a challenging goal for the nano scale FFs to achieve the high yield and extremely low energy consumption simultaneously. This paper proposes an approximation method for accurately estimating a minimum possible operating voltage (VDDmin) of FFs with a small number of Monte-Carlo trials. After that, for a given FF, we find a set of optimal supply voltage and the transistor sizes, which minimizes the energy consumption of the FF with achieving the specific high-sigma yield (e.g., 5\u03c3 yield). Post layout Monte-Carlo simulation results obtained using a commercial 28 nm process technology model demonstrate that the energy consumption of a FF optimized with our approach can be reduced by 17% at the best case with achieving 5\u03c3 yield.\"",
        "Document: \"Energy reduction by built-in body biasing with single supply voltage operation. Energy-efficiency has become the driving force of today's LSI industry. In order to achieve minimum energy operation of LSI, we propose a built-in body biasing technique which generates independent body biases for nMOSFET and pMOSFET separately. We design and fabricate an application circuit integrated with our proposed built-in body bias generation (BBG) circuits in a 65-nm process. The application circuit consists of AES cipher and decipher modules. The BBG does not require an external supply and it is compatible with a dynamic voltage scaling scheme for the application circuit. Cell-based design of the BBG circuit has been applied to facilitate automatic place and route. Both of the AES and the BBG circuits have been routed simultaneously to reduce design and area overhead. In post-silicon, supply voltage and body bias voltages are selected to achieve the minimum energy consumption for a target frequency. From the measurement results, more than 20% of energy reduction is achieved compared with adjusting supply voltage alone.\"",
        "Document: \"Row/column redundancy to reduce SRAM leakage in presence of random within-die delay variation. Traditionally, spare rows/columns have been used in two ways: either to replace too leaky cells to reduce leakage, or to substitute faulty cells to improve yield. In contrast, we first choose a higher threshold voltage (Vth) and/or gate-oxide thickness (Tox) for SRAM transistors at design time to reduce leakage, and then substitute the resulting too slow cells by spare rows/columns. We show that due to within-die delay variation of SRAM cells only a few cells violate target timing at higher Vth or Tox; we carefully choose the Vth and Tox values such that the original memory timing-yield remains intact for a negligible extra delay. On a commercial 90 nm process assuming 3% variation in SRAM cell delay, we obtained 47% leakage reduction by adding only 5 redundant columns at negligible area, dynamic power and delay costs.\"",
        "Document: \"An Architectural Level Energy Reduction Technique For Deep-Submicron Cache Memories. An architectural level technique for a high performance and low energy cache memory is proposed in this paper. The key idea of our approach is to divide a cache memory into several number of cache blocks and to activate a few parts of the cache blocks. The threshold voltage of each cache block is dynamically changed according to the utilization of each block. Frequently accessed cache blocks are woken up and others are put to sleep by controlling the threshold voltage. Since time overhead to change the threshold voltage can not be neglected, predicting a cache block which will be accessed in the next cycle is important. A history based prediction technique to predict cache blocks which should be woken up is also proposed. Experimental results demonstrated that the leakage energy dissipation in cache memories optimized by this approach can be less than 5% of energy dissipation in a cache memory which does not employ the approach\"",
        "Document: \"An integrated optimization framework for reducing the energy consumption of embedded real-time applications. This paper presents a framework for the purpose of energy optimization of embedded real-time systems. We implemented the presented framework as an optimization toolchain and an energy-aware real-time operating system. Our framework is synthetic, that is, multiple techniques optimize the target application together. The main idea of our approach is to utilize a trade-off between energy and performance of the processor configuration. The optimal processor configuration is selected at each appropriate point in the task. Additionally, an optimization technique about the memory allocation is employed in our framework. Our framework is also gradual, that is, the target application is optimized in a step-by-step manner. The characteristic and the behavior of target applications are analyzed and optimized for both intra-task and inter-task levels by our toolchain at the static time. Based on the results of static time optimization, the runtime energy optimization is performed by a real-time operating system according to the behavior of the application. A case study shows that energy minimization is achieved on average while keeping the real-time performance.\n\n\"",
        "Document: \"Voltage scheduling problem for dynamically variable voltage processors. This paper presents a model of dynamically variable voltage processor and basic theorems for power-delay optimization. A static voltage scheduling problem is also proposed and formulated as an integer linear programming (ILP) problem. In the problem, we assume that a core processor can vary its supply voltage dynamically, but can use only a single voltage level at a time. For a given application program and a dynamically variable voltage processor, a voltage scheduling which minimizes energy consumption under an execution time constraint can be found.\"",
        "Document: \"A non-uniform cache architecture for low power system design. This paper proposes a non-uniform cache architecture for reducing the power consumption of memory systems. The non-uniform cache allows having different associativity values (i.e., the number of cache-ways) for different cache-sets. An algorithm determines the optimum number of cache-ways for each cache-set and generates object code suitable for the non-uniform cache memory. The paper also proposes a compiler technique for reducing redundant cache-way accesses and cache-tag accesses. Experiments demonstrate that the technique can reduce the power consumption of memory systems by up to 76% compared to the best result achieved by the conventional method.\"",
        "Document: \"Energy-efficient embedded system design at 90nm and below: a system-level perspective. Energy consumption is a fundamental barrier in taking full advantage of today and future semiconductor manufacturing technologies. This paper presents our recent research activities and results on estimating and reducing energy consumption in nanometer technology system LSIs. This includes techniques and tools for (i) estimating instantaneous energy consumption of embedded processors during an application execution, and (ii) reducing leakage energy in instruction cache memories by taking advantage of value-dependence of SRAM leakage due to within-die Vth variation.\"",
        "Document: \"DLIC: Decoded loop instructions caching for energy-aware embedded processors. With the explosive proliferation of embedded systems, especially through countless portable devices and wireless equipment used, embedded systems have become indispensable to the modern society and people's life. Those devices are often battery driven. Therefore, low energy consumption in embedded processors is important and becomes critical in step with the system complexity. The on-chip instruction cache (I-cache) is usually the most energy-consuming component on the processor chip due to its large size and frequent access operations. To reduce such energy consumption, the existing loop cache approaches use a tiny decoded cache to filter the I-cache access and instruction decode activity for repeated loop iterations. However, such designs are effective for small and simple loops, and only suitable for DSP kernel-like applications. They are not effectual for many embedded applications where complex loops are common. In this article, we propose a decoded loop instruction cache (DLIC) that is small, hence energy efficient, yet can capture most loops, including large nested ones with branch executions, so that a significant amount of I-cache accesses and instruction decoding can be eradicated. The experiments on a set of embedded benchmarks show that our proposed DLIC scheme can reduce energy consumption by up to 87&percnt; as compared to normal cache-only design. On average, 66&percnt; energy can be saved on instruction fetching and decoding, while at a performance overhead of only 1.4&percnt;.\"",
        "Document: \"SRAM leakage reduction by row/column redundancy under random within-die delay variation. Share of leakage in total power consumption of static RAM (SRAM) memories is increasing with technology scaling. Reverse body biasing increases threshold voltage (Vth), which exponentially reduces subthreshold leakage, but it increases SRAM access delay. Traditionally, when all cells of an SRAM block used to have almost the same delay, within-die variations are increasingly widening the delay distribution of cells even within a single SRAM block, and hence, most of these cells are substantially faster than the delay set for the entire block. Consequently, after the reverse body biasing and the resulting delay rise, only a small number of cells violate the original delay of the SRAM block; we propose to replace them with sufficient number of spare rows/columns of SRAM. Our experiments show that the leakage can be reduced by up to 40% in a 90-nm predictive technology by adding less than ten spare columns to an 8-kB SRAM array for a negligible penalty in delay, dynamic power, and area in the presence of 3% uncorrelated random delay variation.\"",
        "1 is \"Speeding up power estimation of embedded software\", 2 is \"On the quantification of sustainability and extensibility of FlexRay schedules\"",
        "Given above information, for an author who has written the paper with the title \"An Integrated Framework For Energy Optimization Of Embedded Real-Time Applications\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006674": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A Meta-Objective Approach for Many-Objective Evolutionary Optimization.':",
        "Document: \"A return-cost-based binary firefly algorithm for feature selection. Various real-world applications can be formulated as feature selection problems, which have been known to be NP-hard. In this paper, we propose an effective feature selection method based on firefly algorithm (FFA), called return-cost-based binary FFA (Rc-BBFA). The proposed method has the capability of preventing premature convergence and is particularly efficient attributed to the following three aspects. An indicator based on the return-cost is first defined to measure a firefly\u2019s attractiveness from other fireflies. Then, a Pareto dominance-based strategy is presented to seek the attractive one for each firefly. Finally, a binary movement operator based on the return-cost attractiveness and the adaptive jump is developed to update the position of a firefly. The experimental results on a series of public datasets show that the proposed method is competitive in comparison with other feature selection algorithms, including the traditional algorithms, the GA-based algorithm, the PSO-based algorithm, and the FFA-based algorithms.\"",
        "Document: \"Interactive genetic algorithm assisted with collective intelligence from group decision making. Interactive genetic algorithms (IGAs) have been successfully applied to optimize problems with aesthetic criteria by embedding the intelligent evaluations of a user into the evolutionary process. User fatigue caused by frequent interactions, however, often greatly impairs the potentials of IGAs on solving complicated optimization problems. Taking the benefits of collective intelligence into account, we here present an IGA with collective intelligence which is derived from a mechanism of group decision making. An IGA with interval individual fitness is focused here and it can be separately conducted by multiple users at the same time. The collective intelligence of all participated users, represented with social and individual knowledge, is first collected by using a modified group decision making method. Then the strategy of applying the collective intelligence to initialize and guide the single evolution of the IGA is given. With such a multi-user promoted IGA framework, the performance of a single IGA is expected to be evidently improved. In a local network environment, the algorithm is applied to a fashion design system and the results empirically demonstrate that the algorithm can not only alleviate user fatigue but also increase the opportunities of IGAs on finding most satisfactory solutions.\"",
        "Document: \"Sets evolution-based particle swarm optimization for many-objective problems. Optimization problems with more than three objectives, i.e., many-objective problems (MaOPs), have gained more and more attentions in the field of evolutionary multi-objective optimization (EMO) in that the powerful Pareto comparisons and evolutionary strategies are very scarce. Particle swarm optimization (PSO) is an effective method for multi-objective problems, however, it has not been well scaled for solving those MaOPs. In this paper, a set evolution guided PSO for MaOPs (S-MOPSO for short) is presented by regarding the sets of solutions together with the solutions themselves as \u201cparticles\u201d in different evolutionary processes. The framework of the algorithm is first presented, and then the optimization objectives of the MaOPs are converted according to the commonly used metrics of EMOs to provide the search space of sets. Accordingly, the method for evolving the sets of solutions in the PSO framework is given along with finely selecting the global and local best sets particles. The merits of the proposed algorithm are experimentally demonstrated by applying it to scalable benchmark many-objective functions.\"",
        "Document: \"Robot path planning in an environment with many terrains based on interval multi-objective PSO. In order to solve the problem of path planning in an environment with many terrains, we propose a method based on interval multi-objective Particle Swarm Optimization (PSO). First, the environment is modeled by the line partition method, and then, according to the distribution of the polygonal lines which form the robot path and taking the velocity's disturbance into consideration, robot's passing time is formulated as an interval by combining Local Optimal Criterion (LOC), and the path's danger degree is estimated through the area ratio between the robot path and the danger source. In addition, the path length is also calculated as an optimization objective. As a result, the robot path planning problem is modeled as an optimization problem with three objectives. Finally, the interval multiobjective PSO is employed to solve the problem above. Simulation and experimental results verify the effectiveness of the proposed method.\"",
        "Document: \"A niching PSO-based multi-robot cooperation method for localizing odor sources. Aiming at the problem of multiple odor sources localization, a multi-robot cooperation method based on niching particle swarm optimization is presented in this study. In this method, a robot is regarded as a particle, particles located at a neighbor form a niche, and different niches are employed to localize different odor sources synchronously. In order to localize more odor sources, the size of each niche is dynamically adjusted based on the aggregation degree of its elements. A niche merging strategy, based on the similarity of optimal particles found by niches, is proposed to prevent particles repeatedly searching for the same region. In addition, some real conditions such as the sampling/recovery time of a sensor and the velocity limit of a robot are considered when updating the position of a particle. Finally, the proposed method is applied to various scenarios of localizing multiple odor sources, and the experimental results confirm its effectiveness.\"",
        "Document: \"A Multi-Modal Multi-Objective Evolutionary Algorithm Using Two-Archive and Recombination Strategies. There have been few researches on solving multi-modal multi-objective optimization problems, whereas they are commonly seen in real-world applications but difficult for the existing evolutionary optimizers. In this study, we propose a novel multi-modal multi-objective evolutionary algorithm using two-archive and recombination strategies. In the proposed algorithm, the properties of decision variables and the relationships among them are analyzed at first to guide the evolutionary search. Then, a general framework using two archives, i.e., the convergence and the diversity archives, is adopted to cooperatively solve these problems. Moreover, the diversity archive simultaneously employs a clustering strategy to guarantee diversity in the objective space and a niche-based clearing strategy to promote the same in the decision space. At the end of evolution process, solutions in the convergence and the diversity archives are recombined to obtain a large number of multiple Pareto optimal solutions. In addition, a set of benchmark test functions and a performance metric are designed for multi-modal multi-objective optimization. The proposed algorithm is empirically compared with two state-of-the-art evolutionary algorithms on these test functions. The comparative results demonstrate that the overall performance of the proposed algorithm is significantly superior to the competing algorithms.\"",
        "Document: \"Evolutionary generation of test data for many paths coverage based on grouping. Abstract: Path-oriented test data generation is an important issue of software testing, but the efficiency of existing methods needs to be further improved. We focus on the problem of generating test data for many paths coverage, and present a method of evolutionary generation of test data for many paths coverage based on grouping. First, target paths are divided into several groups according to their similarities, and each group forms a sub-optimization problem, which transforms a complicated optimization problem into several simpler sub-optimization problems; then a domain-based fitness is designed when genetic algorithms are employed to solve these problems; finally, these sub-optimization problems are simplified along with the process of generating test data, hence improving the efficiency of generating test data. Having analyzed the performance of our method theoretically, we apply it in some typical programs under test, and compare it with some previous methods. The experimental results show that our method has advantage in the number of evaluations and uncovered target paths.\"",
        "Document: \"A Meta-Objective Approach for Many-Objective Evolutionary Optimization. Pareto-based multi-objective evolutionary algorithms experience grand challenges in solving many-objective optimization problems due to their inability to maintain both convergence and diversity in a high-dimensional objective space. Exiting approaches usually modify the selection criteria to overcome this issue. Different from them, we propose a novel meta-objective (MeO) approach that transforms...\"",
        "Document: \"Many-objective evolutionary optimization based on reference points. Graphical abstractThis figure illustrates the flowchart of the proposed reference points-based evolutionary algorithm (RPEA). Its basic procedure is similar to most generational multi-objective evolutionary algorithms. First, an initial population is formed by randomly generating individuals. Then, genetic operators are performed to obtain an offspring population. Next, a set of reference points is generated based on the combined population. Finally, superior solutions are selected according to the reference points to update the parent population. It can be seen that there are two key operators in RPEA: generation of reference points and selection of individuals. In this study, reference points with good performances in convergence and distribution are generated by making full use of information provided by the current population. In addition, superior individuals are selected based on the evaluation of each individual by calculating the distances between the reference points and the individual in the objective space. Display Omitted HighlightsA reference points-based EA for many-objective optimization is proposed.An approach for adaptively generating reference points is proposed.A method of selecting superior individuals based on reference points is proposed.The proposed algorithm performs well on problems with irregular Pareto fronts. Many-objective optimization problems are common in real-world applications, few evolutionary optimization methods, however, are suitable for solving them up to date due to their difficulties. A reference points-based evolutionary algorithm (RPEA) was proposed in this paper to solve many-objective optimization problems. The aim of this study is to exploit the potential of the reference points-based approach to strengthen the selection pressure towards the Pareto front while maintaining an extensive and uniform distribution among solutions. In RPEA, a series of reference points with good performances in convergence and distribution are continuously generated according to the current population to guide the evolution. Furthermore, superior individuals are selected based on the evaluation of each individual by calculating the distances between the reference points and the individual in the objective space. The proposed algorithm was applied to seven benchmark optimization problems and compared with \u017a-MOEA, HypE, MOEA/D and NSGA-III. The results empirically show that the proposed algorithm has a good adaptability to problems with irregular or degenerate Pareto fronts, whereas the other reference points-based algorithms do not. Moreover, it outperforms the other four in 8 out of 21 test instances, demonstrating that it has an advantage in obtaining a Pareto optimal set with good performances.\"",
        "Document: \"Mutant reduction based on dominance relation for weak mutation testing. Context: As a fault-based testing technique, mutation testing is effective at evaluating the quality of existing test suites. However, a large number of mutants result in the high computational cost in mutation testing. As a result, mutant reduction is of great importance to improve the efficiency of mutation testing.Objective: We aim to reduce mutants for weak mutation testing based on the dominance relation between mutant branches.Method: In our method, a new program is formed by inserting mutant branches into the original program. By analyzing the dominance relation between mutant branches in the new program, the non-dominated one is obtained, and the mutant corresponding to the non-dominated mutant branch is the mutant after reduction.Results: The proposed method is applied to test ten benchmark programs and six classes from open-source projects. The experimental results show that our method reduces over 80% mutants on average, which greatly improves the efficiency of mutation testing.Conclusion: We conclude that dominance relation between mutant branches is very important and useful in reducing mutants for mutation testing.\"",
        "1 is \"Improvement Of Presenting Interface By Predicting The Evaluation Order To Reduce The Burden Of Human Interactive Ec Operators\", 2 is \"Self-adaptive differential evolution with local search chains for real-parameter single-objective optimization\"",
        "Given above information, for an author who has written the paper with the title \"A Meta-Objective Approach for Many-Objective Evolutionary Optimization.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006693": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Formal Methods and Automated Tool for Timing-Channel Identification in TCB Source Code':",
        "Document: \"An Energy-Efficient Dynamic Key Management Scheme in Wireless Sensor Networks. In this paper, we present an energy-efficient dynamic key management scheme in which new sensor nodes can join a sensor network securely and compromised nodes can be isolated from the network in time. Unlike in centralized and location-based key management schemes, our scheme doesn't depend on such infrastructure as base stations and robots, thus it possesses a high level of flexibility. By using a pseudo-random function and the elliptic curve digital signature algorithm in our scheme, energy consumption can be reduced significantly in key establishment and maintenance phases. Analysis shows that our scheme has a very low overhead in terms of computation, communication, and storage.\"",
        "Document: \"A Hexagon-Based Key Predistribution Scheme in Sensor Networks. Web services-based event notification is an emerging technology that combines the asynchronous communication feature of event notification mechanisms and the interoperability feature of Web services technologies. Web services-based event notification ...\"",
        "Document: \"Statistical Modeling and Correlation Analysis of End-to-End Delay in Wide Area Networks. End-to-end delay is a very important network performance parameter. Consequently, research on end-to-end delay has received a great deal of interest. In this paper, we model end-to-end delay using statistical methods. In particular, we use three statistical distribution models to study end-to-end delay, i.e., Pareto distribution, normal distribution and lognormal distribution. Our analysis results show that Pareto distribution is most appropriate to model end-to-end delay while normal or normal-related distribution is not. We also study delay correlation in different time ranges to find out if delay in the past can provide any indication on delay in the future. Our study shows that there is indeed a good correlation when the time range is about 1.25 seconds.\"",
        "Document: \"A New Decision-Making Approach for C2C Electronic Trade. In this paper, we propose a new decision-making approach for C2C electronic trade based on economic theory and relevant trust mechanisms in which we incorporate the element of trust and risk into a single model. During buying, an agent learns to select its partners based on the evaluation of trust on potential partners as well as on its own personal risk attitude. During selling, in order to increase the chance of winning a deal, an agent learns to attract customers by adjusting its profit rate, which is related to both risk and trust. The unique feature of our approach is that it provides a flexible decision-making model for trading agents. The approach aims at helping traders seek more profits and, at the same time, preventing themselves from frauds and deceptions during the trade. It is also a good starting point for exploring tradeoffs among risk, reputation and utility.\"",
        "Document: \"A Power Peer-Based Reputation Scheme for Mobile P2P Systems. Lack of trust between peers is one of the most serious drawbacks that obstruct the wide adoption of mobile p2p systems. Since current reputation schemes developed mainly for the fixed wired networks are not fit in such a scenario, new trust solutions are eagerly in demand. This paper presents a power peer-based reputation scheme, where peers are classified into two groups, power peers and mobile peers and a power peer has zero or more mobile peers. We design two ways of selecting power peers, greedy method and maximal independent set method. The proposed scheme establishes a trusted mobile environment for mobile p2p systems. It effectively avoids the communication overhead in global trust computation because each power peer maintains the appropriate reputation information of its mobile peers. The simulation results show that our scheme is highly robust and scalable in the dynamic environment of mobile networks.\"",
        "Document: \"An access authentication protocol for trusted handoff in wireless mesh networks. WMNs (Wireless Mesh Networks) are a new wireless broadband network structure based completely on IP technologies and have rapidly become a broadband access measure to offer high capacity, high speed and wide coverage. Trusted handoff in WMNs requires that mobile nodes complete access authentication not only with a short delay, but also with the security protection for the mobile nodes as well as the handoff network. In this paper, we propose a trusted handoff protocol based on several technologies, such as hierarchical network model, ECC (Elliptic Curve Cryptography), trust evaluation and gray relevance analysis. In the protocol, the mobile platform's configuration must be measured before access to the handoff network can proceed and only those platforms whose configuration meets the security requirements can be allowed to access the network. We also verify the security properties through formal analysis based on an enhanced Strand model and evaluate the performance of the proposed protocol through simulation to show that our protocol is more advantageous than the EMSA (Efficient Mesh Security Association) authentication scheme in terms of success rate and average delay.\"",
        "Document: \"Active misconfiguration detection in Ethernet networks based on analysis of end-to-end anomalies. The complexity of data networks is constantly increasing, as does the occurrence of faults and misconfigurations. Consequently, network operators are increasingly concerned with the negative effects that misconfigurations have on QoS (Quality of Service) and the difficulty of effective troubleshooting. This paper addresses the detection of Duplex ity Mismatch (DM) of media access devices in Ethernet networks. From the broad spectrum of sources we surveyed, including logging data from real networks, it appears that DM is surprisingly common and very severe. We show how DM introduces degenerative traffic anomalies capable of drastically reducing flow throughput. We, then, propose a novel detection algorithm based on end-to-end active probing. Our investigation is complemented by the implementation of a SW prototype. Extensive experimental evaluation is conducted in a real-world production LAN. The achieved results are encouraging and show that our prototype can be a very useful tool. Our evaluation attained a high success rate of misconfiguration detection: 99.72%. False positive and false negative rates are extremely contained: 0.00% and 0.28%, respectively. Therefore, our prototype appears to be a robust and reliable detection instrument from which network administrators and field engineers can benefit. \u00a9 2004 IEEE.\"",
        "Document: \"Formal Methods and Automated Tool for Timing-Channel Identification in TCB Source Code. We characterize the properties of timing channels that are reflected in source code and present formal methods for the identification of these channels in source code of trusted computing bases (TCBs). Our study differs significantly from previous ones which focus on a high-level characterization of timing channels without leading to practical methods for their identification [11,\"",
        "Document: \"Key predistribution in sensor networks. Sensor networks are widely used in a variety of commercial and military applications due to their self-organization characteristics and distributed nature. As a fundamental requirement for providing security functionality in sensor networks, key predistribution plays a central role in authentication and encryption. In this paper, we describe the hexagon-based key predistribution scheme and show how it can improve the performance of key predistribution in sensor network through the use of bivariate polynomials in a hexagonal coordinate system based on the deployment information about the expected locations of the sensor nodes. More specifically, we show that the hexagon-based key scheme can improve the probability of establishing pairwise keys between sensor nodes by more than 40% over previous schemes.\"",
        "Document: \"On The Security Of Data Collection And Transmission From Wireless Sensor Networks In The Context Of Internet Of Things. In the context of Internet of Things (IoT), multiple cooperative nodes in wireless sensor networks (WSNs) can be used to monitor an event, jointly generate a report and then send it to one or more Internet nodes for further processing. A primary security requirement in such applications is that every event data report be authenticated to intended Internet users and effectively filtered on its way to the Internet users to realize the security of data collection and transmission from the WSN. However, most present schemes developed for WSNs don't consider the Internet scenario while traditional mechanisms developed for the Internet are not suitable due to the resource constraint of sensor nodes. In this paper, we propose a scheme, which we refer to as Data Authentication and En-route Filtering (DAEF), for WSNs in the context of IoT. In DAEF, signature shares are generated and distributed based on verifiable secret sharing cryptography and an efficient ID-based signature algorithm. Our security analysis shows that DAEF can defend against node compromise attacks as well as denial of service (DoS) attacks in the form of report disruption and selective forwarding. We also analyze energy consumption to show the advantages of DAEF over some comparable schemes.\"",
        "1 is \"Automated reassembly of file fragmented images using greedy algorithms\", 2 is \"Modeling Botnet Propagation Using Time Zones\"",
        "Given above information, for an author who has written the paper with the title \"Formal Methods and Automated Tool for Timing-Channel Identification in TCB Source Code\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006706": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Analysis and management of heterogeneous user mobility in large-scale downlink systems':",
        "Document: \"Waveform Design For Massive Miso Downlink With Energy-Efficient Receivers Adopting 1-Bit Adcs. In high-density low-bitrate Internet-of-Things (IoT) use case of 5G networks, the terminals and sensors are to be of extremely low-cost and low energy-consuming. Typically, the analog-to-digital converters (ADCs) dominate the power-budget of receiver chains, in particular if the quantization resolution is high. Hence, receiver architectures deploying 1-bit ADCs are of high interest towards realizing low-cost, high energy-efficiency device solutions. In this paper, we study the waveform design and optimization for a narrowband low-bitrate massive MISO downlink targeting to achieve rates higher than 1 bits/sec (per real-dimension) where the terminal receivers adopt only simple 1-bit quantization (per real-dimension) with oversampling. In this respect, first we show that for a particular precoder structure, the overall link is equivalent to that of an AWGN SISO with controlled intersymbol interference (ISI). The filter design problem for generating the desired ISI in such SISO links has been studied in previous works, however, the only known method in literature is a computationally demanding brute force search method. As a novel contribution, we develop models and tools that elaborate on the conditions to be satisfied for unique detection and existence of solution for the filter coefficients. Then, as a concrete example, the developed models and tools are utilized to show that in the absence of noise, five-times oversampling is required for unique detection of 16-QAM input alphabet. Building on these findings, we then develop novel algorithms that can efficiently design the filter coefficients. Examples and simulations are provided to elaborate on filter coefficient design and optimization, and to illustrate good SER performance of the MISO link with 1-bit receiver even at SNRs down to 5 dB.\"",
        "Document: \"Optimizing multi-cell massive MIMO for spectral efficiency: How Many users should be scheduled?. Massive MIMO is a promising technique to increase the spectral efficiency of cellular networks, by deploying antenna arrays with hundreds or thousands of active elements at the base stations and performing coherent beamforming. A common rule-of-thumb is that these systems should have an order of magnitude more antennas, N, than scheduled users, K, because the users' channels are then likely to be quasi-orthogonal. However, it has not been proved that this rule-of-thumb actually maximizes the spectral efficiency. In this paper, we analyze how the optimal number of scheduled users, K*, depends on N and other system parameters. The value of K* in the large-N regime is derived in closed form, while simulations are used to show what happens at finite N, in different interference scenarios, and for different beamforming.\"",
        "Document: \"Network Deployment for Maximal Energy Efficiency in Uplink with Multislope Path Loss. This work aims to design the uplink (UL) of a cellular network for maximal energy efficiency (EE). Each base station (BS) is randomly deployed within a given area and is equipped with M antennas to serve K user equipments (UEs). A multislope (distance-dependent) path loss model is considered and linear processing is used, under the assumption that channel state information is acquired by using pil...\"",
        "Document: \"Joint Precoding and Load Balancing Optimization for Energy-Efficient Heterogeneous Networks. This paper considers a downlink heterogeneous network, where different types of multi-antenna base stations (BSs) communicate with a number of single-antenna users. Multiple BSs can serve the users by spatial multiflow transmission techniques. Assuming imperfect channel state information at both BSs and users, the precoding, load balancing, and BS operation mode are jointly optimized for improving the network energy efficiency. We minimize the weighted total power consumption while satisfying quality of service constraints at the users. This problem is non-convex, but we prove that for each BS mode combination, the considered problem has a hidden convexity structure. Thus, the optimal solution is obtained by an exhaustive search over all possible BS mode combinations. Furthermore, by iterative convex approximations of the non-convex objective function, a heuristic algorithm is proposed to obtain a suboptimal solution of low complexity. We show that although multi-cell joint transmission is allowed, in most cases, it is optimal for each user to be served by a single BS. The optimal BS association condition is parameterized, which reveals how it is impacted by different system parameters. Simulation results indicate that putting a BS into sleep mode by proper load balancing is an important solution for energy savings.\"",
        "Document: \"Distributed Power Control in Downlink Cellular Massive MIMO Systems. This paper compares centralized and distributed methods to solve the power minimization problem with qualityof- service (QoS) constraints in the downlink (DL) of multicell Massive multiple-input multiple-output (MIMO) systems. In particular, we study the computational complexity, number of parameters that need to be exchanged between base stations (BSs), and the convergence of iterative implementations. Although a distributed implementation based on dual decomposition (which only requires statistical channel knowledge at each BS) typically converges to the global optimum after a few iterations, many parameters need to be exchanged to reach convergence.\"",
        "Document: \"Energy efficiency optimization in hardware-constrained large-scale MIMO systems. Large-scale multiple-input multiple-output (MIMO) communication systems can bring substantial improvement in spectral efficiency and/or energy efficiency, due to the excessive degrees-of-freedom and huge array gain. However, large-scale MIMO is expected to deploy lower-cost radio frequency (RF) components, which are particularly prone to hardware impairments. Unfortunately, compensation schemes are not able to remove the impact of hardware impairments completely, such that a certain amount of residual impairments always exists. In this paper, we investigate the impact of residual transmit RF impairments (RTRI) on the spectral and energy efficiency of training-based point-to-point large-scale MIMO systems, and seek to determine the optimal training length and number of antennas which maximize the energy efficiency. We derive deterministic equivalents of the signal-to-noise-and-interference ratio (SINR) with zero-forcing (ZF) receivers, as well as the corresponding spectral and energy efficiency, which are shown to be accurate even for small number of antennas. Through an iterative sequential optimization, we find that the optimal training length of systems with RTRI can be smaller compared to ideal hardware systems in the moderate SNR regime, while larger in the high SNR regime. Moreover, it is observed that RTRI can significantly decrease the optimal number of transmit and receive antennas.\"",
        "Document: \"Downlink Power Control For Massive Mimo Cellular Systems With Optimal User Association. This paper aims to minimize the total transmit power consumption for Massive MIMO (multiple-input multiple-output) downlink cellular systems when each user is served by the optimized subset of the base stations (BSs). We derive a lower bound on the ergodic spectral efficiency (SE) for Rayleigh fading channels and maximum ratio transmission (MRT) when the BSs cooperate using non-coherent joint transmission. We solve the joint user association and downlink transmit power minimization problem optimally under fixed SE constraints. Furthermore, we solve a max-min fairness problem with user specific weights that maximizes the worst SE among the users. The optimal BS-user association rule is derived, which is different from maximum signal-to-noise-ratio (max-SNR) association. Simulation results manifest that the proposed methods can provide good SE for the users using less transmit power than in small-scale systems and that the optimal user association can effectively balance the load between BSs when needed.\"",
        "Document: \"Energy-efficient future wireless networks: A marriage between massive MIMO and small cells. How would a cellular network designed for high energy efficiency look like? To answer this fundamental question, we model cellular networks using stochastic geometry and optimize the energy efficiency with respect to the density of base stations, the number of antennas and users per cell, the transmit power levels, and the pilot reuse. The highest efficiency is neither achieved by a pure small-cell approach, nor by a pure massive MIMO solution. Interestingly, it is the combination of these approaches that provides the highest energy efficiency; small cells contributes by reducing the propagation losses while massive MIMO enables multiplexing of users with controlled interference.\"",
        "Document: \"Training-based Bayesian MIMO channel and channel norm estimation. Training-based estimation of channel state information in multi-antenna systems is analyzed herein. Closed-form expressions for the general Bayesian minimum mean square error (MMSE) estimators of the channel matrix and the squared channel norm are derived in a Rayleigh fading environment with known statistics at the receiver side. When the second-order channel statistics are available also at the transmitter, this information can be exploited in the training sequence design to improve the performance. Herein, mean square error (MSE) minimizing training sequences are considered. The structure of the general solution is developed, with explicit expressions at high and low SNRs and in the special case of uncorrelated receive antennas. The optimal length of the training sequence is equal or smaller than the number of transmit antennas.\"",
        "Document: \"Optimality Properties and Low-Complexity Solutions to Coordinated Multicell Transmission. Base station cooperation can theoretically improve the throughput of multicell systems by coordinating interference and serving cell edge terminals through multiple base stations. In practice, the extent of cooperation is limited by the increase in backhaul signaling and computational demands. To address these concerns, we propose a novel distributed cooperation structure where each base station has responsibility for the interference towards a set of terminals, while only serving a subset of them with data. Weighted sum rate maximization is considered, and conditions for beamforming optimality and the optimal transmission structure are derived using Lagrange duality theory. This leads to distributed low-complexity transmission strategies, which are evaluated on measured multiantenna channels in a typical urban multicell environment.\"",
        "1 is \"Downlink multicell processing with limited-backhaul capacity\", 2 is \"Relay search algorithms for coded cooperative systems\"",
        "Given above information, for an author who has written the paper with the title \"Analysis and management of heterogeneous user mobility in large-scale downlink systems\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006806": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'StoryFlow: tracking the evolution of stories.':",
        "Document: \"GPU-assisted computation of centroidal Voronoi tessellation. Centroidal Voronoi tessellations (CVT) are widely used in computational science and engineering. The most commonly used method is Lloyd's method, and recently the L-BFGS method is shown to be faster than Lloyd's method for computing the CVT. However, these methods run on the CPU and are still too slow for many practical applications. We present techniques to implement these methods on the GPU for computing the CVT on 2D planes and on surfaces, and demonstrate significant speedup of these GPU-based methods over their CPU counterparts. For CVT computation on a surface, we use a geometry image stored in the GPU to represent the surface for computing the Voronoi diagram on it. In our implementation a new technique is proposed for parallel regional reduction on the GPU for evaluating integrals over Voronoi cells.\"",
        "Document: \"Computing self-supporting surfaces by regular triangulation. Masonry structures must be compressively self-supporting; designing such surfaces forms an important topic in architecture as well as a challenging problem in geometric modeling. Under certain conditions, a surjective mapping exists between a power diagram, defined by a set of 2D vertices and associated weights, and the reciprocal diagram that characterizes the force diagram of a discrete self-supporting network. This observation lets us define a new and convenient parameterization for the space of self-supporting networks. Based on it and the discrete geometry of this design space, we present novel geometry processing methods including surface smoothing and remeshing which significantly reduce the magnitude of force densities and homogenize their distribution.\"",
        "Document: \"Computing singular points of plane rational curves. We compute the singular points of a plane rational curve, parametrically given, using the implicitization matrix derived from the @m-basis of the curve. It is shown that singularity factors, which are defined and uniquely determined by the elementary divisors of the implicitization matrix, contain all the information about the singular points, such as the parameter values of the singular points and their multiplicities. Based on this observation, an efficient and numerically stable algorithm for computing the singular points is devised, and inversion formulae for the singular points are derived. In particular, high order singular points can be detected and computed effectively. This approach based on singularity factors can also determine whether a rational curve has any non-ordinary singular points that contain singular points in its infinitely near neighborhood. Furthermore, a method is proposed to determine whether a singular point is ordinary or not. Finally, a conjecture in [Chionh, E.-W., Sederberg, T.W., 2001. On the minors of the implicitization bezout matrix for a rational plane curve. Computer Aided Geometric Design 18, 21-36] regarding the multiplicity of the singular points of a plane rational curve is proved.\"",
        "Document: \"Constrained 3D shape reconstruction using a combination of surface fitting and registration. We investigate 3D shape reconstruction from measurement data in the presence of constraints. The constraints may fix the surface type or set geometric relations between parts of an object's surface, such as orthogonality, parallelity and others. It is proposed to use a combination of surface fitting and registration within the geometric optimization framework of squared distance minimization (SDM). In this way, we obtain a quasi-Newton like optimization algorithm, which in each iteration simultaneously registers the data set with a rigid motion to the fitting surface and adapts the shape of the fitting surface. We present examples to show the applicability of our method to constrained 3D shape fitting for reverse engineering of CAD models and to high accuracy fitting with kinematic surfaces, which include surfaces of revolution (reconstructed from fragments of archeological pottery) and spiral surfaces, which are fitted to 3D measurement data of shells. Our optimization algorithm can combine registration of multiple scans of an object and model fitting into a single optimization process which is shown to be superior to the traditional procedure, which first registers the data and then fits a model to it.\"",
        "Document: \"Simulated annealing based hand tracking in a discrete space. Hand tracking is a challenging problem due to the complexity of searching in a 20+ degrees of freedom (DOF) space for an optimal estimation of hand configuration. This paper represents the feasible hand configurations as a discrete space, which avoids learning to find parameters as general configuration space representations do. Then, we propose an extended simulated annealing method with particle filter to search for optimal hand configuration in this discrete space, in which simplex search running in multi-processor is designed to predict the hand motion instead of initializing the simulated annealing randomly, and particle filter is employed to represent the state of the tracker at each layer for searching in high dimensional configuration space. The experimental results show that the proposed method makes the hand tracking more efficient and robust.\"",
        "Document: \"O-CNN: octree-based convolutional neural networks for 3D shape analysis. We present O-CNN, an Octree-based Convolutional Neural Network (CNN) for 3D shape analysis. Built upon the octree representation of 3D shapes, our method takes the average normal vectors of a 3D model sampled in the finest leaf octants as input and performs 3D CNN operations on the octants occupied by the 3D shape surface. We design a novel octree data structure to efficiently store the octant information and CNN features into the graphics memory and execute the entire O-CNN training and evaluation on the GPU. O-CNN supports various CNN structures and works for 3D shapes in different representations. By restraining the computations on the octants occupied by 3D surfaces, the memory and computational costs of the O-CNN grow quadratically as the depth of the octree increases, which makes the 3D CNN feasible for high-resolution 3D models. We compare the performance of the O-CNN with other existing 3D CNN solutions and demonstrate the efficiency and efficacy of O-CNN in three shape analysis tasks, including object classification, shape retrieval, and shape segmentation.\"",
        "Document: \"Adaptive query suggestion for difficult queries. Query suggestion is a useful tool to help users formulate better queries. Although this has been found highly useful globally, its effect on different queries may vary. In this paper, we examine the impact of query suggestion on queries of different degrees of difficulty. It turns out that query suggestion is much more useful for difficult queries than easy queries. In addition, the suggestions for difficult queries should rely less on their similarity to the original query. In this paper, we use a learning-to-rank approach to select query suggestions, based on several types of features including a query performance prediction. As query suggestion has different impacts on different queries, we propose an adaptive suggestion approach that makes suggestions only for difficult queries. We carry out experiments on real data from a search engine. Our results clearly indicate that an approach targeting difficult queries can bring higher gain than a uniform suggestion approach.\"",
        "Document: \"Planar Hexagonal Meshing for Architecture. Mesh surfaces with planar hexagonal faces, what we refer to as PH meshes, offer an elegant way of paneling freeform architectural surfaces due to their node simplicity (i.e., valence-3 nodes) and naturally appealing layout. We investigate PH meshes to understand how the shape, size, and pattern of PH faces are constrained by surface geometry. This understanding enables us to develop an effective method for paneling freeform architectural surfaces with PH meshes. Our method first constructs an ideal triangulation of a given smooth surface, guided by surface geometry. We show that such an ideal triangulation leads to a Dupin-regular PH mesh via tangent duality on the surface. We have developed several novel and effective techniques for improving undesirable mesh layouts caused by singular behaviors of surface curvature. We compute support structures associated with PH meshes, including exact vertex offsets and approximate edge offsets, as demanded in panel manufacturing. The efficacy of our method is validated by a number of architectural examples.\"",
        "Document: \"Quadric surface extraction by variational shape approximation. Based on Lloyd iteration, we present a variational method for extracting general quadric surfaces from a 3D mesh surface. This work extends the previous variational methods that extract only planes or special types of quadrics, i.e., spheres and circular cylinders. Instead of using the exact L2 error metric, we use a new approximate L2 error metric to make our method more efficient for computing with general quadrics. Furthermore, a method based on graph cut is proposed to smooth irregular boundary curves between segmented regions, which greatly improves the final results.\"",
        "Document: \"Surface Approximation via Asymptotic Optimal Geometric Partition. In this paper, we present a novel method on surface partition from the perspective of approximation theory. Different from previous shape proxies, the ellipsoidal variance proxy is proposed to penalize the partition results falling into disconnected parts. On its support, the Principle Component Analysis (PCA) based energy is developed for asymptotic cluster aspect ratio and size control. We provi...\"",
        "1 is \"Small-Scale Incident Detection based on Microposts\", 2 is \"A general framework for surface modeling using geometric partial differential equations\"",
        "Given above information, for an author who has written the paper with the title \"StoryFlow: tracking the evolution of stories.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006809": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Soft Iterative Channel Estimation With Subspace And Rank Tracking':",
        "Document: \"An Analysis of Performance Improvement by a Helper for Wireless Sensor Networks. Wireless sensor networks (WSNs) are the core component in the big data era. Due to the unreliable transmission environment, it is significantly useful to introduce a helper to refine the system performance. To begin with, we formulate the system model of WSNs as a problem of multiterminal source coding. Subsequently, we propose a 3D distributed compress-bin scheme and derive a corresponding inner bound by analyzing the expected rate-distortion. Finally, we investigate the performance improvement of a helper by comparing the derived inner bound with the Berger-Tung inner bound and through simulation. Both the theoretical bounds and simulation results indicate that a helper can obviously improve the system performance.\"",
        "Document: \"On the MMSE criterion for space-time coded signaling in the presence of unknown interference. The primary goal of this letter is to provide comparative assessments to the two cases where different criteria are used in joint over antenna minimum mean squared error (JA-MMSE) spatial filtering for space-time coded systems in the presence of unknown interference. It is shown that additional constraints to preserve the space-time coded signal structure in JA-MMSE spatial filtering place floor in bit error rate (BER) performance, while imposing no additional constraint places no error floor.\"",
        "Document: \"Exploitation of 2D binary source correlation using turbo block codes with fine-tuning. This article proposes a joint source-channel coding technique for two-dimensional (2D) binary Markov sources by using concatenated turbo block codes composed of two Bose, Chaudhuri, Hocquenghem (BCH) codes, of which output is followed by a rate-1 recursive systematic convolutional code. The source correlation of all rows and columns of the 2D source is well exploited by using a modified Bahl\u2013Cocke\u2013Jelinek\u2013Raviv (BCJR) algorithm for the decoding of the BCH codes. Simulation results show that the proposed technique outperforms in terms of bit error rate the codes that exploits one-dimensional (1D) source correlation using the modified BCJR algorithm, and obviously the conventional system without source correlation exploitation. In order to further improve the performance, this article aims to make fine-tuning of the code parameters, given the source correlation property, that can achieve performance even closer to the theoretical limit than without the fine-tuning. Finally, results of image transmission simulations using two images, one having strong and the other weak 2D correlation, are presented to demonstrate the effectiveness of our proposed technique.\"",
        "Document: \"Soft Iterative Channel Estimation With Subspace And Rank Tracking. This letter presents an adaptative soft-based method for channel estimation in turbo receivers. The proposed approach is based on the particular algebraic structure of multipath Rayleigh-fading channels, and it is suited for mobile systems where the multipath pattern (namely, the times of delay) changes slowly over the time. The method is implemented through a rank-and-subspace tracking algorithm that allows to adapt the estimate to the multipath variations and also to reduce the computational cost with respect to the batch implementation based on eigenvalue decomposition. A performance analysis, in terms of mean square error of the channel estimate and bit error rate, shows the advantages of the proposed technique in communications over time-varying wireless channels.\"",
        "Document: \"Distributed Joint Source-Channel Coding for Correlated Sources Using Non-systematic Repeat-Accumulate Based Codes. In this paper, we propose a technique for coding the data from multiple correlated binary sources, with the aim of providing an alternative solution to the correlated source compression problem. Using non-systematic repeat-accumulate based codes, it is possible to achieve compression which is close to the Slepian---Wolf bound without relying on massive puncturing. With the technique proposed in this paper, instead of puncturing, compression is achieved by increasing check node degrees. Hence, the code rate can be more flexibly adjusted with the proposed technique in comparison with the puncturing-based schemes. Furthermore, the technique is applied to distributed joint source-channel coding (DJSCC). It is shown that in many cases tested, the proposed scheme can achieve mutual information very close to one with the lower signal-to-noise power ratio than turbo and low density generator matrix based DJSCC in additive white Gaussian noise channel. The convergence property of the system is also evaluated via the extrinsic information transfer analysis.\"",
        "Document: \"Outage probability of correlated binary source transmission over fading multiple access channels. This paper investigates the outage behavior of transmitting correlated binary sources with arbitrary code rate over quasi-static Rayleigh fading multiple access channel (MAC). The sufficient condition for lossless communication with arbitrary code rate is obtained by assuming separate source and channel coding, which indicates the two rate regions specified by the Slepian-Wolf theorem and the MAC capacity region intersect. An upper bound of the outage probability is derived based on the sufficient condition. Asymptotic property shows the second order diversity gain can be achieved only when the tow sources are fully correlated. Numerical results demonstrate the accuracy of the derived upper bound. The \u03b5-outage achievable rate, which is the largest rate of transmission such that the outage probability is smaller than a predefined \u03b5 value, is also analyzed. It is found the \u03b5-outage achievable rate becomes lager with higher transmit power and/or stronger source correlation.\"",
        "Document: \"PAPR Constrained Power Allocation for Multicarrier Transmission in Multiuser SIMO Communications. Peak-to-average power ratio (PAPR)\u00a0constrained power allocation for multicarrier transmission in multiuser single-input multiple-output (SIMO)\u00a0communications is considered in this paper. Reducing the PAPR in any transmission system is beneficial because it allows the use of inexpensive energy-efficient power amplifiers. In this paper, we formulate a power allocation problem for single-carrier (SC)\u00a0frequency division multiple access (FDMA)\u00a0and orthogonal FDMA (OFDMA)\u00a0transmission with instantaneous PAPR constraints. Moreover, a statistical approach is considered in which the power variance of the transmitted waveform is controlled. The constraints for the optimization problems are derived as a function of transmit power allocation and two successive convex approximations (SCAs)\u00a0are derived for each of the constraints based on a change of variables (COV)\u00a0and geometric programming (GP). In addition, the optimization problem is constrained by a user-specific quality of service (QoS)\u00a0constraint. Hence, the proposed power allocation strategy jointly takes into account the channel quality and the PAPR characteristics of the power amplifier. The numerical results show that the proposed power allocation strategy can significantly improve the transmission efficiency of power-limited users. Therefore, it is especially beneficial for improving the performance for cell edge users.\"",
        "Document: \"On convergence constrained precoder design for iterative frequency domain MIMO detector. This paper proposes a novel linear precoder design technique for single carrier single-user multiple input multiple output (MIMO) systems with frequency-domain (FD) soft cancellation (SC) minimum mean squared error (MMSE) iterative equalization where the convergence properties of the equalizer are taken into account. The proposed precoder design technique, convergence constrained precoding (CCP), minimizes the transmission power while it achieves the target mutual information for each stream after the iterations at the receiver side. We show that the optimality criterion for the proposed design can be formulated as a convex optimization problem. The results demonstrate that our proposed technique outperforms the existing linear precoding techniques by ensuring the convergence with a reduced transmission power. Furthermore, we show that with CCP we can adjust transmission according to convergence properties of the iterative equalizer in a more flexible way than, e.g., minimum sum mean squared error (MinSumMSE) and maximum information rate (MaxRate) precoding.\"",
        "Document: \"Ceo Problem Based Analysis Of D2d Cooperative User Pairing. We interpret the device-to-device (D2D) cooperative relaying framework as an instance of a two-node binary chief executive officer (CEO) problem. Noise-corrupted versions of a binary sequence are transmitted by two nodes to a single destination node over orthogonal multiple access channel. A lower bound of the bit error probability (BEP) is obtained by minimizing a distortion function subject to constraints on inequalities based on the source-channel separation theorem. We derive the rate-distortion function for a binary multiterminal source coding problem. The distortion function is then established by evaluating the relationship between two problems for majority and optimal decision criteria. Our proposed encoding/decoding algorithms using concatenated convolutional codes and joint decoding scheme are used to verify the lower bound of the BEP. The results are interpreted as a criterion to select cooperating D2D user pairs.\"",
        "Document: \"Low Complexity Time-Concatenated Turbo Equalization for Block Transmission without Guard Interval: Part 2 - Application to SC-FDMA. Abstract The primary objective of this paper is to apply the CHATUE algorithm, presented in the Part-1 paper of this article, to multi-user Single Carrier Frequency Division Multiple Access (SC-FDMA) Systems. The CHATUE algorithm connects turbo equalizers neighboring in time in the absence of Cyclic Prefix or Guard Interval, where the latest version of the reduced complexity equalization technique, Frequency Domain Soft Cancellation Minimum Mean Square Error turbo equalization is utilized not only to mitigate the inter-carrier interference but also to eliminate the inter-block interferences from the neighboring blocks. Furthermore, doped accumulator is combined with our proposed CHATUE-SC-FDMA system. Extrinsic Information Transfer analysis is used to demonstrate the improvement in convergence property as well as to analyze the bit error rate threshold. This paper provides in detail the time-concatenated turbo equalization algorithm for SC-FDMA, referred to as CHATUE-SC-FDMA, and evaluates its performances. When deriving the algorithm, we also propose, without imposing significant performance degradation, an approximation technique to eliminate the necessity of the covariance matrix inversion.\"",
        "1 is \"Evaluation of Response Times in Industrial WLANs\", 2 is \"On the iterative decoding of multilevel codes\"",
        "Given above information, for an author who has written the paper with the title \"Soft Iterative Channel Estimation With Subspace And Rank Tracking\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006815": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Enhancing web search in the medical domain via query clarification.':",
        "Document: \"Assessing the impact of a health intervention via user-generated Internet content. Assessing the effect of a health-oriented intervention by traditional epidemiological methods is commonly based only on population segments that use healthcare services. Here we introduce a complementary framework for evaluating the impact of a targeted intervention, such as a vaccination campaign against an infectious disease, through a statistical analysis of user-generated content submitted on web platforms. Using supervised learning, we derive a nonlinear regression model for estimating the prevalence of a health event in a population from Internet data. This model is applied to identify control location groups that correlate historically with the areas, where a specific intervention campaign has taken place. We then determine the impact of the intervention by inferring a projection of the disease rates that could have emerged in the absence of a campaign. Our case study focuses on the influenza vaccination program that was launched in England during the 2013/14 season, and our observations consist of millions of geo-located search queries to the Bing search engine and posts on Twitter. The impact estimates derived from the application of the proposed statistical framework support conventional assessments of the campaign.\"",
        "Document: \"Enhancing digital libraries using missing content analysis. This work shows how the content of a digital library can be enhanced to better satisfy its users' needs. Missing content is identified by finding missing content topics in the system's query log or in a pre-defined taxonomy of required knowledge. The collection is then enhanced with new relevant knowledge, which is extracted from external sources that satisfy those missing content topics. Experiments we conducted measure the precision of the system before and after content enhancement. The results demonstrate a significant improvement in the system effectiveness as a result of content enhancement and the superiority of the missing content enhancement policy over several other possible policies.\"",
        "Document: \"A self-optimized job scheduler for heterogeneous server clusters. Heterogeneous clusters and grid infrastructures are becoming increasingly popular. In these computing infrastructures, machines have different resources, including memory sizes, disk space, and installed software packages. These differences give rise to a problem of over-provisioning, that is, sub-optimal utilization of a cluster due to users requesting resource capacities greater than what their jobs actually need. Our analysis of a real workload file (LANL CM5) revealed differences of up to two orders of magnitude between requested memory capacity and actual memory usage. This paper presents an algorithm to estimate actual resource capacities used by batch jobs. Such an algorithm reduces the need for users to correctly predict the resources required by their jobs, while at the same time managing the scheduling system to obtain superior utilization of available hardware. The algorithm is based on the Reinforcement Learning paradigm; it learns its estimation policy on-line and dynamically modifies it according to the overall cluster load. The paper includes simulation results which indicate that our algorithm can yield an improvement of over 30% in utilization (overall throughput) of heterogeneous clusters.\"",
        "Document: \"Instrumenting where it hurts: an automatic concurrent debugging technique. As concurrent and distributive applications are becoming more common and debugging such applications is very difficult, practical tools for automatic debugging of concurrent applications are in demand. In previous work, we applied automatic debugging to noise-based testing of concurrent programs. The idea of noise-based testing is to increase the probability of observing the bugs by adding, using instrumentation, timing \"noise\" to the execution of the program. The technique of finding a small subset of points that causes the bug to manifest can be used as an automatic debugging technique. Previously, we showed that Delta Debugging can be used to pinpoint the bug location on some small programs. In the work reported in this paper, we create and evaluate two algorithms for automatically pinpointing program locations that are in the vicinity of the bugs on a number of industrial programs. We discovered that the Delta Debugging algorithms do not scale due to the non-monotonic nature of the concurrent debugging problem. Instead we decided to try a machine learning feature selection algorithm. The idea is to consider each instrumentation point as a feature, execute the program many times with different instrumentations, and correlate the features (instrumentation points) with the executions in which the bug was revealed. This idea works very well when the bug is very hard to reveal using instrumentation, correlating to the case when a very specific timing window is needed to reveal the bug. However, in the more common case, when the bugs are easy to find using instrumentation points ranked high by the feature selection algorithm is not high enough. We show that for these cases, the important value is not the absolute value of the evaluation of the feature but the derivative of that value along the program execution path. As a number of groups expressed interest in this research, we built an open infrastructure for automatic debugging algorithms for concurrent applications, based on noise injection based concurrent testing using instrumentation. The infrastructure is described in this paper.\"",
        "Document: \"Anxiety and Information Seeking: Evidence From Large-Scale Mouse Tracking. People seeking information through search engines are assumed to behave similarly, regardless of the topic which they are searching. Here we use mouse tracking, which is correlated with gaze, to show that the information seeking patterns of people differ dramatically depending on their level of anxiety at the time of the search. We investigate the behavior of people during searches for medical symptoms, ranging from benign indications, where users are not usually anxious, to ones which could harbinger life-threatening conditions, where extreme anxiety is expected. We show that for the latter, 90% of people never saw more than the top 67% of the screen, compared to over 95% scanned by people seeking information on benign symptoms, even though relevant documents are similarly distributed in the results pages to these queries. Based on this observation, we develop a model which can predict the level of anxiety experienced by a user, using attributes derived from mouse tracking data and other user interactions. The model achieves Kendall's Tau of 0.48 with the medical severity of the symptoms searched. We show the importance of using information about the users? level of anxiety as predicted by the model, when measuring search engine performance. Our results prove that ignoring this information can lead to significant over-estimation of performance. Additionally, we show the utility of the model in three special instances: where multiple symptoms are searched concurrently; where the searcher has an underlying medical condition; and when users seek information on ways to commit suicide. In the latter, our results demonstrate the importance of help-line notices, and emphasize the need to measure the effective number of results seen by the user. Our results indicate that measures of relevance which use anxiety information can lead to more accurate understanding of the quality of search results, especially when delivering potentially life-saving information to users.\n\n\"",
        "Document: \"Workshop on health search and discovery: helping users and advancing medicine. This workshop brings together researchers and practitioners from industry and academia to discuss search and discovery in the medi-cal domain. The event focuses on ways to make medical and health information more accessible to laypeople (including enhancements to ranking algorithms and search interfaces), and how we can dis-cover new medical facts and phenomena from information sought online, as evidenced in query streams and other sources such as social media. This domain also offers many opportunities for appli-cations that monitor and improve quality of life of those affected by medical conditions, by providing tools to support their health-related information behavior.\"",
        "Document: \"On the Relationship between Novelty and Popularity of User-Generated Content. This work deals with the task of predicting the popularity of user-generated content. We demonstrate how the novelty of newly published content plays an important role in affecting its popularity. More specifically, we study three dimensions of novelty. The first one, termed contemporaneous novelty, models the relative novelty embedded in a new post with respect to contemporary content that was generated by others. The second type of novelty, termed self novelty, models the relative novelty with respect to the user\u2019s own contribution history. The third type of novelty, termed discussion novelty, relates to the novelty of the comments associated by readers with respect to the post content. We demonstrate the contribution of the new novelty measures to estimating blog-post popularity by predicting the number of comments expected for a fresh post. We further demonstrate how novelty based measures can be utilized for predicting the citation volume of academic papers.\"",
        "Document: \"Recommendations Meet Web Browsing: Enhancing Collaborative Filtering Using Internet Browsing Logs. Collaborative filtering (CF) recommendation systems are one of the most popular and successful methods for recommending products to people. CF systems work by finding similarities between different people according to their past purchases, and using these similarities to suggest possible items of interest. In this work we show that CF systems can be enhanced using Internet browsing data and search engine query logs, both of which represent a rich profile of individuals' interests.We introduce two approaches to enhancing user modeling using these data. We do not assume the existence of explicit ratings, but rather rely on unweighted, positive signals, that are available in most commercial contexts. We demonstrate the value of our approach on two real datasets each comprising of the activities of tens of thousands of individuals. The first dataset details the downloads of Windows Phone 8 mobile applications and the second - item views in an online retail store. Both datasets are enhanced using anonymized Internet browsing logs.Our results show that prediction accuracy is improved by up to 72%. This improvement is largest when building a model which can predict for the entire catalog of items, not just popular ones. Finally, we discuss additional benefits of our approach, which include: improved recommendations for users with few past purchases and enabling recommendations based on short-term purchase intent.\"",
        "Document: \"Demographic differences in search engine use with implications for cohort selection. The correlation between the demographics of users and the text they write has been investigated through literary texts and, more recently, social media. However, differences pertaining to language use in search engines has not been thoroughly analyzed, especially for age and gender differences. Such differences are important especially due to the growing use of search engine data in the study of human health, where queries are used to identify patient populations. Using three datasets comprising of queries from multiple general-purpose Internet search engines we investigate the correlation between demography (age, gender, and income) and the text of queries submitted to search engines. Our results show that females and younger people use longer queries. This difference is such that females make approximately 25% more queries with 10 or more words. In the case of queries which identify users as having specific medical conditions we find that females make 53% more queries than expected, and that this results in patient cohorts which are highly skewed in gender and age, compared to known patient populations. We show that methods for cohort selection which use additional information beyond queries where users indicate their condition are less skewed. Finally, we show that biased training cohorts can lead to differential performance of models designed to detect disease from search engine queries. Our results indicate that studies where demographic representation is important, such as in the study of health aspect of users or when search engines are evaluated for fairness, care should be taken in the selection of search engine data so as to create a representative dataset.\"",
        "Document: \"What makes a query difficult?. This work tries to answer the question of what makes a query difficult. It addresses a novel model that captures the main components of a topic and the relationship between those components and topic difficulty. The three components of a topic are the textual expression describing the information need (the query or queries), the set of documents relevant to the topic (the Qrels), and the entire collection of documents. We show experimentally that topic difficulty strongly depends on the distances between these components. In the absence of knowledge about one of the model components, the model is still useful by approximating the missing component based on the other components. We demonstrate the applicability of the difficulty model for several uses such as predicting query difficulty, predicting the number of topic aspects expected to be covered by the search results, and analyzing the findability of a specific domain.\"",
        "1 is \"Support-Vector Networks\", 2 is \"Glean: using syntactic information in document filtering\"",
        "Given above information, for an author who has written the paper with the title \"Enhancing web search in the medical domain via query clarification.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006851": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Two-sorted Point-Interval Temporal Logics':",
        "Document: \"Horn Fragments of the Halpern-Shoham Interval Temporal Logic (Technical Report).   We investigate the satisfiability problem for Horn fragments of the Halpern-Shoham interval temporal logic depending on the type (box or diamond) of the interval modal operators, the type of the underlying linear order (discrete or dense), and the type of semantics for the interval relations (reflexive or irreflexive). For example, we show that satisfiability of Horn formulas with diamonds is undecidable for any type of linear orders and semantics. On the contrary, satisfiability of Horn formulas with boxes is tractable over both discrete and dense orders under the reflexive semantics and over dense orders under the irreflexive semantics, but becomes undecidable over discrete orders under the irreflexive semantics. Satisfiability of binary Horn formulas with both boxes and diamonds is always undecidable under the irreflexive semantics. \"",
        "Document: \"Decidable and Undecidable Fragments of Halpern and Shoham's Interval Temporal Logic: Towards a Complete Classification. Interval temporal logics are based on temporal structures where time intervals, rather than time instants, are the primitive ontological entities. They employ modal operators corresponding to various relations between intervals, known as Allen's relations. Technically, validity in interval temporal logics translates to dyadic second-order logic, thus explaining their complex computational behavior. The full modal logic of Allen's relations, called HS, has been proved to be undecidable by Halpern and Shoham under very weak assumptions on the class of interval structures, and this result was discouraging attempts for practical applications and further research in the field. A renewed interest has been recently stimulated by the discovery of interesting decidable fragments of HS. This paper contributes to the characterization of the boundary between decidability and undecidability of HS fragments. It summarizes known positive and negative results, it describes the main techniques applied so far in both directions, and it establishes a number of new undecidability results for relatively small fragments of HS.\"",
        "Document: \"Horn Fragments of the Halpern-Shoham Interval Temporal Logic. We investigate the satisfiability problem for Horn fragments of the Halpern-Shoham interval temporal logic depending on the type (box or diamond) of the interval modal operators, the type of the underlying linear order (discrete or dense), and the type of semantics for the interval relations (reflexive or irreflexive). For example, we show that satisfiability of Horn formulas with diamonds is undecidable for any type of linear orders and semantics. On the contrary, satisfiability of Horn formulas with boxes is tractable over both discrete and dense orders under the reflexive semantics and over dense orders under the irreflexive semantics but becomes undecidable over discrete orders under the irreflexive semantics. Satisfiability of binary Horn formulas with both boxes and diamonds is always undecidable under the irreflexive semantics.\"",
        "Document: \"Undecidability of Interval Temporal Logics with the Overlap Modality. We investigate fragments of Halpern-Shoham\u2019s interval logic HS involving the modal operators for the relations of left or right overlap of intervals. We prove that most of these fragments are undecidable, by employing a non-trivial reduction from the octant tiling problem.\"",
        "Document: \"Non-finite Axiomatizability and Undecidability of Interval Temporal Logics with C, D, and T. Interval logics are an important area of computer science. Although attention has been mainly focused on unary operators, an early work by Venema (1991) introduced an expressively complete interval logic language called CDT, based on binary operators, which has many potential applications and a strong theoretical interest. Many very natural questions about CDT and its fragments, such as (non-)finite axiomatizability and (un-)decidability, are still open (as a matter of fact, only a few undecidability results, including the undecidability of CDT, are known). In this paper, we answer most of these questions, showing that almost all fragments of CDT, containing at least one binary operator, are neither finitely axiomatizable with standard rules nor decidable. A few cases remain open.\"",
        "Document: \"Propositional interval neighborhood logics: Expressiveness, decidability, and undecidable extensions. In this paper, we investigate the expressiveness of the variety of propositional interval neighborhood logics (PNL), we establish their decidability on linearly ordered domains and some important subclasses, and we prove the undecidability of a number of extensions of PNL with additional modalities over interval relations. All together, we show that PNL form a quite expressive and nearly maximal decidable fragment of Halpern\u2013Shoham\u2019s interval logic HS.\"",
        "Document: \"DL-Lite and Interval Temporal Logics: a Marriage Proposal. Description logics of the DL-Lite family are widely used in knowledge representation because of their low computational complexity and rather good expressivity sufficient to capture important conceptual modelling constructs and the OWL2 QL profile of the Ontology Web Language (OWL). Recently, various point-based temporal extensions of DL-Lite have been investigated. Here, we propose to extend DL-Lite with fragments of Halpern and Shoham's interval logic of Allen's relations (HS). We formally define such extensions and show how they can be successfully used in knowledge representation. In the quest for a decidable logic, we discuss the challanges in combining decidable fragments of HS with DL-Lite.\"",
        "Document: \"J48ss: A Novel Decision Tree Approach For The Handling Of Sequential And Time Series Data. Temporal information plays a very important role in many analysis tasks, and can be encoded in at least two different ways. It can be modeled by discrete sequences of events as, for example, in the business intelligence domain, with the aim of tracking the evolution of customer behaviors over time. Alternatively, it can be represented by time series, as in the stock market to characterize price histories. In some analysis tasks, temporal information is complemented by other kinds of data, which may be represented by static attributes, e.g., categorical or numerical ones. This paper presents J48SS, a novel decision tree inducer capable of natively mixing static (i.e., numerical and categorical), sequential, and time series data for classification purposes. The novel algorithm is based on the popular C4.5 decision tree learner, and it relies on the concepts of frequent pattern extraction and time series shapelet generation. The algorithm is evaluated on a text classification task in a real business setting, as well as on a selection of public UCR time series datasets. Results show that it is capable of providing competitive classification performances, while generating highly interpretable models and effectively reducing the data preparation effort.\"",
        "Document: \"Generalizing Allen's Theory of Time to Tree-Like Structures. Allen's Interval Algebra is one of the most prominent formalisms in the area of qualitative temporal (and, by extension, spatial) reasoning. However, its applications are naturally restricted to linear flows of time. While there is some recent work focused on studying relations between intervals (and also between intervals and points) on branching structures, there is no rigorous study of the first-order theory of branching time. In this paper, we approach this problem under a very general definition of time structures as tree-like lattices. Allen's representation theorem shows that \\\"meets\\\" is expressively complete for the class of all unbounded linear orders, and it is easy to see that it is also complete for the class of all linear orders. Here we prove that, surprisingly, \\\"meets\\\" remains complete for the class of all unbounded tree-like lattices, and we provide an easy axiomatization of the class of all unbounded tree-like lattices in the branching language. Then, we show that \\\"meets\\\" becomes incomplete in the class of all tree-like lattices, we give a minimal complete set of three relations for this case along with an axiomatization, which turns out to be particularly challenging to obtain.\"",
        "Document: \"Right Propositional Neighborhood Logic over Natural Numbers with Integer Constraints for Interval Lengths. Traditional notions of refinement of algebraic specifications, based on signature morphisms, are often too rigid to capture a number of relevant transformations in the context of software design, reuse and adaptation. This paper proposes an alternative notion of specification refinement, building on recent work on logic interpretation. The concept is discussed, its theory partially developed, its use illustrated through a number of examples.\"",
        "1 is \"Multi-Dimensional Modal Logic as a Framework for Spatio-Temporal Reasoning\", 2 is \"Automatic synthesis of a global behavior from multiple distributed behaviors\"",
        "Given above information, for an author who has written the paper with the title \"Two-sorted Point-Interval Temporal Logics\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006860": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'How difficult are exams? A framework for assessing the complexity of introductory programming exams':",
        "Document: \"Managing Requirements Change the Informal Way: When Saying \u2018No\u2019 is Not an Option. Software has always been considered as malleable. Changes to software requirements are inevitable during the development process. Despite many software engineering advances over several decades, requirements changes are a source of project risk, particularly when businesses and technologies are evolving rapidly. Although effectively managing requirements changes is a critical aspect of software engineering, conceptions of requirements change in the literature and approaches to their management in practice still seem rudimentary. The overall goal of this study is to better understand the process of requirements change management. We present findings from an exploratory case study of requirements change management in a globally distributed setting. In this context we noted a contrast with the traditional models of requirements change. In theory, change control policies and formal processes are considered as a natural strategy to deal with requirements changes. Yet we observed that \"informal requirements changes\" (InfRc) were pervasive and unavoidable. Our results reveal an equally 'natural' informal change management process that is required to handle InfRc in parallel. We present a novel model of requirements change which, we argue, better represents the phenomenon and more realistically incorporates both the informal and formal types of change.\"",
        "Document: \"An introduction to program comprehension for computer science educators. The area of program comprehension comprises a vast body of literature, with numerous conflicting models having been proposed. Models are typically grounded in experimental studies mostly involving experienced programmers. The question of how to relate this material to the teaching and learning of programming for novices has proven challenging for many researchers. In this critical review from a computer science educational perspective, the authors compare and contrast the way in which different models conceptualize program comprehension. This provides new insights into learning issues such as content, sequence, learning obstacles, effective learning tasks and teaching methods, as well as into the assessment of learning\"",
        "Document: \"A course in collaborative computing: collaborative learning and research with a global perspective. Recent calls for a new discipline of 'web science' have proposed extending computer science to incorporate the social dimensions of computing. In this paper we outline a Masters course in Collaborative Computing, which employs a combination of collaborative pedagogy, collaborative technologies, and a corpus of research data from Global Virtual Teams to blend the technology and the social dimensions within a research linked course context. We review the effectiveness of this model of learning and the conduct of the course over the five years since its inception.\"",
        "Document: \"A Reflection on the First Run of the Runestone Project at Tongji University: Observations on Cross Cultural Distributed Teams vs Face to Face Teams. China has long been an education import nation, so non-commercial and mutually beneficial collaborations between Chinese and Western universities are rare. Nevertheless, the School of Software Engineering at Tongji University in China has developed mutually beneficial collaboration with several Western universities, among them the most important partner is Uppsala University. In spite of the great challenges encountered by both sides during developing and conducting the collaboration due to their sharply contrasting cultural backgrounds and different educational and political systems, the collaboration between the institutions has been deepening and widening steadily and continuously. Following successful student and teacher exchange programs, the Rune stone project was launched between Uppsala and Tongji Universities in 2009. It was taken as a triumph that the Sino-Swedish globally distributed Rune stone teams fulfilled the course requirements. However, it was also noticed that some advantages of cross cultural collaboration in learning which were observed in the face to face teams were not realized in the Sino-Swedish Rune stone teams. The students in the cross cultural face to face teams displayed evident complementarities in their work. This positive effect of the different cultural backgrounds seemingly disappeared when the students moved from the face to face teams to the globally distributed teams. This report records observations on students' work in the two types of cross cultural teams. In addition, on the basis of these reflections on the experience, some practical measures and areas for research are suggested in the hope of helping improve such global collaborations in future.\"",
        "Document: \"A Critical Evaluation of Failure in a Nearshore Outsourcing Project: What Dilemma Analysis Can Tell Us. Global Software Engineering (GSE) research contains few examples consciously applying what Glass and colleagues have termed an 'evaluative-critical' approach. In this study we apply dilemma analysis to conduct a critical review of a major (and ongoing) near shore Business Process Outsourcing project in New Zealand. The project has become so troubled that a Government Minister has recently been assigned responsibility for troubleshooting it. The 'Novo pay' project concerns the implementation of a nationwide payroll system responsible for the payment of some 110,000 teachers and education sector staff. An Australian company won the contract for customizing and implementing the Novo pay system, taking over from an existing New Zealand service provider. We demonstrate how a modified form of dilemma analysis can be a powerful technique for highlighting risks and stakeholder impacts from empirical data, and that adopting an evaluative-critical approach to such projects can usefully highlight tensions and barriers to satisfactory project outcomes.\"",
        "Document: \"Onshore to Near-Shore Outsourcing Transitions: Unpacking Tensions. This study is directed towards highlighting tensions of incoming and outgoing vendors during outsourcing in a near-shore context. Incoming-and-outgoing of vendors generate a complex form of relationship in which the participating organizations cooperate and compete simultaneously. It is of great importance to develop knowledge about this kind of relationship typically in the current GSE-related multi-sourcing environment. We carried out a longitudinal case study and utilized data from the 'Novo pay' project, which is available in the public domain. This project involved an outgoing New Zealand based vendor and incoming Australian based vendor. The results show that the demand for the same human resources, dependency upon cooperation and collaboration between vendors, reliance on each other system's configurations and utilizing similar strategies by the client, which worked for the previous vendor, generated a set of tensions which needed to be continuously managed throughout the project.\"",
        "Document: \"Global collaboration in course delivery: are we there yet?. Some recent experiences with Global Educational Collaborations have given me occasion to reflect on how sustainable such initiatives are, and whether they will over time become embedded in the fabric of modern University education, especially for courses such as Global Software Engineering. Swigger and colleagues argue to the contrary at least for now, observing that \"technical barriers such as unreliable software and institutional regulations discourage most teachers from exploring distributed learning\" [1]. But does this pessimistic view truly recognize the realities of the age? For Giddens \"one of the dominant characteristics of modernity is the separation of time from space made possible by the standardization of time across the world\", with \"human efforts to standardize temporal frameworks inscribed in official time zones\" [2]. Is a Global Collaboration then, inherently a product of the trend towards universal time, wherein time is the scarce resource of 'clock' time (based on what might be termed a temponomic world view [3, P.61].\"",
        "Document: \"How Best to Teach Global Software Engineering? Educators Are Divided. Pioneering educators discuss how they inject realism into global-software-engineering education.\"",
        "Document: \"On the necessity of removing \"cruelty\" from the teaching of computing. In his famous article [1] Edsger Dijkstra reflected upon how cruel it would be truly to teach computer science. For some reason the CS community over the years have taken the sadistic element of his entreaties to heart. Why is this so? Does it have intuitive appeal to those CS educators who believe in the \"hard man\" school of computer science and the \"real programmers don't eat quiche\" model of education?\"",
        "Document: \"Using SoDIS\u2122 as a risk analysis process: a teaching perspective. There are several difficulties we face when showing our students key processes and techniques for software development. In this paper, issues related to teaching students how to manage risks in software projects are profiled. The concepts and process for implementing Software Development Impact Statements (SoDIS) are outlined; with its supporting CASE tool the \"SoDIS Project Auditor\" being described. Different ways of applying the SoDIS process and the CASE tool are demonstrated, through some brief illustrative case studies. The paper suggests ways of using the process and the tool to enhance teaching in computing courses including software development projects, software engineering, project management, ethics and professionalism. This work occurs under the umbrella of the SoDIS SEPIA collaborative research programme which aims to promulgate use of the SoDIS process, in both industrial and educational computing spheres.\n\n\"",
        "1 is \"BDI Agents: From Theory to Practice\", 2 is \"Abstraction ability as an indicator of success for learning computing science?\"",
        "Given above information, for an author who has written the paper with the title \"How difficult are exams? A framework for assessing the complexity of introductory programming exams\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006866": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Lower-Complexity Layered Belief-Propagation Decoding Of Ldpc Codes':",
        "Document: \"Delta\u2013Sigma D/A Converter Using Binary- Weighted Digital-to-Analog Differentiator for Second-Order Mismatch Shaping. A multibit digital-analog (D/A) differentiator is used in the forward correction path of a dual-truncation delta-sigma (DeltaSigma) D/A converter (DAC) to obtain the desired second-order noise-shaping function for converting mismatch-induced in-band quantization noise to out-of-band frequencies. The multibit D/A differentiator can be configured by embedding binary-weighted current-steering DAC ele...\"",
        "Document: \"A Novel Fully Synthesizable All-Digital RF Transmitter for IoT Applications. In this paper, a fully synthesizable all-digital transmitter (ADTX) is first proposed. This transmitter (TX) uses Cartesian architecture and supports wide-band quadratic-amplitude modulation with wide carrier frequency range. Furthermore, the design methodology for ADTX and corresponding bandpass filter is discussed. This TX is synthesized with digital register transfer level-graphic database syst...\"",
        "Document: \"Lower-Complexity Layered Belief-Propagation Decoding Of Ldpc Codes. The design of LDPC decoders with low complexity, high throughput, and good performance is a critical task. A well-known strategy is to design structured codes such as quasi-cyclic LDPC (QC-LDPC) that allow partially-parallel decoders. Sequential schedules, such as Layered Belief-Propagation (LBP), converge faster than the traditional flooding schedule while allowing parallel decoding of QC-LDPC codes. In this paper, we propose a novel low-complexity sequential schedule called Zigzag LBP (Z-LBP). Current LBP schedules do not allow partially-parallel architectures in the regime of high-rate codes with small-to-medium blocklengths. Our proposed algorithm can still be implemented in a partially-parallel manner in this regime. Z-LBP provides the same benefits as LBP including faster convergence speed and lower frame error rates than flooding.\"",
        "Document: \"A 10-Bit 2-GS/s DAC-DDFS-IQ-Controller Baseband Enabling a Self-Healing 60-GHz Radio-on-Chip. A 10-bit 2-GS/s mixed-signal baseband (BB) circuit, which enables a self-healing 60-GHz 4-Gb/s radio-on-chip implemented in a 65-nm complementary metal-oxide semiconductor, is described. The BB circuit autonomously senses and optimizes transmitter (TX) P1dB, OIM3, and image suppression, reducing the yield loss because of process variations. On-chip test tones are generated using a 10-bit 2-GS/s cu...\"",
        "Document: \"Design of an Interconnect Architecture and Signaling Technology for Parallelism in Communication. The need for efficient interconnect architectures beyond the conventional time-division multiplexing (TDM) protocol-based interconnects has been brought on by the continued increase of required communication bandwidth and concurrency of small-scale digital systems. To improve the overall system performance without increasing communication resources and complexity, this paper presents a cost-effective interconnect architecture, communication protocol, and signaling technology that exploits parallelism in board-level communication, resulting in shorter latency and higher concurrency on a shared bus or link: the proposed source synchronous CDMA interconnect (SSCDMA-I) enables dual concurrent transactions on a single wire line as well as flexible input/output (I/O) reconfiguration. The SSCDMA-I utilizes 2-bit orthogonal CDMA coding and a variation of source synchronous clocking for multilevel superposition; a single 3-level SSCDMA-I line operates as if it consists of dual virtual time-multiplexed interconnects, which exploits communication parallelism with a reduced number of pins, wires, and complexity. The unique multiple access capability of the SSCDMA-I improves real-time communication between multiple semiconductor intellectual property (IP) blocks on a shared link or bus by reducing the bus contention interference from simultaneous traffic requests and by taking advantage of shorter request latency. The prototype transceiver chip is implemented in 0.18-m CMOS and the 10-cm test PC board system achieves an aggregate data rate of 2.5 Gb/s/pin between four off-chip (2Tx-to-2Rx) I/Os.\"",
        "Document: \"A low-PDP and low-area repeater using passive CTLE for on-chip interconnects. This paper presents an improved repeater circuit that preserves the advantages of the inverter repeater and achieves a lower power, delay, and area by applying proper equalization. Designed and measured in 65nm CMOS technology, the proposed repeater achieves 44% lower power-delay product (PDP) while occupies 46% lower area.\"",
        "Document: \"RF/wireless-interconnect: The next wave of connectivity. In the era of the nanometer CMOS technology, due to stringent system requirements in power, performance and other fundamental\n physical limitations (such as mechanical reliability, thermal constraints, overall system form factor, etc.), future VLSI\n systems are relying more on ultra-high data rates (up to 100 Gbps/pin or 20 Tbps aggregate), scalable, re-configurable, highly\n compact and reliable interconnect fabric. To overcome such challenges, we first explore the use of multiband RF/wireless-interconnects\n which can communicate simultaneously through multiple frequency bands with low power signal transmission, reconfigurable bandwidth\n and excellent mechanical flexibility and reliability. We then review recent advances in RF/wireless-interconnect in four different\n potential application domains, which include network-on-chips (NoCs), 3-dimensional integrated circuit (3DIC), advanced memory\n interface and ultra-high speed contactless connectors. Based on those developments, we further propose the future research\n direction on future inter- and intra-VLSI interconnect system through the comparison of performance and the proper communication\n range for all three types of interconnects, including communication data throughput, range and power consumption (pJ/bit)\n among the RF/wireless-interconnects, the optical interconnects and traditional parallel repeated bus.\"",
        "Document: \"A 600-MSPS 8-bit CMOS ADC Using Distributed Track-and-Hold With Complementary Resistor/Capacitor Averaging. An 8-bit 600 megasamples-per-second (MSPS) analog-to-digital converter (ADC) has been implemented in 0.18-mum CMOS to achieve a minimum signal-to-noise-and-distortion ratio (SNDR) of 40 dB and a spurious-free dynamic range (SFDR) of 45 dB with input-signal bandwidth up to 200 MHz. The ADC is also capable of sampling up to 1 gigasamples/s and maintaining 39-dB SNDR at an input-signal frequency of 5...\"",
        "Document: \"Invited - A 2.2 GHz SRAM with high temperature variation immunity for deep learning application under 28nm. With the coming era of Big Data, hardware implementation of machine learning has become attractive for many applications, such as real-time object recognition and face recognition. The implementation of machine learning algorithms needs intensive memory access, and SRAM is critical for the overall performance. This paper proposes a new design of high speed SRAM for machine learning purposes. With fast access time (cycle time: 650 ps, access time: 350 ps), low sensitivity to temperature variation and high configurability (less than 10% performance difference between 125_rcw_tt vs 0_rcw_tt), the proposed SRAM is a better candidate for hardware machine learning system than the conventional SRAM. Compared with Samsung HL 152, our design has smaller size (121\u00d743 um2 vs 127\u00d744 um2) with half the number of pins ports (12 vs 25) and higher speed (2.2GHz vs 0.8GHz).\"",
        "Document: \"A 40-mW 7-bit 2.2-GS/s Time-Interleaved Subranging CMOS ADC for Low-Power Gigabit Wireless Communications. A 7-bit, 2.2-GS/s time-interleaved subranging CMOS analog-to-digital converter (ADC) for low-power gigabit wireless communication system-on-a-chip (SoC) is presented. A time-splitting subranging architecture is invented to significantly boost the speed of individual ADC channels. In addition, a low-power and fast-settling distributed resistor array for reference voltages is proposed to mitigate ga...\"",
        "1 is \"Circuits Techniques And Microsystems Assembly For Intracortical Multichannel Eng Recording\", 2 is \"PSP array processing for multipath fading channels\"",
        "Given above information, for an author who has written the paper with the title \"Lower-Complexity Layered Belief-Propagation Decoding Of Ldpc Codes\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "006956": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Adversarial Feature Selection Against Evasion Attacks.':",
        "Document: \"Access control by RFID and face recognition based on neural network. Radio frequency identification (RFID) technology has been widely adopted in access control system. However, the people holding the RFID card passing through the access control may not be the authorized one. Therefore, an access control system combining RFID technology and face recognition based on neural network is presented in this work. The system recognizes the face of the person holding the RFID card and denies access if they do not match. We adopt a Radial Basis Function Neural Network (RBFNN) to learn the face of authorized card holders and save the parameters of RBFNN only. This could reduce storage when the number of card holders getting large. Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA) features are extracted to reduce the dimensions of face image data. The Localized Generalization Error Model (L-GEM) is adopted to train a RBFNN for better generalization capability. The face recognition system is first evaluated by benchmarking ORL face image database. The whole access control system is then tested in a real environment. Experimental results show that the proposed method has a good performance and could improve the security of RFID access control.\"",
        "Document: \"Intelligent book positioning for library using RFID and book spine matching. In library, the management of books is very complication and timing costing. The location of books could be altered by librarian, students, teachers and any one around the library. Therefore, allocating a book is not an easy task in big library. Indoor positioning is an important technology to help storage management and customer services providing. RFID provides a good wireless platform to facilitate indoor positioning. However, duo to the small width of each book spine, adopting positioning based RFID alone is not enough to locate books in a library. In this work, we combine image matching with L-GEM based RBFNN to enhance the accuracy and robustness of the book locating system. We apply this new method in a library environment to position the certain books. Experimental results show that the proposed method is highly accurate and robust to white noise of RFID signals. \u00a9 2011 IEEE.\"",
        "Document: \"One-and-a-Half-Class Multiple Classifier Systems for Secure Learning Against Evasion Attacks at Test Time. Pattern classifiers have been widely used in adversarial settings like spam and malware detection, although they have not been originally designed to cope with intelligent attackers that manipulate data at test time to evade detection. While a number of adversary-aware learning algorithms have been proposed, they are computationally demanding and aim to counter specific kinds of adversarial data manipulation. In this work, we overcome these limitations by proposing a multiple classifier system capable of improving security against evasion attacks at test time by learning a decision function that more tightly encloses the legitimate samples in feature space, without significantly compromising accuracy in the absence of attack. Since we combine a set of one-class and two-class classifiers to this end, we name our approach one-and-a-half-class (1.5C) classification. Our proposal is general and it can be used to improve the security of any classifier against evasion attacks at test time, as shown by the reported experiments on spam and malware detection.\"",
        "Document: \"Stock investment decision support for Hong Kong market using RBFNN based candlestick models. Candlestick pattern, as an efficient method in technical analysis, is widely used in decision support of stock investment. From historical data, there is a no 100% guarantee for a stock price increasing after the appearance of a bullish candlestick pattern. The main aim of this paper is to enhance the prediction ability of Candlestick Patterns using a Multiple Classifier System (MCS) consisting of Radial Basis Function Neural Network (RBFNN) trained by a Localized Generalization Error Model (L-GEM). The RBFNN classifies particular candlestick pattern to be a real bullish candlestick pattern or not based on training with past data. The MCS fusing RBFNN for different patterns makes the final prediction of the stock price trend. In this paper, stock price data of 40 stocks in Hong Kong Hang Seng Component Index is collected to carry out the investment simulation experiment. Experimental result shows that the proposed method yields statistically significant profit when compared with a random investment strategy and candlestick investment without RBFNN. \u00a9 2011 IEEE.\"",
        "Document: \"Empirical study on the architecture selection of RBFNN using L-GEM for multi-class problems. Generalization ability is very important for pattern recognition and classification. However, the generalization error cannot be computed directly because we do not know the real input distribution and classifications of unseen samples. The Localized Generalization Error Model (L-GEM) was proposed to provide an upper bound of generalization error for unseen samples similar to training samples. The L-GEM upper bound (R* SM) is computed for each output neuron of a Radial Basis Function Neural Network (RBFNN). For a multi-class classification problem, there are more than one output neurons. For a K-class problem, there will be K L-GEM values, i.e. one for each output neuron. How to use these K L-GEM values to select the architecture of a RBFNN is still an open problem. One could use average, maximum and minimum value among these K L-GEM values to estimate the overall performance of the RBFNN under investigation. All three of them are reasonable and provide some information about the generalization capability of the RBFNN. In this work, we empirically examine these three fusion methods for using L-GEM to select RBFNN architectures for four UCI datasets. Experimental results show that maximum and average fusion methods perform well. \u00a9 2011 IEEE.\"",
        "Document: \"Blind steganalysis with high generalization capability for different image databases using L-GEM. Steganography hides secret messages in images (stego images) and create a huge security thread to society. In contrast, steganalysis is a technique to determine whether there are secret messages being embedded in images. Differences in image databases have great influences to the performance of steganalysis. In real world applications, images from different sources could have large differences and it is impossible to train the classifier with all image databases available on the Internet. Therefore, a steganalysis system generalizing well with respect to differences among different image databases is important to real applications. In this paper, we expand the Markov features and apply L-GEM based neural network in our method to enhance the generalization capability of steganalysis. Experimental results show that the generalization capability of our method is noticeably better than the existing steganalysis for different training and testing image databases. \u00a9 2011 IEEE.\"",
        "Document: \"Dynamic hierarchical semantic network based image retrieval using relevance feedback. In order to improve the retrieval accuracy of image retrieval, semantic-based image retrieval has become popular in the recent years. It is because this kind of retrieval method could narrow semantic gap between the visual features and the high-level semantic features. However, most of the existing methods of the semantic-based image retrieval are limited to fixed number of semantic features. A dynamic hierarchical semantic network method is proposed in this paper to overcome this limitation. The proposed dynamic hierarchical semantic network is constructed by relevance feedback. The number of semantic features can be dynamically changed according to user feedbacks. Moreover, the semantic features are allowed to have different levels of abstraction. In addition, the proposed method also integrates low-level visual feature-based image retrieval style, which could full use of the advantages of visual feature-based image retrieval and semantic-based image retrieval. Experimental results show that the proposed method achieves higher retrieval accuracy than fixed number of semantic feature method and only one level semantic feature method. \u00a9 2011 IEEE.\"",
        "Document: \"RFID reader deployment using RBFNN with L-GEM. RFID technology has been deployed in a wide range of industries. The deployment of RFID readers is essential to the success of RFID applications. RFID is a wireless communication technology and therefore the interaction between the RFID tag and reader is easily influenced by many factors. Current RFID reader deployment based on designer experience or predefined mathematical model without field test. When the RFID system is fielded, the performance may be unsatisfactory. Therefore, we propose a neural network based method to select the best location of RFID reader deployment among possible choices in the field. The major factor affecting the performance of neural network is the architecture selection which we will select via a minimization of its localized generalization error to enhance the neural network's generalization capability. Radial basis function neural network is adapted owing to its fast learning speed. The neural network will suggest the best location iteratively to reduce the number of cost and labor intensive random trials. Experimental results on an artificial warehouse show that the proposed method outperforms random trials.\"",
        "Document: \"L-GEM based RBFNN for news anchorperson detection with Dominant Color Descriptor. News reports in TV provide important and timely information about the city and the world to citizens. Moreover, data mining and indexing of news video clips provide a good source of information. However, news video usually consists of more than one news story. One must split them into individual news before indexing. Owing to the nature of news reports, the news anchorperson usually appears in the transition of two news story. Therefore, we propose a new method to find the news anchorperson shot in news video. The MPEG-7 Dominant Color Descriptor (DCD) is adopted to describe video frames. Radial Basis Function Neural Network (RBFNN) trained by minimizing the Localized Generalization Error (L-GEM) is adopted to classify the occurrence of news anchorperson in video frames. Experimental results show that the proposed method is accurate for different news videos from different TV stations.\"",
        "Document: \"Adversarial Feature Selection Against Evasion Attacks. Pattern recognition and machine learning techniques have been increasingly adopted in adversarial settings such as spam, intrusion, and malware detection, although their security against well-crafted attacks that aim to evade detection by manipulating data at test time has not yet been thoroughly assessed. While previous work has been mainly focused on devising adversary-aware classification algorithms to counter evasion attempts, only few authors have considered the impact of using reduced feature sets on classifier security against the same attacks. An interesting, preliminary result is that classifier security to evasion may be even worsened by the application of feature selection. In this paper, we provide a more detailed investigation of this aspect, shedding some light on the security properties of feature selection against evasion attacks. Inspired by previous work on adversary-aware classifiers, we propose a novel adversary-aware feature selection model that can improve classifier security against evasion attacks, by incorporating specific assumptions on the adversary's data manipulation strategy. We focus on an efficient, wrapper-based implementation of our approach, and experimentally validate its soundness on different application examples, including spam and malware detection.\"",
        "1 is \"Evasion Attacks against Machine Learning at Test Time.\", 2 is \"How to win an evolutionary arms race\"",
        "Given above information, for an author who has written the paper with the title \"Adversarial Feature Selection Against Evasion Attacks.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007040": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Robust H-infinity control for uncertain time-delay TCP/AQM network system':",
        "Document: \"Notice of Violation of IEEE Publication Principles Extended target tracking using an IMM based nonlinear Kalman filters. Notice of Violation of IEEE Publication Principles \"Extended Target Tracking Using an IMM Based Nonlinear Kalman Filters,\" by Yucheng Zhou, Jiahe Xu, Yuanwei Jing, Georgi M. Dimirovski in the Proceedings of the 2010 American Control Conference, July 2010, pp. 6870-6875 After careful and considered review of the content and authorship of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles. This paper contains significant portions of original text from the paper cited below. The original text was copied without attribution (including appropriate references to the original author(s) and/or paper title) and without permission. Due to the nature of this violation, reasonable effort should be made to remove all past references to this paper, and future references should be made to the following article: \"Extended Target Tracking Using an IMM based Rao-Blackwellised Unscented Kalman Filter,\" by Zhiwen Zhong; Huadong Meng; Xiqin Wang in the Proceedings of the 9th International Conference on Signal Processing, October 2008, pp.2409-2412 The unscented Kalman filter (UKF) and ensemble Kalman filter (EnKF) are developed to extended target tracking problem for high resolution sensors. The nonlinear Kalman filters are based on an ellipsoidal model, which is proposed to exploit sensor measurement of target extent. The ellipsoidal model can provide extra information to enhance tracking accuracy, data association performance, and target identification. In contrast to the most commonly used extended Kalman filter (EKF), the UKF and EnKF provide more accurate and reliable estimation performance, due to the presence of high nonlinearity of the model. Correspondingly, the EnKF has lower computational complexity than the UKF. An interacting multiple model (IMM) technique is combined with the filters to adapt the target maneuver and motion mode switching pro- lem which is vital for nonlinear filtering. The developed IMM-UKF and IMM-EnKF algorithms on extended target tracking problem are validated and evaluated by computer simulations.\"",
        "Document: \"Optimal Research and Numerical Simulation for Scheduling No-Wait Flow Shop in Steel Production. This paper considers the m-machine flow shop scheduling problem with the no-wait constraint to minimize total completion time which is the typical model in steel production. First, the asymptotic optimality of the Shortest Processing Time (SPT) first rule is proven for this problem. To further evaluate the performance of the algorithm, a new lower bound with performance guarantee is designed. At the end of the paper, numerical simulations show the effectiveness of the proposed algorithm and lower bound.\"",
        "Document: \"Unscented Kalman-Bucy filtering for nonlinear continuous-time systems with multiple delayed measurements. The unscented Kalman-Bucy filter (UKBF) is developed to nonlinear continuous-time systems with multiple delayed measurements. An explicit and simpler solution to the unscented Kalman-Bucy filtering problem is presented for such systems. The approach applied is the reorganized innovation analysis. The obtained UKBF is given in terms of Riccati differential equations. A numerical example is given to demonstrate the proposed approach. \u00a9 2010 AACC.\"",
        "Document: \"Output-feedback stabilization for stochastic nonlinear systems whose linearizations are not stabilizable. This brief paper investigates the problem of output-feedback stabilization for a class of high-order stochastic nonlinear systems which are neither necessarily feedback linearizable nor affine in the control input. Based on the ideas of the homogeneous systems theory and the adding a power integrator technique, an output-feedback controller is constructed to ensure that the equilibrium at the origin of the closed-loop system is globally asymptotically stable (GAS) in probability. The efficiency of the output-feedback controller is demonstrated by a simulation example.\"",
        "Document: \"Adaptive Finite-Time Command Filtered Controller Design for Nonlinear Systems With Output Constraints and Input Nonlinearities. This work addresses a finite-time tracking control issue for a class of nonlinear systems with asymmetric time-varying output constraints and input nonlinearities. To guarantee the finite-time convergence of tracking errors, a novel finite-time command filtered backstepping approach is presented by using the command filtered backstepping technique, finite-time theory, and barrier Lyapunov functions. The newly proposed method can not only reduce the complexity of computation of the conventional backstepping control and compensate filtered errors caused by dynamic surface control but also can ensure that the output variables are restricted in compact bounding sets. Moreover, the proposed controller is applied to robot manipulator systems, which guarantees the practical boundedness of all the signals in the closed-loop system. Finally, the effectiveness and practicability of the developed control strategy are validated by a simulation example.\"",
        "Document: \"Synchronization of a class of dynamical complex networks with nonsymmetric coupling based on decentralized control. In this paper, the synchronization problem for dynamical complex networks composed of general Lur'e systems is dealt with. Based on the Jordan canonical transformation method and a Lur'e-Postnikov function, the global synchronization criteria for dynamical complex networks with non-symmetric coupling are established and formulated as LMI. A decentralized control strategy based on projection lemma and Lur'e-Postnikov function is proposed in order to reduce the conservativeness. A dynamical complex network composed of identical Chua's oscillators is adopted as a numerical example to demonstrate the effectiveness of the proposed results.\"",
        "Document: \"Dynamic output feedback for T-S fuzzy model based on chaotic systems with uncertainties. In this paper, a novel Takagi-Sugeno (T-S) fuzzy model-based method is proposed for controlling a class of chaotic systems with uncertain parameters. The T-S fuzzy model is employed for accurately modeling the chaotic systems. Based on the T-S fuzzy model, the parallel distributed compensation (PDC) technique is applied to design a dynamic output feedback controller for the chaotic systems. By using Lyapunov stability theory, the robust stability of the closed-loop system is proved. The condition in this paper is represented in the form of linear matrix inequalities (LMIs). The new method considers the interactions among all fuzzy subsystems. Finally, as an illustrative example, Chen system is considered to verify the effectiveness of the control scheme.\"",
        "Document: \"An LMI approach to slip ratio control of vehicle Antilock braking systems. In this paper, the state equation for the dynamics of quarter car is established, and the nonlinear Antilock braking system is transformed into linear uncertain system model. So the stability problem of nonlinear Antilock braking systems becomes the robust stability problem of linear uncertain systems. Sliding mode control approach is employed to guarantee robust stability of linear uncertain systems. The stable sliding surface is designed by using linear matrix inequalities (LMI) to reduce the influence of mismatched uncertainties. Moreover the design of sliding mode control law is presented also. The system robust stability can be guaranteed and the chattering around the sliding surface in sliding mode control is obviously reduced by the proposed approach.\"",
        "Document: \"Fault-Tolerant control of a class of switched nonlinear systems with application to flight control. This paper considers robust fault-tolerant control problem of a class of uncertain switched nonlinear systems. A new state feedback fault-tolerant control method is proposed for global stabilization of the nonlinear switched systems against actuator faults and structure uncertainties. Compared with the existing results on fault-tolerant control of switched systems, this paper mainly features: 1) the proposed controller can stabilize a class of nonlinear systems with actuator faults and its nominal systems (i.e., without actuator faults) without necessarily changing any structures and/or parameters of the proposed controllers; 2) the proposed method treats all actuators in a unified way without necessarily classifying all actuators into faulty actuators and healthy ones; 3) the proposed method is independent on arbitrary switching polices. Simulation studies on a numerical example and on the longitudinal dynamics of an F-18 aircraft operating on different heights show the effectiveness of the proposed method.\"",
        "Document: \"AQM algorithm based on Kelly's scheme using sliding mode control. This paper deals with the congestion control problem for queues in TCP/IP networks. In order to improve the congestion control performance for queues, based on the utility optimization source model proposed by Kelly, the linear and terminal sliding active queue management (AQM) algorithms are designed. Especially in the terminal sliding AQM algorithm, a special nonlinear terminal sliding surface is proposed in order to force queue length in router to reach the desired value in finite time. The upper bound of the time is also obtained. Simulation results demonstrate that the proposed sliding mode AQM controllers can obviously improve the performance of congestion control for queue length in routers.\"",
        "1 is \"On-line path planning in an unknown polygonal environment\", 2 is \"On H\u221e control for linear systems with interval time-varying delay\"",
        "Given above information, for an author who has written the paper with the title \"Robust H-infinity control for uncertain time-delay TCP/AQM network system\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007044": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Incremental learning of feature space and classifier for face recognition.':",
        "Document: \"Incremental training of support vector machines using truncated hypercones. We discuss incremental training of support vector machines in which we approximate the regions, where support vector candidates exist, by truncated hypercones. We generate the truncated surface with the center being the center of unbounded support vectors and with the radius being the maximum distance from the center to support vectors. We determine the hypercone surface so that it includes a datum, which is far away from the separating hyperplane. Then to cope with non-separable cases, we shift the truncated hypercone along the rotating axis in parallel in the opposite direction of the separating hyperplane. We delete the data that are in the truncated hypercone and keep the remaining data as support vector candidates. In computer experiments, we show that we can delete many data without deteriorating the generalization ability.\"",
        "Document: \"Convergence Improvement Of Active Set Support Vector Training. In our previous work we have developed an active set training method of L2 support vector machines (SVMs) using Newton's method. Because the method allows a solution to be infeasible during training, convergence of the method is not guaranteed. In this paper, we guarantee convergence of active set training by limiting the corrections under the constraints when slow convergence is detected. Namely, we start training the L2 SVM with a subset of training data, delete non-positive dual variables from the working set as well as the variables with margins larger than or equal to 1, add violating variables to the working set, and repeat training. We monitor the number of violation fluctuations and if it exceeds a specified value, we obtain a feasible solution prohibiting addition of violating variables. Then for a feasible solution, we start active set training limiting the corrections within a feasible solution. By computer experiments, we show that the proposed training method is faster and more stable than the previous method.\"",
        "Document: \"Feature Selection And Fast Training Of Subspace Based Support Vector Machines. In this paper, we propose two methods for subspace based support vector machines (SS-SVMs) which are subspace based least squares support vector machines (SSLS-SVMs) and subspace based linear programming support vector machines (SSLP-SVMs): 1) optimum selection of the dictionaries of each class subspace from the standpoint of classification separability, and 2) speeding up training SS-SVMs. In method 1), for SSLS-SVMs, we select the dictionaries with optimized weights, and for SSLP-SVMs, we select the dictionaries without non-negative constraints. In method 2), the empirical feature space is obtained by using only the training data belonging to a class instead of using all the training data. Thus the dimension of the empirical feature space and training cost become lower. We demonstrate the effectiveness of the proposed methods over the conventional method for two-class bench mark datasets.\"",
        "Document: \"KPCA-based training of a kernel fuzzy classifier with ellipsoidal regions. In a fuzzy classifier with ellipsoidal regions, a fuzzy rule, which is based on the Mahalanobis distance, is defined for each class. Then the fuzzy rules are tuned so that the recognition rate of the training data is maximized. In most cases, one fuzzy rule per one class is enough to obtain high generalization ability. But in some cases, we need to partition the class data to define more than one rule per class.\"",
        "Document: \"A novel approach to feature selection based on analysis of class regions. This paper presents a novel approach to feature selection based on analysis of class regions which are generated by a fuzzy classifier. A measure for feature evaluation is proposed and is defined as the exception ratio. The exception ratio represents the degree of overlaps in the class regions, in other words, the degree of having exceptions inside of fuzzy rules generated by the fuzzy classifier. It is shown that for a given set of features, a subset of features that has the lowest sum of the exception ratios has the tendency to contain the most relevant features, compared to the other subsets with the same number of features. An algorithm is then proposed that performs elimination of irrelevant features. Given a set of remaining features, the algorithm eliminates the next feature, the elimination of which minimizes the sum of the exception ratios. Next, a terminating criterion is given. Based on this criterion, the proposed algorithm terminates when a significant increase in the sum of the exception ratios occurs due to the next elimination. Experiments show that the proposed algorithm performs well in eliminating irrelevant features while constraining the increase in recognition error rates for unknown data of the classifiers in use.\"",
        "Document: \"Sparse least squares support vector regressors trained in the reduced empirical feature space. In this paper we discuss sparse least squares support vector regressors (sparse LS SVRs) defined in the reduced empirical feature space, which is a subspace of mapped training data. Namely, we define an LS SVR in the primal form in the empirical feature space, which results in solving a set of linear equations. The independent components in the empirical feature space are obtained by deleting dependent components during the Cholesky factorization of the kernel matrix. The independent components are associated with support vectors and controlling the threshold of the Cholesky factorization we obtain a sparse LS SVM. For linear kernels the number of support vectors is the number of input variables at most and if we use the input axes as support vectors, the primal and dual forms are equivalent. By computer experiments we show that we can reduce the number of support vectors without deteriorating the generalization ability.\"",
        "Document: \"Training of support vector machines with Mahalanobis kernels. Radial basis function (RBF) kernels are widely used for support vector machines. But for model selection, we need to optimize the kernel parameter and the margin parameter by time-consuming cross validation. To solve this problem, in this paper we propose using Mahalanobis kernels, which are generalized RBF kernels. We determine the covariance matrix for the Mahalanobis kernel using the training data corresponding to the associated classes. Model selection is done by line search. Namely, first the margin parameter is optimized and then the Mahalanobis kernel parameter is optimized. According to the computer experiments for two-class problems, a Mahalanobis kernel with a diagonal covariance matrix shows better generalization ability than a Mahalanobis kernel with a full covariance matrix, and a Mahalanobis kernel optimized by line search shows comparable performance with that with an RBF kernel optimized by grid search.\"",
        "Document: \"Fast Training of Support Vector Machines by Extracting Boundary Data. Support vector machines have gotten wide acceptance for their high generalization ability for real world applications. But the major drawback is slow training for classification problems with a large number of training data. To overcome this problem, in this paper, we discuss extracting boundary data from the training data and train the support vector machine using only these data. Namely, for each training datum we calculate the Mahalanobis distances and extract those data that are misclassified by the Mahalanobis distances or that have small relative differences of the Mahalanobis distances. We demonstrate the effectiveness of the method for the benchmark data sets.\"",
        "Document: \"Improved parameter tuning algorithms for fuzzy classifiers. We propose two methods for tuning membership functions of a fuzzy classifier by the support-vector-machine (SVM) like training. For each class, we define a membership function in the feature space. In the first method, we tune the slopes of the membership functions so that the margin between classes is maximized. This method is similar to a linear all-at-once SVM. We call this AAO tuning. In the second method, for each class the membership function is tuned so that the margin between the class and the remaining classes are maximized. This method is similar to a linear one-against-all SVM. This is called OAA tuning. According to the computer experiment, the kernel-discriminant-analysis (KDA) based fuzzy classifiers tuned by AAO tuning and by OAA tuning and SVM show comparable classification performance.\"",
        "Document: \"Fusing sequential minimal optimization and Newton\u2019s method for support vector training. Sequential minimal optimization (SMO) is widely used for training support vector machines (SVMs) because of fast training. But the training slows down when a large margin parameter value is used. Training by Newton\u2019s method (NM) accelerates training in such a situation but it slows down for a small margin parameter value. To solve this problem, in this paper we fuse SMO with NM and call it SMO-NM. Because slow training is caused by repetitive corrections of the same variables, we modify the working set selection when they are detected. We call the variables that are selected by SMO, SMO variables. At the current step, if a variable selected as an SMO variable was selected in a previous step, we consider that a loop is detected. And in addition to the SMO variables, we add, to the working set, the unbounded variables that were selected as SMO variables and correct the variables by NM. If no loop is detected, the training procedure is the same as that of SMO. As a variant of this working set strategy, we further add violating variables to the working set. We clarify that if the classification problem is not linearly separable in the feature space, the solutions of L1/L2 SVMs (with the linear sum/square sum of slack variables) are unbounded as the margin parameter value approaches infinity but that, if the mapped training data are not linearly independent in the feature space, the solution of the least squares SVM is unbounded as the margin parameter approaches infinity. We also clarify the condition, in which the increment of the objective function value by SMO-NM is larger than that by SMO. We evaluate SMO-NM for several benchmark data sets and confirm the effectiveness over SMO especially for a large margin parameter value.\"",
        "1 is \"Fast multiresolution image querying\", 2 is \"The Random Subspace Method for Constructing Decision Forests\"",
        "Given above information, for an author who has written the paper with the title \"Incremental learning of feature space and classifier for face recognition.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007060": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Drawing attention to the dangerous':",
        "Document: \"Voting Advice Applications: Missing Value Estimation Using Matrix Factorization And Collaborative Filtering. A Voting Advice Application (VAA) is a web application that recommends to a voter the party or the candidate, who replied like him/her in an online questionnaire. Every question is responding to the political positions of each party. If the voter fails to answer some questions, it is likely the VAA to offer him/her the wrong candidate. Therefore, it is necessary to inspect the missing data (not answered questions) and try to estimate them. In this paper we formulate the VAA missing value problem and investigate several different approaches of collaborative filtering to tackle it. The evaluation of the proposed approaches was done by using the data obtained from the Cypriot presidential elections of February 2013 and the parliamentary elections in Greece in May, 2012. The corresponding datasets are made freely available to other researchers working in the areas of VAA and recommender systems through the Web.\"",
        "Document: \"Exploring the Time Course of Facial Expressions with a Fuzzy System. Recognizing facial expressions is one of the important challenges of current research in Human-Computer Interaction (HCI). Previous research show the limits of recognition based on a single static image, and analyzing video sequences seems more promising. We explore here three fuzzy systems for the classification of basic facial expressions and compare their performances with a template-correlation approach. We then use those to examine the time course dynamics of facial expressions. The system's inputs are the relative variations of distances defined by salient facial points from one frame to the next. For maximum compatibility, those facial points (eyebrows, eyes, mouth) were chosen from the set of points defined in the standard MPEG-4 specifications, and so that their automatic extraction is tractable. The first results suggest that some expressions can be recognized early after the onset. For other expressions, it is in general possible to reduce significantly the number of possibilities. Forming early hypotheses regarding the expression could be necessary for a system to work in real-time, since other steps may have to follow: prediction of user's action, choice of computer's action, etc... This also has implications for the recognition of milder expressions.\"",
        "Document: \"Automatic annotation of image databases based on implicit crowdsourcing, visual concept modeling and evolution. In this paper a novel approach for automatically annotating image databases is proposed. Despite most current schemes that are just based on spatial content analysis, the proposed method properly combines several innovative modules for semantically annotating images. In particular it includes: (a) a GWAP-oriented interface for optimized collection of implicit crowdsourcing data, (b) a new unsupervised visual concept modeling algorithm for content description and (c) a hierarchical visual content display method for easy data navigation, based on graph partitioning. The proposed scheme can be easily adopted by any multimedia search engine, providing an intelligent way to even annotate completely non-annotated content or correct wrongly annotated images. The proposed approach currently provides very interesting results in limited-size both standard and generic datasets and it is expected to add significant value especially to billions of non-annotated images existing in the Web. Furthermore expert annotators can gain important knowledge relevant to user new trends, language idioms and styles of searching.\"",
        "Document: \"Semantic Gap between People: An Experimental Investigation Based on Image Annotation. Image annotation still remains the method of preference in multimedia search despite the development of many content-based multimedia retrieval platforms. Manual annotation is an extremely labour-intensive and time consuming task while the annotation expresses the view of a particular annotator at a specific context and time. Although the semantic gap has attracted large amount of research interest, the age and gender gaps in manual annotation have not been examined in detail. The aim of this study was to explore the gender and age differences in (1) the way of annotating images and, (2) the inter-annotator agreement. Our questionnaire based survey was conducted using 40 Cypriot citizens divided into two age groups who were asked to annotate an image dataset using a vocabulary of 52 keywords. Our results indicate that there are age differences in the way people annotate images, while the gender differences are smaller than our assumptions. Furthermore, there is an adequate agreement among participants for both age and gender groups.\"",
        "Document: \"Combining GAs and RBF neural networks for fuzzy rule extraction from numerical data. The idea of using RBF neural networks for fuzzy rule extraction from numerical data is not new. The structure of this kind of architectures, which supports clustering of data samples, is favorable for considering clusters as ifthen rules. However, in order for real if-then rules to be derived, proper antecedent parts for each cluster need to be constructed by selecting the appropriate subspace of input space that best matches each cluster's properties. In this paper we address the problem of antecedent part construction by (a) initializing the hidden layer of an RBF-Resource Allocating Network using an unsupervised clustering technique whose metric is based on input dimensions that best relate the data samples in a cluster, and (b) by pruning input connections to hidden nodes in a per node basis, using an innovative Genetic Algorithm optimization scheme.\"",
        "Document: \"Human action annotation, modeling and analysis based on implicit user interaction. This paper proposes an integrated framework for analyzing human actions in video streams. Despite most current approaches that are just based on automatic spatiotemporal analysis of sequences, the proposed method introduces the implicit user-in-the-loop concept for dynamically mining semantics and annotating video streams. This work sets a new and ambitious goal: to recognize, model and properly use \"average user's\" selections, preferences and perception, for dynamically extracting content semantics. The proposed approach is expected to add significant value to hundreds of billions of non-annotated or inadequately annotated video streams existing in the Web, file servers, databases etc. Furthermore expert annotators can gain important knowledge relevant to user preferences, selections, styles of searching and perception.\"",
        "Document: \"Investigating The Scalability Of Algorithms, The Role Of Similarity Metric And The List Of Suggested Items Construction Scheme In Recommender Systems. The continuous increase in demand for new products and services on the market brought the need for systematic improvement of recommendation technologies. Recommender systems proved to be the answer to the data overload problem and an advantage for e-business. Nevertheless, challenges that recommender systems face, like sparsity and scalability, affect their performance in real-world situations where both the number of users and items are high and item rating is infrequent. In this article we propose a cluster based recommendation approach using genetic algorithms. Users are grouped into clusters based on their past choices and preferences and receive recommendations from the other cluster members with the aid of an innovative recommendation scheme called Top-N voted items. Similarity between users is computed using the max_norm Pearson coefficient. This is a modified form of the widely used Pearson coefficient and it is used to prevent very active users dominating recommendations. We compare our approach with five well established recommendation methods with the aid of three different datasets. These datasets vary in terms of the number of users, the number of items, and the sparsity of ratings. As a result important conclusions are drawn about the efficiency of each method with respect to scalability and dataset's sparsity.\"",
        "Document: \"Modeling User Networks in Recommender Systems. Recommender systems, in the collaborative filtering variation, are popular tools used to drive users out of information clutter, by letting them select \u00c2\u00bfinteresting\u00c2\u00bf items based on the preferences of similarly minded users. In such a system as more users come in to evaluate items (be they information pieces, products or otherwise), a network of users starts to be formed. In this paper we are interested in the dynamics of such a network, in particular we investigate if there is a hidden law that captures the essence of such networks irrespective of their size. The discovery of such a law would allow, among other usages, generation of synthetic data sets, realistic enough to be used for simulation purposes. Furthermore, it would be useful for information-seeking activities such as locating known experts or influential users on a particular subject. Similar work in related fields suggested the existence of power-laws, which seem to be ubiquitous. However, in our work we did not detect the presence of such a law, instead we discovered an exponential relationship between the nodes of a graph representing users, and edges representing similarity between users. In particular the logarithm of the degree of node is linearly related to the ranking of the node in a decreasing order. The above conclusion is justified by extended experiments on two versions of the movie lens data set (one comprised 100,000 user evaluations, while the other comprised 1,000,0000 evaluations).\"",
        "Document: \"Object Classification Using the MPEG-7 Visual Descriptors: An Experimental Evaluation Using State of the Art Data Classifiers. MPEG-7 visual descriptors include the color, texture and shape descriptor and were introduced, after a long period of evaluation, for efficient content-based image retrieval. A total of 22 different kind of features are included, nine for color, eight for texture and five for shape. Encoded values of these features vary significantly and their combination, as a means for better retrieval, is neither straightforward nor efficient. Despite their extensive usage MPEG-7 visual descriptors have never compared concerning their retrieval performance; thus the question which descriptor to use for a particular image retrieval scenario stills unanswered. In this paper we report the results of an extended experimental study on the efficiency of the various MPEG-7 visual features with the aid of the Weka tool and a variety of well-known data classifiers. Our data consist of 1952 images from the athletics domain, containing 7686 manually annotated objects corresponding to eight different classes. The results indicate that combination of selected MPEG-7 visual features may lead to increased retrieval performance compared to single descriptors but this is not a general fact. Furthermore, although the models created using alternative training schemes have similar performance libSVM is by far more effective in model creation in terms of training time and robustness to parameter variation.\"",
        "Document: \"Classifying Images from Athletics Based on Spatial Relations. Spatial relations between image regions are used in this paper for image classification in a rule-based fash- ion. In the particular case where image regions correspond to semantically interpretable objects the rules provide the means for justifying classification in a human-familiar man- ner. In the work presented here instances of particular ob- ject classes are detected combining bottom-up (learnable models based on simple features) and top-down informa- tion(object models consisting of primitive geometric shapes such as lines). The rule-based system acts as a model for the spatial configuration of objects. Experimental results in the athletic domain show that despite inaccuracy in object detection, spatial relations allow for efficient discrimination between visually similar images classes.\"",
        "1 is \"Recommending twitter users to follow using content and collaborative filtering approaches\", 2 is \"Origins of a repetitive and co-contractive biphasic pattern of muscle activation in Parkinson's disease.\"",
        "Given above information, for an author who has written the paper with the title \"Drawing attention to the dangerous\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007106": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Reducing conservativeness in stability conditions of affine fuzzy systems using fuzzy Lyapunov function':",
        "Document: \"Inter-MARIO: A Fast and Seamless Mobility Protocol to Support Inter-Pan Handover in 6LoWPAN. Mobility management is one of the most important research issues in 6LoWPAN, which is a standardizing IP-based Wireless Sensor Networks(IP-WSN) protocol. Since the IP-WSN application domain is expanded to real-time applications such as healthcare and surveillance systems, a fast and seamless handover becomes an important criterion for mobility support in 6LoWPAN. Unfortunately, since existing mobility protocols for 6LoWPAN have not solved how to reduce handover delay, we propose a new fast and seamless mobility protocol to support inter-PAN handover in 6LoWPAN, named inter-MARIO. In our protocol, a partner node, which serves as an access point for a mobile node, preconfigures the future handover of the mobile node by sending mobile node's information to candidate neighbor PANs and providing neighbor PAN information like channel information to the mobile node. Also, the preconfigured information enables the foreign agent to send a surrogate binding update message to a home agent instead of the mobile node. By the preconfiguration and surrogate binding update, inter-MARIO decreases channel scan delay and binding message exchange delay, which are elements of handover delay. Additionally, we define a compression method for binding messages, which achieves more compression than existing methods, by reducing redundant fields. We compare signaling cost and binding message exchange delay with existing mobility protocols analytically and we evaluate handover delay by simulation. Analysis and simulation results indicate that our approach has promising fast, seamless, and lightweight properties.\"",
        "Document: \"Bounded energy allocation and scheduling for real-time Embedded Systems. For energy-constrained real-time embedded systems, the power-delay tradeoff property of Volta ge-Clock Scaling (VCS) needs to be carefully considered in scheduling real-time tasks for meeting strict timing requirements. In addition, non-real-time tasks require minimized response times. Thus, a bounded energy allocation model should be adopted to solve a combination of optimization objectives in systems having mixed hard and soft real-time tasks. In this paper, we propose an energy sharing model that allocates the energy budget among hard and soft real-time tasks, exploiting the interplay between utilization and energy consumption in VCS-based earliest-deadline-first scheduling (VCS-EDF). Also a dynamic scheduling is presented which is designed to reduce energy consumption by switching between two scheduling policies and utilizing an explicit pattern of event occurrences at run-time. Through simulations, we show that this Dual-Policy Dynamic Scheduling can outperform in the reduction of energy consumption while introducing small delay in average response time of non-real-time tasks.\"",
        "Document: \"Intra-MARIO: A Fast Mobility Management Protocol for 6LoWPAN. One of the major challenges in 6LoWPAN is to provide continuous services while mobile nodes\u2019 movements with minimizing network inaccessible time caused due to handoffs. Even though MIPv6, HMIPv6, and PMIPv6 are commonly accepted standards to address this in IP networks, they cannot inherently avoid the degradation in communication quality during handoff, since they are not designed with consideration of constrained node networks like 6LoWPAN. In this paper, we propose a new fast mobility management protocol for 6LoWPAN, named intra-MARIO. To minimize handoff delay and enhance service availability, intra-MARIO introduces three important components, which are a fast rejoin scheme for handoff management with an adaptive polling based movement detection and multi-hop pointer forwarding schemes for location management. To justify the effectiveness, we have conducted extensive simulations by comparing intra-MARIO with prior schemes like a basic mobility management scheme and a PMIPv6-based protocol. We then implement intra-MARIO on top of our 6LoWPAN platform (SNAIL) and evaluate the performance of intra-MARIO. The results highlight that intra-MARIO reduces overall handoff delay with low power consumption and minimizes packet losses during handoffs, compared to prior mobility protocols.\"",
        "Document: \"A Context Middleware Based on Sensor and RFID Information. Pervasive computing environments have traditionally used distributed sensors to gather user context. EPC (Electronic Product Code) information extracted from RFID tags could additionally provide identification information associated with that context. However, the process of matching sensor and RFID information around the same context is not a trivial task. In our previous work, we proposed to move the sensors to the same objects that carry the RFID tags. By providing association capabilities among those objects using their EPCs, we can effectively build user and object context information while maintaining a global context identity. In this paper we analyze how the use of the EPC Network could greatly benefit our framework by providing the means for extracting, organizing and querying the context data. Additionally, we outline how services can be offered to the context data producers and how they can be built in a pervasive manner by linking them directly to the EPC Network infrastructure.\"",
        "Document: \"A user-steered guide robot for the blind. This study deals with a guide robot for blind people. User inputs (i.e., commands for movements) are obtained from blind people through a joystick, which has two potentiometers. Some noise unintentionally caused by the user is included in this input. To reject the unwanted signal, the robot has to recognize the intention of the user even with these noises. In order to make the robot follow the intention, an Omni wheel-based driving unit is attached under the joystick through a link. Although the Omni wheel moves easily along narrow curves, it is important to make the user's wrist or arm holding the joystick comfortable by planning proper motion trajectory. To do this, a fuzzy logic control is employed to identify the user's intention and determine movement velocity and direction of the robot from the joystick signal.\"",
        "Document: \"Rfms: Real-Time Flood Monitoring System With Wireless Sensor Networks. In this paper, we present RFMS, the Real-time Flood Monitoring System with wireless sensor networks, which is deployed in two volcanic islands Ulleung-do and Dok-do located in the East Sea near to the Korean Peninsula and developed for flood monitoring. RFMS measures river and weather conditions through wireless sensor nodes equipped with different sensors. Measured information is employed for early-warning via diverse types of services such as SMS (Short Message Service) and a web service.\"",
        "Document: \"Traffic-aware stateless multipath routing for fault-tolerance in IEEE 802.15. 4 wireless mesh networks. Single-path routing is widely used in wireless networks due to low resource consumption. However, it is vulnerable to link failure because such a failure may adversely affect an entire path. To overcome this, multipath routing has been proposed providing fault-tolerance. In this paper, we propose a novel multipath routing protocol called traffic-aware stateless multipath routing (TSMR) based on an overlaid tree topology comprising two topologies, namely, bounded degree tree (BDT) and root-oriented directional tree (RODT). BDT is strong on reducing routing overhead, and RODT is resilient against lossy links. By synergistically overlaying them, TSMR dynamically selects the local optimal path according to the given traffic flow and the failure on the primary path. In particular, TSMR enables stateless and low overhead routing despite multipath routing by keeping only one-hop neighbors to maintain multiple paths. To evaluate TSMR, we conducted simulations with a shadowing model reflecting lossy links, and compared with single and multipath routing protocols, such as ZTR, STR, AODV, and RPL. The simulation results show that the overall performance of TSMR surpasses that of others for packet delivery ratio, end-to-end delay, control overhead, memory consumption, and power consumption regardless of network size, number of sessions, and traffic flow.\"",
        "Document: \"Ripple flooding in wireless sensor networks. Flooding is a fundamental operation to support route discovery and time synchronization in wireless sensor networks. A simple flooding causes the broadcast storm problem and leads to long latency to complete the broadcast. This paper presents the Ripple flooding scheme (RFS) for wireless sensor networks (WSNs). The proposed scheme improves the convergence speed of flooding by using a synchronized packet rebroadcasting instead of using a CSMA (Carrier Sense Multiple Access) MAC based packet rebroadcasting. We analyse the multipath delay spread feasibility on a receiver for the packets from multiple senders. The theoretical upper bound of packet loss rate (PLR) is less than 13%. The experimental result shows the same as the PLR of the theoretical analysis. We implement the RFS on the sensor node with the MSP430 microcontroller and the CC2420 transceiver and evaluate the major performance metrics including the convergence time and the reliability in various network scales and densities. The convergence time is at least 3.5 times faster than the CSMA MAC based flooding in the dense network environments. The transmission reliability of the RFS is slightly better than the reliability of the CSMA MAC based flooding because of the implicit overhearing scheme provided by the RFS.\"",
        "Document: \"Efficient and privacy-enhanced object traceability based on unified and linked EPCIS events. HighlightsThe graph-oriented approach for standard visibility event/master data is proposed.The approach is better than the event-oriented approach as follows.The approach provides a unified view for both event and master data.The approach alleviates recursive and redundant trace queries.The approach supports resource-level access control as well as event-level one. EPC Information Service (EPCIS) is a de-facto standard for visibility event/master data in supply chains, and the standard is widely known as a key solution to achieve object traceability. Despite its expressive power, this information service is prone to make application developers write queries redundantly and recursively, especially in distributed environments; additionally, it is hard for business partners to protect sensitive business contexts in each object level due to its event-oriented persistence approach. Therefore, we propose a graph-oriented persistence approach for the visibility data to achieve efficient and privacy-enhanced object traceability based on unified and linked EPCIS events.\"",
        "Document: \"Practical security analysis for the constrained node networks: Focusing on the DTLS protocol. With the explosive popularity of the Internet of Things (IoT) which enables the global connectivity of surrounding objects, the importance of security and privacy is getting more recognized recently. Unlike traditional network entities, devices in IoT normally have constrained resources, which makes it hard to provide full-featured security. To provide a secure channel between Constrained Nodes (CN), Datagram Transport Layer Security (DTLS) is currently used as the de facto security protocol to secure application messages. This paper introduces the practical analysis of DTLS focusing on the Constrained-Node Networks (CNN). We have investigated security considerations for CNN and analyzed the performance of DTLS with a real implementation on an IoT connectivity platform named SNAIL (Sensor Networks for an All-IP World). For a more practical approach, we have additionally implemented the Lightweight Machine to Machine (LwM2M) protocol on SNAIL and evaluated the security functionalities' network performance. Our various evaluations have shown the impact of embedded cryptography, multi-hop topology, link-layer quality and other constraints. This research would give a comprehensive guidance to whom that wants to provide secure services efficiently to their potential users.\"",
        "1 is \"Improving SLP efficiency and extendability by using global attributes and preference filters\", 2 is \"A Joint Resource Allocation Scheme for Multiuser Two-Way Relay Networks\"",
        "Given above information, for an author who has written the paper with the title \"Reducing conservativeness in stability conditions of affine fuzzy systems using fuzzy Lyapunov function\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007153": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Table Cell Search for Question Answering.':",
        "Document: \"Learning and inference over constrained output. We study learning structured output in a discriminative framework where values of the output variables are estimated by local classifiers. In this framework, complex dependencies among the output variables are captured by constraints and dictate which global labels can be inferred. We compare two strategies, learning independent classifiers and inference based training, by observing their behaviors in different conditions. Experiments and theoretical justification lead to the conclusion that using inference based learning is superior when the local classifiers are difficult to learn but may require many examples before any discernible difference can be observed.\"",
        "Document: \"The Value Of Semantic Parse Labeling For Knowledge Base Question Answering. We demonstrate the value of collecting semantic parse labels for knowledge base question answering. In particular, (1) unlike previous studies on small-scale datasets, we show that learning from labeled semantic parses significantly improves overall performance, resulting in absolute 5 point gain compared to learning from answers, (2) we show that with an appropriate user interface, one can obtain semantic parses with high accuracy and at a cost comparable or lower than obtaining just answers, and (3) we have created and shared the largest semantic-parse labeled dataset to date in order to advance research in question answering.\"",
        "Document: \"Learning Discriminative Projections for Text Similarity Measures. Traditional text similarity measures consider each term similar only to itself and do not model semantic relatedness of terms. We propose a novel discriminative training method that projects the raw term vectors into a common, low-dimensional vector space. Our approach operates by finding the optimal matrix to minimize the loss of the pre-selected similarity function (e.g., cosine) of the projected vectors, and is able to efficiently handle a large number of training examples in the high-dimensional space. Evaluated on two very different tasks, cross-lingual document retrieval and ad relevance measure, our method not only outperforms existing state-of-the-art approaches, but also achieves high accuracy at low dimensions and is thus more efficient.\"",
        "Document: \"Clickthrough-based latent semantic models for web search. This paper presents two new document ranking models for Web search based upon the methods of semantic representation and the statistical translation-based approach to information retrieval (IR). Assuming that a query is parallel to the titles of the documents clicked on for that query, large amounts of query-title pairs are constructed from clickthrough data; two latent semantic models are learned from this data. One is a bilingual topic model within the language modeling framework. It ranks documents for a query by the likelihood of the query being a semantics-based translation of the documents. The semantic representation is language independent and learned from query-title pairs, with the assumption that a query and its paired titles share the same distribution over semantic topics. The other is a discriminative projection model within the vector space modeling framework. Unlike Latent Semantic Analysis and its variants, the projection matrix in our model, which is used to map from term vectors into sematic space, is learned discriminatively such that the distance between a query and its paired title, both represented as vectors in the projected semantic space, is smaller than that between the query and the titles of other documents which have no clicks for that query. These models are evaluated on the Web search task using a real world data set. Results show that they significantly outperform their corresponding baseline models, which are state-of-the-art.\"",
        "Document: \"Multi-document summarization by maximizing informative content-words. We show that a simple procedure based on maximizing the number of informative content-words can produce some of the best reported results for multi-document summarization. We first assign a score to each term in the document cluster, using only frequency and position information, and then we find the set of sentences in the document cluster that maximizes the sum of these scores, subject to length constraints. Our overall results are the best reported on the DUC-2004 summarization task for the ROUGE-1 score, and are the best, but not statistically significantly different from the best system in MSE-2005. Our system is also substantially simpler than the previous best system.\"",
        "Document: \"Basic Reasoning with Tensor Product Representations.   In this paper we present the initial development of a general theory for mapping inference in predicate logic to computation over Tensor Product Representations (TPRs; Smolensky (1990), Smolensky & Legendre (2006)). After an initial brief synopsis of TPRs (Section 0), we begin with particular examples of inference with TPRs in the 'bAbI' question-answering task of Weston et al. (2015) (Section 1). We then present a simplification of the general analysis that suffices for the bAbI task (Section 2). Finally, we lay out the general treatment of inference over TPRs (Section 3). We also show the simplification in Section 2 derives the inference methods described in Lee et al. (2016); this shows how the simple methods of Lee et al. (2016) can be formally extended to more general reasoning tasks. \"",
        "Document: \"Partitioned logistic regression for spam filtering. Naive Bayes and logistic regression perform well in different regimes. While the former is a very simple generative model which is efficient to train and performs well empirically in many applications,the latter is a discriminative model which often achieves better accuracy and can be shown to outperform naive Bayes asymptotically. In this paper, we propose a novel hybrid model, partitioned logistic regression, which has several advantages over both naive Bayes and logistic regression. This model separates the original feature space into several disjoint feature groups. Individual models on these groups of features are learned using logistic regression and their predictions are combined using the naive Bayes principle to produce a robust final estimation. We show that our model is better both theoretically and empirically. In addition, when applying it in a practical application, email spam filtering, it improves the normalized AUC score at 10% false-positive rate by 28.8% and 23.6% compared to naive Bayes and logistic regression, when using the exact same training examples.\"",
        "Document: \"The importance of syntactic parsing and inference in semantic role labeling. We present a general framework for semantic role labeling. The framework combines a machine-learning technique with an integer linear programming-based inference procedure, which incorporates linguistic and structural constraints into a global decision process. Within this framework, we study the role of syntactic parsing information in semantic role labeling. We show that full syntactic parsing information is, by far, most relevant in identifying the argument, especially, in the very first stage---the pruning stage. Surprisingly, the quality of the pruning stage cannot be solely determined based on its recall and precision. Instead, it depends on the characteristics of the output candidates that determine the difficulty of the downstream problems. Motivated by this observation, we propose an effective and simple approach of combining different semantic role labeling systems through joint inference, which significantly improves its performance. Our system has been evaluated in the CoNLL-2005 shared task on semantic role labeling, and achieves the highest F1 score among 19 participants.\"",
        "Document: \"Question Answering with Knowledge Base, Web and Beyond. In this tutorial, we give the audience a coherent overview of the research of question answering (QA). We first introduce a variety of QA problems proposed by pioneer researchers and briefly describe the early efforts. By contrasting with the current research trend in this domain, the audience can easily comprehend what technical problems remain challenging and what the main breakthroughs and opportunities are during the past half century. For the rest of the tutorial, we select three categories of the QA problems that have recently attracted a great deal of attention in the research community, and present the tasks with the latest technical survey. We conclude the tutorial by discussing the new opportunities and future directions of QA research.\"",
        "Document: \"Embedding Entities and Relations for Learning and Inference in Knowledge Bases.   We consider learning representations of entities and relations in KBs using the neural-embedding approach. We show that most existing models, including NTN (Socher et al., 2013) and TransE (Bordes et al., 2013b), can be generalized under a unified learning framework, where entities are low-dimensional vectors learned from a neural network and relations are bilinear and/or linear mapping functions. Under this framework, we compare a variety of embedding models on the link prediction task. We show that a simple bilinear formulation achieves new state-of-the-art results for the task (achieving a top-10 accuracy of 73.2% vs. 54.7% by TransE on Freebase). Furthermore, we introduce a novel approach that utilizes the learned relation embeddings to mine logical rules such as \"BornInCity(a,b) and CityInCountry(b,c) => Nationality(a,c)\". We find that embeddings learned from the bilinear objective are particularly good at capturing relational semantics and that the composition of relations is characterized by matrix multiplication. More interestingly, we demonstrate that our embedding-based rule extraction approach successfully outperforms a state-of-the-art confidence-based rule mining approach in mining Horn rules that involve compositional reasoning. \"",
        "1 is \"Learning to recommend with explicit and implicit social relations\", 2 is \"Optimization as a Model for Few-Shot Learning\"",
        "Given above information, for an author who has written the paper with the title \"Table Cell Search for Question Answering.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007255": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Rock Porosity Prediction Using Multilayer Perceptions':",
        "Document: \"Approximating support vector machine with artificial neural network for fast prediction. \u2022Hybrid neural network (HNN), a method to accelerate prediction speed of support vector machine (SVM) is proposed.\u2022The proposed method approximates SVM using artificial neural network (ANN).\u2022The proposed method yields much faster prediction speed without compromising prediction accuracy.\u2022The application of this method can improve practical usability of SVM.\"",
        "Document: \"Improvement of Kittler and Illingworth's minimum error thresholding. A simple modification to Kittler and Illingworth's minimum error thresholding method was made and the performance of the modified version was compared with that of the original version empirically. By correcting the biased estimates of variances of model distributions, a significant improvement in performance was found. The improvement was most outstanding among not-well-separated, but still bimodal histograms. In fact, the modification provides a more robust method. The new version is nearly computationally equivalent in complexity to the original version.\"",
        "Document: \"Evaluating the reliability level of virtual metrology results for flexible process control: a novelty detection-based approach. The purpose of virtual metrology (VM) in semiconductor manufacturing is to support process monitoring and quality control by predicting the metrological values of every wafer without an actual metrology process, based on process sensor data collected during the operation. Most VM-based quality control schemes assume that the VM predictions are always accurate, which in fact may not be true due to some unexpected variations that can occur during the process. In this paper, therefore, we propose a means of evaluating the reliability level of VM prediction results based on novelty detection techniques, which would allow flexible utilization of the VM results. Our models generate a high-reliability score for a wafer's VM prediction only when its process sensor values are found to be consistent with those of the majority of wafers that are used in model building; otherwise, a low-reliability score is returned. Thus, process engineers can selectively utilize VM results based on their reliability level. Experimental results show that our reliability generation models are effective; the VM results for wafers with a high level of reliability were found to be much more accurate than those with a low level.\"",
        "Document: \"Applying convolution filter to matrix of word-clustering based document representation. Word-clustering based document representation approaches have been suggested recently to overcome previous limitations such as high dimensionality or loss of innate interpretation; they show higher classification performance than other recent methods. Thus, we present a novel way to combine the advantages of various word-clustering based representation approaches. Instead of previous approaches, which represent documents in vector form, we represent documents in matrix form while concatenate various representation results. And we proposed another novel way to apply convolution filter to those representation while rearranging the elements by preserving the semantic distance. In order to verify the representation performance of our proposed methods, we utilized the kinds of dataset: customer-voice data from LG Electronics, public Reuter news dataset and 20 Newsgroup dataset. The results demonstrated that the proposed method outperforms all other methods and achieves a classification accuracy of 88.73%, 89.16%, and 88.06% for each dataset.\"",
        "Document: \"Learning competition and cooperation. Competitive activation mechanisms introduce competitive orinhibitory interactions between units through functional mechanismsinstead of inhibitory connections. A unit receives input fromanother unit proportional to its own activation as well as to thatof the sending unit and the connection strength between the two.This, plus the finite output from any unit, induces competitionamong units that receive activation from the same unit. Here wepresent a backpropagation learning rule for use with competitiveactivation mechanisms and show empirically how this learning rulesuccessfully trains networks to perform an exclusive-OR task and adiagnosis task. In particular, networks trained by this learningrule are found to outperform standard backpropagation networks withnovel patterns in the diagnosis problem. The ability of competitivenetworks to bring about context-sensitive competition andcooperation among a set of units proved to be crucial in diagnosingmultiple disorders.\"",
        "Document: \"Multivariate Control Charts Based on Hybrid Novelty Scores. We propose a new nonparametric multivariate control chart that integrates a novelty score. The proposed control chart uses as its monitoring statistic a hybrid novelty score, calculated based on the distance to local observations as well as on the distance to the convex hull constructed by its neighbors. The control limits of the proposed control chart were established based on a bootstrap method. A rigorous simulation study was conducted to examine the properties of the proposed control chart under various scenarios and compare it with existing multivariate control charts in terms of average run length (ARL) performance. The simulation results showed that the proposed control chart outperformed both the parametric and nonparametric Hotelling's T-2 control charts, especially in nonnormal situations. Moreover, experimental results with real semiconductor data demonstrated the applicability and effectiveness of the proposed control chart. To increase the capability to detect small mean shift, we propose an exponentially weighted hybrid novelty score control chart. Simulation results indicated that exponentially weighted hybrid score charts outperformed the hybrid novelty score based control charts.\"",
        "Document: \"Focusing on non-respondents: Response modeling with novelty detectors. This paper proposes to use novelty detection approaches to alleviate the class imbalance in response modeling. Two novelty detectors, one-class support vector machine (1-SVM) and learning vector quantization for novelty detection (LVQ-ND), are compared with binary classifiers for a catalogue mailing task with DMEF4 dataset. The novelty detectors are more accurate and more profitable when the response rate is low. When the response rate is relatively high, however, a support vector machine model with modified misclassification costs performs the best. In addition, the novelty detectors turn in higher profits with a low mailing cost, while the SVM model is the most profitable with a high mailing cost.\"",
        "Document: \"Response modeling with support vector regression. Response modeling has become a key factor to direct marketing. In general, there are two stages in response modeling. The first stage is to identify respondents from a customer database while the second stage is to estimate purchase amounts of the respondents. This paper focuses on the second stage where a regression, not a classification, problem is solved. Recently, several non-linear models based on machine learning such as support vector machines (SVM) have been applied to response modeling. However, there is a major difficulty. A typical training dataset for response modeling is so large that modeling takes very long, or, even worse, modeling may be impossible. Therefore, sampling methods have been usually employed in practice. However a sampled dataset usually leads to lower accuracy. In this paper, we employed an @e-tube based sampling for support vector regression (SVR) which leads to better accuracy than the random sampling method.\"",
        "Document: \"Constructing a multi-class classifier using one-against-one approach with different binary classifiers. For the one-against-one approach, all the binary classifiers that form a one-against-one classifier should be sufficiently competent. If some of the classifiers are not competent, the consequences might be invalid classification results. To address the problem, we propose diversified one-against-one (DOAO) method that seeks to find the best classification algorithm for each class pair when applying the one-against-one approach to multi-class classification problems. Applying the proposed method makes various classification algorithms to complement each other. Since the best classification algorithm for each class pair is different, the proposed method can obtain improved classification results. Experimental results show that the proposed method outperforms other one-against-one based methods.\"",
        "Document: \"Clustering-Based Reference Set Reduction for k-Nearest Neighbor. Response Modeling is concerned with computing the likelihood of a customer to respond to a marketing campaign. A major problem encountered in response modeling is huge volume of data or patterns. The k-NN has been used in various classification problems for its simplicity and ease of implementation. However, it has not been applied to problems for which fast classification is needed since the classification time rapidly increases as the size of reference set increases. In this paper, we propose a clustering-based preprocessing step in order to reduce the size of reference set. The experimental results showed an 85% decrease in classification time without a loss of accuracy.\"",
        "1 is \"A task-technology fit view of learning management system impact\", 2 is \"Workflow mining with InWoLvE\"",
        "Given above information, for an author who has written the paper with the title \"Rock Porosity Prediction Using Multilayer Perceptions\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007274": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Robust Stability Analysis of a Fuzzy Vehicle Lateral Control System Using Describing Function Method.':",
        "Document: \"Dynamic Calibration and Occlusion Handling Algorithms for Lane Tracking. An approach of rapidly computing the projective width of lanes is presented to predict the projective positions and widths of lanes. The Lane Marking Extraction Finite State Machine is designed to extract points with features of lane markings in the image, and a cubic B-spline is adopted to conduct curve fitting to reconstruct road geometry. A statistical search algorithm is also proposed to corre...\"",
        "Document: \"A vision-based blind spot warning system for daytime and nighttime driver assistance. This paper proposes an effective blind spot warning system (BSWS) for daytime and nighttime conditions. The proposed BSWS includes camera models of a dynamic calibration and blind spot detection (BSD) algorithms for the daytime and nighttime. Under daytime conditions, the proposed system presents the Horizontal Edge and Shadow Composite Region (HESCR) method to extract the searching region and to acquire the shadow location of the targeted vehicles. Additionally, to detect vehicles at nighttime road scenes, the proposed system extracts bright objects and recognizes the paired headlights of the targeted vehicles for the BSD. The BSWS is implemented on a DSP-based embedded platform. The results of the BSWS are obtained by conducting practical experiments on our camera-assisted car on a highway in Taiwan under both nighttime and daytime conditions. Experimental results show that the proposed BSWS is feasible for vehicle detection and collision warning in various daytime and nighttime road environments.\"",
        "Document: \"A real-time vision-based safety assist system. In this paper, a real-time vision-based safety assist system is proposed to provide the driver an assistance system improving the security on driving and after parking. The estimated distances and a friendly intuitive graph are shown on the screen for drivers to examine the distance between other vehicles. The driving status can also be recorded in H.264. Furthermore, the users' mobile phones can monitor the images inside the vehicles to prevent the thieves after parking. The system has been implemented with the embedded systems and tested by the real road environment.\"",
        "Document: \"Arbitrarily shaped image coding by using translation invariant wavelet transforms. Cette \u00e9tude pr\u00e9sente deux ensembles de transformations en ondelettes invariantes par translation pour le codage d'images de forme arbitraire. Chaque ensemble peut \u00eatre vu comme une transformation en ondelettes discr\u00e8te adaptive en forme (SA-DWT) pr\u00e9sentant la propri\u00e9t\u00e9 d'invariance par translation. Les m\u00e9thodes de transformation propos\u00e9es ont les avantages suivants: (1) elles sont invariantes par translation, (2) aucune transition abrupte n'appara\u0131\u0302t sur les contours de l'image, (3) le nombre de pixels est maintenu constant apr\u00e8s transformation, (4) la corr\u00e9lation des pixels est pleinement exploit\u00e9e, et (5) la propri\u00e9t\u00e9 d'auto-similarit\u00e9 au travers des \u00e9chelles est pr\u00e9serv\u00e9e.\"",
        "Document: \"Text Extraction From Complex Document Images Using The Multi-Plane Segmentation Technique. This study presents a new method for extracting characters from various real-life complex document images. The proposed method applies a multi-plane segmentation technique to separate homogeneous objects including text blocks, non-text graphical objects, and background textures into individual object planes. It consists of two stages - automatic localized multilevel thresholding, and multi-plane region matching and assembling. Then a text extraction process can be performed on the resultant planes to detect and extract characters with different characteristics in the respective planes. The proposed method processes document images regionally and adaptively according to their respective local features. This allows preservation of detailed characteristics from extracted characters, especially small characters with thin strokes, as well as gradational illuminations of characters. This also permits background objects with uneven, gradational, and sharp variations in contrast, illumination, and texture to be handled easily and well. Experimental results on real-life complex document images demonstrate that the proposed method is effective in extracting characters with various illuminations, sizes, and font styles from various types of complex document images.\"",
        "Document: \"A low memory QCB-based DWT for JPEG2000 coprocessor supporting large tile size. JPEG2000, which provides a higher compression ratio than the traditional JPEG, is an upcoming compression standard for still images. The experimental results imply that the larger tile size used for JPEG2000 results in better image quality. However, processing the large tile image requires more memory in the hardware implementation. To reduce the hardware resources, a QCB (quad codeblock) based DWT method is proposed to support the processing of large tile images with low memory. Based on the QCB-based DWT engine, three code-blocks belonging to LH0, HL0 and HH0 bands can be generated recursively after each fixed time slice, and the EBC (embedded block coding) processors can directly process the three code-blocks. It can save the size of tile memory by up to 75%. Moreover, the remaining 1/4 size of tile memory can be decreased through the zero holding extension for unavailable data. That is, it only requires 24 Kbytes memory to support the processing of a 512\u00d7512 tile image, with slight image degradation, especially at low bit rates. The low memory requirement makes the hardware implementation practicable.\"",
        "Document: \"Nighttime Vehicle Detection for Driver Assistance and Autonomous Vehicles. This study presents an effective method for detecting vehicles in front of the camera-assisted car during nighttime driving. The proposed method detects vehicles based on detecting and locating vehicle headlights and taillights using techniques of image segmentation and pattern analysis. First, to effectively extract bright objects of interest, a segmentation process based on automatic multilevel thresholding is applied on the grabbed road-scene images. Then the extracted bright objects are processed by a rule-based procedure, to identify the vehicles by locating and analyzing their vehicle light patterns, and estimate their distances to the camera-assisted car. Experimental results demonstrate the effectiveness of the proposed method on detecting vehicles at night\"",
        "Document: \"Robust Stability Analysis of a Fuzzy Vehicle Lateral Control System Using Describing Function Method. In this paper, the robust stability analysis of a fuzzy vehicle lateral system with perturbed parameters is presented. Firstly, the fuzzy controller can be linearized by utilizing the describing function method with experiments. After the describing function is obtained, the stability analysis of the vehicle lateral control system with the variations of velocity and friction is then carried out by the use of parameter plane method. Afterward some limit cycle loci caused by the fuzzy controller can be easily pointed out in the parameter plane. Computer simulation shows the efficiency of this approach.\"",
        "Document: \"Robust Image Measurement And Analysis Based On Perspective Transformations. In the paper, perspective transformation is used to project points from the front horizon of the camera to an image plane and thus to measure the distance between the detected object and the camera. With the information about points on the ground, the projective positions of every tip in a rigid object can be figured out through transformation. Features of the object's projection at different distances, such as size and shape, can also be predicted. Besides, the paper has analyzed difference in the result of the measurement and errors caused by the application of different parameters. The information assists engineers of vision-based detection system in determining the parameters of the system and identifying features of an object's projection to accelerate the detection. Also, the camera parameters compensate automatically when being influenced by outer force to promote the effects of detection and make a robust system.\"",
        "Document: \"Analysis And Architecture Design For High Performance Jpeg2000 Coprocessor. The high throughput and less internal memory requirement are two key issues of integrating the JPEG2000 encoding system. For this bottleneck of integration, a QCB (quad code block)-based DWT method is proposed to achieve more efficient parallelism in the JPEG2000 coprocessor. Based on the QCB-based DWT engine, the code blocks can be completely generated after every fixed time slice, recursively. Thus, the overall JPEG2000 encoding system arrives higher parallelism between DWT and EBCOT processors and thus preserves the high throughput, as DWT does. By changing the output timing of the DWT process and parallelizing with EBCOT, the internal tile memory size can be reduced by a factor of 4. Moreover, with smoother encoding flow of overall encoding system, the memory access cycles between the internal tile memory and code block memory also decreases.\"",
        "1 is \"Fast motion vector estimation using multiresolution-spatio-temporal correlations\", 2 is \"Synthesis of fuzzy model-based designs to synchronization andsecure communications for chaotic systems\"",
        "Given above information, for an author who has written the paper with the title \"Robust Stability Analysis of a Fuzzy Vehicle Lateral Control System Using Describing Function Method.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007343": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Multi-topic Information Filtering with a Single User Profile':",
        "Document: \"Supporting ontology driven document enrichment within communities of practice. Formative work by Lave and Wenger has articulated how practices emerge through the interplay of informal processes with symbolic codifications and artifacts. In this paper, we describe how ontologies can serve as symbolic tools within a community of practice supporting communication and knowledge sharing. We show that when a community's perspective on an issue is stable, it opens the possibility for introducing knowledge services, based on an ontology co-constructed by knowledge engineers with stakeholders. Using a case study we describe our approach, ontology driven document enrichment, looking at how ontology construction and population can be supported by web based technologies.\"",
        "Document: \"OntoWeaver-S: Integrating Web Services into Data-Intensive Web Sites. Designing web sites is a complex task. Ad-hoc rapid prototyping easily leads to unsatisfactory results, e.g. poor maintainability and extensibility. However, existing web design frameworks focus exclusively on data presentation: the development of specific functionalities is still achieved through low-level programming. In this paper we address this issue by describing our work on the integration of (semantic) web services into a web design framework, OntoWeaver. The resulting architecture, OntoWeaver-S, supports rapid prototyping of service- centred data-intensive web sites, which allow access to remote web services. In particular, OntoWeaver-S is integrated with a comprehensive web service platform, IRS-II, for the specification, discovery, and execution of web services. Moreover, it employs a set of comprehensive site ontologies to model and represent all aspects of service-centred data-intensive web sites, and thus is able to offer high level support for the design and development process.\"",
        "Document: \"An approach to construct dynamic service mashups using lightweight semantics. Thousands of Web services have been available online, and mashups built upon them have been creating added value. However, mashups are mostly developed with a predefined set of services and components. The extensions to them always involve programming work. Furthermore, when a service is unavailable, it is challenging for mashups to smoothly switch to an alternative that offers similar functionalities. To address these problems, this paper presents a novel approach to enable mashups to select and invoke semantic Web services on the fly. To extend a mashup with new semantic services, developers are only required to register and publish them as Linked Data. By refining the strategies of service selection, mashups can behave more adaptively and offer higher fault-tolerance.\"",
        "Document: \"MnM: Ontology Driven Semi-automatic and Automatic Support for Semantic Markup. An important precondition for realizing the goal of a semantic web is the ability to annotate web resources with semantic information. In order to carry out this task, users need appropriate representation languages, ontologies, and support tools. In this paper we present MnM, an annotation tool which provides both automated and semi-automated support for annotating web pages with semantic contents. MnM integrates a web browser with an ontology editor and provides open APIs to link to ontology servers and for integrating information extraction tools. MnM can be seen as an early example of the next generation of ontology editors, being web-based, oriented to semantic markup and providing mechanisms for large-scale automatic markup of web pages.\"",
        "Document: \"IRS-II: A Framework and Infrastructure for Semantic Web Services. In this paper we describe IRS-II (Internet Reasoning Service) a framework and implemented infrastructure, whose main goal is to support the publication, location, composition and execution of heterogeneous web services, augmented with semantic descriptions of their functionalities. IRS-II has three main classes of features which distinguish it from other work on semantic web services. Firstly, it supports one-click publishing of standalone software: IRS-II automatically creates the appropriate wrappers, given pointers to the standalone code. Secondly, it explicitly distinguishes between tasks (what to do) and methods (how to achieve tasks) and as a result supports capability-driven service invocation; flexible mappings between services and problem specifications; and dynamic, knowledge-based service selection. Finally, IRS-II services are web service compatible - standard web services can be trivially published through the IRS-II and any IRS-II service automatically appears as a standard web service to other web service infrastructures. In the paper we illustrate the main functionalities of IRS-II through a scenario involving a distributed application in the healthcare domain.\"",
        "Document: \"A core ontology for business process analysis. Business Process Management (BPM) aims at supporting the whole life-cycle necessary to deploy and maintain business processes in organisations. An important step of the BPM life-cycle is the analysis of the processes deployed in companies. However, the degree of automation currently achieved cannot support the level of adaptation required by businesses. Initial steps have been performed towards including some sort of automated reasoning within Business Process Analysis (BPA) but this is typically limited to using taxonomies. We present a core ontology aimed at enhancing the state of the art in BPA. The ontology builds upon a Time Ontology and is structured around the process, resource, and object perspectives as typically adopted when analysing business processes. The ontology has been extended and validated by means of an Events Ontology and an Events Analysis Ontology aimed at capturing the audit trails generated by Process-Aware Information Systems and deriving additional knowledge.\"",
        "Document: \"Translating Semantic Web Service based business process models. We describe a model-driven translation approach between Semantic Web Service based business process models in the context of the SUPER project. In SUPER we provide a set of business process ontologies for enabling access to the business process space inside the organisation at the semantic level. One major task in this context is to handle the translations between the provided ontologies in order to navigate from different views at the business level to the IT view at the execution level. In this paper we present the results of our translation approach, which transforms instances of BPMO to instances of sBPEL.\"",
        "Document: \"Semantic Web Services, Part 2. Semantic Web services (SWS) has been a vigorous technology research area for about six years, producing a great deal of innovative work. Part 2 of this Trends & Controversies department continues exploring the state of the art, current practices, and future directions for Semantic Web services.\"",
        "Document: \"ClaiMaker: Weaving a Semantic Web of Research Papers. The usability of research papers on the Web would be enhanced by a system that explicitly modelled the rhetorical relations between claims in related papers. We describe ClaiMaker, a system for modelling readers' interpretations of the core content of papers. ClaiMaker provides tools to build a Semantic Web representation of the claims in research papers using an ontology of relations. We demonstrate how the system can be used to make inter-document queries.\"",
        "Document: \"Scholarly Discourse as Computable Structure. In their initial proposal for structural computing (SC), N\u00fcrnberg et al. (18) point to hypertext argumentation systems as an example of an application domain in which structure is of first-order importance. In this paper we summarise the goals and implementation of a knowledge based hypertext environment called ScholOnto (for Scholarly Ontologies), which aims to provide researchers with computational support in representing and analysing the structure of scholarly claims, argumentation and perspectives. A specialised web server will provide a medium for researchers to contest the significance of concepts and emergent structures. In so doing, participants construct an evolving structure that reflects a community's understandings of its field, and which can support computational services for scholars. Using structural analyses of scholarly argumentation, we consider the connections with structural computing, and propose a number of requirements for generic SC environments.\"",
        "1 is \"A competence theory approach to problem solving method construction\", 2 is \"Using Multiple Clause Constructors in Inductive Logic Programming for Semantic Parsing\"",
        "Given above information, for an author who has written the paper with the title \"Multi-topic Information Filtering with a Single User Profile\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007365": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Stochastic programming models for general redundancy-optimization problems':",
        "Document: \"Optimal contracts for the agency problem with multiple uncertain information. There is usually such a kind of agency problem where one principal authorizes one agent to perform more than one task at the same time. However, the potential output of each task cannot be exactly predicted in advance, so there exist simultaneously multiple types of uncertain information about the potential outputs of all the tasks. In this case, how to design the optimal contract and how to investigate the impacts of the diversity of uncertain information on such an optimal contract become important and challenging for decision makers. Motivated by this, to filter out the uncertainty in the possible incomes, we firstly focus on the optimal contract when both the two participators' potential incomes are measured by their respective expected incomes. Following that, as an important innovation, confidence level is introduced to quantify the degree of the agent's risk aversion, and the effects of the agent's attitude toward risk on the optimal contract and the principal's income are taken into account. Based on this view, two classes of uncertain agency models are developed, and then the sufficient and necessary conditions for the optimal contracts are presented with the detailed proofs and analyses. Compared with the traditional agency model, the innovations and advantages of the proposed work are briefly summarized, and the effectiveness of the work is further demonstrated by the computational results in a portfolio selection problem.\"",
        "Document: \"Random fuzzy alternating renewal processes. Random fuzzy theory offers an appropriate mechanism to model random fuzzy phenomena, with a random fuzzy variable defined as a function from a credibility space to a collection of random variables. Based on this theory, this paper presents the results of an investigation into the representation of properties of alternating renewal processes that are described by sequences of positive random fuzzy vectors. It provides a theorem on the limit value of the average chance of a given random fuzzy event in terms of \u201csystem being on at time t\u201d. The resultant model coincides with that attainable by stochastic analysis when the random fuzzy vectors degenerate to random vectors.\"",
        "Document: \"Random fuzzy age-dependent replacement policy. This paper discusses the age-dependent replacement policy, in which the interarrival lifetimes of components are characterized as random fuzzy variables. A random fuzzy expected value model is presented and shown how it can be applied to reduce the loss of system failures. To solve the proposed model, a simultaneous perturbation stochastic approximation (SPSA) algorithm based on random fuzzy simulation is developed to search the optimal solution. At the end of this paper, a numerical example is enumerated.\"",
        "Document: \"Contract designing for a supply chain with uncertain information based on confidence level. Graphical abstractDisplay Omitted HighlightsA contract designing model is developed for a supply chain with uncertain information based on confidence level.The optimal order quantity of the retailer is determined by the inverse distribution of the external demand and the confidence level.The magnitudes of the confidence level and the high fixed cost affect the equilibrium contract menus.When the confidence level of the supply chain tends to be 0 or 1, the supplier with the low fixed cost possesses a competing advantage over the other supplier. We consider a contract-design problem for two competing heterogeneous suppliers working with a common retailer. The retailer's type low-volume or high-volume is unknown to the suppliers. One supplier has a high variable cost and a low fixed cost, whereas the other has a low variable cost and a high fixed cost, and their variable costs are uncertain. They sell the same products to the retailer, and each supplier offers the retailer a menu of contracts. The retailer chooses the contract that maximizes her alternative profit based on her confidence level instead of her expected profit. In this setting, we find that the retailer's optimal order quantity is determined by the inverse distribution of the external demand and the confidence level. Furthermore, higher confidence levels correlate with lower order quantities. We also show that the equilibrium contract menus depend on the magnitudes of the confidence level and the high fixed cost. Importantly, if the confidence level of the supply chain tends to be 0 or 1, the supplier with the low fixed cost possesses a competitive advantage over the other supplier. In some cases, the supplier with the low fixed cost may choose not to serve the high-volume retailer to avoid excessive information rent.\"",
        "Document: \"A Hybrid Intelligent Algorithm For Reliability Optimization Problems. This paper focuses on general cold standby redundancy systems with imperfect switches and the lifetime of system is modeled as a fuzzy variable. The system performance-alpha-system lifetime-characterized in the context of credibility is investigated. In order to estimate the system performance, a fuzzy simulation is designed. A standby redundancy fuzzy chance-constrained programming model is established to optimize this system performance under cost constraint. In order to solve the proposed model, we also design a hybrid intelligent algorithm which uses fuzzy simulation to generate a training data set, the backpropagation algorithm to train a neural network to approximate the uncertain function and genetic algorithm to optimize the system performance. Finally, a numerical experiment is discussed to illustrate the idea of the modeling and the effectiveness of the proposed algorithm.\"",
        "Document: \"Renewal process with fuzzy interarrival times and rewards. This paper considers a renewal process in which the interarrival times and rewards are characterized as fuzzy variables. A fuzzy elementary renewal theorem shows that the expected number of renewals per unit time is just the expected reciprocal of the interarrival time. Furthermore, the expected reward per unit time is provided by a fuzzy renewal reward theorem. Finally, a numerical example is presented for illustrating the theorems introduced in the paper.\"",
        "Document: \"The impact of risk attitude in new product development under dual information asymmetry. \u2022Four classes of uncertain principal agent models are presented.\u2022The closed form expressions for the optimal wage contracts are derived.\u2022The information values of the idea and the effort are characterized.\u2022Several interesting managerial insights are provided.\"",
        "Document: \"Stochastic programming models for general redundancy-optimization problems. This paper provides a unified modeling idea for both parallel and standby redundancy optimization problems. A spectrum of redundancy stochastic programming models is constructed to maximize the mean system-lifetime, \u03b1-system lifetime, or system reliability. To solve these models, a hybrid intelligent algorithm is presented. Some numerical examples illustrate the effectiveness of the proposed algorithm. This paper considers both parallel redundant systems and standby redundant systems whose components are connected with each other in a logical configuration with a known system structure function. Three types of system performance-expected system lifetime, \u03b1-system lifetime and system reliability-are introduced. A stochastic simulation is designed to estimate these system performances. In order to model general redundant systems, a spectrum of redundancy stochastic programming models is established. Stochastic simulation, NN and GA are integrated to produce a hybrid intelligent algorithm for solving the proposed models. Finally, the effectiveness of the hybrid intelligent algorithm is illustrated by some numerical examples.\"",
        "Document: \"Trade credit contracting under asymmetric credit default risk: Screening, checking or insurance. \u2022A decentralized supply chain with asymmetric credit level information is modeled.\u2022The high credit type retailer\u2019s consumption is always limited.\u2022The low credit type retailer consistently obtains a longer credit period.\u2022The insurance mechanism is only employed when the retailer\u2019s credit state is relatively poor.\u2022The default risk gap is the main factor for choosing the screening or checking mechanism.\"",
        "Document: \"Differential evolution with individual-dependent and dynamic parameter adjustment. Differential evolution (DE) is a powerful and versatile evolutionary algorithm for global optimization over continuous search space, whose performance is significantly influenced by its mutation operator and control parameters (population size, scaling factor and crossover rate). In order to enhance the performance of DE, we adopt a new variant of classic mutation operator, a gradual decrease rule for population size, an individual-dependent and dynamic strategy to generate the required values of scaling factor and crossover rate during the evolutionary process, respectively. In the proposed variant of DE (denoted by IDDE), the adopted mutation operator merges the superiority of two classic mutation operators (DE/best/2 and DE/rand/2) together, and the adjustment mechanism of control parameters applies the fitness value information of each individual and dynamic fluctuation rule, which can provide a better balance between the exploration ability and exploitation ability. To verify the performance of proposed IDDE, a suite of thirty benchmark functions is applied to conduct the simulation experiment. The simulation results demonstrate that the proposed IDDE performs significantly better than five state-of-the-art DE variants and other two evolutionary algorithms.\"",
        "1 is \"Mean-variance model for portfolio optimization problem in the simultaneous presence of random and uncertain returns.\", 2 is \"A Stock Model With Jumps For Uncertain Markets\"",
        "Given above information, for an author who has written the paper with the title \"Stochastic programming models for general redundancy-optimization problems\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007539": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Being rational or aggressive? A revisit to Dunbar's number in online social networks':",
        "Document: \"Structural analysis of network traffic matrix via relaxed principal component pursuit. The network traffic matrix is widely used in network operation and management. It is therefore of crucial importance to analyze the components and the structure of the network traffic matrix, for which several mathematical approaches such as Principal Component Analysis (PCA) were proposed. In this paper, we first argue that PCA performs poorly for analyzing traffic matrix that is polluted by large volume anomalies, and then propose a new decomposition model for the network traffic matrix. According to this model, we carry out the structural analysis by decomposing the network traffic matrix into three sub-matrices, namely, the deterministic traffic, the anomaly traffic and the noise traffic matrix, which is similar to the Robust Principal Component Analysis (RPCA) problem previously studied in [13]. Based on the Relaxed Principal Component Pursuit (Relaxed PCP) method and the Accelerated Proximal Gradient (APG) algorithm, we present an iterative approach for decomposing a traffic matrix, and demonstrate its efficiency and flexibility by experimental results. Finally, we further discuss several features of the deterministic and noise traffic. Our study develops a novel method for the problem of structural analysis of the traffic matrix, which is robust against pollution of large volume anomalies.\"",
        "Document: \"On the average similarity degree between solutions of random k-SAT and random CSPs. To study the structure of solutions for random k-SAT and random CSPs, this paper introduces the concept of average similarity degree to characterize how solutions are similar to each other. It is proved that under certain conditions, as r (i.e. the ratio of constraints to variables) increases, the limit of average similarity degree when the number of variables approaches infinity exhibits phase transitions at a threshold point, shifting from a smaller value to a larger value abruptly. For random k-SAT this phenomenon will occur when k \u2265 5. It is further shown that this threshold point is also a singular point with respect to r in the asymptotic estimate of the second moment of the number of solutions. Finally, we discuss how this work is helpful to understand the hardness of solving random instances and a possible application of it to the design of search algorithms.\"",
        "Document: \"DIGRank: using global degree to facilitate ranking in an incomplete graph. PageRank has been broadly applied to get credible rank sequences of nodes in many networks such as the web, citation networks, or online social networks. However, in the real world, it is usually hard to ascertain a complete structure of a network, particularly a large-scale one. Some researchers have begun to explore how to get a relatively accurate rank more efficiently. They have proposed some local approximation methods, which are especially designed for quickly estimating the PageRank value of a new node, after it is just added to the network. Yet, these local approximation methods rely on the link server too much, and it is difficult to use them to estimate rank sequences of nodes in a group. So we propose a new method called DIGRank, which uses global Degree to facilitate Ranking in an Incomplete Graph and which takes into account the frequent need for applications to rank users in a community, retrieve pages in a particular area, or mine nodes in a fractional or limited network. Based on experiments in small-world and scale-free networks generated by models, the DIGRank method performs better than other local estimation methods on ranking nodes in a given subgraph. In the models, it tends to perform best in graphs that have low average shortest path length, high average degree, or weak community structure. Besides, compared with an local PageRank and an advanced local approximation method, it significantly reduces the computational cost and error rate.\"",
        "Document: \"The Automated Acquisition of Suggestions from Tweets. This paper targets at automatically detecting and classifying user's suggestions from tweets. The short and informal nature of tweets, along with the imbalanced characteristics of suggestion tweets, makes the task extremely challenging. To this end, we develop a classification framework on Factorization Machines, which is effective and efficient especially in classification tasks with feature sparsity settings. Moreover, we tackle the imbalance problem by introducing cost-sensitive learning techniques in Factorization Machines. Extensively experimental studies on a manually annotated real-life data set show that the proposed approach significantly improves the baseline approach, and yields the precision of 71.06% and recall of 67.86%. We also investigate the reason why Factorization Machines perform better. Finally, we introduce the first manually annotated dataset for suggestion classification. Copyright \u00a9 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\"",
        "Document: \"An Exact Algorithm Based on MaxSAT Reasoning for the Maximum Weight Clique Problem. Recently, MaxSAT reasoning is shown very effective in computing a tight upper bound for a Maximum Clique (MC) of a (unweighted) graph. In this paper, we apply MaxSAT reasoning to compute a tight upper bound for a Maximum Weight Clique (MWC) of a wighted graph. We first study three usual encodings of MWC into weighted partial MaxSAT dealing with hard clauses, which must be satisfied in all solutions, and soft clauses, which are weighted and can be falsified. The drawbacks of these encodings motivate us to propose an encoding of MWC into a special weighted partial MaxSAT formalism, called LW (Literal-Weighted) encoding and dedicated for upper bounding an MWC, in which both soft clauses and literals in soft clauses are weighted. An optimal solution of the LW MaxSAT instance gives an upper bound for an MWC, instead of an optimal solution for MWC. We then introduce two notions called the Top-k literal failed clause and the Top-k empty clause to extend classical MaxSAT reasoning techniques, as well as two sound transformation rules to transform an LW MaxSAT instance. Successive transformations of an LW MaxSAT instance driven by MaxSAT reasoning give a tight upper bound for the encoded MWC. The approach is implemented in a branch-and-bound algorithm called MWCLQ. Experimental evaluations on the broadly used DIMACS benchmark, BHOSLIB benchmark, random graphs and the benchmark from the winner determination problem show that our approach allows MWCLQ to reduce the search space significantly and to solve MWC instances effectively. Consequently, MWCLQ outperforms state-of-the-art exact algorithms on the vast majority of instances. Moreover, it is surprisingly effective in solving hard and dense instances.\"",
        "Document: \"The Limits of Horn Logic Programs. Given a sequence {\ufffdn} of Horn logic programs, the limitof {\ufffdn} is the set of the clauses such that every clause inbelongs to almost everyn and every clause in infinitely manyn's belongs toalso. The limit programis still Horn but may be infinite. In this paper, we consider if the least Herbrand model of the limit of a given Horn logic program sequence {\ufffdn} equals the limit of the least Herbrand models of each logic programn. It is proved that this property is not true in general but holds if under an assumption which can be syntactically checked and be satisfied by a class of Horn logic programs. Thus, under this assumption we can approach the least Herbrand model of the limitby the sequence of the least Herbrand models of each finite programn. We also prove that if a finite Horn logic program satisfies this assumption, then the least Herbrand model of this program is recursive. Finally, by use of the concept of stability from dynamical systems, we prove that this assumption is exactly a sufficient condition t o guarantee the stability of fixed points for Horn logic programs.\"",
        "Document: \"Exact phase transitions in random constraint satisfaction problems. In this paper we propose a new type of random CSP model, called Model RB, which is a revision to the standard Model B. It is proved that phase transitions from a region where almost all problems are satisfiable to a region where almost all problems are unsatisfiable do exist for Model RB as the number of variables approaches infinity. Moreover, the critical values at which the phase transitions occur are also known exactly. By relating the hardness of Model RB to Model B, it is shown that there exist a lot of hard instances in Model RB.\"",
        "Document: \"A Logic Distance-Based Method for Deploying Probing Sources in the Topology Discovery. Internet topology plays a vital role in studying network's internal structure and properties. Currently traceroute-based topology discovery is the main approach to map the network. However, the deployments of probing sources are usually quite costly and complex. Even if the total numbers of sources are the same, the overall coverage of the sampled network may vary significantly for different sources. As a result, it is of great importance for a topology discovery project to select a limited set of probing sources to detect more nodes and links. The aim of this paper is to investigate how to select a fixed set of probing sources to maximize the coverage of the sampled network. We propose a novel logic distance-based method to make source placement decisions. Also we evaluate our approach and compare it with other known methods on real network topology and generated topologies.\"",
        "Document: \"A hierarchy and probability-based approach for inferring AS relationships. The commercial relationships between Autonomous Systems (ASes) are of great importance to understand the Internet reachability and calculate the AS-level paths. Several algorithms have been proposed to solve the AS relationship inference problem and applied to the data of IPv4 network. In assuming that the provider is typically larger than its customers, and the peers usually have comparable sizes, the suggested algorithms exploit the AS degree information to infer AS relationships. In analysis of the AS relationships in the IPv6 network, however, we find that quite a few of the inference results induced by the present approaches are different from the inferences in the IPv4 network. With respect to this observation, we analyze the root cause of the discrepancy and propose an algorithm which combines the AS hierarchy information, an inherent nature of the Internet structure that we can hardly neglect while analyzing the AS relationships, with the optimization model of Type-of-Relationship (ToR) problem to infer the AS relationships more realistically and stably. In this paper, we first present a methodology to classify ASes into four hierarchies, and then use the AS hierarchy information to infer AS relationships. By taking advantage of these partial AS relationship information, we introduce an improved algorithm to solve the ToR problem for the remaining AS pairs. The experimental results support our algorithm in two aspects. On one hand, the comparison with previous works in the IPv4 network shows that most of our inferring AS relationships are consistent with their inferences, while more inferences of our approach are confirmed by the export policies stored in the Internet Routing Registry (IRR) databases. On the other hand, 94.82% of our inference relationships in the IPv6 network are consistent with the inferences in the IPv4 network, which illustrates that our algorithm is more stable than previous algorithms.\"",
        "Document: \"Union Closed Tree Convex Sets. We show that the union closed sets conjecture holds for tree convex sets. The union closed sets conjecture says that in every union closed set system, there is an element to appear in at least half of the members of the system. In tree convex set systems, there is a tree on the elements such that each subset induces a subtree. Our proof relies on the well known Helly property of hypertrees and an equivalent graph formulation of the union closed sets conjecture given in (Bruhn, H., Charbit, P. and Telle, J. A.: The graph formulation of the union-closed sets conjecture. Proc. of EuroComb 2013, 73-78 (2013)).\"",
        "1 is \"Generating Images with Perceptual Similarity Metrics based on Deep Networks.\", 2 is \"Non-negative patch alignment framework.\"",
        "Given above information, for an author who has written the paper with the title \"Being rational or aggressive? A revisit to Dunbar's number in online social networks\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007542": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Defect-tolerance in cellular nanocomputers':",
        "Document: \"Fuzzy Logic Approach to 3D Magnetic Resonance Image Segmentation. This paper proposes an approach of fuzzy logic to 3D MR image segmentation. We show a fuzzy knowledge representation method to represent the knowledge needed to segment the target portions, and apply our method to 3D MR human brain image segmentation. In it we consider position knowledge, boundary surface knowledge and intensity knowledge. They are expressed by fuzzy if-then rules and compiled to a total degree as the measure of segmentation. The degree is evaluated in region growing technique and which segments the whole brain region into the left cerebral hemisphere, the right cerebral hemisphere, the cerebellum and the brain stem. The experimental result on 36 MR voxel data shows that our method extracted the portions precisely.\"",
        "Document: \"Specific Health Examination Data Prediction for Female Subjects with Unhealthy-Level Visceral Fat Using Self-Organizing Maps. In this paper, a data-prediction method is presented for female subjects with unhealthy-level visceral fat, using self-organizing maps (SOMs). It assumes that the original data measured in successive three years are available for the subjects associated with training data for map learning. It introduces the standardization for original item values to prepare the data. When predicting original item values, the data of the subject to be explored is presented to the trained map to determine a winner. A set of the training data making that winner fire is picked up, and the differences between original data measured in the second year and that measured in the third year are calculated for each of the subjects with such training data. The proposed method adds the differences averaged over such subjects to the original data of the subject to be explored, to obtain prediction result. It is revealed that the favorable prediction accuracy is achieved for hemoglobin A1c.\"",
        "Document: \"On Design of Fail-Safe Cellular Arrays. In this paper, we discuss the design of a fail-safe cellular array composed of switch cells. First, we show the design method using a binary decision diagram. Next, we assume stuck-at faults of switch cells to be fault models and discuss the fail-safe property for our array. For all the single faults and part of the multiple faults, our array keeps the fail-safe property. Next, for our arrays realizing randomly generated functions, we derive the ratio of the number of double faults that never break the fail-safe property to the total number of double faults. Finally, in order to demonstrate the advantages of our array, we compare our array with other arrays.\"",
        "Document: \"Learning-Based On-Line Testing in Feedforward Neural Networks. Learning-based on-line testing in feedforward neural networks (NNs) is discussed. After the convergence of the ordinary learning, the re-learning employing two sigmoid activation functions per neuron in the last layer of the NN is made. It sets up the range of erroneous potentials produced from the last layer, and enables us to detect faults without extra hardware.\"",
        "Document: \"Multiple-Valued Product-of-Sums Expression with Truncated Sum. Truncated sum (TSUM for short) can be useful for MV-PLAs realization. This paper introduces multiple-valued product-of-sums expressions where sum refers to TSUM and product does MIN. We investigate the multiple-valued product-of-sums expressions and show the minimization method and the simulation results. We describe the minimization method based on binary Quine-McCluskey algorithm. It is proved that in the minimal product-of-sums expressions, the implicate number of the expressions with TSUM is equivalent to the number of those with MAX. Next, we propose multiple-valued product-of-sums expressions with variables. The expressions involve the TSUM of variables and nonzero constants as the coefficients of the implicates. The minimization method is also proposed. Finally, we show the simulation results for some multiple-valued arithmetic functions. In them, an efficiency of the product-of-sums expressions with variables is shown and some comparisons are made.Keywords, multiple-Valued logic design, truncated sum, product-of-sums form, minimization\"",
        "Document: \"Three-dimensional classification of insect neurons using self-organizing maps. In this paper, systematic three-dimensional classification is presented for sets of interneuron slice images of silkworm moths, using self-organizing maps. Fractal dimension values are calculated for target sets to quantify denseness of their branching structures, and are employed as element values in training data for constructing a map. The other element values are calculated from the sets to which labeling and erosion are applied, and they quantifies whether the sets include thick main dendrites. The classification result is obtained as clusters with units in the map. The proposed classification employing only two elements in training data achieves as high accuracy as the manual classification made by neuroscientists.\"",
        "Document: \"Classification of interview sheets using self-organizing maps for determination of ophthalmic examinations. In this paper, a method of determining examinations is presented for outpatients visiting the department of ophthalmology. It assumes that each of the interview sheets belongs to one of the four classes, and copes with the examination determination as the classification of the sheets using self-organizing maps. Training data presented to the maps are generated from handwriting sentences in the sheets. Some nouns, adjectives and adverbs that ophthalmologists consider to be of comparative importance are chosen as elements of the training data. The element values basically depend on frequencies of the chosen words appearing in the sentences. After map learning is complete, neurons in the map are labeled. The data class associated with the sheet to be checked is given as the label of the winner neuron for the presented data. It is established that the proposed method achieves as favorable classification accuracy as initial determination made by ophthalmologists.\"",
        "Document: \"On Determination of Ophthalmic Examinations Using Support Vector Machines. In this paper, a method of determining examination groups is presented for new outpatients visiting the department of ophthalmology, using support vector machines (SVM's). Assuming that interview sheets are divided into four classes, the proposed method copes with the examination determination as the classification of the sheets. The data are generated from handwriting sentences in the sheets, and they are arranged in the form of a matrix. Some nouns and adjectives in the sentences are chosen as elements of the matrix, and are assigned to columns of the matrix. The sentences in the sheets are assigned to rows of the matrix. Frequencies of the chosen words appearing in the sentences are basically given as element values in the matrix, and weighting is applied to element values associated with the words having comparatively high frequencies for sentences belonging to two classes at most. Normal SVM learning constructs a discrimination model, and defines four discriminant functions associated with the model. Since one-versus-all approach is employed, the class of data to be examined is determined according to output values of the four functions. It is established that the determination made by the proposed method achieves as favorable accuracy as the first determination made by an average ophthalmologist.\"",
        "Document: \"Dynamics of discrete-time quaternionic hopfield neural networks. We analyze a discrete-time quaternionic Hopfield neural network with continuous state variables updated asynchronously. The state of a neuron takes quaternionic value which is four-dimensional hypercomplex number. Two types of the activation function for updating neuron states are introduced and examined. The stable states of the networks are demonstrated through an example of small network.\"",
        "Document: \"A Defect Localization Scheme for Cellular Nanocomputers. Computers with device feature sizes of a few nanometers\u2014so-called nanocomputers\u2014are expected within a few decades, but this\n expectation is accompanied by the realization that the boundary conditions of such systems differ substantially from those\n of current VLSI-based computers. Prominent among the concerns is the increased degree of permanent defects that will affect\n nanocomputers, such as defects caused by imperfections at the manufacturing stage, but also defects occurring later, possibly\n even during the use of these systems. New techniques to deal with defects are called for, but given the huge number of devices\n involved, such techniques may need to be self-contained: they need be applicable at local levels without outside control,\n even while computations continue to take place. This paper proposes an important element in such techniques, i.e. the localization\n of defects among a huge number of devices. It employs a cellular automaton-based architecture, and uses statistical techniques\n combined with randomly moving configurations in the cellular space to estimate defect locations.\"",
        "1 is \"Built-in self-test of FPGA interconnect\", 2 is \"A computation-universal two-dimensional 8-state triangular reversible cellular automaton\"",
        "Given above information, for an author who has written the paper with the title \"Defect-tolerance in cellular nanocomputers\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007583": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Design for collective intelligence: pop-up communities in MOOCs.':",
        "Document: \"Fostering trust in virtual project teams: Towards a design framework grounded in a TrustWorthiness ANtecedents (TWAN) schema. Several collaboration problems in virtual project teams that work in knowledge-intensive contexts can be attributed to a hampered process of interpersonal trust formation. Solutions to trust formation problems need to be based on an understanding of how interpersonal trust forms in face-to-face project teams as well as on insight into how this process differs in virtual teams. Synthesizing literature from various disciplines, we propose a model for the formation of interpersonal trust between project team members. Taking this model as a starting point, we analyse how virtual settings may alter or even obstruct the process of trust formation. One method to improve the formation of interpersonal trust in virtual settings is to facilitate the assessment of trustworthiness. This can be done by making information available about individual virtual project team members. Previous research in virtual project teams focussed principally on the medium by which information is spread, for example, by phone, mail, or videoconferencing. Most researchers failed to take the specific content of the information into account, although there is general agreement that personal, non-task-related information is important to foster trust. For this, we propose to use the antecedents of trustworthiness, which until now have mainly been used as a framework to measure trust, as a design framework instead. This framework of antecedents can also be used to determine which type of information is relevant to assess each other's trustworthiness. We review existing literature on the antecedents of trustworthiness and extend the well-accepted antecedents of 'ability', 'benevolence' and 'integrity' with several other antecedents, such as 'communality' and 'accountability'. Together, these form the TrustWorthiness ANtecedents (TWAN) schema. We describe how these antecedents can be used to determine which information is relevant for team members assessing others' trustworthiness. In future research we will first verify this extended cognitive schema of trustworthiness (TWAN) empirically and then apply it to the design of artefacts or guidelines, such as a personal identity profile to support the assessment of trustworthiness in virtual project teams.\"",
        "Document: \"How To Create Flexible Runtime Delivery Of Distance Learning Courses. Distance Learning Providers serve large populations of learners, frequently offering the same course to different groups of learners over lengthy periods of time. This article argues that (re-) running e-learning courses with different combinations of learners and staff requires a distinction between abstract representations of courses and instances of these representations involving specific learners and support staff. The article provides an analysis of the requirements for multiple deliveries of courses, describes the design of an approach which meets these requirements together with experiences resulting from implementation of the design. The article concludes with a discussion of the approach.\"",
        "Document: \"Toward an Integrated Competence-Based System Supporting Lifelong Learning and Employability: Concepts, Model, and Challenges. Efficient and effective lifelong learning requires that people can make informed decisions about their continuous personal development in the different stages of their life. In this paper we state that lifelong learners need to be characterized as decision-makers. In order to improve the quality of their decisions, we propose the development of an integrated lifelong learning and employment support system, which traces learners' competence development and provides a decision support environment. An abstract conceptual model has been developed and the main design ideas have been documented using Z notation. Moreover, we analyzed the main technical challenges for the realization of the target system: competence information fusion, decision analysis models, spatial indexing structures and browsing structures and visualization of competence-related information objects.\"",
        "Document: \"Ad hoc transient communities: towards fostering knowledge sharing in learning networks. To enhance users' social embedding within learning networks, we propose to establish ad hoc transient communities. These communities serve a particular goal, exist for a limited period of time and operate according to specific social exchange policies that foster knowledge sharing. This paper explores the theoretical underpinnings of such communities. To this end, it identifies five theories that explain how group interaction affects community behaviour and argues that these can shed light on the expected behaviour of ad hoc transient communities. The paper also examines three conditions which community policies should adhere to in order to foster knowledge sharing and it suggests that ad hoc transient communities should be designed accordingly. Finally, it analyses how ad hoc transient communities can be implemented in an Information Technology (IT) platform which is currently under development, the TenCompetence Personal Competence Manager (PCM), and discusses the current practices in communities outside the learning arena. Finally, the paper explores the avenues for further work.\"",
        "Document: \"Using personal professional networks for learning in social work: need for insight into the real-world context. Professionals in social work practice depend on a high level of skills, intellectual ability and a wide knowledge base to find innovative solutions for the complex problems they encounter. They learn by experience and through social interaction using dialogue and discussion with relevant others to create new knowledge. To support their learning, they search for the most suitable and most relevant dialogue partner available in their extensive personal professional network. This is a difficult, high-skilled task, for which little technological support is available. This paper presents a literature review on the learning needs of these professionals and considers the use of technology as a means of supporting this type of learning. It argues for the need for more insight into the strategies used by professionals in building, maintaining and activating connections in their personal professional network for learning purposes.\"",
        "Document: \"Fostering online social capital through peer-support. Fostering social capital in online learning networks is essential for networked learning. Peer-support through so-called ad hoc transient groups AHTGs can help to socialise the network. A user-centric qualitative experiment was carried out to gauge the effect of AHTGs on social capital. Participants, who had access to AHTGs and a forum, were asked to fill in questionnaires and to participate in email interviews. The results confirm the hypothesis that participants believe the use of AHTGs will positively affect all aspects of social capital, except strengthening existing relationships. The participants viewed the AHTGs and the forum that was provided as sufficiently distinct from each other that these interaction environments could coexist and be mutually beneficial.\"",
        "Document: \"Matchmaking In Learning Networks: Bringing Learners Together For Knowledge Sharing. In this article we describe a system that matches learners with complementary content expertise in reaction to a learner-request for knowledge sharing. It works through the formation of ad hoc, transient communities, that exist for a limited period of time and stimulate learners socially to interact. The matchmaking system consists of a request module, a population module and a community module, all supported by a database that contains learning content, learner information and output of the system. The request module allows the learner to type in a request, the time span in which an answer should be provided and the content it is related to. The population module selects suitable learners to populate the community by determining their content competence, sharing competence, eligibility and availability. Modular Object-orientated Dynamic Learning Environment ( MOODLE) is used to host the community. A first experiment is briefly described that shows that content competence can be successfully determined using our method. Future experiments are discussed that aim at establishing the feasibility of the overall design.\"",
        "Document: \"A Design Model For Lifelong Learning Networks. The provision of lifelong learning facilities is considered to be a major new direction for higher and distance teaching educational institutes catering for the demands of industry and society. ICT networks will in future support seamless, ubiquitous access to lifelong learning facilities at home, at work, in schools and universities. This implies the development of new ways of organizing learning delivery that goes beyond course and programme-centric models. It envisions a learner-centred, learner-controlled model of distributed lifelong learning. We present a conceptual model for the support of lifelong learning which is based on notions from self-organization theory, learning communities, agent technologies and learning technology specifications such as IMS Learning Design. An exploratory implementation has been developed and used in practice. We reflect on the findings and future directions.\"",
        "Document: \"Which Recommender System Can Best Fit Social Learning Platforms?. This study aims to develop a recommender system for social learning platforms that combine traditional learning management systems with commercial social networks like Facebook. We therefore take into account social interactions of users to make recommendations on learning resources. We propose to make use of graph-walking methods for improving performance of the well-known baseline algorithms. We evaluate the proposed graph-based approach in terms of their F1 score, which is an effective combination of precision and recall as two fundamental metrics used in recommender systems area. The results show that the graph-based approach can help to improve performance of the baseline recommenders; particularly for rather sparse educational datasets used in this study.\"",
        "Document: \"Using peer-support to connect learning network participants to each other: an interdisciplinary approach. A large-scale experiment is presented which examines the feasibility of using a new method of peer-support called ad hoc transient groups AHTGs to foster social capital of learning network participants. In AHTGs participants that have a request are helped by other participants in a dedicated private space 'ad-hoc' which exists for a limited amount of time only 'transience'. To test the hypotheses that AHTGs foster social capital, AHTGs were introduced to a subset of the eTwinning learning network +130.000 teachers. To validate the results, a No-intervention group and a comparison group that used a forum to ask questions instead of AHTGs were also examined. Results show that AHTGs seem to foster social capital on the level of relationship characteristics and mutual support. Results on sense of connectedness were inconclusive. It is concluded that AHTGs have a decentralising effect, making the network less dependent on a few key participants. Furthermore, AHTGs have clearly been shown to have a low threshold to ask a question. Within the Forum group only a few core participants asked questions, yet many participants replied. It is concluded that AHTGs foster social capital in a different way when compared to a forum.\"",
        "1 is \"Deconstructing and reconstructing: Transforming primary science learning via a mobilized curriculum\", 2 is \"Electronic brainstorming in small and large groups\"",
        "Given above information, for an author who has written the paper with the title \"Design for collective intelligence: pop-up communities in MOOCs.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007606": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Registration of 3D Point Clouds and Meshes: A Survey from Rigid to Nonrigid':",
        "Document: \"Adding and subtracting eigenspaces with eigenvalue decomposition and singular value decomposition. This paper provides algorithms for adding and subtracting eigenspaces, thus allowing for incremental updating and downdating of data models. Importantly, and unlike previous work, we keep an accurate track of the mean of the data, which allows our methods to be used in classification applications. The result of adding eigenspaces, each made from a set of data, is an approximation to that which would obtain were the sets of data taken together. Subtracting eigenspaces yields a result approximating that which would obtain were a subset of data used. Using our algorithms, it is possible to perform \u2018arithmetic\u2019 on eigenspaces without reference to the original data. Eigenspaces can be constructed using either eigenvalue decomposition (EVD) or singular value decomposition (SVD). We provide addition operators for both methods, but subtraction for EVD only, arguing there is no closed-form solution for SVD. The methods and discussion surrounding SVD provide the principle novelty in this paper. We illustrate the use of our algorithms in three generic applications, including the dynamic construction of Gaussian mixture models.\"",
        "Document: \"A Robust Solution to Multi-modal Image Registration by Combining Mutual Information with Multi-scale Derivatives. In this paper we present a novel method for performing image registration of different modalities. Mutual Information (MI) is an established method for performing such registration. However, it is recognised that standard MI is not without some problems, in particular it does not utilise spatial information within the images. Various modifications have been proposed to resolve this, however these only offer slight improvement to the accuracy of registration. We present Feature Neighbourhood Mutual Information (FNMI) that combines both image structure and spatial neighbourhood information which is efficiently incorporated into Mutual Information by approximating the joint distribution with a covariance matrix (c.f. Russakoff's Regional Mutual Information). Results show that our approach offers a very high level of accuracy that improves greatly on previous methods. In comparison to Regional MI, our method also improves runtime for more demanding registration problems where a higher neighbourhood radius is required. We demonstrate our method using retinal fundus photographs and scanning laser ophthalmoscopy images, two modalities that have received little attention in registration literature. Registration of these images would improve accuracy when performing demarcation of the optic nerve head for detecting such diseases as glaucoma.\"",
        "Document: \"Automatic landmarking for building biological shape models. In this article we present a new method for automatic landmark extraction from the contours of biological specimens. Our ultimate goal is to enable automatic identification of biological specimens in photographs and drawings held in a database. We propose to use Active Appearance Models for visual indexing of both photographs and drawings. Automatic landmark extraction will assist us in building the models. Below we de- scribe the results of using our method on draw- ings and photographs of examples of diatoms, and present an Active Shape Model built using auto- matically extracted data.\"",
        "Document: \"Numerical Methods for Beautification of Reverse Engineered Geometric Models. Boundary representation models reconstructed from 3D range data suffer from various inaccuracies caused by noise in the data and the model building software. The quality of such models can be improved in a beautification step, which finds geometric regularities approximately present in the model and tries to impose a consistent subset of these regularities on the model. A framework for beautification and numerical methods to select and solve a consistent set of constraints deduced from a set of regularities are presented. For the initial selection of consistent regularities likely to be part of the model's ideal design priorities, and rules indicating simple inconsistencies between the regularities are employed. By adding regularities consecutively to an equation system and trying to solve it by using quasi-Newton optimization methods, inconsistencies and redundancies are detected. The results of experiments are encouraging and show potential for an expansion of the methods based on degree of freedom analysis.\"",
        "Document: \"Behaviour transfer between expressive talking heads. Performance driven animation (also called expression mapping) is an important area of research in computer graphics. The technique relies on using an input face to animate either a computer generated face or a real face [Cosker et al. 2008; Vlasic et al. 2005]. One of the core challenges with facial animation is the automatic synthesis of facial dynamics.\"",
        "Document: \"Viewpoint Selection for Complete Surface Coverage of Three Dimensional Objects. Many machine vision tasks, e.g. object recognition and object inspection, cannot be performed robustly from a single image. For certain tasks (e.g. 3D object recognition and automated inspection) the availability of multiple views of an object is a requirement. This paper presents a novel approach to selecting a minimised number of views that allow each object face to be adequately viewed according to specified constraints on viewpoints and other features. The planner is generic and can be employed for a wide range of multiple view acquisition systems, ranging from camera systems mounted on the end of a robot arm, i.e. an eye-in-hand camera setup, to a turntable and fixed stereo cameras to allow different views of an object to be obtained. The results (both simulated and real) given focus on planning with a fixed camera and turntable.\"",
        "Document: \"Local topological beautification of reverse engineered models. Boundary representation models reconstructed from 3D range data suffer from various inaccuracies caused by noise in the data and by numerical errors in the model building software. The quality of such models can be improved in a beautification step, where geometric regularities need to be detected and imposed on the model, and defects requiring topological change need to be corrected. This paper considers changes to the topology such as the removal of short edges, small faces and sliver faces, filling of holes in the surface of the model (arising due to missing data), adjusting pinched faces, etc. A practical algorithm for detecting and correcting such problems is presented. Analysis of the algorithm and experimental results show that the algorithm is able to quickly provide the desired changes. Most of the time required for topological beautification is spent on adjusting the geometry to agree with the new topology.\"",
        "Document: \"Quantitative analysis of facial movement--a review of three-dimensional imaging techniques. Objective analysis of facial movement forms an important consideration in the assessment and outcome of several medical disciplines. Technological advances in the field of medical imaging have meant techniques to measure facial movement have evolved from subjective grading scales to facial-marker-based tracking systems to the most recent development of true three-dimensional marker-free systems. The aim of this paper is to provide a comprehensive review of the literature in this evolving field of medical imaging particularly focusing on three-dimensional analysis of facial movement and outlining the current concepts in objective analysis of the data set.\"",
        "Document: \"Robust segmentation of primitives from range data in the presence of geometric degeneracy. This paper addresses a common problem in the segmentation of range images. We would like to identify and fit surfaces of known type wherever these are a good fit. This paper presents methods for the least-squares fitting of spheres, cylinders, cones, and tori to 3D point data, and their application within a segmentation framework. Least-squares fitting of surfaces other than planes, even of simple geometric type, has been rarely studied. Our main application areas of this research are reverse engineering of solid models from depth-maps and automated 3D inspection where reliable extraction of these surfaces is essential. Our fitting method has the particular advantage of being robust in the presence of geometric degeneracy, i.e., as the principal curvatures of the surfaces being fitted decrease (or become more equal), the results returned naturally become closer and closer to those surfaces of \u9a74simpler type,\u9a74 i.e., planes, cylinders, cones, or spheres, which best describe the data. Many other methods diverge because, in such cases, various parameters or their combination become infinite.\"",
        "Document: \"Automatic analysis of composite activities in video sequences using Key Action Discovery and hierarchical graphical models. Modelling human activities as temporal sequences of their constituent actions has been the object of much research effort in recent years. However, most of this work concentrates on tasks where the action vocabulary is relatively small and/or each activity can be performed in a limited number of ways. In this work, we propose a novel and robust framework for analysing prolonged activities arising in tasks which can be effectively achieved in a variety of ways, which we name mid-term activities. We show that we are able to efficiently analyse and recognise such activities and also detect potential errors in their execution. To achieve this, we introduce an activity classification method which we name the Key Action Discovery system. We demonstrate that this method combined with temporal modelling of activities' constituent actions with the aid of hierarchical graphical models offers higher classification accuracy compared to current activity identification schemes.\"",
        "1 is \"Global stereo reconstruction under second-order smoothness priors.\", 2 is \"DynTex: A comprehensive database of dynamic textures\"",
        "Given above information, for an author who has written the paper with the title \"Registration of 3D Point Clouds and Meshes: A Survey from Rigid to Nonrigid\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007608": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Adaptive One-Way Functions and Applications':",
        "Document: \"A Group Signature Scheme from Lattice Assumptions. Group signature schemes allow users to sign messages on behalf of a group while (1) maintaining anonymity (within that group) with respect to an outside observer, yet (2) ensuring traceability of a signer (by the group manager) when needed. In this work we give the first construction of a group signature scheme based on lattices (more precisely, the learning with errors assumption), in the random oracle model. Towards our goal, we construct a new algorithm for sampling a basis for an orthogonal lattice, together with a trapdoor, that may be of independent interest.\"",
        "Document: \"Adaptive One-Way Functions and Applications. We introduce new and general complexity theoretic hardness assumptions. These assumptions abstract out concrete properties of a random oracle and are significantly stronger than traditional cryptographic hardness assumptions; however, assuming their validity we can resolve a number of long-standing open problems in cryptography.\"",
        "Document: \"SHIELD: Scalable Homomorphic Implementation of Encrypted Data-Classifiers. Homomorphic encryption (HE) systems enable computations on encrypted data, without decrypting and without knowledge of the secret key. In this work, we describe an optimized Ring Learning With Errors (RLWE) based implementation of a variant of the HE system recently proposed by Gentry, Sahai and Waters (GSW). Although this system was widely believed to be less efficient than its contemporaries, we demonstrate quite the opposite behavior for a large class of applications. We first highlight and carefully exploit the algebraic features of the system to achieve significant speedup over the state-of-the-art HE implementation, namely the IBM homomorphic encryption library (HElib). We introduce several optimizations on top of our HE implementation, and use the resulting scheme to construct a homomorphic Bayesian spam filter, secure multiple keyword search, and a homomorphic evaluator for binary decision trees. Our results show a factor of $10\\\\times$ improvement in performance (under the same security settings and CPU platforms) compared to IBM HElib for these applications. Our system is built to be easily portable to GPUs (unlike IBM HElib) which results in an additional speedup of up to a factor of $103.5\\\\times$ to offer an overall speedup of $1{,}035\\\\times$ .\"",
        "Document: \"A simple BGN-Type cryptosystem from LWE. We construct a simple public-key encryption scheme that supports polynomially many additions and one multiplication, similar to the cryptosystem of Boneh, Goh, and Nissim (BGN). Security is based on the hardness of the learning with errors (LWE) problem, which is known to be as hard as certain worst-case lattice problems. Some features of our cryptosystem include support for large message space, an easy way of achieving formula-privacy, a better message-to-ciphertext expansion ratio than BGN, and an easy way of multiplying two encrypted polynomials. Also, the scheme can be made identity-based and leakage-resilient (at the cost of a higher message-to-ciphertext expansion ratio).\"",
        "Document: \"A Tight Bound for Set Disjointness in the Message-Passing Model. In a multiparty message-passing model of communication, there are k players. Each player has a private input, and they communicate by sending messages to one another over private channels. While this model has been used extensively in distributed computing and in secure multiparty computation, lower bounds on communication complexity in this model and related models have been somewhat scarce. In recent work, strong lower bounds of the form \u03a9(n \u00b7 k) were obtained for several functions in the message-passing model; however, a lower bound on the classical set disjoint ness problem remained elusive. In this paper, we prove a tight lower bound of \u03a9(n \u00b7 k) for the set disjoint ness problem in the message passing model. Our bound is obtained by developing information complexity tools for the message-passing model and proving an information complexity lower bound for set disjoint ness.\"",
        "Document: \"Circuit-ABE from LWE: Unbounded Attributes and Semi-Adaptive Security. We construct an LWE-based key-policy attribute-based encryption ABE scheme that supports attributes of unbounded polynomial length. Namely, the size of the public parameters is a fixed polynomial in the security parameter and a depth bound, and with these fixed length parameters, one can encrypt attributes of arbitrary length. Similarly, any polynomial size circuit that adheres to the depth bound can be used as the policy circuit regardless of its input length recall that a depth d circuit can have as many as $$2^d$$ inputs. This is in contrast to previous LWE-based schemes where the length of the public parameters has to grow linearly with the maximal attribute length. We prove that our scheme is semi-adaptively secure, namely, the adversary can choose the challenge attribute after seeing the public parameters but before any decryption keys. Previous LWE-based constructions were only able to achieve selective security. We stress that the \\\"complexity leveraging\\\" technique is not applicable for unbounded attributes. We believe that our techniques are of interest at least as much as our end result. Fundamentally, selective security and bounded attributes are both shortcomings that arise out of the current LWE proof techniques that program the challenge attributes into the public parameters. The LWE toolbox we develop in this work allows us to delay this programming. In a nutshell, the new tools include a way to generate an a-priori unbounded sequence of LWE matrices, and have fine-grained control over which trapdoor is embedded in each and every one of them, all with succinct representation.\"",
        "Document: \"3-Message Zero Knowledge Against Human Ignorance. The notion of Zero Knowledge has driven the field of cryptography since its conception over thirty years ago. It is well established that two-message zero-knowledge protocols for NP do not exist, and that four-message zero-knowledge arguments exist under the minimal assumption of one-way functions. Resolving the precise round complexity of zero-knowledge has been an outstanding open problem for far too long. In this work, we present a three-message zero-knowledge argument system with soundness against uniform polynomial-time cheating provers. The main component in our construction is the recent delegation protocol for RAM computations Kalai and Paneth, TCC 2016B and Brakerski, Holmgren and Kalai, ePrint 2016. Concretely, we rely on a three-message variant of their protocol based on a key-less collision-resistant hash functions secure against uniform adversaries as well as other standard primitives. More generally, beyond uniform provers, our protocol provides a natural and meaningful security guarantee against real-world adversaries, which we formalize following Rogaway's \\\"human-ignorance\\\" approach VIETCRYPT 2006: in a nutshell, we give an explicit uniform reduction from any adversary breaking the soundness of our protocol to finding collisions in the underlying hash function.\"",
        "Document: \"Robustness of the Learning with Errors Assumption. Starting with the work of Ishai-Sahai-Wagner and Micali-Reyzin, a new goal has been set within the theory of cryptography community, to design cryptographic primitives that are secure against large classes of side-channel attacks. Recently, many works have focused on designing various cryptographic primitives that are robust (retain security) even when the secret key is \"leaky\", under various intractability assumptions. In this work we propose to take a step back and ask a more basic question: which of our cryptographic assumptions (rather than cryptographic schemes) are robust in presence of leakage of their underlying secrets? Our main result is that the hardness of the learning with error (LWE) problem implies its hardness with leaky secrets. More generally, we show that the standard LWE assumption implies that LWE is secure even if the secret is taken from an arbitrary distribution with sufficient entropy, and even in the presence of hard-to-invert auxiliary inputs. We exhibit various applications of this result. 1. Under the standard LWE assumption, we construct a symmetric-key encryption scheme that is robust to secret key leakage, and more generally maintains security even if the secret key is taken from an arbitrary distribution with sufficient entropy (and even in the presence of hard-to-invert auxiliary inputs). 2. Under the standard LWE assumption, we construct a (weak) obfuscator for the class of point func- tions with multi-bit output. We note that in most schemes that are known to be robust to leakage, the parameters of the scheme depend on the maximum leakage the system can tolerate, and hence the efficiency degrades with the maximum anticipated leakage, even if no leakage occurs at all! In contrast, the fact that we rely on a robust assumption allows us to construct a single symmetric-key encryption scheme, with parameters that are independent of the anticipated leakage, that is robust to any leakage (as long as the secret key has sufficient entropy left over). Namely, for any k < n (where n is the size of the secret key), if the secret key has only entropyk, then the security relies on the LWE assumption with secret size roughlyk.\"",
        "Document: \"Computing Blindfolded: New Developments in Fully Homomorphic Encryption. A fully homomorphic encryption scheme enables computation of arbitrary functions on encrypted data. Fully homomorphic encryption has long been regarded as cryptography's prized \"holy grail\" - extremely useful yet rather elusive. Starting with the groundbreaking work of Gentry in 2009, the last three years have witnessed numerous constructions of fully homomorphic encryption involving novel mathematical techniques, and a number of exciting applications. We will take the reader through a journey of these developments and provide a glimpse of the exciting research directions that lie ahead.\"",
        "Document: \"Lattice-based FHE as secure as PKE. We show that (leveled) fully homomorphic encryption (FHE) can be based on the hardness of O(n1.5+\u03b5)-approximation for lattice problems (such as GapSVP) under quantum reductions for any \u03b5 \u232a 0 (or O(n2+\u03b5)-approximation under classical reductions). This matches the best known hardness for \"regular\" (non-homomorphic) lattice based public-key encryption up to the \u03b5 factor. A number of previous methods had hit a roadblock at quasipolynomial approximation. (As usual, a circular security assumption can be used to achieve a non-leveled FHE scheme.) Our approach consists of three main ideas: Noise-bounded sequential evaluation of high fan-in operations; Circuit sequentialization using Barrington's Theorem; and finally, successive dimension-modulus reduction.\"",
        "1 is \"Lower Bounds for Concurrent Self Composition\", 2 is \"Local zero knowledge\"",
        "Given above information, for an author who has written the paper with the title \"Adaptive One-Way Functions and Applications\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007718": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'SPARTAN: Semantic integration of big spatio-temporal data from streaming and archival sources':",
        "Document: \"Identifying the most influential data objects with reverse top-k queries. Top-k queries are widely applied for retrieving a ranked set of the k most interesting objects based on the individual user preferences. As an example, in online marketplaces, customers (users) typically seek a ranked set of products (objects) that satisfy their needs. Reversing top-k queries leads to a query type that instead returns the set of customers that find a product appealing (it belongs to the top-k result set of their preferences). In this paper, we address the challenging problem of processing queries that identify the top-m most influential products to customers, where influence is defined as the cardinality of the reverse top-k result set. This definition of influence is useful for market analysis, since it is directly related to the number of customers that value a particular product and, consequently, to its visibility and impact in the market. Existing techniques require processing a reverse top-k query for each object in the database, which is prohibitively expensive even for databases of moderate size. In contrast, we propose two algorithms, SB and BB, for identifying the most influential objects: SB restricts the candidate set of objects that need to be examined, while BB is a branch-and-bound algorithm that retrieves the result incrementally. Furthermore, we propose meaningful variations of the query for most influential objects that are supported by our algorithms. Our experiments demonstrate the efficiency of our algorithms both for synthetic and real-life datasets.\"",
        "Document: \"Efficient execution plans for distributed skyline query processing. In this paper, we study the generation of efficient execution plans for skyline query processing in large-scale distributed environments. In such a setting, each server stores autonomously a fraction of the data, thus all servers need to process the skyline query. An execution plan defines the order in which the individual skyline queries are processed on different servers, and influences the performance of query processing. Querying servers consecutively reduces the amount of transferred data and the number of queried servers, since skyline points obtained by one server prune points in the subsequent servers, but also increases the latency of the system. To address this trade-off, we introduce a novel framework, called SkyPlan, for processing distributed skyline queries that generates execution plans aiming at optimizing the performance of query processing. Thus, we quantify the gain of querying consecutively different servers. Then, execution plans are generated that maximize the overall gain, while also taking into account additional objectives, such as bounding the maximum number of hops required for the query or balancing the load on different servers fairly. Finally, we present an algorithm for distributed processing based on the generated plan that continuously refines the execution plan during in-network processing. Our framework consistently outperforms the state-of-the-art algorithm.\"",
        "Document: \"Processing of Rank Joins in Highly Distributed Systems. In this paper, we study efficient processing of rank joins in highly distributed systems, where servers store fragments of relations in an autonomous manner. Existing rank-join algorithms exhibit poor performance in this setting due to excessive communication costs or high latency. We propose a novel distributed rank-join framework that employs data statistics, maintained as histograms, to determine the subset of each relational fragment that needs to be fetched to generate the top-k join results. At the heart of our framework lies a distributed score bound estimation algorithm that produces sufficient score bounds for each relation, that guarantee the correctness of the rank-join result set, when the histograms are accurate. Furthermore, we propose a generalization of our framework that supports approximate statistics, in the case that the exact statistical information is not available. An extensive experimental study validates the efficiency of our framework and demonstrates its advantages over existing methods.\"",
        "Document: \"Angle-based space partitioning for efficient parallel skyline computation. Recently, skyline queries have attracted much attention in the database research community. Space partitioning techniques, such as recursive division of the data space, have been used for skyline query processing in centralized, parallel and distributed settings. Unfortunately, such grid-based partitioning is not suitable in the case of a parallel skyline query, where allpartitions are examined at the same time, since many data partitions do not contribute to the overall skyline set, resulting in a lot of redundant processing. In this paper we propose a novel angle-based space partitioning scheme using the hyperspherical coordinates of the data points. We demonstrate both formally as well as through an exhaustive set of experiments that this new scheme is very suitable for skyline query processing in a parallel share-nothing architecture. The intuition of our partitioning technique is that the skyline points are equally spread to all partitions. We also show that partitioning the data according to the hyperspherical coordinates manages to increase the average pruning power of points within a partition. Our novel partitioning scheme alleviates most of the problems of traditional grid partitioning techniques, thus managing to reduce the response time and share the computational workload more fairly. As demonstrated by our experimental study, our technique outperforms grid partitioning in all cases, thus becoming an efficient and scalable solution for skyline query processing in parallel environments.\"",
        "Document: \"A survey of large-scale analytical query processing in MapReduce. Enterprises today acquire vast volumes of data from different sources and leverage this information by means of data analysis to support effective decision-making and provide new functionality and services. The key requirement of data analytics is scalability, simply due to the immense volume of data that need to be extracted, processed, and analyzed in a timely fashion. Arguably the most popular framework for contemporary large-scale data analytics is MapReduce, mainly due to its salient features that include scalability, fault-tolerance, ease of programming, and flexibility. However, despite its merits, MapReduce has evident performance limitations in miscellaneous analytical tasks, and this has given rise to a significant body of research that aim at improving its efficiency, while maintaining its desirable properties. This survey aims to review the state of the art in improving the performance of parallel query processing using MapReduce. A set of the most significant weaknesses and limitations of MapReduce is discussed at a high level, along with solving techniques. A taxonomy is presented for categorizing existing research on MapReduce improvements according to the specific problem they target. Based on the proposed taxonomy, a classification of existing research is provided focusing on the optimization objective. Concluding, we outline interesting directions for future parallel data processing systems.\"",
        "Document: \"DESENT: decentralized and distributed semantic overlay generation in P2P networks. The current approach in web searching, i.e., using centralized search engines, rises issues that question their future applicability: 1) coverage and scalability, 2) freshness, and 3) information monopoly. Performing web search using a P2P architecture that consists of the actual web servers has the potential to tackle those issues. In order to achieve the desired performance and scalability, as well as enhancing search quality relative to centralized search engines, semantic overlay networks (SONS) connecting peers storing semantically related information can be employed. The lack of global content/topology knowledge in a P2P system is the key challenge in forming SONS, and this paper describes an unsupervised approach for decentralized and distributed generation of SONS (DESENT). Through simulations and analytical cost models we verify our claims regarding performance, scalability, and quality.\"",
        "Document: \"Multidimensional routing indices for efficient distributed query processing. Traditional routing indices in peer-to-peer (P2P) networks are mainly designed for document retrieval applications and maintain aggregated one-dimensional values representing the number of documents that can be obtained in a certain direction in the network. In this paper, we introduce the concept of multidimensional routing indices (MRIs), which are suitable for handling multidimensional data represented by minimum bounding regions (MBRs). Depending on data distribution on peers, the aggregation of the MBRs may lead to MRIs that exhibit extremely poor performance, which renders them ineffective. Thus, focusing on a hybrid unstructured P2P network, we analyze the parameters for building MRIs of high selectivity. We present techniques that boost the query routing performance by detecting similar peers and grouping and reassigning these peers to other parts of the hybrid network in a distributed and scalable way. We demonstrate the advantages of our approach using large-scale simulations.\"",
        "Document: \"Schema Caching for Improved XML Query Processing in P2P Systems. The advent and popularity of the World Wide Web (WWW) has enabled access to a variety of semi-structured data and, when available, this data follows some common XML schema. On the other hand the distribution of content has made centralized solutions inappropriate, entering the era of peer-to-peer (P2P) computing, where content is stored in XML databases residing on peers. In this paper, we propose XML schema caching as a summary indexing technique for searching in P2P networks. We study XML query routing in unstructured P2P networks, comparing different search strategies and showing the advantages of our approach in terms of completeness of the search.\"",
        "Document: \"On temporal-constrained sub-trajectory cluster analysis. Cluster analysis over Moving Object Databases (MODs) is a challenging research topic that has attracted the attention of the mobility data mining community. In this paper, we study the temporal-constrained sub-trajectory cluster analysis problem, where the aim is to discover clusters of sub-trajectories given an ad-hoc, user-specified temporal constraint within the dataset\u2019s lifetime. The problem is challenging because: (a) the time window is not known in advance, instead it is specified at query time, and (b) the MOD is continuously updated with new trajectories. Existing solutions first filter the trajectory database according to the temporal constraint, and then apply a clustering algorithm from scratch on the filtered data. However, this approach is extremely inefficient, when considering explorative data analysis where multiple clustering tasks need to be performed over different temporal subsets of the database, while the database is updated with new trajectories. To address this problem, we propose an incremental and scalable solution to the problem, which is built upon a novel indexing structure, called Representative Trajectory Tree (). ReTraTree acts as an effective spatio-temporal partitioning technique; partitions in  correspond to groupings of sub-trajectories, which are incrementally maintained and assigned to representative (sub-)trajectories. Due to the proposed organization of sub-trajectories, the problem under study can be efficiently solved as simply as executing a query operator on , while insertion of new trajectories is supported. Our extensive experimental study performed on real and synthetic datasets shows that our approach outperforms a state-of-the-art in-DBMS solution supported by PostgreSQL by orders of magnitude.\"",
        "Document: \"Exploratory product search using top-k join queries. Given a relation that contains main products and a set of relations corresponding to accessory products that can be combined with a main product, the Exploratory Top-k Join query retrieves the k best combinations of main and accessory products based on user preferences. As a result, the user is presented with a set of k combinations of distinct main products, where a main product is combined with accessory products only if the combination has a better score than the single main product. We model this problem as a rank-join problem, where each combination is represented by a tuple from the main relation and a set of tuples from (some of) the accessory relations. The nature of the problem is challenging because the inclusion of accessory products is not predefined by the user, but instead all potential combinations (joins) are explored during query processing in order to identify the highest scoring combinations. Existing approaches cannot be directly applied to this problem, as they are designed for joining a predefined set of relations. In this paper, we present algorithms for processing exploratory top-k joins that adopt the pull-bound framework for rank-join processing. We introduce a novel algorithm (XRJN) which employs a more efficient bounding scheme and allows earlier termination of query processing. We also provide theoretical guarantees on the performance of this algorithm, by proving that XRJN is instance-optimal. In addition, we consider a pulling strategy that boosts the performance of query processing even further. Finally, we conduct a detailed experimental study that demonstrates the efficiency of the proposed algorithms in various setups. HighlightsWe propose a new exploration technique using eXploratory Top-k Join (XTJ) queries.We analyze the XTJ-query properties and we propose an efficient algorithm (XRJN).We provide strong theoretical guarantees on the performance of our algorithm.We introduce an improved pulling strategy that reduces the overall processing cost.We generalize XTJ queries to efficiently combinations organized in groups and to include various aggregation functions.\"",
        "1 is \"Analysis of the Clustering Properties of the Hilbert Space-Filling Curve\", 2 is \"Spatial join selectivity using power laws\"",
        "Given above information, for an author who has written the paper with the title \"SPARTAN: Semantic integration of big spatio-temporal data from streaming and archival sources\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007761": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'The Structure and Performance of Efficient Interpreters':",
        "Document: \"Retargeting JIT Compilers by using C-Compiler Generated Executable Code. JIT compilers produce fast code, whereas interpreters are easy to port between architectures. We propose to combine the advantages of these language implementation techniques as follows: we generate native code by concatenating and patching machine code fragments taken from interpreter-derived code (generated by a C compiler); we completely eliminate the interpreter dispatch overhead and accesses to the interpreted code by patching jump target addresses and other constants into the fragments. In this paper we present the basic idea, discuss some issues in more detail, and present results from a proof-of-concept implementation, providing speedups of up to 1.87 over the fastest previous interpreter-based technique, and performance comparable to simple native-code compilers. The effort required for retargeting our implementation from the 386 to the PPC architecture was less than a person-day.\"",
        "Document: \"Virtual machine showdown: stack versus registers. Virtual machines (VMs) are commonly used to distribute programs in an architecture-neutral format, which can easily be interpreted or compiled. A long-running question in the design of VMs is whether stack architecture or register architecture can be implemented more efficiently with an interpreter. We extend existing work on comparing virtual stack and virtual register architectures in two ways. Firstly, our translation from stack to register code is much more sophisticated. The result is that we eliminate an average of more than 47% of executed VM instructions, with the register machine bytecode size only 25% larger than that of the corresponding stack bytecode. Secondly we present an implementation of a register machine in a fully standard-compliant implementation of the Java VM. We find that, on the Pentium 4, the register architecture requires an average of 32.3% less time to execute standard benchmarks if dispatch is performed using a C switch statement. Even if more efficient threaded dispatch is available (which requires labels as first class values), the reduction in running time is still approximately 26.5% for the register architecture.\"",
        "Document: \"Fpga Based Sparse Matrix Vector Multiplication Using Commodity Dram Memory. Sparse matrix by vector multiplication (SMV) is a key operation of many scientific and engineering applications. Field Programmable Gate Arrays (FPGAs) have the potential to significantly improve the performance of computationally intensive applications which are dominated by SMV. A shortcoming of most existing FPGA SMV implementations is that they use on-chip Block RAM or external SRAM to store the matrix, which severely limits the problem size. Real applications, such as Finite Element Analysis (FEA), require large memories. Realistically this capacity can only be provided by commodity DRAM. In this paper we address the problem of SMV for large matrices using commodity memory. We implement SPAR, a special purpose architecture that was previously proposed for large SMV computations in a VLSI co-processor using cheap external memory. We present an empirical evaluation of the SPAR architecture for use on FPGAs and highlight challenges that arise when tackling realistic FEA problems.\"",
        "Document: \"Optimizing interpreters by tuning opcode orderings on virtual machines for modern architectures: or: how I learned to stop worrying and love hill climbing. Virtual machines (VMs) are commonly used to execute programs written in languages such as Java, Python and Lua. VMs are typically implemented using an interpreter, a JIT compiler, or some combination of the two. A long-standing question in the design of VM interpreters is whether it is worthwhile to reorder the cases in the main interpreter loop to improve code locality. We investigate this phenomenon using an iterative, feedback-directed approach. We show that the ordering of the cases in the interpreter loop has a significant impact on performance on recent processors. Using hardware performance counters, we demonstrate that the performance improvement is primarily the result of indirect branch prediction, not instruction cache locality. We propose a number of strategies to achieve better orderings, and evaluate these strategies in the Python and Lua virtual machine interpreters. We show speedups of up to 40%.\"",
        "Document: \"Practical Algorithms for Finding Extremal Sets. The minimal sets within a collection of sets are defined as the ones that do not have a proper subset within the collection, and the maximal sets are the ones that do not have a proper superset within the collection. Identifying extremal sets is a fundamental problem with a wide range of applications in SAT solvers, data mining, and social network analysis. In this article, we present two novel improvements of the high-quality extremal set identification algorithm, AMS-Lex, described by Bayardo and Panda. The first technique uses memoization to improve the execution time of the single-threaded variant of the AMS-Lex, while our second improvement uses parallel programming methods. In a subset of the presented experiments, our memoized algorithm executes more than 400 times faster than the highly efficient publicly available implementation of AMS-Lex. Moreover, we show that our modified algorithm's speedup is not bounded above by a constant and that it increases as the length of the common prefixes in successive input itemsets increases. We provide experimental results using both real-world and synthetic datasets, and show our multithreaded variant algorithm outperforming AMS-Lex by 3 to 6 times. We find that on synthetic input datasets, when executed using 16 CPU cores of a 32-core machine, our multithreaded program executes about as fast as the state-of-the-art parallel GPU-based program using an NVIDIA GTX 580 graphics processing unit.\"",
        "Document: \"Measuring the impact of object-oriented techniques in grande applications: a method-level analysis. In this work we seek to provide a foundation for the study of the level of use of object-oriented techniques in Java programs in general, and scientific applications in particular. Specifically, we focus on the use of small methods, and the frequency with which they are called, since this forms the basis for the study of method inlining, an important optimisation technique. We compare the Grande and SPEC benchmark suites, and note a significant difference in the nature and composition of these suites.\"",
        "Document: \"An improved simulated annealing heuristic for static partitioning of task graphs onto heterogeneous architectures. We present a simulated annealing based partitioning technique for mapping task graphs, onto heterogeneous processing architectures. Task partitioning onto homogeneous architectures to minimize the makespan of a task graph, is a known NP-hard problem. Heterogeneity greatly complicates the aforementioned partitioning problem, thus making heuristic solutions essential. A number of heuristic approaches have been proposed, some using simulated annealing. We propose a simulated annealing method with a novel NEXT STATE function to enable exploration of different regions of the global search space when the annealing temperature is high and making the search more local as the temperature drops. The novelty of our approach is two fold: (1) we go a step further than the existing scientific literature, considering heterogeneity at levels of task parallelism, data parallelism and communication. (2) We present a novel algorithm that uses simulated annealing to find better partitions in the presence of heterogeneous architectures, data parallel execution units, and significant data communication costs. We conduct a statistical analysis of the performance of the proposed method, which shows that our approach clearly outperforms the existing simulated annealing method.\"",
        "Document: \"Low-Cost microarchitectural techniques for enhancing the prediction of return addresses on high-performance trace cache processors. This paper discusses the effects of the prediction of return addresses in high-performance processors designed with trace caches. We show that a traditional return address stack used in such a processor predicts return addresses poorly if a trace cache line contains a function call and a return. This situation can often be observed for processors demanding aggressive instruction fetch bandwidth. Thus, we propose two potential schemes to improve the prediction accuracy of return addresses. We demonstrate that the proposed schemes increase the return address prediction rates reasonably using minimal hardware support. We also analyze the effects of various trace cache configurations on the return address prediction accuracy such as trace cache set associativity, cache size and line size. Our experimental results show that the average return address prediction accuracy across several benchmarks can be up to 11% better than a traditional return address stack in a high-performance processor with a trace cache.\"",
        "Document: \"Platform Independent Dynamic Java Virtual Machine Analysis: The Java Grande Forum Benchmark Suite. In this paper we present a platform independent analysis of the dynamic profiles of Java programs when executing on the Java Virtual Machine. The Java programs selected are taken from the Java Grande Forum benchmark suite and five different Java-to-bytecode compilers are analysed. The results presented describe the dynamic instruction usage frequencies. as well as the sizes of the local variable, parameter and operand stacks during execution on the JVM.These results, presenting a picture of the actual (rather than presumed) behaviour of the JVM, have implications both for the coverage aspects of the Java Grande benchmark suites, for the performance of the Java-to-bytecode compilers and for the design of the JVM. Copyright (C) 2003 John Wiley Sons, Ltd.\"",
        "Document: \"Multiple-Valued Caches for Power-Efficient Embedded Systems. We present an abstract axiomatization of generalized entropy using the notion of ordinal number and the new concept of systemic set of equivalence relations. The aximoatization applies to arbitrary sets and extends previous results obtained for the finite ...\"",
        "1 is \"Movement imitation with nonlinear dynamical systems in humanoid robots\", 2 is \"Mapping stream programs onto heterogeneous multiprocessor systems\"",
        "Given above information, for an author who has written the paper with the title \"The Structure and Performance of Efficient Interpreters\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007791": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Algorithms for selective enumeration of prime implicants':",
        "Document: \"Skin lesion image segmentation using a color genetic algorithm. The development of computer-aided diagnosis systems for skin cancer detection has attracted a lot of interest in the research community. In particular, the availability of an accurate automatic segmentation tool for detecting skin lesions from background skin is of primary importance for the overall diagnosis system. In this paper we investigate the capability of a color image segmentation method based on Genetic Algorithms in discriminating skin lesions. Experimental results show that the segmentation approach is able to detect lesion borders quite accurately, thus coupled with a merging technique of the surrounding region could reveal a promising method for isolating skin tumor.\"",
        "Document: \"Twitter-based Influenza Surveillance: An Analysis of the 2016-2017 and 2017-2018 Seasons in Italy. Influenza surveillance through social media data is becoming an important research topic because it could enhance the capabilities of official surveillance systems in monitoring the outbreak of seasonal flu, by providing healthcare organization with improved situational awareness. In this paper, the two influenza seasons 2016-2017 and 2017-2018, restricted to Italy, are investigated by analyzing the tweets posted by users regarding influenza-like illness. Two types of analysis are performed. The first studies the correlation between the tweets containing the most frequent flu related words with the data provided by the Italian InfluNet surveillance system. The second one examines the sentiment of people on the medicines used to heal flu. We show that there is a strict correlation between the reports published on the InfluNet system, and the contents posted by Twitter users about their symptoms and health state. Moreover, we found that the sentiment expressed by people regarding the treatment, in terms of medicines, taken to heal seems rather negative.\n\n\"",
        "Document: \"DynamicNet: an effective and efficient algorithm for supporting community evolution detection in time-evolving information networks. DynamicNet, an effective and efficient algorithm for supporting community evolution detection in time-evolving information networks is presented and experimentally evaluated in this paper. DynamicNet introduces a graph-based model-theoretic approach to represent time-evolving information networks, and to capture how they change over time. A central feature of DynamicNet is represented by the ability of supporting matching-based community evolution detection, by identifying several classes of community transitions. Experimental results clearly demonstrate the reliability and the efficiency of our proposal.\"",
        "Document: \"Analysis of the Italian Tweet Political Sentiment in 2014 European Elections. Nowadays, the increasing popularity of micro-blogging websites such as Twitter, makes available an impressive amount of information about users and their social behavior. These media contents give the opportunity to study opinions, sentiments, attitudes, reactions towards particular events. In this paper the sentiment of Italian Twitter users in the period of 2014 European elections has been investigated, and an analysis of the opinions regarding parties and leaders along a period of three months is presented. Moreover, hashtag networks have been generated and clustered, and communities of users, who posted tweets containing the hashtags appearing in these clusters, have been extracted with the aim of unveiling and tracking thematic discussion groups.\"",
        "Document: \"Using Self-similarity to Adapt Evolutionary Ensembles for the Distributed Classification of Data Streams. Distributed stream-based classification methods have many important applications such as sensor data analysis, network security, and business intelligence. An important challenge is to address the issue of concept drift in the data stream environment, which is not easily handled by the traditional learning techniques. This paper presents a Genetic Programming (GP) based boosting ensemble method for the classification of distributed streaming data able to adapt in presence of concept drift. The approach handles flows of data coming from multiple locations by building a global model obtained by the aggregation of the local models coming from each node. The algorithm uses a fractal dimension-based change detection strategy, based on self-similarity of the ensemble behavior, that permits the capture of time-evolving trends and patterns in the stream, and to reveal changes in evolving data streams. Experimental results on a real life data set show the validity of the approach in maintaining an accurate and up-to-date GP ensemble.\"",
        "Document: \"Multiobjective evolutionary community detection for dynamic networks. A multiobjective genetic algorithm for detecting communities in dynamic networks, i.e., networks that evolve over time, is proposed. The approach leverages on the concept of evolutionary clustering, assuming that abrupt changes of community structure in short time periods are not desirable. The algorithm correctly detects communities and it is shown to be very competitive w.r.t. some state-of-the-art methods.\"",
        "Document: \"Outlier mining in large high-dimensional data sets. In this paper, a new definition of distance-based outlier and an algorithm, called HilOut, designed to efficiently detect the top n outliers of a large and high-dimensional data set are proposed. Given an integer k, the weight of a point is defined as the sum of the distances separating it from its k nearest-neighbors. Outlier are those points scoring the largest values of weight. The algorithm HilOut makes use of the notion of space-filling curve to linearize the data set, and it consists of two phases. The first phase provides an approximate solution, within a rough factor, after the execution of at most d + 1 sorts and scans of the data set, with temporal cost quadratic in d and linear in N and in k, where d is the number of dimensions of the data set and N is the number of points in the data set. During this phase, the algorithm isolates points candidate to be outliers and reduces this set at each iteration. If the size of this set becomes n, then the algorithm stops reporting the exact solution. The second phase calculates the exact solution with a final scan examining further the candidate outliers that remained after the first phase. Experimental results show that the algorithm always stops, reporting the exact solution, during the first phase after much less than d + 1 steps. We present both an in-memory and disk-based implementation of the HilOut algorithm and a thorough scaling analysis for real and synthetic data sets showing that the algorithm scales well in both cases.\"",
        "Document: \"GP ensemble for distributed intrusion detection systems. In this paper an intrusion detection algorithm based on GP ensembles is proposed. The algorithm runs on a distributed hybrid multi-island model-based environment to monitor security-related activity within a network. Each island contains a cellular genetic program whose aim is to generate a decision-tree predictor, trained on the local data stored in the node. Every genetic program operates cooperatively, yet independently by the others, by taking advantage of the cellular model to exchange the outmost individuals of the population. After the classifiers are computed, they are collected to form the GP ensemble. Experiments on the KDD Cup 1999 Data show the validity of the approach.\"",
        "Document: \"Experimental evaluation of topological-based fitness functions to detect complexes in PPI networks. The detection of groups of proteins sharing common biological features is an important research issue, intensively investigated in the last few years, because of the insights it can give in understanding cell behavior. In this paper we present an extensive experimental evaluation campaign aiming at exploring the capability of Genetic Algorithms (GAs) to find clusters in protein-protein interaction networks, when different topological-based fitness functions are employed. A complete experimentation on the yeast protein-protein interaction network, along with a comparative evaluation of the effectiveness in detecting true complexes on the yeast and human networks, reveals GAs as a feasible and competitive computational technique to cope with this problem.\"",
        "Document: \"A comorbidity network approach to predict disease risk. A prediction model that exploits the past medical patient history to determine the risk of individuals to develop future diseases is proposed. The model is generated by using the set of frequent diseases that contemporarily appear in the same patient. The illnesses a patient could likely be affected in the future are obtained by considering the items induced by high confidence rules generated by the frequent diseases. Furthermore, a phenotypic comorbidity network is built and its structural properties are studied in order to better understand the connections between illnesses. Experimental results show that the proposed approach is a promising way for assessing disease risk.\"",
        "1 is \"Hypothetical datalog: complexity and expressibility\", 2 is \"CFinder: locating cliques and overlapping modules in biological networks\"",
        "Given above information, for an author who has written the paper with the title \"Algorithms for selective enumeration of prime implicants\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007808": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'The Effect Of Random Restrictions On Formula Size':",
        "Document: \"The Complexity of Satisfiability of Small Depth Circuits. Say that an algorithm solving a Boolean satisfiability problem x on n variables is improved if it takes time poly(|x|)2 cn for some constant c i.e., if it is exponentially better than a brute force search. We show an improved randomized algorithm for the satisfiability problem for circuits of constant depth d and a linear number of gates cn: for each d and c, the running time is 2(1 \u9a74 \u9a74)n where the improvement $\\delta\\geq 1/O(c^{2^{d-2}-1}\\lg^{3\\cdot 2^{d-2}-2}c)$, and the constant in the big-Oh depends only on d. The algorithm can be adjusted for use with Grover's algorithm to achieve a run time of $2^{\\frac{1-\\delta}{2}n}$ on a quantum computer.\"",
        "Document: \"On the Complexity of Succinct Zero-Sum Games. We study the complexity of solving succinct zero-sum games, i.e., the games whose payoff matrix M is given implicitly by a Boolean circuit C such that M (i, j) = C(i, j). We complement the known EXP-hardness of computing the exact value of a succinct zero-sum game by several results on approximating the value. (1) We prove that approximating the value of a succinct zero-sum game to within an additive factor is complete for the class promise-S_2^P, the \"promise\" version of S_2^P. To the best of our knowledge, it is the first natural problem shown complete for this class. (2) We describe a ZPP^NP algorithm for constructing approximately optimal strategies, and hence for approximating the value, of a given succinct zero-sum game. As a corollary, we obtain, in a uniform fashion, several complexity-theoretic results, e.g., a ZPP^NP algorithm for learning circuits for SAT [7] and a recent result by Cai [9] that S_2^P \u9a74 ZPP^NP. (3) We observe that approximating the value of a succinct zero-sum game to within a multiplicative factor is in PSPACE, and that it cannot be in promise-S_2^P unless the polynomial-time hierarchy collapses. Thus, under a reasonable complexity-theoretic assumption, multiplicative-factor approximation of succinct zero-sum games is strictly harder than additive-factor approximation.\"",
        "Document: \"Which problems have strongly exponential complexity?. Abstract For several NP-complete problems, there have been a progression of better but still exp onential algorithms. In this paper, we address the relative likelihood of sub-exponential algorithmsfor these problems. We introduce a generalized reduction which we call Sub-Exponential Reduction Family (SERF) that preserves sub-exponential complexity. We show that Circuit- SAT is SERF-complete for allNP-search problems, and that for any fixed , -SAT, -Colorability, -Set Cover, Independent Set, Clique, Vertex Cover, are SERF\u2010complete for the class SNP of search problems expressible by second order existential formulas whose first order part is universal. In particular, sub- exponential complexity for any one of the above problems implies the same for all others. We also look at the issue of proving strongly exponential lower bo unds for AC ; that is, bounds of the form . This problem is even open for depth-3 circuits. In fact, such a bound for dept h-3 circuits with even limited (at most ) fan-in for bottom-level gates would imply a nonlinear size lower bound,fo r logarithmic depth circuits. We show that with high probability even degree 2 random GF(2) polynomials require stron gly exponential size for circuits for,. We thus exhibit a much smaller space of,functions such that almost every function in this class requires str ongly exponential size circuits. As a corollary, we derive a pseudorandom generator (requiri ng bits of advice) that maps  bits into a larger number,of bits so that computing,parity on the r ange is hard for,circuits. Our main technical lemma is an algorithm that, for any fixed , represents an arbitrary -CNF formula as a disjunc- tion of -CNF formulas that aresparse, that is, each has clauses.\"",
        "Document: \"A tool for obtaining tighter security analyses of pseudorandom function based constructions, with applications to PRP to PRF conversion.  We present a general probabilistic lemma that can be applied to upper bound the advantage ofan adversary in distinguishing between two families of functions. Our lemma reduces the task ofupper bounding the advantage to that of upper bounding the ratio of two probabilities associated tothe adversary, when this ratio is is viewed as a random variable. It enables us to obtain significantlytighter analyses than more conventional methods.In this paper we apply the technique to the problem of... \"",
        "Document: \"Can every randomized algorithm be derandomized?. Among the most important modern algorithmic techniques is the use of random decisions. Starting in the 1970's, many of the most significant results were randomized algorithms solving basic compuatational problems that had (to that time) resisted efficient deterministic computation. (Ber72, SS79, Rab80, Sch80, Zip79, AKLLR). In contrast, many of the most exciting recent work has been on derandomizing these same algorithms, coming up with efficient deterministic versions, e.g., (AKS02, Rein05). This raises the question, can such results be obtained for all randomized algorithms? Will the remaining classical randomized algorithms be derandomized by similar techniques?Clear but complicated answers to these questions have emerged from complexity-theoretic studies of randomized complexity classes (e.g., RP and BPP) and pseudo-random generators. These questions are inextricably linked to another basic problem in complexity: which functions require large circuits to compute?In this talk, we'll survey some results from the theory of derandomization. I'll stress connections to other questions, especially circuit complexity, explicit extractors, hardness amplification, and error-correcting codes. Much of the talk is based on joint work with Valentine Kabanets and Avi Wigderson, but it will also include results by many other researchers.A priori, possibilities concerning the power of randomized algorithms include:Randomization always helps speed up intractable problems, i.e., EXP=BPP.The extent to which randomization helps is problem-specific. Depending on the problem, it can reduce complexity by any amount from not at all to exponentially.True randomness is never needed, and random choices can always be simulated deterministically, i.e., P=BPP..Either of the last two possibilities seem plausible, but most consider the first wildly implausible. However, while a strong version of the middle possibility has been ruled out, the implausible first one is still open. Recent results indicate both that the last, P=BPP, is both very likely to be the case and very difficult to prove.More precisely: Either no problem in E has strictly exponential circuit complexity or P=BPP. This seems to be strong evidence that, in fact, P=BPP, since otherwise circuits can always shortcut computation time for hard problems. (NW, BFNW, IW97, STV01, SU01, Uma02).Either BPP=EXP, or any problem in BPP has a deterministic sub-exponential time algorithm that works on almost all instances. In other words, either randomness solves every hard problem, or it does not help exponentially, except on rare instances. This rules out strong problem-dependence, since if randomization helps exponentially for many instances of some problem, we can conclude that it helps exponentially for all intractible problems. (IW98). If RP=P , then either the permanent problem requires super-polynomial algebraic circuits or there is a problem in NEXP that has no polynomial-size Boolean circuit. (IKW01, KI). That is, proving the last possibility requires one to prove a new circuit lower bound, and so is likely to be difficult. (Moreover, we do not need the full hypothesis that P=RP to obtain the same conclusion: it actually suffices that the Schwartz-Zippel identity testing algorithm be derandomizable. Thus, we will not be able to derandomize even the \"classic\" algorithms without proving circuit lower bounds.)All of these results use the hardness-vs-randomness paradigm introduced by Yao (Yao82, see also BM, Levin): Use a hard computational problem to define a small set of \"pseudo-random\" strings, that no limited adversary can distinguish from random. Use these \"pseudo-random\" strings to replace the random choices in a probabilistic algorithm. The algorithm will not have enough time to distinguish the pseudo-random sequences from truly random ones, and so will behave the same as it would given random sequences.\"",
        "Document: \"A zero one law for RP. We show that if RP has p-measure nonzero then ZPP=EXP. As corollaries, we obtain a zero-one law for RP, and that both probabilistic classes ZPP and RP have the same p-measure. Finally we prove that if NP has p-measure nonzero then NP=AM.\"",
        "Document: \"Hill-climbing finds random planted bisections. We analyze the behavior of hill-climbing algorithms for the minimum bisection problem on instances drawn from the \u201cplanted bisection\u201d random graph model, Gn,p,q, previously studied in [3, 4, 10, 11, 14, 9, 7]. This is one of the few problem distributions for which various popular heuristic methods, such as simulated annealing, have been proven to succeed. However, it has been open whether these sophisticated methods were necessary, or whether simpler heuristics would also work. Juels [14] made the first progress towards an answer by showing that simple hill-climbing does suffice for very wide separations between p and q.Here we give a more complete answer. A simple, polynomial-time, hill-climbing algorithm for this problem is given and shown to succeed in finding the planted bisection with high probability if p - q = &OHgr; (n-\u00bdln3n). For dense graphs, this matches the condition for optimality of the planted bisection to within a polylogarithmic factor. Furthermore, we show that a generic randomized hill-climbing algorithm succeeds in finding the planted bisection in polynomial time if p - q = &OHgr; (n-\u00bc ln3 n), for any \u2208 0. This algorithm, studied also by [14], is a degenerate case of both Metropolis and go-with-the-winners, and the range here properly includes those analyzed in [11, 9, 14]. So this result implies, extends, and unifies those from [11, 9, 14]. Thus, to get a provable distinction between simulated annealing and hill-climbing for natural problems will require considerable progress both on new positive results for SA and new negative results for hill-climbing methods.\"",
        "Document: \"Complexity of k-SAT. The problem of k-SAT is to determine if the given k-CNF has a satisfying solution. It is a celebrated open question as to whether it requires exponential time to solve k-SAT for k/spl ges/3. Define s/sub k/ (for k/spl ges/3) to be the infimum of {/spl delta/: there exists an O(2/sup /spl delta/n/) algorithm for solving k-SAT}. Define ETH (Exponential-Time Hypothesis) for k-SAT as follows: for k/spl ges/3, s/sub k/>0. In other words, for k/spl ges/3, k-SA does not have a subexponential-time algorithm. In this paper we show that s/sub k/ is an increasing sequence assuming ETH for k-SAT: Let s/sub /spl infin// be the limit of s/sub k/. We in fact show that s/sub k//spl les/(1-d/k) s/sub /spl infin// for some constant d>0.\"",
        "Document: \"Linear gaps between degrees for the polynomial calculus modulo distinct primes. Two important algebraic proof systems are the Nullstellensatz system and the polynomial calculus (also called the Grobner system). The Nullstellensatz system is a propositional proof system based on Hilbert's Nullstellensatz, and the polynomial calculus (PC) is a proof system which allows derivations of polynomials, over some field. The complexity of a proof in these systems is measured in terms of the degree of the polynomials used in the proof. The mod p counting principle can be formulated as a set MODpn of constant-degree polynomials expressing the negation of the counting principle. The Tseitin mod p principles, TSn(p), are translations of the MODpn into the Fourier basis. The present paper gives linear lower bounds on the degree of polynomial calculus refutations of MODpn over p fields of characteristic q \u2260 p and over rings Zq with q,p relatively prime. These are the first linear lower bounds for the polynomial calculus. As it is well-known to be easy to give constant degree polynomial calculus (and even Nullstellensatz) refutations of the MOD pn polynomials over Fp, our results imply that the MODpn polynomials have a linear gap between proof complexity for the polynomial calculus over Fp and over Fq. We also obtain a linear gap for the polynomial calculus over rings Zp and Zq where p, q do not have identical prime factors\"",
        "Document: \"A personal view of average-case complexity. The structural theory of average-case complexity, introduced by Levin (1986), gives a formal setting for discussing the types of inputs for which a problem is difficult. This is vital to understanding both when a seemingly difficult (e.g. NP-complete) problem is actually easy on almost all instances, and to determining which problems might be suitable for applications requiring hard problems, such as cryptography. The paper attempts to summarize the state of knowledge in this area, including some \u201cfolklore\u201d results that have not explicitly appeared in print. We also try to standardize and unify definitions. Finally, we indicate what we feel are interesting research directions. We hope that the paper motivates more research in this area and provide an introduction to the area for people new to it\"",
        "1 is \"Algorithms and Complexity Results for #SAT and Bayesian Inference\", 2 is \"Interactive proof systems and alternating time-space complexity\"",
        "Given above information, for an author who has written the paper with the title \"The Effect Of Random Restrictions On Formula Size\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007818": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A procedure to detect problems of processes in software development projects using Bayesian networks.':",
        "Document: \"Modelagem de Bancos de Dados em Tempo-real. This work introduces a method to model Real-time Databases (RTDB) based on an object based Petri net notation named EG-CPN. This notation is enriched in order to promote the efficient des- cription of a model integrating the RTDB and the Real-time system. The method makes available to the designer contructs allowing, for example, the declaration of logic and timing restrictions as well as concurrent execution of methods in a RTDB application. Moreover, we present an example where the method is applied in the modeling and validation of database and control objects of a flexible manufacturing system cell.\"",
        "Document: \"A Topic Aware-based Approach to Maximize Social Influence. The use of social networks has shown great potential for information diffusion and formation of public opinion. One key problem that has attracted researchers interest is Topic-based Influence Maximization, that refers to finding a small set of users on a social network that have the ability to influence a substantial portion of users on a given topic. The proposed solutions, however, are not suitable for large-scale social networks and must incorporate mechanisms for determining social influence among users on each topic of interest. Consequently, for these approaches, it becomes difficult or even unfeasible to deal quickly and efficiently with constant changes in the structure of social networks. This problem is particularly relevant as the topics of interest of users and the social influence they exert on each other for every topic are considered together. In this work, it is proposed a scalable solution, that makes use of data mining over an information propagation log, in order to directly select the initial set of influential users on a particular topic without the need to incorporate a previous step for learning users social influence with regard to that topic. As an additional benefit, the targeted seed set also offers an approximation guarantee of the optimal solution. Finally, it is presented a design of experiments over a data set containing information propagation data from a real social network. As main results, we have found some evidences that the proposed solution maintains a trade-off between scalability and accuracy.\"",
        "Document: \"A Flexible Middleware for Service Provision Over Heterogeneous Pervasive Networks. Pervasive computing has gained much attention from the research community due to the possibility of deploying the first pervasive environments. Therefore, many software solutions are emerging, with the intent of facilitating the development of pervasive applications. Within this scope, in this paper, we introduce a service oriented middleware for pervasive computing, enhanced with runtime flexibility, extensibility for applications, and heterogeneous service provision. Our goal is to enable the middleware and its applications to be adapted to changing operational scenarios. Furthermore, different protocols can be used to discover and access services.\"",
        "Document: \"Using equivalence classes for testing programs for safety instrumented systems. The reliability on Safety Instrumented Systems (SIS) is critical for the safe operation of many industrial applications. In particular, SIS play an important role in oil and gas processing plants. SIS are responsible not only for the continuous operation of the plant, it also keeps the plant in a safe state, avoiding damages to the environment and minimizing risks to employees. Therefore, the correct behavior of such systems is an important goal to achieve when building industrial plants. Verification and testing of SIS programs is a very hard task to accomplish. This happens mainly for two reasons. First, testing the real system is very expensive and sometimes it may take a huge amount of time, weeks or even months. Second, those systems deal with a huge number of variables. It is not always possible for a human tester to check if all of them are correct when performing tests. Providing an automatic and formal testing approach for such systems is an important contribution for the development of such systems. In this work we introduce a new method for generating test cases for SIS programs running on Programable Logic Controller (PLC). As we did on previous work, ISA 5.2 diagrams are used as specification of the systems, but now we are using a hardware-in-the-loop technique, and the target artifact is a software running on a PLC. To avoid the execution of redundant tests, we introduced a new test case generation algorithm that is based on equivalence classes. Finally, we discussed a study case in which our method is used to detect error, that were introduced on purpose, on a simple system.\"",
        "Document: \"An infrastructure for developing context aware applications in pervasive environments. This paper presents the Lotus, an infrastructure to develop context-aware applications, providing the mechanisms for acquiring, representing, reasoning and delivering of contextual information. This infrastructure consists of a framework, which is composed by an API and a middleware, a language to represent contextual information and a mechanism to share and delivery contextual information.\"",
        "Document: \"Multi(Uni)Cast Dccp For Live Content Distribution With P2p Support. Real time multimedia content transmission on the Internet is essential for the most current applications such as voice over IP, video conference, games and web TV. The most popular Internet transport protocols - TCP and UDP - do not suffice when one needs to transmit data from these applications. As a consequence, IETF has been working in new transport protocols that enhance the quality of these multimedia applications. Among all these protocols, DCCP (RFC 4340) is the most effective for multimedia content transmission on the Internet. However, DCCP is not effective in scenarios with many receivers nodes and one sender node. Therefore, this work proposes the Mult(Uni)cast DCCP, a DCCP variant that enables the multimedia data transmission from one to various nodes and supporting non-reliable traffic congestion control. The MU-DCCP uses either multicast or unicast flows according to the network support and data sharing among receiver nodes. The obtained results show that the usage of the MU-DCCP significantly reduces the data congestion in the network while improving the application scalability in terms of the number of receiver nodes.\"",
        "Document: \"Broadcast routing in wireless sensor networks with dynamic power management and multi-coverage backbones. In a wireless sensor network (WSN), nodes are power constrained. As a consequence, protocols must be energy efficient to prolong the network lifetime while keeping some quality-of-service (QoS) requirements. In WSNs, most protocols resort to the broadcast of control messages like, for example, for the topology control (TC) of the network. On its turn, TC itself can be applied to improve the broadcast of data packets in the network, and because only a subset of nodes need to be active at any time, it is possible to extend the network lifetime. We investigate some alternatives to improve broadcasting in WSN for an extended network lifetime. This is accomplished in two ways. First, we adapt the dynamic power management with scheduled switching modes (DPM-SSM) technique to a blind flooding protocol (i.e., FLOOD). To capture the battery capacity recovery effect as a result of applying DPM, we consider a more realistic battery model (i.e., Rakhmatov-Vrudhula battery model). Second, we implement a multi-coverage TC solution for computing an energy efficient broadcast backbone. Extensive simulation results using the NS2 network simulator show that it is possible to extend the network lifetime while keeping good broadcasting performance.\"",
        "Document: \"Pervasive Advertising: An Approach For Consumers And Advertisers. Pervasive Advertising stands out from other forms of ad serving by using contextual information of consumers. However, providing relevant ads is not enough for effective Pervasive Advertising. It is also necessary to address the objectives of the advertisers, leading to a trade off between advertisers' objectives and consumers' needs. This paper presents a solution based on multiagent negotiation to solve this impasse. Experiments have demonstrated that the solution is effective in finding a balance between the needs of the consumers and the objectives of the advertisers.\"",
        "Document: \"A Location And Bandwidth Aware P2p Video On Demand System For Mobile Devices. Due the increasing number of new wireless technologies and the advent of new mobile devices, new services and applications are necessary to supply the needs of these devices users. Video on-Demand (VoD) services have become one of these services. In this paper we present a proposal of a P2P Mobile Video on-Demand (VoD) System focused oil mobile devices where they can act as both, media servers as well as clients. Due to different possible communication interfaces that are available for mobile devices, we considered that they can be in networks with different throughput characteristics, and also, they can be distributed through the Internet. Therefore, our proposal make possible to build content distribution trees in the application layer considering two major aspects related to mobility: the available bandwidth that the mobile device can share with the network, and; the relative position of the mobile device in the network.\"",
        "Document: \"The Design of Real-Time Distributed information Systems with Object-Oriented and Fault-Tolerant Characteristics.  For real-time distributed information systems (RTDIS), a high degree of reliability, availability andsafety is usually required. In this paper, we first discussthe major features that a design methodologyfor RTDIS should possess. A survey of current designapproaches is then given. The G-Net methodologyfor the design of RTDIS with object orientedand fault tolerance characteristics is presented. Itis shown that fault tolerant G-Nets supporting rollbackrecovery and N-version programming... \"",
        "1 is \"Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning.\", 2 is \"Breast cancer detection using cartesian genetic programming evolved artificial neural networks\"",
        "Given above information, for an author who has written the paper with the title \"A procedure to detect problems of processes in software development projects using Bayesian networks.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007829": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'IDSS: a novel representation for woven fabrics.':",
        "Document: \"A 27.1 mW, 7.5-to-11.1 Gb/s single-loop referenceless CDR with direct Up/dn control. A 7.5-to-11.1 Gb/s half-rate referenceless clock and data recovery (CDR) with a compact frequency acquisition scheme is proposed. Using the bang-bang phase-frequency detector with a direct up/dn control, the referenceless CDR is realized by a single-loop architecture which performs both phase and frequency acquisition in the same loop. The proposed frequency acquisition scheme achieves a wide capture range of 3.6 Gb/s and reduces cycle-slips. The proposed CDR is fabricated in 65-nm CMOS technology and occupies an active area of 0.04 mm\n<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sup>\n. At the data rate of 10 Gb/s, the proposed CDR consumes 27.1 mW from 1.3-V supply.\"",
        "Document: \"An ethernet switch architecture for bandwidth provision of broadband access networks. Ethernet is one of the potential candidates capable of meeting the bandwidth demands of subscribers and successfully supporting various services in broadband access networks. However, the lack of a traffic management capability is a major obstacle to successful deployment. In this article we suggest an Ethernet switch architecture that offers performance-guaranteed broadband access services. We also discuss the micro-architecture of the switch and associated design issues. Switches in the proposed architecture identify subscriber traffic without handling higher layer protocols, regulate it with per subscriber traffic policing, and ensure deterministic network performance. The implemented switch ASIC offers 28 gigabit Ethernet ports, integrating 9 million logic gates and 51 memories holding a total of 3.4 Mb to store data and control states on a single 10 mm x 10 mm die using 0.13 mum CMOS technology. The performance evaluation results, verified using a network performance analyzer, show that the proposed architecture guarantees transfer rates of individual subscribers based on their service agreements, effectively protecting network resources from excess traffic, which adversely affects the QoS of existing users' traffic.\"",
        "Document: \"A 1.2-V-only 900-mW 10 gb ethernet transceiver and XAUI interface with robust VCO tuning technique. This paper describes the design and the implementation of a fully integrated 10 Gb Ethernet transceiver in a 0.13-\u03bcm CMOS process using only a 1.2 V supply. A coarse control algorithm that combines a voltage range monitoring circuit with a frequency lock detector provides a robust operation against process, voltage, and temperature (PVT) variations for a VCO with a ring oscillator. With the use of a blind oversampling DPLL architecture, four channels of XAUI transceivers can share a single PLL, eliminating the clock synchronization problem between channels. Also, the total number of clock domains for the entire chip is reduced to three, making the integration of the XAUI with the 10G transceiver much simpler. The test chip consumes 898 mW from a 1.2 V supply.\"",
        "Document: \"A partially switched-opamp technique for high-speed low-power pipelined analog-to-digital converters. This paper presents a partially switched-opamp technique for a high-speed, low-power pipelined analog-to-digital converter (ADC). Unlike a conventional switched-opamp technique, only the second stage of a two-stage opamp is switched with the enhanced power efficiency and the drawbacks of an opamp sharing technique and a conventional switched-opamp technique are addressed. The prototype of 8-bit 200-MS/s pipelined ADC is implemented in a 0.18-\u03bcm CMOS process technology. This converter achieves 55.8-dB spurious free dynamic range, 47.3-dB signal-to-noise-plus-distortion ratio, 7.68 effective number of bits for a 90-MHz input at full sampling rate, and consumes 30-mW from a 1.8-V supply. The active area of the ADC is 0.15 mm2.\"",
        "Document: \"A 0.25-Mu M Cmos 1.9-Ghz Phs Rf Transceiver With A 150-Khz Low-If Architecture. We present a 1.9-GHz Personal Handy-phone System (PHS) transceiver, fully integrated and fabricated in 0.25-mu m CMOS technology. The receiver is based on a 150-kHz low-IF architecture and meets the fast channel switching and DC-offset cancellation requirements of PHS. It includes a low-noise amplifier (LNA), a downconversion mixer, a complex filter, and a programmable gain amplifier. A fractional-N frequency synthesizer achieves seamless handover with a 25 tts channel switching time and a phase noise of -121 dBc/Hz at a 600-kHz offset frequency, with compliant ACS performance. The receiver provides -105 dBm sensitivity and 55 dBc ACS at a 600-kHz frequency offset. The transmitter is based on the direct modulation architecture and consists of an upconversion mixer and a pre-driver stage. The gain of the pre-driver is digitally controllable to suit any type of commercial power amplifier. The transmitter shows a 3% EVM and a 65 dBc ACPR at a 600-kHz offset frequency. The whole transceiver occupies 15.2 mm(2) and dissipates 70 mA in RX and 44 mA in TX, with a 2.8-V supply.\"",
        "Document: \"Design Optimization of On-Chip Inductive Peaking Structures for 0.13- $\\mu{\\hbox {m}}$ CMOS 40-Gb/s Transmitter Circuits. This paper describes design methodologies for the optimal inductive peaking structures used for the 40-Gb/s serializing transmitter circuits presented in. The implemented transmitter had more than 400 on-chip inductors and transformers in order to achieve the bandwidth required for the 38.4-Gb/s operation demonstrated in a 0.13-\u03bcm CMOS process. A bridged T-coil network with inverted mutual couplin...\"",
        "Document: \"Comprehensive analysis and control of design parameters for power gated circuits. Power gating in circuits is one of the effective technologies to allow low leakage and high performance operations. The key design considerations in the power mode transitions of power gating technology are minimizing the wakeup delay (for achieving high performance), the peak current (for reducing power supply/ground noise), and the total size of sleep transistors (for reducing area/design complexity). This work aims to analyze and establish the relations between the three important design parameters: 1) the maximum current flowing from/to power/ground; 2) the wakeup (sleep to active mode transition) delay; and 3) the total size of sleep transistors. With the understanding of relations between the parameters, we propose solution to the problem of finding logic clusters and their wakeup schedule that minimize the wakeup delay while satisfying the peak current constraint in wakeup time and performance loss constraint in normal operation. Specifically, we solve the problem by formulating it into repeated (incremental) applications of finding a maximum clique in a graph. From the experiments using ISCAS benchmarks, it is shown that our proposed technique is able to explore the search space, finding solutions with 71% and 30% reduced sizes of sleep transistors and 39% and 54% reduced wakeup delay, compared to the results by the previous work.\"",
        "Document: \"An all-digital bang-bang PLL using two-point modulation and background gain calibration for spread spectrum clock generation. An all-digital spread spectrum clock generator (SSCG) using two-point modulation is presented. To calibrate the gain mismatch between two modulation paths, a background gain calibration method is proposed. To reduce power consumption and design complexity, the bang-bang phase-frequency detector (BBPFD) is used instead of the time-to-digital converter (TDC). The prototype chip has been fabricated in a 65-nm CMOS process and it consumes 6 mW at 2.5 GHz. The measured minimum rms jitter is 1.58 ps.\"",
        "Document: \"A design of an area-efficient 10-GHz phase-locked loop for source-synchronous, multi-channel links in 90-nm CMOS technology. This paper presents a design of an area-efficient 10-GHz PLL for source-synchronous, multi-channel applications. To be applied in the multi-channel application, the proposed PLL is implemented without use of any high-cost inductor to minimize silicon area while achieving 10-GHz operation frequency. A modified CML type ring-VCO is used to make the VCO outputs have consistent signal amplitude. The proposed PLL is fabricated in 90-nm low-power CMOS technology. The prototype IC occupies 0.075mm2 of active area and dissipates 87.6-mW power from 1.2-V supply including a 10-GHz clock driver.\"",
        "Document: \"Design of CMOS 5 Gb/s 4-PAM transceiver frontend for low-power memory interface. A 5Gb/s four-level pulse amplitude modulation (4-PAM) transceiver front-end for low-power memory interface is proposed. Since the most power-consuming blocks in high-speed link front-end are drivers, and equalizers, in this work, we have used 4-PAM voltage mode driver to reduce the power consumption of driver and equalizer. Moreover, an analysis to minimize voltage mode driver power consumption is presented. In order to eliminate the reflection in a multi-drop bus, an impedance-matched bi-directional multi-drop bus has implemented. Simulation results show the proposed transceiver front-end has power efficiency of 1.7 mW/Gbps. Circuit design and simulation were done in 0.13-\u03bcm CMOS technology.\"",
        "1 is \"Fully integrated CMOS fractional-N frequency divider for wide-band mobile applications with spurs reduction.\", 2 is \"Efficient Detection of Network Motifs\"",
        "Given above information, for an author who has written the paper with the title \"IDSS: a novel representation for woven fabrics.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007831": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'How Clones are Maintained: An Empirical Study':",
        "Document: \"Learning from bug-introducing changes to prevent fault prone code. A version control system, such as CVS/SVN, can provide the history of software changes performed during the evolution of a software project. Among all the changes performed there are some which cause the introduction of bugs, often resolved later with other changes. In this paper we use a technique to identify bug-introducing changes to train a model that can be used to predict if a new change may introduces or not a bug. We represent software changes as elements of a n-dimensional vector space of terms coordinates extracted from source code snapshots. The evaluation of various learning algorithms on a set of open source projects looks very promising, in particular for KNN (K-Nearest Neighbor algorithm) where a significant tradeoff between precision and recall has been obtained.\"",
        "Document: \"Understanding SQL through iconic interfaces. Visual query languages represent an evolution, in terms of understandability and adaptability, with respect to traditional textual languages. We present an iconic query system that enables the interaction of a novice user with a relational database. Our goal is to help a novice user to learn and comprehend the relational data model and a textual query language such as SQL, through the use of the iconic metaphore. In this sense our approach is different from most of the visual query systems proposed in the literature that present the user with a higher level query language, hiding the underlying data model. We also present results from an experiment conducted with first year students to evaluate the effectiveness of our approach.\"",
        "Document: \"Introducing Workflow Management in Software Maintenance Processes. Software organizations are moving from traditional software factory models towards virtual organization models, where distributed teams converge in a temporary network with the aim of integrating different competences or solving problems in a cooperative way. Most workflow management systems of last generation are web based and this makes them a viable enabling technology for remodeling both the organization structure and its processes in order to move towards a virtual organization model and increase its competitiveness. We present a case study of introducing workflow technologies in a large software enterprise. In particular, a workflow-based prototype implementation for the management of the ordinary maintenance process is discussed.\"",
        "Document: \"Migrating Legacy Systems to the Web: an Experience Report. A key to successfully moving to the Internet while salvaging the past investments in centralised, mainframe-oriented software development is migrating core legacy applications towards Web-enabled, client-server architectures. This paper presents the main results and lessons learned from a migration project aimed at integrating an existing COBOL system into a Web-enabled infrastructure. The original system has been decomposed into its user-interface and server (application logic and database) components. The user interface has been migrated into a Web browser shell using Microsoft Active Server Pages (ASP) and VBScript. The server component has been wrapped with dynamic load libraries written in Microfocus Object COBOL, loaded into Microsoft Internet Information Server, and accessed by the ASP pages.\"",
        "Document: \"The Evolution and Decay of Statically Detected Source Code Vulnerabilities. The presence of vulnerable statements in the source code is a crucial problem for maintainers: properly monitoring and, if necessary, removing them is highly desirable to ensure high security and reliability To this aim, a number of static analysis tools have been developed to detect the presence of instructions that can be subject to vulnerability attacks, ranging from buffer overflow exploitations to command injection and cross-site scripting. Based on the availability of existing tools and of data extracted from software repositories, this paper reports an empirical study on the evolution of vulnerable statements detected in three software systems with different static analysis tools. Specifically, the study investigates on vulnerability evolution trends and on the decay time exhibited by different kinds of vulnerabilities.\"",
        "Document: \"A Literature Review of Business/IT Alignment Strategies. In the last years, the alignment issue was addressed in several researches and numerous methods, techniques and tools were proposed. Indeed, the business and IT performance are tightly coupled, and enterprises cannot be competitive if their business and IT strategies are not aligned. This paper proposes a literature review useful for evaluating different alignment approaches, with the aim of discovering similarity, maturity, capability to measure, model, asses and evolve the alignment level existing among business and technological assets of an enterprise. The proposed framework is applied to analyse the alignment research published in the Information & Management journal and the Journal of Strategic Information Systems, that are the ones that more published on this topic. The achieved evaluation results are presented.\"",
        "Document: \"WebEv -- A Collaborative Environment for Supporting Measurement Frameworks. Assessment activities play a key role when an enterprise wants to evolve its business processes for adopting innovative information and communication technologies. The assessment regards business processes and supporting software systems and, to be effective, it requires the support of adequate software environments that facilitate the collaboration among the involved assessors. The definition and implementation of such a kind of environment implies the previous definition of a measurement framework that may guide the specification and design of the environment.This paper presents the collaborative software environment WebEv to support any kind of assessment framework whose definition is based on the Goal Question Metric paradigm. The environment is flexible and easily customizable to any context and arising needs. It can represent a practical mean for the collection of the available quantitative information regarding the processes and/or software systems of an enterprise.\"",
        "Document: \"FlowManager: a workflow management system based on Petri nets. The use of workflow technology to provide automated support to the management and execution of software engineering processes has become of great interest for large software companies that nowadays are moving towards the new model of \"virtual\" organizations. Therefore, workflow systems that allow co-operation among team members in a distributed environment are now a primary concern. In this paper we present a new workflow management system, named FlowManager, which has all the potential to be used in such application domain. FlowManager, in fact, will be a component of a larger European GENESIS project (Generalized Environment for Process Management in Cooperative Software Engineering), whose objective is to develop a non-invasive and open source environment for modeling software engineering processes and managing co-operation among geographically distributed teams.\"",
        "Document: \"Relating the Evolution of Design Patterns and Crosscutting Concerns. Program slicing is a program-reduction technique for extracting statements that may influence other statements. While there exist efficient algorithms to slice sequential programs precisely, there are only two algorithms for precise slicing of concurrent ...\"",
        "Document: \"Introducing Quality System in Small and Medium Enterprises: An Experience Report. The institutionalization of a Quality System improves the levels of technical and managerial efficiency of Enterprises. Moreover, the market itself solicits the acquisition of a Quality Certification for getting a steady confirmation of Enterprise's capabilities. The introduction of a Quality System in Small and Medium Enterprises can entail prohibitive costs for them and affect their agility and flexibility. The paper proposes a lightweight approach as a solution to either avoid or reduce such drawbacks; it consists of a method for redesigning processes and a software system to control and monitoring processes' execution. Consequently, a research question arises: is the approach suitable for establishing a Quality System effectively in a Small Medium Enterprise? In order to have a preliminary evaluation of the proposed approach, a case study has been carried out in an Italian Enterprise, aiming at owning VISION 2000 Certification.\"",
        "1 is \"Expanding identifiers to normalize source code vocabulary\", 2 is \"Group support for the recording and sharing of maintenance rationale\"",
        "Given above information, for an author who has written the paper with the title \"How Clones are Maintained: An Empirical Study\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007871": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'On the Protection of fMRI Images in Multi-domain Environments':",
        "Document: \"Making sense of cloud-sensor data streams via Fuzzy Cognitive Maps and Temporal Fuzzy Concept Analysis. \u2022Definition of an hybrid approach for situation awareness that tries to balance the application of unsupervized data analysis techniques and the use of human expert knowledge to make sense of such analysis results.\u2022Definition of methodology that integrates Temporal Fuzzy Concept Analysis and Fuzzy Cognitive Maps, also supported by semantic technologies, to realize the aforementioned approach.\u2022As a real-world scenario we consider the recognition of human activities and the projection of them in the near future to address, for instance, energy saving, safety issues, and so on.\"",
        "Document: \"An ontological multi-criteria optimization system for Workforce Management. Workforce Management (WFM) is becoming a core decisional approach for optimizing different enterprise processes such as operational activities needed to maintain a high production rate. However, in order to solve complex optimization problems it is necessary to analyze and deal with a plethora of distributed and semantically different information defining the collection of criteria from which enterprise activities depend. For this reason, this paper introduces a novel WFM system that, by using an ontological representation of knowledge related to the different aspects of an enterprise activity, exploits a multi-criteria decision making approach for selecting the most suitable strategies to face WFM issues.\"",
        "Document: \"A segmentation method for images compressed by fuzzy transforms. In this paper we describe a segmentation method applied to images which are compressed by using Fuzzy Transforms. The segmentation of the images is realized via the FGFCM (Fast Generalized Fuzzy C-Means) clustering algorithm, which is robust to noise and outliers. The optimal number of clusters is determined via the PCAES (Partition Coefficient And Exponential Separation) validity index. We use a similarity measure defined via Lukasiewicz t-norm for comparison between the original image and the reconstructed images. The best results are obtained if this similarity measure overcomes a threshold value, experimentally determined from the analysis of the trend of it with respect to the PSNR (Peak Signal to Noise Ratio).\"",
        "Document: \"A fuzzy hybrid method for image decomposition problem. We use an hybrid approach based on a genetic algorithm and on the gradient descent method in order to decompose an image.\n In the pre-processing phase the genetic algorithm is used for finding two suitable initial families of fuzzy sets that decompose\n R in accordance to the well known concept of Schein rank. These fuzzy sets are successively used in the descent gradient algorithm\n which determines the final fuzzy sets, useful for the reconstruction of the image. The experiments are executed on some images\n extracted from the the SIDBA standard image database.\n \"",
        "Document: \"A Fuzzy-Based Approach to the Analysis of Financial Investments. In this paper we study the application of a fuzzy algebra to the task of classifying financial investments. A classification system is developed based on several financial indicators and on a fuzzy interpretation of them in terms of linguistic labels and triangular fuzzy numbers. A fuzzy algebra expressly created for clustering and its properties are then discussed. Finally an application example is given using data from a sample of firms whose securities are exchanged in the Boston Stock Exchange.\"",
        "Document: \"Integrating object-oriented paradigms and logic programming: the OPLA language. OPLA is an object-oriented (OO) logic programming language built on top of Common Lisp Object System (CLOS) OPLA enhances logic programming with advanced features of OO paradigm, such as multiple-inheritance, multi-methods, and the constraint-based control flow mechanism. In particular, we discuss the features of the OPLA language used to solve non trivial problems of user-interface development, such as the management of graphical constraints on windows, and the application of law-based delegation. The novel features of OPLA are quantitatively discussed, by reporting their impact in the realization of significant applications\"",
        "Document: \"The Opla System: Designing Complex Systems In An Object-Oriented Logic Programming Framework. This paper describes the OPLA language, an object-oriented (OO) logic programming language layered on Common Lisp Object System. OPLA allows the benefits of the OO approach to the design, organization and realization of large logic programs thanks to an OO programming environment. Thanks to a deep link with the underlying CLOS language, OPLA offers important: features of the OO paradigm, such as multiple inheritance, multi-methods (multi-argument discrimination) and constraint-based control flow mechanism. These characteristics play an important role in OO Prolog programming, improving the readability of the code and augmenting the maintainability of the applications. This paper presents the OPLA language and some experiences resulting from the design and implementation of graphic interfaces which have been realized in OPLA itself. The OPLA programming style facilitates the management of some difficult problems which are basic issues of user-interface development, such as the management of graphical constraints on windows, the distinction of specifying the graphics of objects from their behavior, and law-based delegation. The implementation methodology which was adopted to realize the bulk of the OPLA system is completely independent of hardware constraints; it exploits an incremental, abstract implementation technique based on a high-level generator mechanism, Our approach has been evaluated both in terms of software design and in terms of overall performances. The novel features of OPLA are quantitatively discussed by reporting their impact in the realization of the significant applications.\"",
        "Document: \"Joining fuzzy transform and local learning for wind power forecasting. In this paper, we propose a computational scheme for the problem of wind power forecasting. Such scheme combines a local weighted regression model with fuzzy transform. The latter provides a way to reduce the cardinality of the learning problem, resulting in a more efficient algorithm. Numerical examples show the effectiveness of the proposed approach.\"",
        "Document: \"A Context-Aware Fuzzy Linguistic Consensus Model Supporting Innovation Processes. Nowadays, many research works are moving toward the definition of models for human decision support systems within business process executions. Existing solutions, in general, do not take into account the context in which such processes run but they provide rigid models that could erroneously support decision-making activities when a different context needs to be considered. This work focuses on the definition of a framework to support and trace human decision-making activities, in business processes, when more heterogeneous decision-makers have to find a consensus to select one alternative among the others. One class of such processes is that of Innovation Processes. In particular, the main result described here is a Context-aware Fuzzy Linguistic Consensus Model, based on Fuzzy Logic, Semantic Web technologies and Reinforcement Learning, that considers heterogeneous decision makers with different levels of influence (assigned by considering their past decisions) in the context where the decision activity takes place.\"",
        "Document: \"A framework for context-aware heterogeneous group decision making in business processes. In Business Process Management great attention is given to Computational Intelligence for supporting process life-cycle. Several approaches have been defined to support human decision making. The main drawback is that there are no solid criteria for determining optimal decisions since context, matter of discussion, and involved actors may differ at each execution. This work focuses on the definition of a framework to support and trace human decision making activities, in business processes, when heterogeneous decision-makers have to find a consensus to select most promising alternative to follow. The framework relies on Fuzzy Consensus Model and implements Reinforcement Learning algorithm to learn weight of the decision-makers through the analysis of past process executions considering context and performances of business processes. Context awareness relies on semantic web technologies enabling ontological reasoning to evaluate context similarity used to assign the right weight to the involved decision-makers also in the case when more general or more specific context occurs. The framework has been instantiated in the case study of Supply Chain Management. The analysis of the simulation results reveal that the proposed weight learning algorithm and the considered initial weight association strategies (Starting Weight and Training Executions), even if the cold start, give to decision-makers the chance to fill the gap with respect to more experienced decision makers.\"",
        "1 is \"Similarity-based Web Service Matchmaking\", 2 is \"Towards Principled Methods for Training Generative Adversarial Networks.\"",
        "Given above information, for an author who has written the paper with the title \"On the Protection of fMRI Images in Multi-domain Environments\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007960": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'GHEVC: An Efficient HEVC Decoder for Graphics Processing Units.':",
        "Document: \"Exploiting task and data parallelism for advanced video coding on hybrid CPU + GPU platforms. Considering the prevalent usage of multimedia applications on commodity computers equipped with both CPU and GPU devices, the possibility of simultaneously exploiting all parallelization capabilities of such hybrid platforms for high performance video encoding has been highly quested for. Accordingly, a method to concurrently implement the H.264/ advanced video coding (AVC) inter-loop on hybrid GPU\u00a0+\u00a0CPU platforms is proposed in this manuscript. This method comprises dynamic dependency aware task distribution methods and real-time computational load balancing over both the CPU and the GPU, according to an efficient dynamic performance modeling. With such optimal balance, the set of rather optimized parallel algorithms that were conceived for video coding on both the CPU and the GPU are dynamically instantiated in any of the existing processing devices, to minimize the overall encoding time. The proposed model does not only provide an efficient task scheduling and load balancing for H.264/AVC inter-loop, but it also does not introduce any significant computational burden to the time-limited video coding application. Furthermore, according to the presented set of experimental results, the proposed scheme has proved to provide speedup values as high as 2.5 when compared with highly optimized GPU-only encoding solutions or even other state of the art algorithm. Moreover, by simply using the existing computational resources that usually equip most commodity computers the proposed scheme is able to achieve inter-loop encoding rates as high as 40 fps at a HD 1920\u00a0\u00d7\u00a01080 resolution.\"",
        "Document: \"GHEVC: An Efficient HEVC Decoder for Graphics Processing Units. The high compression efficiency that is provided by the high efficiency video coding (HEVC) standard comes at the cost of a significant increase of the computational load at the decoder. Such an increased burden is a limiting factor to accomplish real-time decoding, specially for high definition video sequences (e.g., Ultra HD 4K). In this scenario, a highly parallel HEVC decoder for the state-of-the-art graphics processor units (GPUs) is presented, i.e., GHEVC. Contrasting to our previous contributions, the data-parallel GHEVC decoder integrates the whole decompression pipeline (except for the entropy decoding), both for intra- and interframes. Furthermore, its processing efficiency was highly optimized by keeping the decompressed frames in the GPU memory for subsequent inter frame prediction. The proposed GHEVC decoder is fully compliant with the HEVC standard, where explicit synchronization points ensure the correct HEVC module execution order. Moreover, the GPU-based HEVC decoder is experimentally evaluated for different GPU devices, an extensive range of recommended HEVC configurations and video sequences, where an average frame rate of 145, 318, and 605 frames per second for Ultra HD 4K, WQXGA, and Full HD, respectively, was obtained in the Random Access configuration with the NVIDIA GeForce GTX TITAN X GPU.\"",
        "Document: \"Stream data prefetcher for the GPU memory interface. Data caches are often unable to efficiently cope with the massive and simultaneous requests imposed by the SIMT execution model of modern GPUs. While software-aided cache management techniques and scheduling approaches were early considered, efficient prefetching schemes are regarded as the most viable solution to improve the efficiency of the GPU memory subsystem. Accordingly, a new GPU prefetching mechanism is proposed, by extending the stream computing model beyond the actual GPU processing core, thus broadening it toward the memory interface. The proposed prefetcher takes advantage of the available cache management resources and combines a low-profile architecture with a dedicated pattern descriptor specification, which is used to explicitly encode each kernel memory access pattern. The obtained results show that the proposed mechanism increases the 1 data cache hit rate by an average of 61%, resulting in performance speedups as high as 9.2 and consequent energy efficiency improvements as high as 11.\"",
        "Document: \"Adaptive In-Cache Streaming for Efficient Data Management. The design of adaptive architectures is frequently focused on the sole adaptation of the processing blocks, often neglecting the power/performance impact of data transfers and data indexing in the memory subsystem. In particular, conventional address-based models, supported on cache structures to mitigate the memory wall problem, often struggle when dealing with memory-bound applications or arbitr...\"",
        "Document: \"Multi-level parallelization of advanced video coding on hybrid CPU+GPU platforms. A dynamic model for parallel H.264/AVC video encoding on hybrid GPU+CPU systems is proposed. The entire inter-prediction loop of the encoder is parallelized on both the CPU and the GPU, and a computationally efficient model is proposed to dynamically distribute the computational load among these processing devices on hybrid platforms. The presented model includes both dependency aware task scheduling and load balancing algorithms. According to the obtained experimental results, the proposed dynamic load balancing model is able to push forward the computational capabilities of these hybrid parallel platforms, achieving a speedup of up to 2 when compared with other equivalent state-of-the-art solutions. With the presented implementation, it was possible to encode 25 frames per second for HD 1920\u00d71080 resolution, even when exhaustive motion estimation is considered.\"",
        "Document: \"Unified transform architecture for AVC, AVS, VC-1 and HEVC high-performance codecs. A unified architecture for fast and efficient computation of the set of two-dimensional (2-D) transforms adopted by the most recent state-of-the-art digital video standards is presented in this paper. Contrasting to other designs with similar functionality, the presented architecture is supported on a scalable, modular and completely configurable processing structure. This flexible structure not only allows to easily reconfigure the architecture to support different transform kernels, but it also permits its resizing to efficiently support transforms of different orders (e.g. order-4, order-8, order-16 and order-32). Consequently, not only is it highly suitable to realize high-performance multi-standard transform cores, but it also offers highly efficient implementations of specialized processing structures addressing only a reduced subset of transforms that are used by a specific video standard. The experimental results that were obtained by prototyping several configurations of this processing structure in a Xilinx Virtex-7 FPGA show the superior performance and hardware efficiency levels provided by the proposed unified architecture for the implementation of transform cores for the Advanced Video Coding (AVC), Audio Video coding Standard (AVS), VC-1 and High Efficiency Video Coding (HEVC) standards. In addition, such results also demonstrate the ability of this processing structure to realize multi-standard transform cores supporting all the standards mentioned above and that are capable of processing the 8k Ultra High Definition Television (UHDTV) video format (7,680 \u00d7 4,320 at 30 fps) in real time.\"",
        "Document: \"Least squares motion estimation algorithm in the compressed DCT domain for H.26x/MPEG-x video sequences. A new compressed domain motion estimation algorithm that makes use of the DCT coefficients directly obtained from the H.26x or MPEG-x video stream is presented The proposed algorithm is based on an iterative scheme that computes the new motion vectors by applying a least squares estimation technique. To reduce its computational effort, the algorithm may consider only an arbitrary subset of non-null DCT coefficients. The performance of the algorithm was assessed in a DCT domain H.263 video transcoder where the obtained motion vectors provided the means to significantly enhance the quality of the temporal prediction scheme with a consequent reduction of the required bit-rate.\"",
        "Document: \"Cooperative CPU+GPU deblocking filter parallelization for high performance HEVC video codecs. Heterogeneous platforms integrating several CPU cores and GPU accelerators have established in several application domains, from desktop, server and mobile. To take full advantage of such platforms, video encoders/decoders have to exploit a broader design space, by cooperatively executing in all the available CPU and GPU cores. To attain such objective, three novel contributions that aim the exploitation of the maximum parallelism level in an HEVC deblocking filter are presented: i) a highly optimized CPU parallel implementation, which outperforms the current state of the art; ii) the first known GPU implementation of the HEVC deblocking filter; and iii) an hybrid and load-balanced CPU+GPU implementation, where all the available resources cooperatively execute, in order to maximize the attained performance. The obtained experimental results demonstrated the ability to achieve processing times as low as 0.8 ms and 0.5 ms to filter 1080p I-type and B-type frames, respectively, corresponding to speedup factors as high as 17 and 9.\"",
        "Document: \"Automatic Synthesis of Motion Estimation Processors Based on a New Class of Hardware Architectures. A new class of fully parameterizable multiple array architectures for motion estimation in video sequences based on the Full-Search Block-Matching algorithm is proposed in this paper. This class is based on a new and efficient AB2 single array architecture with minimum latency, maximum throughput and full utilization of the hardware resources. It provides the ability to configure the target processor within the boundary values imposed for the configuration parameters concerning the algorithm setup, the processing time and the circuit area. With this purpose, a software configuration tool has been implemented to determine the set of possible configurations which fulfill the requisites of a given video coder. Experimental results using both FPGA and ASIC technologies are presented. In particular, the implementation of a single array processor configuration on a single-chip is illustrated, evidencing the ability to estimate motion vectors in real-time.\"",
        "Document: \"Application Specific Programmable IP Core for Motion Estimation: Technology Comparison Targeting Efficient Embedded Co-Processing Units. The implementation of a recently proposed IP core of an efficient motion estimation co-processor is considered. Some significant functional improvements to the base architecture are proposed, as well as the presentation of a detailed description of the interfacing between the co-processor and the main processing unit of the video encoding system. Then, a performance analysis of two distinct implementations of this IP core is presented, considering two different target technologies: a high performance FPGA device, from the Xilinx Virtex-II Pro family, and an ASIC based implementation, using a 0.18um CMOS StdCell library. Experimental results have shown that the two alternative implementations have quite similar performance levels and allow the estimation of motion vectors in real-time.\"",
        "1 is \"Manipulation and compositing of MC-DCT compressed video\", 2 is \"Two Methods of Rijndael Implementation in Reconfigurable Hardware\"",
        "Given above information, for an author who has written the paper with the title \"GHEVC: An Efficient HEVC Decoder for Graphics Processing Units.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007962": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Scalable and Efficient Virtual Memory Sharing in Heterogeneous SoCs with TLB Prefetching and MMU-Aware DMA Engine':",
        "Document: \"Lightweight Virtual Memory Support for Zero-Copy Sharing of Pointer-Rich Data Structures in Heterogeneous Embedded SoCs. While high-end heterogeneous systems are increasingly supporting heterogeneous uniform memory access (hUMA), their low-power counterparts still lack basic features like virtual memory support for accelerators. Instead of simply passing pointers, explicit data management involving copies is needed which hampers programmability and performance. In this work, we evaluate a mixed hardware/software sol...\"",
        "Document: \"Playing with Fire: Transactional Memory Revisited for Error-Resilient and Energy-Efficient MPSoC Execution. As silicon integration technology pushes toward atomic dimensions, errors due to static and dynamic variability are an increasing concern. To avoid such errors, designers often turn to \\\"guardband\\\" restrictions on the operating frequency and voltage. If guardbands are too conservative, they limit performance and waste energy, but less conservative guardbands risk moving the system closer to its Critical Operating Point (COP), a frequency-voltage pair that, if surpassed, causes massive instruction failures. In this paper, we propose a novel scheme that allows to dynamically adjust to an evolving COP and operate at highly reduced margins, while guaranteeing forward progress. Specifically, our scheme dynamically monitors the platform and adaptively adjusts to the COP among multiple cores, using lightweight checkpointing and roll-back mechanisms adopted from Hardware Transactional Memory (HTM) for error recovery. Experiments demonstrate that our technique is particularly effective in saving energy while also offering safe execution guarantees. To the best of our knowledge, this work is the first to describe a full-fledged HTM implementation for error-resilient and energy-efficient MPSoC execution.\"",
        "Document: \"Runtime Support for Multiple Offload-Based Programming Models on Embedded Manycore Accelerators. Many modern high-end embedded systems are designed as heterogeneous systems-on-chip (SoCs), where a powerful general purpose multicore host processor is coupled to a manycore accelerator. The host executes legacy applications on top of standard operating systems, while the accelerator runs highly parallel code kernels within those applications. Several programming models are currently being proposed to program such accelerator-based systems, OpenCL and OpenMP being the most relevant examples. In the near future it will be common to have multiple applications, coded with different programming models, concurrently requiring the use of the manycore accelerator. In this paper we present a runtime system for a cluster-based manycore accelerator, optimized for the concurrent execution of OpenMP and OpenCL kernels. The runtime supports spatial partitioning of the manycore, where clusters can be grouped into several \\\"virtual\\\" accelerator instances. Our runtime design is modular and relies on a \\\"generic\\\" component for resource (cluster) scheduling, plus \\\"specialized\\\" components which efficiently deploy generic offload requests into an implementation of the target programming model's semantics. We evaluate the proposed runtime system on a real heterogeneous system, the STMicroelectronics STHORM development board.\"",
        "Document: \"He-P2012: Performance and Energy Exploration of Architecturally Heterogeneous Many-Cores. The end of Dennardian scaling in advanced technologies brought about new architectural templates to overcome the so-called utilization wall and provide Moore's Law-like performance and energy scaling in embedded SoCs. One of the most promising templates, architectural heterogeneity, is hindered by high cost due to the design space explosion and the lack of effective exploration tools. Our work provides three contributions towards a scalable and effective methodology for design space exploration in embedded MC-SoCs. First, we present the He-P2012 architecture, augmenting the state-of-art STMicroelectronics P2012 platform with heterogeneous shared-L1 coprocessors called HW processing elements (HWPE). Second, we propose a novel methodology for the semi-automatic definition and instantiation of shared-memory HWPEs from a C source, supporting both simple and structured data types. Third, we demonstrate that the integration of HWPEs can provide significant performance and energy efficiency benefits on a set of benchmarks originally developed for the homogeneous P2012, achieving up to 123x speedup on the accelerated code region (~98 % of Amdahl's law limit) while saving 2/3 of the energy.\"",
        "Document: \"SoC-TM: integrated HW/SW support for transactional memory programming on embedded MPSoCs. Two overriding concerns in the development of embedded MPSoCs are ease of programming and hardware complexity. In this paper we present SoC-TM, an integrated HW/SW solution for transactional programming on embedded MPSoCs. Our proposal leverages a Hardware Transactional Memory (HTM) design, based on a dedicated HW module for conflict management, whose functionality is exposed to the software through compiler directives, implemented as an extension to the popular OpenMP programming model. To further improve ease of programming, our framework supports speculative parallelism, thanks to the ability of enforcing a given commit order in hardware. Our experimental results confirm that SoC-TM is a viable and cost-effective solution for embedded MPSoCs, in terms of energy, performance and productivity.\"",
        "Document: \"OpenMP-based Synergistic Parallelization and HW Acceleration for On-Chip Shared-Memory Clusters. Modern embedded MPSoC designs increasingly couple hardware accelerators to processing cores to trade between energy efficiency and platform specialization. To assist effective design of such systems there is the need on one hand for clear methodologies to streamline accelerator definition and instantiation, on the other for architectural templates and run-time techniques that minimize processors-to-accelerator communication costs. In this paper we present an architecture featuring tightly-coupled processors and accelerators, with zero-copy communication. Efficient programming is supported by an extended OpenMP programming model, where custom directives allow to specialize code regions for execution on parallel cores, accelerators, or a mix of the two. Our integrated approach enables fast yet accurate exploration of accelerator-based HW and SW architectures.\"",
        "Document: \"Full system simulation of many-core heterogeneous SoCs using GPU and QEMU semihosting. Modern system-on-chips are evolving towards complex and heterogeneous platforms with general purpose processors coupled with massively parallel manycore accelerator fabrics (e.g. embedded GPUs). Platform developers are looking for efficient full-system simulators capable of simulating complex applications, middleware and operating systems on these heterogeneous targets. Unfortunately current virtual platforms are not able to tackle the complexity and heterogeneity of state-of-the-art SoCs. Software emulators, such as the open-source QEMU project, cope quite well in terms of simulation speed and functional accuracy with homogeneous coarse-grained multi-cores. The main contribution of this paper is the introduction of a novel virtual prototyping technique which exploits the heterogeneous accelerators available in commodity PCs to tackle the heterogeneity challenge in full-SoC system simulation. In a nutshell, our approach makes it possible to partition simulation between the host CPU and GPU. More specifically, QEMU runs on the host CPU and the simulation of manycore accelerators is offloaded, through semi-hosting, to the host GPU. Our experimental results confirm the flexibility and efficiency of our enhanced QEMU environment.\"",
        "Document: \"Lightweight virtual memory support for many-core accelerators in heterogeneous embedded SoCs. While high-end heterogeneous systems are increasingly supporting heterogeneous uniform memory access (hUMA) as envisioned by the Heterogeneous System Architecture (HSA) foundation, their low-power counterparts targeting the embedded domain still lack basic features like virtual memory support for accelerators. As opposed to simply passing virtual address pointers, explicit data management involving copies is needed to share data between host processor and accelerators which hampers programmability and performance.\n\nIn this work, we present a mixed hardware/software solution to enable lightweight virtual memory support for many-core accelerators in heterogeneous embedded systems-on-chip (SoCs). Based on an input/output translation lookaside buffer (IOTLB), efficiently managed by a kernel-level driver module running on the host, our solution features a considerably lower design complexity compared to conventional input/output memory management units. Using our evaluation platform based on the Xilinx Zynq-7000 SoC with a many-core accelerator implemented in the programmable logic, we demonstrate the effectiveness of our solution and the benefits of virtual memory support for embedded heterogeneous SoCs.\n\n\"",
        "Document: \"On the Relevance of Architectural Awareness for Efficient Fork/Join Support on Cluster-Based Manycores. Several recent manycores leverage a hierarchical design, where small-medium numbers of cores are grouped inside clusters and enjoy low-latency, high-bandwidth local communication through fast L1 scratchpad memories. Several clusters can be interconnected through a network-on-chip (NoC), which ensures system scalability but introduces non-uniform memory access (NUMA) effects: the cost to access a specific memory location depends of the physical path that corresponding transactions traverse. These peculiarities of the HW must clearly be carefully taken into account when designing support for programming models. In this paper we study how architectural awareness is key to supporting efficient and streamlined fork/join primitives. We compare hierarchical fork/join operations to \\\"flat\\\" ones, where there is no notion of the hierarchical interconnection system, considering two real-world manycores: Intel SCC and STMicro-electronics STHORM.\"",
        "Document: \"A HLS-Based Toolflow to Design Next-Generation Heterogeneous Many-Core Platforms with Shared Memory. This work describes how we use High-Level Synthesis to support design space exploration (DSE) of heterogeneous many-core systems. Modern embedded systems increasingly couple hardware accelerators and processing cores on the same chip, to trade specialization of the platform to an application domain for increased performance and energy efficiency. However, the process of designing such a platform is complex and error-prone, and requires skills on algorithmic aspects, ardware synthesis, and software engineering. DSE can partially be automated, and thus simplified, by coupling the use of HLS tools and virtual prototyping platforms. In this paper we enable the design space exploration of heterogeneous many-cores adopting a shared-memory architecture template, where communication and synchronization between the hardware accelerators and the cores happens through L1 shared memory. This communication infrastructure leverages a \\\"zero-copy\\\" scheme, which simplifies both the design process of the platform and the development of applications on top of it. Moreover, the shared-memory template perfectly fits the semantics of several high-level programming models, such as OpenMP. We provide programmers with simple yet powerful abstractions to exploit accelerators from within an OpenMP application, and propose a low-cost implementation of the necessary runtime support. An HLS-based automatic design flow is set up, to quickly explore the design space using a cycleaccurate virtual platform.\"",
        "1 is \"Embedded Binarized Neural Networks.\", 2 is \"Software performance estimation strategies in a system-level design tool\"",
        "Given above information, for an author who has written the paper with the title \"Scalable and Efficient Virtual Memory Sharing in Heterogeneous SoCs with TLB Prefetching and MMU-Aware DMA Engine\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007979": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Integrated civilian-military pre-positioning of emergency supplies: A multiobjective optimization approach.':",
        "Document: \"DPCG: an efficient density peaks clustering algorithm based on grid. To deal with the complex structure of the data set, density peaks clustering algorithm (DPC) was proposed in 2014. The density and the delta-distance are utilized to find the clustering centers in the DPC method. It detects outliers efficiently and finds clusters of arbitrary shape. But unfortunately, we need to calculate the distance between all data points in the first process, which limits the running speed of DPC algorithm on large datasets. To address this issue, this paper introduces a novel approach based on grid, called density peaks clustering algorithm based on grid (DPCG). This approach can overcome the operation efficiency problem. When calculating the local density, the idea of the grid is introduced to reduce the computation time based on the DPC algorithm. Neither it requires calculating all the distances nor much input parameters. Moreover, DPCG algorithm successfully inherits the all merits of the DPC algorithm. Experimental results on UCI data sets and artificial data show that the DPCG algorithm is flexible and effective.\"",
        "Document: \"MiCroKit 3.0: an integrated database of midbody, centrosome and kinetochore. During cell division/mitosis, a specific subset of proteins is spatially and temporally assembled into protein super complexes in three distinct regions, i.e. centrosome/spindle pole, kinetochore/centromere and midbody/cleavage furrow/phragmoplast/bud neck, and modulates cell division process faithfully. Although many experimental efforts have been carried out to investigate the characteristics of these proteins, no integrated database was available. Here, we present the MiCroKit database (http://microkit.biocuckoo.org) of proteins that localize in midbody, centrosome and/or kinetochore. We collected into the MiCroKit database experimentally verified microkit proteins from the scientific literature that have unambiguous supportive evidence for subcellular localization under fluorescent microscope. The current version of MiCroKit 3.0 provides detailed information for 1489 microkit proteins from seven model organisms, including Saccharomyces cerevisiae, Schizasaccharomyces pombe, Caenorhabditis elegans, Drosophila melanogaster, Xenopus laevis, Mus musculus and Homo sapiens. Moreover, the orthologous information was provided for these microkit proteins, and could be a useful resource for further experimental identification. The online service of MiCroKit database was implemented in PHP+MySQL+JavaScript, while the local packages were developed in JAVA 1.5 (J2SE 5.0).\"",
        "Document: \"CPSS: a computational platform for the analysis of small RNA deep sequencing data. Next generation sequencing (NGS) techniques have been widely used to document the small ribonucleic acids (RNAs) implicated in a variety of biological, physiological and pathological processes. An integrated computational tool is needed for handling and analysing the enormous datasets from small RNA deep sequencing approach. Herein, we present a novel web server, CPSS (a computational platform for the analysis of small RNA deep sequencing data), designed to completely annotate and functionally analyse microRNAs (miRNAs) from NGS data on one platform with a single data submission. Small RNA NGS data can be submitted to this server with analysis results being returned in two parts: (i) annotation analysis, which provides the most comprehensive analysis for small RNA transcriptome, including length distribution and genome mapping of sequencing reads, small RNA quantification, prediction of novel miRNAs, identification of differentially expressed miRNAs, piwi-interacting RNAs and other non-coding small RNAs between paired samples and detection of miRNA editing and modifications and (ii) functional analysis, including prediction of miRNA targeted genes by multiple tools, enrichment of gene ontology terms, signalling pathway involvement and protein-protein interaction analysis for the predicted genes. CPSS, a ready-to-use web server that integrates most functions of currently available bioinformatics tools, provides all the information wanted by the majority of users from small RNA deep sequencing datasets.\"",
        "Document: \"Prior knowledge guided differential evolution. Differential evolution (DE) has become one of the most popular paradigms of evolutionary algorithms. Over the past two decades, some prior knowledge has been obtained in the DE research community. It is an interesting topic to enhance the performance of DE by taking advantage of such prior knowledge. Along this line, a prior knowledge guided DE (called PKDE) is proposed in this paper. In PKDE, we extract two levels of prior knowledge, i.e., the macro- and micro-levels. In order to integrate these two levels of prior knowledge in an effective way, the control parameters of PKDE are tuned based on two distributions (i.e., Cauchy and normal distribution), with the aim of alleviating the premature convergence at the early stage and speeding up the convergence toward the global optimal solution at the later stage. In addition, the self-adaptive mutation strategy is implemented based on our previous study. PKDE is compared with eight DE variants and seven non-DE algorithms on two sets of benchmark test functions from IEEE CEC2005 and IEEE CEC2014. Systematic experiments demonstrate that the overall performance of PKDE is very competitive.\"",
        "Document: \"A semi-supervised approximate spectral clustering algorithm based on HMRF model. Before clustering, we usually have some background knowledge about the data structure. Pairwise constraints are commonly used background knowledge. For graph partition problems, pairwise constraints can be naturally added to the graph edge. This paper integrates pairwise constraints into the objective function of graph cuts and derive the semi-supervised approximate spectral clustering based on Hidden Markov Random Fields (HMRF). This algorithm utilize the mathematical connection between HMRF semi-supervised clustering and approximate weighted kernel k-means. The approximate weighted kernel k-means is used to calculate the optimal clustering results of HMRF spectral clustering. The effectiveness of the proposed algorithm is verified on several benchmark data sets. Experiments show that adding more pairwise constraints will help improve the clustering performance. Our method has advantages for the challenging clustering tasks of large-scale nonlinear data because of the high efficiency and less memory consumption.\"",
        "Document: \"Measures of uncertainty for neighborhood rough sets. Uncertainty measures are critical evaluating tools in machine learning fields, which can measure the dependence and similarity between two feature subsets and can be used to judge the significance of features in classifying and clustering algorithms. In the classical rough sets, there are some uncertainty tools to measure a feature subset, including accuracy, roughness, information entropy, rough entropy, etc. These measures are applicable to discrete-valued information systems, but not suitable to real-valued data sets. In this paper, by introducing the neighborhood rough set model, each object is associated with a neighborhood subset, named a neighborhood granule. Several uncertainty measures of neighborhood granules are proposed, which are neighborhood accuracy, information quantity, neighborhood entropy and information granularity in the neighborhood systems. Furthermore, we prove that these uncertainty measures satisfy non-negativity, invariance and monotonicity. The maximum and minimum of these measures are also given. Theoretical analysis and experimental results show that information quantity, neighborhood entropy and information granularity measures are better than the neighborhood accuracy measure in the neighborhood systems.\"",
        "Document: \"MBA: a literature mining system for extracting biomedical abbreviations. The exploding growth of the biomedical literature presents many challenges for biological researchers. One such challenge is from the use of a great deal of abbreviations. Extracting abbreviations and their definitions accurately is very helpful to biologists and also facilitates biomedical text analysis. Existing approaches fall into four broad categories: rule based, machine learning based, text alignment based and statistically based. State of the art methods either focus exclusively on acronym-type abbreviations, or could not recognize rare abbreviations. We propose a systematic method to extract abbreviations effectively. At first a scoring method is used to classify the abbreviations into acronym-type and non-acronym-type abbreviations, and then their corresponding definitions are identified by two different methods: text alignment algorithm for the former, statistical method for the latter.A literature mining system MBA was constructed to extract both acronym-type and non-acronym-type abbreviations. An abbreviation-tagged literature corpus, called Medstract gold standard corpus, was used to evaluate the system. MBA achieved a recall of 88% at the precision of 91% on the Medstract gold-standard EVALUATION Corpus.We present a new literature mining system MBA for extracting biomedical abbreviations. Our evaluation demonstrates that the MBA system performs better than the others. It can identify the definition of not only acronym-type abbreviations including a little irregular acronym-type abbreviations (e.g., <CNS1, cyclophilin seven suppressor>), but also non-acronym-type abbreviations (e.g., <Fas, CD95>).\"",
        "Document: \"A rapid learning algorithm for vehicle classification. \u2022A fast learning algorithm is introduced for real-time vehicle classification.\u2022A fast feature selection method for AdaBoost is presented by combining a sample\u2019s feature value with its class label.\u2022A rapid incremental learning algorithm of AdaBoost is designed.\"",
        "Document: \"The performance research of artificial bee colony algorithm on the large scale global optimisation problems. AbstractSwarm intelligent optimisation algorithms have been researched widely in recent years. Artificial Bee Colony ABC algorithm is a kind of swarm intelligent algorithm with briefness and efficiency for global optimisation problems. This paper focuses on the performance of ABC algorithm on the large-scale global optimisation problems. The algorithm has been tested on 12 benchmark functions from relevant literature. The experiment results of ABC algorithm on the large-scale global optimisation problems have been shown in this paper. The experiment results indicated that as the dimensions of the test functions increase, the performance of ABC on most test problems deteriorates gradually.\"",
        "Document: \"Improved gene expression programming to solve the inverse problem for ordinary differential equations. Many complex systems in the real world evolve with time. These dynamic systems are often modeled by ordinary differential equations in mathematics. The inverse problem of ordinary differential equations is to convert the observed data of a physical system into a mathematical model in terms of ordinary differential equations. Then the model may be used to predict the future behavior of the physical system being modeled. Genetic programming has been taken as a solver of this inverse problem. Similar to genetic programming, gene expression programming could do the same job since it has a similar ability of establishing the model of ordinary differential systems. Nevertheless, such research is seldom studied before. This paper is one of the first attempts to apply gene expression programming for solving the inverse problem of ordinary differential equations. Based on a statistic observation of traditional gene expression programming, an improvement is made in our algorithm, that is, genetic operators should act more often on the dominant part of genes than on the recessive part. This may help maintain population diversity and also speed up the convergence of the algorithm. Experiments show that this improved algorithm performs much better than genetic programming and traditional gene expression programming in terms of running time and prediction precision.\"",
        "1 is \"Research of new strategies for improving CBR system\", 2 is \"An agent-based approach to multisensor coordination\"",
        "Given above information, for an author who has written the paper with the title \"Integrated civilian-military pre-positioning of emergency supplies: A multiobjective optimization approach.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "007983": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Curved support structures and meshes with spherical vertex stars':",
        "Document: \"Global Stabilization of Stochastic Nonlinear Systems Via $C^1$ and $C^{\\infty }$ Controllers. The problem of designing C 1 or C controllers for a class of stochastic nonlinear systems (SNSs) in lower-triangular form is studied in this note. By using the well-known backstepping method, the concept of homogeneity with monotone degrees (HWMD) and the sign function approach, we construct a C 1 state feedback controller recursively. Meanwhile, by employing a polynomial Lyapunov function with sign functions, we prove that the solution to SNSs is globally asymptotically stable in probability. Furthermore, based on the concept of HWMD, it shows that C 1 controllers for a class of three-dimensional SNSs can be modified to C controllers under certain conditions. Finally, two simulation examples are given to demonstrate the effectiveness of the proposed controllers.\"",
        "Document: \"A Novel Least Square Twin Support Vector Regression. This paper proposes a new method for regression named lp norm least square twin support vector regression (PLSTSVR), which is formulated by the idea of twin support vector regression (TSVR). Different from TSVR, our new model is an adaptive learning procedure with p-norm SVM (\\({{0<p\\le 2}}\\)), where p is viewed as an adjustable parameter and can be automatically chosen by data. An iterative algorithm is suggested to solve PLSTSVR efficiently. In each iteration, only a series systems of linear equations (LEs) are solved. Experiments carried out on several standard UCI datasets and synthetic datasets show the feasibility and effectiveness of the proposed method.\"",
        "Document: \"Modulation Recognition of Digital Multimedia Signal Based on Data Feature Selection. AbstractAutomatic modulation recognition is very important for the receiver design in the broadband multimedia communication system, and the reasonable signal feature extraction and selection algorithm is the key technology of Digital multimedia signal recognition. In this paper, the information entropy is used to extract the single feature, which are power spectrum entropy, wavelet energy spectrum entropy, singular spectrum entropy and Renyi entropy. And then, the feature selection algorithm of distance measurement and Sequential Feature SelectionSFS are presented to select the optimal feature subset. Finally, the BP neural network is used to classify the signal modulation. The simulation result shows that the four-different information entropy can be used to classify different signal modulation, and the feature selection algorithm is successfully used to choose the optimal feature subset and get the best performance.\"",
        "Document: \"Web-Based Traffic Sentiment Analysis: Methods and Applications. With the booming of social media, sentiment analysis has developed rapidly in recent years. However, only a few studies focused on the field of transportation, which failed to meet the stringent requirements of safety, efficiency, and information exchange of intelligent transportation systems (ITSs). We propose the traffic sentiment analysis (TSA) as a new tool to tackle this problem, which provides a new prospective for modern ITSs. Methods and models in TSA are proposed in this paper, and the advantages and disadvantages of rule- and learning-based approaches are analyzed based on web data. Practically, we applied the rule-based approach to deal with real problems, presented an architectural design, constructed related bases, demonstrated the process, and discussed the online data collection. Two cases were studied to demonstrate the efficiency of our method: the \u201cyellow light rule\u201d and \u201cfuel price\u201d in China. Our work will help the development of TSA and its applications.\"",
        "Document: \"Generalized Humbert polynomials via generalized Fibonacci polynomials. In this paper, we define the generalized (p, q)-Fibonacci polynomials un, m(x) and generalized (p, q)-Lucas polynomials vn, m(x), and further introduce the generalized Humbert polynomials un,m(r)(x) as the convolutions of un, m(x). We give many expressions, expansions, recurrence relations and differential recurrence relations of un,m(r)(x), and study the matrices and determinants related to the polynomials un, m(x), vn, m(x) and un,m(r)(x). Finally, we present an algebraic interpretation for the generalized Humbert polynomials un,m(r)(x). It can be found that various well-known polynomials are special cases of un, m(x), vn, m(x) or un,m(r)(x). Therefore, by introducing the general polynomials un, m(x), vn, m(x) and un,m(r)(x), we have a unified approach to dealing with many special polynomials in the literature.\"",
        "Document: \"Detecting Communities on Topic of Transportation With Sparse Crowd Annotations. Social networks contain a large amount of information on transportation, e.g., traffic accidents, congestions, and vehicles. Such information is the original ideas of people with respect to real-world transportation issues, and detecting communities on the topic of transportation from the information will benefit many ITS applications. However, real-world social network nodes often contain multiple attributes, and the network can be very large. The two properties can lead to confusion and unscalability problem to clustering methods. In this paper, we propose a semisupervised method, namely, $\\\\underline{Tra}$nsportation $\\\\underline{C}$ommunity $\\\\underline{De}$tection (TRACED), to address this problem. TRACED allows multiple individuals to select their familiar nodes as the participants of a certain community, and thus, the confusion of multiple attributes can be largely reduced. Moreover, the proposed method can be expanded to large networks since it is able to conduct an effective clustering with low time complexity. With the help of TRACED, we can detect densely connected communities on the topic of transportation for further studies.\"",
        "Document: \"Measurement and analysis of topology and information propagation on Sina-Microblog. Sina-Microblog, the earliest and biggest microblogging service in China, has become one of the most popular media in information propagation. In order to gain insights into the topological and information diffusing characteristics of microblogging network in China, we crawled Sina-Microblog for about 3 months and obtain the trace of its topology and topics. Compared with other online social networks, our measurement study shows a number of interesting findings. Our data suggests that Sina-Microblog network has apparent small-world effect and scale-free characteristic, specially, the outdegree distribution appears to have multiple separate power-law regimes with different exponents. We also observe the overlay graph of Sina-Microblog represents assortative mixing pattern and weak correlation of indegree and outdegree. Moreover, by constructing the cascades of different topics, our data suggests that the distribution of cascades size follows a power-law and heavy-tailed property with the slope approximately -2, and the common motifs of cascades with different topics are very similar, above 93% of them are isolated nodes. In order to find the formative motivity of hot cascades, we find that they always evolve to the structures like `star pattern' and `two-polar pattern', which are mainly due to the indegree of participating nodes, and are also correlated with the content of tweet.\"",
        "Document: \"Building the Semantic Relations-Based Web Services Registry through Services Mining. Bridging the gap between service consumer and service provider, Web services registry (WSR) plays an important role in the Web services architecture. But the main obstacle in front of service register is short of adequate knowledge. In this paper, we present an approach that combines social networking and Semantic Web technology with WSR to facilitate the dynamic Web Services Composition (WSC) based on AI reasoning and planning approach. Firstly, we clarify the novel WSR, named Services Network (SN), and present its concepts and components. Secondly, we outline how to build SN through Services Mining and some of the issues that should be considered. Thirdly, we evaluate how this network style registry could facilitate automated WSC. Lastly, a short comprehensive overview of existing related works is also included.\"",
        "Document: \"A novel image restoration scheme based on structured side information and its application to image watermarking. This paper presents a new image restoration method based on a linear optimization model which restores part of the image from structured side information (SSI). The SSI can be transmitted to the receiver or embedded into the image itself by a digital watermarking technique. In this paper we focus on a special type of SSI for digital watermarking where the SSI is composed of mean values of 4x4 image blocks which can be used to restore manipulated blocks. Different from existing image restoration methods for similar types of SSI, the proposed method minimizes image discontinuity according to a relaxed definition of smoothness based on a 3x3 averaging filter of four adjacent pixel value differences, and the objective function of the optimization model has a second regularization term corresponding to a second-order smoothness criterion. Our experiments on 100 test images showed that given complete information of the SSI, the proposed image restoration technique can outperform the state-of-the-art model based on a simple linear optimization model by around 2dB in terms of an average Peak Signal-to-Noise Ratio (PSNR) value and around 0.04 in terms of a Structural Similarity Index (SSIM) value. We also tested the robustness of the image restoration method when it is applied to a self-restoration watermarking scheme and the experimental results showed that it is moderately robust to errors in SSI (which is embedded as a watermark) caused by JPEG compression (the average PSNR value remains above 16.5dB even when the JPEG QF is 50), additive Gaussian white noises (the average PSNR value is approximately 18.4dB when the noise variance @s^2 is 10) and image rescaling assuming the original image size is known at the receiver side (e.g. the average PSNR value is approximately 19.6dB when the scaling ratio is 1.4).\"",
        "Document: \"A 38 Gb/s to 43 Gb/s Monolithic Optical Receiver in 65 nm CMOS Technology. A scaled 40 Gb/s optical receiver incorporating a transimpedance amplifier (TIA), a limiting amplifier (LA), a clock and data recovery (CDR), and a 1:4 demultiplexer was proposed in 65 nm CMOS technology. The TIA employs a regulated cascode structure to achieve low input resistance and a stable dc operating point, whereas the LA adopts the third-order interleaving active feedback technique to obtain greater bandwidth and flatter frequency response. A 10 GHz LC-based voltage controlled oscillator with a ring structure that generates eight phases is presented. A quarter-rate phase detector in the CDR samples the 40 Gb/s input data, which are retimed and demultiplexed into four sets of 10 Gb/s output data. Experimental results show that the recovered clock exhibits a phase noise of -112.39 dBc/Hz@10 MHz from a carrier frequency of 10 GHz, in response to 231-1 PRBS input. The retimed and demultiplexed data exhibit a peak-peak jitter of 4.46 ps and an RMS jitter of 1.18 ps. The core circuit of the receiver consumes 160 mW from a 1.2 V supply.\"",
        "1 is \"Discrete Differential Geometry\", 2 is \"An Implicit Shape Model for Combined Object Categorization and Segmentation\"",
        "Given above information, for an author who has written the paper with the title \"Curved support structures and meshes with spherical vertex stars\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008002": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A hybrid evolutionary approach to the university course timetabling problem':",
        "Document: \"Solving feature selection problem using intelligent double treatment iterative composite neighbourhood structure algorithm. Attribute reduction is one of the main contributions in rough set theory RST that tries to discover all possible reducts by eliminating redundant attributes while maintaining the information of the problem in hand. In this paper, we propose a meta-heuristic methodology called a double treatment iterative improvement algorithm with intelligent selection of composite neighbourhood structure, to solve the attribute reduction problems and to obtain near optimal reducts. The algorithm works iteratively by only accepting an improved solution. The proposed approach has been tried on a set of 13 benchmark datasets taken from the University of California Irvine UCI machine learning repository in line with the state-of-the-art methods. Thirteen datasets have been chosen due to the differences in size and complexity in order to test the stability of the proposed algorithm. The experimental results demonstrate that the proposed approach is able to produce competitive results for the tested datasets.\"",
        "Document: \"A tabu-based memetic approach for examination timetabling problems. Constructing examination timetable for higher educa tional institutions is a very complex work due to the complexity of timetabling problems. The objective of examination timetabling problem is to satisfy the hard constraints and minimize the violations of soft con straints. In this paper, a tabu-based memetic approach has been applied and evaluated against the latest methodologies in the literature on standard benchmark problems. The approach which hybridizes a concept of tabu list an d memetic algorithm. The tabu list is used to penalise neighbourhood structu res that are unable to generate better solutions after the crossover and m utation operators being applied to the selected solutions from the populati on pool. We demonstrate that our approach is able to enhance the quality of the solutions by carefully selecting the effective neighbourhood structures. H ence, some best known results have been obtained .\"",
        "Document: \"Fish swarm intelligent algorithm for the course timetabling problem. In this work, a simulation of fish swarm intelligence has been applied on the course timetabling problem. The proposed algorithm simulates the movements of the fish when searching for food inside a body of water (refer as a search space). The search space is classified based on the visual scope of fishes into three categories which are crowded, not crowded and empty areas. Each fish represents a solution in the solution population. The movement direction of solutions is determined based on a Nelder-Mead simplex algorithm. Two types of local search i.e. a multi decay rate great deluge (where the decay rate is intelligently controlled by the movement direction) and a steepest descent algorithm have been applied to enhance the quality of the solution. The performance of the proposed approach has been tested on a standard course timetabling problem. Computational experiments indicate that our approach produces best known results on a number of these benchmark problems.\"",
        "Document: \"Investigation on Image Processing Techniques for Diagnosing Paddy Diseases. The main objective of this research is to develop a prototype system for diagnosing paddy diseases, which are Blast Disease (BD), Brown-Spot Disease (BSD), and Narrow Brown-Spot Disease (NBSD). This paper concentrates on extracting paddy features through off-line image. The methodology involves image acquisition, converting the RGB images into a binary image using automatic thresholding based on local entropy threshold and Otsu method. A morphological algorithm is used to remove noises by using region filling technique. Then, the image characteristics consisting of lesion type, boundary colour, spot colour, and broken paddy leaf colour are extracted from paddy leaf images. Consequently, by employing production rule technique, the paddy diseases are recognized about 94.7 percent of accuracy rates. This prototype has a very great potential to be further improved in the future.\"",
        "Document: \"Multi-tiered S-SOA, Parameter-Driven New Islamic Syariah Products of Holistic Islamic Banking System (HiCORE): Virtual Banking Environment. The Holistic Islamic Banking System (HiCORE), a banking system suitable for virtual banking environment, created based on universityindustry collaboration initiative between Universiti Kebangsaan Malaysia (UKM) and Fuziq Software Sdn Bhd. HiCORE was modeled on a multitiered Simple - Services Oriented Architecture (S-SOA), using the parameterbased semantic approach. HiCORE's existence is timely as the financial world is looking for a new approach to creating banking and financial products that are interest free or based on the Islamic Syariah principles and jurisprudence. An interest free banking system has currently caught the interest of bankers and financiers all over the world. HiCORE's Parameter-based module houses the Customer-information file (CIF), Deposit and Financing components. The Parameter based module represents the third tier of the multi-tiered Simple SOA approach. This paper highlights the multi-tiered parameter- driven approach to the creation of new Islamiic products based on the `dalil' (Quran), `syarat' (rules) and `rukun' (procedures) as required by the syariah principles and jurisprudence reflected by the semantic ontology embedded in the parameter module of the system.\"",
        "Document: \"Investigating Ahuja-Orlin's Large Neighbourhood Search Approach for Examination Timetabling. Since the 1960's, automated approaches to examination timetabling have been explored and a wide variety of approaches have been investigated and developed. In this paper we build upon a recently presented, sequential solution improvement technique which searches efficiently over a very large set of 'adjacent' (neighbourhood) solutions. This solution search methodology, originally developed by Ahuja and Orlin, has been applied successfully in the past to a number of difficult combinatorial optimization problems. It is based on an improvement graph representation of solution adjacency and identifies improvement moves by finding cycle exchange operations using a modified shortest path label-correcting algorithm. We have drawn upon Ahuja-Orlin's basic methodology to develop an effective automated exam timetabling technique. We have evaluated our approach against the latest methodologies in the literature on standard benchmark problems. We demonstrate that our approach produces some of the best known.\"",
        "Document: \"Fuzzy job-shop scheduling problems: A review. \u2022A review and classification of fuzzy job shop scheduling problems (JSSPs).\u2022Variation of constraints and objectives investigated in Fuzzy JSSPs.\u2022Exact and heuristic methods applied on Fuzzy JSSPs.\u2022Meta-heuristic approaches applied on Fuzzy JSSPs at pre-processing, initialization and improvement steps.\"",
        "Document: \"Hybridizing firefly algorithms with a probabilistic neural network for solving classification problems. \u2022Hybridizes the firefly algorithm with simulated annealing, where simulated annealing is applied to control the randomness step inside the firefly algorithm.\u2022A L\u00e9vy flight is embedded within the firefly algorithm to better explore the search space.\u2022A combination of firefly, L\u00e9vy flight and simulated annealing is investigated to further improve the solution.\"",
        "Document: \"A combined approach for clustering based on K-means and gravitational search algorithms. Clustering is an attractive and important task in data mining that is used in many applications. Clustering refers to grouping together data objects so that objects within a cluster are similar to one another, while objects in different clusters are dissimilar. K-means is a simple and efficient algorithm that is widely used for data clustering. However, its performance depends on the initial state of centroids and may trap in local optima. The gravitational search algorithm (GSA) is one effective method for searching problem space to find a near optimal solution. In this paper, we present a hybrid data clustering algorithm based on GSA and k-means (GSA-KM), which uses the advantages of both algorithms. The GSA-KM algorithm helps the k-means algorithm to escape from local optima and also increases the convergence speed of the GSA algorithm. We compared the performance of GSA-KM with other well-known algorithms, including k-means, genetic algorithm (GA), simulated annealing (SA), ant colony optimization (ACO), honey bee mating optimization (HBMO), particle swarm optimization (PSO) and gravitational search algorithm (GSA). Five real and standard datasets from the UCI repository have been used to demonstrate the results of the algorithms. The experimental results are encouraging in terms of the quality of the solutions and the convergence speed of the proposed algorithm.\"",
        "Document: \"A Multi-Start Very Large Neighbourhood Search Approach with Local Search Methods for Examination Timetabling. This paper investigates a hybridisation of the very large neighbourhood search approach with local search methods to address examination timetabling problems. In this paper, we describe a 2 phase approach. The first phase employs \"multi start\" to carry out search upon a very large neighbourhood of solutions using graph theoretical algorithms implemented on an improvement graph. The second phase makes further improvements by utilising a local search method. We present experimental results which show that this combined approach compares favourably with other algorithms on the standard benchmark problems.\"",
        "1 is \"Packing Problems with Orthogonal Rotations\", 2 is \"Heterogeneous cross domain ranking in latent space\"",
        "Given above information, for an author who has written the paper with the title \"A hybrid evolutionary approach to the university course timetabling problem\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008024": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Robotic UBIquitous COgnitive Network.':",
        "Document: \"Nearest neighbor search in metric spaces through Content-Addressable Networks. Most of the peer-to-peer search techniques proposed in the recent years have focused on the single-key retrieval. However, similarity search in metric spaces represents an important paradigm for content-based retrieval in many applications. In this paper we introduce an extension of the well-known Content-Addressable Network paradigm to support storage and retrieval of more generic metric space objects. In particular we address the problem of executing the nearest neighbors queries, and propose three different algorithms of query propagation. An extensive experimental study on real-life data sets explores the performance characteristics of the proposed algorithms by showing their advantages and disadvantages.\"",
        "Document: \"A scalable nearest neighbor search in p2p systems. Similarity search in metric spaces represents an important paradigm for content-based retrieval in many applications. Existing centralized search structures can speed-up retrieval, but they do not scale up to large volume of data because the response time is linearly increasing with the size of the searched file. In this article, we study the problem of executing the nearest neighbor(s) queries in a distributed metric structure, which is based on the P2P communication paradigm and the generalized hyperplane partitioning. By exploiting parallelism in a dynamic network of computers, the query execution scales up very well considering both the number of distance computations and the hop count between the peers. Results are verified by experiments on real-life data sets.\"",
        "Document: \"MI-File: using inverted files for scalable approximate similarity search. We propose a new efficient and accurate technique for generic approximate similarity searching, based on the use of inverted files. We represent each object of a dataset by the ordering of a number of reference objects according to their distance from the object itself. In order to compare two objects in the dataset, we compare the two corresponding orderings of the reference objects. We show that this representation enables us to use inverted files to obtain very efficiently a very small set of good candidates for the query result. The candidate set is then reordered using the original similarity function to obtain the approximate similarity search result. The proposed technique performs several orders of magnitude better than exact similarity searches, still guaranteeing high accuracy. To also demonstrate the scalability of the proposed approach, tests were executed with various dataset sizes, ranging from 200,000 to 100 million objects.\"",
        "Document: \"A content-addressable network for similarity search in metric spaces. In this paper we present a scalable and distributed access structure for similarity search in metric spaces. The approach is based on the Content-addressable Network (CAN) paradigm, which provides a Distributed Hash Table (DHT) abstraction over a Cartesian space. We have extended the CAN structure to support storage and retrieval of generic metric space objects. We use pivots for projecting objects of the metric space in an N-dimensional vector space, and exploit the CAN organization for distributing the objects among the computing nodes of the structure. We obtain a Peer-to-Peer network, called the MCAN, which is able to search metric space objects by means of the similarity range queries. Experiments conducted on our prototype system confirm full scalability of the approach.\"",
        "Document: \"A Metric Index for Approximate Text Management. Text collections of data need not only search support for identical objects, but the approximate matching is even more important. A suitable metric to such a task is the edit distance measure. However, the quadratic computa- tional complexity of edit distance prevents from apply- ing naive storage organizations, such as the sequential search, and more sophisticated search structures must be applied. We have investigated the properties of the D-index to approximate searching and matching in text databases. The experiments confirm a very good perfor- mance for retrieving close objects and sub-linear scala- bility to process large files. Even the similarity joins can be performed efficiently.\"",
        "Document: \"Large Scale Image Retrieval Using Vector of Locally Aggregated Descriptors. Vector of locally aggregated descriptors (VLAD) is a promising approach for addressing the problem of image search on a very large scale. This representation is proposed to overcome the quantization error problem faced in Bag-of-Words (BoW) representation. However, text search engines have not be used yet for indexing VLAD given that it is not a sparse vector of occurrence counts. For this reason BoW approach is still the most widely adopted method for finding images that represent the same object or location given an image as a query and a large set of images as dataset. In this paper, we propose to enable inverted files of standard text search engines to exploit VLAD representation to deal with large-scale image search scenarios. We show that the use of inverted files with VLAD significantly outperforms BoW in terms of efficiency and effectiveness on the same hardware and software infrastructure.\"",
        "Document: \"A Benchmark Dataset for Human Activity Recognition and Ambient Assisted Living. We present a data benchmark for the assessment of human activity recognition solutions, collected as part of the EU FP7 RUBICON project, and available to the scientific community. The dataset provides fully annotated data pertaining to numerous user activities and comprises synchronized data streams collected from a highly sensor-rich home environment. A baseline activity recognition performance obtained through an Echo State Network approach is provided along with the dataset.\"",
        "Document: \"Deep learning for decentralized parking lot occupancy detection. \u2022We propose an effective CNN architecture for visual parking occupancy detection.\u2022The CNN architecture is small enough to run on smart cameras.\u2022The proposed solution performs and generalizes better than other SotA approaches.\u2022We provide a new training/validation dataset for parking occupancy detection.\"",
        "Document: \"A mediator-based approach for integrating heterogeneous multimedia sources. In many applications, the information required by the user cannot be found in just one source, but has to be retrieved from many varying sources. This is true not only of formatted data in database management systems, but also of textual documents and multimedia data, such as images and videos. We propose a mediator system that provides the end-user with a single query interface to an integrated view of multiple heterogeneous data sources. We exploit the capabilities of the MOMIS integration system and the MILOS multimedia data management system. Each multimedia source is managed by an instance of MILOS, in which a collection of multimedia records is made accessible by means of similarity searches employing the query-by-example paradigm. MOMIS provides an integrated virtual view of the underlying multimedia sources, thus offering unified multimedia access services. Two features are that MILOS is flexible--it is not tied to any particular similarity function--and the MOMIS's mediator query processor only exploits the ranks of the local answers.\"",
        "Document: \"Using Apache Lucene to Search Vector of Locally Aggregated Descriptors. Surrogate Text Representation (STR) is a profitable solution to efficient similarity search on metric space using conventional text search engines, such as Apache Lucene. This technique is based on comparing the permutations of some reference objects in place of the original metric distance. However, the Achilles heel of STR approach is the need to reorder the result set of the search according to the metric distance. This forces to use a support database to store the original objects, which requires efficient random I/O on a fast secondary memory (such as flash-based storages). In this paper, we propose to extend the Surrogate Text Representation to specifically address a class of visual metric objects known as Vector of Locally Aggregated Descriptors (VLAD). This approach is based on representing the individual sub-vectors forming the VLAD vector with the STR, providing a finer representation of the vector and enabling us to get rid of the reordering phase. The experiments on a publicly available dataset show that the extended STR outperforms the baseline STR achieving satisfactory performance near to the one obtained with the original VLAD vectors.\"",
        "1 is \"Mobile agents point the WAY: context sensitive service delivery through mobile lightweight agents\", 2 is \"Leader election algorithms for mobile ad hoc networks\"",
        "Given above information, for an author who has written the paper with the title \"Robotic UBIquitous COgnitive Network.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008031": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Automated Testing of Software that Uses Machine Learning APIs':",
        "Document: \"Study on 2-tuple linguistic assessment method based on grey cluster with incomplete attribute weight information. Grey cluster decision-making is an important part of grey system theory. And 2-tuple linguistic assessment resolves the calculation problems of the result of expert qualitative linguistic assessment. But both of them not mention how to calculate the weight of index. Based on the definitions of entropy, a method of getting weight is proposed, and considers the character of grey cluster decision-making and 2-tuple linguistic assessment, this paper proposed the method of 2-tuple linguistic assessment based on grey cluster. It can combine qualitative analysis and quantitative analysis well.\"",
        "Document: \"Some results about grey mathematics. Purpose - The purpose of this paper is to explain the connotation of grey number, which is the basic unit of grey mathematics and the key to establish the mathematic framework of grey system theory. Design/methodology/approach - From the grey hazy set, the paper re-defines grey number and the operation of grey-number element, then some properties are obtained. Based on them, the operation of grey-matrix element is given. The general definition of grey function and its operation are also proposed. Findings - The connotation of grey number is elaborated and the elementary framework of grey mathematics can be established. Research limitations/implications - The researched object, objective and techniques of grey system theory have not been logically proposed and they may influence the comprehension of grey system theory. The obtained results of grey mathematics may significantly promote its development. Practical implications - The paper can enable managers to control the complex system with missing information by using the quantitative approaches. Originality/value - Grey mathematics may become a branch of mathematics to deal with proximate calculation.\"",
        "Document: \"On algorithm rules of interval grey numbers based on the \u201cKernel\u201d and the degree of greyness of grey numbers. Algorithm of grey numbers and grey algebraic system is the theoretical basis of grey system theory. It is possessed of very important significance in the developing course of grey system theory. A definition for grey \u201cKernel\u201d has been put forward first. The algorithm axiom, algorithm rules of interval grey numbers and a new algebraic system is built based on the \u201cKernel\u201d and the degree of greyness of grey numbers. And the properties of the algorithm are studied. Up to now, the algorithm of interval grey numbers have been transformed to algorithm of real numbers. So, the difficult problem for set up algorithm of interval grey numbers have been solved to a certain degree. The algorithm of interval grey numbers could be extended to the case of algorithm of grey algebraic equation, grey differential equation and grey matrix which including interval grey numbers. The algorithm system of interval grey numbers also opened a new passageway for research on grey input-output and grey programming, etc.\"",
        "Document: \"Predicting the research output/growth of selected countries: application of Even GM (1, 1) and NDGM models. The study aims to forecast the research output of four selected countries (USA, China, India and Pakistan) using two models of Grey System Theory\u2014Even Model GM (1, 1) and Nonhomogeneous Discrete Grey Model (NDGM). The study also conducts publication growth analysis using relative growth rate (RGR) and the doubling time (D t). The linear and exponential regression analyses were also performed for comparison. The study also proposes and successfully tests two novel synthetic models for RGR and D t that facilities the comparison of the countries\u2019 performance when actual data and forecasted data produce different sequences of performance in the given period of time. The data of documents published by the four countries from 2005 to 2016 was collected from SJR/Scopus website. Performance criterion was Mean Absolute Percentage Error. The study confirms that NDGM is a better model for forecasting research output as its accuracy level is higher than that of the Even Model GM (1, 1) and statistical regression models. The results revealed that USA is likely to continue leading in research output at least till 2025 however the research output difference between USA and China is likely to reduce. The study reveals that the less developed countries tend to possess higher relative growth rate in publications whereas the more developed countries tend to possess lower relative growth rate. Further, the more developed countries need more time for publications to double in numbers for a given relative growth rate and less developed countries need less time to do so. The study is original in term of its analysis of the problem using the models involved in the study. The study suggests that the strategies of USA and China to enhance the research output of their respective countries seem productive for the time being however in long run less developed countries have greater competitive advantage over the more developed countries because of their publication growth rate and time required to double the number of publications. The study reported nearly linear trend of growth in research output among the countries. The study is primarily important for the academic policy makers and encourages them to take corrective measures if the growth rate of their academic/publishing sector is not reasonable.\"",
        "Document: \"A stochastic network model for ordering analysis in multi-stage supply chain systems. In this paper we consider the ordering problem in demand driven multi-stage supply chain system. Although ordering decisions are made independently at different supply chain levels which involve manufactures, wholesalers, retailers and customers, the economic performance of ordering policies of supply chain members are heavily interdependent due to the supply\u2013demand relationship among them. Based on the graphical evaluation and review technique, we first present a novel stochastic network mathematical model for order and cost distribution analysis. Further, to investigate the effects of different sub-orders on total system order and the impact of ordering cost on supply chain economic performance, sensitivity analysis models for order fluctuations and ordering cost changes are developed respectively. Finally, a detailed simulation is presented to promote a better understanding of the model approach. The models and algorithms are stylized; however, they are applicable to complex supply chains. The strategy parameters in simulation are presented for various ordering policy settings. This leads to a decision support system for the multi-stage supply chain under various ordering policy scenarios, and will be helpful in identifying the key links in ordering networks and for designing effective strategies to improve supply chain performance.\"",
        "Document: \"Modeling and simulation of stranded passengers' transferring decision-making on the basis of herd behaviors. Purpose - The purpose of this paper is to reveal the pattern of passengers' transferring on occasion of a large crowd being stranded at transportation hubs (such as a bus station, railway station, airport, etc.) in climate disasters, and then propose the proper policy recommendations for the government to evacuate stranded passengers. Design/methodology/approach - A model is established based on Bayesian network and influence diagram to catch the features of a passenger's decision-making process, and the transition probabilities of passengers are revised on the basis of the theory of herd behaviors in information to describe the influence of group behaviors on passenger individuals. Subsequently, a multi-agent model is developed in Repast platform in Java language, and simulation and analysis are also made. Findings - The results of simulation show that it is possible to apply the theory of herd behaviors and the multi-agent method in analyzing the effectiveness of government policies on evacuating stranded passengers in climate disasters. Originality/value - The research of this paper has important practical significance for the government to developing policies to evacuating stranded passengers in climate disasters, and is a useful exploration to open up new methodologies for emergency management.\"",
        "Document: \"Grey system model with the fractional order accumulation. The perturbation theory of least squares method is applied to explain why the traditional accumulated generating operator violates the principle of new information priority of Grey system theory. A new Grey system model with the fractional order accumulation is put forward and the priority of new information can be better reflected when the accumulation order number becomes smaller in the in-sample model. But Grey system model cannot deal with the systems with memory when the accumulation order number is 0 in the in-sample model. The results of practical numerical examples demonstrate that the new Grey model provides very remarkable predication performance compared with the traditional Grey model for small sample. \u00a9 2012 Elsevier B.V.\"",
        "Document: \"A note on the sequence of weakening buffer operator. Based on the present theories of operators, this paper give a new result on the weakening buffer operator. f is a monotonously increasing function, g is its inverse function. If d is a weakening buffer operator, and x(k)d is made of x(k),....,x(n),k = 1,2,...n. If we substitute x(k) by f(x(k)),k = 1,...,n. The new result is m=[f(x(k))]d. If g(m) =e, then e is also a weakening buffer operator.\"",
        "Document: \"A Brief Introduction to Grey Systems Theory. The scientific background that grey systems theory comes into being, the astonishing progress that grey systems theory has made in the world of learning and its wide-ranging applications in the entire spectrum of science, and the characteristics of unascertained systems include incomplete information and inaccuracies in data are presented in this paper. The scientific principle of simplicity and how precise models suffer from inaccuracies are also shown. We compared grey systems with other kinds of uncertainty models such as stochastic probability, rough set theory, and fuzzy mathematics. Finally, the elementary concepts and fundamental principles of grey systems, and main components of grey systems theory are introduced briefly. \u00a9 2011 IEEE.\"",
        "Document: \"A novel interval grey prediction model considering uncertain information. Current studies on grey systems are mainly focused on known and deterministic information, rather than uncertain one. Different from previous schemes, this paper proposes an innovative prediction model based on grey number information, which extends its application dealing with uncertain information. By exploiting the geometric features of grey numbers on a two-dimensional surface, all grey numbers can be converted into real numbers without losing any information by means of proposed algorithms. Then a prediction model is established based on those real number sequences. In addition, a general case simulation is carried out to verify the effectiveness and practicability of the proposed model.\"",
        "1 is \"A clustering algorithm using an evolutionary programming-based approach\", 2 is \"Predicting stock price using fuzzy grey prediction system\"",
        "Given above information, for an author who has written the paper with the title \"Automated Testing of Software that Uses Machine Learning APIs\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008118": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A Polynomial Cutting Surfaces Algorithm for the Convex Feasibility Problem Defined by Self-Concordant Inequalities':",
        "Document: \"Some Properties of the Augmented Lagrangian in Cone Constrained Optimization. A large class of optimization problems can be modeled as minimization of an objective function subject to constraints given in a form of set inclusions. In this paper, we discuss augmented Lagrangian duality for such optimization problems. We formulate the augmented Lagrangian dual problems and study conditions ensuring existence of the corresponding augmented Lagrange multipliers. We also discuss sensitivity of optimal solutions to small perturbations of augmented Lagrange multipliers.\"",
        "Document: \"A Multiple-Cut Analytic Center Cutting Plane Method for Semidefinite Feasibility Problems. We consider the problem of finding a point in a nonempty bounded convex body $\\Gamma$ in the cone of symmetric positive semidefinite matrices ${\\cal S}^m_+$. Assume that $\\Gamma$ is defined by a separating oracle, which, for any given $m\\ti m$ symmetric matrix $\\hat{Y}$, either confirms that $\\hat Y \\in \\Gamma$ or returns several selected cuts, i.e., a number of symmetric matrices Ai, i=1,. . .,p, p\\le p_{\\max}$, such that $\\Gamma$ is in the polyhedron $ \\{ Y \\in {\\cal S}^m_+ \\mid A_i \\bullet Y \\le A_i \\bullet \\hat{Y}, i=1,\\ldots,p \\}.$ We present a multiple-cut analytic center cutting plane algorithm. Starting from a trivial initial point, the algorithm generates a sequence of positive definite matrices which are approximate analytic centers of a shrinking polytope in ${\\cal S}^m_+$. The algorithm terminates with a point in $\\Gamma$ within $O(m^3p_{\\max}/\\epsilon^2)$ Newton steps (to leading order), where $\\epsilon$ is the maximum radius of a ball contained in $\\Gamma$.\"",
        "Document: \"Distributionally robust \\(L_1\\)-estimation in multiple linear regression. Linear regression is one of the most important and widely used techniques in data analysis, for which a key step is the estimation of the unknown parameters. However, it is often carried out under the assumption that the full information of the error distribution is available. This is clearly unrealistic in practice. In this paper, we propose a distributionally robust formulation of \\(L_1\\)-estimation (or the least absolute value estimation) problem, where the only knowledge on the error distribution is that it belongs to a well-defined ambiguity set. We then reformulate the estimation problem as a computationally tractable conic optimization problem by using duality theory. Finally, a numerical example is solved as a conic optimization problem to demonstrate the effectiveness of the proposed approach.\"",
        "Document: \"Establishing Nash equilibrium of the manufacturer\u2013supplier game in supply chain management. We study a game model of multi-leader and one-follower in supply chain optimization where n suppliers compete to provide a single product for a manufacturer. We regard the selling price of each supplier as a pre-determined parameter and consider the case that suppliers compete on the basis of delivery frequency to the manufacturer. Each supplier's profit depends not only on its own delivery frequency, but also on other suppliers' frequencies through their impact on manufacturer's purchase allocation to the suppliers. We first solve the follower's (manufacturer's) purchase allocation problem by deducing an explicit formula of its solution. We then formulate the n leaders' (suppliers') game as a generalized Nash game with shared constraints, which is theoretically difficult, but in our case could be solved numerically by converting to a regular variational inequality problem. For the special case that the selling prices of all suppliers are identical, we provide a sufficient and necessary condition for the existence and uniqueness of the Nash equilibrium. An explicit formula of the Nash equilibrium is obtained and its local uniqueness property is proved.\"",
        "Document: \"Applying a Newton Method to Strictly Convex Separable Network Quadratic Programs. By introducing quadratic penalty terms, a strictly convex separable network quadratic program can be reduced to an unconstrained optimization problem whose objective is a continuously differentiable piecewise quadratic function. A recently developed nonsmooth version of Newton's method is applied to the reduced problem. The generalized Newton direction is computed by an iterative procedure which exploits the special network data structures that originated from the network simplex method. New features of the algorithm include the use of min-max bases and a dynamic strategy in computation of the Newton directions. Some preliminary computational results are presented. The results suggest the use of \"warm start\" instead of \"cold start.\"\"",
        "Document: \"Successive convex approximations to cardinality-constrained convex programs: a piecewise-linear DC approach. In this paper we consider cardinality-constrained convex programs that minimize a convex function subject to a cardinality constraint and other linear constraints. This class of problems has found many applications, including portfolio selection, subset selection and compressed sensing. We propose a successive convex approximation method for this class of problems in which the cardinality function is first approximated by a piecewise linear DC function (difference of two convex functions) and a sequence of convex subproblems is then constructed by successively linearizing the concave terms of the DC function. Under some mild assumptions, we establish that any accumulation point of the sequence generated by the method is a KKT point of the DC approximation problem. We show that the basic algorithm can be refined by adding strengthening cuts in the subproblems. Finally, we report some preliminary computational results on cardinality-constrained portfolio selection problems.\"",
        "Document: \"The toll effect on price of anarchy when costs are nonlinear and asymmetric. We examine the efficiency of the optimal tolls by establishing the bound for the price of anarchy when the levied tolls are also considered as a part of the cost functions. For linear and nonlinear asymmetric cost functions, we prove that the price of anarchy of the system with tolls is lower than that without tolls. Furthermore, we show that the total disutility caused to the users by the tolls is bounded by a multiple of the original optimal system cost.\"",
        "Document: \"Quadratic cost flow and the conjugate gradient method. By introducing quadratic penalty terms, a convex non-separable quadratic network program can be reduced to an unconstrained optimization problem whose objective function is a piecewise quadratic and continuously differentiable function. A conjugate gradient method is applied to the reduced problem and its convergence is proved. The computation exploits the special network data structures originated from the network simplex method. This algorithmic framework allows direct extension to multicommodity cost flows. Some preliminary computational results are presented.\"",
        "Document: \"An Analytic Center Cutting Plane Method for Semidefinite Feasibility Problems. Semidefinite feasibility problems arise in many areas of operations research. The abstract form of these problems can be described as finding a point in a nonempty bounded convex body G in the cone of symmetric positive semidefinite matrices. Assume that G is defined by an oracle, which for any givenm xm symmetric positive semidefinite matrix Y either confirms that Y e G or returns a cut, i.e., a symmetric matrixA such that G is in the half-space { Y :A \u8117\u00b7Y = A \u8117\u00b7 Y}. We study an analytic center cutting plane algorithm for this problem. At each iteration, the algorithm computes an approximate analytic center of a working set defined by the cutting plane system generated in the previous iterations. If this approximate analytic center is a solution, then the algorithm terminates; otherwise the new cutting plane returned by the oracle is added into the system. As the number of iterations increases, the working set shrinks and the algorithm eventually finds a solution to the problem. All iterates generated by the algorithm are positive definite matrices. The algorithm has a worst-case complexity of O *( m3 /e 2 )on the total number of cuts to be used, where e is the maximum radius of a ball contained by G.\"",
        "Document: \"Two-stage stochastic linear programs with incomplete information on uncertainty. \u2022The model only uses the first and second moments information of random variables.\u2022We show that the new model is indeed a second-order cone optimization problem.\u2022Preliminary numerical experiments indicate the computational advantage of this model.\"",
        "1 is \"Joint optimal power control and beamforming in wireless networks using antenna arrays\", 2 is \"Limiting subgradients of minimal time functions in Banach spaces\"",
        "Given above information, for an author who has written the paper with the title \"A Polynomial Cutting Surfaces Algorithm for the Convex Feasibility Problem Defined by Self-Concordant Inequalities\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008165": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Trends in augmented reality tracking, interaction and display: A review of ten years of ISMAR':",
        "Document: \"Sociality of Mobile Collaborative AR: Augmenting a Dual-Problem Space for Social Interaction in Collaborative Social Learning. Mobile collaborative augmented reality (AR) technology demonstrates great potential for augmenting learning experience in collaboration. The convergence of AR, collaboration and handheld devices opens up more opportunities for strengthening the effectiveness of social interaction in collaborative learning. We conceptualize it as a new form of collaborative learning, collaborative AR social learning. Based on the reflections of the sociality characteristics of mobile collaborative AR, we develop a model for better understanding this emerging instructional medium. It has implications for the development of mobile collaborative AR applications for educational purposes in the future.\"",
        "Document: \"Subtle cueing for visual search in head-tracked head worn displays. Goal-oriented visual search in augmented reality can be facilitated by using visual cues to call attention to a target. However, traditional use of explicit cues can degrade visual search performance due to scene distortion, occlusion and addition of visual clutter. In contrast, Subtle Cueing has been previously proposed as an alter-native to explicit cueing, but little is known about how well it works for head-tracked head worn displays (HWDs). We investigated the effect of Subtle Cueing for head-tracked head worn displays, using visual search research methods in simulated augmented reality environments. Our user study found that Subtle Cueing improves visual search performance, and serves as a feasible cueing mechanism for AR environments using HWDs.\"",
        "Document: \"A Simulation of Bonding Effects and Their Impacts on Pedestrian Dynamics. This paper simulates bonding effects inside pedestrian crowds. Based on the social force model, this paper derives an exponential formulation of the bonding force, as opposed to the repulsive force, and surveys the degree of interpersonal cohesion under various circumstances. Parameters associated with the model are calibrated by preliminary simulation runs. With the proper simulation environment configuration, the effect of the bonding force is extensively demonstrated. Results show that the bonding force results in pedestrians' walking speeds being different from their initial intended ones. Specifically, delays in walking and the overtaking phenomenon, which are empirically observed, are explained using this model. In the zigzag walkway defined in the experiment, up to approximately 4% fewer pedestrians are able to escape in the prescribed time, due to bonding effects. To sum up, the bonding forces cause negative effects on pedestrian evacuation and should be taken into consideration for crowd dynamics research.\"",
        "Document: \"Confucius Computer: bridging intergenerational communication through illogical and cultural computing. Confucius Computer is a new form of illogical cultural computing based on the Eastern paradigms of balance and harmony. The system uses new media to revive and model ancient Eastern and Confucius philosophies and teachings, presenting them in new contexts, such as online social chat, music and food. Based on the model of Eastern mind and teaching, the system enables users to have meaningful social network communication with a virtual Confucius. The Confucius Computer system offers a new artistic playground for interactive music-painting creation based on our Confucius music filters and the ancient model of Cycles of Balance. Confucius Computer also allows users to explore the traditional Chinese medicine concept of Yin-Yang through interactive recipe creation. Detailed descriptions of the systems are presented in this paper. Our user studies showed that users gave positive feedbacks to their experience of interacting with Confucius Computer. They believed that this media could improve intergenerational interaction and promote a sense of calmness.\"",
        "Document: \"Spatial Representation Of A Virtual Room Space: Perspective And Vertical Movement. Evidence from prior research has demonstrated that exocentric views of the environment can facilitate the acquisition of survey knowledge in a virtual environment. The present study examined the effect of different exocentric views on judging the relative direction of objects. During the participants' vertical movement in a virtual room, participants learned the spatial layout in one of three conditions: two-perspective, attentive-elevation, and normal-elevation conditions, where the number of the exocentric perspectives from which the spatial layout was observed was different. After spatial learning, they made the judgment of the relative direction of objects. The analysis of spatial judgment showed that as the number of exocentric perspectives increased, the accuracy improved in the mental representation of spatial vertical information and spatial information in novel directions. Results indicated that the increased number of exocentric perspectives during the vertical movement facilitated the flexible acquisition of survey knowledge. Applications of this study included the design of effective navigation aids in virtual multilevel buildings.\"",
        "Document: \"Spatial Navigation in a Virtual Multilevel Building: The Role of Exocentric View in Acquiring Survey Knowledge. The present study aimed to test the function of the exocentric view on the acquisition of survey knowledge during spatial navigation in a virtual multilevel building. Subjects navigated a virtual three-level building in three conditions. In the first condition, subjects navigated the building without any aid. In the second condition, subjects navigated the building with the aid of a three-dimensional (3D) floor map which illustrated the spatial layout on each level from one exocentric perspective. In the third condition, subjects could watch the spatial layout on each level from the exocentric perspective when traveling to another level by an elevator. After navigation, all subjects made the judgment of spatial relative direction. The analyses of the accuracy of spatial judgments showed that the accuracy of judgment of spatial horizontal direction was significantly improved when subjects observed the exocentric views of levels in the last two conditions; the judgment of spatial vertical direction was significantly worse in the 3D floor map condition than in other two conditions. Furthermore, the accuracy of judgment of both spatial horizontal and vertical directions was best in the direction faced by subjects when they first enter each level. The results suggested that the content of exocentric view should be carefully designed to improve the acquisition of survey knowledge. The application of the findings included the design of 3D map for the navigation in the virtual multilevel building.\"",
        "Document: \"Dynamic three-dimensional information visualization for quantitative information in augmented reality systems. Quantitative information visualization such as statistical graphics is concemed with the visual representation of quantitative and categorical data for statistical analysis. With improvements in graphics display technology, it is now possible to make use of motion to stimulate recognition of patterns and structure embedded in quantitative data. Past studies have shown that the judged final position of a moving target is often displaced in the direction of the anticipated future motion of the target. Termed as representational momentum, these memory distortions have a strong relationship with the target's velocity. This study investigated human performance in visualizing dynamic quantitative information in augmented reality environments. Statistical results showed that the differences in speed and percentage change affect subject's accuracy in perceiving quantitative information significantly. On the other hand, the differences in display devices (Head-mounted-display and Liquid-crystal-display) did not indicate significant effects on subject's performance. Our results also showed that as the speed increases, the errors made in judging the final position of the moving bar also increases.\"",
        "Document: \"Social life logging: can we describe our own personal experience by using collective intelligence?. A famous Gestalt psychologist Kurt Koffka left a statement \\\"The whole is other than the sum of its parts.\\\" Similarly, collective intelligence such as social tagging exposes a social milieu that cannot be obtained from the descriptions of each individual. Previous automatic (or passive) life logging projects mainly focused on recording the individual life activity however, sometimes it is difficult to recollect the situation from their own perspective logs alone. In this project, we propose a social life logging system called \\\"KiokuHacker\\\" (Kioku means memory in Japanese) that encourages the user to describe their life activity by using a massive amount of processed geotagged social tagging from the Internet. The result of a one year user test not only shows that our social life logging system encourages the user's reminiscence which the user cannot recollect by oneself but also indicates that the user evokes their reminiscence which is not directly related with to the tags/scenes the system displayed.\"",
        "Document: \"An independent visual background reduced simulator sickness in a driving simulator. Simulator sickness (SS)-virtual environment (VE) sickness is expected to become increasingly troublesome as VE technology evolves. This paper investigated using an independent visual background (IVB) to reduce SS and VE sickness. The IVB is a visual scene component that provides visual motion and orientation cues that match those from the vestibular receptors. In this study, the IVB was stationary, fixed with respect to inertial space. Two experiments were conducted. The first experiment examined the differences in visual motion-induced postural disturbance as a function of simultaneous exposure to an IVB. Subjects exhibited less balance disturbance when the IVB was presented. An expected statistically significant interaction between IVB presence absence and visual scene motion oscillation frequency was observed. In the second experiment, subjects reported less SS when the IVB was presented during the VE exposure. We suggest that an IVB may alleviate disturbance when conflicting visual and inertial cues evoke SS.\"",
        "Document: \"An \u201cindependent visual background\u201d reduced balance disturbance envoked by visual scene motion: implication for alleviating simulator sickness. Simulator sickness (SS) / virtual environment (VE) sickness is expected to become increasingly troublesome as VE technology evolves [20]. Procedures to alleviate SS / VE sickness have been of limited value [12]. This paper investigated a possible procedure to reduce SS and VE sickness. Postural disturbance was evoked by visual scene motion at different frequencies. Differences in disturbance were examined as a function of simultaneous exposure to an \u201cindependent visual background\u201d (IVB). Eight subjects were tested at two scene motion frequencies and three different IVB conditions using a within-subjects design. An expected statistically significant interaction between IVB condition and frequency was observed. For low frequency scene movements, subjects exhibited less balance disturbance when the IVB was presented. We suggest that an IVB may alleviate disturbance when conflicting visual and inertial cues are likely to result in simulator or VE sickness.\"",
        "1 is \"A remote Chinese chess game using mobile phone augmented reality\", 2 is \"Active Appearance Models\"",
        "Given above information, for an author who has written the paper with the title \"Trends in augmented reality tracking, interaction and display: A review of ten years of ISMAR\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008305": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Codes in the Damerau Distance for Deletion and Adjacent Transposition Correction.':",
        "Document: \"Information Theoretic Bounds for Tensor Rank Minimization over Finite Fields. We consider the problem of noiseless and noisy low- rank tensor completion from a set of random linear measurements. In our derivations, we assume that the entries of the tensor belong to a finite field of arbitrary size and that reconstruction is based on a rank minimization framework. The derived results show that the smallest number of measurements needed for exact reconstruction is upper bounded by the product of the rank, the order, and the dimension of a cubic tensor. Furthermore, this condition is also sufficient for unique minimization. Similar bounds hold for the noisy rank minimization scenario, except for a scaling function that depends on the channel error probability.\"",
        "Document: \"Doubly threshold graphs for social network modeling. Threshold graphs are recursive deterministic network models that capture properties of certain social and economic interactions. One drawback of these graph families is that they they have limited constrained generative attachment rules. To mitigate this problem, we introduce a new class of graphs termed Doubly Threshold (DT) graphs which may be succinctly described through vertex weights that govern the existence of edges via two inequalities. One inequality imposes the constraint that the sum of weights of adjacent vertices has to exceed a specified threshold. The second inequality ensures that adjacent vertices have a bounded difference of their weights. We provide a succinct characterization and decomposition of DT graphs and analyze their forbidden induced subgraphs which we compare to those of known social networks. We also present a method for performing vertex weight assignments on DT graphs that satisfy the defining constraints.\"",
        "Document: \"Correlation Clustering with Constrained Cluster Sizes and Extended Weights Bounds. We consider the problem of correlation clustering on graphs with constraints on both the cluster sizes and the positive and negative weights of edges. Our contributions are twofold: first, we introduce the problem of correlation clustering with bounded cluster sizes. Second, we extend the regime of weight values for which the clustering may be performed with constant approximation guarantees in polynomial time and apply the results to the bounded cluster size problem.\"",
        "Document: \"Poisson Group Testing: A Probabilistic Model for Boolean Compressed Sensing. We introduce a novel probabilistic group testing framework, termed Poisson group testing, in which the number of defectives follows a right-truncated Poisson distribution. The Poisson model has a number of new applications, including dynamic testing with diminishing relative rates of defectives. We consider both nonadaptive and semi-adaptive identification methods. For nonadaptive methods, we derive a lower bound on the number of tests required to identify the defectives with a probability of error that asymptotically converges to zero; in addition, we propose test matrix constructions for which the number of tests closely matches the lower bound. For semi-adaptive methods, we describe a lower bound on the expected number of tests required to identify the defectives with zero error probability. In addition, we propose a stage-wise reconstruction algorithm for which the expected number of tests is only a constant factor away from the lower bound. The methods rely only on an estimate of the average number of defectives, rather than on the individual probabilities of subjects being defective.\"",
        "Document: \"Weighted Superimposed Codes and Constrained Integer Compressed Sensing. We introduce a new family of codes, termed weighted superimposed codes (WSCs). This family generalizes the class of Euclidean superimposed codes (ESCs), used in multiuser identification systems. WSCs allow for discriminating all bounded, integer-valued linear combinations of real-valued codewords that satisfy prescribed norm and nonnegativity constraints. By design, WSCs are inherently noise tolerant. Therefore, these codes can be seen as special instances of robust compressed sensing schemes. The main results of the paper are lower and upper bounds on the largest achievable code rates of several classes of WSCs. These bounds suggest that, with the codeword and weighting vector constraints at hand, one can improve the code rates achievable by standard compressive sensing techniques.\"",
        "Document: \"The trapping redundancy of linear block codes. We generalize the notion of the stopping redundancy in order to study the smallest size of a trapping set in Tanner graphs of linear block codes. In this context, we introduce the notion of the trapping redundancy of a code, which quantifies the relationship between the number of redundant rows in any parity-check matrix of a given code and the size of its smallest trapping set. Trapping sets with certain parameter sizes are known to cause error-floors in the performance curves of iterative belief propagation (BP) decoders, and it is therefore important to identify decoding matrices that avoid such sets. Bounds on the trapping redundancy are obtained using probabilistic and constructive methods, and the analysis covers both general and elementary trapping sets. Numerical values for these bounds are computed for the [2640,1320] Margulis code and the class of projective geometry codes, and compared with some new code-specific trapping set size estimates.\"",
        "Document: \"Synchronization and Deduplication in Coded Distributed Storage Networks. We consider the problem of synchronizing coded data in distributed storage networks undergoing insertion and deletion edits. We present modifications of distributed storage codes that allow updates in the parity-check values to be performed with one round of communication at low bit rates and with small storage overhead. Our main contributions are novel protocols for synchronizing frequently updated and semi-static data based on functional intermediary coding involving permutation and Vandermonde matrices.\"",
        "Document: \"Similarity distances between permutations. We address the problem of computing distances between rankings that take into account similarities between elements. The need for evaluating such distances arises in applications such as machine learning, social sciences and data storage. The problem may be summarized as follows: Given two rankings and a positive cost function on transpositions that depends on the similarity of the elements involved, find a smallest cost sequence of transpositions that converts one ranking into another. Our focus is on costs that may be described via special tree structures and on rankings modeled as permutations. The presented results include a quadratic-time algorithm for finding a minimum cost transform for a single cycle; and a linear time, 5/3-approximation algorithm for permutations that contain multiple cycles.\"",
        "Document: \"Permutation (d,k) codes: efficient enumerative coding and phrase length distribution shaping. We introduce a new enumerative encoding method for (d,k) codes. The encoding algorithm, which is based on enumeration of multiset permutations, is conceptually simpler and computationally less expensive than other algorithms proposed thus far. We also describe a new application of enumerative encoding methods for phrase length distribution shaping of run-length-limited (RLL) sequences. We demonstrate that by reducing the probability of occurrence of long phrases in maxentropic RLL sequences, the frequency of patterns that account for most of the errors in magnetic recording systems can be decreased\"",
        "Document: \"Permutation Decoding and the Stopping Redundancy Hierarchy of Cyclic and Extended Cyclic Codes. We introduce the notion of the stopping redundancy hierarchy of a linear block code as a measure of the tradeoff between performance and complexity of iterative decoding for the binary erasure channel. We derive lower and upper bounds for the stopping redundancy hierarchy via Lovasz's local lemma (LLL) and Bonferroni-type inequalities, and specialize them for codes with cyclic parity-check matrices. Based on the observed properties of parity-check matrices with good stopping redundancy characteristics, we develop a novel decoding technique, termed automorphism group decoding, that combines iterative message passing and permutation decoding. We also present bounds on the smallest number of permutations of an automorphism group decoder needed to correct any set of erasures up to a prescribed size. Simulation results demonstrate that for a large number of algebraic codes, the performance of the new decoding method is close to that of maximum-likelihood (ML) decoding.\"",
        "1 is \"The number of transversals in a Latin square\", 2 is \"Dequantizing Compressed Sensing: When Oversampling and Non-Gaussian Constraints Combine\"",
        "Given above information, for an author who has written the paper with the title \"Codes in the Damerau Distance for Deletion and Adjacent Transposition Correction.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008312": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'On-off frequency-shift keying for wideband fading channels':",
        "Document: \"Characterization of optimal input distributions for Gaussian-mixture noise channels. This paper addresses the characterization of optimal input distributions for the general additive quadrature Gaussian-mixture (GM) noise channel under an average power constraint. The considered model can be used to represent a wide variety of communication channels, such as the well-known Bernoulli-Gaussian and Middleton Class-A impulsive noise channels, co-channel interference in cellular communications, and cognitive radio channels under imperfect spectrum sensing. We first demonstrate that there exists a unique input distribution achieving the channel capacity and the optimal input has an uniformly distributed phase. By using the Kuhn-Tucker conditions (KTC) and Bernstein's theorem, we then demonstrate that there are always a finite number of mass points on any bounded interval in the optimal amplitude distribution. Equivalently, the optimal amplitude input distribution is discrete. Furthermore, by applying a novel bounding technique on the KTC, it is then shown that the optimal amplitude distribution has a finite number of mass points.\"",
        "Document: \"Joint Mode Selection and Resource Allocation for D2D Communications via Vertex Coloring. Device-to-device (D2D) communication underlaid with cellular networks is a new paradigm, proposed to enhance the performance of cellular networks. By allowing a pair of D2D users to communicate directly and share the same spectral resources with the cellular users, D2D communication can achieve higher spectral efficiency, improve the energy efficiency, and lower the traffic delay. In this paper, we propose a novel joint mode selection and channel resource allocation algorithm via the vertex coloring approach. We decompose the problem into three subproblems and design algorithms for each of them. In the first step, we divide the users into groups using a vertex coloring algorithm. In the second step, we solve the power optimization problem using the interior-point method for each group and conduct mode selection between the cellular mode and D2D mode for D2D users, and we assign channel resources to these groups in the final step. Numerical results show that our algorithm achieves higher sum rate and serves more users with relatively small time consumption compared with other algorithms. Also, the influence of system parameters and the tradeoff between sum rate and the number of served users are studied through simulation results.\"",
        "Document: \"Optimal Power Allocation for Amplify and Forward Relaying with Finite Blocklength Codes and QoS Constraints. In this work, motivated by emerging low-latency applications, we consider an amplify-and-forward relaying network operating with finite blocklength (FBL) codes subject to delay quality of service (QoS) constraints. Hence, we address both transmission delay (via FBL codes) and queueing delay (via delay QoS requirements). We first derive the QoS-constrained throughput of the network. Subsequently, we state a resource allocation problem aiming at allocating the power between the source and the relay to maximize the throughput. The convexity of the problem is proved and the optimal power allocation policy is provided. Via simulations, we confirm the accurateness of our analytical model. In addition, we provide interesting insights on the system behavior by characterizing the impact of the error probability, the QoS-exponent and coding blocklength on the throughput performance.\"",
        "Document: \"Energy-efficient resource allocation for SWIPT in multiple access channels. In this paper, we study optimal resource allocation strategies for simultaneous information and power transfer (SWIPT) focusing on the system energy efficiency. We consider two-user multiple access channels in which energy harvesting (EH) and information decoding (ID) nodes are spatially separated. We formulate optimization problems that maximize system energy efficiency while taking harvested energy constraints into account. These are concave-linear fractional problems, and hence Karush-Kuhn-Tucker (KKT) conditions are necessary and sufficient to obtain globally optimal solution. Solving these optimization problems, we provide analytical expressions for optimal transmit power allocation among the source nodes, and identify the corresponding energy efficiency. We confirm the theoretical analysis via numerical results. Furthermore, we also characterize the effect of circuit power consumption on the system's efficiency as the harvested energy demand varies.\"",
        "Document: \"Secure Transmission of Delay-Sensitive Data over Wireless Fading Channels. In this paper, throughput and energy efficiency of secure wireless transmission of delay sensitive data generated by random sources is studied. A fading broadcast model in which the transmitter sends confidential and common messages to two receivers is considered. It is assumed that the common and confidential data, generated from Markovian sources, are stored in buffers prior to transmission, and...\"",
        "Document: \"QoS-driven power control for fading channels with arbitrary input distributions. In this paper, the optimal power control policy that maximizes the effective capacity for arbitrary input distributions in fading channels subject to an average power constraint is studied. A low-complexity power control algorithm is proposed. In addition, energy efficiency is investigated by characterizing both the minimum energy per bit and wideband slope for arbitrary signaling in the low-power regime when channel side information (CSI) is available only at the receiver. With perfect CSI at both the transmitter and receiver, the optimal power adaptation strategy in this regime is also determined. Through numerical results, performance comparison with constant power scheme and optimal power adaptation strategy for different signal constellations and Gaussian signals is given. The impact of QoS constraints, input distributions, and average transmit power level on the proposed power control policy, maximum achievable effective capacity and energy efficiency is analyzed.\"",
        "Document: \"Analysis of Energy Efficiency in Fading Channels under QoS Constraints. Energy efficiency in fading channels in the presence of QoS constraints is studied. Effective capacity, which provides the maximum constant arrival rate that a given process can support while satisfying statistical delay constraints, is considered. Spectral efficiency-bit energy tradeoff is analyzed in the low-power and wideband regimes by employing the effective capacity formulation, rather than the Shannon capacity, and energy requirements under QoS constraints are identified. The analysis is conducted for the case in which perfect channel side information (CSI) is available at the receiver and also for the case in which perfect CSI is available at both the receiver and transmitter. In particular, it is shown in the low-power regime that the minimum bit energy required in the presence of QoS constraints is the same as that attained when there are no such limitations. However, this performance is achieved as the transmitted power vanishes. Through the wideband slope analysis, the increased energy requirements at low but nonzero power levels are determined. A similar analysis is also conducted in the wideband regime, and minimum bit energy and wideband slope expressions are obtained. In this regime, the required bit energy levels are found to be strictly greater than those achieved when Shannon capacity is considered. Overall, an energy-delay tradeoff is characterized.\"",
        "Document: \"On the performance limits of cognitive MIMO channels. In this paper, throughput of cognitive multiple-input multiple-output (MIMO) systems operating under quality-of-service (QoS) constraints is studied. It is assumed that transmission power and the covariance of the input signal vector are varied depending on the sensed activities of primary users in the system. Considering the reliability of the transmission and channel sensing results, a state-transition model is provided. Effective capacity is determined, and expressions for the first and second derivatives of the effective capacity are obtained at SNR =0. The minimum bit energy requirements in the presence of QoS limitations are identified. Numerical results are provided.\"",
        "Document: \"Learning-Based Delay-Aware Caching in Wireless D2D Caching Networks. Recently, wireless caching techniques have been studied to satisfy lower delay requirements and offload traffic from peak periods. By storing parts of the popular files at the mobile users, users can locate some of their requested files in their own caches or the caches at their neighbors. In the latter case, when a user receives files from its neighbors, device-to-device (D2D) communication is performed. The D2D communication underlaid with cellular networks is also a new paradigm for the upcoming wireless systems. By allowing a pair of adjacent D2D users to communicate directly, D2D communication can achieve higher throughput, better energy efficiency, and lower traffic delay. In this paper, we propose an efficient learning-based caching algorithm operating together with a non-parametric estimator to minimize the average transmission delay in D2D-enabled cellular networks. It is assumed that the system does not have any prior information regarding the popularity of the files, and the non-parametric estimator is aimed at learning the intensity function of the file requests. An algorithm is devised to determine the best 'files,user' pairs that provide the best delay improvement in each loop to form a caching policy with very low-transmission delay and high throughput. This algorithm is also extended to address a more general scenario, in which the distributions of fading coefficients and the values of system parameters potentially change over time. Via numerical results, the superiority of the proposed algorithm is verified by comparing it with a naive algorithm, in which all users simply cache their favorite files, and by comparing with a probabilistic algorithm, the users cache a file with a probability that is proportional to its popularity.\"",
        "Document: \"Error Probability Analysis of Peaky Signaling over Fading Channels. In this paper, the performance of signaling strategies with high peak-to-average power ratio is analyzed in both coherent and noncoherent fading channels. Two recently proposed modulation schemes, namely on-off binary phase-shift keying and on-off quaternary phase-shift keying, are considered. For these modulation formats, the optimal decision rules used at the detector are identified and analytical expressions for t he error probabilities are obtained. Numerical techniques are employed to compute the error probabilities. It is concluded that increasing the peakedness of the signals results in reduced error rates for a given power level and hence improve the energy efficiency. I. I NTRODUCTION In wireless communications, when the receiver and trans- mitter have only imperfect knowledge of the channel condi- tions, efficient transmission strategies have a markedly di f- ferent structure than those employed over perfectly known channels. For instance, Abou-Faycal et al. (1) studied the noncoherent Rayleigh fading channel where the receiver and transmitter has no channel side information, and showed that the capacity-achieving input amplitude distribution is discrete with a finite number of mass points. It has also been shown that there always exists a mass point at the origin. Another key result for noncoherent channels is the requirement of transmission with high peak-to-average power ratio in the low signal-to-noise ratio (SNR) regime (2). In ( 3), two types of signaling schemes are defined: on-off binary-sh ift keying (OOBPSK) and on-off quaternary phase-shift keying (OOQPSK). These modulations are obtained by overlaying on-off keying on phase-shift keying. The peakedness of these signals are controlled by changing the probability of no transmission. OOQPSK is shown to be an optimally efficient modulation scheme for transmission over noncoherent Rician fading channels in the low-SNR regime. Motivated by the above-mentioned results, we study in this paper the error performance of using signals with high peak-to-average power ratios (PAR) over both coherent and noncoherent fading channels.\"",
        "1 is \"Capacity of fading channels with channel side information\", 2 is \"Opportunities and Challenges of Wireless Sensor Networks in Smart Grid\"",
        "Given above information, for an author who has written the paper with the title \"On-off frequency-shift keying for wideband fading channels\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008438": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Rejection Strategies for Learning Vector Quantization - A Comparison of Probabilistic and Deterministic Approaches.':",
        "Document: \"Optimal local rejection for classifiers. We analyse optimal rejection strategies for classifiers with input space partitioning, e.g. prototype-based classifiers, support vector machines or decision trees using certainty measures such as the distance to the closest decision border. We provide new theoretical results: we link this problem to the multiple choice knapsack problem and devise an exact polynomial-time dynamic programming (DP) scheme to determine optimal local thresholds on given data. Further, we propose a linear time, memory efficient approximation thereof. We show in experiments that the approximation has a competitive performance compared to the full DP. Further, we evaluate the performance of classification with rejection in various benchmarks: we conclude that local rejection is beneficial in particular for simple classifiers, while the improvement is less pronounced for advanced classifiers. An evaluation on speech prosody and biomedical data highlights the benefit of local thresholds.\"",
        "Document: \"Online figure-ground segmentation with adaptive metrics in generalized LVQ. We address the problem of fast figure-ground segmentation of single objects from cluttered backgrounds to improve object learning and recognition. For the segmentation, we use an initial foreground hypothesis to train a classifier for figure and ground on topographically ordered feature maps with generalized learning vector quantization. We investigate the contribution of several adaptive metrics to enable generalization to the main object parts and derive a foreground classification, which yields an improved bottom-up hypothesis. We show that metrics adaptation is a powerful enrichment, where generalizing the Euclidean metrics towards local matrices of relevance factors leads to a higher classification accuracy and considerable robustness on partially inconsistent supervised information. Additionally, we verify our results in an online learning scenario and show that figure-ground segregation using this adaptive metrics enables a considerably higher recognition performance on segmented object views.\"",
        "Document: \"Combining Offline And Online Classifiers For Life-Long Learning. One of the greatest challenges of life-long learning architectures is how to efficiently and reliably cope with the stability-plasticity dilemma. We propose an extension of a flexible system combining a static offline classifier and an incremental online classifier that is well suited for life-long learning scenarios. The pre-trained offline classifier preserves ground knowledge that should be respected during training, while the online classifier enables learning of new or specific information encountered during use. The combination is realised by a dynamic classifier selection strategy based on confidences of both ingredients. We report exemplary results of this architecture for the case of learning vector quantization (LVQ) for several data sets, thereby including an extensive comparison to alternative state of the art algorithms for incremental learning such as incremental generalised LVQ and the support vector machine.\"",
        "Document: \"Peripersonal space and object recognition for humanoids. This work is concerned with a framework for visual object recognition in real world tasks. Our approach is motivated by biological findings of the representation of space around the body, the so-called peripersonal space. We show that the principles behind those findings can lead to a natural structuring of object recognition tasks in artificial systems. We demonstrate this by the supervised learning and recognition of 20 complex-shaped objects from unsegmented visual input\"",
        "Document: \"A neural network architecture for automatic segmentation of fluorescence micrographs. A system for the automatic segmentation of fluorescence micrographs is presented. In the first step, positions of fluorescent cells are detected by a fast learning neural network, which acquires the visual knowledge from a set of training cell-image patches selected by the user. Guided by the detected cell positions the system extracts in the second step the contours of the cells. For contour extraction, a recurrent neural network model is used to approximate the cell shapes. Even though the micrographs are noisy and the fluorescent cells vary in shape and size, the system detects at minimum 95% of the cells.\"",
        "Document: \"A Computational Feature Binding Model of Human Texture Perception. We present a computational model for human texture perception which assigns functional principles to the Gestalt laws of similarity and proximity. Motivated by early vision mechanisms, in a first step local texture features are extracted by utilizing multi-scale filtering and non-linear spatial pooling. In the second stage, features are grouped according to the spatial feature binding model of the Competitive Layer Model (CLM) (Wersing, Steil, & Ritter, 2001). The CLM uses cooperative and competitive interactions in a recurrent network, where binding is expressed by the layer-wise coactivation of feature-representing neurons. The Gestalt law of similarity is expressed by a non-euclidean distance measure in the abstract feature space with proximity being taken into account by a spatial component. To choose the stimulus dimensions which allow the most salient similarity-based texture segmentation, the feature similarity metrics is reduced to the directions of maximum variance. We show that our combined texture feature extraction and binding model performs segmentation in strong conformity with human perception. The examples range from classical microtextures and Brodatz textures to other classical Gestalt stimuli, which offer a new perspective on the role of texture for more abstract similarity grouping.\"",
        "Document: \"Class-Specific sparse coding for learning of object representations. We present two new methods which extend the traditional sparse coding approach with supervised components. The goal of these extensions is to increase the suitability of the learned features for classification tasks while keeping most of their general representation performance. A special visualization is introduced which allows to show the principal effect of the new methods. Furthermore some first experimental results are obtained for the COIL-100 database.\"",
        "Document: \"Unsupervised Learning of Combination Features for Hierarchical Recognition Models. We propose a cortically inspired hierarchical feedforward model for recognition and investigate a new method for learning optimal combination-coding cells in intermediate stages of the hierarchical network. The model architecture is characterized by weight-sharing, pooling, and Winner-Take-All nonlinearities. We show that an unsupervised sparse coding learning rule can be used to obtain a recognition architecture that is competitive with other more formally abstracted recognition approaches based on supervised learning. We evaluate the performance on object and face databases.\"",
        "Document: \"Feature binding and relaxation labeling with the competitive layer model. We discuss the relation of the Competitive Layer Model (CLM) to Relaxation Labeling (RL) with regard to feature binding and labeling problems. The CLM uses cooperative and competitive interactions to partition a set of input features into groups by energy minimization. As we show, the stable attractors of the CLM provide consistent and unambiguous labelings in the sense of RL and we give an efficient stochastic simulation procedure for their identification. In addition to binding the CLM exhibits contextual activity modulation to rep- resent stimulus salience. We incorporate deterministic annealing for avoidance of local minima and show how figure-ground segmentation and grouping can be combined for the CLM application of contour grouping on a real image.\"",
        "Document: \"Unsupervised Extraction Of Design Components For A 3d Parts-Based Representation. During CAD development and any kind of design optimisation over years a huge amount of geometries accumulate in a design department. To organize and structure these designs with respect to reusability, a hierarchical set of components on different scalings is extracted by the designers. This hierarchy allows to compose designs from several parts and to adapt the composition to the current task. Nevertheless, this hierarchy is imposed by humans and relies on their experiences. In the present paper a computational method is proposed for an unsupervised extraction of design components from a large repository of geometries. Methods known from the field of object and pattern recognition in images are transferred to the 3D design space to detect relevant features of geometries. The non-negative matrix factorization algorithm (NMF) is extended and tuned to the given task for an autonomous detection of design components. The results of the NMF additionally provide an overview on the distribution of these components in the design repository. The extracted components sum up in a parts-based representation which serves as a base for manual or computational design development or optimisation respectively.\"",
        "1 is \"Face recognition: a convolutional neural-network approach.\", 2 is \"Alignment of metabolic pathways\"",
        "Given above information, for an author who has written the paper with the title \"Rejection Strategies for Learning Vector Quantization - A Comparison of Probabilistic and Deterministic Approaches.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008463": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Improving language modeling by using distance and co-occurrence information of word-pairs and its application to LVCSR':",
        "Document: \"Guidelines for Word Alignment Evaluation and Manual Alignment. The purpose of this paper is to provide guidelines for building a word alignment evaluation scheme. The notion of word alignment quality depends on the application: here we review standard scoring metrics for full text alignment and give explanations on how to use them better. We discuss strategies to build a reference corpus, and show that the ratio between ambiguous and unambiguous links in the reference has a great impact on scores measured with these metrics. In particular, automatically computed alignments with higher precision or higher recall can be favoured depending on the value of this ratio. Finally, we suggest a strategy to build a reference corpus particularly adapted to applications where recall plays a significant role, like in machine translation. The manually aligned corpus we built for the Spanish-English European Parliament corpus is also described. This corpus is freely available.\"",
        "Document: \"Using annotations on Mechanical Turk to perform supervised polarity classification of Spanish customer comments. One of the major bottlenecks in the development of data-driven AI Systems is the cost of reliable human annotations. The recent advent of several crowdsourcing platforms such as Amazon\u2019s Mechanical Turk, allowing requesters the access to affordable and rapid results of a global workforce, greatly facilitates the creation of massive training data. Most of the available studies on the effectiveness of crowdsourcing report on English data. We use Mechanical Turk annotations to train an Opinion Mining System to classify Spanish consumer comments. We design three different Human Intelligence Task (HIT) strategies and report high inter-annotator agreement between non-experts and expert annotators. We evaluate the advantages/drawbacks of each HIT design and show that, in our case, the use of non-expert annotations is a viable and cost-effective alternative to expert annotations.\"",
        "Document: \"Tuning Machine Translation Parameters with SPSA. Most of statistical machine translation systems are combi- nations of various models, and tuning of the scaling fac- tors is an important step. However, this optimisation prob- lem is hard because the objective function has many local minima and the available algorithms cannot achieve a global optimum. Consequently, optimisations starting from differ- ent initial settings can converge to fairly different solutions. We present tuning experiments with the Simultaneous Per- turbation Stochastic Approximation (SPSA) algorithm, and compare them to tuning with the widely used downhill sim- plex method. With IWSLT 2006 Chinese-English data, both methods showed similar performance, but SPSA was more robust to the choice of initial settings.\"",
        "Document: \"Barcelona Media SMT system description for the IWSLT 2009. This paper describes the Barcelona Media SMT system in the IWSLT 2009 evaluation campaign. The Barcelona Media system is an statis- tical phrase-based system enriched with source context information. Adding source context in an SMT system is interesting to enhance the translation in order to solve lexical and structural choice errors. The novel technique uses a similarity metric among each test sentence and each training sentence. First experimental results of this technique are reported in the Arabic and Chinese Basic Traveling Expression Corpus (BTEC) task. Although working in a single domain, there are ambi- guities in SMT translation units and slight improvements in BLEU are shown in both tasks (Zh2En and Ar2En).\"",
        "Document: \"Automatic evaluation of end-to-end dialog systems with adequacy-fluency metrics. \u2022Automatic metric for evaluating natural language generated sentences for dialog systems.\u2022Integration of adequacy and fluency information to jointly evaluate semantic and syntactic information.\u2022Higher correlation (up to 12.8%) with human evaluations than the best current objective metrics at a system and sentence level.\"",
        "Document: \"An IR-Based Strategy for Supporting Chinese-Portuguese Translation Services in Off-line Mode. This paper describes an Information Retrieval engine that is used to support our Chinese-Portuguese machine translation services when no internet connection is available. Our mobile translation app, which is deployed on a portable device, relies by default on a server-based machine translation service, which is not accessible when no internet connection is available. For providing translation support under this condition, we have developed a contextualized off-line search engine that allows the users to continue using the app.\"",
        "Document: \"IRIS: a chat-oriented dialogue system based on the vector space model. This system demonstration paper presents IRIS (Informal Response Interactive System), a chat-oriented dialogue system based on the vector space model framework. The system belongs to the class of example-based dialogue systems and builds its chat capabilities on a dual search strategy over a large collection of dialogue samples. Additional strategies allowing for system adaptation and learning implemented over the same vector model space framework are also described and discussed.\"",
        "Document: \"Squeezing bottlenecks: Exploring the limits of autoencoder semantic representation capabilities. We present a comprehensive study on the use of autoencoders for modelling text data, in which (differently from previous studies) we focus our attention on the various issues. We explore the suitability of two different models binary deep autencoders (bDA) and replicated-softmax deep autencoders (rsDA) for constructing deep autoencoders for text data at the sentence level. We propose and evaluate two novel metrics for better assessing the text-reconstruction capabilities of autoencoders. We propose an automatic method to find the critical bottleneck dimensionality for text representations (below which structural information is lost); and finally we conduct a comparative evaluation across different languages, exploring the regions of critical bottleneck dimensionality and its relationship to language perplexity.\"",
        "Document: \"Continuous space models for CLIR. We present and evaluate a novel technique for learning cross-lingual continuous space models to aid cross-language information retrieval (CLIR). Our model, which is referred to as external-data composition neural network (XCNN), is based on a composition function that is implemented on top of a deep neural network that provides a distributed learning framework. Different from most existing models, which rely only on available parallel data for training, our learning framework provides a natural way to exploit monolingual data and its associated relevance metadata for learning continuous space representations of language. Cross-language extensions of the obtained models can then be trained by using a small set of parallel data. This property is very helpful for resource-poor languages, therefore, we carry out experiments on the English-Hindi language pair. On the conducted comparative evaluation, the proposed model is shown to outperform state-of-the-art continuous space models with statistically significant margin on two different tasks: parallel sentence retrieval and ad-hoc retrieval.\"",
        "Document: \"The structure of political discussion networks: a model for the analysis of online deliberation. This paper shows that online political discussion networks are, on average, wider and deeper than the networks generated by other types of discussions: they engage a larger number of participants and cascade through more levels of nested comments. Using data collected from the Slashdot forum, this paper reconstructs the discussion threads as hierarchical networks and proposes a model for their comparison and classification. In addition to the substantive topic of discussion, which corresponds to the different sections of the forum (such as Developers, Games, or Politics), we classify the threads according to structural features like the maximum number of comments at any level of the network (i.e. the width) and the number of nested layers in the network (i.e. the depth). We find that political discussion networks display a tendency to cluster around the area that corresponds to wider and deeper structures, showing a significant departure from the structure exhibited by other types of discussions. We propose using this model to create a framework that allows the analysis and comparison of different internet technologies for the promotion of political deliberation.\"",
        "1 is \"Low-Resource Keyword Search Strategies For Tamil\", 2 is \"Subjectivity and sentiment analysis: An overview of the current state of the area and envisaged developments\"",
        "Given above information, for an author who has written the paper with the title \"Improving language modeling by using distance and co-occurrence information of word-pairs and its application to LVCSR\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008464": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Integrating local classifiers through nonlinear dynamics on label graphs with an application to image segmentation':",
        "Document: \"Fast planar correlation clustering for image segmentation. We describe a new optimization scheme for finding high-quality clusterings in planar graphs that uses weighted perfect matching as a subroutine. Our method provides lower-bounds on the energy of the optimal correlation clustering that are typically fast to compute and tight in practice. We demonstrate our algorithm on the problem of image segmentation where this approach outperforms existing global optimization techniques in minimizing the objective and is competitive with the state of the art in producing high-quality segmentations.\"",
        "Document: \"Pixel-wise Attentional Gating for Parsimonious Pixel Labeling. To achieve parsimonious inference  per-pixel labeling tasks with a limited computational budget, we propose a emph{Pixel-wise Attentional Gating} unit (emph{PAG}) that learns to selectively process a subset of spatial locations at each layer of a deep convolutional network. PAG is a generic, architecture-independent, problem-agnostic mechanism that can be readily plugged in to an existing model with fine-tuning.  utilize PAG  two ways: 1) learning spatially varying pooling fields that improve model performance without the extra computation cost associated with multi-scale pooling, and 2) learning a dynamic computation policy for each pixel to decrease total computation while maintaining accuracy. We extensively evaluate PAG on a variety of per-pixel labeling tasks, including semantic segmentation, boundary detection, monocular depth and surface normal estimation.  demonstrate that PAG allows competitive or state-of-the-art performance on these tasks. Our experiments show that PAG learns dynamic spatial allocation of computation over the input image which provides better performance trade-offs compared to related approaches (e.g., truncating deep models or dynamically skipping whole layers). Generally, we observe PAG can reduce computation by $10%$ without noticeable loss  accuracy and performance degrades gracefully when imposing stronger computational constraints.\"",
        "Document: \"Scale-invariant contour completion using conditional random fields. We present a model of curvilinear grouping using piece-wise linear representations of contours and a conditional random field to capture continuity and the frequency of different junction types. Potential completions are generated by building a constrained Delaunay triangulation (CDT) over the set of contours found by a local edge detector. Maximum likelihood parameters for the model are learned from human labeled ground truth. Using held out test data, we measure how the model, by incorporating continuity structure, improves boundary detection over the local edge detector. We also compare performance with a baseline local classifier that operates on pairs of edgels. Both algorithms consistently dominate the low-level boundary detector at all thresholds. To our knowledge, this is the first time that curvilinear continuity has been shown quantitatively useful for a large variety of natural images. Better boundary detection has immediate application in the problem of object detection and recognition\"",
        "Document: \"Cue Integration for Figure/Ground Labeling. We present a model of edge and region grouping using a conditional random eld built over a scale-invariant representation of images to inte- grate multiple cues. Our model includes potentials that capture low-level similarity, mid-level curvilinear continuity and high-level object shape. Maximum likelihood parameters for the model are learned from human labeled groundtruth on a large collection of horse images using belief propagation. Using held out test data, we quantify the information gained by incorporating generic mid-level cues and high-level shape.\"",
        "Document: \"Covering trees and lower-bounds on quadratic assignment. Many computer vision problems involving feature corre- spondence among images can be formulated as an assign- ment problem with a quadratic cost function. Such problems are computationally infeasible in general but recent ad- vances in discrete optimization such as tree-reweighted be- lief propagation (TRW) often provide high-quality solutions. In this paper, we improve upon these algorithms in two ways. First, we introduce covering trees, a variant of TRW which provide the same bounds on the MAP energy as TRW with far fewer variational parameters. Optimization of these parameters can be carried out efficiently using either fixed-point iterations (as in TRW) or sub-gradient based techniques. Second, we introduce a new technique that uti- lizes bipartite matching applied to the min-marginals pro- duced with covering trees in order to compute a tighter lower-bound for the quadratic assignment problem. We apply this machinery to the problem of finding correspon- dences with pairwise energy functions, and demonstrate the resulting hybrid method outperforms TRW alone and a recent related subproblem decomposition algorithm on benchmark image correspondence problems.\"",
        "Document: \"Sparse Representations for Object and Ego-motion Estimation in Dynamic Scenes. Disentangling the sources of visual motion in a dynamic scene during self-movement or ego motion is important for autonomous navigation and tracking. In the dynamic image segments of a video frame containing independently moving objects, optic flow relative to the next frame is the sum of the motion fields generated due to camera and object motion. The traditional ego-motion estimation methods ass...\"",
        "Document: \"Learning Optimal Parameters for Multi-target Tracking with Contextual Interactions. We describe an end-to-end framework for learning parameters of min-cost flow multi-target tracking problem with quadratic trajectory interactions including suppression of overlapping tracks and contextual cues about co-occurrence of different objects. Our approach utilizes structured prediction with a tracking-specific loss function to learn the complete set of model parameters. In this learning framework, we evaluate two different approaches to finding an optimal set of tracks under a quadratic model objective, one based on an linear program (LP) relaxation and the other based on novel greedy variants of dynamic programming that handle pairwise interactions. We find the greedy algorithms achieve almost equivalent accuracy to the LP relaxation while being up to 10$$\\\\times $$\u00d7 faster than a commercial LP solver. We evaluate trained models on three challenging benchmarks. Surprisingly, we find that with proper parameter learning, our simple data association model without explicit appearance/motion reasoning is able to achieve comparable or better accuracy than many state-of-the-art methods that use far more complex motion features or appearance affinity metric learning.\"",
        "Document: \"Learning to Detect Natural Image Boundaries Using Brightness and Texture. The goal of this work is to accurately detect and localize boundaries in natural scenes using local image measurements. We formulate features that respond to characteristic changes in brightness and texture associated with natural boundaries. In order to combine the information from these features in an optimal way, a classifier is trained using human labeled images as ground truth. We present precision-recall curves showing that the resulting detector outperforms existing approaches.\"",
        "Document: \"Contour detection and hierarchical image segmentation. This paper investigates two fundamental problems in computer vision: contour detection and image segmentation. We present state-of-the-art algorithms for both of these tasks. Our contour detector combines multiple local cues into a globalization framework based on spectral clustering. Our segmentation algorithm consists of generic machinery for transforming the output of any contour detector into a hierarchical region tree. In this manner, we reduce the problem of image segmentation to that of contour detection. Extensive experimental evaluation demonstrates that both our contour detection and segmentation methods significantly outperform competing algorithms. The automatically generated hierarchical segmentations can be interactively refined by user-specified annotations. Computation at multiple image resolutions provides a means of coupling our system to recognition applications.\"",
        "Document: \"Tightening MRF Relaxations with Planar Subproblems.   We describe a new technique for computing lower-bounds on the minimum energy configuration of a planar Markov Random Field (MRF). Our method successively adds large numbers of constraints and enforces consistency over binary projections of the original problem state space. These constraints are represented in terms of subproblems in a dual-decomposition framework that is optimized using subgradient techniques. The complete set of constraints we consider enforces cycle consistency over the original graph. In practice we find that the method converges quickly on most problems with the addition of a few subproblems and outperforms existing methods for some interesting classes of hard potentials. \"",
        "1 is \"Multiscale representations of Markov random fields\", 2 is \"More Effective Distributed ML via a Stale Synchronous Parallel Parameter Server.\"",
        "Given above information, for an author who has written the paper with the title \"Integrating local classifiers through nonlinear dynamics on label graphs with an application to image segmentation\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008500": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Inference of development activities from interaction with uninstrumented applications.':",
        "Document: \"Build Predictor: More Accurate Missed Dependency Prediction in Build Configuration Files. Software build system (e.g., Make) plays an important role in compiling human-readable source code into an executable program. One feature of build system such as make-based system is that it would use a build configuration file (e.g., Make file) to record the dependencies among different target and source code files. However, sometimes important dependencies would be missed in a build configuration file, which would cause additional debugging effort to fix it. In this paper, we propose a novel algorithm named Build Predictor to mine the missed dependncies. We first analyze dependencies in a build configuration file (e.g., Make file), and establish a dependency graph which captures various dependencies in the build configuration file. Next, considering that a build configuration file is constructed based on the source code dependency relationship, we establish a code dependency graph (code graph). Build Predictor is a composite model, which combines both dependency graph and code graph, to achieve a high prediction performance. We collected 7 build configuration files from various open source projects, which are Zlib, putty, vim, Apache Portable Runtime (APR), memcached, nginx, and Tengine, to evaluate the effectiveness of our algorithm. The experiment results show that compared with the state-of-the-art link prediction algorithms used by Xia et al., our Build Predictor achieves the best performance in predicting the missed dependencies.\"",
        "Document: \"Predicting Crashing Releases of Mobile Applications. Context: The quality of mobile applications has a vital impact on their user's experience, ratings and ultimately overall success. Given the high competition in the mobile application market, i.e., many mobile applications perform the same or similar functionality, users of mobile apps tend to be less tolerant to quality issues. Goal: Therefore, identifying these crashing releases early on so that they can be avoided will help mobile app developers keep their user base and ensure the overall success of their apps. Method: To help mobile developers, we use machine learning techniques to effectively predict mobile app releases that are more likely to cause crashes, i.e., crashing releases. To perform our prediction, we mine and use a number of factors about the mobile releases, that are grouped into six unique dimensions: complexity, time, code, diffusion, commit, and text, and use a Naive Bayes classified to perform our prediction. Results: We perform an empirical study on 10 open source mobile applications containing a total of 2,638 releases from the F-Droid repository. On average, our approach can achieve F1 and AUC scores that improve over a baseline (random) predictor by 50% and 28%, respectively. We also find that factors related to text extracted from the commit logs prior to a release are the best predictors of crashing releases and have the largest effect. Conclusions: Our proposed approach could help to identify crash releases for mobile apps.\"",
        "Document: \"Customer satisfaction feedback in an IT outsourcing company: a case study on the insigma Hengtian company. To reduce budget and improve competitive power, some companies would outsource their information technology (IT) functions to a third-party company referred to as an IT outsourcing company. After an outsourcing company completes a project, it would collect feedback from the customer. Analyzing this feedback could help to further improve the service of the outsourcing company. To our best knowledge, there are limited studies on customer satisfaction feedback. In this paper, we perform a case study of customer satisfaction feedback in an IT outsourcing company. We analyze 3 years of customer satisfaction feedback reports in Insigma Hengtian, which is the one of the well-known IT outsourcing companies in China with more than 2,000 employees. Each report specifies the overall satisfaction of a customer, and feedback on factors that contribute to customer satisfaction including: business knowledge and technical skills, work attitude, communication skills, on-time delivery, cost-effectiveness, etc. In total, we investigate 8 factors which are related to customer satisfaction. Next, we build a logistic regression model and analyze the statistical significance and collinearity characteristics of the independent factors used to build the model. We find that among the 8 factors, business knowledge and technical skills, on-time delivery, and cost-effectiveness are the 3 most important factors, and all of them have positive impact to customer satisfaction.\"",
        "Document: \"Characterizing and identifying reverted commits. In practice, a popular and coarse-grained approach for recovering from a problematic commit is to revert it (i.e., undoing the change). However, reverted commits could induce some issues for software development, such as impeding the development progress and increasing the difficulty for maintenance. In order to mitigate these issues, we set out to explore the following central question: can we characterize and identify which commits will be reverted? In this paper, we characterize commits using 27 commit features and build an identification model to identify commits that will be reverted. We first identify reverted commits by analyzing commit messages and comparing the changed content, and extract 27 commit features that can be divided into three dimensions, namely change, developer and message, respectively. Then, we build an identification model (e.g., random forest) based on the extracted features. To evaluate the effectiveness of our proposed model, we perform an empirical study on ten open source projects including a total of 125,241 commits. Our experimental results show that our model outperforms two baselines in terms of AUC-ROC and cost-effectiveness (i.e., percentage of detected reverted commits when inspecting 20% of total changed LOC). In terms of the average performance across the ten studied projects, our model achieves an AUC-ROC of 0.756 and a cost-effectiveness of 0.746, significantly improving the baselines by substantial margins. In addition, we found that \u201cdeveloper\u201d is the most discriminative dimension among the three dimensions of features for the identification of reverted commits. However, using all the three dimensions of commit features leads to better performance.\"",
        "Document: \"Revisiting supervised and unsupervised models for effort-aware just-in-time defect prediction. Effort-aware just-in-time (JIT) defect prediction aims at finding more defective software changes with limited code inspection cost. Traditionally, supervised models have been used; however, they require sufficient labelled training data, which is difficult to obtain, especially for new projects. Recently, Yang et al. proposed an unsupervised model (i.e., LT) and applied it to projects with rich historical bug data. Interestingly, they reported that, under the same inspection cost (i.e., 20 percent of the total lines of code modified by all changes), it could find about 12% - 27% more defective changes than a state-of-the-art supervised model (i.e., EALR) when using different evaluation settings. This is surprising as supervised models that benefit from historical data are expected to perform better than unsupervised ones. Their finding suggests that previous studies on defect prediction had made a simple problem too complex. Considering the potential high impact of Yang et al.\u2019s work, in this paper, we perform a replication study and present the following new findings: (1) Under the same inspection budget, LT requires developers to inspect a large number of changes necessitating many more context switches. (2) Although LT finds more defective changes, many highly ranked changes are false alarms. These initial false alarms may negatively impact practitioners\u2019 patience and confidence. (3) LT does not outperform EALR when the harmonic mean of Recall and Precision (i.e., F1-score) is considered. Aside from highlighting the above findings, we propose a simple but improved supervised model called CBS+, which leverages the idea of both EALR and LT. We investigate the performance of CBS+ using three different evaluation settings, including time-wise cross-validation, 10-times 10-fold cross-validation and cross-project validation. When compared with EALR, CBS+ detects about 15% - 26% more defective changes, while keeping the number of context switches and initial false alarms close to those of EALR. When compared with LT, the number of defective changes detected by CBS+ is comparable to LT\u2019s result, while CBS+ significantly reduces context switches and initial false alarms before first success. Finally, we discuss how to balance the tradeoff between the number of inspected defects and context switches, and present the implications of our findings for practitioners and researchers.\"",
        "Document: \"Automatic, high accuracy prediction of reopened bugs. Bug fixing is one of the most time-consuming and costly activities of the software development life cycle. In general, bugs are reported in a bug tracking system, validated by a triage team, assigned for someone to fix, and finally verified and closed. However, in some cases bugs have to be reopened. Reopened bugs increase software maintenance cost, cause rework for already busy developers and in some cases even delay the future delivery of a software release. Therefore, a few recent studies focused on studying reopened bugs. However, these prior studies did not achieve high performance (in terms of precision and recall), required manual intervention, and used very simplistic techniques when dealing with this textual data, which leads us to believe that further improvements are possible. In this paper, we propose ReopenPredictor, which is an automatic, high accuracy predictor of reopened bugs. ReopenPredictor uses a number of features, including textual features, to achieve high accuracy prediction of reopened bugs. As part of ReopenPredictor, we propose two algorithms that are used to automatically estimate various thresholds to maximize the prediction performance. To examine the benefits of ReopenPredictor, we perform experiments on three large open source projects--namely Eclipse, Apache HTTP and OpenOffice. Our results show that ReopenPredictor outperforms prior work, achieving a reopened F-measure of 0.744, 0.770, and 0.860 for Eclipse, Apache HTTP and OpenOffice, respectively. These results correspond to an improvement in the reopened F-measure of the method proposed in the prior work by Shihab et al. by 33.33, 12.57 and 3.12 % for Eclipse, Apache HTTP and OpenOffice, respectively.\"",
        "Document: \"Identifying self-admitted technical debt in open source projects using text mining. Technical debt is a metaphor to describe the situation in which long-term code quality is traded for short-term goals in software projects. Recently, the concept of self-admitted technical debt (SATD) was proposed, which considers debt that is intentionally introduced, e.g., in the form of quick or temporary fixes. Prior work on SATD has shown that source code comments can be used to successfully detect SATD, however, most current state-of-the-art classification approaches of SATD rely on manual inspection of the source code comments. In this paper, we proposed an automated approach to detect SATD in source code comments using text mining. In our approach, we utilize feature selection to select useful features for classifier training, and we combine multiple classifiers from different source projects to build a composite classifier that identifies SATD comments in a target project. We investigate the performance of our approach on 8 open source projects that contain 212,413 comments. Our experimental results show that, on every target project, our approach outperforms the state-of-the-art and the baselines approaches in terms of F1-score. The F1-score achieved by our approach ranges between 0.518 - 0.841, with an average of 0.737, which improves over the state-of-the-art approach proposed by Potdar and Shihab by 499.19%. When compared with the text mining-based baseline approaches, our approach significantly improves the average F1-score by at least 58.49%. When compared with a natural language processing-based baseline, our approach also significantly improves its F1-score by 27.95%. Our proposed approach can be used by project personnel to effectively identify SATD with minimal manual effort.\"",
        "Document: \"An Empirical Study of Classifier Combination for Cross-Project Defect Prediction. To help developers better allocate testing and debugging efforts, many software defect prediction techniques have been proposed in the literature. These techniques can be used to predict classes that are more likely to be buggy based on past history of buggy classes. These techniques work well as long as a sufficient amount of data is available to train a prediction model. However, there is rarely enough training data for new software projects. To deal with this problem, cross-project defect prediction, which transfers a prediction model trained using data from one project to another, has been proposed and is regarded as a new challenge for defect prediction. So far, only a few cross-project defect prediction techniques have been proposed. To advance the state-of-the-art, in this work, we investigate 7 composite algorithms, which integrate multiple machine learning classifiers, to improve cross-project defect prediction. To evaluate the performance of the composite algorithms, we perform experiments on 10 open source software systems from the PROMISE repository which contain a total of 5,305 instances labeled as defective or clean. We compare the composite algorithms with CODEP Logistic, which is the latest cross-project defect prediction algorithm proposed by Panichella et al., in terms of two standard evaluation metrics: cost effectiveness and F-measure. Our experiment results show that several algorithms outperform CODEP Logistic: Max performs the best in terms of F-measure and its average F-measure outperforms that of CODEP Logistic by 36.88%. Bagging J48 performs the best in terms of cost effectiveness and its average cost effectiveness outperforms that of CODEP Logistic by 15.34%.\"",
        "Document: \"Cross-Project Change-Proneness Prediction. Software change-proneness prediction (whether or not class files in a project will be changed in the next release) can help software developers to focus on preventive actions to reduce maintenance costs, and managers to allocate resources more effectively. Prior studies found that change-proneness prediction works well if there is sufficient amount of training data to build a model. However, it is not feasible for projects with limited historical data especially for new projects. To address this issue, cross-project change-proneness prediction, which builds a prediction model by using data in another project (i.e., source project), and predicts the change-proneness in a target project, is proposed. Considering there are a large number of source projects, one challenge for cross-project change-proneness prediction is that given a target project, how to automatically select a source project which could show good prediction accuracy on it. In this paper, we propose a selective cross-project (SCP) model for change-proneness prediction. SCP automatically finds the source project which has the similar data distribution with the target project by measuring distribution similarity between source and target projects. We evaluate SCP by conducting an empirical study on 14 open source projects. We compare it with 2 most related change-proneness models, including RCP (Random Cross-Project prediction) proposed by Malhotra and Bansal, and CLAMI+ developed by Yan et al. Experiment results show that SCP improves RCP and CLAMI+ by 25.34% and 4.30% in terms of AUC respectively; and by 171.42% and 172.31% in terms of cost-effectiveness, respectively.\"",
        "Document: \"Combined classifier for cross-project defect prediction: an extended empirical study. To facilitate developers in effective allocation of their testing and debugging efforts, many software defect prediction techniques have been proposed in the literature. These techniques can be used to predict classes that are more likely to be buggy based on the past history of classes, methods, or certain other code elements. These techniques are effective provided that a sufficient amount of data is available to train a prediction model. However, sufficient training data are rarely available for new software projects. To resolve this problem, cross-project defect prediction, which transfers a prediction model trained using data from one project to another, was proposed and is regarded as a new challenge in the area of defect prediction. Thus far, only a few cross-project defect prediction techniques have been proposed. To advance the state of the art, in this study, we investigated seven composite algorithms that integrate multiple machine learning classifiers to improve cross-project defect prediction. To evaluate the performance of the composite algorithms, we performed experiments on 10 open-source software systems from the PROMISE repository, which contain a total of 5,305 instances labeled as defective or clean. We compared the composite algorithms with the combined defect predictor where logistic regression is used as the meta classification algorithm (CODEP Logistic ), which is the most recent cross-project defect prediction algorithm in terms of two standard evaluation metrics: cost effectiveness and F-measure. Our experimental results show that several algorithms outperform CODEP Logistic : Maximum voting shows the best performance in terms of F-measure and its average F-measure is superior to that of CODEP Logistic  by 36.88%. Bootstrap aggregation (BaggingJ48) shows the best performance in terms of cost effectiveness and its average cost effectiveness is superior to that of CODEP Logistic  by 15.34%.\"",
        "1 is \"User interactions in social networks and their implications\", 2 is \"Empirical evaluation of the tarantula automatic fault-localization technique\"",
        "Given above information, for an author who has written the paper with the title \"Inference of development activities from interaction with uninstrumented applications.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008531": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'SignTutor: An Interactive System for Sign Language Tutoring':",
        "Document: \"Elliptical local binary patterns for face recognition. In this paper, we propose a novel variant of Local Binary Patterns (LBP) so-called Elliptical Local Binary Patterns (ELBP) for face recognition. In ELBP, we use horizontal and vertical ellipse patterns to capture micro facial feature of face images in both horizontal and vertical directions. ELBP is applied in face recognition with dimension reduction step by Whitened Principal Component Analysis (WPCA). Our experiment results upon AR, FERET and Surveillance Cameras Face (SCface) databases prove the advantages of ELBP over LBP for face recognition under different conditions and with ELBP WPCA we can get very remarkable results.\"",
        "Document: \"SignTutor: An Interactive System for Sign Language Tutoring. Language learning can only advance with practice and corrective feedback. The interactive system, SignTutor, evaluates users' signing and gives multimodal feedback to help improve signing.\"",
        "Document: \"Patch based local phase quantization of monogenic components for face recognition. In this paper, we propose a novel feature extraction method for Face recognition called patch based Local Phase Quantization of Monogenic components (PLPQMC). From the input image, the directional Monogenic bandpass components are generated. Then, each pixel of a bandpass image is replaced by the mean value of its rectangular neighborhood. Next, LPQ histogram sequences are computed upon those images. Finally, these histogram sequences are concatenated for constituting a global representation of the face image. Using the proposed method for feature extraction, we construct a new face recognition system with Whitened Principal Component Analysis (WPCA) for dimensionality reduction, k-nearest neighbor classifier and weighted angle distance for classification. Performance evaluations on two public face databases FERET and SCface show that our method is efficient against some challenging issues, e.g. expressions, illumination, time-lapse, low resolution, and it is competing with state-of-the-art methods.\"",
        "Document: \"Adaptation of ASM to Lips Edge Detection. \n Seeing the talker\u2019s lips in addition to audition can improve speech understanding which is rather based on lips shape temporal\n evolution than on absolute mouth shape. In this article, we propose an adaptation of Active Shape Model (ASM) to the extraction of lips shape over an image sequence. The algorithm does not require any make-up or markers and works\n under natural lighting conditions.\n \n \n After the definition of a training base, initial mouth model is iteratively deformed under constraints according to spatiotemporal\n energies depending either on luminance or hue. A robust prior detection of four points of the model is proposed in order to\n automatically and accurately initialize the egde detection.\n \n \n \n The success of our approach is tested on many image sequences of multi-speakers with multi-speaking.\n \n \"",
        "Document: \"Real-time implementations of an MRF-based motion detection algorithm. The main concern in image processing is the computation cost. Markov random field (MRF)-based algorithms particularly require a significant amount of computation. The paper investigates three solutions to implement a simple, but robust, MRF-based motion detection algorithm in real time: SIMD machine, DSP-based image processing board, and analog resistive network. Details and performances of each implementation are given and a comparison between each realization is made. The underlying goal of this work is to study if real-time implementations of MRF-based algorithms are feasible or not. The answer is positive in the case of quite simple algorithms, but reserved with more complex ones.\"",
        "Document: \"A New Spatiotemporal Approach for Image Analysis. Application to Motion Detection. Image sequence analysis involves 3D data. Consequently, we propose a new spatiotemporal global approach for image sequence processing where an image sequence is regarded as a 3D data flow. This approach is illustrated in the case of motion detection in a Markovian framework. This leads to the development of a 3D Markov Random Field based algorithm which takes into account in the same way spatial and temporal dimensions. The required relaxation algorithm runs on (x,y,t) for the whole image sequence. Motion detection results illustrate the efficiency of this algorithm.\"",
        "Document: \"Video viewing: do auditory salient events capture visual attention?. We assess whether salient auditory events contained in soundtracks modify eye movements when exploring videos. In a previous study, we found that, on average, nonspatial sound contained in video soundtracks impacts on eye movements. This result indicates that sound could play a leading part in visual attention models to predict eye movements. In this research, we go further and test whether the effect of sound on eye movements is stronger just after salient auditory events. To automatically spot salient auditory events, we used two auditory saliency models: the discrete energy separation algorithm and the energy model. Both models provide a saliency time curve, based on the fusion of several elementary audio features. The most salient auditory events were extracted by thresholding these curves. We examined some eye movement parameters just after these events rather than on all the video frames. We showed that the effect of sound on eye movements (variability between eye positions, saccade amplitude, and fixation duration) was not stronger after salient auditory events than on average over entire videos. Thus, we suggest that sound could impact on visual exploration not only after salient events but in a more global way.\"",
        "Document: \"How far we can improve micro features based face recognition systems?. This paper presents improvements for face recognition methods that use LBP descriptor as a main technique in encoding micro features of face images. Our improvements are focused on the feature extraction and dimension reduction steps. In feature extraction, we use a variant of Local Binary Pattern (LBP) so-called Elliptical Local Binary Pattern (ELBP), which is more efficient than LBP for extracting micro facial features of the human face. ELBP of one pixel is built by thresholding its gray value with its P neighboring pixels on a horizontal ellipse. ELBP operator is applied in Pattern of Oriented Edge Magnitudes (POEM) to build Elliptical POEM (EPOEM) descriptor. The dimension reduction step is conducted by using Singular Value Decomposition (SVD) based Whitened Principal Component Analysis (WPCA). For performance evaluation of our improvements, we compare them with LBP based, POEM based approaches and other popular face recognition systems. The experimental results on state-of-the-art FERET and AR face databases prove the advantages and effectiveness of our improvements.\"",
        "Document: \"An MRF based motion detection algorithm implemented on analog resistive network. We present an algorithm based on MRF modelling for motion detection in image sequences and give a modified version for implementation on analog resistive network. Energy minimization is realized by a network relaxing to its state of minimal power dissipation. It takes a few nanoseconds and replaces advantageously time consuming stochastic or suboptimal deterministic relaxation algorithms. The elementary cell of the network is presented along with the environment needed to feed it with the required inputs. Two network architectures are proposed, derived from CCD camera principle. Software simulations of a 128\u00d7128 network demonstrate the good behaviour of the modified algorithm on real sequences. Electrical simulations of a 16\u00d716 network with ideal components give promising results. Implementation of the CMOS circuit with VLSI technology is under study at our laboratory.\"",
        "Document: \"Local Patterns of Gradients for Face Recognition. AbstractWe present a novel feature extraction method named local patterns of gradients (LPOGs) for robust face recognition. LPOG uses block-wised elliptical local binary patterns (BELBP), a refined variant of ELBP, and local phase quantization (LPQ) operators directly on gradient images for capturing local texture patterns to build up a feature vector of a face image. From one input image, two directional gradient images are computed. A symmetric pair of BELBP and a LPQ operator are then separately applied upon each gradient image to generate local patterns images. Histogram sequences of local patterns images' nonoverlapped subregions are finally concatenated to form the LPOG vector for the given image. Based on LPOG descriptor, we propose a novel face recognition system which exploits whitened principal component analysis (WPCA) for dimension reduction and weighted angle-based distance for classification. Experimental results on three large public databases (FERET, AR, and SCface) prove that LPOG WPCA system is robust against a wide range of challenges, such as illumination, expression, occlusion, pose, time-lapse variations, and low resolution. In addition, comparison with other systems shows that LPOG WPCA significantly outperforms the state-of-the-art methods. Computationally, timing benchmarks also demonstrate that our LPOG method is faster than many advanced feature extraction algorithms and can be applied in real-world applications.\"",
        "1 is \"Fast Discrete Curvelet Transforms\", 2 is \"Robust face recognition via sparse representation.\"",
        "Given above information, for an author who has written the paper with the title \"SignTutor: An Interactive System for Sign Language Tutoring\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008540": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Affect-aware tutors: recognising and responding to student affect':",
        "Document: \"Customizing the Instructional Grid. The Web contains hundreds of thousands of educational resources available any time and any place. However no smart technology is available to help teachers and students locate appropriate resources customized to their needs and social characteristics. When educational resources are indexed, it is often done by demographics, such as student age and grade. This article describes customized Grid Learning Services (GLS) that will personalize instruction based on an individual's presumed knowledge and cognitive and learning needs. The customized GLS will use real-time student modeling, the Semantic Web, intelligent agents, and pre-tests of cognitive, affective, and social characteristics to personalize the selection of educational resources and problems. Components of the customized GLS include an ontology construction agent, goal-based retrieval mechanisms, a lesson planner, and student and pedagogical agents.\"",
        "Document: \"Steps from explanation planning to model construction dialogues. Human explanatory dialogue is an activity in which participants interactively construct explanatory models of the topic phenomenon. However, current explanation planning technology does not support such dialogue. In this paper we describe contributions in the areas of discourse planning architectures, heuristics for knowledge communication, and user interface design that take steps towards addressing this problem. First, our explanation planning architecture independently applies various constraints on the content and organization of explanation, avoiding the inflexibility and contextual assumptions of schematic discourse plans. Second, certain planning operators simulate a human explainer's efforts to choose and incrementally develop models of the topic phenomenon. Third, dialogue occurs in the medium of a \"live information\" interface designed to serve as the representational medium through which the activities of the machine and human are coupled. Collectively these contributions facilitate interactive model construction in human-machine dialogue.\"",
        "Document: \"Coaching within a domain independent inquiry environment. We describe a portable coaching environment used within a domain-independent inquiry-learning infrastructure. This coach reasons about a student's knowledge and offers pertinent, domain-specific feedback. It promotes good inquiry behavior by critiquing the student's hypotheses and supporting data and relationships among propositions. Four inquiry tutors in separate disciplines have been developed that use embedded expert knowledge bases and reusable domain-independent rules. We describe the functionality of the coach within an art history domain, discuss the implementation of the coach, and elaborate on the options given to domain authors for customization.\"",
        "Document: \"Improving Student Models by Reasoning about Cognitive Ability, Emotions and Gender. We explore how cognitive, socio-biological and emotional conditions of the student help predict behavior within an ITS, and how instruction should be adapted depending on these variables to improve educational outcomes. Cognitive, social and emotional factors tend to be more permanent in nature than student's knowledge. Our approach is to diagnose them with pre-tests before the user starts using the system.\"",
        "Document: \"Improving math learning through intelligent tutoring and basic skills training. We studied the effectiveness of a math fact fluency tool integrated with an intelligent tutor as a means to improve student performance in math standardized tests. The study evaluated the impact of Math Facts Retrieval Training (MFRT) on 250 middle school students and analyzed the main effects of the training by itself and also as a supplement to the Wayang Tutoring System on easy and hard items of the test. Efficacy data shows improved student performance on tests and positive impact on mathematics learning. We also report on interaction effects of MFRT with student gender and incoming math ability.\"",
        "Document: \"Emotion Sensors Go To School. This paper describes the use of sensors in intelligent tutors to detect students' affective states and to embed emotional support. Using four sensors in two classroom experiments the tutor dynamically collected data streams of physiological activity and students' self-reports of emotions. Evidence indicates that state-based fluctuating student emotions are related to larger, longer-term affective variables such as self-concept in mathematics. Students produced self-reports of emotions and models were created to automatically infer these emotions from physiological data from the sensors. Summaries of student physiological activity, in particular data streams from facial detection software, helped to predict more than 60% of the variance of students emotional states, which is much better than predicting emotions from other contextual variables from the tutor, when these sensors are absent. This research also provides evidence that by modifying the \u201ccontext\u201d of the tutoring system we may well be able to optimize students' emotion reports and in turn improve math attitudes.\"",
        "Document: \"Transfer Learning and Representation Discovery in Intelligent Tutoring Systems. We describe a novel framework developed for transfer learning within reinforcement learning (RL) problems. Then we exhibit how this framework can be extended to intelligent tutoring systems (ITS). We compose an algorithm that automatically constructs a graphical representation based on the transfer framework. We evaluate this on a real-world ITS example and show that the model constructed by our approach performs better than previously published results. We propose that transfer learning is a useful and related area to explore for furthering intelligent tutoring systems.\"",
        "Document: \"Social and Caring Tutors. \n If computers are to interact naturally with humans, they must express social competencies and recognize human emotion. This\n talk describes the role of technology in responding to both affect and cognition and examines research to identify student\n emotions (frustration, boredom and interest) with around 80% accuracy using hardware sensors and student self-reports. We\n also discuss \u201ccaring\u201d computers that use animated learning companions to talk about the malleability of intelligence and importance\n of effort and perseverance. Gender differences were noted in the impact of these companions on student affect as were differences\n for students with learning disabilities. In both cases, students who used companions showed improved math attitudes, increased\n motivation and reduced frustration and anxiety over the long term. We also describe social tutors that scaffold collaborative\n problem solving in ill-defined domains. These tutors use deep domain understanding of students\u2019 dialogue to recognize (with\n over 85% accuracy) students who are engaged in useful learning activities. Finally, we describe tutors that help online participants\n engaged in situations involving differing opinions, e.g., in online dispute mediation, bargaining, and civic deliberation\n processes.\n \n \"",
        "Document: \"Reasoning from Data Rather than Theory.  The current framework for constructing intelligenttutoring systems (ITS) is to use psychological/pedagogical theories of learning, and encodethis knowledge into the tutor. However, this approachis both expensive and not sufficiently flexibleto support reasoning that some system designerswould like intelligent tutors to do. Therefore,we propose using machine learning to automaticallyderive models of student performance.Data are gathered from students using the tutor.These... \"",
        "Document: \"Connect the Dots to Prove It: A Novel Way to Learn Proof Construction. This paper describes a new method for helping students improve their ability to develop proofs, a skill necessary for comprehending and appreciating the foundational topics of computer science. Our method transforms ordinary pen-and-paper homework problems into a puzzle-like game, where students connect dots to justify assertions, in a quest to reach a desired goal. We have implemented a software tutoring system using this method, for students to use at home as an optional study aid. Potentially, our system could one day become a full replacement for traditional hand-written homework, which has the additional benefit for course instructors of automating the grading of student work. Our system is also easy to adapt to any class that requires students to write proofs, and it is easy for instructors to create new problems to use with this system. This stands in contrast to many other educational tools for teaching proofs, which are limited to specific topic domains. We have demonstrated the versatility of our system by testing it in two computer science classes at a large public university. One was a Sophomore-level discrete mathematics course where the students were learning first-order prepositional logic, and the other was a Junior-level algorithms course where students were being first exposed to the concept of NP-completeness. Students from our experiments reported that they would like our system to be used in more of their classes.\n\n\"",
        "1 is \"Rotation Invariant Neural Network-Based Face Detection\", 2 is \"Affective and behavioral predictors of novice programmer achievement\"",
        "Given above information, for an author who has written the paper with the title \"Affect-aware tutors: recognising and responding to student affect\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008566": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Topic 6: Grid and Cluster Computing: Models, Middleware and Architectures':",
        "Document: \"Scalable Parallel Program Debugging with Process Isolation and Grouping. Using cyclic debugging techniques for large-scale parallel programs is often prohibited by a program's size and runtime and the associated computing costs. The hitherto conventional solution of down-scaling the number of processes and/or the problem size is only partially satisfying, because the program's behavior may differ significantly compared to the original scale and some errors may even vanish completely. A different solution is offered by process isolation, which allows to extract and execute single processes of an arbitrary parallel program. By simulating the surroundings of the selected process, this method pretends the program's behavior on full scale. Combined with a grouping strategy, it allows to reduce the number of actually running processes to a small group instead of a single process. This enables the application of well-established debugging tools and their functionality for error detection with manageable numbers of processes. In addition, the grouping mechanism allows to adjust the size of the traces needed to simulate process interaction.\"",
        "Document: \"Communication Pattern Based Performance Prediction on the nCUBE 2 Multiprocessor System. This paper shows the usage of the PAPS toolset for performance prediction on the nCUBE 2 multiprocessor system. Two Petri net models for communication on the nCUBE 2 with different levels of accuracy are developed and investigated. The representation of network contention within the Petri net models is validated for the message broadcast communication pattern. Parameter values for communication are determined for the nCUBE 2 machine. A parallel implementation of the Gaussian elimination algorithm is used to investigate the accuracy of the predicted absolute execution time depending on the choosen Petri net communication model.\"",
        "Document: \"Parallel computing and the Grid-experiences and applications. In recent years, Grid computing evolved from first implementations as prototype Grid environments to large-scale production Grid infrastructures utilised during everyday work by scientists around the world. This demonstrates that the concept of the Grid is more than merely a marketing phrase, but instead an enabler for new application domains in parallel and distributed computing. Among others, the EU project EGEE (\"Enabling Grids for E-Science\") is probably the worlds largest initiative for establishing a permanent Grid infrastructure on a 24\u00a0\u00d7\u00a07 basis. Such a Grid that is always on and there to serve the community just like the ubiquitous networking infrastructures today is on the horizon, waiting for users to utilise it in their applications. This paper explores the different possibilities for utilising Grids in parallel applications, focusing on different parallel computing aspects as provided by Grid environments. Basic concepts such as using the Grid as a large data storage and management basis or as a collection of distributed supercomputers represent a new approach to computational science, where users are expecting to utilise unprecedented amounts of performance for tackling their scientific problems.\"",
        "Document: \"Monitoring Strategies for Hypercube Systems. The basis for analyzing and improving the reliability and efficiency of software is a monitoring tool. Besides the usual difficulties of diagnostic tools, parallel computing also introduces the probe effect to the monitoring process. This effect describes altered program behaviors observed when delays are introduced into concurrent programs through the use of instrumentation. The probability of the probe effect depends on the amount of overhead that is produced with the monitoring functionality.This paper describes the Event Monitoring Utility EMU, which offers an approach for monitoring of distributed memory multiprocessors. The current implementation takes advantage of the hypercube topology for further reduction of the overhead caused by the instrumentation. Therefore EMU contains various monitoring models with different impact on the observed program. The knowledge of these models can help the user to reduce the overhead of the monitor as much as possible.\"",
        "Document: \"Shortcut Replay: A Replay Technique for Debugging Long-Running Parallel Programs. Applications running on HPC Platforms, PC clusters, or computational grids are often long-running parallel programs. Debugging these programs is a challenge due to the lack of efficient debugging tools and the inherent possibility of nondeterminism in parallel programs. To overcome the problem of nondeterminism, several sophisticated record&replay mechanisms have been developed. However, the substantial problem of the waiting time during re-execution was not sufficiently investigated in the past. This paper shows that the waiting time is in some cases unlimited with currently available methods, which prohibits efficient interactive debugging tools. In contrast, the new shortcut replay method combines checkpointing and debugging techniques. It controls the replayed execution based on the trace data in order to minimize the waiting time during debugging long-running parallel programs.\"",
        "Document: \"An IT-infrastructure capability model. The scale and complexity of todays IT-Infrastructures represent a challenge to understand the effects of modifications to these IT-Infrastructures or their components. This motivates the following research question: \"How to evaluate high-level, global capability effects induced through low-level, local modifications to IT-Infrastructures?\". Answering this question requires understanding of IT-Infrastructures in general and especially of their building blocks and of the complex interdependencies and ratios between capabilities, which have to be considered while planning modifications. After further illustrating the environment by providing an exemplary scenario, the question is described in detail. Preliminary results and the next steps to conduct on the way to answer the above mentioned research question are presented.\"",
        "Document: \"Investigating the scalability of openFOAM for the solution of transport equations and large eddy simulations. OpenFOAM is a mainstream open-source framework for flexible simulation in several areas of CFD and engineering whose syntax is a high level representation of the mathematical notation of physical models. We use the backward-facing step geometry with Large Eddy Simulations (LES) and semiimplicit methods to investigate the scalability and important MPI characteristics of OpenFOAM. We find that the master-slave strategy introduces an unexpected bottleneck in the communication of scalar values when more than a hundred MPI tasks are employed. An extensive analysis reveals that this anomaly is present only in a few MPI tasks but results in a severe overall performance reduction. The analysis work in this paper is performed with the tool IPM, a portable profiling and workload characterization tool for MPI programs.\"",
        "Document: \"I have a DRIHM: A Case Study in Lifting Computational Science Services Up to the Scientific Mainstream. While we are witnessing a transition from petascale to exascale computing, we experience, when teaching students and scientists to adopt distributed computing infrastructures for computational science, what Geoffrey A. Moore once coined  chasm between the visionaries in computational science and the early majority of scientific pragmatists. Using the EU-funded DRIHM project (Distributed Research Infrastructure for Hydro-Meteorology) as a case study, we observe that innovative research infrastructures have difficulties to be accepted by the scientific pragmatists because the infrastructure and scientific services are not mainstream enough. Excellence in workforce, however, can only be achieved if the tools are not only available but also used. In  paper we report on how this chasm is exhibited in the DRIHM case, how it was (partially) crossed, and what can be learned from  experience for more general cases. (C) 2016 Elsevier B.V. All rights reserved.\"",
        "Document: \"Notes on Nondeterminism in Message Passing Programs. Nondeterministic program behavior can lead to different results in subsequent program runs based on the same input data. This kind of problem can be seen in any program, but is even magnified in a parallel execution context due to the existence of several independent but communicating tasks. Even though this kind of nondeterminism is commonplace and in many cases even useful for the implementation of applications, it often leads to sporadically occurring errors. These bugs are difficult to reproduce and represent a heavy challenge during testing and debugging. The biggest problem, however, may be the unawareness of users about the existence of nondeterministic choices and their consequences. In order to raise the awareness and to provoke discussions about this serious problem, this paper provides an exemplary overview of nondeterministic behavior in message passing programs. With simple examples, it is demonstrated how nondeterminism can vigorously affect the behavior and the final results of software and how the behavior can change between different architectures.\"",
        "Document: \"An Integrated Record&Replay Mechanism for Nondeterministic Message Passing Programs. Nondeterminism is a characteristic of many parallel programs that needs dedicated support from analysis tools and programming environments. In order to allow cyclic debugging of such programs, record&replay mechanisms are used most frequently. Such techniques operate in two phases, where the record phase traces a program's execution that can be arbitrarily repeated during subsequent replay phases. In contrast to most existing approaches, this paper describes a mechanism that is transparently integrated in the underlying message passing interface. The main advantage of this approach is its omnipresence, such that a program's execution can be repeated immediately after it has been observed. Other benefits are the lack of instrumentation and a corresponding simplification of the whole technique for inexperienced users. The difficulties addressed by this approach are concerned with the amount of monitor overhead, which must neither perturb the program's execution nor generate huge amounts of trace data.\"",
        "1 is \"Introduction to web services architecture\", 2 is \"The UNICORE Architecture: Seamless Access to Distributed Resources\"",
        "Given above information, for an author who has written the paper with the title \"Topic 6: Grid and Cluster Computing: Models, Middleware and Architectures\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008651": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Privacy-Preserving Multi-Keyword Ranked Search over Encrypted Cloud Data':",
        "Document: \"Secure Optimization Computation Outsourcing in Cloud Computing: A Case Study of Linear Programming. Cloud computing enables an economically promising paradigm of computation outsourcing. However, how to protect customers confidential data processed and generated during the computation is becoming the major security concern. Focusing on engineering computing and optimization tasks, this paper investigates secure outsourcing of widely applicable linear programming (LP) computations. Our mechanism design explicitly decomposes LP computation outsourcing into public LP solvers running on the cloud and private LP parameters owned by the customer. The resulting flexibility allows us to explore appropriate security/efficiency tradeoff via higher-level abstraction of LP computation than the general circuit representation. Specifically, by formulating private LP problem as a set of matrices/vectors, we develop efficient privacy-preserving problem transformation techniques, which allow customers to transform the original LP into some random one while protecting sensitive input/output information. To validate the computation result, we further explore the fundamental duality theorem of LP and derive the necessary and sufficient conditions that correct results must satisfy. Such result verification mechanism is very efficient and incurs close-to-zero additional cost on both cloud server and customers. Extensive security analysis and experiment results show the immediate practicability of our mechanism design.\"",
        "Document: \"Improved known-plaintext attack to permutation-only multimedia ciphers. \u2022Based on a rigorous theoretical analysis, we propose an improved known-plaintext attack (KPA) on permutation-only multimedia encryption algorithms.\u2022The theoretical study results in some novel KPA algorithms, one of which runs at least two orders of magnitude than that of the state-of-the-art methods.\"",
        "Document: \"Enabling Privacy-Preserving Image-Centric Social Discovery. The increasing popularity of images at social media sites is posing new opportunities for social discovery applications, i.e., suggesting new friends and discovering new social groups with similar interests via exploring images. To effectively handle the explosive growth of images involved in social discovery, one common trend for many emerging social media sites is to leverage the commercial public cloud as their robust backend data center. While extremely convenient, directly exposing content-rich images and the related social discovery results to the public cloud also raises new acute privacy concerns. In light of the observation, in this paper we propose a privacy-preserving social discovery service architecture based on encrypted images. As the core of such social discovery is to compare and quantify similar images, we first adopt the effective Bag-of-Words model to extract the \\\"visual similarity content\\\" of users' images into image profile vectors, and then model the problem as similarity retrieval of encrypted high-dimensional image profiles. To support fast and scalable similarity search over hundreds of thousands of encrypted images, we propose a secure and efficient indexing structure. The resulting design enables social media sites to obtain secure, practical, and accurate social discovery from the public cloud, without disclosing the encrypted image content. We formally prove the security and discuss further extensions on user image update and the compatibility with existing image sharing social functionalities. Extensive experiments on a large Flickr image dataset demonstrate the practical performance of the proposed design. Our qualitative social discovery results show consistency with human perception.\"",
        "Document: \"SBVLC: Secure Barcode-based Visible Light Communication for Smartphones. 2D barcodes have enjoyed a significant penetration rate in mobile applications. This is largely due to the extremely low barrier to adoption \u2013 almost every camera-enabled smartphone can scan 2D barcodes. As an alternative to NFC technology, 2D barcodes have been increasingly used for security-sensitive mobile applications including mobile payments and personal identification. However, the security of barcode-based communication in mobile applications has not been systematically studied. Due to the visual nature, 2D barcodes are subject to eavesdropping when they are displayed on the smartphone screens. On the other hand, the fundamental design principles of 2D barcodes make it difficult to add security features. In this paper, we propose SBVLC - a secure system for barcode-based visible light communication (VLC) between smartphones. We formally analyze the security of SBVLC based on geometric models and propose physical security enhancement mechanisms for barcode communication by manipulating screen view angles and leveraging user-induced motions. We then develop three secure data exchange schemes that encode information in barcode streams. These schemes are useful in many security-sensitive mobile applications including private information sharing, secure device pairing, and contactless payment. SBVLC is evaluated through extensive experiments on both Android and iOS smartphones.\"",
        "Document: \"Privacy-preserving outsourcing of image global feature detection. The amount and availability of user-contributed image data have been dramatically increased during the past ten years. Popular multimedia social networks, e.g. Flicker, commonly utilize user image data to construct user behavior models, social preferences, etc., for the purpose of effective advertisement, better user retention and attraction, and many others. Existing practices of data utilization, however, seriously deteriorate users' personal privacy and have led to increasing criticisms and legislation pressures. In this paper, we aim to construct a privacy-preserving feature detection scheme over encrypted image data. The proposed system enables an interested party to perform a variety of image feature detection tasks, including visual descriptors in MPEG-7 standard, while protecting user privacy relating to image contents. We implement a prototype system based on somewhat homomorphic encryption scheme and the benchmark Caltech256 database. The experimental results show that our system can guarantee effective image feature detection without sacrificing user privacy.\"",
        "Document: \"Ensuring Data Storage Security In Cloud Computing. Cloud Computing has been envisioned as the next-generation architecture of IT Enterprise. In contrast to traditional solutions, where the IT services are under proper physical, logical and personnel controls, Cloud Computing moves the application software and databases to the large data centers, where the management of the data and services may not be fully trustworthy. This unique attribute, however, poses many new security challenges which have not been well understood. In this article, we focus on cloud data storage security, which has always been an important aspect of quality of service. To ensure the correctness of users' data in the cloud, we propose an effective and flexible distributed scheme with two salient features, opposing to its predecessors. By utilizing the homomorphic token with distributed verification of erasure-coded data, our scheme achieves the integration of storage correctness insurance and data error localization, i.e., the identification of misbehaving server(s). Unlike most prior works, the new scheme further supports secure and efficient dynamic operations on data blocks, including: data update, delete and append. Extensive security and performance analysis shows that the proposed scheme is highly efficient and resilient against Byzantine failure, malicious data modification attack, and even server colluding attacks.\"",
        "Document: \"Semi-Supervised Federated Learning for Travel Mode Identification From GPS Trajectories. GPS trajectories serve as a significant data source for travel mode identification along with the development of various GPS-enabled smart devices. However, such data directly integrate user private information, thus hindering users from sharing data with third parties. On the other hand, existing identification methods heavily depend on the respective manual travel mode annotations, whose product...\"",
        "Document: \"Incipient fault detection of nonlinear dynamical systems via deterministic learning. Detection of incipient faults is an important and challenging issue in the area of fault diagnosis and prognosis. This paper presents a new incipient fault detection approach for nonlinear dynamical systems via deterministic learning. Through defining and establishing the banks of health, sub-health and incipient fault modes, the incipient fault detectability condition is derived with the fault mismatch function. The system dynamics underlying three kinds of system modes are accurately approximated via deterministic learning firstly. Secondly, a bank of estimators is constructed using the learned modes. A set of residuals is achieved by comparing the bank of estimators with the monitored system. According to the smallest residual principle and the fault mismatch function, if the average L1 norm of the residual, which is associated with one of the incipient fault modes, is smaller than the others at a time instant td, the system incipient fault is detected. Finally, the incipient fault detectability is analyzed rigorously. Numerical simulation is investigated to demonstrate the effectiveness of the approach. The significance of the paper lies in that the learned system modes are utilized to achieve rapid detection in the initial stage of incipient fault for nonlinear dynamical systems.\"",
        "Document: \"Bringing execution assurances of pattern matching in outsourced middleboxes. Migrating middleboxes to third-party service providers (e.g., clouds and ISPs) has drawn widespread attentions recently from both industry and academia. While its benefits on reduced local cost and increased service scalability are well understood, such deployment also introduces new security concerns, due to the fact that these boxes are no longer under the direct control of enterprises. Among others, one fundamental desideratum here is to ensure that those middleboxes consistently perform network functions as intended. In this work, we propose practical solutions towards enabling runtime execution assurances of outsourced middleboxes with high confidence. As an initial effort, we target on pattern matching based network functions, which cover a broad class of middlebox applications such as instruction detection, web firewall, and traffic classification. For efficiency, our design follows the same roadmap of probabilistic checking that provides tunable levels of assurance, as in outsourced computation and distributed computing literature. We show how to synthesize the design intuitions in the context of outsourced middleboxes and the dynamic network effect. We present diligent technical instantiations, in the case of single middlebox and the composition of multiple middlebox service chaining, respectively. For a large batch of packets, sufficiently high assurance levels can be achieved by pre-processing only a few randomly selected packets, with marginal overhead. Evaluations of our system prototype on Amazon EC2 show that, the processing of 1000 packets, which includes pattern matching and execution proof generation, results in 200\u2013500ms latency and throughput up to 360Mbps.\"",
        "Document: \"A privacy-aware cloud-assisted healthcare monitoring system via compressive sensing. Wireless sensors are being increasingly used to monitor/collect information in healthcare medical systems. For resource-efficient data acquisition, one major trend today is to utilize compressive sensing, for it unifies traditional data sampling and compression. Despite the increasing popularity, how to effectively process the ever-growing healthcare data and simultaneously protect data privacy, while maintaining low overhead at sensors, remains challenging. To address the problem, we propose a privacy-aware cloud-assisted healthcare monitoring system via compressive sensing, which integrates different domain techniques with following benefits. By design, acquired sensitive data samples never leave sensors in unprotected form. Protected samples are later sent to cloud, for storage, processing, and disseminating reconstructed data to receivers. The system is privacy-assured where cloud sees neither the original samples nor underlying data. It handles well sparse and general data, and data tampered with noise. Theoretical and empirical evaluations demonstrate the system achieves privacy-assurance, efficiency, effectiveness, and resource-savings simultaneously.\"",
        "1 is \"Friendship and mobility: user movement in location-based social networks\", 2 is \"Robust uncertainty principles: exact signal reconstruction from highly incomplete frequency information\"",
        "Given above information, for an author who has written the paper with the title \"Privacy-Preserving Multi-Keyword Ranked Search over Encrypted Cloud Data\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008658": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Motif matching using gapped patterns.':",
        "Document: \"Disk-based k-mer counting on a PC. The k-mer counting problem, which is to build the histogram of occurrences of every k-symbol long substring in a given text, is important for many bioinformatics applications. They include developing de Bruijn graph genome assemblers, fast multiple sequence alignment and repeat detection.We propose a simple, yet efficient, parallel disk-based algorithm for counting k-mers. Experiments show that it usually offers the fastest solution to the considered problem, while demanding a relatively small amount of memory. In particular, it is capable of counting the statistics for short-read human genome data, in input gzipped FASTQ file, in less than 40 minutes on a PC with 16 GB of RAM and 6 CPU cores, and for long-read human genome data in less than 70 minutes. On a more powerful machine, using 32 GB of RAM and 32 CPU cores, the tasks are accomplished in less than half the time. No other algorithm for most tested settings of this problem and mammalian-size data can accomplish this task in comparable time. Our solution also belongs to memory-frugal ones; most competitive algorithms cannot efficiently work on a PC with 16 GB of memory for such massive data.By making use of cheap disk space and exploiting CPU and I/O parallelism we propose a very competitive k-mer counting procedure, called KMC. Our results suggest that judicious resource management may allow to solve at least some bioinformatics problems with massive data on a commodity personal computer.\"",
        "Document: \"Disk-based genome sequencing data compression.   Motivation: High-coverage sequencing data have significant, yet hard to exploit, redundancy. Most FASTQ compressors cannot efficiently compress the DNA stream of large datasets, since the redundancy between overlapping reads cannot be easily captured in the (relatively small) main memory. More interesting solutions for this problem are disk-based~(Yanovsky, 2011; Cox et al., 2012), where the better of these two, from Cox~{\\it et al.}~(2012), is based on the Burrows--Wheeler transform (BWT) and achieves 0.518 bits per base for a 134.0 Gb human genome sequencing collection with almost 45-fold coverage.   Results: We propose ORCOM (Overlapping Reads COmpression with Minimizers), a compression algorithm dedicated to sequencing reads (DNA only). Our method makes use of a conceptually simple and easily parallelizable idea of minimizers, to obtain 0.317 bits per base as the compression ratio, allowing to fit the 134.0 Gb dataset into only 5.31 GB of space.   Availability: http://sun.aei.polsl.pl/orcom under a free license. \"",
        "Document: \"Tight and simple Web graph compression for forward and reverse neighbor queries. Analyzing Web graphs has applications in determining page ranks, fighting Web spam, detecting communities and mirror sites, and more. This study is however hampered by the necessity of storing a major part of huge graphs in the external memory which prevents efficient random access to edge (hyperlink) lists. A number of algorithms involving compression techniques have thus been presented, to represent Web graphs succinctly, but also providing random access. Those techniques are usually based on differential encodings of the adjacency lists, finding repeating nodes or node regions in the successive lists, more general grammar-based transformations or 2-dimensional representations of the binary matrix of the graph. In this paper we present three Web graph compression algorithms. The first can be seen as engineering of the Boldi and Vigna (2004) [8] method. We extend the notion of similarity between link lists and use a more compact encoding of residuals. The algorithm works on blocks of varying size (in the number of input lists) and sacrifices access time for better compression ratio, achieving more succinct graph representation than other algorithms reported in the literature. The second algorithm works on blocks of the same size in the number of input lists. Its key mechanism is merging the block into a single ordered list. This method achieves much more attractive space-time tradeoffs. Finally, we present an algorithm for bidirectional neighbor query support, which offers compression ratios better than those known from the literature.\"",
        "Document: \"Effective asymmetric XML compression. The innate verbosity of the extensible markup language (XML) remains one of its main weaknesses, especially when large documents are concerned. This problem can be solved with the aid of dedicated XML compression algorithms. In this work, we describe XML word-replacing transform (XML-WRT), a fast and fully reversible XML transform, which, when combined with generally used LZ77-style compression algorithms, allows to attain high compression ratios, comparable to those achieved by the current state-of-the-art XML compressors. The resulting compression scheme is asymmetric in the sense that its decoder is much faster than the coder. This is a desirable practical property, as in many XML applications data are read much more often than written. The key features of the transform are dictionary-based encoding of both document structure and content, separation of different content types into multiple streams, and dedicated encoding of specific patterns, including numbers and dates. The test results show that the proposed transform improves the XML compression efficiency of general-purpose compressors on average by 35&percnt; in case of gzip, and 17&percnt; in case of LZMA. Compared with the current state-of-the-art SCMPPM algorithm, XML-WRT with LZMA attains over 2&percnt; better compression ratio, while being 55&percnt; faster. Copyright \u00a9 2007 John Wiley & Sons, Ltd.\"",
        "Document: \"On Two Variants of the Longest Increasing Subsequence Problem. Finding a longest increasing subsequence (LIS) of a given sequence is a classic problem in string matching, with applications mostly in computational biology. Recently, many variations of this problem have been introduced. We present new algorithms for two such problems: the longest increasing circular subsequence (LICS) and the slope-constrained longest increasing subsequence (SLIS). For LICS, our algorithm improves one of the most competitive techniques if the length of the output sequence is close to its expected value 2 root n + o(root n). In the algorithm for SLIS, we show how to gain from modem successor search data structures, which is not trivial for this task.\"",
        "Document: \"Range mode and range median queries in constant time and sub-quadratic space. Given a list of n items and a function defined over sub-lists, we study the space required for computing the function for arbitrary sub-lists in constant time. For the function mode we improve the previously known space bound O(n^2/logn) to O(n^2loglogn/log^2n) words. For median the space bound is improved to O(n^2loglog^2n/log^2n) words from O(n^2@?log^(^k^)n/logn), where k is an arbitrary constant and log^(^k^) is the iterated logarithm.\"",
        "Document: \"A general compression algorithm that supports fast searching. The task of compressed pattern matching [2] is to report all the occurences of a given pattern P in a text T available in compressed form. Certain compression algorithms allow for searching without prior decoding which may be practical, especially if the search is faster than in the non-compressed representation. Most of the known schemes, however, either assume a text formed into words, or are complex and rather theoretical. \"",
        "Document: \"Approximate pattern matching with k-mismatches in packed text. Given strings P of length m and T of length n over an alphabet of size @s, the string matching with k-mismatches problem is to find the positions of all the substrings in T that are at Hamming distance at most k from P. If T can be read only one character at the time the best known bounds are O(nklogk) and O(n+nk/wlogk) in the word-RAM model with word length w. In the RAM models (including AC^0 and word-RAM) it is possible to read up to @?w/log@s@? characters in constant time if the characters of T are encoded using @?log@s@? bits. The only solution for k-mismatches in packed text works in O((nlog@s/logn)@?mlog(k+logn/log@s)/w@?+n^@e) time, for any @e0. We present an algorithm that runs in time O(n@?w/(mlog@s)@?(1+logmin(k,@s)logm/log@s)) in the AC^0 model if m=O(w/log@s) and T is given packed. We also describe a simpler variant that runs in time O(n@?w/(mlog@s)@?logmin(m,logw/log@s)) in the word-RAM model. The algorithms improve the existing bound for w=@W(log^1^+^@en), for any @e0. Based on the introduced technique, we present algorithms for several other approximate matching problems.\"",
        "Document: \"KMC 2: Fast and resource-frugal k-mer counting. Motivation: Building the histogram of occurrences of every k-symbol long substring of nucleotide data is a standard step in many bioinformatics applications, known under the name of k-mer counting. Its applications include developing de Bruijn graph genome assemblers, fast multiple sequence alignment and repeat detection. The tremendous amounts of NGS data require fast algorithms for k-mer counting, preferably using moderate amounts of memory. Results: We present a novel method for k-mer counting, on large datasets about twice faster than the strongest competitors (Jellyfish 2, KMC 1), using about 12GB (or less) of RAM. Our disk-based method bears some resemblance to MSPKmerCounter, yet replacing the original minimizers with signatures (a carefully selected subset of all minimizers) and using (k, x)-mers allows to significantly reduce the I/O and a highly parallel overall architecture allows to achieve unprecedented processing speeds. For example, KMC 2 counts the 28-mers of a human reads collection with 44-fold coverage (106GB of compressed size) in about 20 min, on a 6-core Intel i7 PC with an solid-state disk.\"",
        "Document: \"Tight And Simple Web Graph Compression. Analysing Web graphs has applications in determining page ranks, fighting Web spam, detecting communities and mirror sites, and more. This study is however hampered by the necessity of storing a major part of huge graphs in the external memory, which prevents efficient random access to edge (hyperlink) lists. A number of algorithms involving compression techniques have thus been presented, to represent Web graphs succinctly but also providing random access. Those techniques are usually based on differential encodings of the adjacency lists, finding repeating nodes or node regions in the successive lists, more general grammar-based transformations or 2-dimensional representations of the binary matrix of the graph. In this paper we present a Web graph compression algorithm which can be seen as engineering of the Boldi and Vigna (2004) method. We extend the notion of similarity between link lists, and use a more compact encoding of residuals. The algorithm works on blocks of varying size (in the number of input lines) and sacrifices access time for better compression ratio, achieving more succinct graph representation than other algorithms reported in the literature. Additionally, we show a simple idea for 2-dimensional graph representation which also achieves state-of-the-art compression ratio.\"",
        "1 is \"Constructing Word-Based Text Compression Algorithms\", 2 is \"Extracting approximate patterns\"",
        "Given above information, for an author who has written the paper with the title \"Motif matching using gapped patterns.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008736": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Security analysis and resource requirements of group-oriented user access control for hardware-constrained wireless network services.':",
        "Document: \"Enabling User Access Control in Energy-constrained Wireless Smart Environments. This work introduces a novel access control solution for wireless network services in Internet of Things scenarios. We focus on a minimal use of computation, energy and storage resources at wireless sensors so as to address constrained devices: the proposed methods for key distribution and access control rely on extremely fast key derivation functions and, for the same reason, memory usage is reduced since keys are computed on the fly when needed. Our solution achieves privacy, authentication, semantic security, low energy, low computational demand and impacts mitigation of compromised devices on a simple manner. The access control provided is based on user identity and time intervals. We discuss these properties, compare our proposal to previous related work and provide experimental results that confirm its viability.\"",
        "Document: \"A parameter-based service discovery protocol for mobile ad-hoc networks. Application of traditional service discovery solutions to mobile ad-hoc networks is a challenging task due to their intrinsic dynamic nature and the absence of any central information manager. However, service discovery is a critical aspect of service oriented technologies, e.g. remote service execution or, particularly service composition. We propose a solution for service discovery in mobile ad-hoc networks which is based on the dissemination of information about services' parameters instead of service unique identifiers. Disseminated information is subsequently used during service search in order to reduce the number of propagated messages. In our solution, performed searches are maintained in the network until they are explicitly cancelled by source nodes. We also state that the usage of a shared taxonomy of parameter types reduces the number of propagated messages during dissemination and search. The proposed protocol has been fully implemented and tested using a network simulator.\"",
        "Document: \"Interactive live-streaming technologies and approaches for web-based applications. Interactive live streaming is a key feature of applications and platforms in which the actions of the viewers affect the content of the stream. In those, a minimal capture-display delay is critical. Though recent technological advances have certainly made it possible to provide web-based interactive live-streaming, little research is available that compares the real-world performance of the different web-based schemes. In this paper we use educational remote laboratories as a case study. We analyze the restrictions that web-based interactive live-streaming applications have, such as a low delay. We also consider additional characteristics that are often sought in production systems, such as universality and deployability behind institutional firewalls. The paper describes and experimentally compares the most relevant approaches for the study. With the provided descriptions and real-world experimental results, researchers, designers and developers can: a) select among the interactive live-streaming approaches which are available for their real-world systems, b) decide which one is most appropriate for their purpose, and c) know what performance and results they can expect.\"",
        "Document: \"MASSHA: An agent-based approach for human activity simulation in intelligent environments. Human activity recognition has the potential to become a real enabler for ambient assisted living technologies. Research on this area demands the execution of complex experiments involving humans interacting with intelligent environments in order to generate meaningful datasets, both for development and validation. Running such experiments is generally expensive and troublesome, slowing down the research process. This paper presents an agent-based simulator for emulating human activities within intelligent environments: MASSHA. Specifically, MASSHA models the behaviour of the occupants of a sensorised environment from a single-user and multiple-user point of view. The accuracy of MASSHA is tested through a sound validation methodology, providing examples of application with three real human activity datasets and comparing these to the activity datasets produced by the simulator. Results show that MASSHA can reproduce behaviour patterns that are similar to those registered in the real datasets, achieving an overall accuracy of 93.52% and 88.10% in frequency and 98.27% and 99.09% in duration for the single-user scenario datasets; and a 99.3% and 88.25% in terms of frequency and duration for the multiple-user scenario.\"",
        "Document: \"Security analysis and resource requirements of group-oriented user access control for hardware-constrained wireless network services. We extend and analyse a previous access control solution for wireless network services with group-based authorization. Authentication and encryption are provided, and access control relies on user identity, group membership and time intervals. Both the basic solution and the extension focus on minimizing computation, energy, storage and communications on the sensor side: computations involved rely on symmetric cryptography and key derivation functions, and no additional messages between user and sensor are needed. The performance of our solution is proven by experiments on a highly constrained platform such as Arduino. Finally, its security is validated against the AVISPA tool.\"",
        "Document: \"To switch off the coffee-maker or not: that is the question to be energy-efficient at work. There are some barriers to reduce energy consumption in shared spaces where many people use common electronic devices (e.g. dilution of responsibility, the trade-off between comfort and necessity, absentmindedness, or the lack of support to foster energy-efficiency). The workplace is a challenging scenario since the economic incentives are not present to increase energy awareness. To tackle some of these issues we have augmented a shared coffee-maker with eco-feedback to turn it into a green ally of the workers. Its design rationale is twofold: Firstly, to make the coffee-maker able to learn its own usage pattern. Secondly, to communicate persuasively and in real-time to users whether it is more efficient to leave the appliance on or off during certain periods of time along the workday. The goal is to explore a human-machine team towards energy efficiency and awareness, i.e. whether giving the initiative to users to decide how to operate the common appliances, but being assisted by them, is a better choice than automation or mere informative eco-feedback.\"",
        "Document: \"Collaboration-Centred Cities through Urban Apps Based on Open and User-Generated Data. This paper describes the IES Cities platform conceived to streamline the development of urban apps that combine heterogeneous datasets provided by diverse entities, namely, government, citizens, sensor infrastructure and other information data sources. This work pursues the challenge of achieving effective citizen collaboration by empowering them to prosume urban data across time. Particularly, this paper focuses on the query mapper; a key component of the IES Cities platform devised to democratize the development of open data-based mobile urban apps. This component allows developers not only to use available data, but also to contribute to existing datasets with the execution of SQL sentences. In addition, the component allows developers to create ad hoc storages for their applications, publishable as new datasets accessible by other consumers. As multiple users could be contributing and using a dataset, our solution also provides a data level permission mechanism to control how the platform manages the access to its datasets. We have evaluated the advantages brought forward by IES Cities from the developers' perspective by describing an exemplary urban app created on top of it. In addition, we include an evaluation of the main functionalities of the query mapper.\"",
        "Document: \"An Extensible Architecture for the Integration of Remote and Virtual Laboratories in Public Learning Tools. Remote laboratories are software and hardware tools that allow students to remotely access real equipment located in universities. The integration of remote laboratories in learning tools (learning management systems, content management systems, or personal learning environments) has been achieved to integrate remote laboratories as part of the learning curricula. A cross-institutional initiative ...\"",
        "Document: \"An Open and Scalable Web-Based Interactive Live-Streaming architecture: The WILSP Platform. Interactive live-streaming applications and platforms face particular challenges: the actions of the viewer's affect the content of the stream. A minimal capture-render delay is critical. This is the case of applications, such as remote laboratories, which allow students to view specific hardware through a webcam, and interact with it remotely in close to real time. It is also the case of other applications, such as videoconferencing or remote rendering. In the latest years, several commercial live-streaming platforms have appeared. However, the most of them have two significant limitations. First, because they are oriented toward standard live-streaming, their capture-render delay tends to be too high for interactive live-streaming. Second, their architectures and sources are closed. That makes them unsuitable for many research and practical purposes, especially when customization is required. This paper presents the requirements for an interactive live-streaming platform, focusing on remote lab needs as a case study. Then, it proposes an architecture to satisfy those requirements that relies on Redis to achieve high scalability. The architecture is based on open technologies, and has been implemented and published as open source. From a client-side perspective, it is web-based and mobile-friendly. It is intended to be useful for both research and practical purposes. Finally, this paper experimentally evaluates the proposed architecture through its contributed implementation, analyzing its performance and scalability.\"",
        "Document: \"An Approach To Subjectivity Detection On Twitter Using The Structured Information. In this paper, we propose an approach to the subjectivity detection on Twitter micro texts that explores the uses of the structured information of the social network framework. The sentiment analysis on Twitter has been usually performed through the automatic processing of the texts. However, the established limit of 140 characters and the particular characteristics of the texts reduce drastically the accuracy of Natural Language Processing (NLP) techniques. Under these circumstances, it becomes necessary to study new data sources that allow us to extract new useful knowledge to represent and classify the texts. The structured information, also called meta-information or meta-data, provide us with alternative features of the texts that can improve the classification tasks. In this study we have analysed the use of features extracted from the structured information in the subjectivity detection task, as a first step of the polarity detection task, and their integration with classical features.\"",
        "1 is \"Virtual channels in networks on chip: implementation and evaluation on hermes NoC\", 2 is \"Exploratory undersampling for class-imbalance learning.\"",
        "Given above information, for an author who has written the paper with the title \"Security analysis and resource requirements of group-oriented user access control for hardware-constrained wireless network services.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008744": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Stochastic Approach to Separate Diffuse and Specular Reflections':",
        "Document: \"Hand shape recognition using distance transform and shape decomposition. Hand shape is a natural and human-friendly interface for human-computer interaction. This paper proposes a real- time and 2D vision-based hand shape recognition method. The method is robust to hand pose changes because the hand pose is neutralized after recognizing a hand pose using distance transform, principal component analysis (PCA), and histogram analysis. Also, the context-based recognition method using shape decomposition can effectively recognize tiny changes of fingers. The method worked at 44.8 fps and had a recognition rate of 83% on average in the experiment with 800 images including 5 hand shapes and 16 hand poses.\"",
        "Document: \"Emotion Recognition from Dance Image Sequences Using Contour Approximation. We present a novel approach that exploits shape context to recognize emotion from monocular dance image sequences. The method makes use of contour information as well as region-based shape information. The procedure of the method is as follows. First, we compute binary silhouette images and its bounding box from dance images. Next, we extract the quantitative features that represent the quality of the motion of a dance. Then, we find meaningful low-dimensional structures, removing redundant information but retaining essential information possessing high discrimination power, of the features using SVD (Singular Value Decomposition). Finally, we classify the low-dimensional features into predefined emotional categories using TDMLP (Time Delayed MultiLayer Perceptron). Experimental results demonstrate the validity of the proposed method.\"",
        "Document: \"Self-contained wall-climbing robot with closed link mechanism. A self-contained wall-climbing robot called MRWALLSPECT-II, is presented. The robot has characteristic features in the kinematic design of the leg with closed link mechanism. It reduces the number of actuators compared to the previous ones and makes it possible to have high force-to-weight ratio. Those are enabled by adopting a simple and robust gait pattern mimicking a living creature. Also, as the robot is designed to be a self-contained one, it includes all the components such as air compressors, vacuum generators, an embedded controller, batteries, a CCD camera, sensors, motors and electronics in itself without any tether cable. Its communication with the operating site in the ground is conducted through wireless LAN. In the paper, its principles of design and configuration of the robot are introduced\"",
        "Document: \"Optimal local searching for fast and robust textureless 3D object tracking in highly cluttered backgrounds. Edge-based tracking is a fast and plausible approach for textureless 3D object tracking, but its robustness is still very challenging in highly cluttered backgrounds due to numerous local minima. To overcome this problem, we propose a novel method for fast and robust textureless 3D object tracking in highly cluttered backgrounds. The proposed method is based on optimal local searching of 3D-2D correspondences between a known 3D object model and 2D scene edges in an image with heavy background clutter. In our searching scheme, searching regions are partitioned into three levels (interior, contour, and exterior) with respect to the previous object region, and confident searching directions are determined by evaluating candidates of correspondences on their region levels; thus, the correspondences are searched among likely candidates in only the confident directions instead of searching through all candidates. To ensure the confident searching direction, we also adopt the region appearance, which is efficiently modeled on a newly defined local space (called a searching bundle). Experimental results and performance evaluations demonstrate that our method fully supports fast and robust textureless 3D object tracking even in highly cluttered backgrounds.\"",
        "Document: \"Rapid Generation Of The State Codebook In Side Match Vector Quantization. Side match vector quantization (SMVQ) has been originally developed for image compression and is also useful for steganography. SMVQ requires to create its own state codebook for each block in both encoding and decoding phases. Since the conventional method for the state codebook generation is extremely time-consuming, this letter proposes a fast generation method. The proposed method is tens times faster than the conventional one without loss of perceptual visual quality.\"",
        "Document: \"Robust estimation of camera parameters from image sequence for video composition. In video composition systems, viewing points and image planes of multiple images to be composed have to be properly integrated for natural composed images, especially when some of the images undergo camera operations. Camera parameters are needed for the integration. In this paper, we propose a robust method for the estimation of camera parameters from image sequence. We first establish correspondence of feature points between consecutive image fields. After the establishment, we formulate a nonlinear least-square data fitting problem. When the image sequence contains moving objects, and/or when the correspondence establishment is not successful for some feature points, we get bad observations, outliers. They should be eliminated properly for a good estimation. Therefore, we propose an iterative algorithm for rejecting the outliers and fitting the camera parameters in turn. We attempt to decrease both the number of outliers and the energy of residual errors, given finite observations. Thus, we formulate a budget problem, outlier rejection formula. Then we construct a simple scheme for solving the problem. Finally, we show the validity of the proposed method using computer generated data sets and real image sequences.\"",
        "Document: \"IoT Delegate: Smart Home Framework for Heterogeneous IoT Service Collaboration. With Internet of Things (IoT) technology, home environment becomes smarter than ever. Not only smart devices such as smart phone or smart TV, but also various IoT devices including sensor, smart thermostat, and smart scale has now become very common on the market. These devices have connectivity to the Internet, so that user can read data from the device or control the device using Internet technology. However, due to diversity of smart home requirements, device collaboration in smart home remains a challenging task still. Usually smart home is built with various technologies to fulfill its own purpose, and these purposes cover very wide area from controlling low-power sensor devices to controlling high-performance devices like smart TV and smart phone. This variety of smart home requirements makes smart home very complicated due to mixed network architecture, protocol and technology. In this paper, a framework to enable managing and collaborating heterogeneous IoT devices in smart home environment is proposed. Several programming models are defined in the proposed framework to make application development for heterogeneous devices more intuitive. The proposed framework has been implemented as a web service, and a case study with real-world smart home IoT devices is presented.\"",
        "Document: \"Analytic fusion of visual cues in model-based camera tracking. Model-based camera tracking is a technology that estimates a camera pose by tracking visual cues, i.e. points and edges on a known 3D scene model, in camera images. In model-based camera tracking, it has been a main challenge how to use the visual cues effectively for better performance. In this paper, we carefully analyze the dependency of the visual cues on tracking conditions (or environments) and propose a formula for integrating the visual cues cooperatively into a single framework based on the analysis. Then, we demonstrate that the analytic integration outperforms separate use of either cue and expedient integration of visual cues in arbitrary environments through experiments with synthetic camera images for which ground truth camera poses are given.\"",
        "Document: \"A protocol aided concatenated forward error control for wireless ATM. The knowledge of wireless ATM cell headers in the uplink data bursts can be used to improve the performance of the concatenated FEC system. This paper proposes a protocol aided error control scheme for wireless ATM. The proposed scheme achieves the performance improvement of about 0.2-1.0 dB at a BER of 10(-3) in the fading channels with respect to the conventional concatenated FEC.\"",
        "Document: \"A Mobile Spherical Mosaic System. This paper proposes a new mobile mosaic system using the spherical surface for the panoramic image synthesis. The proposed system consists of image acquisition interface, exposure compensation, local image alignment, spherical projection, and blending. The image acquisition interface guides the users to capture images by displaying a wire frame on the spherical surface. The camera pose and direction are estimated by Gyro sensor and accelerometer. We correct the different brightness levels of images using local means and variances in the overlapped regions. Then, the images are locally realigned since the sensor information has much noise. We implement the successive template matching on the spherical surface, which reduces the sensor errors and misalignment in the 3-D rotational motion. After getting the 3-D rotation information of images, the images are rotated in the virtual 3-D space and projected on the spherical surface. We derive the spherical projection using the radius of virtual mosaic sphere and focal length. Finally, the overlapped multiple images are blended only at the boundaries on the spherical surface. We have implemented the system in the usual tablet PC and mobile phone. According to the various experiments, the proposed spherical mosaic system composes the full environment around the user in real-time.\"",
        "1 is \"Localization Based on Building Recognition\", 2 is \"Vision-based hand pose estimation: A review\"",
        "Given above information, for an author who has written the paper with the title \"Stochastic Approach to Separate Diffuse and Specular Reflections\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008799": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'High performance MPI library over SR-IOV enabled infiniband clusters':",
        "Document: \"Scalable MiniMD Design with Hybrid MPI and OpenSHMEM. The MPI programming model has been widely used for scientific applications. The emergence of Partitioned Global Address Space (PGAS) programming models presents an alternative approach to improve programmability. With the global data view and lightweight communication operations, PGAS has the potential to increase the performance of scientific applications at scale. However, since the PGAS models are emerging, it is unlikely that entire applications will be re-written with them. Instead, unified communication runtimes have paved the way for a new class of hybrid applications that can leverage the benefits of both MPI and PGAS models. In this paper, we re-design an existing MPI based scientific mini-application (MiniMD) with MPI and OpenSHMEM programming models. We propose two alternative designs using MPI and OpenSHMEM programming models and compare performance and scalability of those designs with the original MPI-based implementation. Our performance evaluations using MVAPICH2-X (Unified MPI+PGAS Communication Runtime over InfiniBand) show a 17% reduction in total execution time, compared to existing MPI-based design with 1,024 cores.\"",
        "Document: \"A Comprehensive Study of MapReduce Over Lustre for Intermediate Data Placement and Shuffle Strategies on HPC Clusters. With high performance interconnects and parallel file systems, running MapReduce over modern High Performance Computing (HPC) clusters has attracted much attention due to its uniqueness of solving data analytics problems with a combination of Big Data and HPC technologies. Since the MapReduce architecture relies heavily on the availability of local storage media, the Lustre-based global storage in HPC clusters poses many new opportunities and challenges. In this paper, we perform a comprehensive study on different MapReduce over Lustre deployments and propose a novel high-performance design of YARN MapReduce on HPC clusters by utilizing Lustre as the additional storage provider for intermediate data. With a deployment architecture where both local disks and Lustre are utilized for intermediate data storage, we propose a novel priority directory selection scheme through which RDMA-enhanced MapReduce can choose the best intermediate storage during runtime by on-line profiling. Our results indicate that, we can achieve 44 percent performance benefit for shuffle-intensive workloads in leadership-class HPC systems. Our priority directory selection scheme can improve the job execution time by 63 percent over default MapReduce while executing multiple concurrent jobs. To the best of our knowledge, this is the first such comprehensive study for YARN MapReduce with Lustre and RDMA.\"",
        "Document: \"Modeling and Designing Fault-Tolerance Mechanisms for MPI-Based MapReduce Data Computing Framework. Fault-tolerance is a significant property for distributed and parallel computing systems. An emerging trend of Big Data computing is to combine MPI and MapReduce technologies in a single framework. The distinctive state model in this kind of frameworks brings challenges to designing an efficient and transparent fault-tolerance mechanism. In this paper, a state model analysis method is proposed for uniformly modeling independent MPI, MapReduce and MPI-based MapReduce data computing frameworks. Based on this analysis, a library-level fault-tolerance mechanism with global persistent state model is proposed, a data-staging and routine-sharing based checkpoint approach is designed within this mechanism. The proposed mechanism has been implemented in DataMPI, a communication library supporting MPI-based MapReduce data computing applications. The experiments show that it can transparently enable fault-tolerance for applications. Taking TeraSort as an example, it introduces only 6.8% time overhead and 11% space overhead. For a failure-resume execution, it has a 10%-32% performance advantage compared with the naive checkpoint solutions based on local or parallel storages. The proposed mechanism also provides superior performance and resource utilization compared with Hadoop for both fault-free and failure-resume executions.\"",
        "Document: \"Cutting the Tail: Designing High Performance Message Brokers to Reduce Tail Latencies in Stream Processing. Over the last decade, organizations have become heavily reliant on providing near-instantaneous insights to the end user based on vast amounts of data collected from various sources in real-time. In order to accomplish this task, a stream processing pipeline is constructed, which in its most basic form, consists of a Stream Processing Engine (SPE) and a Message Broker (MB). The SPE is responsible for performing actual computations on the data and providing insights from it. MB, on the other hand, acts as an intermediate queue to which data is written by ephemeral sources and then fetched by the SPE to perform computations on. Due to the inherent real-time nature of such a pipeline, low latency is a highly desirable feature for them. Thus, several existing research works in the community focus on improving latency and throughput of the streaming pipeline. However, there is a dearth of studies optimizing the tail latencies of such pipelines. Moreover, the root cause of this high tail latency is still vague. In this paper, we propose a model-based approach to analyze in-depth the reasons behind high tail latency in streaming systems such as Apache Kafka. Having found the MB to be a major contributor of messages with high tail latencies in a streaming pipeline, we design and implement an RDMA-enhanced high-performance MB, called Frieda, with the higher goal of accelerating any arbitrary stream processing pipeline regardless of the SPE used. Our experiments show a reduction of up to 98% in 99.9th percentile latency for microbenchmarks and up to 31% for full-fledged stream processing pipeline constructed using Yahoo! Streaming Benchmark.\"",
        "Document: \"High-Performance Coarray Fortran Support with MVAPICH2-X: Initial Experience and Evaluation. Coarray Fortran (CAF) is a parallel programming paradigm that extends Fortran for the partitioned global address space (PGAS) programming model at the language level. The current runtime implementations of CAF are mainly using MPI or GASNet as underlying communication components. MVAPICH2-X is a hybrid MPI+PGAS programming library with a Unified Communication Runtime (UCR) design. In this paper, the classic implementation of CAF runtime in Open UH is redesigned and rebuilt on top of MVAPICH2-X. The proposed design does not only enable the support of MPI+CAF hybrid programming model, but also provides superior performance on most of the CAF one-sided operations and the newly proposed collective operations in Fortran 2015 specification. A comprehensive evaluation with different benchmarks and applications has been performed. Comparing with current GASNet-based solutions, the CAF runtime with MVAPICH2-X can improve the bandwidths of put and bidirectional operations up to 3.5X for inter-node communication, and improve the bandwidths of collective communication operations represented by broadcast up to 3.0X on 64 processes. It also reduces the execution time of NPB CAF benchmarks by up to 18% on 256 processes.\"",
        "Document: \"Swift-X: Accelerating OpenStack Swift with RDMA for Building an Efficient HPC Cloud. Running Big Data applications in the cloud has become extremely popular in recent times. To enable the storage of data for these applications, cloud-based distributed storage solutions are a must. OpenStack Swift is an object storage service which is widely used for such purposes. Swift is one of the main components of the OpenStack software package. Although Swift has become extremely popular in recent times, its proxy server based design limits the overall throughput and scalability of the cluster. Swift is based on the traditional TCP/IP sockets based communication which has known performance issues such as context-switch and buffer copies for each message transfer. Modern high-performance interconnects such as InfiniBand and RoCE offer advanced features such as RDMA and provide high bandwidth and low latency communication. In this paper, we propose two new designs to improve the performance and scalability of Swift. We propose changes to the Swift architecture and operation design. We propose high-performance implementations of network communication and I/O modules based on RDMA to provide the fastest possible object transfer. In addition, we use efficient hashing algorithms to accelerate object verification in Swift. Experimental evaluations with microbenchmarks, Swift stack benchmark (ssbench), and synthetic application workloads reveal up to 2x and 7.3x performance improvement with our two proposed designs for put and get operations. To the best of our knowledge, this is the first work towards accelerating OpenStack Swift with RDMA over high-performance interconnects in the literature.\"",
        "Document: \"Scalable Graph500 design with MPI-3 RMA. The MPI two-sided programming model has been widely used for scientific applications. However, the benefits of MPI one-sided communication are still not well exploited. Recently, MPI-3 Remote Memory Access (RMA) was introduced with several advanced features which provide better performance, programmability, and flexibility over MPI-2 RMA. However, few studies have shown the benefits of using MPI-3 RMA for scientific applications. In this paper, we take advantage of the new features from MPI-3 RMA to re-design a scalable Graph500 benchmark. Our design achieves much better overlap of communication and computation than the default two sided based implementation. The results show that the proposed design can achieve up to 2X improvement compared with the best MPI based implementation running with 4,096 cores. To the best of our knowledge, this is the first paper to re-design a high performance and scalable Graph500 with MPI-3 RMA.\"",
        "Document: \"DataMPI: Extending MPI to Hadoop-Like Big Data Computing. MPI has been widely used in High Performance Computing. In contrast, such efficient communication support is lacking in the field of Big Data Computing, where communication is realized by time consuming techniques such as HTTP/RPC. This paper takes a step in bridging these two fields by extending MPI to support Hadoop-like Big Data Computing jobs, where processing and communication of a large number of key-value pair instances are needed through distributed computation models such as MapReduce, Iteration, and Streaming. We abstract the characteristics of key-value communication patterns into a bipartite communication model, which reveals four distinctions from MPI: Dichotomic, Dynamic, Data-centric, and Diversified features. Utilizing this model, we propose the specification of a minimalistic extension to MPI. An open source communication library, DataMPI, is developed to implement this specification. Performance experiments show that DataMPI has significant advantages in performance and flexibility, while maintaining high productivity, scalability, and fault tolerance of Hadoop.\"",
        "Document: \"High performance MPI library over SR-IOV enabled infiniband clusters. Virtualization has become a central role in HPC Cloud due to easy management and low cost of computation and communication. Recently, Single Root I/O Virtualization (SR-IOV) technology has been introduced for high-performance interconnects such as InfiniBand and can attain near to native performance for inter-node communication. However, the SR-IOV scheme lacks locality aware communication support, which leads to performance overheads for inter-VM communication within a same physical node. To address this issue, this paper first proposes a high performance design of MPI library over SR-IOV enabled InfiniBand clusters by dynamically detecting VM locality and coordinating data movements between SR-IOV and Inter-VM shared memory (IVShmem) channels. Through our proposed design, MPI applications running in virtualized mode can achieve efficient locality-aware communication on SR-IOV enabled InfiniBand clusters. In addition, we optimize communications in IVShmem and SR-IOV channels by analyzing the performance impact of core mechanisms and parameters inside MPI library to deliver better performance in virtual machines. Finally, we conduct comprehensive performance studies by using point-to-point and collective benchmarks, and HPC applications. Experimental evaluations show that our proposed MPI library design can significantly improve the performance for point-to-point and collective operations, and MPI applications with different InfiniBand transport protocols (RC and UD) by up to 158%, 76%, 43%, respectively, compared with SR-IOV. To the best of our knowledge, this is the first study to offer a high performance MPI library that supports efficient locality aware MPI communication over SR-IOV enabled InfiniBand clusters.\"",
        "Document: \"Can Inter-VM Shmem Benefit MPI Applications on SR-IOV Based Virtualized Infiniband Clusters?. Single Root I/O Virtualization (SR-IOV) technology has been introduced for high-performance interconnects such as InfiniBand. Recent studies mainly focus on performance characteristics of high-performance communication middleware (e.g. MPI) and applications on SR-IOV enabled HPC clusters. However, current SR-IOV based MPI applications do not take advantage of the locality-aware communication on intra-host inter-VM environment. Although Inter-VM Shared Memory (IVShmem) has been proven to support efficient locality-aware communication, the performance benefits of IVShmem for MPI libraries on virtualized environments are yet to be explored. In this paper, we present a comprehensive performance evaluation for IVShmem backed MPI using micro-benchmarks and HPC applications. The performance evaluations show that, through IVShmem, the performance of MPI point-to-point and collective operations can be improved up to 193% and 91%, respectively. The application performance can be improved up to 96%, compared to SR-IOV. The results further show that IVShmem just brings minor overhead compared to native environment.\"",
        "1 is \"Flattening on the Fly: Efficient Handling of MPI Derived Datatypes\", 2 is \"Pvfs Over Infiniband: Design And Performance Evaluation\"",
        "Given above information, for an author who has written the paper with the title \"High performance MPI library over SR-IOV enabled infiniband clusters\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008838": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Combining text and audio-visual features in video indexing':",
        "Document: \"Efficient search in document image collections. This paper presents an efficient indexing and retrieval scheme for searching in document image databases. In many non-European languages, optical character recognizers are not very accurate. Word spotting - word image matching - may instead be used to retrieve word images in response to a word image query. The approaches used for word spotting so far, dynamic time warping and/or nearest neighbor search, tend to be slow. Here indexing is done using locality sensitive hashing (LSH) - a technique which computes multiple hashes - using word image features computed at word level. Efficiency and scalability is achieved by content-sensitive hashing implemented through approximate nearest neighbor computation. We demonstrate that the technique achieves high precision and recall (in the 90% range), using a large image corpus consisting of seven Kalidasa's (a well known Indian poet of antiquity) books in the Telugu language. The accuracy is comparable to using dynamic time warping and nearest neighbor search while the speed is orders of magnitude better - 20000 word images can be searched in milliseconds.\"",
        "Document: \"On Influence of Line Segmentation in Efficient Word Segmentation in Old Manuscripts. The objective of this work is to show the importance of a good line segmentation to obtain better results in the segmentation of words of historical documents. We have used the approach developed by Manmatha and Rothfeder to segment words in old handwritten documents. In their work the lines of the documents are extracted using projections. In this work, we have developed an approach to segment lines more efficiently. The new line segmentation algorithm tackles with skewed, touching and noisy lines, so it is significantly improves word segmentation. Experiments using Spanish documents from the Marriages Database of the Barcelona Cathedral show that this approach reduces the error rate by more than 20%.\"",
        "Document: \"Large scale document image retrieval by automatic word annotation. In this paper, we present a practical and scalable retrieval framework for large-scale document image collections, for an Indian language script that does not have a robust OCR. OCR-based methods face difficulties in character segmentation and recognition, especially for the complex Indian language scripts. We realize that character recognition is only an intermediate step toward actually labeling words. Hence, we re-pose the problem as one of directly performing word annotation. This new approach has better recognition performance, as well as easier segmentation requirements. However, the number of classes in word annotation is much larger than those for character recognition, making such a classification scheme expensive to train and test. To address this issue, we present a novel framework that replaces naive classification with a carefully designed mixture of indexing and classification schemes. This enables us to build a search system over a large collection of 1,000 books of Telugu, consisting of 120K document images or 36M individual words. This is the largest searchable document image collection for a script without an OCR that we are aware of. Our retrieval system performs significantly well with a mean average precision of 0.8.\"",
        "Document: \"A Novel Word Spotting Method Based on Recurrent Neural Networks. Keyword spotting refers to the process of retrieving all instances of a given keyword from a document. In the present paper, a novel keyword spotting method for handwritten documents is described. It is derived from a neural network-based system for unconstrained handwriting recognition. As such it performs template-free spotting, i.e., it is not necessary for a keyword to appear in the training set. The keyword spotting is done using a modification of the CTC Token Passing algorithm in conjunction with a recurrent neural network. We demonstrate that the proposed systems outperform not only a classical dynamic time warping-based approach but also a modern keyword spotting system, based on hidden Markov models. Furthermore, we analyze the performance of the underlying neural networks when using them in a recognition task followed by keyword spotting on the produced transcription. We point out the advantages of keyword spotting when compared to classic text line recognition.\"",
        "Document: \"Finding text in images.  There are many applications in which the automatic detection and recognition of text embedded in images isuseful. These applications include multimedia systems, digital libraries, and Geographical Information Systems.When machine generated text is printed against clean backgrounds, it can be converted to a computer readbleform (ASCII) using current Optical Character Recognition (OCR) technology. However, text is often printedagainst shaded or textured backgrounds or is embedded in images.... \"",
        "Document: \"A scale space approach for automatically segmenting words from historical handwritten documents. Many libraries, museums, and other organizations contain large collections of handwritten historical documents, for example, the papers of early presidents like George Washington at the Library of Congress. The first step in providing recognition/ retrieval tools is to automatically segment handwritten pages into words. State of the art segmentation techniques like the gap metrics algorithm have been mostly developed and tested on highly constrained documents like bank checks and postal addresses. There has been little work on full handwritten pages and this work has usually involved testing on clean artificial documents created for the purpose of research. Historical manuscript images, on the other hand, contain a great deal of noise and are much more challenging. Here, a novel scale space algorithm for automatically segmenting handwritten (historical) documents into words is described. First, the page is cleaned to remove margins. This is followed by a gray-level projection profile algorithm for finding lines in images. Each line image is then filtered with an anisotropic Laplacian at several scales. This procedure produces blobs which correspond to portions of characters at small scales and to words at larger scales. Crucial to the algorithm is scale selection, that is, finding the optimum scale at which blobs correspond to words. This is done by finding the maximum over scale of the extent or area of the blobs. This scale maximum is estimated using three different approaches. The blobs recovered at the optimum scale are then bounded with a rectangular box to recover the words. A postprocessing filtering step is performed to eliminate boxes of unusual size which are unlikely to correspond to words. The approach is tested on a number of different data sets and it is shown that, on 100 sampled documents from the George Washington corpus of handwritten document images, a total error rate of 17 percent is observed. The technique outperforms a state-of-the-art gap metrics word-segmentation algorithm on this collection.\"",
        "Document: \"Holistic Word Recognition for Handwritten Historical Documents. Most offline handwriting recognition approaches proceed by segmenting words into smaller pieces (usually characters) which are recognized separately. The recognition result of a word is then the composition of the individually recognized parts. Inspired by results in cognitive psychology, researchers have begun to focus on holistic word recognition approaches. Here we present a holistic word recognition approach for single-author historical documents, which is motivated by the fact that for severely degraded documents a segmentation of words into characters will produce very poor results. The quality of the original documents does not allow us to recognize them with high accuracy - our goal here is to produce transcriptions that will allow successful retrieval of images, which has been shown to be feasible even in such noisy environments.We believe that this is the first systematic approach to recognizing words in historical manuscripts with extensive experiments. Our experiments show recognition accuracy of 65%, which exceeds performance of other systems which operate on non-degraded input images (non historical documents).\"",
        "Document: \"BLSTM Neural Network Based Word Retrieval for Hindi Documents. Retrieval from Hindi document image collections is a challenging task. This is partly due to the complexity of the script, which has more than 800 unique ligatures. In addition, segmentation and recognition of individual characters often becomes difficult due to the writing style as well as degradations in the print. For these reasons, robust OCRs are non existent for Hindi. Therefore, Hindi document repositories are not amenable to indexing and retrieval. In this paper, we propose a scheme for retrieving relevant Hindi documents in response to a query word. This approach uses BLSTM neural networks. Designed to take contextual information into account, these networks can handle word images that can not be robustly segmented into individual characters. By zoning the Hindi words, we simplify the problem and obtain high retrieval rates. Our simplification suits the retrieval problem, while it does not apply to recognition. Our scalable retrieval scheme avoids explicit recognition of characters. An experimental evaluation on a dataset of word images gathered from two complete books demonstrates good accuracy even in the presence of printing variations and degradations. The performance is compared with baseline methods.\"",
        "Document: \"Finding words in alphabet soup: Inference on freeform character recognition for historical scripts. This paper develops word recognition methods for historical handwritten cursive and printed documents. It employs a powerful segmentation-free letter detection method based upon joint boosting with histograms of gradients as features. Efficient inference on an ensemble of hidden Markov models can select the most probable sequence of candidate character detections to recognize complete words in ambiguous handwritten text, drawing on character n-gram and physical separation models. Experiments with two corpora of handwritten historic documents show that this approach recognizes known words more accurately than previous efforts, and can also recognize out-of-vocabulary words.\"",
        "Document: \"Partial duplicate detection for large book collections. A framework is presented for discovering partial duplicates in large collections of scanned books with optical character recognition (OCR) errors. Each book in the collection is represented by the sequence of words (in the order they appear in the text) which appear only once in the book. These words are referred to as \"unique words\" and they constitute a small percentage of all the words in a typical book. Along with the order information the set of unique words provides a compact representation which is highly descriptive of the content and the flow of ideas in the book. By aligning the sequence of unique words from two books using the longest common subsequence (LCS) one can discover whether two books are duplicates. Experiments on several datasets show that DUPNIQ is more accurate than traditional methods for duplicate detection such as shingling and is fast. On a collection of 100K scanned English books DUPNIQ detects partial duplicates in 30 min using 350 cores and has precision 0.996 and recall 0.833 compared to shingling with precision 0.992 and recall 0.720. The technique works on other languages as well and is demonstrated for a French dataset.\"",
        "1 is \"Detecting cultural differences using consumer-generated geotagged photos\", 2 is \"Compiling Bilingual Lexicon Entries From a Non-Parallel English-Chinese Corpus\"",
        "Given above information, for an author who has written the paper with the title \"Combining text and audio-visual features in video indexing\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008923": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'New ratios for the detection and classification of CJD in multisequence MRI of the brain':",
        "Document: \"Optimal medial surface generation for anatomical volume representations. Medial representations are a widely used technique in abdominal organ shape representation and parametrization. Those methods require good medial manifolds as a starting point. Any medial surface used to parameterize a volume should be simple enough to allow an easy manipulation and complete enough to allow an accurate reconstruction of the volume. Obtaining good quality medial surfaces is still a problem with current iterative thinning methods. This forces the usage of generic, pre-calculated medial templates that are adapted to the final shape at the cost of a drop in volume reconstruction. This paper describes an operator for generation of medial structures that generates clean and complete manifolds well suited for their further use in medial representations of abdominal organ volumes. While being simpler than thinning surfaces, experiments show its high performance in volume reconstruction and preservation of medial surface main branching topology.\"",
        "Document: \"Marginal shape deep learning: applications to pediatric lung field segmentation. Representation learning through deep learning (DL) architecture has shown tremendous potential for identification, localization, and texture classification in various medical imaging modalities. However, DL applications to segmentation of objects especially to deformable objects are rather limited and mostly restricted to pixel classification. In this work, we propose marginal shape deep learning (MaShDL), a framework that extends the application of DL to deformable shape segmentation by using deep classifiers to estimate the shape parameters. MaShDL combines the strength of statistical shape models with the automated feature learning architecture of DL. Unlike the iterative shape parameters estimation approach of classical shape models that often leads to a local minima, the proposed framework is robust to local minima optimization and illumination changes. Furthermore, since the direct application of DL framework to a multi-parameter estimation problem results in a very high complexity, our framework provides an excellent run-time performance solution by independently learning shape parameter classifiers in marginal eigenspaces in the decreasing order of variation. We evaluated MaShDL for segmenting the lung field from 314 normal and abnormal pediatric chest radiographs and obtained a mean Dice similarity coefficient of 0.927 using only the four highest modes of variation (compared to 0.888 with classical ASM(1) (p-value=0.01) using same configuration). To the best of our knowledge this is the first demonstration of using DL framework for parametrized shape learning for the delineation of deformable objects.\"",
        "Document: \"Generic Method For Intensity Standardization Of Medical Images Using Multiscale Curvelet Representation. Most computer-aided diagnosis (CAD) methods in medical imaging are finely tuned for the settings of training data, i.e., the acquisition protocol and machine settings. Therefore, they may fail to perform optimally on images acquired under a different protocol. Intensity standardization, or mapping the acquired data to a predefined intensity profile, can alleviate this challenge. In this work, we present a generic method for intensity standardization of 2D/3D medical images using localized subband energy scaling of the multi-scale curvelet transform. During the training phase, reference data are first decomposed into scale and orientation localized subbands using the multiscale curvelet transform, followed by calculating a reference energy value for each subband. During the testing stage, the localized energy of each subband is scaled to the reference localized energy value from the training stage through an iterative process. We validated our generic standardization method on 2D chest Xrays (CXR) and 3D T1-weighted MRI sequences acquired using different scanners on a group of both healthy and diseased subjects. A significant improvement (Dice coefficient of 0.91 +/- 0.05 versus 0.68 +/- 0.13, p-value<0.001) was obtained in the whole brain segmentation accuracy after standardization. Similarly, for air-trapping quantification, the standardization improved the correlation with the expert visual assessment of air-trapping from CXR from R=0.32 to R=0.93. The proposed intensity standardization technique could be adopted as a pre-processing step for 2D and 3D data to improve the accuracy of CAD on data obtained from variable sources.\"",
        "Document: \"Partitioned Shape Modeling with On-the-Fly Sparse Appearance Learning for Anterior Visual Pathway Segmentation.   MRI quantification of cranial nerves such as anterior visual pathway (AVP) in MRI is challenging due to their thin small size, structural variation along its path, and adjacent anatomic structures. Segmentation of pathologically abnormal optic nerve (e.g. optic nerve glioma) poses additional challenges due to changes in its shape at unpredictable locations. In this work, we propose a partitioned joint statistical shape model approach with sparse appearance learning for the segmentation of healthy and pathological AVP. Our main contributions are: (1) optimally partitioned statistical shape models for the AVP based on regional shape variations for greater local flexibility of statistical shape model; (2) refinement model to accommodate pathological regions as well as areas of subtle variation by training the model on-the-fly using the initial segmentation obtained in (1); (3) hierarchical deformable framework to incorporate scale information in partitioned shape and appearance models. Our method, entitled PAScAL (PArtitioned Shape and Appearance Learning), was evaluated on 21 MRI scans (15 healthy + 6 glioma cases) from pediatric patients (ages 2-17). The experimental results show that the proposed localized shape and sparse appearance-based learning approach significantly outperforms segmentation approaches in the analysis of pathological data. \"",
        "Document: \"Quantification of kidneys from 3D ultrasound in pediatric hydronephrosis. This paper introduces a complete framework for the quantification of renal structures (parenchyma, and collecting system) in 3D ultrasound (US) images. First, the segmentation of the kidney is performed using Gabor-based appearance models (GAM), a variant of the popular active shape models, properly tailored to the imaging physics of US image data. The framework also includes a new graph-cut based method for the segmentation of the collecting system, including brightness and contrast normalization, and positional prior information. The significant advantage (p = 0.03) of the new method over previous approaches in terms of segmentation accuracy has been successfully verified on clinical 3DUS data from pediatric cases with hydronephrosis. The promising results obtained in the estimation of the volumetric hydronephrosis index demonstrate the potential of our new framework to quantify anatomy in US and asses the severity of hydronephrosis.\"",
        "Document: \"Automatic Analysis of Pediatric Renal Ultrasound Using Shape, Anatomical and Image Acquisition Priors. In this paper we present a segmentation method for ultrasound (US) images of the pediatric kidney, a difficult and barely studied problem. Our method segments the kidney on 2D sagittal US images and relies on minimal user intervention and a combination of improvements made to the Active Shape Model (ASM) framework. Our contributions include particle swarm initialization and profile training with rotation correction. We also introduce our methodology for segmentation of the kidney's collecting system (CS), based on graph-cuts (GC) with intensity and positional priors. Our intensity model corrects for intensity bias by comparison with other biased versions of the most similar kidneys in the training set. We prove significant improvements (p < 0.001) with respect to classic ASM and GC for kidney and CS segmentation, respectively. We use our semi-automatic method to compute the hydronephrosis index (HI) with an average error of 2.67 +/- 5.22 percentage points similar to the error of manual HI between different operators of 2.31 +/- 4.54 percentage points.\"",
        "Document: \"Multi-organ segmentation with missing organs in abdominal CT images. Currently, multi-organ segmentation (MOS) in abdominal CT can fail to handle clinical patient population with missing organs due to surgical resection. In order to enable the state-of-the-art MOS for these clinically important cases, we propose 1) automatic missing organ detection (MOD) by testing abnormality of post-surgical organ motion and organ-specific intensity homogeneity, and 2) atlas-based MOS of 10 abdominal organs that handles missing organs automatically. The proposed methods are validated with 44 abdominal CT scans including 9 diseased cases with surgical organ resections, resulting in 93.3% accuracy for MOD and improved overall segmentation accuracy by the proposed MOS method when tested on difficult diseased cases.\"",
        "Document: \"Multi-organ segmentation from multi-phase abdominal CT via 4D graphs using enhancement, shape and location optimization. The interpretation of medical images benefits from anatomical and physiological priors to optimize computer-aided diagnosis (CAD) applications. Diagnosis also relies on the comprehensive analysis of multiple organs and quantitative measures of soft tissue. An automated method optimized for medical image data is presented for the simultaneous segmentation of four abdominal organs from 4D CT data using graph cuts. Contrast-enhanced CT scans were obtained at two phases: non-contrast and portal venous. Intra-patient data were spatially normalized by non-linear registration. Then 4D erosion using population historic information of contrast-enhanced liver, spleen, and kidneys was applied to multi-phase data to initialize the 4D graph and adapt to patient specific data. CT enhancement information and constraints on shape, from Parzen windows, and location, from a probabilistic atlas, were input into a new formulation of a 4D graph. Comparative results demonstrate the effects of appearance and enhancement, and shape and location on organ segmentation.\"",
        "Document: \"Atrial septal defect tracking in 3d cardiac ultrasound. We are working to develop beating-heart atrial septal defect (ASD) closure techniques using real-time 3-D ultrasound guidance. The major image processing challenges are the low image quality and the high frame rate. This paper presents comparative results for ASD tracking in sequences of 3D cardiac ultrasound. We introduce a block flow technique, which combines the velocity computation from optical flow for an entire block with template matching. Enforcing similarity constraints to both the previous and first frames ensures optimal and unique solutions. We compare the performance of the proposed algorithm with that of block matching and optical flow on six in-vivo 4D datasets acquired from porcine beating-heart procedures. Results show that our technique is more stable and has higher sensitivity than both optical flow and block matching in tracking ASDs. Computing velocity at the block level, our technique is much faster than optical flow and comparable in computation cost to block matching.\"",
        "Document: \"Hierarchical constrained local model using ICA and its application to Down syndrome detection. Conventional statistical shape models use Principal Component Analysis (PCA) to describe shape variations. However, such a PCA-based model assumes a Gaussian distribution of data. A model with Independent Component Analysis (ICA) does not require the Gaussian assumption and can additionally describe the local shape variation. In this paper, we propose a Hierarchical Constrained Local Model (HCLM) using ICA. The first or coarse level of HCLM locates the full landmark set, while the second level refines a relevant landmark subset. We then apply the HCLM to Down syndrome detection from photographs of young pediatric patients. Down syndrome is the most common chromosomal condition and its early detection is crucial. After locating facial anatomical landmarks using HCLM, geometric and local texture features are extracted and selected. A variety of classifiers are evaluated to identify Down syndrome from a healthy population. The best performance achieved 95.6% accuracy using support vector machine with radial basis function kernel. The results show that the ICA-based HCLM outperformed both PCA-based CLM and ICA-based CLM.\"",
        "1 is \"Animal: Validation And Applications Of Nonlinear Registration-Based Segmentation\", 2 is \"Automatic multi-resolution shape modeling of multi-organ structures.\"",
        "Given above information, for an author who has written the paper with the title \"New ratios for the detection and classification of CJD in multisequence MRI of the brain\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "008998": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'RTP: robust tenant placement for elastic in-memory database clusters':",
        "Document: \"Elastic online analytical processing on RAMCloud. A shared-nothing architecture is state-of-the-art for deploying a distributed analytical in-memory database management system: it preserves the in-memory performance advantage by processing data locally on each node but is difficult to scale out. Modern switched fabric communication links such as InfiniBand narrow the performance gap between local and remote DRAM data access to a single order of magnitude. Based on these premises, we introduce a distributed in-memory database architecture that separates the query execution engine and data access: this enables a) the usage of a large-scale DRAM-based storage system such as Stanford's RAMCloud and b) the push-down of bandwidth-intensive database operators into the storage system. We address the resulting challenges such as finding the optimal operator execution strategy and partitioning scheme. We demonstrate that such an architecture delivers both: the elasticity of a shared-storage approach and the performance characteristics of operating on local DRAM.\"",
        "Document: \"RTP: robust tenant placement for elastic in-memory database clusters. In the cloud services industry, a key issue for cloud operators is to minimize operational costs. In this paper, we consider algorithms that elastically contract and expand a cluster of in-memory databases depending on tenants' behavior over time while maintaining response time guarantees. We evaluate our tenant placement algorithms using traces obtained from one of SAP's production on-demand applications. Our experiments reveal that our approach lowers operating costs for the database cluster of this application by a factor of 2.2 to 10, measured in Amazon EC2 hourly rates, in comparison to the state of the art. In addition, we carefully study the trade-off between cost savings obtained by continuously migrating tenants and the robustness of servers towards load spikes and failures.\"",
        "Document: \"Concurrent Execution Of Mixed Enterprise Workloads On In-Memory Databases. In the world of enterprise computing, single applications are often classified either as transactional or analytical. From a data management perspective, both application classes issue a database workload with commonly agreed characteristics. However, traditional database management systems (DBMS) are typically optimized for one or the other. Today, we see two trends in enterprise applications that require bridging these two workload categories: (1) enterprise applications of both classes access a single database instance and (2) longer-running, analytical-style queries issued by transactional applications. As a reaction to this change, in-memory DBMS on multi-core CPUs have been proposed to handle the mix of transactional and analytical queries in a single database instance. However, running heterogeneous queries potentially causes situations where longer running queries block shorter running queries from execution. A task-based query execution model with priority-based scheduling allows for an effective prioritization of query classes. This paper discusses the impact of task granularity on responsiveness and throughput of an in-memory DBMS. We show that a larger task size for long running operators negatively affects the response time of short running queries. Based on this observation, we propose a solution to limit the maximum task size with the objective of controlling the mutual performance impact of query classes.\"",
        "Document: \"Data Loading and Caching Strategies in Service-Oriented Enterprise Applications. The use of distributed functionality through service interfaces provides enhanced application functionality. In contrast to traditional desktop applications, enterprise applications have to consider performance aspects as a result of the distributed application setup. Cost models identified data transmissions as cost-drivers in service-oriented applications because of their substantial size.An object cache helps to avoid redundant service calls and it improves perceived application performance. We present implementation details about our abstract application layer for consumption of enterprise services providing dynamic load and transparent caching methods. Dynamic loading of business object data facilitates comfortable use of enterprise services by clients without expertise on service implementations.We examine various server- and client-side caching strategies and implemented a performance testbed. Our performance results are discussed and solutions forenterprise architectures are drawn.\"",
        "Document: \"Fast updates on read-optimized databases using multi-core CPUs. Read-optimized columnar databases use differential updates to handle writes by maintaining a separate write-optimized delta partition which is periodically merged with the read-optimized and compressed main partition. This merge process introduces significant overheads and unacceptable downtimes in update intensive systems, aspiring to combine transactional and analytical workloads into one system. In the first part of the paper, we report data analyses of 12 SAP Business Suite customer systems. In the second half, we present an optimized merge process reducing the merge overhead of current systems by a factor of 30. Our linear-time merge algorithm exploits the underlying high compute and bandwidth resources of modern multi-core CPUs with architecture-aware optimizations and efficient parallelization. This enables compressed in-memory column stores to handle the transactional update rate required by enterprise applications, while keeping properties of read-optimized databases for analytic-style queries.\"",
        "Document: \"Lightweight collection and storage of software repository data with DataRover. The ease of setting up collaboration infrastructures for software engineering projects creates a challenge for researchers that aim to analyze the resulting data. As teams can choose from various available software-as-a-service solutions and can configure them with a few clicks, researchers have to create and maintain multiple implementations for collecting and aggregating the collaboration data in order to perform their analyses across different setups. The DataRover system presented in this paper simplifies this task by only requiring custom source code for API authentication and querying. Data transformation and linkage is performed based on mappings, which users can define based on sample responses through a graphical front end. This allows storing the same input data in formats and databases most suitable for the intended analysis without requiring additional coding. Furthermore, API responses are continuously monitored to detect changes and allow users to update their mappings and data collectors accordingly. A screencast of the described use cases is available at\"",
        "Document: \"An adaptive aggregate maintenance approach for mixed workloads in columnar in-memory databases. The mixed database workloads generated by enterprise applications can be categorized into short-running transactional as well as long-running analytical queries with resource-intensive data aggregations. The introduction of materialized views can accelerate the execution of aggregate queries significantly. However, the overhead of materialized view maintenance has to be taken into account and varies mainly depending on the ratio of queries accessing the materialized view to queries altering the base data, which we define as insert ratio. On the basis of our constructed cost models for the identified materialized view maintenance strategies, we can determine the best performing strategy for the currently monitored workload. While a naive switching approach already improves the performance over staying with a single maintenance strategy, we show that an adaptive aggregate maintenance approach with inclusion of the workload history and switching costs can further improve the overall performance of a mixed workload. This behavior is demonstrated with benchmarks in a columnar in-memory database.\"",
        "Document: \"ScrumLint: identifying violations of agile practices using development artifacts. Linting tools automatically identify source code fragments that do not follow a set of predefined standards. Such feedback tools are equally desirable for \\\"linting\\\" agile development processes. However, providing concrete feedback on process conformance is a challenging task, due to the intentional lack of formal agile process models. In this paper, we present ScrumLint, a tool that tackles this issue by analyzing development artifacts. On the basis of experiences with an undergraduate agile software engineering course, we defined a collection of process metrics. These contain the core ideas of agile methods and report deviations. Using this approach, development teams receive immediate feedback on their executed development practices. They can use this knowledge to improve their workflows, or can adapt the metrics to better reflect their project reality.\"",
        "Document: \"Facing the genome data deluge: efficiently identifying genetic variants with in-memory database technology. Next-generation sequencing enables whole genome sequencing within a few hours at a minimum of cost. However, this technology imposes new challenges to computational genome analysis tasks in terms of efficiently processing an increasing amount of error-prone data.\n\nIn this work, we focus on addressing these challenges for identifying Single Nucleotide Polymorphisms as one type of genetic variants in genome data. We propose the application of a column-store in-memory database for efficient data processing to profit from built-in compression and parallelization techniques and accessing data directly from main memory instead of slower disk space. We provide a statistical model that is sensitive to input data quality and utilizes knowledge from language population studies. Comparisons with state-of-the-art tools show that our approach outperforms traditional procedures on average by magnitudes of speed whilst requiring less administration efforts.\n\n\"",
        "Document: \"Fused Table Scans: Combining AVX-512 and JIT to Double the Performance of Multi-Predicate Scans. Recent work has started to combine two approaches for faster query execution: Vectorization and Just-in-Time Compilation (JIT). Combining the advantages of block-at-a-time and tuple-at-a-time processing opens up opportunities for performance improvements. In this context, we present the Fused Table Scan, a just-in-time-compiled scan operator for consecutive table scans. It makes use of the Intel AVX-512 instruction set to combine multiple predicates in a single scan. This reduces the number of instruction-level branches and makes memory access more efficient. We show that the Fused Table Scan doubles the scan performance in most cases and can achieve a speed-up of up to a factor of ten over sequential execution. Furthermore, we discuss which query plans profit from the Fused Table Scan and how the operator code can be generated at runtime.\"",
        "1 is \"Data Cleaning: Problems and Current Approaches\", 2 is \"Bridging physical and virtual worlds: complex event processing for RFID data streams\"",
        "Given above information, for an author who has written the paper with the title \"RTP: robust tenant placement for elastic in-memory database clusters\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009063": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'New High-Rate Convolutional Codes For Concatenated Schemes':",
        "Document: \"Error Correcting Coding for a Nonsymmetric Ternary Channel. Ternary channels can be used to model the behavior of some memory devices,\nwhere information is stored in three different levels. In this paper, error\ncorrecting coding for a ternary channel where some of the error transitions are\nnot allowed, is considered. The resulting channel is non-symmetric, therefore\nclassical linear codes are not optimal for this channel. We define the\nmaximum-likelihood (ML) decoding rule for ternary codes over this channel and\nshow that it is complex to compute, since it depends on the channel error\nprobability. A simpler alternative decoding rule which depends only on code\nproperties, called $\\da$-decoding, is then proposed. It is shown that\n$\\da$-decoding and ML decoding are equivalent, i.e., $\\da$-decoding is optimal,\nunder certain conditions. Assuming $\\da$-decoding, we characterize the error\ncorrecting capabilities of ternary codes over the non-symmetric ternary\nchannel. We also derive an upper bound and a constructive lower bound on the\nsize of codes, given the code length and the minimum distance. The results\narising from the constructive lower bound are then compared, for short sizes,\nto optimal codes (in terms of code size) found by a clique-based search. It is\nshown that the proposed construction method gives good codes, and that in some\ncases the codes are optimal.\"",
        "Document: \"Green Communication via Power-Optimized HARQ Protocols. Recently, the efficient use of energy has become an essential research topic for green communication. This paper studies the effect of optimal power controllers on the performance of delay-sensitive communication setups that utilize hybrid automatic repeat request (HARQ). The results are obtained for repetition time diversity (RTD) and incremental redundancy (INR) HARQ protocols. In all cases, the optimal power allocation, minimizing the outage-limited average transmission power, is obtained under both continuous and bursting communication models. Also, we investigate the system throughput in different conditions. The results indicate that the power efficiency is substantially increased if adaptive power allocation is utilized. For example, assume a Rayleigh fading channel with a maximum of two (re)transmission rounds with rates {1, [1/2]} nats-per-channel-use and an outage probability constraint 10 -3 . Then, compared with uniform power allocation, optimal power allocation in RTD reduces the average power by 9 and 11 dB in the bursting and continuous communication models, respectively. In INR, these values are obtained to be 8 and 9 dB, respectively.\"",
        "Document: \"Reconfigurable Analog Decoder For A Serially Concatenated Convolutional Code. In this paper, the design of a fully analog iterative decoder for a serially concatenated convolutional code is presented. The decoder is reconfigurable in both block length and code rate. An interleaver size up to 2400 bit is considered. The decoder core implements a single SISO working on a window of the whole code trellis. It is then reused several times to decode the two constituent codes. The resulting decoder performs iterations, but it is fully analog. The extrinsic information exchanged in the decoding process is stored in an analog memory and permuted through a reconfigurable interleaver. Behavioral analysis of the decoder as well as precision and mismatch impact on performance are reported in the paper.\"",
        "Document: \"Trapping set enumerators for repeat multiple accumulate code ensembles. The serial concatenation of a repetition code with two or more accumulators has the advantage of a simple encoder structure. Furthermore, the resulting ensemble is asymptotically good and exhibits minimum distance growing linearly with block length. However, in practice these codes cannot be decoded by a maximum likelihood decoder, and iterative decoding schemes must be employed. For low-density parity-check codes, the notion of trapping sets has been introduced to estimate the performance of these codes under iterative message passing decoding. In this paper, we present a closed form finite length ensemble trapping set enumerator for repeat multiple accumulate codes by creating a trellis representation of trapping sets. We also obtain the asymptotic expressions when the block length tends to infinity and evaluate them numerically.\"",
        "Document: \"On the Analysis of Weighted Nonbinary Repeat Multiple-Accumulate Codes. In this paper, we consider weighted nonbinary repeat multiple-accumulate\n(WNRMA) code ensembles obtained from the serial concatenation of a nonbinary\nrate-1/n repeat code and the cascade of L>= 1 accumulators, where each encoder\nis followed by a nonbinary random weighter. The WNRMA codes are assumed to be\niteratively decoded using the turbo principle with maximum a posteriori\nconstituent decoders. We derive the exact weight enumerator of nonbinary\naccumulators and subsequently give the weight enumerators for WNRMA code\nensembles. We formally prove that the symbol-wise minimum distance of WNRMA\ncode ensembles asymptotically grows linearly with the block length when L >= 3\nand n >= 2, and L=2 and n >= 3, for all powers of primes q >= 3 considered,\nwhere q is the field size. Thus, WNRMA code ensembles are asymptotically good\nfor these parameters. We also give iterative decoding thresholds, computed by\nan extrinsic information transfer chart analysis, on the q-ary symmetric\nchannel to show the convergence properties. Finally, we consider the binary\nimage of WNRMA code ensembles and compare the asymptotic minimum distance\ngrowth rates with those of binary repeat multiple-accumulate code ensembles.\"",
        "Document: \"Using Short Synchronous WOM Codes to Make WOM Codes Decodable. In the framework of write-once memory (WOM) codes, it is important to distinguish between codes that can be decoded directly and those that require the decoder to know the current generation so as to successfully decode the state of the memory. A widely used approach to constructing WOM codes is to design first nondecodable codes that approach the boundaries of the capacity region and then make them decodable by appending additional cells that store the current generation, at an expense of rate loss. In this paper, we propose an alternative method to making nondecodable WOM codes decodable by appending cells that also store some additional data. The key idea is to append to the original (nondecodable) code a short synchronous WOM code and write generations of the original code and the synchronous code simultaneously. We consider both the binary and the nonbinary case. Furthermore, we propose a construction of synchronous WOM codes, which are then used to make nondecodable codes decodable. For short-to-moderate block lengths, the proposed method significantly reduces the rate loss as compared to the standard method.\"",
        "Document: \"Unifying Analysis and Design of Rate-Compatible Concatenated Codes. An improved concatenated code structure, which generalizes parallel and serially concatenated convolutional codes is presented and investigated. The structure is ideal for designing low-complexity rate-compatible code families with good performance in both the waterfall and error floor regions. As an additional feature, the structure provides a unified analysis and design framework, which includes...\"",
        "Document: \"Design of APSK Constellations for Coherent Optical Channels with Nonlinear Phase Noise. We study the design of amplitude phase-shift keying (APSK) constellations for a coherent fiber-optical communication system where nonlinear phase noise (NLPN) is the main system impairment. APSK constellations can be regarded as a union of phase-shift keying (PSK) signal sets with different amplitude levels. A practical two-stage (TS) detection scheme is analyzed, which performs close to optimal detection for high enough input power. We optimize APSK constellations with 4, 8, and 16 points in terms of symbol error probability (SEP) under TS detection for several combinations of input power and fiber length. For 16 points, performance gains of 3.2 dB can be achieved at a SEP of 10 -2 compared to 16-QAM by choosing an optimized APSK constellation. We also demonstrate that in the presence of severe nonlinear distortions, it may become beneficial to sacrifice a constellation point or an entire constellation ring to reduce the average SEP. Finally, we discuss the problem of selecting a good binary labeling for the found constellations.\"",
        "Document: \"Asymptotic Analysis and Spatial Coupling of Counter Braids. A counter braid (CB) is a novel counter architecture introduced by Lu et al. in 2007 for per-flow measurements on high-speed links which can be decoded with low complexity using message passing (MP). CBs achieve an asymptotic compression rate (under optimal decoding) that matches the entropy lower bound of the flow size distribution. In this paper, we apply the concept of spatial coupling to CBs t...\"",
        "Document: \"On Low-Complexity Decoding of Product Codes for High-Throughput Fiber-Optic Systems. We study low-complexity iterative decoding algorithms for product codes. We revisit two algorithms recently proposed by the authors based on bounded distance decoding (BDD) of the component codes that improve the performance of conventional iterative BDD (iBDD). We then propose a novel decoding algorithm that is based on generalized minimum distance decoding of the component codes. The proposed algorithm closes over 50% of the performance gap between iBDD and turbo product decoding (TPD) based on the Chase-Pyndiah algorithm at a bit error rate of 10\n<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">-5</sup>\n. Moreover, the algorithm only leads to a limited increase in complexity with respect to iBDD and has significantly lower complexity than TPD. The studied algorithms are particularly interesting for high-throughput fiberoptic communications.\"",
        "1 is \"A Terabit/Second Satellite System For European Broadband Access: A Feasibility Study\", 2 is \"Performance analysis of maximum ratio combining with imperfect channel estimation in the presence of cochannel interferences\"",
        "Given above information, for an author who has written the paper with the title \"New High-Rate Convolutional Codes For Concatenated Schemes\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009076": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Cost-performance tradeoff in multi-hop aggregation for statistical inference':",
        "Document: \"Score Function Features for Discriminative Learning: Matrix and Tensor Framework.   Feature learning forms the cornerstone for tackling challenging learning problems in domains such as speech, computer vision and natural language processing. In this paper, we consider a novel class of matrix and tensor-valued features, which can be pre-trained using unlabeled samples. We present efficient algorithms for extracting discriminative information, given these pre-trained features and labeled samples for any related task. Our class of features are based on higher-order score functions, which capture local variations in the probability density function of the input. We establish a theoretical framework to characterize the nature of discriminative information that can be extracted from score-function features, when used in conjunction with labeled samples. We employ efficient spectral decomposition algorithms (on matrices and tensors) for extracting discriminative components. The advantage of employing tensor-valued features is that we can extract richer discriminative information in the form of an overcomplete representations. Thus, we present a novel framework for employing generative models of the input for discriminative learning. \"",
        "Document: \"Cost-performance tradeoff in multi-hop aggregation for statistical inference. The problem of distributed fusion for binary hypothesis testing in a multihop network is considered. The sensor measurements are spatially correlated according to a Markov random field (MRF) under both the hypotheses. A fusion scheme for detection involves selection and localized processing of a subset of sensor measurements, fusion of these processed values to form a sufficient statistic, and its delivery to the fusion center. The goal is to find a fusion scheme that achieves optimal linear tradeoff between the total routing costs and the resulting detection error exponent at the fusion center. The Neyman-Pearson error exponent, under a fixed type-I bound, is shown to be the limit of the normalized sum of the Kullback-Leibler distances (KLD) over the maximal cliques of the MRF under some convergence conditions. It is shown that optimal fusion reduces to a prize- collecting Steiner tree (PCST) with the approximation factor preserved when the cliques of the MRF are disjoint. The PCST is found over an expanded communication graph with virtual nodes added for each non-trivial maximal clique of the MRF and their KLD assigned as the node penalty.\"",
        "Document: \"Unsupervised Learning of Word-Sequence Representations from Scratch via Convolutional Tensor Decomposition. Author(s): Huang, F; Anandkumar, A | Abstract: Unsupervised text embeddings extraction is crucial for text understanding in machine learning. Word2Vec and its variants have received substantial success in mapping words with similar syntactic or semantic meaning to vectors close to each other. However, extracting context-aware word-sequence embedding remains a challenging task. Training over large corpus is difficult as labels are difficult to get. More importantly, it is challenging for pre-trained models to obtain word-sequence embeddings that are universally good for all downstream tasks or for any new datasets. We propose a two-phased ConvDic+DeconvDec framework to solve the problem by combining a word-sequence dictionary learning model with a word-sequence embedding decode model. We propose a convolutional tensor decomposition mechanism to learn good word-sequence phrase dictionary in the learning phase. It is proved to be more accurate and much more efficient than the popular alternating minimization method. In the decode phase, we introduce a deconvolution framework that is immune to the problem of varying sentence lengths. The word-sequence embeddings we extracted using ConvDic+DeconvDec are universally good for a few downstream tasks we test on. The framework requires neither pre-training nor prior/outside information.\"",
        "Document: \"Tensor decompositions for learning latent variable models. This work considers a computationally and statistically efficient parameter estimation method for a wide class of latent variable models--including Gaussian mixture models, hidden Markov models, and latent Dirichlet allocation--which exploits a certain tensor structure in their low-order observable moments (typically, of second- and third-order). Specifically, parameter estimation is reduced to the problem of extracting a certain (orthogonal) decomposition of a symmetric tensor derived from the moments; this decomposition can be viewed as a natural generalization of the singular value decomposition for matrices. Although tensor decompositions are generally intractable to compute, the decomposition of these specially structured tensors can be efficiently obtained by a variety of approaches, including power iterations and maximization approaches (similar to the case of matrices). A detailed analysis of a robust tensor power method is provided, establishing an analogue of Wedin's perturbation theorem for the singular vectors of matrices. This implies a robust and computationally tractable estimation approach for several popular latent variable models.\"",
        "Document: \"StrassenNets: Deep learning with a multiplication budget. A large fraction of the arithmetic operations required to evaluate deep neural networks (DNNs) are due to matrix multiplications, both in convolutional and fully connected layers. Matrix multiplications can be cast as $2$-layer sum-product networks (SPNs) (arithmetic circuits), disentangling multiplications and additions. We leverage this observation for end-to-end learning of low-cost (in terms of multiplications) approximations of linear operations in DNN layers. Specifically, we propose to replace matrix multiplication operations by SPNs, with widths corresponding to the budget of multiplications we want to allocate to each layer, and learning the edges of the SPNs from data. Experiments on CIFAR-10 and ImageNet show that this method applied to ResNet yields significantly higher accuracy than existing methods for a given multiplication budget, or leads to the same or higher accuracy compared to existing methods while using significantly fewer multiplications. Furthermore, our approach allows fine-grained control of the tradeoff between arithmetic complexity and accuracy of DNN models. Finally, we demonstrate that the proposed framework is able to rediscover Strassenu0027s matrix multiplication algorithm, i.e., it can learn to multiply $2 times 2$ matrices using only $7$ multiplications instead of $8$.\"",
        "Document: \"Non-intrusive transaction monitoring using system logs. We consider the problem of online monitoring of transaction instances in enterprise environments, based on footprints left by the instances in system logs and using a state-based reference model of the transaction. Unlike existing approaches, we do not rely on any platform-specific knowledge, neither do we assume footprints to carry correlating identifiers, as injected through instrumentation. We outline a solution for tracking transaction instances at individual and aggregate levels, present preliminary results on theoretical analysis of monitoring precision and conclude with directions of ongoing and future research.\"",
        "Document: \"Tracking in a spaghetti bowl: monitoring transactions using footprints. The problem of tracking end-to-end service-level transactions in the absence of instrumentation support is considered. The transaction instances progress through a state-transition model and generate time-stamped footprints on entering each state in the model. The goal is to track individual transactions using these footprints even when the footprints may not contain any tokens uniquely identifying the transaction instances that generated them. Assuming a semi-Markov process model for state transitions, the transaction instances are tracked probabilistically by matching them to the available footprints according to the maximum likelihood (ML) criterion. Under the ML-rule, for a two-state system, it is shown that the probability that all the instances are matched correctly is minimized when the transition times are i.i.d. exponentially distributed. When the transition times are i.i.d. distributed, the ML-rule reduces to a minimum weight bipartite matching and reduces further to a first-in first-out match for a special class of distributions. For a multi-state model with an acyclic state transition digraph, a constructive proof shows that the ML-rule reduces to splicing the results of independent matching of many bipartite systems.\"",
        "Document: \"signSGD: compressed optimisation for non-convex problems. Training large neural networks requires distributing learning across multiple workers, where the cost of communicating gradients can be a significant bottleneck. signSGD alleviates this problem by transmitting just the sign of each minibatch stochastic gradient. We prove that it can get the best of both worlds: compressed gradients and SGD-level convergence rate. signSGD can exploit mismatches between L1 and L2 geometry: when noise and curvature are much sparser than the gradients, signSGD is expected to converge at the same rate or faster than full-precision SGD. Measurements of the L1 versus L2 geometry of real networks support our theoretical claims, and we find that the momentum counterpart of signSGD is able to match the accuracy and convergence speed of Adam on deep Imagenet models. We extend our theory to the distributed setting, where the parameter server uses majority vote to aggregate gradient signs from each worker enabling 1-bit compression of worker-server communication in both directions. Using a theorem by Gauss, we prove that the non-convex convergence rate of majority vote matches that of distributed SGD. Thus, there is great promise for sign-based optimisation schemes to achieve both communication efficiency and high accuracy.\"",
        "Document: \"Open Problem: Approximate Planning of POMDPs in the class of Memoryless Policies. Author(s): Azizzadenesheli, K; Lazaric, A; Anandkumar, A | Abstract: Planning plays an important role in the broad class of decision theory. Planning has drawn much attention in recent work in the robotics and sequential decision making areas. Recently, Reinforcement Learning (RL), as an agent-environment interaction problem, has brought further attention to planning methods. Generally in RL, one can assume a generative model, e.g. graphical models, for the environment, and then the task for the RL agent is to learn the model parameters and find the optimal strategy based on these learnt parameters. Based on environment behavior, the agent can assume various types of generative models, e.g. Multi Armed Bandit for a static environment, or Markov Decision Process (MDP) for a dynamic environment. The advantage of these popular models is their simplicity, which results in tractable methods of learning the parameters and finding the optimal policy. The drawback of these models is again their simplicity: these models usually underfit and underestimate the actual environment behavior. For example, in robotics, the agent usually has noisy observations of the environment inner state and MDP is not a suitable model. More complex models like Partially Observable Markov Decision Process (POMDP) can compensate for this drawback. Fitting this model to the environment, where the partial observation is given to the agent, generally gives dramatic performance improvement, sometimes unbounded improvement, compared to MDP. In general, finding the optimal policy for the POMDP model is computationally intractable and fully non convex, even for the class of memoryless policies. The open problem is to come up with a method to find an exact or an approximate optimal stochastic memoryless policy for POMDP models.\"",
        "Document: \"Energy Efficient Routing for Statistical Inference of Markov Random Fields. The problem of routing of sensor observations for optimal detection of a Markov random field (MRF) at a designated fusion center is analyzed. Assuming that the correlation structure of the MRF is defined by the nearest-neighbor dependency graph, routing schemes which minimize the total energy consumption are analyzed. It is shown that the optimal routing scheme involves data fusion at intermediate nodes and requires transmissions of two types viz., the raw sensor data and the aggregates of log-likelihood ratio (LLR). The raw data is transmitted among the neighbors in the dependency graph and local contributions to the LLR are computed. These local contributions are then aggregated and delivered to the fusion center. A 2-approximation routing algorithm (DFMRF) is proposed and it has a transmission multidigraph consisting of the dependency graph and the directed minimum spanning tree, with the directions toward the fusion center.\"",
        "1 is \"Scalable Scheduling of Updates in Streaming Data Warehouses\", 2 is \"Bayesian SegNet: Model Uncertainty in Deep Convolutional Encoder-Decoder Architectures for Scene Understanding\"",
        "Given above information, for an author who has written the paper with the title \"Cost-performance tradeoff in multi-hop aggregation for statistical inference\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009083": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Panel: new directions for conceptual modeling':",
        "Document: \"On Computing the Importance of Entity Types in Large Conceptual Schemas. The visualization and the understanding of large conceptual schemas require the use of specific methods. These methods generate clustered, summarized or focused schemas that are easier to visualize and to understand. All of these methods require computing the importance of each entity type in the schema. In principle, the totality of knowledge defined in the schema could be relevant for the computation of that importance but, up to now, only a small part of that knowledge has been taken into account. In this paper, we extend six existing methods for computing the importance of entity types by taking into account all the relevant knowledge defined in the structural and behavioural parts of the schema. We experimentally evaluate the original and the extended versions of those methods with two large real-world schemas. We present the two main conclusions we have drawn from the experiments.\"",
        "Document: \"A Method for Explaining the Behaviour of Conceptual Models. Traditional information modelling methods have been concerned with the important task of checking whether a model correctly and adequately describes a piece of reality and/or the users' intended requirements, that is, with model validation. In this paper, we present a new method for model validation which can be applied to conceptual models based on the concept of transaction. It provides explanations of the results of model execution. We extend the facilities of methods developed so far in this context by providing answers to questions about the value of derived information, to questions about how an information can be made true or false, and to hypothetical questions.\"",
        "Document: \"The CSTL processor: a tool for automated conceptual schema testing. In this demonstration paper, we present the CSTL Processor, a tool to support the validation of two fundamental quality properties of conceptual schemas (correctness and completeness) by testing. The CSTL Processor supports the management, execution and automatic computation of the verdicts of test cases which formalize stakeholders' needs and expectations.\"",
        "Document: \"A tool for filtering large conceptual schemas. The wealth of knowledge the conceptual schemas of many real-world information systems contain makes them very useful to their potential target audience. However, the sheer size of those schemas makes it difficult to extract knowledge from them. There are many information system development activities in which people needs to get a piece of the knowledge contained in a large conceptual schema. We present an information filtering tool in which a user focuses on one or more entity types of interest for her task at hand, and the tool automatically filters the schema in order to obtain a reduced conceptual schema including a set of entity and relationship types (and other knowledge) relevant to that task.\"",
        "Document: \"Derived types and taxonomic constraints in conceptual modeling. This paper analyzes the relationships between derived types and taxonomic constraints. The objectives are to see which taxonomic constraints are entailed by derivation rules and to analyze how taxonomic constraints can be satisfied in presence of derived types. We classify derived entity types into several classes. The classification reveals the taxonomic constraints entailed in each case. These constraints must be base constraints (defined in the taxonomy) or derivable from them. We show how the base taxonomic constraints can be satisfied, either by the derivation rules (or the whoie schema), or by enforcement. We also show that our results extend naturally to taxonomies of relationship types.Our results are general and could be incorporated into many conceptual modeling environments and tools. The expected benefits are an improvement in the verification of the consistency between taxonomic constraints and derivation rules, and a guide (to the information system designer) for the determination of the taxonomic constraints that must be enforced in the final system.\"",
        "Document: \"Planning Based on View Updating in Deductive Databases. This paper describes an approach to planning based on viewing the planning problem description as a deductive database and on extending techniques for the view update problem in deductive databases to solve planning problems. We provide a formal and expressive model to represent a planning problem and a sound and complete plan generation method to solve it. From this work it becomes clear that a planning problem and a view update problem have strong similarities.\"",
        "Document: \"Object Interaction in Object-Oriented Deductive Conceptual Models. We present the main components of an object-oriented deductive approach to conceptual modelling of information systems. This approach does not model object interaction explicitly. However interaction among objects can be derived by means of a formal procedure that we outline.\"",
        "Document: \"An eclipse plugin for validating names in UML conceptual schemas. Many authors agree on the importance of choosing good names for conceptual schema elements. Several proposals of naming guidelines are available in the literature, but the support offered by current CASE tools is very limited and, in many cases, insufficient. In this demonstration we present an Eclipse plugin that implements a specific proposal of naming guidelines. The implemented proposal provides a guideline for every kind of named element in UML. By using this plugin, the modelers can automatically check whether the names they gave to UML elements are grammatically correct and generate a verbalization that can be analysed by domain experts.\"",
        "Document: \"A General Method for Pruning OWL Ontologies. In the past, most ontologies have been developed essentially from scratch, but in the last decade several research projects have appeared that use large ontologies to create new ontologies in a semiautomatic (or assisted) way. When using a large ontology to create a more specific one, a key aspect is to delete, as much automatically as possible, the elements of the large ontology that are irrelevant for the specific domain. This activity is commonly performed by a pruning method. There are several approaches for pruning ontologies, and they differ in the kind of ontology that they prune and the way the relevant concepts are selected and identified. This paper adapts an existing pruning method to OWL ontologies, and extends it to deal with the instances of the ontology to prune. Furthermore, different ways of selecting relevant concepts are studied. The method has been implemented. We illustrate the method by applying it to a case study that prunes a spatial ontology based on the Cyc ontology.\"",
        "Document: \"The Events Method for View Updating in Deductive Databases. We propose a new method for view updating in deductive databases. The method is based on events and transition rules, which explicitly define the insertions and deletions induced by a database update. Using these rules, an extension of the SLDNF procedure allows us to obtain all valid translations of view update requests. The main advantages of the method are its simplicity, the uniform treatment of insert and delete requests and the integration of integrity checking during the derivation process. The method has the full power of the methods developed so far, without some of their limitations. Two basic approaches have been proposed to solve the problem. The first suggests treating views as abstract data types, so that the definition of a view includes all permissible view updates together with their translation. The second approach is to define a general translation procedure (a translator). Inputs to the translator are a view definition, a view update and the current database, and the output is a database update that translate the view update, satisfying some properties (7). In this paper, we follow the translator approach. Bancilhon and Spyratos (2) proposed an elegant method for defining translators in the context of relational databases. In their approach, a translator is a mapping that associates with each view update a unique database update called a \"translation\". The translation takes the database to a state mapping onto the updated view, and leaving invariant the information not visible within the view. Tomasic's method (16) deals with view updates in definite deductive databases. He proposes procedures based on SLD-resolution for computing translations. These procedures provide a basis for methods which reduce the number of translations.\"",
        "1 is \"On the Transformation of Object-Oriented Conceptual Models to Logical Theories\", 2 is \"How to Design a General Rule Markup Language?\"",
        "Given above information, for an author who has written the paper with the title \"Panel: new directions for conceptual modeling\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009107": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Dissemination and competition between contents in lossy Susceptible Infected Susceptible (SIS) social networks':",
        "Document: \"Offloading Traffic Hotspots Using Moving Small Cells. In this paper, the concept of moving small cells in mobile networks is presented and evaluated taking into account the dynamics of the system. We consider a small cell moving according to a Manhattan mobility model which is the case when the small cell is deployed on the top of a bus following a predefined trajectory in areas which are generally crowded. Taking into account the distribution of user locations, we study the dynamic level considering a queuing model composed of multi-class Processor Sharing queues. Macro and small cells are assumed to be operating in the same bandwidth. Consequently, they are coupled due to the mutual interferences generated by each cell to the other. Our results show that deploying moving small cells could be an efficient solution to offload traffic hotspots.\"",
        "Document: \"Measurement-based admission control in UMTS. In this paper, we develop an efficient Call Admission Control (CAC) algorithm for UMTS systems. We first introduce the expressions that we developed for Signal-to-Interference (SIR) for both uplink and downlink, to obtain a novel CAC algorithm that takes into account, in addition to SIR constraints, the effects of mobility, coverage as well as the wired capacity behind the base station, for the uplink, and the maximal transmission power of the base station, for the downlink. As of its implementation, we investigate the measurement-based approach as a means to predict future, both handoff and new, call arrivals and thus manage different priority levels depending on a tunable coefficient. Compared to classical CAC algorithms, our CAC mechanism achieves better performance in terms of outage probability and QoS management.\"",
        "Document: \"Mapping of performance metrics between IP and ATM. End-to-end QoS requires mapping of QoS between heterogeneous subnetworks of the end-to-end path. For this mapping to be accurate, we argue, in this paper, that it should be based on the performance metrics. We propose a method to map loss and delay between IP, particularly integrated services (intserv), and ATM, using deterministic service curves. First, we consider a single IP flow and show our original method for mapping of backlog and delay between IP and ATM. Second, we extend our mapping to the case of the three intserv classes served by a single ATM switch implementing a priority-based discipline. Third, we focus on the cases of non-conformant traffic and congested systems. For our mapping method to cover those cases, we introduce loss in the non-lossy service curves approach, through two different ways. Extensive numerical simulations yield qualitative as well as quantitative mapping of loss and delay between intserv and ATM.\"",
        "Document: \"Optimal Energy Management Strategies In Mobile Networks Powered By A Smart Grid. We focus in this paper on energy management strategies for mobile network, equipped with battery storage capacity as well as local energy production capability, and powered by a smart grid. At each time instant, the mobile network operator has to decide whether to operate its network based on its own energy resources or the smart grid ones, with a possibility to sell energy to the smart grid as well. We formulate our problem using Markov Decision Process (MDP) and derive an optimal policy, which minimizes the telecommunication operator energy bill, using dynamic programming algorithm. We show the optimally of our solution by numerical comparison with the case of being exclusively powered by the grid. Our numerical applications allow to further understand when the operator has an incentive to buy energy, whether it is beneficial for him to act as aggregator (energy seller) as well as the size of the battery to deploy.\"",
        "Document: \"Cooperative relaying in OFDMA networks based on the joint use of hierarchical modulation and link adaptation. In cooperative relaying, the base station broadcasts a signal to the destination, the relay overhears it and forwards it in a next time slot to the destination. The latter combines the two received copies of the signal in order to reconstruct the original one. In this study, we consider a relay-based OFDMA network, as is the case of long term evolution advanced systems. We, first, make use of hierarchical modulation to send additional information to the relay so as to enable it to reconstruct a more robust copy of the original signal, and, second, adapt the transmission from the relay to the destination by taking advantage of the typically good radio conditions between them so as to reduce the cost of additional resources needed by relaying and hence improve the overall system capacity. We model such a system at the user level, considering a realistic arrival and departure setting and quantify the gains thus achieved in a cross-layer manner, in terms of throughput and blocking probability. We, eventually, propose an enhancement to the afore-mentioned scheme that takes advantage of the good radio conditions of users who are close to the base station to send them an additional stream using hierarchical modulation.\"",
        "Document: \"Joint uplink and downlink capacity considerations in admission control in multiservice CDMA/HSDPA systems. TCP-based data flows generate packets and ACKs in two directions, be it in the wireline or wireless networks. In the latter case, packets are typically found in the downlink whereas ACKs are in the uplink. Those two links are asymmetric in the case of CDMA-based High Data Rate (HDR)/High-Speed Downlink Packet Access (HSDPA) systems, the uplink being much slower than the downlink and thus, in some cases, restrictive in terms of the achievable throughput of the TCP flow. The aim of this work is to evaluate the performance of such a setting, in the presence of both streaming and elastic traffic, under a dynamic scenario where users arrive to the system and leave it after completion of their service. We specifically quantify the impact of the uplink on the overall performance of TCP and study the model variations according to several parameters such as load, file size and radio conditions.\"",
        "Document: \"How To Improve The Performance In Delay Tolerant Networks Under Manhattan Mobility Model. Delay Tolerant networks (DTNs) are one type of wireless networks where the number of nodes per unit area is small and hence the connectivity between the nodes is intermittent. In this case, the performance in terms of transport of information from source to destination relies on the mobility of the nodes which would cause their encounters and hence the relay of information from one node to another as well as on the routing protocol that is deployed. There exists several mobility patterns, each yielding a different performance of the network. In this work, we show first that the Manhattan mobility pattern performs worse than other widely-used ones, such as Random WayPoint. In the second part of this work, our aim is to propose and evaluate a new proposal, based on the deployment of fixed relays, so as to enhance the performance of Manhattan.\"",
        "Document: \"Dynamic Shaping for Self-Similar Traffic Using Network Calculus. The focus of this paper is the shaping of self-similar traffic at the access of an optical node. We propose a novel algorithm that dynamically shapes the incoming traffic, based on service curves equations, in order to meet the optical nodes constraints in terms of buffer size or delay. We first estimate arrival parameters within various time intervals in order to make the incoming traffic fit into a token bucket traffic specification (Tspec) format. We then derive the shaping parameters based on deterministic service curves. Those shaping parameters vary dynamically according to the Tspec of every time window. We eventually set those parameters back into the original model in order to meet some QoS constraints at the optical network level.\"",
        "Document: \"Mapping of Loss and Delay Between IP and ATM Using Network Calculus. End-to-end QoS requires accurate mapping of QoS classes and particularly QoS parameters between the different, heterogeneous layers and protocols present at the terminal equipment and intermediate nodes. In the IP over ATM context, both technologies offer QoS capabilities but differ in architecture, service classes and performance parameters. In this paper, we consider mapping of loss and delay between Integrated Services IP and ATM, using Network Calculus (NC), a min-plus algebra formulation. NC targets lossless systems only and is thus suitable for guaranteed services. However, as our aim is to quantify and map loss and delay in the presence of loss, we extend the theory so as to account for lossy systems too, as shall be the case for nonconformant traffic and congested systems. Loss is introduced by setting constraints on both the delay and the buffer size at the network elements. Numerical applications are used to further illustrate the mapping of loss and delay between IP and ATM.\"",
        "Document: \"Tracking Message Spread in Mobile Delay Tolerant Networks. We consider a delay tolerant network under two message forwarding schemes-a non-replicative direct delivery scheme and a replicative epidemic routing scheme. Our objective is to track the degree of spread of a message in the network. Such estimation can be used for on-line control of message dissemination. With a homogeneous mobility model with pairwise i.i.d. exponential inter-meeting times, we rigorously derive the system dynamic and measurement equations for optimal tracking by a Kalman filter. Moreover, we provide a framework for tracking a large class of processes that can be modeled as density-dependent Markov chains. We also apply the same filter with a heterogeneous mobility, where the aggregate inter-meeting times exhibit a power law with exponential tail as in real-world mobility traces, and show that the performance of the filter is comparable to that with homogeneous mobility. Through customized simulations, we demonstrate the trade-offs and provide several insightful observations on how the number of observers impacts the filter performance.\"",
        "1 is \"Practical algorithms for a family of waterfilling solutions\", 2 is \"The Dynamics of Reinforcement Learning in Cooperative Multiagent Systems\"",
        "Given above information, for an author who has written the paper with the title \"Dissemination and competition between contents in lossy Susceptible Infected Susceptible (SIS) social networks\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009181": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Towards better CPU power management on multicore smartphones':",
        "Document: \"Performance Study on a CSMA/CA-Based MAC Protocol for Multi-User MIMO Wireless LANs. A multi-antenna access point (AP) can communicate simultaneously with multiple clients, however, this multi-user MIMO (MU-MIMO) capability is underutilized in conventional 802.11 wireless LANs (WLANs). To address this problem, researchers have recently developed a CSMA/CA-based MAC protocol to support concurrent transmissions from different clients. In this paper, we propose an analytical model to characterize the saturation throughput and mean access delay of this CSMA/CA-based MAC protocol operating in an MU-MIMO WLAN. We also consider and model a distributed opportunistic transmission scheme, where clients are able to contend for the concurrent transmission opportunities only when their concurrent rates exceed a threshold. Comparisons with simulation results show that our analytical model provides a close estimation of the network performance. By means of the developed model, we evaluate the throughput and delay performance with respect to different network parameters, including the backoff window sizes, the number of AP's antennas, the network size, and the threshold of the opportunistic transmission scheme. Performance optimization over key parameters is also conducted for the transmission schemes.\"",
        "Document: \"Distributed Opportunistic Scheduling With QoS Constraints for Wireless Networks With Hybrid Links. Opportunistic scheduling for a wireless network with hybrid links is studied in this paper. Specifically, two link types are considered: a link of the first type always has a much lower transmission rate than a link of the second type. To avoid starvation in the first type of links, two link types must be treated differently in opportunistic scheduling, and quality of service (QoS) constraints, su...\"",
        "Document: \"Analog Network Coding Without Restrictions on Superimposed Frames. The applicability of analog network coding (ANC) to a wireless network is constrained by several limitations: 1) some ANC schemes demand fine-grained frame-level synchronization, which cannot be practically achieved in a wireless network; 2) others support only a specific type of modulation or require equal frame size in concurrent transmissions. In this paper, a new ANC scheme, called restriction-free analog network coding (RANC), is developed to eliminate the above limitations. It incorporates several function blocks, including frame boundary detection, joint channel estimation, waveform recovery, circular channel estimation, and frequency offset estimation, to support random concurrent transmissions with arbitrary frame sizes in a wireless network with various linear modulation schemes. To demonstrate the distinguished features of RANC, two network applications are studied. In the first application, RANC is applied to support a new relaying scheme called multi-way relaying, which significantly improves the spectrum efficiency as compared to two-way relaying. In the second application, RANC enables random-access-based ANC in an ad hoc network where flow compensation can be gracefully exploited to further improve the throughput performance. RANC and its network applications are implemented and evaluated on universal software radio peripheral (USRP) software radio platforms. Extensive experiments confirm that all function blocks of RANC work effectively without being constrained by the above limitations. The overall performance of RANC is shown to approach the ideal case of interference-free communications. The results of experiments in a real network setup demonstrate that RANC significantly outperforms existing ANC schemes and achieves constraint-free ANC in wireless networks.\"",
        "Document: \"Comparison Of Performances Of Pmlsm Fed By Two Power Supplies. Inverters provided non-sinusoidal voltage or current, and the tooth-slot and winding distribution of the motor could lead to a mass of space harmonic components, severely deteriorating the motor performance. In order to know the cause of electromagnetic thrust and mover velocity fluctuation of low-velocity PMLSM fed by SPWM-VS, the steady-state performances of low-velocity PMLSM are analyzed in this paper. the performances of permanent magnet linear synchronous motor by sinusoidal pulse width module voltage source inverter (SPWM-VI-PMLSM) and sinusoidal voltage source inverter (S-VS) were researched used the field-circuit coupled adaptive time-stepping finite element method. The characteristic of having thick air gap is considered in the field-circuit 2D model. The cosimulation using state equation and time-step finite element equation is used, the time step of the state equation is smaller than that of the time-step finite element equation. PWM-VI-PMLSM and S-VS-PMLSM have the same current periodicity and similar current amplitude. The current of SPWM-VI-PMLSM has various harmonic components distorted as a result of magnetic saturation and non-sinusoidal air gap field. The tangential electromagnetic thrust of SPWM-VI-PMLSM under steady state oscillate with a period decided by pole pitch, and it has various harmonic components dampening thrust fluctuation, Thus S-VS-PMLSM has better performance than SPWM-VIPMLSM. S-VS-PMLSM and SPWM-VI-PMLSM have the same periodicity of slip fluctuation, the slip value of SPWMVI- PMLSM is larger than that of S-VS-PMLSM. The simulation results accords with experimental data.\"",
        "Document: \"Infrastructure planning and topology optimization for reliable mobile big data transmission under cloud radio access networks. With the development of user-centered and environment sensing technology of 5G, large capacity and ubiquitous coverage and massive data collection and processing will bring new challenges in wireless networks. The cloud radio access network (C-RAN) has been envisioned to provide a new wireless architecture for reliable transmission of mobile big data. In this paper, we focus on network planning deployment issue based on the optical mixed diet (OMD) technology. Specifically, the ring and spur topology optimization (RSTO) problem under the C-RAN architecture is investigated. The RSTO problem is formulated as a generic integer linear program (ILP) which can optimally (i) minimize the network deploying cost; (ii) identify the locations of Remote Radio Units (RRUs) and optical add-drop multiplexers (OADMs); (iii) identify the association relations between RRUs and OADMs; and (iv) satisfy the mobile coverage requirements so as to allow the mobile big data to be transmitted through the RRUs. We propose a new heuristic algorithm based on C-RAN architecture. Numerical results validate the ILP formulation and show the performance benefits of the proposed algorithm in terms of efficiency and effectiveness against Gurobi, which is an ILP solver. In numerical studies, we also demonstrate the performance benefits of the incorporation of CoMP technology in terms of total deployment cost reduction.\"",
        "Document: \"A hybrid centralized routing protocol for 802.11s WMNs. Wireless mesh networks (WMNs) are being widely accepted as a critical wireless access solution for various applications. Due to minimal mobility in mesh nodes, a backbone topology can be effectively maintained in WMN using a proactive routing protocol. In IEEE 802.11s standard, a tree-based routing (TBR) protocol is adopted as a viable proactive routing protocol for aWMN with user traffic flowing to/from a wired network through a root (i.e., a mesh portal). However, the performance of the TBR protocol degrades rapidly as the user traffic becomes dominated by intra-mesh traffic. The reason is that the routing path through the root even for intra-mesh traffic unnecessarily overloads the root. Furthermore, the TBR performance becomes more severe when the network size of WMN is large, which could lead to the huge amount of intra-mesh traffic towards the root. To overcome these problems, we propose a new routing mechanism, root driven routing (RDR) protocol, for the root to quickly determine the best-metric route for any source-destination pair of intra-mesh traffic. For inter-mesh traffic, the original TBR protocol is employed. Thus, the hybrid centralized routing protocol that combines TBR and RDR and is adaptive to all traffic scenarios. Our simulation results reveal that the proposed RDR protocol outperforms the TBR protocol with much lower average end-to-end delay and much higher packet delivery ratio for intra-mesh traffic. The simulation results also provide some insight into the right tradeoff between the TBR protocol and the RDR protocol to achieve the best performance of the hybrid centralized routing protocol for WMNs.\"",
        "Document: \"Detent Force Analysis And Optimization For Vertical Permanent-Magnet Linear Synchronous Motor With Fractional-Slot Windings. This paper investigates the detent force modeling and optimization of a iron-core type 16-pole 15-slot permanent magnet linear synchronous motor (PMLSM) for ropeless elevator applications. Variable network non-linear magnetic equivalent circuit (VNMEC) model is established to predict the detent force of PMLSM. The topology structure of equivalent magnetic circuit is developed and the permeance are derived and calculated. The end effect of two end teeth is essential for detent force analysis and it is focused in the modeling. Magnetic saturation of primary tooth also is taken into account and nonlinear permanence is calculated. Some 3-D finite-element numerical calculations are used to validate the feasibility of the proposed method. Then proposed VNMEC is employed to calculate and optimize the detent force considering the end tooth dimensions. In final, experimental results are further used to verify the validation of proposed model.\"",
        "Document: \"How does a neuron perform subtraction? \u2013 arithmetic rules of synaptic integration of excitation and inhibition. Numerous learning rules have been devised to carry out computational tasks in various neural network models. However, the rules for determining how a neuron integrates thousands of synaptic inputs on the dendritic arbors of a realistic neuronal model are still largely unknown. In this study, we investigated the properties of integration of excitatory and inhibitory postsynaptic potentials in a reconstructed pyramidal neuron in the CA1 region of the hippocampus. We found that the integration followed a nonlinear subtraction rule (the Cross-Shunting Rule, or CS rule). Furthermore, the shunting effect is dependent on the spatial location of inhibitory synapses, but not that of excitatory synapses. The shunting effect of inhibitory inputs was also found to promote the synchronization of neuronal firing when the CS rule was applied to a small scale neural network.\"",
        "Document: \"An OFDM-TDMA/SA MAC protocol with QoS constraints for broadband wireless LANs. Orthogonal frequency division multiplexing (OFDM) is an important technique to support high speed transmission of broadband traffic in wireless networks, especially broadband wireless local area networks (LANs). Based on OFDM, a new multiple access scheme, called OFDM-TDMA with subcarrier allocation (OFDM-TDMA/SA), is proposed in this paper. It provides more flexibility in resource allocation than other multiple access schemes such as OFDM-TDMA, OFDM-frequency division multiple access (OFDM-FDMA), and orthogonal frequency division multiple access (OFDMA). With OFDM-TDMA/SA, a medium access control (MAC) is designed for broad-band wireless LANs. It optimizes bit allocation in subcarriers so that maximum bits are transmitted in each OFDM symbol under a frequency selective fading environment. The OFDM-TDMA/SA MAC protocol also supports three classes of traffic such as guaranteed, controlled-load, and best effort services. Based on the optimum subcarrier bit-allocation algorithm and considering heterogeneous QoS constraints of multimedia traffic, a hierarchical scheduling scheme is proposed to determine the subcarriers and time slots in which a mobile terminal can transmit packets. In such a way, the OFDM-TDMA/SA MAC protocol significantly increases system throughput in a frequency selective fading environment and guarantees QoS of multimedia traffic. Computer simulation is carried out to evaluate the performance of the OFDM-TDMA/SA MAC protocol. Results show that the new MAC protocol outperforms other MAC protocols for OFDM-based wireless LANs.\"",
        "Document: \"A New Cooperative Spectrum Sensing Scheme for Cognitive Ad-Hoc Networks. As the radio spectrum is becoming more and more crowded, cognitive radio has recently become a hot research topic to improve the spectrum utilization efficiency. It is well known that the success of cognitive radio depends heavily on fast and efficient spectrum sensing that is very difficult in practice. Toward this end, this paper introduces a new guard-resident cooperative spectrum sensing scheme for a cognitive ad-hoc network. In particular, we classify cognitive nodes as either  or  based on the spectrum neighbor decision and distributed boundary search. The guard nodes sense the spectrum and then inform the resident nodes that are greatly relieved from spectrum sensing about the radio environmental changes. The analysis and simulation results show that the proposed algorithm can significantly reduce the total spectrum sensing load without sacrificing the sensing accuracy.\"",
        "1 is \"Virtual topologies for WDM star LANs-the regular structures approach\", 2 is \"A pursuer-evader game for sensor networks\"",
        "Given above information, for an author who has written the paper with the title \"Towards better CPU power management on multicore smartphones\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009202": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'FPGA-based image processing system for remote robot control':",
        "Document: \"Sensible interface using multi-sensors in mobile device. This research aim is to create sensible interactions between multisensors in mobile devices. The proposal covers sensible interactions for all three kinds of methods: Mobile Application Interaction, Mobile to Mobile Interaction, and Mobile to Home Appliance Interaction. Five components are built in the handheld sensible interface device: distance measurement component, acceleration measure component, main processing component, haptic generation component, and wireless communication component. This research can support emotional user experience better than the button type input method or touch type input method can do. In addition, the proposal developed from the research can support Mobile to Mobile Interaction and Mobile to Home Appliance Interaction. The implement of the proposed sensible interaction method will demonstrate an appliance with a navigation application and digital TV control.\"",
        "Document: \"A 12-inch wafer prealigner. A prealigner is an equipment of moving a wafer so that it is set at the desired position and orientation. In this paper, a 12-inch wafer prealigner that can recognize optical characters on a 12-inch wafer and generate smooth velocity command profiles for motors is proposed and developed. The recognition of optical characters on a 12-inch wafer makes it possible to manage each 12-inch wafer individually, which is required in processing a 12-inch wafer due to its cost. Also, since smooth velocity command profiles have small jerks, they may make better precision or shorter time in aligning a wafer.\"",
        "Document: \"Real-time binary shape matching system based on FPGA. In this paper, we propose a simple binary shape matching algorithm and its hardware implementation. A shape matching algorithm is a method to measure the similarity of objects in an image. This technology is generally used in image retrieval, inspection, and object detection. The proposed method uses a transformation matrix that handles translation, rotation, and scaling. With a combined transformation matrix, we can easily obtain the transformed image and compare with this transform image and pattern image. After examination of the basic algorithm, we consider the hardware perspective. We analyze the difference between the software and hardware implementations and present revision factors for hardware implementation. Finally, we propose a dedicated hardware binary matching system. It is implemented on a field programmable gate array (FPGA) using the VHSIC hardware description language (VHD). The implementation is evaluated using experimental result and a device utilization summary.\"",
        "Document: \"Reduction of Wake-up Time for Partial Networking in Automotive System. Vehicle manufacturers have been conducting continuous research on energy conservation and efficient energy management of vehicles in accordance with the increased power consumption due to increasing fuel consumption and increasing number of high performance vehicles. Therefore, an alternative energy management system that utilizes the Electric/Electronics (E/E) architecture of the vehicle has attracted attention. 'Partial Networking' is known as a typical energy saving method, made possible by the progress of E/E architecture. A partial network enables the individual wake-up of each node by using a partial Controller Area Network (CAN) transceiver. However, the Electronic Control Unit (ECU) is subject to the constraints of a wake-up time, which is needed to reset the sequence of the microprocessor and the initialization of each module when the ECU initiates wake-up. This is because the partial network proceeds under the control of the regulator in the ECU, whereby the partial network cannot use ECU, as it requires prompt operation when the operating message is received. Therefore, in this paper, we will build an embedded system and experiment environment based on a partial CAN transceiver for the wake-up time measurement in actual partial networking. In addition, we develop an algorithm that reduces the wake-up time by using an embedded system. Finally, we will describe the efficiency of the proposed method by comparing with the existing partial network.\n\n\"",
        "Document: \"Adaptive ternary-derivative pattern for disparity enhancement. High dynamic range conditions are major obstacles to the implementation of practical stereovision systems in real scenes. We address this problem by introducing an adaptive local ternary-derivative pattern (ALTDP) which is a fusion of the local ternary pattern (LTP) and local derivative pattern (LDP). We make three main contributions in this study: (i) ALTDP encodes more detail information than LDP by extending to eight directions; (ii) ALDTP is better at discriminating and less sensitive to noise in uniform regions with three-value encoding (-1,0,1) without using a pre-defined threshold; and (iii) ALTDP significantly improves the performance of hierarchical belief propagation (BP) by substituting ALTDP data cost for the different intensity data cost. Moreover, our proposed method performs slightly better than LBP and LDP with three datasets: synthetic sequences (set 2) in the EISATS dataset, bright differences sequences (set 5) in the EISATS dataset, and the bumblebee xb3 dataset.\"",
        "Document: \"Method of software redundancy for home healthcare system based on ISO 26262. ISO 26262 is an international functional safety standard for automotive systems. In contrast to the automotive field, where the configuration of redundant devices for safety is essential, the home healthcare field does not enforce redundancy. Thus, home healthcare devices that require high reliability, such as artificial hearts, can be susceptible to potential dangers. We therefore propose a safety redundancy method for home healthcare devices that is based on ISO 26262.\"",
        "Document: \"Pipelined Virtual Camera Configuration For Real-Time Image Processing Based On Fpga. Real-time image processing is important for many application areas which require a quick response from events in a scene. Since real-time image processing involves large amount of computations, many approaches have been proposed to solve this problem especially using a dedicated hardware system. However, they are not sufficiently adapted to practical use because their dedicated hardware architecture is not suitable to carry out multiple tasks even in the case of a reconfigurable architecture. This paper proposes a pipelined virtual camera configuration which can perform several image processing tasks, especially at the low and intermediate levels, through the reconfiguration of the system. By synchronizing the entire system with the pixel clock, each processing modules can regard the others as a virtual camera. As a result, both the performance and the degree of reconfiguration are significantly increased.\"",
        "Document: \"Teaching the robot by guidance using an exoskeleton device in assembly lines. Robots used in industry rely on complex and time-consuming programming, such as a teaching panel and offline programming. Different intuitive programming methods have been proposed to overcome this drawback. A method using a force/torque sensor is practical and has been used in several organizations. However, problems, such as precision, have to be overcome in this method. This paper presents a method using an exoskeleton device that can overcome the problems based on robot teaching. Its usefulness is demonstrated compared to a method using a force/torque sensor.\"",
        "Document: \"Architecture of RETE network hardware accelerator for real-time context-aware system. Context-aware systems, such as intelligent home-care systems or mobile communication devices that are aware of the channel environment, need reasoning ability with numerous rules to manage the current context. Reasoning techniques based on rule-based systems can be used for the efficient reasoning method of these numerous rules. The RETE algorithm has been used for the matching of reasoning rules in rule-based systems. However, the characteristics of the RETE algorithm cause it to have poor performance in Von Neumann computer systems. In this paper, we propose a novel architecture for the RETE network hardware accelerator, which provides efficient reasoning processing performance. Using the parallel RETE network hardware architecture, this accelerator can overcome the architectural constraints imposed by Von Neumann computer systems.\"",
        "Document: \"Domain Transformation-Based Efficient Cost Aggregation for Local Stereo Matching. Binocular stereo matching is one of the most important algorithms in the field of computer vision. Adaptive support-weight approaches, the current state-of-the-art local methods, produce results comparable to those generated by global methods. However, excessive time consumption is the main problem of these algorithms since the computational complexity is proportionally related to the support window size. In this paper, we present a novel cost aggregation method inspired by domain transformation, a recently proposed dimensionality reduction technique. This transformation enables the aggregation of 2-D cost data to be performed using a sequence of 1-D filters, which lowers computation and memory costs compared to conventional 2-D filters. Experiments show that the proposed method outperforms the state-of-the-art local methods in terms of computational performance, since its computational complexity is independent of the input parameters. Furthermore, according to the experimental results with the Middlebury dataset and real-world images, our algorithm is currently one of the most accurate and efficient local algorithms.\"",
        "1 is \"Speeded-Up Robust Features (SURF)\", 2 is \"Locating multiple optima using particle swarm optimization\"",
        "Given above information, for an author who has written the paper with the title \"FPGA-based image processing system for remote robot control\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009200": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Energy efficient datapath synthesis using dynamic frequency clocking and multiple voltages':",
        "Document: \"ILP Models for Energy and Transient Power Minimization During Behavioral Synthesis. The reduction of peak power, peak power differential,average power and energy are equally important in thedesign of low-power battery driven portable applications.In this paper, we introduce a parameter called \"cyclepower function\" (CPF-DFC) that captures the above powercharacteristics in the context of multiple supply voltage(MV) and dynamic frequency clocking (DFC) based designs.Further, we present ILP formulations for the minimizationof CPF-DFC during datapath scheduling. We conductedexperiments on selected high-level synthesis benchmarksfor various resource constraints. Experimental results showthat significant reduction in power, energy, and energy delayproduct, can be obtained using the proposed method.\"",
        "Document: \"A Game-Theoretic Approach for Binding in Behavioral Synthesis. In this paper, we describe a new algorithm based ongame theory for minimizing the average power of a circuitduring binding in behavioral synthesis. The problem is formulatedas an auction based non-cooperative finite gamefor which a solution is proposed based on the Nash equilibrium.For the binding algorithm, each functional unit in thedatapath is modeled as a player bidding for executing anoperation with the estimated power consumption as the bid.The operations are bound to the modules such that the totalpower consumption is minimized. Further, the techniquesof functional unit sharing, path balancing and register assignmentare incorporated within the binding algorithm forpower reduction. The proposed algorithm yields power reductionwithout any increase in area or delay overhead. Experimentalresults indicate that the proposed game theoreticsolution for binding yields an improvement of 13.9% overthe linear programming (LP) method in [16].\"",
        "Document: \"Run-time power-gating in caches of GPUs for leakage energy savings. In this paper, we propose a novel microarchitectural technique for run-time power-gating caches of GPUs to save leakage energy. The L1 cache (private to a core) can be put in a low-leakage sleep mode when there are no ready threads to be scheduled, and the L2 cache can be put in sleep mode when there is no memory request. The sleep mode is state-retentive, which precludes the necessity to flush the caches after they are woken up. The primary reason for the effectiveness our technique lies in the fact that the latency of detecting cache inactivity, putting a cache to sleep and waking it up before it is accessed, is completely hidden microarchitecturally. The technique incurs insignificant overheads in terms of power and area. Experiments were performed using the GPGPU-Sim simulator on benchmarks that was set up using the CUDA framework. The power and latency modeling of the cache arrays for measuring the wake-up latency and the break-even periods is performed using a 32-nm SOI IBM technology model. Based on experiments on 16 different GPU workloads, the average energy savings achieved by the proposed technique is 54%.\"",
        "Document: \"Cascaded Bayesian inferencing for switching activity estimation with correlated inputs. In this paper, we investigate the estimation of switching activity in VLSI circuits using a graphical probabilistic model based on cascaded Bayesian networks (CBNs). First, we develop a theoretical analysis for Bayesian inferencing of switching activity and then derive upper bounds for certain circuit parameters which, in turn, are useful in establishing the cascade structure of the CBN model. We formulate an elegant framework for maintaining probabilistic consistency in the interfacing boundaries across the CBNs during the inference process using a tree-dependent (TD) probability distribution function. A TD distribution is an approximation of the true joint probability function over the switching variables, with the constraint that the underlying BN representation is a tree. The tree approximation of the true joint probability function can be arrived at by using a maximum weight spanning tree (MWST) built using pairwise mutual information about the switching occurring at pairs of signal lines on the boundary. Further, we show that the proposed TD distribution function can be used to model correlations among the primary inputs which is critical for accuracy in modeling of switching activity. Experimental results for ISCAS circuits are presented to illustrate the efficacy of the proposed CBN models.\"",
        "Document: \"ILP models for simultaneous energy and transient power minimization during behavioral synthesis. In low-power design for battery-driven portable applications, the reduction of peak power, peak power differential, cycle difference power, average power and energy are equally important. These are different forms of dynamic power dissipation of a CMOS circuit, which is predominant compared to static power dissipation for higher switching activity. The peak power, the cycle difference power, and the peak power differential drive the transient characteristic of a CMOS circuit. In this article, we propose an ILP-based framework for the reduction of energy and transient power through datapath scheduling during behavioral synthesis. A new metric called \u201cmodified cycle power function\u201d (CPF&ast;) is defined that captures the above power characteristics and facilitates integer linear programming formulations. The ILP-based datapath scheduling schemes with CPF&ast; as objective function are developed assuming three modes of datapath operation, such as, single supply voltage and single frequency (SVSF), multiple supply voltages and dynamic frequency clocking (MVDFC), and multiple supply voltages and multicycling (MVMC). We conducted experiments on selected high-level synthesis benchmark circuits for various resource constraints and estimated power, energy and energy delay product for each of them. Experimental results show that significant reductions in power, energy and energy delay product can be obtained.\"",
        "Document: \"CASM: A VLSI Chip for Approximate String Matching. The edit distance between two strings a1, \u9a74, am and b1, \u9a74, bn is the minimum cost s of a sequence of editing operations (insertions, deletions and substitutions) that convert one string into the other. This paper describes the design and implementation of a linear systolic array chip for computing the edit distance between two strings over a given alphabet. An encoding scheme is proposed which reduces the number of bits required to represent a state in the computation. The architecture is a parallel realization of the standard dynamic programming algorithm proposed by Wagner and Fischer, and can perform approximate string matching for variable edit costs. More importantly, the architecture does not place any constraint on the lengths of the strings that can be compared. It makes use of simple basic cells and requires regular nearest-neighbor communication, which makes it suitable for VLSI implementation. A prototype of this array has been built at the University of South Florida.\"",
        "Document: \"A Linear Time Algorithm for Wire Sizing with Simultaneous Optimization of Interconnect Delay and Crosstalk Noise. In this paper, we propose a new methodology for wire sizing with simultaneous optimization of interconnect delay and crosstalk noise in deep submicron VLSI circuits. The wire sizing problem is modeled as an optimization problem formulated as a normal form game and solved using the Nash equilibrium. Game theory allows the optimization of multiple metrics with conflicting objectives. This property is exploited in modeling the wire sizing problem while simultaneously optimizing interconnect delay and crosstalk noise, which are conflicting in nature. The nets connecting the driving cell and the driven cell are divided into net segments. The net segments within a channel are modeled as players, the range of possible wire sizes forms the set of strategies and the payoff function is derived as the geometric mean of interconnect delay and crosstalk noise. The net segments are optimized from the ones closest to the driven cell towards the ones at the driving cell. The complete information about the coupling effects among the nets is extracted after the detailed routing phase. The resulting algorithm for wire sizing is linear in terms of the number of wire segments in the given circuit. Experimental results on several medium and large open core designs indicate that the proposed algorithm yields an average reduction of 21.48% in interconnect delay and 26.25% in crosstalk noise over and above the optimization from the Cadence place and route tools without any area overhead. The algorithm performs significantly better than simulated annealing and genetic search as established through experimental results. A mathematical proof of existence for Nash equilibrium solution for the proposed wire sizing formulation is provided.\"",
        "Document: \"Simultaneous optimization of total power, crosstalk noise, and delay under uncertainty. Technology scaling has not only magnified the effects of device process variations, but it has also precipitated the need for simultaneous optimization of several performance metrics. In this paper, we propose a novel gate sizing approach for multi-metric optimization of delay, power, and crosstalk noise. The algorithm is based on the concepts of mathematical programming, and models the process variation uncertainty considering spatial correlations. The approach identifies leakage power, dynamic power, and crosstalk noise as the objectives, and the optimized gate delays are kept as constraints. Initially, the deterministic upper and lower bounds of the objectives are identified, and during the final step, a crisp non-linear programming problem is formulated using these boundary values. The problem is solved using KNITRO, an interior-point based optimization solver. The proposed model maximizes the variation resistance, thus providing higher yield. ITC'99 benchmarks were used to test the proposed approach, and the results indicate that our algorithm identifies the solution points that are closest to the nominal bounds, while maintaining high timing yield.\"",
        "Document: \"Efficient computation of gabor filter based multiresolution responses. Multiresolution representation in machine vision systems provides a means to detect, analyze and interpret the information content of an image at multiple resolutions. The pyramidal representation of images suggested by Burt and Crowley and the hierarchical construction of orientation and velocity tuned filters developed by Fleet and Jepson are some of the examples for multiresolution representation of image details. In recent years, Gabor filter based methods have been suggested by several researchers for machine vision applications such as edge detection, texture classification and optical flow estimation. These applications require generating a family of Gabor filters tuned to several resolutions and orientations and computing their responses. The computation of Gabor filter responses at multiple resolutions and orientations is a very computationally intensive task. Two efficient schemes for computing Gabor filter based multiresolution responses are proposed. The first scheme namely, recursive filtering method generates a family of Gabor filters and their responses starting from a high resolution and proceeding towards the lowest resolution in steps of an octave. The second approach namely, successive sampling method generates the same responses in the reverse order. In both these methods the symmetric, asymmetric and wavelet nature of Gabor filters are exploited in order to speed up the computation. The applicability and usefulness of the schemes are established through experimental results. Special purpose architectures are proposed to implement the proposed techniques in hardware in order to further speed up the computation.\"",
        "Document: \"Computation of lower and upper bounds for switching activity: a unified approach. Accurate switching activity estimation is crucial for power budgeting. It is impractical to obtain an accurate estimate by simulating the circuit for all possible inputs. An alternate approach would be to compute tight bounds for the switching activity. In this paper, we propose a non-simulative method to compute bounds for switching activity at the logic level. First, we show that the switching activity can be modeled as the Bayesian distance for an abstract two class problem. The computation of the upper and lower bounds for the switching activity is unified in to a single function, \u03c8(\u03b1,p,\u03c1), where \u03b1 is a parameter, \u03c1 is the temporal correlation factor and p is the signal probability. The constraints on \u03b1 for \u03c8(\u03b1,p,\u03c1) to be tight upper and lower bounds are derived. The proposed approach computes bounds for individual gate switching. Experimental results are obtained by taking spatial and temporal correlations into account. The computations are simple and fast\"",
        "1 is \"A semi-custom voltage-island technique and its application to high-speed serial links\", 2 is \"Gate sizing using a statistical delay model\"",
        "Given above information, for an author who has written the paper with the title \"Energy efficient datapath synthesis using dynamic frequency clocking and multiple voltages\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009295": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'NTIRE 2017 Challenge on Single Image Super-Resolution: Methods and Results.':",
        "Document: \"Recovering Realistic Texture in Image Super-Resolution by Deep Spatial Feature Transform. Despite that convolutional neural networks (CNN) have recently demonstrated high-quality reconstruction for single-image super-resolution (SR), recovering natural and realistic texture remains a challenging problem. In this paper, we show that it is possible to recover textures faithful to semantic classes. In particular, we only need to modulate features of a few intermediate layers in a single network conditioned on semantic segmentation probability maps. This is made possible through a novel Spatial Feature Transform (SFT) layer that generates affine transformation parameters for spatial-wise feature modulation. SFT layers can be trained end-to-end together with the SR network using the same loss function. During testing, it accepts an input image of arbitrary size and generates a high-resolution image with just a single forward pass conditioned on the categorical priors. Our final results show that an SR network equipped with SFT can generate more realistic and visually pleasing textures in comparison to state-of-the-art SRGAN [27] and EnhanceNet [38].\"",
        "Document: \"Time-Delayed Correlation Analysis for Multi-Camera Activity Understanding. We propose a novel approach to understanding activities from their partial observations monitored through multiple non-overlapping cameras separated by unknown time gaps. In our approach, each camera view is first decomposed automatically into regions based on the correlation of object dynamics across different spatial locations in all camera views. A new Cross Canonical Correlation Analysis (xCCA) is then formulated to discover and quantify the time delayed correlations of regional activities observed within and across multiple camera views in a single common reference space. We show that learning the time delayed activity correlations offers important contextual information for (i) spatial and temporal topology inference of a camera network; (ii) robust person re-identification and (iii) global activity interpretation and video temporal segmentation. Crucially, in contrast to conventional methods, our approach does not rely on either intra-camera or inter-camera object tracking; it thus can be applied to low-quality surveillance videos featured with severe inter-object occlusions. The effectiveness and robustness of our approach are demonstrated through experiments on 330 hours of videos captured from 17 cameras installed at two busy underground stations with complex and diverse scenes.\"",
        "Document: \"Dimensionality reduction of protein mass spectrometry data using random projection. Protein mass spectrometry (MS) pattern recognition has recently emerged as a new method for cancer diagnosis. Unfortunately, classification performance may degrade owing to the enormously high dimensionality of the data. This paper investigates the use of Random Projection in protein MS data dimensionality reduction. The effectiveness of Random Projection (RP) is analyzed and compared against Principal Component Analysis (PCA) by using three classification algorithms, namely Support Vector Machine, Feed-forward Neural Networks and K-Nearest Neighbour. Three real-world cancer data sets are employed to evaluate the performances of RP and PCA. Through the investigations, RP method demonstrated better or at least comparable classification performance as PCA if the dimensionality of the projection matrix is sufficiently large. This paper also explores the use of RP as a pre-processing step prior to PCA. The results show that without sacrificing classification accuracy, performing RP prior to PCA significantly improves the computational time.\"",
        "Document: \"Image Super-Resolution Using Deep Convolutional Networks. We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparsecoding- based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage. We explore different network structures and parameter settings to achieve trade-offs between performance and speed. Moreover, we extend our network to cope with three color channels simultaneously, and show better overall reconstruction quality.\"",
        "Document: \"Person Re-Identification By Manifold Ranking. Existing person re-identification methods conventionally rely on labelled pairwise data to learn a task-specific distance metric for ranking. The value of unlabelled gallery instances is generally overlooked. In this study, we show that it is possible to propagate the query information along the unlabelled data manifold in an unsupervised way to obtain robust ranking results. In addition, we demonstrate that the performance of existing supervised metric learning methods can be significantly boosted once integrated into the proposed manifold ranking-based framework. Extensive evaluation is conducted on three benchmark datasets.\"",
        "Document: \"Quantifying Facial Age by Posterior of Age Comparisons. We introduce a novel approach for annotating large quantity of in-the-wild facial images with high-quality posterior age distribution as labels. Each posterior provides a probability distribution of estimated ages for a face. Our approach is motivated by observations that it is easier to distinguish who is the older of two people than to determine the personu0027s actual age. Given a reference database with samples of known ages and a dataset to label, we can transfer reliable annotations from the former to the latter via human-in-the-loop comparisons. We show an effective way to transform such comparisons to posterior via fully-connected and SoftMax layers, so as to permit end-to-end training in a deep network. Thanks to the efficient and effective annotation approach, we collect a new large-scale facial age dataset, dubbed `MegaAgeu0027, which consists of 41,941 images. Data can be downloaded from our project page mmlab.ie.cuhk.edu.hk/projects/MegaAge and github.com/zyx2012/Age_estimation_BMVC2017. With the dataset, we train a network that jointly performs ordinal hyperplane classification and posterior distribution learning. Our approach achieves state-of-the-art results on popular benchmarks such as MORPH2, Adience, and the newly proposed MegaAge.\"",
        "Document: \"Incremental activity modeling in multiple disjoint cameras. Activity modeling and unusual event detection in a network of cameras is challenging, particularly when the camera views are not overlapped. We show that it is possible to detect unusual events in multiple disjoint cameras as context-incoherent patterns through incremental learning of time delayed dependencies between distributed local activities observed within and across camera views. Specifically, we model multicamera activities using a Time Delayed Probabilistic Graphical Model (TD-PGM) with different nodes representing activities in different decomposed regions from different views and the directed links between nodes encoding their time delayed dependencies. To deal with visual context changes, we formulate a novel incremental learning method for modeling time delayed dependencies that change over time. We validate the effectiveness of the proposed approach using a synthetic data set and videos captured from a camera network installed at a busy underground station.\"",
        "Document: \"Exploiting Deep Generative Prior for Versatile Image Restoration and Manipulation. Learning a good image prior is a long-term goal for image restoration and manipulation. While existing methods like deep image prior (DIP) capture low-level image statistics, there are still gaps toward an image prior that captures rich image semantics including color, spatial coherence, textures, and high-level concepts. This work presents an effective way to exploit the image prior captured by a generative adversarial network (GAN) trained on large-scale natural images. As shown in Fig. 1, the deep generative prior (DGP) provides compelling results to restore missing semantics, e.g., color, patch, resolution, of various degraded images. It also enables diverse image manipulation including random jittering, image morphing, and category transfer. Such highly flexible restoration and manipulation are made possible through relaxing the assumption of existing GAN inversion methods, which tend to fix the generator. Notably, we allow the generator to be fine-tuned on-the-fly in a progressive manner regularized by feature distance obtained by the discriminator in GAN. We show that these easy-to-implement and practical changes help preserve the reconstruction to remain in the manifold of nature images, and thus lead to more precise and faithful reconstruction for real images. Code is available at \n<uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/XingangPan/deep-generative-prior</uri>\n.\"",
        "Document: \"Pedestrian Attribute Recognition At Far Distance. The capability of recognizing pedestrian attributes, such as gender and clothing style, at far distance, is of practical interest in far-view surveillance scenarios where face and body close-shots are hardly available. We make two contributions in this paper. First, we release a new pedestrian attribute dataset, which is by far the largest and most diverse of its kind. We show that the large-scale dataset facilitates the learning of robust attribute detectors with good generalization performance. Second, we present the benchmark performance by SVM-based method and propose an alternative approach that exploits context of neighboring pedestrian images for improved attribute inference.\"",
        "Document: \"Merge or Not? Learning to Group Faces via Imitation Learning. Face grouping remains a challenging problem despite the remarkable capability of deep learning approaches in learning face representation. In particular, grouping results can still be egregious given profile faces and a large number of uninteresting faces and noisy detections. Often, a user needs to correct the erroneous grouping manually. In this study, we formulate a novel face grouping framework that learns clustering strategy from ground-truth simulated behavior. This is achieved through imitation learning (a. k. a apprenticeship learning or learning by watching) via inverse reinforcement learning (IRL). In contrast to existing clustering approaches that group instances by similarity, our framework makes sequential decision to dynamically decide when to merge two face instances/groups driven by short-and long-term rewards. Extensive experiments on three benchmark datasets show that our framework outperforms unsupervised and supervised baselines.\"",
        "1 is \"The Perceptual Organization of Texture Flow: A Contextual Inference Approach\", 2 is \"Multiresolution Models for Object Detection\"",
        "Given above information, for an author who has written the paper with the title \"NTIRE 2017 Challenge on Single Image Super-Resolution: Methods and Results.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009302": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'SHARE-ODS: an ontology data service for search and rescue operations':",
        "Document: \"Web-scale classification: web classification in the big data era. This paper provides an overview of the workshop Web-Scale Classification: Web Classification in the Big Data Era which was held in New York City, on February 28th as a workshop of the seventh International Conference on Web Search and Data Mining. The goal of the workshop was to discuss and assess recent research focusing on classification and mining in Web-scale category systems. The workshop brought together members of several communities such web mining, machine learning, text classification and social media mining.\"",
        "Document: \"Event Recognition For Unobtrusive Assisted Living. Developing intelligent systems towards automated clinical monitoring and assistance for the elderly is attracting growing attention. USEFIL is an FP7 project aiming to provide health-care assistance in a smart-home setting. We present the data fusion component of USEFIL which is based on a complex event recognition methodology. In particular, we present our knowledge-driven approach to the detection of Activities of Daily Living (ADL) and functional ability, based on a probabilistic version of the Event Calculus. To investigate the feasibility of our approach, we present an empirical evaluation on synthetic data.\"",
        "Document: \"A probabilistic logic programming event calculus. We present a system for recognising human activity given a symbolic representation of video content. The input of our system is a set of time-stamped short-term activities (STA) detected on video frames. The output is a set of recognised long-term activities (LTA), which are pre-defined temporal combinations of STA. The constraints on the STA that, if satisfied, lead to the recognition of an LTA, have been expressed using a dialect of the Event Calculus. In order to handle the uncertainty that naturally occurs in human activity recognition, we adapted this dialect to a state-of-the-art probabilistic logic programming framework. We present a detailed evaluation and comparison of the crisp and probabilistic approaches through experimentation on a benchmark dataset of human surveillance videos.\"",
        "Document: \"Probabilistic Cascading for Large Scale Hierarchical Classification. Hierarchies are frequently used for the organization of objects. Given a hierarchy of classes, two main approaches are used, to automatically classify new instances: flat classification and cascade classification. Flat classification ignores the hierarchy, while cascade classification greedily traverses the hierarchy from the root to the predicted leaf. In this paper we propose a new approach, which extends cascade classification to predict the right leaf by estimating the probability of each root-to-leaf path. We provide experimental results which indicate that, using the same classification algorithm, one can achieve better results with our approach, compared to the traditional flat and cascade classifications.\"",
        "Document: \"MyCites: An Intelligent Information System for Maintaining Citations. The evaluation of their research work and its effect has always been one of scholars' greatest concerns. The use of citations for that purpose, as proposed by Eugene Garfield, is nowadays widely accepted as the most reliable method. However, gathering a scholar's citations constitutes a particularly laborious task, even in the current Internet era, as one needs to correctly combine information from miscellaneous sources. There exists therefore a need for automating this process. Numerous academic search engines try to cover this need, but none of them addresses successfully all related problems. In this paper we present an approach that facilitates to a great extent citation analysis by taking advantage of new algorithms to deal with these problems.\"",
        "Document: \"Representation models for text classification: a comparative analysis over three web document types. Text classification constitutes a popular task in Web research with various applications that range from spam filtering to sentiment analysis. To address it, patterns of co-occurring words or characters are typically extracted from the textual content of Web documents. However, not all documents are of the same quality; for example, the curated content of news articles usually entails lower levels of noise than the user-generated content of the blog posts and the other Social Media. In this paper, we provide some insight and a preliminary study on a tripartite categorization of Web documents, based on inherent document characteristics. We claim and support that each category calls for different classification settings with respect to the representation model. We verify this claim experimentally, by showing that topic classification on these different document types offers very different results per type. In addition, we consider a novel approach that improves the performance of topic classification across all types of Web documents: namely the n-gram graphs. This model goes beyond the established bag-of-words one, representing each document as a graph. Individual graphs can be combined into a class graph and graph similarities are then employed to position and classify documents into the vector space. Accuracy is increased due to the contextual information that is encapsulated in the edges of the n-gram graphs; efficiency, on the other hand, is boosted by reducing the feature space to a limited set of dimensions that depend on the number of classes, rather than the size of the vocabulary. Our experimental study over three large-scale, real-world data sets validates the higher performance of n-gram graphs in all three domains of Web documents.\"",
        "Document: \"An overview of the BIOASQ large-scale biomedical semantic indexing and question answering competition. This article provides an overview of the first BIOASQ challenge, a competition on large-scale biomedical semantic indexing and question answering (QA), which took place between March and September 2013. BIOASQ assesses the ability of systems to semantically index very large numbers of biomedical scientific articles, and to return concise and user-understandable answers to given natural language questions by combining information from biomedical articles and ontologies.The 2013 BIOASQ competition comprised two tasks, Task 1a and Task 1b. In Task 1a participants were asked to automatically annotate new PUBMED documents with MESH headings. Twelve teams participated in Task 1a, with a total of 46 system runs submitted, and one of the teams performing consistently better than the MTI indexer used by NLM to suggest MESH headings to curators. Task 1b used benchmark datasets containing 29 development and 282 test English questions, along with gold standard (reference) answers, prepared by a team of biomedical experts from around Europe and participants had to automatically produce answers. Three teams participated in Task 1b, with 11 system runs. The BIOASQ infrastructure, including benchmark datasets, evaluation mechanisms, and the results of the participants and baseline methods, is publicly available.A publicly available evaluation infrastructure for biomedical semantic indexing and QA has been developed, which includes benchmark datasets, and can be used to evaluate systems that: assign MESH headings to published articles or to English questions; retrieve relevant RDF triples from ontologies, relevant articles and snippets from PUBMED Central; produce \"exact\" and paragraph-sized \"ideal\" answers (summaries). The results of the systems that participated in the 2013 BIOASQ competition are promising. In Task 1a one of the systems performed consistently better from the NLM's MTI indexer. In Task 1b the systems received high scores in the manual evaluation of the \"ideal\" answers; hence, they produced high quality summaries as answers. Overall, BIOASQ helped obtain a unified view of how techniques from text classification, semantic indexing, document and passage retrieval, question answering, and text summarization can be combined to allow biomedical experts to obtain concise, user-understandable answers to questions reflecting their real information needs.\"",
        "Document: \"Learning Rules for Large Vocabulary Word Sense Disambiguation. Word Sense Disambiguation (WSD) is the process of distinguishing between different senses of a word. In general, the disambiguation rules differ for different words. For this reason, the automatic construction of disambiguation rules is highly desirable. One way to achieve this aim is by applying machine learning techniques to training data containing the various senses of the ambiguous words. In the work presented here, the decision tree learning algorithm C4.5 is applied on a corpus of financial news articles. Instead of concentrating on a small set of ambiguous words, as done in most of the related previous work, all content words of the examined corpus are disambiguated. Furthermore, the effectiveness of word sense disambiguation for different parts of speech (nouns and verbs) is examined empirically.\"",
        "Document: \"Run-time composite event recognition. Events are particularly important pieces of knowledge, as they represent activities of special significance within an organisation: the automated recognition of events is of utmost importance. We present RTEC, an Event Calculus dialect for run-time event recognition and its Prolog implementation. RTEC includes a number of novel techniques allowing for efficient run-time recognition, scalable to large data streams. It can be used in applications where data might arrive with a delay from, or might be revised by, the underlying event sources. We evaluate RTEC using a real-world application.\"",
        "Document: \"An Event Calculus for Event Recognition. Systems for symbolic event recognition accept as input a stream of time-stamped events from sensors and other computational devices, and seek to identify high-level composite events, collections of events that satisfy some pattern. RTEC is an Event Calculus dialect with novel implementation and `windowing' techniques that allow for efficient event recognition, scalable to large data streams. RTEC supports the expression of rather complex events, such as `two people are fighting', using simple primitives. It can operate in the absence of filtering modules, as it is only slightly affected by data that are irrelevant to the events we want to recognise. Furthermore, RTEC can deal with applications where event data arrive with a (variable) delay from, and are revised by, the underlying sources. RTEC can update already recognised events and recognise new events when data arrive with a delay or following data revision. We evaluate RTEC both theoretically, presenting a complexity analysis, and experimentally, using two real-world applications. The evaluation shows that RTEC can support real-time event recognition and is capable of meeting the performance requirements identified in a survey of event processing use cases.\"",
        "1 is \"Spectrum Assignment in Cognitive Radio Networks: A Comprehensive Survey\", 2 is \"Predictive complex event processing: a conceptual framework for combining complex event processing and predictive analytics\"",
        "Given above information, for an author who has written the paper with the title \"SHARE-ODS: an ontology data service for search and rescue operations\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009359": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Modular Performance Analysis of Energy-Harvesting Real-Time Networked Systems':",
        "Document: \"Time Stamp Based Scheduling for Energy Harvesting Systems with Hybrid Nonvolatile Hardware Support. Nonvolatile processors have manifested strong vitality in energy harvesting systems due to their endurable features to intermittent power supply. However, repeating configurations of peripherals still occupy too much task execution time, which substantially reduces effectiveness of previous scheduling algorithms. In this paper, we adopt the hybrid nonvolatile hardware platform and then propose a time stamp based scheduling algorithm. The experimental results present that the proposed algorithm matches the platform seamlessly and outperforms state-of-the-art algorithms both in effectiveness and efficiency.\"",
        "Document: \"PRIME: A Novel Processing-in-Memory Architecture for Neural Network Computation in ReRAM-Based Main Memory. Processing-in-memory (PIM) is a promising solution to address the \\\"memory wall\\\" challenges for future computer systems. Prior proposed PIM architectures put additional computation logic in or near memory. The emerging metal-oxide resistive random access memory (ReRAM) has showed its potential to be used for main memory. Moreover, with its crossbar array structure, ReRAM can perform matrix-vector multiplication efficiently, and has been widely studied to accelerate neural network (NN) applications. In this work, we propose a novel PIM architecture, called PRIME, to accelerate NN applications in ReRAM based main memory. In PRIME, a portion of ReRAM crossbar arrays can be configured as accelerators for NN applications or as normal memory for a larger memory space. We provide microarchitecture and circuit designs to enable the morphable functions with an insignificant area overhead. We also design a software/hardware interface for software developers to implement various NNs on PRIME. Benefiting from both the PIM architecture and the efficiency of using ReRAM for NN computation, PRIME distinguishes itself from prior work on NN acceleration, with significant performance improvement and energy saving. Our experimental results show that, compared with a state-of-the-art neural processing unit design, PRIME improves the performance by ~2360\u00d7 and the energy consumption by ~895\u00d7, across the evaluated machine learning benchmarks.\"",
        "Document: \"A Unified Methodology for Designing Hardware Random Number Generators Based on Any Probability Distribution. We propose a unified methodology for converting any probability distribution which satisfies a few simple conditions to a uniformly distributed random bit stream. The proposed methodology is based on a bit truncation scheme and can be trivially implemented by basic circuit modules. A sufficient condition is derived to determine the optimal truncation. The proposed methodology is verified by three ...\"",
        "Document: \"Dynamic Power and Energy Management for Energy Harvesting Nonvolatile Processor Systems. Self-powered systems running on scavenged energy will be a key enabler for pervasive computing across the Internet of Things. The variability of input power in energy-harvesting systems limits the effectiveness of static optimizations aimed at maximizing the input-energy-to-computation ratio. We show that the resultant gap between available and exploitable energy is significant, and that energy storage optimizations alone do not significantly close the gap. We characterize these effects on a real, fabricated energy-harvesting system based on a nonvolatile processor. We introduce a unified energy-oriented approach to first optimize the number of backups, by more aggressively using the stored energy available when power failure occurs, and then optimize forward progress via improving the rate of input energy to computation via dynamic voltage and frequency scaling and self-learning techniques. We evaluate combining these schemes and show capture of up to 75.5% of all input energy toward processor computation, an average of 1.54 \u00d7 increase over the best static \u201cForward Progress\u201d baseline system. Notably, our energy-optimizing policy combinations simultaneously improve both the rate of forward progress and the rate of backup events (by up to 60.7% and 79.2% for RF power, respectively, and up to 231.2% and reduced to zero, respectively, for solar power). This contrasts with static frequency optimization approaches in which these two metrics are antagonistic.\"",
        "Document: \"A 3us wake-up time nonvolatile processor based on ferroelectric flip-flops. Nonvolatile processors offer a number of desirable properties including instant on/off, zero standby power and resilience to power failures. This paper presents a fabricated nonvolatile processor based on ferroelectric flip-flops. These flipflops are used in a distributed fashion and are able to maintain system states without any power supply indefinitely. An efficient controller is employed to achieve parallel reads and writes to the flip-flops. A reconfigurable voltage detection system is designed for automatic system backup during power failures. Measurement results show that this nonvolatile processor can operate continuously even under power failures occurring at 20 KHz. It can backup system states within 7\u03bcs and restore them within 3 \u03bcs. Such capabilities will provide a new level of support to chip-level fine-grained power management and energy harvesting applications.\"",
        "Document: \"Software assisted non-volatile register reduction for energy harvesting based cyber-physical system. Wearable devices are important components as information collector in many cyber-physical systems. Energy harvesting instead of battery is a better power source for these wearable devices due to many advantages. However, harvested energy is naturally unstable and program execution will be interrupted frequently. Non-volatile processors demonstrate promising advantages to back up volatile state before the system energy is depleted. However, it also introduces non-negligible energy and area overhead. Since the chip size is a vital factor for wearable devices, in this work, we target non-volatile register reduction for application-specific systems. We propose to analyze the application program and determine efficient backup positions, by which the necessary non-volatile register file size can be significantly reduced. The evaluation results deliver an average of 62.9% reduction on non-volatile register file size for stack backup, with negligible storage overheads.\"",
        "Document: \"A Dual-Threshold Scheme Along with Security Reinforcement for Energy Efficient Nonvolatile Processors. With the increasing scale and decreasing size of the Internet of Things (IoTs) devices, energy harvesting systems have been proposed to power the systems instead of batteries. Addressing the problem that harvested energy is unstable, nonvolatile processors (NVPs) have been proposed to hold intermediate data and avoid frequent program restarting from the beginning. However, NVPs often suffer frequent backup and recovery operations, wasting a lot of energy and system resources. To further improve the performance of NVPs, this paper proposes a dual-threshold method to maximize execution progress by enabling a system to hibernate to wait for power resumption instead of backing up data directly upon power interruptions. In particular, the appropriate retention and backup thresholds are discussed in details in order to achieve the goal of minimizing power failures and maximizing computation progress. In the meantime, the possible attacks to NVPs with dual-threshold and solutions combating these threats are discussed to guarantee NVP's security. The evaluation results show an average of up to 82.3% reduction on power failures and 1.5x speedup on forward progress by the proposed dual-threshold method compared to the conventional single threshold scheme.\"",
        "Document: \"Nonvolatile Processor Architectures: Efficient, Reliable Progress with Unstable Power. Nonvolatile processors (NVPs) have integrated nonvolatile memory to preserve task-intermediate on-chip state during power emergencies. NVPs hide data backup and restoration from the executing software to provide an execution mode that will always eventually complete the current task. NVPs are emerging as a promising solution for energy-harvesting scenarios, in which the available power supply is u...\"",
        "Document: \"DVFS-Based Long-Term Task Scheduling for Dual-Channel Solar-Powered Sensor Nodes. Solar-powered sensor nodes (SCSNs) with energy storages have the greatest potential and are widely used in the coming era of the Internet of Things, since they avoid tedious battery maintenance tasks. However, because the solar energy source is unstable and limited, the sensor nodes suffer from high deadline miss ratio (DMR). To achieve better DMR, the existing scheduling algorithms find the best ...\"",
        "Document: \"SPaC: a segment-based parallel compression for backup acceleration in nonvolatile processors. Nonvolatile processor (NVP) has become an emerging topic in recent years. The conventional NV processor equips each flip-flop with a nonvolatile storage for data backup, which results in much faster backup speed with significant area overheads. A compression based architecture (PRLC) solved the area problem but with a nontrivial increasing on backup time. This paper provides a segment-based parallel compression (SPaC) architecture to achieve tradeoffs between area and backup speed. Furthermore, we use an off-line and online hybrid method to balance the workloads of different compression modules in SPaC. Experimental results show that SPaC can achieve 76% speed up against PRLC and meanwhile reduces the area by 16% against conventional NV processors.\"",
        "1 is \"Delay Aware Reliable Transport In Wireless Sensor Networks\", 2 is \"A stack-based resource allocation policy for realtime processes\"",
        "Given above information, for an author who has written the paper with the title \"Modular Performance Analysis of Energy-Harvesting Real-Time Networked Systems\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009371": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Automatic IP address configuration in VANETs':",
        "Document: \"Cross-Layer Architecture for Differentiated Services in Ad Hoc Networks. In the last few years, wireless ad hoc networks experienced a great diffusion thanks to their simple deployment everywhere and whenever needed. Already used for entertainment applications (e.g., instant messaging) or for specific domains (e.g., military) we think such technology is mature enough to support also real-time services provision.Real time applications pose several challenges, in particular issues such as limited bandwidth, unreliable channels, topology evolutions and power consumption make Quality of Service (QoS) management a mandatory task to be addressed. In this paper, we propose a general QoS systemthat makes use of Differentiated Services strategies by exploiting the concept of virtual backbones. Our key ideais to use an auto-configured virtual backbone to set up aDifferentiated Service area in order to opportunely manageQoS flows. Also, thanks to a new distributed Call Admission Control algorithm, it avoids the arise of overloading situations. Performance measurements, based on simulative techniques and carried out to test the feasibility of theproposed system, are finally presented.\"",
        "Document: \"Exploiting the FIWARE cloud platform to develop a remote patient monitoring system. FIWARE represents a new European Cloud platform that aims to land on the international ICT market bringing prominent novel advantages for societies. In fact, it provides new compelling and novel software components, available through APIs, able to give developers new valuable Cloud platform functionalities. The main contribution of this work consists in providing software architects an useful experience regarding the adoption of FIWARE for the design of a Cloud and Internet of Things (IoT) architecture. More specifically, we describe how can be possible to use the FIWARE Cloud platform to speed up the design of a real e-health Remote Patient Monitoring (RPM) architecture with an agile software development methodology. Our architecture aims to allow care givers to improve remote assistance to patients at home, optimizing the management of the workflow of doctors, physicians, medical assistants, and other involved hospital operators. In this paper, we specifically describe the main FIWARE components that we have adopted to design our architecture and how they have been integrated.\"",
        "Document: \"Enriched E-R model to design hybrid database for big data solutions. Advances in database technologies are moving the attention of data managers from well known structured relational databases (SQL-Like DBs) towards NoSQL approaches, especially to address big data issues. However, changing technologies in consolidated data management system is hard and requires great investments. Deploying hybrid SQL-NoSQL approaches could be a good solution to speed up the transition in many domains and information systems, but a formal data model to design hybrid databases is necessary. This paper presents a new data model aimed at solving this issue. Starting from the well known E-R model, we introduce some additional components to identify data and \u201cbig data\u201d in the system, in order to drive the implementation of SQL-like solutions to manage data, and NoSQL solutions to manage big data. The paper also discusses a Hospital Information System use case, to clearly show how the proposed enriched E-R model can be successfully adopted.\"",
        "Document: \"Towards Osmotic Computing: Looking at Basic Principles and Technologies. Osmotic Computing is becoming the new paradigm in the area of Computing. This paper shows how it can represents the glue of recent topics including Cloud, Edge and Fog Computing, and Internet of Things (IoT). Osmotic Computing introduces elements allowing to treat computation, networking, storage, data transfer and management among Cloud and IoT devices in Edge computing layers in a more harmonized fashion. In particular, we discuss how it can enable an abstraction of services that could bring into a new Software Defined of Everything era.\"",
        "Document: \"Towards the integration between IoT and Cloud computing: an approach for the secure self-configuration of embedded devices. AbstractThe secure boot-up and setup of Internet of Things (IoT) devices connected over the Cloud represent a challenging open issue. This paper deals with the automatic configuration of IoT devices in a secure way through the Cloud, in order to provide new added-value services. After a discussion on the limits of current IoT and Cloud solutions in terms of secure self-configuration, we present a Cloud-based architecture that allows IoT devices to interact with several federated Cloud providers. In particular, we present two possible scenarios, that is, single Cloud and a federated Cloud environments, interacting with IoT devices and we address specific issues of both. Moreover, we present several design highlights on how to operate considering real open hardware and software products already available in the market.\"",
        "Document: \"An Architecture for Runtime Customization of Smart Devices. Smart environments represent a relatively uncharted ICT territory where plenty of sensor and actuator devices can be enrolled on-demand in order to realize high value-added services. A few application scenarios, such as Smart Cities, have already been explored. However, in order to finally enable such a paradigm, several issues have to be dealt with. In particular, from a developer perspective the high degree of heterogeneity for devices (ranging from cheap sensors to smart phones) could represent a hurdle for software design. In this paper, we present an innovative architecture that aims at providing a common reference platform for repurposing of devices i.e., reshaping their operational behavior for emergent and unforeseen requirements. Thanks to its modular and plugin based design, the proposed architecture is poised to ease implementation of both low-level (e.g., device discovery, code compilation, binary deployment) and high-level (e.g., service composition, data management) duties. We present the general architecture, then focusing on device-side aspects, while also providing two simple use cases that demonstrate the suitability of the proposed approach.\"",
        "Document: \"A Federated System for MapReduce-Based Video Transcoding to Face the Future Massive Video-Selfie Sharing Trend. The massive use of mobile devices and social networks is causing the birth of a new compulsive users' behaviour. The activity photo selfie sharing is gradually turning into video selfie. These videos will be transcoded into multiple formats to support different visualization mode. We think there will be the need to have systems that can support, in a fast, efficient and scalable way, the millions of requests for video sharing and viewing. We think that a single Cloud Computing services provider cannot alone cope with this huge amount of incoming data (Big Data), so in this paper we propose a Cloud Federation-based system that exploiting the Hadoop MapReduce paradigm performs the video transcoding in multiple format and its distribution in a fastest and most efficient possible way. Experimental results highlight the major factors involved for job deployment in a federated Cloud environment and the efficiency of the proposed system and show how the Federation improves the performances of a MapReduce Job execution acting on a additional parallelization level.\"",
        "Document: \"An Authentication Model for IoT Clouds. Nowadays, the combination between Cloud computing and Internet of Things (IoT) is pursuing new levels of efficiency in delivering services, representing a tempting business opportunity for IT operators of increasing their revenues. However, security is considered as one of the major factors that slows down the rapid and large scale adoption and deployment of both IoT and Cloud computing. In this paper, considering such an IoT Cloud scenario, we present an architectural model and several use cases that allow different types of users to access IoT devices.\"",
        "Document: \"An approach to reduce energy costs through virtual machine migrations in cloud federation. Cloud federation offers new business models to enforce more flexible energy management strategies. Independent Cloud providers are exclusively bounded to the specific energy supplier powering its Data Centers. The situation radically change if we consider a federation of cooperating Cloud providers. In such a context a proper migration of virtual machines among providers can lead to a global energy cost-saving strategy. In this paper, we present an approach to reduce energy cost in a federated Cloud ecosystem. More specifically, we propose an algorithm that allows providers to determine a map of possible destinations for cost-evaluation. Furthermore, we introduce an additional algorithm to determine the optimum energy cost migration path, and, consequently, the best Cloud Data Center where virtual machines should be migrated in order to push down energy costs.\"",
        "Document: \"A framework for real time end to end monitoring and big data oriented management of smart environments. Nowadays, the success of Internet of Things (IoT) applications depends on the intelligence of tools and techniques that can monitor, manage, and verify the correct operations of smart ecosystems including sensors and big data analytics tools, typically deployed in Cloud and Edge computing datacenters. In this paper, we propose a framework for the monitoring and management of IoT system that integrates the AllJoyn functionalities, useful to interconnect IoT devices, MongoDB, to implement Big Data storage, and Storm, to run real-time data analytics. We implemented the proposed framework and we tested its main functionalities in a smart home application scenario. In our experimentation, we investigated three different data patterns, i.e., regular, event-based, and automated, in order to evaluate performance of our framework in terms of response time under different operational conditions. Experimental results show that the latency of the monitoring and service strongly depends on the type of management application running in the system, whereas it is lightly affected by the data patterns.\"",
        "1 is \"Capacity of Ad Hoc wireless networks\", 2 is \"Large-Scale Context Provisioning: A Use-Case for Homogenous Cloud Federation\"",
        "Given above information, for an author who has written the paper with the title \"Automatic IP address configuration in VANETs\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009373": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Polar Coding for the Binary Erasure Channel with Deletions.':",
        "Document: \"Moderate deviations for joint source-channel coding of systems with Markovian memory. We study the (almost lossless) joint source-channel coding problem from the moderate deviations perspective where the bandwidth expansion ratio tends towards the ratio of the channel capacity and source entropy at a rate larger than n-1/2 (n being the channel blocklength) and the error probability decays subexponentially. We consider the stationary ergodic Markov (SEM) source as well as discrete memoryless and additive SEM channels. We also discuss the loss due to separation in the moderate deviations setting.\"",
        "Document: \"Stochastic L-BFGS: Improved Convergence Rates and Practical Acceleration Strategies. We revisit the stochastic limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) algorithm. By proposing a new coordinate transformation framework for the convergence analysis, we prove improved convergence rates and computational complexities of the stochastic L-BFGS algorithms compared to previous works. In addition, we propose several practical acceleration strategies to speed up the empirica...\"",
        "Document: \"Polar Coding for the Binary Erasure Channel with Deletions. We study the application of polar codes in deletion channels by analyzing the cascade of a binary erasure channel (BEC) and a deletion channel. We show how polar codes can be used effectively on a BEC with a single deletion, and propose a list decoding algorithm with a cyclic redundancy check for this case. The decoding complexity is O(N2 log N), where N is the blocklength of the code. An importan...\"",
        "Document: \"Zero-Error Shift-Correcting and Shift-Detecting Codes. Motivated by communication scenarios such as timing channels (in queuing systems, molecular communications, etc.) and bit-shift channels (in magnetic recording systems), we study the error control problem in cases where the dominant type of noise are symbol shifts. In particular, two channel models are introduced and their zero-error capacities determined by an explicit construction of optimal zero-error codes. Model A can be informally described as follows: 1) The information is stored in an n-cell register, where each cell can either be left empty, or can contain a particle of one of P possible types, and 2) due to the imperfections of the device every particle is shifted k cells away from its original position over time, where k is drawn from a certain range of integers, without the possibility of reordering particles. Model B is an abstraction of a singleserver queue: 1) The transmitter sends symbols/packets from a P -ary alphabet through a queuing system with an infinite buffer, and 2) each packet is being processed by the server for a number of time slots k \u2208 {0, 1, . . . ,K}. Several variations of the above models are also discussed, e.g., with multiple particles per cell, with additional types of noise, and the continuous-time case. The models are somewhat atypical due to the fact that the length of the channel output in general differs from that of the corresponding input, and that this length depends on the noise (shift) pattern as well as on the input itself. This will require the notions of a zero-error code and the zero-error capacity, as introduced by Shannon, to be generalized.\"",
        "Document: \"Rank-One NMF-Based Initialization for NMF and Relative Error Bounds Under a Geometric Assumption. We propose a geometric assumption on nonnegative data matrices such that under this assumption, we are able to provide upper bounds (both deterministic and probabilistic) on the relative error of nonnegative matrix factorization (NMF). The algorithm we propose first uses the geometric assumption to obtain an exact clustering of the columns of the data matrix; subsequently, it employs several rank-...\"",
        "Document: \"Error exponent of the common-message broadcast channel with variable-length feedback. We derive upper and lower bounds on the reliability function for the discrete memoryless broadcast channel with common message and variable-length feedback. We show that the bounds are tight when the broadcast channel is stochastically degraded. We adapt and supplement new ideas to Yamamoto and Itoh's two-phase coding scheme for the direct part and Burnashev's proof technique for the converse part.\"",
        "Document: \"Joint Optimization of Collaborative Sensing and Radio Resource Allocation in Small-Cell Networks. The current trend towards heterogeneous networks requires some sort of self-organization capability, in terms of self-optimization and self-configuration. The basic steps enabling self-optimization typically require listening to the environment, learning and adapting resource allocation consequently. In this work, we propose a joint optimization of sensing parameters and radio resource allocation in order to maximize the opportunistic throughput, under the constraint of limiting undue interference towards primary users. The method applies to wireless networks where the sensing nodes are allowed to cooperate to improve their local decision capabilities using a fully decentralized approach based on distributed consensus. The proposed optimization maximizes the opportunistic throughput, taking into account decision errors, sensing time and time necessary to achieve a consensus over the sensed variables. Because of the lack of knowledge of the channel between the cognitive users and the macro-users, the interference constraint is formulated in probabilistic form, depending on the spatial distribution of the nodes and on the channel fading model. Our formulation allows us to find the optimal false alarm rate and, consequently, the optimal decision thresholds jointly with the optimal power/bit allocation. Finally, we show that, given a total power budget constraint, there is an optimal way of distributing the available power between the power spent to achieve consensus and the power spent for transmitting the data payload.\"",
        "Document: \"A Numerical Study on the Wiretap Network With a Simple Network Topology. In this paper, we study a security problem on a simple wiretap network, consisting of a source node S, a destination node D, and an intermediate node R. The intermediate node connects the source and the destination nodes via a set of noiseless parallel channels, with sizes n1 and n2, respectively. A message M is to be sent from S to D. The information in the network may be eavesdropped by a set of...\"",
        "Document: \"Privacy-preserving sharing of horizontally-distributed private data for constructing accurate classifiers. Data mining tasks such as supervised classification can often benefit from a large training dataset. However, in many application domains, privacy concerns can hinder the construction of an accurate classifier by combining datasets from multiple sites. In this work, we propose a novel privacy-preserving distributed data sanitization algorithm that randomizes the private data at each site independently before the data is pooled to form a classifier at a centralized site. Distance-preserving perturbation approaches have been proposed by other researchers but we show that they can be susceptible to security risks. To enhance security, we require a unique non-distance-preserving approach. We use Kernel Density Estimation (KDE) Resampling, where samples are drawn independently from a distribution that is approximately equal to the original data's distribution. KDE Resampling provides consistent density estimates with randomized samples that are asymptotically independent of the original samples. This ensures high accuracy, especially when a large number of samples is available, with low privacy loss. We evaluated our approach on five standard datasets in a distributed setting using three different classifiers. The classification errors only deteriorated by 3% (in the worst case) when we used the randomized data instead of the original private data. With a large number of samples, KDE Resampling effectively preserves privacy (due to the asymptotic independence property) and also maintains the necessary data integrity for constructing accurate classifiers (due to consistency).\"",
        "Document: \"Scaling Exponent and Moderate Deviations Asymptotics of Polar Codes for the AWGN Channel. This paper investigates polar codes for the additive white Gaussian noise (AWGN) channel. The scaling exponent mu of polar codes for a memoryless channel q(Y)|(X) with capacity I (q(Y)|(X)) characterizes the closest gap between the capacity and non-asymptotic achievable rates as follows: For a fixed epsilon is an element of (0, 1), the gap between the capacity I (q(Y)|(X)) and the maximum non-asymptotic rate R-n(*) achieved by a length-n polar code with average error probability epsilon scales as n(-1/mu), i.e., I (q(Y)|(X)) - R-n(*) = Theta (n(-1/mu)). It is well known that the scaling exponent mu for any binary-input memoryless channel (BMC) with I (q(Y)|(X)) is an element of (0, 1) is bounded above by 4.714. Our main result shows that 4.714 remains a valid upper bound on the scaling exponent for the AWGN channel. Our proof technique involves the following two ideas: (i) The capacity of the AWGN channel can be achieved within a gap of O (n(-1/mu) root log n) by using an input alphabet consisting of n constellations and restricting the input distribution to be uniform; (ii) The capacity of a multiple access channel (MAC) with an input alphabet consisting of n constellations can be achieved within a gap of O (n(-1/mu) log n) by using a superposition of log n binary-input polar codes. In addition, we investigate the performance of polar codes in the moderate deviations regime where both the gap to capacity and the error probability vanish as n grows. An explicit construction of polar codes is proposed to obey a certain tradeoff between the gap to capacity and the decay rate of the error probability for the AWGN channel.\"",
        "1 is \"Improving the variable ordering of OBDDs is NP-complete\", 2 is \"Generalized Tunstall codes for sources with memory\"",
        "Given above information, for an author who has written the paper with the title \"Polar Coding for the Binary Erasure Channel with Deletions.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009381": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Hybrid bandwidth allocation for energy saving based on large-scale user behavior in heterogeneous networks':",
        "Document: \"A spectrum-efficient broadcast scheme based on network coding in cellular MIMO systems. In this paper, a network coding broadcast (NC-BC) scheme is proposed to improve the spectral efficiency for multipleinput and multiple-output (MIMO) transmissions. Particularly by network coding the base station broadcast message with each user message respectively, NC-BC can save the radio resource allocated for base station broadcasting, which is necessary in conventional time division-based scheme. Moreover, the iterative precoding design for NC-BC is studied for approaching the optimal performance, and the sub-optimal precoding design with low complexity is also provided. The simulation results show that the proposed NC-BC scheme with precoding optimization can further improve the system transmission performance.\"",
        "Document: \"Attacking OpenSSL ECDSA with a small amount of side-channel information. In this work, we mount a lattice attack on the ECDSA signatures implemented by the latest version of OpenSSL which uses the windowed non-adjacent form method to implement the scalar multiplication. We first develop a new way of extracting information from the side-channel results of the ECDSA signatures. Just given a small fraction of the information about a side-channel result denoted as double-and-add chain, we take advantage of the length of the chain together with positions of two non-zero digits to recover information about the ephemeral key. Combining the information of both the most significant digits and the least significant bits, we are able to gain more information about the ephemeral key. The problem of recovering ECDSA secret key is then translated to the hidden number problem which can be solved by lattice reduction algorithms. Our attack is mounted to the secp256k1 curve, and the result shows that 85 signatures would be enough to recover the secret key, which is better than the result that previous attack gained only utilizing the information extracted from the least significant bits, using about 200 signatures to recover the secret key.\"",
        "Document: \"Efficient adaptive loading algorithm with simplified bandwidth optimization method for OFDM systems. A crucial aspect for adaptive bit and power loading algorithms is to determine all usable subchannels. In this paper, a simplified bandwidth optimization method is proposed to search for all usable subchannels for loading algorithms which aim at the minimization of power consumption with constraint of constant data rate and target bit error rate (BER). Based on this method, an efficient loading algorithm is developed. Simulation results show that the proposed algorithm can achieve the same bit allocation results to that of the well-known Hughes-Hartogs and Fischer et al. algorithm. This algorithm is appropriate for OFDM systems in time-varying channels due to its low complexity\"",
        "Document: \"Multiuser Diversity in Multiuser Two-Hop Cooperative Relay Wireless Networks: System Model and Performance Analysis. Cooperative communications significantly improve the performance of wireless networks with the help of relay nodes, whereas multiuser diversity (MUD) is a kind of diversity inherent in multiuser systems. In this paper, we present a framework to analyze the performance of MUD in the multiuser two-hop cooperative relay networks (TCRNs). Based on this framework, we derive tight closed-form expression...\"",
        "Document: \"Mobility enhancement and performance evaluation for 5G Ultra dense Networks. Future wireless network will address the explosive increase demand of high-data-rate video services as well as massive-access machine type communication (MTC) requests, so that increasing number of small cells are conceived to be densely deployed in hot spots, resulting in an Ultra-dense Network (UDN). As a main issue for the future network, UDN is a step further towards a low-cost, self-configuring and self-optimizing network, while also leading to high-frequent measurement, intolerable handover failure (HOF), as well as huge power consumption in both the terminal and access network. Thus, mobility enhancement in ultra-dense scenario has become a critical problem for the next generation wireless systems. To solve this problem, the split of control plane and user plane (C/U) has become one of the most promising way, as it allows more flexibility and better service control schemes. Inspired by this, a set of macro assisted small cell enhancement schemes is proposed contributing to a novel Data-only Carrier (DoC) system in our previous work. In this paper, for improving the handover (HO) performance, new mobility-enhanced schemes are designed and analyzed in detail in DoC network, taking into consideration of mobility, flexibility and various typical handover scenarios. Simulations are conducted by a system-level platform to illustrate the fundamental relationship between key handover parameters and mobility performance. Numerical results show that the gain of system HOF rises by 53.6% via optimizing and reconfiguring the handover parameters in DoC network. In addition, the DoC network has an excellent performance gain in UDN with 82% HO improvement and 44.34% energy efficiency promotion compared with the current LTE network, which may be a promising mobility enhancement strategy for future 5G networks.\"",
        "Document: \"Resource Allocation for Dual-Hop OFDM Systems with Multiple Decode-and-Forward Relays. In this paper, we consider the optimal resource allocation problem for dual-hop systems with a source-destination pair and multiple decode-and-forward (DF) relays. Orthogonal frequency division multiplexing (OFDM) is adopted in both hops. To maximize the system capacity, we formulate a mixed binary integer programming problem to optimally allocate the subchannels and transmit power for both the source and the relay nodes. Because of the prohibitive complexity, a greedy heuristic algorithm is proposed to decompose the original problem into solvable subproblems. Simulation results demonstrate that the proposed algorithm has a near-optimal performance.\"",
        "Document: \"Investigation of cooperation technologies in heterogeneous wireless networks. Heterogeneous wireless networks based on varieties of radio access technologies (RATs) and standards will coexist in the future. In order to exploit this potential multiaccess gain, it is required that different RATs are managed in a cooperative fashion. This paper proposes two advanced functional architecture supporting the functionalities of interworking between WiMAX and 3GPP networks as a specific case: Radio Control Server- (RCS-) and Access Point- (AP-) based centralized architectures. The key technologies supporting the interworking are then investigated, including proposing the Generic Link Layer (GLL) and researching the multiradio resource management (MRRM) mechanisms. This paper elaborates on these topics, and the corresponding solutions are proposed with preliminary results.\"",
        "Document: \"An Evolutionary Game for User Access Mode Selection in Fog Radio Access Networks. The fog radio access network (F-RAN) is a promising paradigm to provide high spectral efficiency and energy efficiency. Characterizing users to select an appropriate communication mode in F-RANs is critical for performance optimization. With evolutionary game theory, a dynamic mode selection is proposed for F-RANs, in which the competition among the groups of potential users' space is formulated as a dynamic evolutionary game, and the game is solved by an evolutionary equilibrium. Stochastic geometry tool is used to derive the proposals' payoff expressions for both fog access point and device-to-device users by considering node location, cache sizes, as well as the delay cost. The analytical results for the proposed game model and the corresponding solution are evaluated, which show that the evolutionary game-based access mode selection algorithm has a better payoff than the max rate-based algorithm.\"",
        "Document: \"An Efficient Multiuser Frequency-Time Grid(Ftg) Allocation Algorithm For Ofdm-Based Broadband Wireless Systems. In this paper, we propose an efficient frame-based two-dimensional frequency-time grid (FTG) allocation algorithm to maximize the system throughput while satisfying users' rate requirements in a multiuser and multirate service environment. The FTG scheduling algorithm is done per OFDM frame, allocating each frequency-time grid to different users according to both channel state information (CSI) and users' QoS constraints. The performance of the frame-based FTG algorithm is obtained in a multiuser frequency selective fading environment. The results show that the system using the proposed FTG algorithm has significant lower outage probability and higher capacity which is very near the upper bound (maximum allocation method).\"",
        "Document: \"An optimization on GLRT-based detection for LTE PUCCH. This paper gives a basic optimization on the detection of Uplink Control Information (UCI) transmitted in Long Term Evolution (LTE) Physical Uplink Control Channel (PUCCH). UCI includes HARQ-ACK and Channel Quality Indicator (CQI) bits. In this paper we study the detection technique based on the generalized likelihood-ratio test (GLRT) paradigm. On the case that the noise variances in two slots are different, the detection performance of GLRT-based algorithm is improved remarkably. However the GLRT-based detection needs to traverse the all data candidates with a cost of higher computation complexity. And then an optimization on GLRT detection is proposed, which improve the detection performance of about 2.8dB for both ACK/NACK and CQI and reduce the computation complexity by straight-forward combination.\"",
        "1 is \"Performance evaluation for a quasi-synchronous packet radio network (QSPNET)\", 2 is \"Achievable Rate and Energy Efficiency of Hybrid and Digital Beamforming Receivers with Low Resolution ADC.\"",
        "Given above information, for an author who has written the paper with the title \"Hybrid bandwidth allocation for energy saving based on large-scale user behavior in heterogeneous networks\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009401": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'State-space solution to the periodic multirate H\u221e control problem: a lifting approach':",
        "Document: \"Abstractions for nonblocking supervisory control of Extended Finite Automata. An abstraction method for Extended Finite Automata (EFAs), i.e., finite automata extended with variables, using transition projection is presented in this work. A manufacturing system modeled by EFAs is abstracted into subsystems that embody internal interacting dependencies. Synthesis and verification of subsystems are achieved through their model abstractions rather than their global model. Sufficient conditions are presented to guarantee that supervisors result in maximally permissive and nonblocking control. An examples demonstrate the computational effectiveness and practical usage of the approach.\"",
        "Document: \"Coordination of Operations by Relation Extraction for Manufacturing Cell Controllers. A method for generation of the control function for flexible manufacturing cells is presented in this paper. The control function is separated from the rest of the programmable logic control (PLC) program, and partitioned into a high-level part handling the operation sequence and a low-level part defining the execution process of the operations. The program structure enables high-level supervisor synthesis, which alleviates the state-space explosion problem. Information present in earlier steps of the development process is reused and processed, to automatically generate the automata needed for supervisor synthesis. An algorithm for generating automata, from the low-level safety requirements that restrict the high-level behavior, is presented. Algorithms are also presented for extracting the relations between the operations defining the work in the cell, from the synthesized supervisor. These relations give an easy-to-read representation of the control function that makes it interpretable by machine operators and maintenance personnel, an important feature to make the method applicable in an industrial setting. Hence, the control function generated by our method combines the benefits of a traditional supervisor, namely optimality and flexibility, with simplicity and clarity.\"",
        "Document: \"Symbolic Interpretation and Execution of Extended Finite Automata. We introduce a symbolic interpretation and execution technique for Extended Finite Automata (EFAs) and provide an interpreter that symbolically interprets and executes EFAs w.r.t. their (internal) variables. More specifically, the interpreter iterates over the EFA transitions, and by passing each transition, it symbolically interprets and evaluates the condition on the transition w.r.t. the known values of variables, and leaves other variables intact, and when it terminates, it returns the residual model. It is shown that the behavior of the residual system with respect to the original system is left unchanged. Finally, we demonstrate the effectiveness and necessity of the symbolic interpretation and execution combined with Abstractions for the nonblocking supervisory control of two manufacturing systems.\"",
        "Document: \"Generation of restart states for manufacturing systems with discarded workpieces. In earlier work we presented a method for restart of manufacturing systems. After correction of an error, the system resumes normal production by returning to a certain restart state in the control function, and from there reexecutes parts of the work schedule. In the present paper the method is extended to handle also systems where some, but not necessarily all, workpieces are discarded during the error correction preceding the restart. The restart process must then replace the discarded workpieces with new ones. The workpieces that remain in the cell may limit the possibilities of executing certain operations needed to process the new workpieces correctly, thus complicating the restart. Properties of the restart states that guarantee a correct restart are defined, typical situations that make the system impossible to restart are presented, and some suggestions are given for modifications that make the system restartable.\"",
        "Document: \"Restarting Manufacturing Systems; Restart States and Restartability. A method for restart after an error in a manufacturing system is introduced. The method is able to restart systems even after nonforeseen errors that cannot be planned for, and the online part of the restart method does not require use of more powerful computers than a standard Programmable Logic Controller. It is shown what properties the control function must have to ensure that there is at least one restart state for each controller state. Sufficient conditions to guarantee the possibility to restart a system regardless of where an error occurs are given, along with indications on how the system could otherwise be rebuilt to be restartable.\"",
        "Document: \"Reduced-order synthesis of operation sequences. In flexible manufacturing systems a large number of operations need to be coordinated and supervised to avoid blocking and deadlock situations. The synthesis of such supervisors soon becomes unmanageable for industrial manufacturing systems, due to state space explosion. In this paper we therefore develop some reduction principles for a recently presented model based on self-contained operations and sequences of operations. First sequential operation behaviors are identified and related operation models are simplified into one model. Then local transitions without interaction with other operation models are removed. This reduction principle is applied to a synthesis of non-blocking operation sequences, where collisions among moving devices are guaranteed to be avoided by a flexible booking process. The number of states in the synthesis procedure and the computation time is reduced dramatically by the suggested reduction principle.\"",
        "Document: \"Operation behavior modeling using relation identification and visualization algorithms. The behavior of a system can be described by a set of operations - sometimes called activities or tasks. The process to specify these operation seems to be a real challenge in various situations, for example when designing automation systems or keeping track of the work at an emergency department. In practice, operation behavior specification is often quite inflexible because every possible execution route is explicitly defined. In this paper, a modeling method and a relation identification and visualization algorithm is introduced that does not explicitly specify operation routes, where instead the operation behavior is specified using the execution restrictions in transition conditions for each operation. This enables the possibility to create multiple projections of the operation relations to enable better understanding.\"",
        "Document: \"Modeling and Optimization of Hybrid Systems for the Tweeting Factory. In this paper, a predicate transition model for discrete-event systems is generalized to include continuous dynamics, and the result is a modular hybrid predicate transition model. Based on this model, a hybrid Petri net including explicit differential equations and shared variables is also proposed. It is then shown how this hybrid Petri net model can be optimized based on a simple and robust nonlinear programming formulation. The procedure only assumes that desired sampled paths for a number of interacting moving devices are given, while originally equidistant time instances are adjusted to minimize a given criterion. This optimization of hybrid systems is also applied to a real robot station with interacting devices, which results in about 30% reduction in energy consumption. Moreover, a flexible online and event-based information architecture called the Tweeting Factory is proposed. Simple messages (tweets) from all kinds of equipment are combined into high-level knowledge, and it is demonstrated how this information architecture can be used to support optimization of robot stations.\"",
        "Document: \"Symbolic On-the-Fly Synthesis in Supervisory Control Theory. This paper presents an efficient synthesis algorithm and its proof of correctness for computing the controllable, nonblocking, and minimally restrictive supervisor in the supervisory control theory. Conventional synthesis algorithms are based on backward reachability computations, where blocking and uncontrollable states are iteratively found by searching the entire state space several times until a fixed point is reached. Many unnecessary states may be visited in this kind of searching. In this paper, we present an alternative synthesis algorithm based on forward reachability, where a number of synthesis steps are performed during the reachability computations. This approach is inspired from the search techniques in Artificial Intelligence (AI) planning. To handle large-scale problems, the algorithm performs the computations symbolically based on binary decision diagrams. The algorithm has been developed, implemented, and applied to several large-scale benchmarks. It is shown that, on average, the on-the-fly algorithm is more efficient than the conventional synthesis algorithms, in particular for problems with many uncontrollable states.\"",
        "Document: \"Exploiting Sparsity In The Discrete Mechanics And Optimal Control Method With Application To Human Motion Planning. The discrete equations of motion derived using a variational principle are particularly attractive to be used in numerical optimal control methods. This is mainly because: i) they exhibit excellent energy behavior, ii) they extend gracefully to systems with holonomic constraints and iii) they admit compact representation of the discrete state space.In this paper we propose the use of sparse finite differencing techniques for the Discrete Mechanics and Optimal Control method. In particular we show how to efficiently construct estimates of the Jacobian and Hessian matrices when the dynamics of the optimal control problem is discretized using a variational integrator.To demonstrate the effectiveness of this scheme we solve a human motion planning problem of an industrial assembly task, modeled as a multibody system consisting of more than one hundred degrees of freedom.\"",
        "1 is \"Solving semidefinite-quadratic-linear programs using SDPT3\", 2 is \"Expert-guided subgroup discovery: methodology and application\"",
        "Given above information, for an author who has written the paper with the title \"State-space solution to the periodic multirate H\u221e control problem: a lifting approach\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009427": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A novel and fast parallel solution for the vertex cover problem on DNA-based supercomputing.':",
        "Document: \"Minimizing write operation for multi-dimensional DSP applications via a two-level partition technique with complete memory latency hiding. Most scientific and digital signal processing (DSP) applications are recursive or iterative. The execution of these applications on a chip multiprocessor (CMP) encounters two challenges. First, as most of the digital signal processing applications are both computation intensive and data intensive, an inefficient scheduling scheme may generate huge amount of write operation, cost a lot of time, and consume significant amount of energy. Second, because CPU speed has been increased dramatically compared with memory speed, the slowness of memory hinders the overall system performance. In this paper, we develop a Two-Level Partition (TLP) algorithm that can minimize write operation while achieving full parallelism for multi-dimensional DSP applications running on CMPs which employ scratchpad memory (SPM) as on-chip memory (e.g., the IBM Cell processor). Experiments on DSP benchmarks demonstrate the effectiveness and efficiency of the TLP algorithm, namely, the TLP algorithm can completely hide memory latencies to achieve full parallelism and generate the least amount of write operation to main memory compared with previous approaches. Experimental results show that our proposed algorithm is superior to all known methods, including the list scheduling, rotation scheduling, Partition Scheduling with Prefetching (PSP), and Iterational Retiming with Partitioning (IRP) algorithms. Furthermore, the TLP scheduling algorithm can reduce write operation to main memory by 45.35% and reduce the schedule length by 23.7% on average compared with the IRP scheduling algorithm, the best known algorithm.\"",
        "Document: \"Top k Favorite Probabilistic Products Queries. With the development of the economy, products are significantly enriched, and uncertainty has been their inherent quality. The probabilistic dynamic skyline (PDS) query is a powerful tool for customers to use in selecting products according to their preferences. However, this query suffers several limitations: it requires the specification of a probabilistic threshold, which reports undesirable results and disregards important results; it only focuses on the objects that have large dynamic skyline probabilities; and, additionally, the results are not stable. To address this concern, in this paper, we formulate an uncertain dynamic skyline (UDS) query over a probabilistic product set. Furthermore, we propose effective pruning strategies for the UDS query, and integrate them into effective algorithms. In addition, a novel query type, namely the top $k$ favorite probabilistic products (TFPP) query, is presented. The TFPP query is utilized to select $k$ products which can meet the needs of a customer set at the maximum level. To tackle the TFPP query, we propose a TFPP algorithm and its efficient parallelization. Extensive experiments with a variety of experimental settings illustrate the efficiency and effectiveness of our proposed algorithms.\"",
        "Document: \"A novel and fast parallel solution for the vertex cover problem on DNA-based supercomputing. The volume's exponential explosion problem has been the critical factor that constraints the development of the DNA computing. For the objective to decrease the DNA volume complex of the vertex cover problem which is a famous NP-complete problem, a new DNA algorithm is advanced. The new algorithm consists of a parallel reducer, a solution generator, a dense parallel searcher, a sparse parallel searcher and a vertex cover searcher. In a computer simulation, the DNA strands of maximum number required is O(.\u221a3n) on the condition of not varying the time complexity. Besides, this algorithm is highly space-efficient and error-tolerant compared to conventional brute-force searching, and thus can be scaled-up to solve large and hard minimum vertex cover problems. \u00a9 2010 IEEE.\"",
        "Document: \"An Online and Scalable Model for Generalized Sparse Nonnegative Matrix Factorization in Industrial Applications on Multi-GPU. Generalized sparse nonnegative matrix factorization (SNMF) has been proven useful in extracting information and representing sparse data with various types of probabilistic distributions from industrial applications, e.g., recommender systems and social networks.However, current solution approaches for generalized SNMF are based on the manipulation of whole sparse matrices and factor matrices, whi...\"",
        "Document: \"Multi-objective particle swarm optimization algorithm based on game strategies. Particle Swarm Optimization (PSO) is easier to realize and has a better performance than evolutionary algorithm in many fields. This paper proposes a novel multi-objective particle swarm optimization algorithm inspired from Game Strategies (GMOPSO), where those optimized objectives are looked as some independent agents which tend to optimize own objective function. Therefore, a multi- player game model is adopted into the multi-objective particle swarm algorithm, where appropriate game strategies could bring better multi-objective optimization performance. In the algorithm, novel bargain strategy among multiple agents and nondominated solutions archive method are designed for improving optimization performance. Moreover, the algorithm is validated by several simulation experiments and its performance is tested by different benchmark functions.\"",
        "Document: \"An approximation algorithm based on game theory for scheduling simple linear deteriorating jobs. We consider the scheduling of simple linear deteriorating jobs on parallel machines from a new perspective based on game theory. In scheduling, jobs are often controlled by independent and selfish agents, in which each agent tries to select a machine for processing that optimizes its own payoff while ignoring the others. We formalize this situation as a game in which the players are job owners, the strategies are machines, and a player's utility is inversely proportional to the total completion time of the machine selected by the agent. The price of anarchy is the ratio between the worst-case equilibrium makespan and the optimal makespan. In this paper, we design a game theoretic approximation algorithm A and prove that it converges to a pure-strategy Nash equilibrium in a linear number of rounds. We also derive the upper bound on the price of anarchy of A and further show that the ratio obtained by A is tight. Finally, we analyze the time complexity of the proposed algorithm.\"",
        "Document: \"Energy-aware preemptive scheduling algorithm for sporadic tasks on DVS platform. Dynamic Voltage Scaling (DVS) is a key technique for embedded systems to exploit multiple voltage and frequency levels to reduce energy consumption and to extend battery life. There are many DVS-based algorithms proposed for periodic and aperiodic task models. However, there are few algorithms that support the sporadic task model. Moreover, existing algorithms that support the sporadic model lack of functionalities in terms of energy-saving. In this paper, we propose a novel energy-aware scheduling algorithm named Cycle Conserve Dynamic Voltage Scaling for Sporadic Tasks (CC-DVSST) algorithm which is an improvement to DVSST [1]. There is a large amount of time slack in the DVSST scheduling due to the significant difference between the actual execution time and the worst-case scenario. Introducing DVS with EDF, CC-DVSST scales down the voltage of a processor when tasks are completed earlier than they are expected, so that the slack time can be reused for other tasks, hence saving energy. Experimental results show that CC-DVSST can reduce the total amount of energy consumption up to 46% compared to DVSST while retaining the quality of service by meeting the deadlines.\"",
        "Document: \"M-Skyline: Taking sunk cost and alternative recommendation in consideration for skyline query on uncertain data. Traditional probabilistic skyline query over uncertain data returns a tuple of individual recommendations for customers. However, the uncertainty of the dataset brings the possibility that the recommendation is not correct. Once the incorrect candidate is recommended, user needs to query the skyline again (may use a higher probability threshold) and tries to find alternatives. This greatly hurts user experience for those recommendation scenarios where finding out query results to be wrong brings non-negligible sunk cost, such as spending time to visit a recommended interest point. To address this concern, we propose a novel M-Skyline query model that takes consideration of sunk cost and offers backup recommendation. Moreover, in order to optimize the query speed for finding such M-Skyline results, we devise several fast query algorithms. Extensive experiments with both real and synthetic datasets demonstrate the effectiveness and efficiency of our proposed algorithms under various scenarios.\"",
        "Document: \"List scheduling with duplication for heterogeneous computing systems. Effective task scheduling is essential for obtaining high performance in heterogeneous computing systems (HCS). However, finding an effective task schedule in HCS, requires the consideration of the heterogeneity of computation and communication. To solve this problem, we present a list scheduling algorithm, called Heterogeneous Earliest Finish with Duplicator (HEFD). As task priority is a key attribute for list scheduling algorithm, this paper presents a new approach for computing their priority which considers the performance difference in target HCS using variance. Another novel idea proposed in this paper is to try to duplicate all parent tasks and get an optimal scheduling solution. The comparison study, based on both randomly generated graphs and the graphs of some real applications, shows that our scheduling algorithm HEFD significantly surpasses other three well-known algorithms.\"",
        "Document: \"GFlink: An In-Memory Computing Architecture on Heterogeneous CPU-GPU Clusters for Big Data. The increasing main memory capacity and the explosion of big data have fueled the development of in-memory big data management and processing. By offering an efficient in-memory parallel execution model which can eliminate disk I/O bottleneck, existing in-memory cluster computing platforms (e.g., Flink and Spark) have already been proven to be outstanding platforms for big data processing. However...\"",
        "1 is \"List Scheduling Algorithm for Heterogeneous Systems by an Optimistic Cost Table\", 2 is \"Mobile Positioning In Mixed Los/Nlos Conditions Using Modified Ekf Banks And Data Fusion Method\"",
        "Given above information, for an author who has written the paper with the title \"A novel and fast parallel solution for the vertex cover problem on DNA-based supercomputing.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009537": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Visual Execution and Data Visualization in Natural Language Processing':",
        "Document: \"Bootstrapping Term Extractors for Multiple Languages. Terminology extraction resources are needed for a wide range of human language technology applications, including knowledge management, information extraction, semantic search, cross-language information retrieval and automatic and assisted translation. We report a low cost method for creating terminology extraction resources for 21 non-English EU languages. Using parallel corpora and a projection method, we create a General POS Tagger for these languages. We also investigate the use of EuroVoc terms and Wikipedia to automatically create a term grammar for each language. Our results show that these automatically generated resources can assist the term extraction process, achieving similar performance to manually generated resources. All POS tagger and term grammar resources resulting from this work are freely available for download.\"",
        "Document: \"Analysing Temporally Annotated Corpora with CAVaT. We present CAVaT, a tool that performs Corpus Analysis and Validation for TimeML. CAVaT is an open source, modular checking utility for statistical analysis of features specific to temporally-annotated natural language corpora. It provides reporting, highlights salient links between a variety of general and time-specific linguistic features, and also validates a temporal annotation to ensure that it is logically consistent and sufficiently annotated. Uniquely, CAVaT provides analysis specific to TimeML-annotated temporal information. TimeML is a standard for annotating temporal information in natural language text. In this paper, we present the reporting part of CAVaT, and then its error-checking ability, including the workings of several novel TimeML document verification methods. This is followed by the execution of some example tasks using the tool to show relations between times, events, signals and links. We also demonstrate inconsistencies in a TimeML corpus (TimeBank) that have been detected with CAVaT.\"",
        "Document: \"Named Entity Based Document Similarity with SVM-Based Re-ranking for Entity Linking. In this paper we present a novel approach to search a knowledge base for an entry that contains information about a named entity (NE) mention as specified within a given context. A document similarity function (NEBSim) based on NE co-occurrence has been developed to calculate the similarity between two documents given a specific NE mention in one of them. NEBsim is also used in conjunction with the traditional cosine similarity measure to learn a model for ranking. Naive Bayes and SVM classifiers are used to re-rank the retrieved documents. Our experiments, carried out on TAC-KBP 2011 data, show NEBsim achieves significant improvement in accuracy as compared with a cosine similarity approach. They also show that re-ranking using learn to rank techniques can significantly improve the accuracy at high ranks.\"",
        "Document: \"A Corpus-based Study of Temporal Signals.   Automatic temporal ordering of events described in discourse has been of great interest in recent years. Event orderings are conveyed in text via va rious linguistic mechanisms including the use of expressions such as \"before\", \"after\" or \"during\" that explicitly assert a temporal relation -- temporal signals. In this paper, we investigate the role of temporal signals in temporal relation extraction and provide a quantitative analysis of these expres sions in the TimeBank annotated corpus. \"",
        "Document: \"Mining clinical relationships from patient narratives. The Clinical E-Science Framework (CLEF) project has built a system to extract clinically significant information from the textual component of medical records in order to support clinical research, evidence-based healthcare and genotype-meets-phenotype informatics. One part of this system is the identification of relationships between clinically important entities in the text. Typical approaches to relationship extraction in this domain have used full parses, domain-specific grammars, and large knowledge bases encoding domain knowledge. In other areas of biomedical NLP, statistical machine learning (ML) approaches are now routinely applied to relationship extraction. We report on the novel application of these statistical techniques to the extraction of clinical relationships.We have designed and implemented an ML-based system for relation extraction, using support vector machines, and trained and tested it on a corpus of oncology narratives hand-annotated with clinically important relationships. Over a class of seven relation types, the system achieves an average F1 score of 72%, only slightly behind an indicative measure of human inter annotator agreement on the same task. We investigate the effectiveness of different features for this task, how extraction performance varies between inter- and intra-sentential relationships, and examine the amount of training data needed to learn various relationships.We have shown that it is possible to extract important clinical relationships from text, using supervised statistical ML techniques, at levels of accuracy approaching those of human annotators. Given the importance of relation extraction as an enabling technology for text mining and given also the ready adaptability of systems based on our supervised learning approach to other clinical relationship extraction tasks, this result has significance for clinical text mining more generally, though further work to confirm our encouraging results should be carried out on a larger sample of narratives and relationship types.\"",
        "Document: \"Using Dialogue Corpora to Extend Information Extraction Patterns for Natural Language Understanding of Dialogue. This paper examines how Natural Language Process (NLP) resources and online dialogue corpora can be used to extend coverage of Information Extraction (IE) templates in a Spoken Dialogue system. IE templates are used as part of a Natural Language Understanding module for identifying meaning in a user utterance. The use of NLP tools in Dialogue systems is a difficult task given 1) spoken dialogue is often not well-formed and 2) there is a serious lack of dialogue data. In spite of that, we have devised a method for extending IE patterns using standard NLP tools and available dialogue corpora found on the web. In this paper, we explain our method which includes using a set of NLP modules developed using GATE (a General Architecture for Text Engineering), as well as a general purpose editing tool that we built to facilitate the IE rule creation process. Lastly, we present directions for future work in this area.\"",
        "Document: \"New Methods, Current Trends and Software Infrastructure for NLP. The increasing use of 'new methods' in NLP, which this conference series exemplifies, occurs in the context of a wider shift in the nature and concerns of the discipline. This paper begins with a short review of this context and significant trends in the field. The review motivates and leads to a set of requirements for support software of general utility for NLP research and development workers. A freely-available system designed to meet these requirements is described (called GATE - a General Architecture for Text Engineering). Information Extraction (IE), in the sense defined by the Message Understanding Conferences (ARPA (2)), is an NLP application in which many of the new methods have found a home (Hobbs (18); Jacobs ed. (19)). An IE system based on GATE is also available for research purposes, and this is described. Lastly we review related work.\"",
        "Document: \"Summarizing Multi-Party Argumentative Conversations in Reader Comment on News. Existing approaches to summarizing multi-party argumentative conversations in reader comment are extractive and fail to capture the argumentative nature of these conversations. Work on argument mining proposes schemes for identifying argument elements and relations in text but has not yet addressed how summaries might be generated from a global analysis of a conversation based on these schemes. In this paper we: (1) propose an issue-centred scheme for analysing and graphically representing argument in reader comment discussion in on-line news, and (2) show how summaries capturing the argumentative nature of reader comment can be generated from our graphical representation.\"",
        "Document: \"Cross-validating Image Description Datasets and Evaluation Metrics. The task of automatically generating sentential descriptions of image content has become increasingly popular in recent years, resulting in the development of large-scale image description datasets and the proposal of various metrics for evaluating image description generation systems. However, not much work has been done to analyse and understand both datasets and the metrics. In this paper, we propose using a leave-one-out cross validation (LOOCV) process as a means to analyse multiply annotated, human-authored image description datasets and the various evaluation metrics, i.e. evaluating one image description against other human-authored descriptions of the same image. Such an evaluation process affords various insights into the image description datasets and evaluation metrics, such as the variations of image descriptions within and across datasets and also what the metrics capture. We compute and analyse (i) human upper-bound performance; (ii) ranked correlation between metric pairs across datasets; (iii) lower-bound performance by comparing a set of descriptions describing one image to another sentence not describing that image. Interesting observations are made about the evaluation metrics and image description datasets, and we conclude that such cross-validation methods are extremely useful for assessing and gaining insights into image description datasets and evaluation metrics for image descriptions.\"",
        "Document: \"An Annotation Scheme for Reichenbach's Verbal Tense Structure.   In this paper we present RTMML, a markup language for the tenses of verbs and temporal relations between verbs. There is a richness to tense in language that is not fully captured by existing temporal annotation schemata. Following Reichenbach we present an analysis of tense in terms of abstract time points, with the aim of supporting automated processing of tense and temporal relations in language. This allows for precise reasoning about tense in documents, and the deduction of temporal relations between the times and verbal events in a discourse. We define the syntax of RTMML, and demonstrate the markup in a range of situations. \"",
        "1 is \"Duplicate Record Detection: A Survey\", 2 is \"Building a large annotated corpus of English: the penn treebank\"",
        "Given above information, for an author who has written the paper with the title \"Visual Execution and Data Visualization in Natural Language Processing\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009567": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'An energy model approach to people counting for abnormal crowd behavior detection':",
        "Document: \"3D dynamic gesture recognition based on improved HMMs with entropy. Nowadays gesture recognition is a hot topic in the field of human-computer interaction (HCI). HCI develop very fast, and also brings surprise to us constantly. In this paper, we propose a novel approach based on improved HMMs with entropy to recognize the 3D gesture. In our method, there are two steps to recognize a gesture: 1. detect the key nodes of body with extracting the skeleton point. A low-pass filter is utilized to smooth trajectory later. 2. We use improved Hidden Markov Models (HMMs) algorithm which has a virtual start node and a virtual end node with another layer for gesture recognition. In order to decide when to start meaning gesture and when to end non-meaning gesture, we use entropy which can enlarge the searching space to avoid over-fitting and local minimum. Experimental results will demonstrate the performance of proposed approach.\"",
        "Document: \"Dimensionality reduction of data sequences for human activity recognition. Although current human activity recognition can achieve high accuracy rates, data sequences with high-dimensionality are required for a reliable decision to recognize the entire activity. Traditional dimensionality reduction methods do not exploit the local geometry of classification information. In this paper, we introduce the framework of manifold elastic net that encodes the local geometry to find an aligned coordinate system for data representation. The introduced method is efficient because classification error minimization criterion is utilized to directly link the classification error with the selected subspace. In the experimental section, a dataset on human activity recognition is studied from wearable, object, and ambient sensors.\"",
        "Document: \"p-Laplacian Regularization for Scene Recognition. The explosive growth of multimedia data on the Internet makes it essential to develop innovative machine learning algorithms for practical applications especially where only a small number of labeled samples are available. Manifold regularized semi-supervised learning (MRSSL) thus received intensive attention recently because it successfully exploits the local structure of data distribution includ...\"",
        "Document: \"Reinforcement online learning for emotion prediction by using physiological signals. Physiological signals generated from human internal organs can objectively and truly reflect the real-time variations of human emotion and monitor body situation. Recently, with the accessibility of a massive number of physiological signal data, emotion analysis by using physiological signals is attracting an increasing attention and many methods have been reported by using electroencephalogram (EEG) or peripheral physiological signals. Although the prominent online learning methods can predict the emotion status with time varying physiological signals, it does not consider the reward of current operation in each iteration. To tackle this problem, in this paper, we propose a reinforcement online learning (ROL) method for real-time emotion state prediction by exploiting the reward to modify the predictor during the online training iterations. In each iteration, we evaluate the reward and then select some specific instances into predictor learning. It gains both significant time reduction and prominent performance. We apply the reinforcement online learning to least squares (LS) and support vector regression (SVR) for Emotion Prediction, respectively. Extensive experiments are conducted on artificial dataset and real-world physiological signal dataset (DEAP dataset) and the experimental results validate the effectiveness of the proposed method.\"",
        "Document: \"Incorporating Incremental and Active Learning for Scene Classification. Scene classification is useful for automatic organization of personal digital photographs or visual guidance of robots, but it is a time consuming and labor-intensive task to label adequate examples to train robust classifiers. Active learning is a key technique to reduce human-labeling burden by exploring an optimal subset from unlabeled data. In this paper we use a batch mode incremental and active learning framework to construct scene classification models. In traditional batch examples selection methods, there often exists redundancy information between these top informative examples. To alleviate the impact of redundancy, we employ two effective batch selection strategies which one is called multi-pool based BvSB and the other is called K-centroid cluster BvSB, experimental results with widely used 15 scene and UIUC-sports datasets demonstrated that our scheme can get better results than that only using BvSB measurement which does not considering redundancy. In order to improve efficiency, batch mode incremental support vector machines are employed. With the incremental learning scheme, the training process of active learning is much more efficient than that using all the selected examples to retrain the classification models in each round.\"",
        "Document: \"Data-driven facial animation via semi-supervised local patch alignment. This paper reports a novel data-driven facial animation technique which drives a neutral source face to get the expressive target face using a semi-supervised local patch alignment framework. We define the local patch and assume that there exists a linear transformation between a patch of the target face and the intrinsic embedding of the corresponding patch of the source face. Based on this assumption, we compute the intrinsic embeddings of source patches and align these embeddings to form the result. During the course of alignment, we use a set of motion data as shape regularizer to impel the result to approach the unknown target face. The intrinsic embedding can be computed through both locally linear embedding and local tangent space alignment. Experimental results indicate that the proposed framework can obtain decent face driving results. Quantitative and qualitative evaluations of the proposed framework demonstrate its superiority to existing methods. HighlightsAchieve facial animation through manifold-based method.The local patch is defined by the geometry of the face to capture the local topology.This is a semi-supervised framework which can be solved by least square method.\"",
        "Document: \"MeDJ: Multidimensional Emotion-aware Music Delivery for Adolescent. Music listening is an integral part of many adolescents? everyday lives, but it is also a time when adolescents are uniquely vulnerable. Emotional-oriented and avoidance through listening to unsuitable music may bring negative emotion to adolescents and increase the level of their depression. We propose MeDJ, a multidimensional emotion-aware music delivery application, which turns adolescents' music listening into a health and pleasure way. MeDJ aims at helping adolescents to improve emotional management and prevent depression. It is built on a cloud-based platform that enables adolescents and their peers collaboratively recommend suitable music to each other through smartphones. Prototype implementation and initial results of MeDJ have demonstrated its practicability for real-world deployment.\"",
        "Document: \"LF-EME: Local features with elastic manifold embedding for human action recognition. Human action recognition has been an active topic in computer vision. Currently, most of the approaches to this problem can be categorized into two classes. One is based on local features, and the other is based on global features. Meanwhile, manifold learning has become successful in many problems in computer vision, but because of the high variability of human body, the application of manifold learning to human action recognition is limited. We propose a framework based on Elastic Manifold Embedding (EME), a new sparse manifold learning algorithm, together with local interest point features to handle human action recognition. The result of the new framework is very promising in comparison with state-of-the-art methods.\"",
        "Document: \"Single image super-resolution via subspace projection and neighbor embedding. In this paper, we present a novel learning-based single image super-resolution algorithm to address the problems of inefficient learning and improper estimation in coping with nonlinear high-dimensional feature data. Our method named as subspace projection and neighbor embedding (SPNE) first projects the high-dimensional data into two different subspaces respectively, i.e., kernel principal component analysis (KPCA) subspace and modified locality preserving projection (MLPP) subspace to obtain the global and local structures of data. In an optimal low-dimensional feature space, the k-nearest neighbors of each input low-resolution (LR) image patch can be found for efficient learning. Then within similarity measures and proportional factors, the k embedding weights are used to estimate high-frequency information from a training dataset. Finally, we apply iterative back projection (IBP) to further enhance the super-resolution results. Experiments on simulative and actual LR images demonstrate that the proposed approach outperforms the existing NE-based super-resolution methods in terms of visual quality and some selected objective metrics.\"",
        "Document: \"Multiple Instance Learning Via Distance Metric Optimization. Multiple Instance Learning (MIL) has been widely applied in practice, such as drug activity prediction, content-based image retrieval. In MIL, a sample, comprised of a set of instances, is called a bag. Labels are assigned to bags instead of instances. The uncertainty of labels on instances makes MIL different from conventional supervised single instance learning (SIL) tasks. Therefore, it is critical to learn an effective mapping to convert an MIL task to an SIL task. In this paper, we present OptMILES by learning the optimal transformation on the bag-to-instance similarity measure, exploring the optimal distance metric between instances, by an alternating minimization training procedure. We thoroughly evaluate the proposed method on both a synthetic dataset and real world datasets by comparing with representative MIL algorithms. The experimental results suggest the effectiveness of OptMILES.\"",
        "1 is \"Real-time crowd motion analysis\", 2 is \"Correction of systematic odometry errors in mobile robots\"",
        "Given above information, for an author who has written the paper with the title \"An energy model approach to people counting for abnormal crowd behavior detection\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009578": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Energy Efficiency Maximization of Full-Duplex Two-Way Relay With Non-Ideal Power Amplifiers and Non-Negligible Circuit Power.':",
        "Document: \"Performance analysis of two-way MAC layer network coding under finite relay buffer and non-negligible signalling overhead. Two-way exclusive OR XOR relay can enable hidden nodes to exchange data with low delays and high data rate, while keeping signal processing simple. In this paper, we analyse practical two-way XOR relaying systems, where finite relay buffer, non-negligible signalling overhead, and lossy wireless channels are all captured. A two-layer model is developed to characterise such practical two-way relay systems, which is then reformulated into a Markov process after we project and combine inter-layer state transitions of the two-layer model. Using Markov techniques, we evaluate the steady state probabilities of the Markov process and, in turn, the key performance measures of two-way XOR relaying, such as throughput, delay, and packet loss. The accuracy of our model is validated by simulations. Our model can also be used as an online tool to configure the buffer resources, adapting to wireless channel conditions and signalling requirements. Copyright \u00a9 2016 John Wiley & Sons, Ltd.\"",
        "Document: \"Low complexity user pairing and resource allocation of heterogeneous users for uplink virtual MIMO system over LTE-A network. Virtual Multiple-Input Multiple-Output (MIMO) is a promising uplink technology that can meet the throughput demand of Long-Term Evolution-Advanced (LTE-A) systems. However, the complexity of scheduling virtual MIMO is a challenge; existing virtual MIMO is therefore limited to best effort applications. We investigate the resource allocation and scheduling problem in a heterogeneous virtual MIMO system where delay sensitive applications are present. The goal is to maximize the system throughput while maintaining delay bound for delay sensitive traffic. To tackle the complexity challenge, we propose two low-complexity suboptimal algorithms, where the key idea is to reduce the search space and iteratively minimize the rate loss respectively. Simulation results show that the rate loss minimization based heuristic algorithm converges to within 99% of the optimal throughput on average and maintains delay bound for delay sensitive users. It also achieves almost the same fairness performance as the optimal solution.\"",
        "Document: \"Blockchain's adoption in IoT: The challenges, and a way forward. The underlying technology of Bitcoin is blockchain, which was initially designed for financial value transfer only. Nonetheless, due to its decentralized architecture, fault tolerance and cryptographic security benefits such as pseudonymous identities, data integrity and authentication, researchers and security analysts around the world are focusing on the blockchain to resolve security and privacy issues of IoT. However, presently, not much work has been done to assess blockchain's viability for IoT and the associated challenges. Hence, to arrive at intelligible conclusions, this paper carries out a systematic study of the peculiarities of the IoT environment including its security and performance requirements and progression in blockchain technologies. We have identified the gaps by mapping the security and performance benefits inferred by the blockchain technologies and some of the blockchain-based IoT applications against the IoT requirements. We also discovered some practical issues involved in the integration of IoT devices with the blockchain. In the end, we propose a way forward to resolve some of the significant challenges to the blockchain's adoption in IoT.\"",
        "Document: \"Fog Computing-Assisted Energy-Efficient Resource Allocation for High-Mobility MIMO-OFDMA Networks. This paper presents a suboptimal approach for resource allocation of massive MIMO-OFDMA systems for high-speed train (HST) applications. An optimization problem is formulated to alleviate the severe Doppler effect and maximize the energy efficiency (EE) of the system. We propose to decouple the problem between the allocations of antennas, subcarriers, and transmit powers and solve the problem by carrying out the allocations separately and iteratively in an alternating manner. Fast convergence can be achieved for the proposed approach within only several iterations. Simulation results show that the proposed algorithm is superior to existing techniques in terms of system EE and throughput in different system configurations of HST applications.\"",
        "Document: \"Energy Efficiency Maximization of Full-Duplex Two-Way Relay With Non-Ideal Power Amplifiers and Non-Negligible Circuit Power. In this paper, we maximize the energy efficiency (EE) of full-duplex (FD) two-way relay (TWR) systems under non-ideal power amplifiers (PAs) and non-negligible transmission-dependent circuit power. We start with the case where only the relay operates full duplex and two timeslots are required for TWR. Then, we extend to the advanced case, where the relay and the two nodes all operate full duplex, ...\"",
        "Document: \"Provisioning quality-of-service to energy harvesting wireless communications. Energy harvesting (EH) is an innovative way to build long-term and self-sustainable wireless networks. However, an inconstant EH rate may have an adverse effect on the quality-of-service (QoS) of wireless traffic, such as packet delay and error. In this article we discuss techniques that provide QoS to EH powered wireless communications. A new \"dynamic string tautening\" method is presented to produce the most energy efficient schedule with substantially lower complexity, compared to convex optimization techniques. The method adapts to the bursty arrivals of wireless traffic and harvested energy, and ensures that delay-sensitive data will be delivered by deadline. Comprehensive designs of EH powered transmitters are also discussed, where the EH rate, battery capacity, and deadline requirement can be jointly adjusted to leverage QoS and the cost.\"",
        "Document: \"Effective Capacity Analysis in Ultra-Dense Wireless Networks With Random Interference. Ultra-dense networks (UDNs) provide a promising paradigm to cope with exponentially increasing mobile traffic. However, little work has to date considered unsaturated traffic with quality-of-service (QoS) requirements. This paper presents a new cross-layer analytical model to capture the unsaturated traffic of a UDN in the presence of QoS requirements. The effective capacity (EC) of the UDN is derived, taking into account small-scale channel fading and possible interference. Key properties of the EC are revealed. The amount of traffic impacts effective capacity of the UDN due to the sophisticated interactions among small base stations operating in the same frequency. The maximization of total effective capacity is formulated as a non-cooperative game in the paper. The best-response function is derived, iteratively searching the Nash equilibrium point. System simulation results indicate that our proposed model is accurate. The simulations also show the maximum allowed arrival rate with the QoS guarantee, compared with the full interference model.\"",
        "Document: \"A low overhead tree-based energy-efficient routing scheme for multi-hop wireless body area networks. Reliability and energy efficiency are key performance metrics for meeting the requirements of long-term and continuous health monitoring in Wireless Body Area Networks (WBANs). In this paper, we explore energy-efficient routing mechanisms for WBANs. We first present experimental results showing that wireless link quality changes rapidly in WBANs due to body shadowing, and a fixed transmission power results in either wasted energy or low reliability. Moreover, in multi-hop WBANs, as various vital signs are collected from sensors on different body parts, the traffic load among sensor nodes could be severely unbalanced, leading to uneven energy consumption. In this paper, we propose a scheme, we term the tree-based energy-efficient routing scheme (EERS), with low overhead to jointly address adaptive power control and routing in multi-hop WBANs. The proposed scheme can establish an energy-efficient end-to-end path as well as adaptively choose transmission power for sensor nodes. We conduct extensive experiments on a MicaZ WBAN testbed to compare the performance of EERS with the Collection Tree Protocol (CTP) in terms of packet reception ratio (PRR), collection delay, energy consumption, and energy balancing. Experimental results show that EERS outperforms CTP in terms of reliability, delay and energy consumption. In particular, EERS exhibits a mean delay 30% lower than the mean delay of CTP and an energy consumption 10% lower than CTP, while achieving at least 0.95 PRR.\"",
        "Document: \"New Game-Theoretic Approach to Decentralized Path Selection and Sleep Scheduling for Mobile Edge Computing. Network function virtualization (NFV) implements mobile edge computing (MEC) services as software appliances, and allows resources to be adaptively allocated to accommodate demand variations. Scalability and network cost (including operational cost and response latency) are key challenges. This paper presents a new game-theoretic approach to minimizing the network cost, where access points (APs) select MEC servers and routes in a decentralized manner, and unloaded routers and links are deactivated for cost saving. The key idea is that we interpret the minimization of network cost as a mixed game with a non-monotonic cost function capturing both the operational cost and response latency. We prove that the game is conditionally an ordinary potential game and converges to \n<inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$\\alpha $ </tex-math></inline-formula>\n-approximate equilibriums. A closed-form expression is derived for the convergence delay. Another important aspect is that we integrate Stackelberg routing into the proposed mixed game to avoid inefficient equilibriums (with high cost or latency). We prove that the mixed game can converge faster to better equilibriums under linear response latency models. Extensive simulations corroborate the new game-theoretic approach can significantly outperform existing techniques in terms of efficiency, convergence, and scalability.\"",
        "Document: \"Software-defined wireless networking: centralized, distributed, or hybrid?. Scalability is a key issue in large scale WDNs, such as vehicular networks and device-to-device networks. To address the issue, this article extends the SDN concept, and presents a new network architecture that eliminates the need of multi-hop flooding for route discovery, thereby enabling WDNs to scale. The key idea of the new architecture is to split network control and data forwarding by using two separate frequency bands. Another important aspect of the architecture is that computational complexity of routing is split between the SDN controller and the forwarding nodes, thereby allowing nodes to make distributed routing decisions. As a result, network control of the new architecture has a hybrid structure, which improves the operability and scalability of large scale WDNs. Our case study shows that the new architecture is able to substantially improve scalability and reliability of WDNs, especially in mobile environments.\"",
        "1 is \"Backscatter Multiplicative Multiple-Access Systems: Fundamental Limits and Practical Design.\", 2 is \"Energy-harvesting powered transmissions of bursty data packets with strict deadlines\"",
        "Given above information, for an author who has written the paper with the title \"Energy Efficiency Maximization of Full-Duplex Two-Way Relay With Non-Ideal Power Amplifiers and Non-Negligible Circuit Power.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009602": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'A computer system architecture providing a user-friendly man machine interface for accessing assistive technology in cloud computing.':",
        "Document: \"Towards Osmotic Computing: Analyzing Overlay Network Solutions to Optimize the Deployment of Container-Based Microservices in Fog, Edge and IoT Environments. In recent years, the rapid growth of new Cloud technologies acted as an enabling factor for the adoption of microservices based architecture that leverages container virtualization in order to build modular and robust systems. As the number of containers running on hosts increases, it becomes essential to have tools to manage them in a simple, straightforward manner and with a high level of abstraction. Osmotic Computing is an emerging research field that studies the migration, deployment and optimization of microservices from the Cloud to Fog, Edge, and Internet of Things (IoT) environments. However, in order to achieve Osmotic Computing environments, connectivity issues have to be addressed. This paper investigates these connectivity issues leveraging different network overlays. In particular, we analyze the performance of four network overlays that are OVN, Calico, Weave, and Flannel. Our results give a concrete overview in terms of overhead and performances for each proposed overlay solution, helping us to understand which the best overlay solution is. Specifically, we deployed CoAP and FTP microservices which helped us to carry out these benchmarks and collect the results in terms of transfer times.\"",
        "Document: \"How cloud computing can support on-demand assistive services. This paper investigates how Cloud computing can meet the demands of people with disabilities who occasionally use a shared computer. In this situation, customized assistive software can not be available to the user since security policies prevent from having enough privileges to change local system preferences. In order to address such issue, we discuss an open source software architecture combining a web-based remote desktop management solution with virtualization technology. This system allows disabled users to access a virtual desktop running personal assistive software solutions. Hence, the disabled user can interact with the same virtual environment from any networked physical computer via a standard web browser. In the end, we discuss the major technological issue for the achievement of such a scenario.\"",
        "Document: \"Security Requirements in a Federated Cloud Networking Architecture. Cloud federation enables cloud providers to collaborate in order to create a large pool of virtual resources at multiple network locations. Different types of federated cloud architectures have been proposed and implemented up to now. In this context, an effective, agile and secure federation of cloud networking resources is a key aspect for the deployment of federated applications. This paper presents the preliminary security requirements analyzed in the H2020 BEACON Project that aims at researching techniques to federate cloud network resources and defining an integrated cloud management layer that enables an efficient and secure deployment of federated cloud applications. The paper analyses both how to protect the cloud networking infrastructure, and how cloud users can customize the network security for their distributed applications.\"",
        "Document: \"Towards the integration between IoT and Cloud computing: an approach for the secure self-configuration of embedded devices. AbstractThe secure boot-up and setup of Internet of Things (IoT) devices connected over the Cloud represent a challenging open issue. This paper deals with the automatic configuration of IoT devices in a secure way through the Cloud, in order to provide new added-value services. After a discussion on the limits of current IoT and Cloud solutions in terms of secure self-configuration, we present a Cloud-based architecture that allows IoT devices to interact with several federated Cloud providers. In particular, we present two possible scenarios, that is, single Cloud and a federated Cloud environments, interacting with IoT devices and we address specific issues of both. Moreover, we present several design highlights on how to operate considering real open hardware and software products already available in the market.\"",
        "Document: \"Security and Cloud Computing: InterCloud Identity Management Infrastructure. Cloud Computing is becoming one of the most important topics in the IT world. Several challenges are being raised from the adoption of this computational paradigm including security, privacy, and federation. This paper aims to introduce new concepts in cloud computing and security, focusing on heterogeneous and federated scenarios. We present a reference architecture able to address the Identity Management (IdM) problem in the InterCloud context and show how it can be successfully applied to manage the authentication needed among clouds for the federation establishment.\"",
        "Document: \"A Federated System for MapReduce-Based Video Transcoding to Face the Future Massive Video-Selfie Sharing Trend. The massive use of mobile devices and social networks is causing the birth of a new compulsive users' behaviour. The activity photo selfie sharing is gradually turning into video selfie. These videos will be transcoded into multiple formats to support different visualization mode. We think there will be the need to have systems that can support, in a fast, efficient and scalable way, the millions of requests for video sharing and viewing. We think that a single Cloud Computing services provider cannot alone cope with this huge amount of incoming data (Big Data), so in this paper we propose a Cloud Federation-based system that exploiting the Hadoop MapReduce paradigm performs the video transcoding in multiple format and its distribution in a fastest and most efficient possible way. Experimental results highlight the major factors involved for job deployment in a federated Cloud environment and the efficiency of the proposed system and show how the Federation improves the performances of a MapReduce Job execution acting on a additional parallelization level.\"",
        "Document: \"An Authentication Model for IoT Clouds. Nowadays, the combination between Cloud computing and Internet of Things (IoT) is pursuing new levels of efficiency in delivering services, representing a tempting business opportunity for IT operators of increasing their revenues. However, security is considered as one of the major factors that slows down the rapid and large scale adoption and deployment of both IoT and Cloud computing. In this paper, considering such an IoT Cloud scenario, we present an architectural model and several use cases that allow different types of users to access IoT devices.\"",
        "Document: \"An approach to reduce energy costs through virtual machine migrations in cloud federation. Cloud federation offers new business models to enforce more flexible energy management strategies. Independent Cloud providers are exclusively bounded to the specific energy supplier powering its Data Centers. The situation radically change if we consider a federation of cooperating Cloud providers. In such a context a proper migration of virtual machines among providers can lead to a global energy cost-saving strategy. In this paper, we present an approach to reduce energy cost in a federated Cloud ecosystem. More specifically, we propose an algorithm that allows providers to determine a map of possible destinations for cost-evaluation. Furthermore, we introduce an additional algorithm to determine the optimum energy cost migration path, and, consequently, the best Cloud Data Center where virtual machines should be migrated in order to push down energy costs.\"",
        "Document: \"A framework for real time end to end monitoring and big data oriented management of smart environments. Nowadays, the success of Internet of Things (IoT) applications depends on the intelligence of tools and techniques that can monitor, manage, and verify the correct operations of smart ecosystems including sensors and big data analytics tools, typically deployed in Cloud and Edge computing datacenters. In this paper, we propose a framework for the monitoring and management of IoT system that integrates the AllJoyn functionalities, useful to interconnect IoT devices, MongoDB, to implement Big Data storage, and Storm, to run real-time data analytics. We implemented the proposed framework and we tested its main functionalities in a smart home application scenario. In our experimentation, we investigated three different data patterns, i.e., regular, event-based, and automated, in order to evaluate performance of our framework in terms of response time under different operational conditions. Experimental results show that the latency of the monitoring and service strongly depends on the type of management application running in the system, whereas it is lightly affected by the data patterns.\"",
        "Document: \"An Approach to Enable Cloud Service Providers to Arrange IaaS, PaaS, and Saas Using External Virtualization Infrastructures. Nowadays, the cloud computing ecosystem is more and more distributed and heterogeneous. Cloud service providers begin to build their services using cloud-based services offered by other service providers. This raises several issues due to integration between services and provider themselves. In this paper, we propose a practice addressing such a concern in a \"Vertical Supply Chain\" scenario of distributed clouds.\"",
        "1 is \"Secure integration of IoT and Cloud Computing.\", 2 is \"Internet X.509 Public Key Infrastructure Certificate and CRL Profile\"",
        "Given above information, for an author who has written the paper with the title \"A computer system architecture providing a user-friendly man machine interface for accessing assistive technology in cloud computing.\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009613": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'Adaptive notch filtering in the presence of colored noise':",
        "Document: \"Detection and estimation of biochemical sources in arbitrary 2D environments. We develop algorithms for biochemical detection and estimation in arbitrary two-dimensional (2D) environments using integrated sensor arrays. The development of biochemical sensor techniques has been a subject of considerable research interest in recent years. We propose statistical algorithms for automatic monitoring of biochemical agents in realistically shaped 2D environments using multi-sensor measurements. We derive models for the concentration distribution using the diffusion equation and finite element approximations. Using these results, we develop parametric statistical models and a maximum likelihood estimation algorithm to find the parameters of the biochemical agent. To detect the presence of a source, we develop a generalized likelihood ratio test. We demonstrate the applicability of our techniques through numerical examples. Our results are potentially useful for national security, environmental engineering, food industry, oil industry, etc.\"",
        "Document: \"Adaptive notch filtering in the presence of colored noise. The authors analyze the convergence of several adaptive notch filter algorithms for sine waves in colored noise, which were recently proposed in the literature. After pointing out the previous algorithms' potential convergence problems, an algorithm is proposed for this problem that does not have convergence problems and provides accurate estimation results\"",
        "Document: \"Wideband source localization using a distributed acoustic vector-sensor array. We derive fast wideband algorithms, based on measurements of the acoustic intensity, for determining the bearings of a target using an acoustic vector sensor (AVS) situated in free space or on a reflecting boundary. We also obtain a lower bound on the mean-square angular error (MSAE) of such estimates. We then develop general closed-form weighted least-squares (WLS) and reweighted least-squares algorithms that compute the three-dimensional (3-D) location of a target whose bearing to a number of dispersed locations has been measured. We devise a scheme for adaptively choosing the weights for the WLS routine when measures of accuracy for the bearing estimates, such as the lower bound on the MSAE, are available. In addition, a measure of the potential estimation accuracy of a distributed system is developed based on a two-stage application of the Cramer-Rao bound. These 3-D results are quite independent of how bearing estimates are obtained. Naturally, the two parts of the paper are tied together by examining how well distributed arrays of AVSs located on the ground, seabed, and in free space can determine the 3-D position of a target The results are relevant to the localization of underwater and airborne sources using freely drifting, moored, or ground sensors. Numerical simulations illustrate the effectiveness of our estimators and the new potential performance measure.\"",
        "Document: \"MUSIC, maximum likelihood and Cramer-Rao bound: further results and comparisons. A number of results have been presented recently on the statistical performance of the multiple signal characterization (MUSIC) and the maximum-likelihood (ML) estimators for determining the direction of arrival of narrowband plane waves using sensor arrays and the related problem of estimating the parameters of superimposed signals from noisy measurements. It is shown that in the class of weighted MUSIC estimators, the unweighted MUSIC achieves the best performance (i.e. the minimum variance of estimation errors) in large samples. The covariance matrix of the ML estimator is derived, and detailed analytic studies of the statistical efficiency of MUSIC and ML estimators are presented. These studies include performance comparisons of MUSIC and MLE with each other as well as with the ultimate performance corresponding to the Cramer-Rao bound (CRB)\"",
        "Document: \"Multiple Rao-Blackwellized particle filtering for target tracking in urban environments. We propose a new filtering algorithm for joint tracking of multiple target states and the channel state between each pair of antennas in a radar network. The problem of tracking multiple targets in complex scenarios, such as an urban environment, poses a computational challenge as standard particle filtering (SPF) requires large number of particles to obtain an accurate estimate of the high-dimensional state vector. In this paper, we develop a hybrid filter based on the combination of multiple particle filtering (MPF) and Rao-Blackwellized particle filtering (RBPF) by exploiting the structure in the state-space model. Numerical simulations show that the proposed multiple Rao-Blackwellized particle filtering (MRBPF) performs better than the SPF and the RBPF.\"",
        "Document: \"High-order polynomial root tracking algorithm. A new, efficient algorithm for tracking the roots of time-varying polynomials with complex coefficients is presented. The algorithm updates a vector of polynomial roots in response to a perturbation in polynomial coefficients. The update requires only the solution of a single set of linear equations. The algorithm has been used successfully to track the roots of high-order polynomials. The accuracy of the algorithm can be improved by iteration. When operated iteratively, it converges rapidly, and usually requires less than ten iterations to reach the maximum accuracy achievable using sixteen significant digital arithmetic\"",
        "Document: \"Biologically Inspired Coupled Antenna Array for Direction-of-Arrival Estimation. We propose to design a small-size antenna array having high direction-of-arrival (DOA) estimation performance, inspired by the Ormia ochracea's coupled ears. The female Ormia is able to locate male crickets' call accurately, for reproduction purposes, despite the small distance between its ears compared with the incoming wavelength. This phenomenon has been explained by the mechanical coupling between the Ormia's ears, modeled by a pair of differential equations. In this paper, we first solve the differential equations governing the Ormia ochracea's ear response, and convert the response to the prespecified radio frequencies. Using the converted response, we then implement the biologically inspired coupling as a multi-input multi-output filter on a uniform linear antenna array output. We derive the maximum likelihood estimates of source DOAs, and compute the corresponding Cram\u00e9r\u2013Rao bound on the DOA estimation error as a performance measure. We also consider a circular array configuration and compute the mean-square angular error bound on the three-dimensional localization accuracy. Moreover, we propose an algorithm to optimally choose the biologically inspired coupling for maximum localization performance. We use Monte Carlo numerical examples to demonstrate the advantages of the coupling effect.\"",
        "Document: \"Hierarchical particle filtering for target tracking in multi-modal sensor networks. We propose a filtering method, called hierarchical particle filtering, for multi-modal sensor networks in which the unknown state vector is observed, through the measurements, in a hierarchical fashion. We partition the state space and the measurement space into lower dimensional subspaces. At each stage, we find an estimate of one partition using the measurements from the corresponding partition, and the information from the previous stages. We use hierarchical particle filtering for joint initiation, termination and tracking of multiple targets using multi-modal measurements. Numerical simulations demonstrate that the proposed filtering method accurately identifies the number and the categories of targets, and produces a lower mean-squared error (MSE) compared to the MSE obtained using a standard particle filter.\"",
        "Document: \"Electromagnetic vector sensors and active target localization and identification. A new model for actively localizing and identifying targets using electromagnetic vector sensors is developed and analyzed. Target dependent variables are introduced that provide a natural parametrization of the statistics of the target's reflection characteristics (scattering matrix) to arbitrarily polarized transmitted signals. Cramer-Rao bounds on the variance of unbiased estimators of these parameters are derived to show how an estimator's expected performance depends on the target's state and the transmitted signal. The parameters are shown to have physical interpretations and to represent characteristics intrinsic to the target.<>\"",
        "Document: \"Performance bounds for estimating vector systems. We propose a unified framework for the analysis of estimators of geometrical vector quantities and vector systems through a collection of performance measures. Unlike standard performance indicators, these measures have intuitive geometrical and physical interpretations, are independent of the coordinate reference frame, and are applicable to arbitrary parameterizations of the unknown vector or system of vectors. For each measure, we derive both finite-sample and asymptotic lower bounds that hold for large classes of estimators and serve as benchmarks for the assessment of estimation algorithms. Like the performance measures themselves, these bounds are independent of the reference coordinate frame, and we discuss their use as system design criteria\"",
        "1 is \"Optimal Polarized Waveform Design For Active Target Parameter Estimation Using Electromagnetic Vector Sensors\", 2 is \"On the implementation and performance of single and double differential detection schemes\"",
        "Given above information, for an author who has written the paper with the title \"Adaptive notch filtering in the presence of colored noise\", which reference is related? Just choose 1 or 2 without further explanation."
    ],
    "009621": [
        "Here are the related document ranked by relevance, from most relevant to least relevant, for the topic of 'EmotionGAN: Unsupervised Domain Adaptation for Learning Discrete Probability Distributions of Image Emotions.':",
        "Document: \"Transfer Feature Learning with Joint Distribution Adaptation. Transfer learning is established as an effective technology in computer vision for leveraging rich labeled data in the source domain to build an accurate classifier for the target domain. However, most prior methods have not simultaneously reduced the difference in both the marginal distribution and conditional distribution between domains. In this paper, we put forward a novel transfer learning approach, referred to as Joint Distribution Adaptation (JDA). Specifically, JDA aims to jointly adapt both the marginal distribution and conditional distribution in a principled dimensionality reduction procedure, and construct new feature representation that is effective and robust for substantial distribution difference. Extensive experiments verify that JDA can significantly outperform several state-of-the-art methods on four types of cross-domain image classification problems.\"",
        "Document: \"Latent semantic sparse hashing for cross-modal similarity search. Similarity search methods based on hashing for effective and efficient cross-modal retrieval on large-scale multimedia databases with massive text and images have attracted considerable attention. The core problem of cross-modal hashing is how to effectively construct correlation between multi-modal representations which are heterogeneous intrinsically in the process of hash function learning. Analogous to Canonical Correlation Analysis (CCA), most existing cross-modal hash methods embed the heterogeneous data into a joint abstraction space by linear projections. However, these methods fail to bridge the semantic gap more effectively, and capture high-level latent semantic information which has been proved that it can lead to better performance for image retrieval. To address these challenges, in this paper, we propose a novel Latent Semantic Sparse Hashing (LSSH) to perform cross-modal similarity search by employing Sparse Coding and Matrix Factorization. In particular, LSSH uses Sparse Coding to capture the salient structures of images, and Matrix Factorization to learn the latent concepts from text. Then the learned latent semantic features are mapped to a joint abstraction space. Moreover, an iterative strategy is applied to derive optimal solutions efficiently, and it helps LSSH to explore the correlation between multi-modal representations efficiently and automatically. Finally, the unified hashcodes are generated through the high level abstraction space by quantization. Extensive experiments on three different datasets highlight the advantage of our method under cross-modal scenarios and show that LSSH significantly outperforms several state-of-the-art methods.\"",
        "Document: \"Madan: Multi-Source Adversarial Domain Aggregation Network For Domain Adaptation. Domain adaptation aims to learn a transferable model to bridge the domain shift between one labeled source domain and another sparsely labeled or unlabeled target domain. Since the labeled data may be collected from multiple sources, multi-source domain adaptation (MDA) has attracted increasing attention. Recent MDA methods do not consider the pixel-level alignment between sources and target or the misalignment across different sources. In this paper, we propose a novel MDA framework to address these challenges. Specifically, we design a novel Multi-source Adversarial Domain Aggregation Network (MADAN). First, an adapted domain is generated for each source with dynamic semantic consistency while aligning towards the target at the pixel-level cycle-consistently. Second, sub-domain aggregation discriminator and cross-domain cycle discriminator are proposed to make different adapted domains more closely aggregated. Finally, feature-level alignment is performed between the aggregated domain and the target domain while training the task network. For the segmentation adaptation, we further enforce category-level alignment and incorporate multi-scale image generation, which constitutes MADAN+. We conduct extensive MDA experiments on digit recognition, object classification, and simulation-to-real semantic segmentation tasks. The results demonstrate that the proposed MADAN and MADAN+ models outperform state-of-the-art approaches by a large margin.\"",
        "Document: \"Ring Fingerprint Based on Interest Points for Video Copy Detection. In the field of information security, assurance, copyright protection etc., content-based copy detection is more and more important, which consists of two technologies - fingerprint extraction and fingerprint matching. Fingerprints extracted from videos are mainly described as global video fingerprints and local video fingerprints. To make best use of the advantages and bypass the disadvantages of different video fingerprints, we propose the ring fingerprint based on interest points and ordinal measure in this paper. A last we examine the proposed method with lots of experiments and discuss the performance of approaches. The experimental results also demonstrate the effectiveness of the proposed method for video copy detection.\"",
        "Document: \"Real-Time Multimedia Social Event Detection in Microblog. Detecting events from massive social media data in social networks can facilitate browsing, search, and monitoring of real-time events by corporations, governments, and users. The short, conversational, heterogeneous, and real-time characteristics of social media data bring great challenges for event detection. The existing event detection approaches rely mainly on textual information, while the v...\"",
        "Document: \"Learning to Hash With Optimized Anchor Embedding for Scalable Retrieval. Sparse representation and image hashing are powerful tools for data representation and image retrieval respectively. The combinations of these two tools for scalable image retrieval, i.e., sparse hashing (SH) methods, have been proposed in recent years and the preliminary results are promising. The core of those methods is a scheme that can efficiently embed the (high-dimensional) image features into a low-dimensional Hamming space, while preserving the similarity between features. Existing SH methods mostly focus on finding better sparse representations of images in the hash space. We argue that the anchor set utilized in sparse representation is also crucial, which was unfortunately underestimated by the prior art. To this end, we propose a novel SH method that optimizes the integration of the anchors, such that the features can be better embedded and binarized, termed as Sparse Hashing with Optimized Anchor Embedding. The central idea is to push the anchors far from the axis while preserving their relative positions so as to generate similar hashcodes for neighboring features. We formulate this idea as an orthogonality constrained maximization problem and an efficient and novel optimization framework is systematically exploited. Extensive experiments on five benchmark image data sets demonstrate that our method outperforms several state-of-the-art related methods.\"",
        "Document: \"Generating virtual ratings from chinese reviews to augment online recommendations. Collaborative filtering (CF) recommenders based on User-Item rating matrix as explicitly obtained from end users have recently appeared promising in recommender systems. However, User-Item rating matrix is not always available or very sparse in some web applications, which has critical impact to the application of CF recommenders. In this article we aim to enhance the online recommender system by fusing virtual ratings as derived from user reviews. Specifically, taking into account of Chinese reviews' characteristics, we propose to fuse the self-supervised emotion-integrated sentiment classification results into CF recommenders, by which the User-Item Rating Matrix can be inferred by decomposing item reviews that users gave to the items. The main advantage of this approach is that it can extend CF recommenders to some web applications without user rating information. In the experiments, we have first identified the self-supervised sentiment classification's higher precision and recall by comparing it with traditional classification methods. Furthermore, the classification results, as behaving as virtual ratings, were incorporated into both user-based and item-based CF algorithms. We have also conducted an experiment to evaluate the proximity between the virtual and real ratings and clarified the effectiveness of the virtual ratings. The experimental results demonstrated the significant impact of virtual ratings on increasing system's recommendation accuracy in different data conditions (i.e., conditions with real ratings and without).\"",
        "Document: \"Large-scale Cross-modality Search via Collective Matrix Factorization Hashing. By transforming data into binary representation, i.e., Hashing, we can perform high-speed search with low storage cost, and thus, Hashing has collected increasing research interest in the recent years. Recently, how to generate Hashcode for multimodal data (e.g., images with textual tags, documents with photos, and so on) for large-scale cross-modality search (e.g., searching semantically related images in database for a document query) is an important research issue because of the fast growth of multimodal data in the Web. To address this issue, a novel framework for multimodal Hashing is proposed, termed as Collective Matrix Factorization Hashing (CMFH). The key idea of CMFH is to learn unified Hashcodes for different modalities of one multimodal instance in the shared latent semantic space in which different modalities can be effectively connected. Therefore, accurate cross-modality search is supported. Based on the general framework, we extend it in the unsupervised scenario where it tries to preserve the Euclidean structure, and in the supervised scenario where it fully exploits the label information of data. The corresponding theoretical analysis and the optimization algorithms are given. We conducted comprehensive experiments on three benchmark data sets for cross-modality search. The experimental results demonstrate that CMFH can significantly outperform several state-of-the-art cross-modality Hashing methods, which validates the effectiveness of the proposed CMFH.\"",
        "Document: \"Active Learning with Cross-Class Knowledge Transfer. When there are insufficient labeled samples for training a supervised model, we can adopt active learning to select the most informative samples for human labeling, or transfer learning to transfer knowledge from related labeled data source. Combining transfer learning with active learning has attracted much research interest in recent years. Most existing works follow the setting where the class labels in source domain are the same as the ones in target domain. In this paper, we focus on a more challenging cross-class setting where the class labels are totally different in two domains but related to each other in an intermediary attribute space, which is barely investigated before. We propose a novel and effective method that utilizes the attribute representation as the seed parameters to generate the classification models for classes. And we propose a joint learning framework that takes into account the knowledge from the related classes in source domain, and the information in the target domain. Besides, it is simple to perform uncertainty sampling, a fundamental technique for active learning, based on the framework. We conduct experiments on three benchmark datasets and the results demonstrate the efficacy of the proposed method.\"",
        "Document: \"Distributed video coding based on part intracoding and soft side information estimation. Recently, distributed source coding (DSC) has been proposed to implement source compression by exploiting source statistics at the decoder only, which enables low-complexity video coding. However, to date, the video codecs based on DSC have been unable to compress as efficiently as traditional predictive video codecs, such as H.264. So, new techniques have to be investigated to improve the performance of the distributed video coding scheme for practical applications. In this paper, I propose a novel distributed video coding scheme based on part intracoding and soft side information estimation. Firstly, at the encoder side, to improve the compression performance of distributed video coding system, we divide the video data into strongly correlative data encoded by Slepian---Wolf codec and weakly correlative data encoded by Intracoding codec. Secondly, at the decoder side, to improve the accuracy of side information estimation, a soft side information estimation method is proposed, which is more suitable for video coding due to the non-stationary feature of video data. Our experimental results show that the performance of our coding system is better than that of the traditional distributed video coding system while keeping the simple encoding property. Also the concept of soft side information is a new idea in distributed video coding and will significantly influence the side information estimation method.\"",
        "1 is \"From contours to regions: An empirical evaluation\", 2 is \"SEDA: an architecture for well-conditioned, scalable internet services\"",
        "Given above information, for an author who has written the paper with the title \"EmotionGAN: Unsupervised Domain Adaptation for Learning Discrete Probability Distributions of Image Emotions.\", which reference is related? Just choose 1 or 2 without further explanation."
    ]
}