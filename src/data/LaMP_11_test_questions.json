[
    {
        "id": "020",
        "input": "For an author who has written the paper with the title \"Planning and learning in permutation groups\", which reference is related? Just answer with [1] or [2] without explanation. [1]: \"Provably good routing in graphs: regular arrays\" [2]: \"Learning bilinear models for two-factor problems in vision.\"",
        "profile": [
            {
                "title": "Competitive distributed file allocation",
                "abstract": "This paper deals with the file allocation problem (BFR92) con- cerning the dynamic optimization of communication costs to ac- cess data in a distributed environment. We develop a dynamic file re-allocation strategy that adapts online to a sequence of r ead and write requests whose location and relative frequencies are com- pletely unpredictable. This is achieved by replicating the file in response to read requests and migrating the file in response to write requests while paying the associated communications costs, so as to be closer to processors that access it frequently. We develop first explicit deterministic online strategy assuming existence of global information about the state of the network; previous (deterministic) solutions were non-constructiv e and more expensive. Our solution has (optimal) logarithmic competitive ratio. The paper also contains the first explicit deterministic data migration (BS89) algorithm achieving the best known competitive ratio for this problem. Using somewhat different technique, we also develop the first deterministic distributed file allocation algorithm (using only lo- cal information) with poly-logarithmic competitive ratio against a globally-optimized optimal prescient strategy.",
                "id": "0200"
            },
            {
                "title": "Competitive algorithms for the weighted server problem",
                "abstract": "In this paper we deal with a generalization of the k -server problem (Manasse 1988), in which the servers are unequal. In the weighted server model each of the servers is assigned a positive weight. The cost associated with moving a server equals the product of the distance traversed and the server weight. A weighted k -server algorithm is called competitive if the competitive ratio depends only upon the number of servers (i.e., the competitive ratio is independent of the weights associated with the servers and the number of points in the metric space). For the uniform metric space, we give super exponential 2 2 O( k ) -competitive algorithms for any set of weights. If the servers have one of two possible weights, we give deterministic exponential ( k O( k ) ) competitive algorithms and randomized polynomial \u00d5( k 3 ) competitive algorithms. We use the MIN operator for both algorithms. This is the first true application of the randomized MIN operator (Fiat 1991). We show that for any metric space there exists some set of weights such that the deterministic competitive ratio must be exponential ( k \u03a9( k ) ). If the servers are limited to be one of two possible weights then there exist two such weights such that the competitive ratio has a lower bound of 2 \u03a9( k ) . With the randomized upper bound above, this shows a clear separation between deterministic and randomized algorithms for the problem of two weights. One can model the problem of storage management for RAM and E 2 PROM type memories as a weighted server problem with two weights on the uniform metric space.",
                "id": "0201"
            },
            {
                "title": "Rigorous time/space tradeoffs for inverting functions",
                "abstract": "We provide rigorous time-space tradeoffs for inverting any function. Given a function f, we give a time space tradeoff of TS2 = lf3g(~), where q(f) is the probability that two random elements are mapped to the same image under f. We also give a more general tradeoff, TS3 = N3, that can invert any function at any point.",
                "id": "0202"
            },
            {
                "title": "Competitive Algorithms for Layered Graph Traversal",
                "abstract": "A layered graph is a connected, weighted graph whose vertices are partitioned into sets L/sub 0/=(s), L/sub 1/, L/sub 2/, . . ., and whose edges run between consecutive layers. Its width is max( mod L/sub i/ mod ). In the online layered graph traversal problem, a searcher starts at s in a layered graph of unknown width and tries to reach a target vertex t; however, the vertices in layer i and the edges between layers i-1 and i are only revealed when the searcher reaches layer i-1. The authors give upper and lower bounds on the competitive ratio of layered graph traversal algorithms. They give a deterministic online algorithm that is O(9w)-competitive on width-w graphs and prove that for no w can a deterministic online algorithm have a competitive ratio better than 2w/sup -2/ on width-w graphs. They prove that for all w, w/2 is a lower bound on the competitive ratio of any randomized online layered graph traversal algorithm. For traversing layered graphs consisting of w disjoint paths tied together at a common source, they give a randomized online algorithm with a competitive ratio of O(log w) and prove that this is optimal up to a constant factor.",
                "id": "0203"
            },
            {
                "title": "Batch RSA",
                "abstract": "We present a variant of the RSA algorithm called Batch RSA with two important properties:",
                "id": "0204"
            },
            {
                "title": "Tight lower bounds on envy-free makespan approximation",
                "abstract": "In this work we give a tight lower bound on makespan approximation for envy-free allocation mechanisms dedicated to scheduling tasks on unrelated machines. Specifically, we show that no mechanism exists that can guarantee an envy-free allocation of jobs to m machines with a makespan less than a factor of O(logm) of the minimal makespan. Combined with previous results, this paper definitively proves that the optimal algorithm for obtaining a minimal makespan for any envy-free division can at best approximate the makespan to a factor of O(logm).",
                "id": "0205"
            },
            {
                "title": "Strong Price of Anarchy for Machine Load Balancing",
                "abstract": "As deflned by Aumann in 1959, a strong equilibrium is a Nash equilibrium that is resilient to deviations by coalitions. We give tight bounds on the strong price of anarchy for load balancing on related machines. We also give tight bounds for k-strong equilibria, where the size of a deviating coalition is at most k.",
                "id": "0206"
            },
            {
                "title": "Minimal Indices for Successor Search.",
                "abstract": "We give a new successor data structure which improves upon the index size of the Patrascu-Thorup data structures, reducing the index size from O(nw(4/5)) bits to O(n log w) bits, with optimal probe complexity. Alternatively, our new data structure can be viewed as matching the space complexity of the (probe-suboptimal) z-fast trie of Belazzougui et al. Thus, we get the best of both approaches with respect to both probe count and index size. The penalty we pay is an extra O(log w) interregister operations. Our data structure can also be used to solve the weak prefix search problem, the index size of O(n log w) bits is known to be optimal for any such data structure. The technical contributions include highly efficient single word indices, with out-degree w/log w (compared to the w(1/5) out-degree of fusion tree based indices). To construct such high efficiency single word indices we device highly efficient bit selectors which, we believe, are of independent interest.",
                "id": "0207"
            },
            {
                "title": "Envy, Multi Envy, and Revenue Maximization",
                "abstract": "We study the envy free pricing problem faced by a seller who wishes to maximize revenue by setting prices for bundles of items. If there is an unlimited supply of items and agents are single minded then we show that finding the revenue maximizing envy free allocation/pricing can be solved in polynomial time by reducing it to an instance of weighted independent set on a perfect graph.We define an allocation/pricing as multi envy free if no agent wishes to replace her allocation with the union of the allocations of some set of other agents and her price with the sum of their prices. We show that it is coNP-hard to decide if a given allocation/pricing is multi envy free. We also show that revenue maximization multi envy free allocation/pricing is APX hard.Furthermore, we give efficient algorithms and hardness results for various variants of the highway problem.",
                "id": "0208"
            },
            {
                "title": "Bi-criteria linear-time approximations for generalized k-mean/median/center",
                "abstract": "We consider the problem of approximating a set P of n points in Rd by a collection of j-dimensional flats, andextensions thereof, under the standard median / mean / centermeasures, in which we wish to minimize, respectively, the sum of thedistances from each point of P to its nearest flat, the sum of thesquares of these distances, or the maximal such distance.Such problems cannot be approximated unless P=NP but do allowbi-criteria approximations where one allows some leeway in both the numberof flats and the quality of the objective function.We give a very simple bi-criteria approximation algorithm, which producesat most \u03b1(k,j,n) = (k j log n)O(j) flats, which exceeds the optimalobjective value for any k j-dimensional flats by a factor of nomore than \u03b2(j)= 2O(j). Given this bi-criteria approximation, wecan use it to reduce the approximation factor arbitrarily, at the costof increasing the number of flats. Our algorithm hasmany advantages over previous work, in that it is muchmore widely applicable (wider set of objective functions and classes ofclusters) and much more efficient -- reducing the running time bound from O(n Poly(k,j)) to nd \u00b7 (jk)O(j).Our algorithm is randomized and successful with probability 1/2(easily boosted to probabilities arbitrary close to 1).",
                "id": "0209"
            },
            {
                "title": "Efficient contention resolution protocols for selfish agents",
                "abstract": "We seek to understand behavior of selfish agents accessing a broadcast channel. In particular, we consider the natural agent utility where costs are proportional to delay. Access to the channel is modelled as a game in extensive form with simultaneous play. Standard protocols such as Aloha are vulnerable to manipulation by selfish agents. We show that choosing appropriate transmission probabilities for Aloha to achieve equilibrium implies exponentially long delays. We give a very simple protocol for the agents that is in Nash equilibrium and is also very efficient --- other than with exponentially negligible probability --- all n agents will successfully transmit within cn time, for some small constant c.",
                "id": "02010"
            },
            {
                "title": "Rigorous Time/Space Trade-offs for Inverting Functions",
                "abstract": "We provide rigorous time/space trade-offs for inverting any function. Given a function f, we give a time/space trade-off of T S2 = N3 q(f), where q(f) is the probability that two random elements (taken with replacement) are mapped to the same image under f. We also give a more general trade-off, T S3 = N3, that can invert any function at any point.",
                "id": "02011"
            },
            {
                "title": "Zero knowledge proofs of identity",
                "abstract": "In this paper we extend the notion of zero knowledge proofs of membership (which reveal one bit of information) to zero knowledge proofs of knowledge (which reveal no information whatsoever). After formally defining this notion, we show its relevance to identification schemes, in which parties prove their identity by demonstrating their knowledge rather than by proving the validity of assertions. We describe a novel scheme which is provably secure if factoring is difficult and whose practical implementations are about two orders of magnitude faster than RSA-based identification schemes. In the last part of the paper we consider the question of sequential versus parallel executions of zero knowledge protocols, define a new notion of \u201ctransferable information\u201d, and prove that the parallel version of our identification scheme (which is not known to be zero knowledge) is secure since it reveals no transferable information.",
                "id": "02012"
            },
            {
                "title": "Caching Content under Digital Rights Management",
                "abstract": "Digital rights management systems seek to control the use of proprietary material (e.g., copyrighted movies). The use of DRM introduces a new set of caching issues not previously studied. Technically, these problems include elements of ski rental algorithms as well as paging, and their generalizations, online \"capital investment\" and generalized caching (the Landlord algorithm). The introduction of DRM restrictions does not impact the competitive ratio by more than a constant factor.",
                "id": "02013"
            },
            {
                "title": "Optimal Search and One-Way Trading Online Algorithms",
                "abstract": "This paper is concerned with the time series search and one-way tradingproblems. In the (time series) search problem a player is searching for the maximum (or minimum) price in a sequence that unfolds sequentially, one price at a time. Once during this game the player can decide to accept the current price p in which case the game ends and the player's payoff isp .I n theone-way trading problem a trader is given the task of trading dollars to yen. Each day, a new exchange rate is announced and the trader must decide how many dollars to convert to yen according to the current rate. The game ends when the trader trades his entire dollar wealth to yen and his payoff is the number of yen acquired. The search and one-way trading are intimately related. Any (deterministic or randomized) one-way trading algorithm can be viewed as a randomized search algorithm. Using the competitive ratio as a performance measure we determine the optimal competitive performance for several variants of these problems. In particular, we show that a simple threat-based strategy is optimal and we determine its competitive ratio which yields, for realistic values of the problem parameters, surprisingly low competitive ratios. We also consider and analyze a one-way trading game played against an adversary called Nature where the online player knows the probability distribution of the maximum exchange rate and that distribution has been chosen by Nature. Finally, we consider some applications for a special case of portfolio selection called two-way tradingin which the trader may trade back and forth between cash and one asset.",
                "id": "02014"
            },
            {
                "title": "Competitive algorithms for distributed data management",
                "abstract": "We deal with the competitive analysis of algorithms for managing data in a distributed environment. We deal with the file allocation problem, where copies of a file may be be stored in the local storage of some subsets of processors. Copies may be replicated and discarded over time so as to optimize communication costs, but multiple copies must be kept consistent and at least one copy must be stored somewhere in the network at all times. We deal with competitive algorithms for minimizing communication costs, over arbitrary sequences of reads and writes, and arbitrary network topologies. We define the constrained file allocation problem to be the solution of many individual file allocation problems simultaneously, subject to the constraints of local memory size. We give competitive algorithms for this problem on the uniform network topology. We then introduce distributed competitive algorithms for on-line data tracking (a generalization of mobile user tracking) to transform our competitive data management algorithms into distributed algorithms themselves.",
                "id": "02015"
            },
            {
                "title": "Provable Unlinkability Against Traffic Analysis with Low Message Overhead",
                "abstract": "Rackoff and Simon proved that a variant of Chaum's protocol for anonymous communication, later developed as the Onion Routing Protocol, is unlinkable against a passive adversary that controls all communication links and most of the nodes in a communication system. A major drawback of their analysis is that the protocol is secure only if (almost) all nodes participate at all times. That is, even if only n\u00bfN nodes wish to send messages, allN nodes have to participate in the protocol at all times. This suggests necessity of sending dummy messages and a high message overhead.Our first contribution is showing that this is unnecessary. We relax the adversary model and assume that the adversary only controls a certain fraction of the communication links in the communication network. We think this is a realistic adversary model. For this adversary model we show that a low message overhead variant of Chaum's protocol is provably secure.Furthermore, all previous security proofs assumed the a priori distribution on the messages is uniform. We feel this assumption is unrealistic. The analysis we give holds for any a priori information on the communication distribution. We achieve that by combining Markov chain techniques together with information theory tools in a simple and elegant way.",
                "id": "02016"
            },
            {
                "title": "Making chord robust to byzantine attacks",
                "abstract": "Chord is a distributed hash table (DHT) that requires only O(log n) links per node and performs searches with latency and message cost O(log n), where n is the number of peers in the network. Chord assumes all nodes behave according to protocol. We give a variant of Chord which is robust with high probability for any time period during which: 1) there are always at least z total peers in the network for some integer z; 2) there are never more than (1/4\u2013\u03b5)z Byzantine peers in the network for a fixed \u03b5 0; and 3) the number of peer insertion and deletion events is no more than zk for some tunable parameter k. We assume there is an adversary controlling the Byzantine peers and that the IP-addresses of all the Byzantine peers and the locations where they join the network are carefully selected by this adversary. Our notion of robustness is rather strong in that we not only guarantee that searches can be performed but also that we can enforce any set of \u201cproper behavior\u201d such as contributing new material, etc. In comparison to Chord, the resources required by this new variant are only a polylogarithmic factor greater in communication, messaging, and linking costs.",
                "id": "02017"
            },
            {
                "title": "Competitive Odds and Ends",
                "abstract": "Without Abstract",
                "id": "02018"
            },
            {
                "title": "Dynamically Fault-Tolerant Content Addressable Networks",
                "abstract": "We describe a content addressable network which is robust in the face of massive adversarial attacks and in a highly dynamic environment. Our network is robust in the sense that at any time, an arbitrarily large fraction of the peers can reach an arbitrarily large fraction of the data items. The network can be created and maintained in a completely distributed fashion.",
                "id": "02019"
            },
            {
                "title": "When the players are not expectation maximizers",
                "abstract": "Much of Game Theory, including the Nash equilibrium concept, is based on the assumption that players are expectation maximizers. It is known that if players are risk averse, games may no longer have Nash equilibria ([11,6]. We show that 1. Under risk aversion (convex risk valuations), and for almost all games, there are no mixed Nash equilibria, and thus either there is a pure equilibrium or there are no equilibria at all, and, 2. For a variety of important valuations other than expectation, it is NP-complete to determine if games between such players have a Nash equilibrium.",
                "id": "02020"
            },
            {
                "title": "Truth, envy, and truthful market clearing bundle pricing",
                "abstract": "We give a non-trivial class of valuation functions for which we give auctions that are efficient, truthful and envy-free. We give interesting classes of valuations for which one can design such auctions. Surprisingly, we also show that minor modifications to these valuations lead to impossibility results, the most surprising of which is that for a natural class of valuations, one cannot achieve efficiency, truthfulness, envy freeness, individual rationality, and no positive transfers. We also show that such auctions also imply a truthful mechanism for computing bundle prices (\"shrink wrapped\" bundles of items), that clear the market. This extends the class of valuations for which truthful market clearing prices mechanisms exist.",
                "id": "02021"
            },
            {
                "title": "Single valued combinatorial auctions with budgets",
                "abstract": "We consider budget constrained combinatorial auctions where each bidder has a private value for each of the items in some subset of the items and an overall budget constraint. Such auctions capture adword auctions, where advertisers offer a bid for those adwords that (hopefully) target their intended audience, and advertisers also have budgets. It is known that even if all items are identical and all budgets are public it is not possible to be truthful and efficient. Our main result is a novel auction that runs in polynomial time, is incentive compatible, and ensures Pareto-optimality. The auction is incentive compatible with respect to the private valuations whereas the budgets and the sets of interest are assumed to be public knowledge. This extends the result of Dobzinski, Lavi and Nisan (FOCS 2008) for auctions of multiple identical items with bugets to single-valued combinatorial auctions and address one of the basic challenges on auctioning web ads (see Nisan et al, 2009, Google auctions for tv ads).",
                "id": "02022"
            },
            {
                "title": "Untraceable electronic cash",
                "abstract": "The use of credit cards today is an act of faith on the p a t of all concerned. Each party is vulnerable to fraud by the others,\n and the cardholder in particular has no protection against surveillance.\n ",
                "id": "02023"
            },
            {
                "title": "Derandomization of auctions",
                "abstract": "We study the problem of designing seller-optimal auctions, i.e. auctions where the objective is to maximize revenue. Prior to this work, the only auctions known to be approximately optimal in the worst case employed randomization. Our main result is the existence of deterministic auctions that approximately match the performance guarantees of these randomized auctions. We give a fairly general derandomization technique for turning any randomized mechanism into an asymmetric deterministic one with approximately the same revenue. In doing so, we bypass the impossibility result for symmetric deterministic auctions and show that asymmetry is nearly as powerful as randomization for solving optimal mechanism design problems. Our general construction involves solving an exponential-sized flow problem and thus is not polynomial-time computable. To complete the picture, we give an explicit polynomial-time construction for derandomizing a specific auction with good worst-case revenue. Our results are based on toy problems that have a flavor similar to the hat problem from [3].",
                "id": "02024"
            },
            {
                "title": "Correlation Clustering - Minimizing Disagreements on Arbitrary Weighted Graphs",
                "abstract": "We solve several open problems concerning the correlation clustering problem introduced by Bansal, Blunt and Chawla [1]. We give an equivalence argument between these problems and the multicut problem. This implies an O(log n) approximation algorithm for minimizing disagreements on weighted and unweighted graphs. The equivalence also implies that these problems are APX-hard and suggests that improving the upper bound to obtain a constant factor approximation is non trivial. We also briefly discuss some seemingly interesting applications of correlation clustering.",
                "id": "02025"
            },
            {
                "title": "Makespan Minimization via Posted Prices.",
                "abstract": "We consider job scheduling settings, with multiple machines, where jobs arrive online and choose a machine selfishly so as to minimize their cost. Our objective is the classic makespan minimization objective, which corresponds to the completion time of the last job to complete. The incentives of the selfish jobs may lead to poor performance. To reconcile the differing objectives, we introduce posted machine prices. The selfish job seeks to minimize the sum of its completion time on the machine and the posted price for the machine. Prices may be static (i.e., set once and for all before any arrival) or dynamic (i.e., change over time), but they are determined only by the past, assuming nothing about upcoming events. Obviously, such schemes are inherently truthful. We consider the competitive ratio: the ratio between the makespan achievable by the pricing scheme and that of the optimal algorithm. We give tight bounds on the competitive ratio for both dynamic and static pricing schemes for identical, restricted, related, and unrelated machine settings. Our main result is a dynamic pricing scheme for related machines that gives a constant competitive ratio, essentially matching the competitive ratio of online algorithms for this setting. In contrast, dynamic pricing gives poor performance for unrelated machines. This lower bound also exhibits a gap between what can be achieved by pricing versus what can be achieved by online algorithms.",
                "id": "02026"
            },
            {
                "title": "Subjective vs. Objective Reality -- The Risk of Running Late",
                "abstract": "We study selfish agents that have a \"distorted view\" of reality. We introduce a framework of subjective vs. objective reality. This is very useful to model risk averse behavior. Natural quality of service issues can be cast as special cases thereof.In particular, we study two applicable variants of the price of anarchy paradigm, the subjective price of anarchywhere one compares the \"optimal\" subjective outcome to the outcome that arises from selfish subjective reality agents, and the objective price of anarchywhere one compares the optimal objective outcome to that derived by selfish subjective agents.",
                "id": "02027"
            },
            {
                "title": "On-line Competive Algorithms for Call Admission in Optical Networks",
                "abstract": " In the present work we study the on-line call admission problem in optical networks. We present a general technique to deal with the problem of call admission and wavelength selection by reducing this problem to the problem of only call admission. We then give randomized algorithms with logarithmic competitive ratios for specific topologies in two models of optical networks, and consider the case of full-duplex communication as well. A preliminary version of this paper appeared in Proc. of the... ",
                "id": "02028"
            },
            {
                "title": "New algorithms for an ancient scheduling problem",
                "abstract": "We consider the on-line version of the original m-machine scheduling problem: given m machines and n positive real jobs, schedule the n jobs on the m machines so as to minimize the makespan, the completion time of the last job. In the on-line version, as soon as job j arrrives, it must be assigned immediately to one of the m machines.We present two main results. The first is a (2\u2013\u03b5)-competitive deterministic algorithm for all m. The competitive ratio of all previous algorithms approaches 2 as m\u2192  \u221e . Indeed, the problem of improving the competitive ratio for   large m had been open since 1966, when the first algorithm for this problem appeared.The second result is an optimal randomized algorithm for the case m = 2. To the best of our knowledge, our 4/3-competitive algorithm is the first specifically randomized algorithm for the original, m-machine, on-line scheduling problem.",
                "id": "02029"
            },
            {
                "title": "AIM: Another Itemset Miner",
                "abstract": "We present a new algorithm for mining frequent itemsets. Past studies have proposed various algo- rithms and techniques for improving the efficiency of the mining task. We integrate a combination of these techniques into an algorithm which utilize those tech- niques dynamically according to the input dataset. The algorithm main features include depth first search with vertical compressed database, diffset, parent equiva- lence pruning, dynamic reordering and projection. Ex- perimental testing suggests that our algorithm and implementation significantly outperform existing algo- rithms/implementations.",
                "id": "02030"
            },
            {
                "title": "HLDB: location-based services in databases",
                "abstract": "This paper introduces HLDB, the first practical system that can answer exact spatial queries on continental road networks entirely within a database. HLDB is based on hub labels (HL), the fastest point-to-point algorithm for road networks, and its queries are implemented (quite naturally) in standard SQL. Within the database, HLDB answers exact distance queries and retrieves full shortest-path descriptions in real time, even on networks with tens of millions of vertices. The basic algorithm can be extended in a natural way (still in SQL) to answer much more sophisticated queries, such as finding the ten closest fast-food restaurants. We also introduce efficient new HL-based algorithms for even harder problems, such as best via point, ride sharing, and point of interest prediction. The HLDB framework makes it easy to implement these algorithms in SQL, enabling interactive applications on continental road networks.",
                "id": "02031"
            },
            {
                "title": "Provable Unlinkability against Traffic Analysis",
                "abstract": "Chaum [1, 2] suggested a simple and efficient protocol aimed at providing anonymity in the presence of an adversary watching all communication links. Chaum's protocol is known to be insecure. We show that Chaum's protocol becomes secure when the attack model is relaxed and the adversary can control at most 99% of communication links. Our proof technique is markedly different than previous work. We establish a connection with information theory - a connection we believe is useful also elsewhere, and which we believe supplies the correct language to attack the problem. We introduce \"obscurant networks\" - networks that can obscure the destination of each particular player, and we show almost all executions of the protocol include such a network. The security guarantee we supply is very strong. It shows the adversary learns almost no information about any subset of players. Remarkably, we show that this guarantee holds even if the adversary has a-priori information about communication patters (e.g., people tend to speak less with those who do not understand their language). We believe this is an important issue in the real world and is a desirable property any anonymous system should have.",
                "id": "02032"
            },
            {
                "title": "Competitive queue management for latency sensitive packets",
                "abstract": "We consider the online problem of non-preemptive queue management. An online sequence of packets arrive, each of which has an associated intrinsic value. Packets can be accepted to a FIFO queue, or discarded. The profit gained by transmitting a packet diminishes over time and is equal to its value minus the delay. This corresponds to the well known and strongly motivated Naor's model in operations research. We give a queue management algorithm with a competitive ratio equal to the golden ratio (\u03c6 &ap; 1.618) in the case that all packets have the same value, along with a matching lower bound. We also derive \u0398(1) upper and lower bounds on the competitive ratio when packets have different intrinsic values (in the case of differentiated services). We can extend our results to deal with more general models for loss of value over time. Finally, we re-interpret our online algorithms in the context of selfish agents, producing an online mechanism that approximates the optimal social welfare to within a constant factor.",
                "id": "02033"
            },
            {
                "title": "Truth and Envy in Capacitated Allocation Games",
                "abstract": "We study auctions with additive valuations where agents have a limit on the\nnumber of goods they may receive. We refer to such valuations as {\\em\ncapacitated} and seek mechanisms that maximize social welfare and are\nsimultaneously incentive compatible, envy-free, individually rational, and have\nno positive transfers. If capacities are infinite, then sequentially repeating\nthe 2nd price Vickrey auction meets these requirements. In 1983, Leonard showed\nthat for unit capacities, VCG with Clarke Pivot payments is also envy free. For\ncapacities that are all unit or all infinite, the mechanism produces a\nWalrasian pricing (subject to capacity constraints). Here, we consider general\ncapacities. For homogeneous capacities (all capacities equal) we show that VCG\nwith Clarke Pivot payments is envy free (VCG with Clarke Pivot payments is\nalways incentive compatible, individually rational, and has no positive\ntransfers). Contrariwise, there is no incentive compatible Walrasian pricing.\nFor heterogeneous capacities, we show that there is no mechanism with all 4\nproperties, but at least in some cases, one can achieve both incentive\ncompatibility and envy freeness.",
                "id": "02034"
            },
            {
                "title": "Highway Dimension, Shortest Paths, and Provably Efficient Algorithms",
                "abstract": "Computing driving directions has motivated many shortest path heuristics that answer queries on continental scale networks, with tens of millions of intersections, literally instantly, and with very low storage overhead. In this paper we complement the experimental evidence with the first rigorous proofs of efficiency for many of the heuristics suggested over the past decade. We introduce the notion of highway dimension and show how low highway dimension gives a unified explanation for several seemingly different algorithms.",
                "id": "02035"
            },
            {
                "title": "Planning and learning in permutation groups",
                "abstract": "Planning is defined as the problem of synthesizing a desired behavior from given basic operations, and learning is defined as the dual problem of analyzing a given behavior to determine the unknown basic operations. Algorithms for solving these problems in the context of invertible operations on finite-state environments are developed. In addition to their obvious artificial intelligence applications, the algorithms can efficiently find the shortest way to solve Rubik's cube, test ping-pong protocols, and solve systems of equations over permutation groups.",
                "id": "02036"
            },
            {
                "title": "Randomized robot navigation algorithms",
                "abstract": "No abstract available.\n\n",
                "id": "02037"
            },
            {
                "title": "On-line routing of virtual circuits with applications to load balancing and machine scheduling",
                "abstract": "In this paper we study the problem of on-line allocation of routes to virtual circuits (both point-to-point and multicast) where the goal is to route all requests while minimizing the required bandwidth. We concentrate on the case of Permanent virtual circuits (i.e., once a circuit is established it exists forever), and describe an algorithm that achieves on O (log n) competitive ratio with respect to maximum congestin, where nis the number of nodes in the network. Informally, our results show that instead of knowing all of the future requests, it is sufficient to increase the bandwidth of the communication links by an O (log n) factor. We also show that this result is tight, that is, for any on-line algorithm there exists a scenario in which ***(log n) increase in bandwidth is necessary in directed networks.We view virtual circuit routing as a generalization of an on-line load balancing problem, defined as follows: jobs arrive on line and each job must be assigned to one of the machines immediately upon arrival. Assigning a job to a machine increases the machine's load by an amount that depends both on the job and on the machine. The goal is to minimize the maximum load. For the related machines case, we describe the first algorithm that achieves constant competitive ratio. for the unrelated case (with nmachines), we describe a new method that yields O(logn)-competitive algorithm. This stands in contrast to the natural greed approach, whose competitive ratio is exactly n. show that this result is tight, that is, for any on-line algorithm there exists a scenario in which ***(log n) increase in bandwidth is necessary in directed networks.",
                "id": "02038"
            },
            {
                "title": "Optimal oblivious routing in polynomial time",
                "abstract": "A recent seminal result of Racke is that for any network there is an oblivious routing algorithm with a polylog competitive ratio with respect to congestion. Unfortunately, Racke's construction is not polynomial time. We give a polynomial time construction that guarantee's Racke's bounds, and more generally gives the true optimal ratio for any network.",
                "id": "02039"
            },
            {
                "title": "Envy-Free Makespan Approximation",
                "abstract": "We study envy-free mechanisms for assigning tasks to agents, where every task may take a different amount of time to perform by each agent, and the goal is to get all the tasks done as soon as possible (i.e., minimize the makespan). For indivisible tasks, we put forward an envy-free polynomial mechanism that approximates the minimal makespan to within a factor of $O(\\log m)$, where $m$ is the number of machines. This bound is almost tight, as we also show that no envy-free mechanism can achieve a better bound than $\\Omega(\\log m / \\log\\log m)$. This improves the recent result of Mu'alem [On multi-dimensional envy-free mechanisms, in Proceedings of the First International Conference on Algorithmic Decision Theory, F. Rossi and A. Tsoukias, eds., Lecture Notes in Comput. Sci. 5783, Springer, Berlin, 2009, pp. 120-131] who introduced the model and gave an upper bound of $(m+1)/2$ and a lower bound of $2-1/m$. For divisible tasks, we show that there always exists an envy-free poly-time mechanism with optimal makespan. Finally, we demonstrate how our mechanism for envy-free makespan minimization can be interpreted as a market clearing problem.",
                "id": "02040"
            },
            {
                "title": "Implicit O(1) probe search",
                "abstract": "Given a set of n elements from the domain 1, \u2026, m, we investigate how to arrange them in a table of size n, so that searching for an element in the table can be done in constant time.Yao has shown that this cannot be done when the domain is sufficiently large as a function of n ([Yao]). [FNSS] have shown that this can be done when the domain is linear in the number of elements.We give a constructive solution when the domain m is polynomial in the number of elements n, and give a nonconstructive proof for m no larger than exponential in poly(n). We improve upon [Yao] and give better bounds on the maximum m for which implicit O(1) probe search can be done. We achieve our results by showing the tight relationship between hashing and certain encoding problems.",
                "id": "02041"
            },
            {
                "title": "Packet Routing via Min-Cost Circuit Routing",
                "abstract": "In this paper we initiate the study of competitive on-line packet routing algorithms. At any time, any network node may initiate sending a packet to another node. Our goal is to route these packets through the network, while simul- taneously minimizing link bandwidth, buffer usage, and the average delay of a packet. We give efficient centralized on-line packet routing algorithms in this setting. These al- gorithms achieve a constant competitive ratio with respect to the average delay while increasing the link bandwidth by no more than a logarithmic factor. To obtain our packet routing results, we introduce com- petitive algorithms for a new problem called min-cost load circuit routing. Here, the goal is to create on-line virtual circuits in a graph, while trying to simultaneously minimize link bandwidth and (related) communication costs.",
                "id": "02042"
            },
            {
                "title": "Online conflict-free coloring for intervals",
                "abstract": "We consider an online version of the conflict-free coloring of a set of points on the line, where each newly inserted point must be assigned a color upon insertion, and at all times the coloring has to be conflict-free, in the sense that in every interval I there is a color that appears exactly once in I. We present several deterministic and randomized algorithms for achieving this goal, and analyze their performance, that is, the maximum number of colors that they need to use, as a function of the number n of inserted points. We first show that a natural and simple (deterministic) approach may perform rather poorly, requiring \u03a9(\u221an) colors in the worst case. We then modify this approach, to obtain an efficient deterministic algorithm that uses a maximum of \u0398(log2 n) colors. Next, we present two randomized solutions. The first algorithm requires an expected number of at most O(log2 n) colors, and produces a coloring which is valid with high probability, and the second one, which is a variant of our efficient deterministic algorithm, requires an expected number of at most O(log n log log n) colors but always produces a valid coloring. We also analyze the performance of the simplest proposed algorithm when the points are inserted in a random order, and present an incomplete analysis that indicates that, with high probability, it uses only O(log n) colors. Finally, we show that in the extension of this problem to two dimensions, where the relevant ranges are disks, n colors may be required in the worst case. The average-case behavior for disks, and cases involving other planar ranges, are still open.",
                "id": "02043"
            },
            {
                "title": "Competitive paging algorithms",
                "abstract": "The paging problem is that of deciding which pages to keep in a memory of k pages in order to minimize the number of page faults. We develop the marking algorithm, a randomized on-line algorithm for the paging problem. We prove that its expected cost on any sequence of requests is within a factor of 2Hk of optimum. (Where Hk is the kth harmonic number, which is roughly Ink.) The best such factor that can be achieved is Hk. This is in contrast to deterministic algorithms, which cannot be guaranteed to be within a factor smaller than k of optimum. An alternative to comparing an on-line algorithm with the optimum off-line algorithm is the idea of comparing it to several other on-line algorithms. We have obtained results along these lines for the paging problem. Given a set of on-line algorithms",
                "id": "02044"
            },
            {
                "title": "Some Recent Results on Data Mining and Search",
                "abstract": "In this talk we review and survey some recent work and work in progress on data mining and web search. We discuss Latent Semantic Analysis and give conditions under which it is robust. We also consider the problem of collaborative filtering and show how spectral techniques can give a rigorous and robust justification for doing so. We consider the problems of web search and show how both Google and Klienberg's algorithm are robust under a model of web generation, and how this model can be reasonably extended. We then give an algorithm that provably gives the correct result in this extended model. The results surveyed are joint work with Azar, Karlin, McSherry and Saia [2], and Achlioptas, Karlin and McSherry [1].",
                "id": "02045"
            },
            {
                "title": "An implicit data structure for searching a multikey table in logarithmic time",
                "abstract": "A data structure is implicit if it uses no extra strorage beyond the space needed for the data and a constant number of parameters. We describe an implicit data structure for storing n k -key records, which supports searching for a record, under any key, in the asymptotically optimal search time O (1g n ). This is in sharp contrast to an \u03a9(n 1\u2212 1 k ) lower bound which holds if all comparisons must be against the sought key value. The theoretical tools we develop also yield a more practical way to halve the number of memory references used in obvious non-implicit solutions to the multikey table problem.",
                "id": "02046"
            },
            {
                "title": "Combinatorial Auctions with Budgets",
                "abstract": "We consider budget constrained combinatorial auctions where bidder $i$ has a\nprivate value $v_i$, a budget $b_i$, and is interested in all the items in\n$S_i$. The value to agent $i$ of a set of items $R$ is $|R \\cap S_i| \\cdot\nv_i$. Such auctions capture adword auctions, where advertisers offer a bid for\nads in response to an advertiser-dependent set of adwords, and advertisers have\nbudgets. It is known that even of all items are identical and all budgets are\npublic it is not possible to be truthful and efficient. Our main result is a\nnovel auction that runs in polynomial time, is incentive compatible, and\nensures Pareto-optimality for such auctions when the valuations are private and\nthe budgets are public knowledge. This extends the result of Dobzinski et al.\n(FOCS 2008) for auctions of multiple {\\sl identical} items and public budgets\nto single-valued {\\sl combinatorial} auctions with public budgets.",
                "id": "02047"
            },
            {
                "title": "Better algorithms for unfair metrical task systems and applications",
                "abstract": "Unfair metrical task systems are a generalization of online metrical task systems. In this paper we introduce new techniques to combine algorithms for unfair metrical task systems and apply these techniques to obtain improved randomized online algorithms for metrical task systems on arbitrary metric spaces.",
                "id": "02048"
            },
            {
                "title": "A case for associative peer to peer overlays",
                "abstract": "The success of a P2P file-sharing network highly depends on the scalability and versatility of its search mechanism. Two particularly desirable search features are scope (ability to find infrequent items) and support for partial-match queries (queries that contain typos or include a subset of keywords). While centralized-index architectures (such as Napster) can support both these features, existing decentralized architectures seem to support at most one: prevailing protocols (such as Gnutella and FastTrack) support partial-match queries, but since search is unrelated to the query, they have limited scope. Distributed Hash Tables (such as CAN and CHORD) constitute another class of P2P architectures promoted by the research community. DHTs couple index location with the item's hash value and are able to provide scope but can not effectively support partial-match queries; another hurdle in DHT deployment is their tight control the overlay structure and data placement which makes them more sensitive to failures.Associative overlays are a new class of decentralized P2P architectures. They are designed as a collection of unstructured P2P networks (based on popular architectures such as gnutella or FastTrack), and the design retains many of their appealing properties including support for partial match queries, and relative resilience to peer failures. Yet, the search process is orders of magnitude more effective in locating rare items. Our design exploits associations inherent in human selections to steer the search process to peers that are more likely to have an answer to the query.",
                "id": "02049"
            },
            {
                "title": "Correlation clustering in general weighted graphs",
                "abstract": "We consider the following general correlation-clustering problem [N. Bansal, A. Blum, S. Chawla, Correlation clustering, in: Proc. 43rd Annu. IEEE Symp. on Foundations of Computer Science, Vancouver, Canada, November 2002, pp. 238-250]: given a graph with real nonnegative edge weights and a \u2329+\u232a/\u2329-\u232a edge labelling, partition the vertices into clusters to minimize the total weight of cut \u2329+\u232a edges and uncut \u2329-\u232a edges. Thus, \u2329+\u232a edges with large weights (representing strong correlations between endpoints) encourage those endpoints to belong to a common cluster while \u2329-\u232a edges with large weights encourage the endpoints to belong to different clusters. In contrast to most clustering problems, correlation clustering specifies neither the desired number of clusters nor a distance threshold for clustering; both of these parameters are effectively chosen to be best possible by the problem definition.Correlation clustering was introduced by Bansal et al. [Correlation clustering, in: Proc. 43rd Annu. IEEE Syrup. on Foundations of Computer Science, Vancouver, Canada, November 2002, pp. 238-250], motivated by both document clustering and agnostic learning. They proved NP-hardness and gave constant-factor approximation algorithms for the special case in which the graph is complete (full information) and every edge has the same weight. We give an O(log n)-approximation algorithm for the general case based on a linear-programming rounding and the \"region-growing\" technique. We also prove that this linear program has a gap of \u03a9(log n), and therefore our approximation is tight under this approach. We also give an O(r3)-approximation algorithm for Kr, r-minor-free graphs. On the other hand, we show that the problem is equivalent to minimum multicut, and therefore APX-hard and difficult to approximate better than \u0398(log n).",
                "id": "02050"
            },
            {
                "title": "Lower Bounds for On-line Graph Problems with Application to On-line Circuit and Optical Routing",
                "abstract": "We present lower bounds on the competitive ratio of randomized algorithms for a wide class of on-line graph optimization problems, and we apply such results to on-line virtual circuit and optical routing problems. Lund and Yannakakis [The approximation of maximum subgraph problems, in Proceedings of the 20th International Colloquium on Automata, Languages and Programming, 1993, pp. 40-51] give inapproximability results for the problem of finding the largest vertex induced subgraph satisfying any nontrivial, hereditary property pi--e.g., independent set, planar, acyclic, bipartite. We consider the on-line version of this family of problems, where some graph G is fixed and some subgraph H of G is presented on-line, vertex by vertex. The on-line algorithm must choose a subset of the vertices of H, choosing or rejecting a vertex when it is presented, whose vertex induced subgraph satisfies property pi. Furthermore, we study the on-line version of graph coloring whose off-line version has also been shown to be inapproximable [C. Lund and M. Yannakakis, On the hardness of approximating minimization problems, in Proceedings of the 25th ACM Symposium on Theory of Computing, 1993], on-line max edge-disjoint paths, and on-line path coloring problems. Irrespective of the time complexity, we show an Omega(nepsilon) lower bound on the competitive ratio of randomized on-line algorithms for any of these problems. As a consequence, we obtain an Omega(nepsilon) lower bound on the competitive ratio of randomized on-line algorithms for virtual circuit routing on general networks, in contrast to the known results for some specific networks. Similar lower bounds are obtained for on-line optical routing as well.",
                "id": "02051"
            },
            {
                "title": "Tracing Traitors",
                "abstract": " We give cryptographic schemes that help trace the source of leaks when sensitive or proprietary data is made available to a large set of parties. A very relevant application is in the context of pay television, where only paying customers should be able to view certain programs. In this application the programs are normally encrypted and then the sensitive data is the decryption keys that are given to paying customers. If a pirate decoder is found it is desirable to reveal the source of its... ",
                "id": "02052"
            },
            {
                "title": "Revenue maximizing envy-free multi-unit auctions with budgets",
                "abstract": "We study envy-free (EF) mechanisms for multi-unit auctions with budgeted agents that approximately maximize revenue. In an EF auction, prices are set so that every bidder receives a bundle that maximizes her utility amongst all bundles; We show that the problem of revenue-maximizing EF auctions is NP-hard, even for the case of identical items and additive valuations (up to the budget). The main result of our paper is a novel EF auction that runs in polynomial time and provides a approximation of 1/2 with respect to the revenue-maximizing EF auction. A slight variant of our mechanism will produce an allocation and pricing that is more restrictive (so called item pricing) and gives a 1/2 approximation to the optimal revenue within this more restrictive class.",
                "id": "02053"
            },
            {
                "title": "Spectral Analysis of Data",
                "abstract": "Experimental evidence suggests that spectral techniques are valuable for a wide range of applications. A partial list of such applications include (i) semantic analysis of documents used to cluster documents into areas of interest, (ii) collaborative filtering --- the reconstruction of missing data items, and (iii) determining the relative importance of documents based on citation/link structure. Intuitive arguments can explain some of the phenomena that has been observed but little theoretical study has been done. In this paper we present a model for framing data mining tasks and a unified approach to solving the resulting data mining problems using spectral analysis. These results give strong justification to the use of spectral techniques for latent semantic indexing, collaborative filtering, and web site ranking.",
                "id": "02054"
            },
            {
                "title": "Competitive algorithms for distributed data management (extended abstract)",
                "abstract": "We deal with the competitive analysis of algorithms for managing data in a distributed environment. We deal with the file allocation problem ([C], [DF], [ML]), where copies of a file may be stored in the local storage of some subset of processors, copies may be replicated and discarded over time so as to optimize communication costs, but multiple copies must be kept consistent and at least one copy must be stored somewhere in the network at all times. We deal with competitive algorithms for minimizing communication costs, over arbitrary sequences of reads and writes, and arbitrary network topologies. We define the constrained file allocation problem to be the solution of many individual file allocation problems simultaneously, subject to the constraints of local memory size. We give competitive algorithms for this prblem on uniform networks. We then introduce distributed competitive algorithms for on-line data tracking (a generalization of mobile user tracking [AP1, AP3] to transform our competitive distributed data management algorithms into distributed algorithms themselves.",
                "id": "02055"
            },
            {
                "title": "Competitive non-preemptive call control",
                "abstract": "No abstract available.\n\n",
                "id": "02056"
            },
            {
                "title": "On-Line Scheduling on a Single Machine: Minimizing the Total Completion Time",
                "abstract": ".\u00a0\u00a0 This note deals with the scheduling problem of minimizing the sum of job completion times in a system with n jobs and a single machine. We investigate the on-line version of the problem where every job has to be scheduled immediately and irrevocably as soon as it arrives, without any\n information on the later arriving jobs.\n We prove that for any sufficiently smooth, non-negative, non-decreasing function f(n) there exists an O(f(n))-competitive on-line algorithm for minimizing the total completion time if and only if the infinite sum  converges.",
                "id": "02057"
            },
            {
                "title": "Dynamic Traitor Tracing",
                "abstract": "Traitor tracing schemes were introduced to combat the typical piracy sce- nario whereby pirate decoders (or access control smartcards) are manufactured and sold by pirates to illegal subscribers. Those traitor tracing schemes, however, are ineffective for the currently less common scenario where a pirate publishes the periodical access control keys on the Internet or, alternatively, simply rebroadcasts the content via an independent pirate network. This new piracy scenario may become especially attractive (to pirates) in the context of broadband multicast over the Internet. In this paper we con- sider the consequences of this type of piracy and offer countermeasures. We introduce the concept of dynamic traitor tracing which is a practical and efficient tool to combat this type of piracy.",
                "id": "02058"
            },
            {
                "title": "Experimental studies of access graph based heuristics: beating the LRU standard?",
                "abstract": "In this paper we devise new paging heuristics motivated by the access graph model of paging [2]. Unlike the access graph model [2, 7, 4] a,nd the related Markov paging model [8] our heuristics are truly online in that we do not assume any prior knowledge of the program just about to be executed.The Least Recently Used heuristic for paging is remarkably good, and is known experimentally to be superior to many of the suggested alternatives on real program traces [17]. Experiments we've performed suggest that our heuristics beat LRU fairly consistently, over a wide range of cache sizes and programs. The number of page faults can be as low as 1/2 the number of page faults for LRU and is on average 7 to 9 percent less than the number of faults for LRU. (Depending on how this average is computed.)We have built a program tracer that gives the page access sequence for real program executions of 200 - 3,300 thousand page access requests, and our simulations are based on 25 of these program traces. Overall, we have performed several thousand such simulations. While we have no real evidence to suggest that the programs we've traced are typical in any sense, we have made use of an experimental ''protocol'' designed to avoid experimenter bias.We strongly emphasize that our results are only preliminary and that much further work needs to be done.",
                "id": "02059"
            },
            {
                "title": "On Capital Investment",
                "abstract": " We deal with the problem of making capital investments in machines for manufacturing a product. Opportunities for investment occur over time, every such option consists of a capital cost for a new machine and a resulting productivity gain, i.e., a lower production cost for one unit of product. The goal is that of minimizing the total production and capital costs when future demand for the product being produced and investment opportunities are unknown. This can be viewed as a generalization of... ",
                "id": "02060"
            },
            {
                "title": "Efficient sequences of trials",
                "abstract": "We introduce a problem called sequential trial optimization, a generalization of the well studied set cover problem with a new objective function. We give a simple algorithm that achieves a constant factor approximation to this problem. Sequential trial optimization naturally arises in heterogenous search environments such as peer to peer networks.",
                "id": "02061"
            },
            {
                "title": "Online Companion Caching",
                "abstract": "This paper is concerned with online cachingalg orithms for the (n, k)-companion cache, defined by Brehob et. al. (3). In this model the cache is composed of two components: a k-way set-associative cache and a companion fully-associative cache of size n. We show that the deterministic competitive ratio for this problem is (n +1 )(k +1 )\u2212 1, and the randomized competitive ratio is O(log n log k )a nd\u2126(log n +l ogk).",
                "id": "02062"
            },
            {
                "title": "Competitive Analysis of Algorithms",
                "abstract": "Without Abstract",
                "id": "02063"
            },
            {
                "title": "Associative search in peer to peer networks: Harnessing latent semantics",
                "abstract": "The success of a P2P file-sharing network highly depends on the scalability and versatility of its search mechanism. Two particularly desirable search features are scope (ability to find infrequent items) and support for partial-match queries (queries that contain typos or include a subset of keywords). While centralized-index architectures (such as Napster) can support both these features, existing decentralized architectures seem to support at most one: prevailing unstructured P2P protocols (such as Gnutella and FastTrack) deploy a ''blind'' search mechanism where the set of peers probed is unrelated to the query; thus they support partial-match queries but have limited scope. On the other extreme, the recently-proposed distributed hash tables (DHTs) such as CAN and CHORD, couple index location with the item's hash value, and thus have good scope but can not effectively support partial-match queries. Another hurdle to DHTs deployment is their tight control of the overlay structure and the information (part of the index) each peer maintains, which makes them more sensitive to failures and frequent joins and disconnects. We develop a new class of decentralized P2P architectures. Our design is based on unstructured architectures such as Gnutella and FastTrack, and retains many of their appealing properties including support for partial match queries, and relative resilience to peer failures. Yet, we obtain orders of magnitude improvement in the efficiency of locating rare items. Our approach exploits associations inherent in human selections to steer the search process to peers that are more likely to have an answer to the query. We demonstrate the potential of associative search using models, analysis, and simulations.",
                "id": "02064"
            },
            {
                "title": "An improved algorithm for online coloring of intervals with bandwidth",
                "abstract": "We present an improved online algorithm for coloring interval graphs with bandwidth. This problem has recently been studied by Adamy and Erlebach and a 195-competitive online strategy has been presented. We improve this by presenting a 10-competitive strategy. To achieve this result, we use variants of an optimal online coloring algorithm due to Kierstead and Trotter.",
                "id": "02065"
            },
            {
                "title": "Polymorphic arrays: A novel VLSI layout for systolic computers",
                "abstract": "This paper proposes a novel architecture for massively parallel systolic computers, which is based on results from lattice theory. In the proposed architecture, each processor is connected to four other processors via constant-lenght wires in an regular borderless pattern. The mapping of processes to processors is continuous, and the architecture guarantees exceptional load uniformity for rectangular process arrays of arbitrary sizes. In addition, no timesharing is ever required when the ration of processes to processors is smaller than 1//spl radic/5.",
                "id": "02066"
            },
            {
                "title": "Variations on the Hotelling-Downs Model.",
                "abstract": "In this paper we expand the standard Hotelling-Downs model (Hotelling 1929; Downs 1957) of spatial competition to a setting where clients do not necessarily choose their closest candidate (retail product or political). Specifically, we consider a setting where clients may disavow all candidates if there is no candidate that is sufficiently close to the client preferences. Moreover, if there are multiple candidates that are sufficiently close, the client may choose amongst them at random. We show the existence of Nash Equilibria for some such models, and study the price of anarchy and stability in such scenarios.",
                "id": "02067"
            },
            {
                "title": "Competitive k-server algorithms",
                "abstract": "Deterministic competitive k-server algorithms are given for all k and all metric spaces. This settles the k-server conjecture of M.S. Manasse et al. (1988) up to the competitive ratio. The best previous result for general metric spaces was a three-server randomized competitive algorithm and a nonconstructive proof that a deterministic three-server competitive algorithm exists. The competitive ratio the present authors can prove is exponential in the number of servers. Thus, the question of the minimal competitive ratio for arbitrary metric spaces is still open. The methods set forth here also give competitive algorithms for a natural generalization of the k-server problem, called the k-taxicab problem.",
                "id": "02068"
            },
            {
                "title": "Decision Trees: More Theoretical Justification for Practical Algorithms.",
                "abstract": "We study impurity-based decision tree algorithms such as CART, C4.5, etc., so as to better understand their theoretical underpinnings. We consider such algorithms on special forms of functions and distributions. We deal with the uniform distribution and functions that can be described as a boolean linear threshold functions or a read-once DNF. We show that for boolean linear threshold functions and read-once DNF, maximal purity gain and maximal influence are logically equivalent. This leads us to the exact identification of these classes of functions by impurity-based algorithms given sufficiently many noise-free examples. We show that the decision tree resulting from these algorithms has minimal size and height amongst all decision trees representing the function. Based on the statistical query learning model, we introduce the noise-tolerant version of practical decision tree algorithms. We show that if the input examples have small classification noise and are uniformly distributed, then all our results for practical noise-free impurity-based algorithms also hold for their noise-tolerant version.",
                "id": "02069"
            },
            {
                "title": "On the price of stability for designing undirected networks with fair cost allocations",
                "abstract": "In this paper we address the open problem of bounding the price of stability for network design with fair cost allocation for undirected graphs posed in [1]. We consider the case where there is an agent in every vertex. We show that the price of stability is O(loglogn). We prove this by defining a particular improving dynamics in a related graph. This proof technique may have other applications and is of independent interest.",
                "id": "02070"
            },
            {
                "title": "VC-dimension and shortest path algorithms",
                "abstract": "We explore the relationship between VC-dimension and graph algorithm design. In particular, we show that set systems induced by sets of vertices on shortest paths have VC-dimension at most two. This allows us to use a result from learning theory to improve time bounds on query algorithms for the point-to-point shortest path problem in networks of low highway dimension, such as road networks. We also refine the definitions of highway dimension and related concepts, making them more general and potentially more relevant to practice. In particular, we define highway dimension in terms of set systems induced by shortest paths, and give cardinality-based and average case definitions.",
                "id": "02071"
            },
            {
                "title": "Minimal indices for predecessor search",
                "abstract": "We give a new predecessor data structure which improves upon the index size of the P tra\u015fcu-Thorup data structures, reducing the index size from O ( n w 4 / 5 ) bits to O ( n log w ) bits, with optimal probe complexity. Alternatively, our new data structure can be viewed as matching the space complexity of the (probe-suboptimal) z-fast trie of Belazzougui et al. Thus, we get the best of both approaches with respect to both probe count and index size. The penalty we pay is an extra O ( log w ) inter-register operations. Our data structure can also be used to solve the weak prefix search problem, the index size of O ( n log w ) bits is known to be optimal for any such data structure.The technical contributions include highly efficient single word indices, with out-degree w / log w (compared to w 1 / 5 of a fusion tree node). To construct these indices we device highly efficient bit selectors which, we believe, are of independent interest.",
                "id": "02072"
            },
            {
                "title": "Censorship Resistant Peer-to-Peer Networks",
                "abstract": "We present a censorship resistant peer-to-peer network for accessing n data items in a network of n nodes. Each search for a data item in the network takes O(log n) time and requires at most O(log2 n) messages. Our network is censorship resistant in the sense that even after adversarial removal of an arbitrarily large constant fraction of the nodes in the network, all but an arbitrarily small fraction of the remaining nodes can obtain all but an arbitrarily small fraction of the original data items. The network can be created in a fully distributed fashion. It requires only O(log n) memory in each node. We also give a variant of our scheme that has the property that it is highly spam resistant: an adversary can take over complete control of a constant fraction of the nodes in the network and yet will still be unable to generate spam.",
                "id": "02073"
            },
            {
                "title": "Truly online paging with locality of reference",
                "abstract": "The access graph model for paging, defined by (Borodin et al., 1991) and studied in (Irani et al., 1992) has a number of troubling aspects. The access graph has to be known in advance to the paging algorithm and the memory required to represent the access graph itself may be very large. We present a truly online strongly competitive paging algorithm in the access graph model that does not have any prior information on the access sequence. We give both strongly competitive deterministic and strongly competitive randomized algorithms. Our algorithms need only O(k log n) bits of memory, where k is the number of page slots available and n is the size of the virtual address space, i.e., no more memory than needed to store the virtual translation tables for pages in memory. In fact, we can reduce this to O(k log k) bits using appropriate probabilistic data structures. We also extend the locality of reference concept captured by the access graph model to allow changes in the behavior of the underlying process. We formalize this by introducing the concept of an \"extended access graph\". We consider a graph parameter /spl Delta/ that captures the degree of change allowed. We study this new model and give algorithms that are strongly competitive for the (unknown) extended access graph. We can do so for almost all values of /spl Delta/ for which it is possible.",
                "id": "02074"
            },
            {
                "title": "Digital Signatures for Modifiable Collections",
                "abstract": "The common assumption about digital signatures is that they disallow any kind of modification on signed data. However, a more flexible approach is often needed and has been advocated lately, one in which some restricted modifications may still occur, without invalidating the data. This is made possible by offering signatures which are homomorphic with respect to some operation on the message domain. Starting from the signature(s) of some data instance( s), computed by the data owner, anybody else can derive the signature corresponding to a new data instance, if obtained only via some accepted operation from the previous one(s). More, updated signatures should be indistinguishable from the ones computed by the data owner and this updating step should be applicable as many times as needed. This paper deals with the signing of insert-only collections, in which element insertions are accepted but no removals should occur. Newly inserted elements do not have to be signed or known by the initial signer. We propose two techniques: one which transposes the insert-only problem into a delete-only one (which is already solved), and another technique based on zero-knowledge proofs. We also give performance measures and discuss applications.",
                "id": "02075"
            },
            {
                "title": "Making data structures confluently persistent",
                "abstract": "We address a longstanding open problem of [8, 7], and present a general transformation that takes any data structure and transforms it to a confluently persistent data structure. We model this general problem using the concepts of a version DAG (Directed Acyclic Graph) and an instantiation of a version DAG. We introduce the concept of the effective depth of a vertex in the version DAG and use it to derive information theoretic lower bounds on the space expansion of any such transformation for this DAG. We then give a confluently persistent data structure, such that for any version DAG, the time slowdown and space expansion match the information theoretic lower bounds to within a factor of &Ogr;(log2(\u00a6V\u00a6)).",
                "id": "02076"
            },
            {
                "title": "Approaching utopia: strong truthfulness and externality-resistant mechanisms",
                "abstract": "We introduce and study strongly truthful mechanisms and their applications. We use strongly truthful mechanisms as a tool for implementation in undominated strategies for several problems, including the design of externality resistant auctions and a variant of multi-dimensional scheduling.",
                "id": "02077"
            },
            {
                "title": "The Invisible Hand of Dynamic Market Pricing.",
                "abstract": "Walrasian prices, if they exist, have the property that one can assign every buyer some bundle in her demand set, such that the resulting assignment will maximize social welfare. Unfortunately, this assumes carefully breaking ties amongst different bundles in the buyer demand set. Presumably, the shopkeeper cleverly convinces the buyer to break ties in a manner consistent with maximizing social welfare. Lacking such a shopkeeper, if buyers arrive sequentially and simply choose some arbitrary bundle in their demand set, the social welfare may be arbitrarily bad. In the context of matching markets, we show how to compute dynamic prices, based upon the current inventory, that guarantee that social welfare is maximized. Such prices are set without knowing the identity of the next buyer to arrive. We also show that this is impossible in general (e.g., for coverage valuations), but consider other scenarios where this can be done. We further extend our results to Bayesian and bounded rationality models.",
                "id": "02078"
            },
            {
                "title": "Coresets forWeighted Facilities and Their Applications",
                "abstract": "We develop efficient (1 + \\varepsilon)-approximation algorithms for generalized facility location problems. Such facilities are not restricted to being points in \\mathbb{R}^d, and can represent more complex structures such as linear facilities (lines in \\mathbb{R}^d, j-dimensional flats), etc. We introduce coresets for weighted (point) facilities. These prove to be useful for such generalized facility location problems, and provide efficient algorithms for their construction. Applications include: k-mean and k-median generalizations, i.e., find k lines that minimize the sum (or sum of squares) of the distances from each input point to its nearest line. Other applications are generalizations of linear regression problems to multiple regression lines, new SVD/PCA generalizations, and many more. The results significantly improve on previous work, which deals efficiently only with special cases. Open source code for the algorithms in this paper is also available.",
                "id": "02079"
            },
            {
                "title": "Highway Dimension and Provably Efficient Shortest Path Algorithms.",
                "abstract": "Computing driving directions has motivated many shortest path algorithms based on preprocessing. Given a graph, the preprocessing stage computes a modest amount of auxiliary data, which is then used to speed up online queries. In practice, the best algorithms have storage overhead comparable to the graph size and answer queries very fast, while examining a small fraction of the graph. In this article, we complement the experimental evidence with the first rigorous proofs of efficiency for some of the speedup techniques developed over the past decade or variations thereof. We define highway dimension, which strengthens the notion of doubling dimension. Under the assumption that the highway dimension is low (at most polylogarithmic in the graph size), we show that, for some algorithms or their variants, preprocessing can be implemented in polynomial time, the resulting auxiliary data increases the storage requirements by a polylogarithmic factor, and queries run in polylogarithmic time. This gives a unified explanation for the performance of several seemingly different approaches. Our best bounds are based on a result that may be of independent interest: we show that unique shortest paths induce set systems of low VC-dimension, which makes them combinatorially simple.",
                "id": "02080"
            },
            {
                "title": "Nonoblivious hashing",
                "abstract": "Nonoblivious hashing, where information gathered from unsuccessful probes is used to modify subsequent probe strategy, is introduced and used to obtain the following results for static lookup on full tables:(1) An O(1)-time worst-case scheme that uses only logarithmic additional memory, (and no memory when the domain size is linear in the table size), which improves upon previously linear space requirements.(2) An almost sure O(1)-time probabilistic worst-case scheme, which uses no additional memory and which improves upon previously logarithmic time requirements.(3) Enhancements to hashing: (1) and (2) are solved for multikey recors, where search can be performed under any key in time O(1); these schemes also permit properties, such as nearest neighbor and rank, to be determined in logarithmic time.",
                "id": "02081"
            },
            {
                "title": "Censorship Resistant Peer-to-Peer Content Addressable Networks",
                "abstract": " We present a censorship resistant peer-to-peer Content Addressable Network for accessing n data items in a network of n nodes. Each search for a data item in the network takes O(log n) time and requires at most O(log^2 n) messages. Our network is censorship resistant in the sense that even after adversarial removal of an arbitrarily large constant fraction of the nodes in the network, all but an arbitrarily small fraction of the remaining nodes can obtain all but an arbitrarily small fraction... ",
                "id": "02082"
            },
            {
                "title": "Heat and Dump: competitive distributed paging",
                "abstract": "This paper gives a randomized competitive distributed paging algorithm called Heat and Dump, The competitive ratio is logarithmic in the total storage capacity of the network, this is optimal to within a constant factor. This is in contrast to the linear optimal deterministic competitive ratio.",
                "id": "02083"
            },
            {
                "title": "Distributed paging for general networks",
                "abstract": "No abstract available.\n\n",
                "id": "02084"
            },
            {
                "title": "Some issues regarding search, censorship, and anonymity in peer to peer networks",
                "abstract": "In this survey talk we discuss several problems related to peer to peer networks. A host of issues arises in the context of peer to peer networks, including efficiency issues, censorship issues, anonymity issues, etc. While many of these problems have been studied in the past, the file swapping application has taken over the Internet, given these problems renewed impetus. I will discuss papers co-authored with J. Saia, E. Cohen, H. Kaplan, R. Berman, A. Ta-Sham, and others.",
                "id": "02085"
            }
        ]
    },
    {
        "id": "021",
        "input": "For an author who has written the paper with the title \"RKA security beyond the linear barrier: IBE, encryption and signatures\", which reference is related? Just answer with [1] or [2] without explanation. [1]: \"On the odd and the aperiodic correlation properties of the Kasami sequences\" [2]: \"Unrestricted Aggregate Signatures\"",
        "profile": [
            {
                "title": "Bounds on partial correlations of sequences",
                "abstract": "Two approaches to bounding the partial auto- and crosscorrelations of binary sequences are considered. The first approach uses the discrete Fourier transform and bounds for character sums to obtain bounds on partial autocorrelations of m-sequences and on the partial auto- and crosscorrelations for the small Kasami sets and dual-BCH families of sequences. The second approach applies to binary sequences obtained by interleaving m-sequences. A bound on the peak partial correlation of such sequences is derived in terms of the peak partial autocorrelation of the underlying m-sequences. The bound is applied to GMW, No (1987), and other families of sequences for particular parameters. A comparison of the two approaches shows that the elementary method gives generally weaker results but is more widely applicable. On the other hand, both methods show that well-known sequence families can have favorable partial correlation characteristics, making them useful in certain spread-spectrum applications",
                "id": "0210"
            },
            {
                "title": "Programmable Hash Functions in the Multilinear Setting.",
                "abstract": "We adapt the concept of a programmable hash function (PHF, Crypto 2008) to a setting in which a multilinear map is available. This enables new PHFs with previously unachieved parameters. To demonstrate their usefulness, we show how our (standard-model) PHFs can replace random oracles in several well-known cryptographic constructions. Namely, we obtain standard-model versions of the Boneh-Franklin identity-based encryption scheme, the Boneh-Lynn-Shacham signature scheme, and the Sakai-Ohgishi-Kasahara identity-based non-interactive key exchange (ID-NIKE) scheme. The ID-NIKE scheme is the first scheme of its kind in the standard model. Our abstraction also allows to derive hierarchical versions of the above schemes in settings with multilinear maps. This in particular yields simple and efficient hierarchical generalizations of the BF, BLS, and SOK schemes. In the case of hierarchical ID-NIKE, ours is the first such scheme with full security, in either the random oracle model or the standard model. While our constructions are formulated with respect to a generic multilinear map, we also outline the necessary adaptations required for the recent \"noisy\" multilinear map candidate due to Garg, Gentry, and Halevi.",
                "id": "0211"
            },
            {
                "title": "Tag size does matter: attacks and proofs for the TLS record protocol",
                "abstract": "We analyze the security of the TLS Record Protocol, a MAC-then-Encode-then-Encrypt (MEE) scheme whose design targets confidentiality and integrity for application layer communications on the Internet. Our main results are twofold. First, we give a new distinguishing attack against TLS when variable length padding and short (truncated) MACs are used. This combination will arise when standardized TLS 1.2 extensions (RFC 6066) are implemented. Second, we show that when tags are longer, the TLS Record Protocol meets a new length-hiding authenticated encryption security notion that is stronger than IND-CCA.",
                "id": "0212"
            },
            {
                "title": "On the security of RC4 in TLS",
                "abstract": "The Transport Layer Security (TLS) protocol aims to provide confidentiality and integrity of data in transit across untrusted networks. TLS has become the de facto protocol standard for secured Internet and mobile applications. TLS supports several symmetric encryption options, including a scheme based on the RC4 stream cipher. In this paper, we present ciphertext-only plaintext recovery attacks against TLS when RC4 is selected for encryption. Our attacks build on recent advances in the statistical analysis of RC4, and on new findings announced in this paper. Our results are supported by an experimental evaluation of the feasibility of the attacks. We also discuss countermeasures.",
                "id": "0213"
            },
            {
                "title": "Building Key-Private Public-Key Encryption Schemes",
                "abstract": "In the setting of identity-based encryption with multiple trusted authorities, TA anonymity formally models the inability of an adversary to distinguish two ciphertexts corresponding to the same message and identity, but generated using different TA master public-keys. This security property has applications in the prevention of traffic analysis in coalition networking environments. In this paper, we examine the implications of TA anonymity for key-privacy for normal public-key encryption (PKE) schemes. Key-privacy for PKE captures the requirement that ciphertexts should not leak any information about the public-keys used to perform encryptions. Thus key-privacy guarantees recipient anonymity for a PKE scheme. Canetti, Halevi and Katz (CHK) gave a generic transform which constructs an IND-CCA secure PKE scheme using an identity-based encryption (IBE) scheme that is selective-id IND-CPA secure and a strongly secure one-time signature scheme. Their transform works in the standard model (i.e. does not require the use of random oracles). Here, we prove that if the underlying IBE scheme in the CHK transform is TA anonymous, then the resulting PKE scheme enjoys key-privacy. Whilst IND-CCA secure, key-private PKE schemes are already known in the standard-model, our result gives the first generic method of constructing a key-private PKE scheme in the standard model. We then go on to investigate the TA anonymity of multi-TA versions of well-known standard model secure IBE schemes. In particular, we prove the TA anonymity and selective-id IND-CPA security of a multi-TA version of Gentry's IBE scheme. Applying the CHK transform, we obtain a new, efficient key- private, IND-CCA secure PKE scheme in the standard model.",
                "id": "0214"
            },
            {
                "title": "One-round key exchange in the standard model",
                "abstract": "We consider one-round key exchange protocols secure in the standard model. The security analysis uses the powerful security model of Canetti and Krawczyk and a natural extension of it to the ID-based setting. It is shown how Key-Encapsulation Mechanisms (KEMs) can be used in a generic way to obtain two different protocol designs with progressively stronger security guarantees. A detailed analysis of the performance of the protocols is included; surprisingly, when instantiated with specific KEM constructions, the resulting protocols are competitive with the best previous schemes that have proofs only in the Random Oracle Model.",
                "id": "0215"
            },
            {
                "title": "Reactive and Proactive Standardisation of TLS.",
                "abstract": "In the development of TLS 1.3, the IETF TLS Working Group has adopted an \"analysis-prior-to-deployment\" design philosophy. This is in sharp contrast to all previous versions of the protocol. We present an account of the TLS standardisation narrative, examining the differences between the reactive standardisation process for TLS 1.2 and below, and the more proactive standardisation process for TLS 1.3. We explore the possible factors that have contributed to the shift in the TLS WG's design mindset, considering the protocol analysis tools available, the levels of academic involvement and the incentives governing relevant stakeholders at the time of standardisation. In an attempt to place TLS within the broader realm of standardisation, we perform a comparative analysis of standardisation models and discuss the standardisation of TLS within this context.",
                "id": "0216"
            },
            {
                "title": "Pseudo Constant Time Implementations of TLS Are Only Pseudo Secure.",
                "abstract": "Today, about 10% of TLS connections are still using CBC-mode cipher suites, despite a long history of attacks and the availability of better options (e.g. AES-GCM). In this work, we present three new types of attack against four popular fully patched implementations of TLS (Amazon's s2n, GnuTLS, mbed TLS and wolfSSL) which elected to use \"pseudo constant time\" countermeasures against the Lucky 13 attack on CBC-mode. Our attacks combine several variants of the PRIME+PROBE cache timing technique with a new extension of the original Lucky 13 attack. They apply in a cross-VM attack setting and are capable of recovering most of the plaintext whilst requiring only a moderate number of TLS connections. Along the way, we uncovered additional serious (but easy to patch) bugs in all four of the TLS implementations that we studied; in three cases, these bugs lead to Lucky 13 style attacks that can be mounted remotely with no access to a shared cache. Our work shows that adopting pseudo constant time countermeasures is not sufficient to attain real security in TLS implementations in CBC mode.\n\n",
                "id": "0217"
            },
            {
                "title": "Efficient One-Round Key Exchange in the Standard Model",
                "abstract": "We consider one-round key exchange protocols secure in the standard model. The security analysis uses the powerful security model of Canetti and Krawczyk and a natural extension of it to the ID-based setting. It is shown how KEMs can be used in a generic way to obtain two different protocol designs with progressively stronger security guarantees. A detailed analysis of the performance of the protocols is included; surprisingly, when instantiated with specific KEM constructions, the resulting protocols are competitive with the best previous schemes that have proofs only in the random oracle model.",
                "id": "0218"
            },
            {
                "title": "One-time-password-authenticated key exchange",
                "abstract": "To reduce the damage of phishing and spyware attacks, banks, governments, and other security-sensitive industries are deploying onetime password systems, where users have many passwords and use each password only once. If a single password is compromised, it can be only be used to impersonate the user once, limiting the damage caused. However, existing practical approaches to one-time passwords have been susceptible to sophisticated phishing attacks. We give a formal security treatment of this important practical problem. We consider the use of one-time passwords in the context of password-authenticated key exchange (PAKE), which allows for mutual authentication, session key agreement, and resistance to phishing attacks. We describe a security model for the use of one-time passwords, explicitly considering the compromise of past (and future) one-time passwords, and show a general technique for building a secure one-time-PAKE protocol from any secure PAKE protocol. Our techniques also allow for the secure use of pseudorandomly generated and time-dependent passwords.",
                "id": "0219"
            },
            {
                "title": "On the relations between non-interactive key distribution, identity-based encryption and trapdoor discrete log groups",
                "abstract": "This paper investigates the relationships between identity-based non-interactive key distribution (ID-NIKD) and identity-based encryption (IBE). It provides a new security model for ID-NIKD, and a construction that converts a secure ID-NIKD scheme satisfying certain conditions into a secure IBE scheme. This conversion is used to explain the relationship between the ID-NIKD scheme of Sakai, Ohgishi and Kasahara and the IBE scheme of Boneh and Franklin. The paper then explores the construction of ID-NIKD and IBE schemes from general trapdoor discrete log groups. Two different concrete instantiations for such groups provide new, provably secure ID-NIKD and IBE schemes. These schemes are suited to applications in which the Trusted Authority is computationally well-resourced, but clients performing encryption/decryption are highly constrained.",
                "id": "02110"
            },
            {
                "title": "A Cryptographic Analysis of the WireGuard Protocol.",
                "abstract": "WireGuard (Donenfeld, NDSS 2017) is a recently proposed secure network tunnel operating at layer 3. WireGuard aims to replace existing tunnelling solutions like IPsec and OpenVPN, while requiring less code, being more secure, more performant, and easier to use. The cryptographic design of WireGuard is based on the Noise framework. It makes use of a key exchange component which combines long-term and ephemeral Diffie-Hellman values (along with optional preshared keys). This is followed by the use of the established keys in an AEAD construction to encapsulate IP packets in UDP. To date, WireGuard has received no rigorous security analysis. In this paper, we, rectify this. We first observe that, in order to prevent Key Compromise Impersonation (KCI) attacks, any analysis of WireGuard\u2019s key exchange component must take into account the first AEAD ciphertext from initiator to responder. This message effectively acts as a key confirmation and makes the key exchange component of WireGuard a 1.5 RTT protocol. However, the fact that this ciphertext is computed using the established session key rules out a proof of session key indistinguishability for WireGuard\u2019s key exchange component, limiting the degree of modularity that is achievable when analysing the protocol\u2019s security. To overcome this proof barrier, and as an alternative to performing a monolithic analysis of the entire WireGuard protocol, we add an extra message to the protocol. This is done in a minimally invasive way that does not increase the number of round trips needed by the overall WireGuard protocol. This change enables us to prove strong authentication and key indistinguishability properties for the key exchange component of WireGuard under standard cryptographic assumptions.",
                "id": "02111"
            },
            {
                "title": "A Weak Cipher that Generates the Symmetric Group.",
                "abstract": "There has been recent interest in the permutation group generated by the round functions of a block cipher. In this paper we present a cautionary example of a block cipher which generates the full sym- metric group yet is very weak.",
                "id": "02112"
            },
            {
                "title": "e-EMV: emulating EMV for internet payments with trusted computing technologies",
                "abstract": "The introduction of EMV-compliant payment cards, with their improved cardholder veriflcation and card authentication capabilities, has resulted in a dramatic reduction in the levels of fraud seen at Point of Sale (PoS) terminals across Europe. However, this reduction has been accompanied by an alarming increase in the level of fraud associated with Internet-based Card Not Present (CNP) transactions. This increase is largely at- tributable to the weaker authentication procedures involved in CNP transactions. This paper shows how the functionality associated with EMV-compliant payment cards can be securely emulated in software on platforms supporting Trusted Computing technology. We describe a detailed system architecture encompassing user enrollment, card deploy- ment (in the form of software), card activation, and subsequent transaction processing. Our proposal is compatible with the existing EMV transaction processing architecture, and thus integrates fully and naturally with already deployed EMV infrastructure. We show that our proposal, which efiectively makes available the full security of PoS trans- actions for Internet-based CNP transactions, has the potential to signiflcantly reduce the opportunity for fraudulent CNP transactions.",
                "id": "02113"
            },
            {
                "title": "A guide to trust in mobile ad hoc networks",
                "abstract": "In this paper, we examine issues of trust and reputation in mobile ad hoc networks (MANETs). We look at a number of the trust and reputation models that have been proposed, and we highlight open problems in this area. Copyright (c) 2009 John Wiley & Sons, Ltd.",
                "id": "02114"
            },
            {
                "title": "Plaintext Recovery Attacks Against WPA/TKIP.",
                "abstract": "We conduct an analysis of the RC4 algorithm as it is used in the IEEE WPA/TKIP wireless standard. In that standard, RC4 keys are computed on a per-frame basis, with specific key bytes being set to known values that depend on 2 bytes of the WPA frame counter (called the TSC). We observe very large, TSC-dependent biases in the RC4 keystream when the algorithm is keyed according to the WPA specification. These biases permit us to mount an effective statistical, plaintext-recovering attack in the situation where the same plaintext is encrypted in many different frames (the so-called \"broadcast attack\" setting). We assess the practical impact of these attacks on WPA/TKIP.",
                "id": "02115"
            },
            {
                "title": "Trusted Computing: Providing Security for Peer-to-Peer Networks",
                "abstract": "In this paper, we demonstrate the application of Trusted Computing to securing Peer-to-Peer (P2P) networks. We identify a central challenge in providing many of the security services within these networks, namely the absence of stable verifiable peer identities. We employ the functionalities provided by Trusted Computing technology to establish a pseudonymous authentication scheme for peers and extend this scheme to build secure channels between peers for future communications. In support of our work, we illustrate how commands from the Trusted Computing Group (TCG) specifications can be used to implement our approach in P2P networks.",
                "id": "02116"
            },
            {
                "title": "Authenticated-Encryption with padding: a formal security treatment",
                "abstract": "Vaudenay's padding oracle attacks are a powerful type of side-channel attack against systems using CBC mode encryption. They have been shown to work in practice against certain implementations of important secure network protocols, including IPsec and SSL/TLS. A formal security analysis of CBC mode in the context of padding oracle attacks in the chosen-plaintext setting was previously performed by the authors. In this paper, we consider the chosen-ciphertext setting, examining the question of how CBC mode encryption, padding, and an integrity protection mechanism should be combined in order to provably defeat padding oracle attacks. We introduce new security models for the chosen-ciphertext setting which we then use to formally analyse certain authenticated-encryption schemes, namely the three compositions: Pad-then-Encrypt-then-Authenticate (as used in particular configurations of IPsec), Pad-then-Authenticate-then-Encrypt, and Authenticate-then-Pad-then-Encrypt (as used in SSL/TLS).",
                "id": "02117"
            },
            {
                "title": "Unpicking PLAID - A Cryptographic Analysis of an ISO-standards-track Authentication Protocol.",
                "abstract": "The Protocol for Lightweight Authentication of Identity (PLAID) aims at secure and private authentication between a smart card and a terminal. Originally developed by a unit of the Australian Department of Human Services for physical and logical access control, PLAID has now been standardized as an Australian standard AS-5185-2010 and is currently in the fast-track standardization process for ISO/IEC 25185-1. We present a cryptographic evaluation of PLAID. As well as reporting a number of undesirable cryptographic features of the protocol, we show that the privacy properties of PLAID are significantly weaker than claimed: using a variety of techniques, we can fingerprint and then later identify cards. These techniques involve a novel application of standard statistical and data analysis techniques in cryptography. We discuss potential countermeasures to our attacks and comment on our experiences with the standardization process of PLAID.",
                "id": "02118"
            },
            {
                "title": "On the joint security of encryption and signature in EMV",
                "abstract": "We provide an analysis of current and future algorithms for signature and encryption in the EMV standards in the case where a single key-pair is used for both signature and encryption. We give a theoretical attack for EMV's current RSA-based algorithms, showing how access to a partial decryption oracle can be used to forge a signature on a freely chosen message. We show how the attack might be integrated into EMV's CDA protocol flow, enabling an attacker with a wedge device to complete an offline transaction without knowing the cardholder's PIN. Finally, the elliptic curve signature and encryption algorithms that are likely to be adopted in a forthcoming version of the EMV standards are analyzed in the single key-pair setting, and shown to be secure.",
                "id": "02119"
            },
            {
                "title": "Coming of Age: A Longitudinal Study of TLS Deployment.",
                "abstract": "The Transport Layer Security (TLS) protocol is the de-facto standard for encrypted communication on the Internet. However, it has been plagued by a number of different attacks and security issues over the last years. Addressing these attacks requires changes to the protocol, to server- or client-software, or to all of them. In this paper we conduct the first large-scale longitudinal study examining the evolution of the TLS ecosystem over the last six years. We place a special focus on the ecosystem's evolution in response to high-profile attacks.\n\nFor our analysis, we use a passive measurement dataset with more than 319.3B connections since February 2012, and an active dataset that contains TLS and SSL scans of the entire IPv4 address space since August 2015. To identify the evolution of specific clients we also create the---to our knowledge---largest TLS client fingerprint database to date, consisting of 1,684 fingerprints.\n\nWe observe that the ecosystem has shifted significantly since 2012, with major changes in which cipher suites and TLS extensions are offered by clients and accepted by servers having taken place. Where possible, we correlate these with the timing of specific attacks on TLS. At the same time, our results show that while clients, especially browsers, are quick to adopt new algorithms, they are also slow to drop support for older ones. We also encounter significant amounts of client software that probably unwittingly offer unsafe ciphers. We discuss these findings in the context of long tail effects in the TLS ecosystem.\n\n",
                "id": "02120"
            },
            {
                "title": "Pairings for cryptographers",
                "abstract": "Many research papers in pairing-based cryptography treat pairings as a ''black box''. These papers build cryptographic schemes making use of various properties of pairings. If this approach is taken, then it is easy for authors to make invalid assumptions concerning the properties of pairings. The cryptographic schemes developed may not be realizable in practice, or may not be as efficient as the authors assume. The aim of this paper is to outline, in as simple a fashion as possible, the basic choices that are available when using pairings in cryptography. For each choice, the main properties and efficiency issues are summarized. The paper is intended to be of use to non-specialists who are interested in using pairings to design cryptographic schemes.",
                "id": "02121"
            },
            {
                "title": "Trust management for secure information flows",
                "abstract": "In both the commercial and defence sectors a compelling need is emerging for the rapid, yet secure, dissemination of information across traditional organisational boundaries. In this paper we present a novel trust management paradigm for securing pan-organisational information flows that aims to address the threat of information leakage. Our trust management system is built around an economic model and a trust-based encryption primitive wherein: (i) entities purchase a key from a Trust Authority (TA) which is bound to a voluntarily reported trust score r, (ii) information flows are encrypted such that a flow tagged with a recipient trust score R can be decrypted by the recipient only if it possesses the key corresponding to a voluntarily reported score r",
                "id": "02122"
            },
            {
                "title": "A cryptographic tour of the IPsec standards",
                "abstract": "In this article, we provide an overview of cryptography and cryptographic key management as they are specified in IPsec, a popular suite of standards for providing communications security and network access control for Internet communications. We focus on the latest generation of the IPsec standards, recently published as Request for Comments 4301-4309 by the Internet Engineering Task Force, and how they have evolved from earlier versions of the standards.",
                "id": "02123"
            },
            {
                "title": "Statistical Attacks on Cookie Masking for RC4.",
                "abstract": "Taken together, our analyses show that the cookie masking mechanisms as proposed by Levillain et al. only moderately increase the security of RC4 in SSL/TLS.",
                "id": "02124"
            },
            {
                "title": "Anonymous broadcast encryption: adaptive security and efficient constructions in the standard model",
                "abstract": "In this paper we consider anonymity in the context of Broadcast Encryption (BE). This issue has received very little attention so far and all but one of the currently available BE schemes fail to provide anonymity. Yet, we argue that it is intrinsically desirable to provide anonymity in standard applications of BE and that it can be achieved at a moderate cost. We provide a security definition for Anonymous Broadcast Encryption (ANOBE) and show that it is achievable assuming only the existence of IND-CCA secure public key encryption (PKE). Focusing on reducing the size of ciphertexts, we then give two generic constructions for ANOBE. The first is from any anonymous (key-private) IND-CCA secure PKE scheme, and the second is from any IBE scheme that satisfies a weak security notion in the multi-TA setting. Furthermore, we show how randomness re-use techniques can be deployed in the ANOBE context to reduce computational and communication costs, and how a new cryptographic primitive --- anonymous hint systems --- can be used to speed up the decryption process in our ANOBE constructions. All of our results are in the standard model, achieving fully collusion-resistant ANOBE schemes secure against adaptive IND-CCA adversaries.",
                "id": "02125"
            },
            {
                "title": "Generalized Reed-Muller codes and power control in OFDM modulation",
                "abstract": "Controlling the peak-to-mean envelope power ratio (PMEPR) of orthogonal frequency-division multiplexed (OFDM) transmissions is a notoriously difficult problem, though one which is of vital importance for the practical application of OFDM in low-cost applications. The utility of Golay complementary sequences in solving this problem has been recognized for some time. In this paper, a powerful theory linking Golay complementary sets of polyphase sequences and Reed-Muller codes is developed. Our main result shows that any second-order coset of a q-ary generalization of the first order Reed-Muller code can be partitioned into Golay complementary sets whose size depends only on a single parameter that is easily computed from a graph associated with the coset. As a first consequence, recent results of Davis and Jedwab (see Electron. Lett., vol.33, p.267-8, 1997) on Golay pairs, as well as earlier constructions of Golay (1949, 1951, 1961), Budisin (1990) and Sivaswamy (1978) are shown to arise as special cases of a unified theory for Golay complementary sets. As a second consequence, the main result directly yields bounds on the PMEPRs of codes formed from selected cosets of the generalized first order Reed-Muller code. These codes enjoy efficient encoding, good error-correcting capability, and tightly controlled PMEPR, and significantly extend the range of coding options for applications of OFDM using small numbers of carriers",
                "id": "02126"
            },
            {
                "title": "Augmenting Internet-Based Card Not Present Transactions with Trusted Computing (Extended Abstract)",
                "abstract": "We demonstrate how Trusted Computing technology can be used to enhance the security of Internet-based Card Not Present (CNP) transactions. We focus on exploiting features of Trusted Computing as it is being deployed today, relying only on the presence of client-side Trusted Platform Modules. We discuss the threats to CNP transactions that remain even with our enhancements in place, focussing in particular on the threat of malware, and how it can be ameliorated.",
                "id": "02127"
            },
            {
                "title": "Plaintext-Dependent decryption: a formal security treatment of SSH-CTR",
                "abstract": "This paper presents a formal security analysis of SSH in counter mode in a security model that accurately captures the capabilities of real-world attackers, as well as security-relevant features of the SSH specifications and the OpenSSH implementation of SSH. Under reasonable assumptions on the block cipher and MAC algorithms used to construct the SSH Binary Packet Protocol (BPP), we are able to show that the SSH BPP meets a strong and appropriate notion of security: indistinguishability under buffered, stateful chosen-ciphertext attacks. This result helps to bridge the gap between the existing security analysis of the SSH BPP by Bellare et al. and the recently discovered attacks against the SSH BPP by Albrecht et al. which partially invalidate that analysis.",
                "id": "02128"
            },
            {
                "title": "Improved Reconstruction Attacks on Encrypted Data Using Range Query Leakage",
                "abstract": "We analyse the security of database encryption schemes supporting range queries against persistent adversaries. The bulk of our work applies to a generic setting, where the adversary's view is limited to the set of records matched by each query (known as access pattern leakage). We also consider a more specific setting where rank information is also leaked, which is inherent inherent to multiple recent encryption schemes supporting range queries. We provide three attacks. First, we consider full reconstruction, which aims to recover the value of every record, fully negating encryption. We show that for dense datasets, full reconstruction is possible within an expected number of queries N log N + O(N), where N is the number of distinct plaintext values. This directly improves on a quadratic bound in the same setting by Kellaris et al. (CCS 2016). Second, we present an approximate reconstruction attack recovering all plaintext values in a dense dataset within a constant ratio of error, requiring the access pattern leakage of only O(N) queries. Third, we devise an attack in the common setting where the adversary has access to an auxiliary distribution for the target dataset. This third attack proves highly effective on age data from real-world medical data sets. In our experiments, observing only 25 queries was sufficient to reconstruct a majority of records to within 5 years. In combination, our attacks show that current approaches to enabling range queries offer little security when the threat model goes beyond snapshot attacks to include a persistent server-side adversary.",
                "id": "02129"
            },
            {
                "title": "Modular security proofs for key agreement protocols",
                "abstract": "The security of key agreement protocols has traditionally been notoriously hard to establish. In this paper we present a modular approach to the construction of proofs of security for a large class of key agreement protocols. By following a modular approach to proof construction, we hope to enable simpler and less error-prone analysis and proof generation for such key agreement protocols. The technique is compatible with Bellare-Rogaway style models as well as the more recent models of Bellare et al. and Canetti and Krawczyk. In particular, we show how the use of a decisional oracle can aid the construction of proofs of security for this class of protocols and how the security of these protocols commonly reduces to some form of Gap assumption.",
                "id": "02130"
            },
            {
                "title": "On the existence and construction of good codes with low peak-to-average power ratios",
                "abstract": "The first lower bound on the peak-to-average power ratio (PAPR) of a constant energy code of a given length n, minimum Euclidean distance and rate is established. Conversely, using a nonconstructive Varshamov-Gilbert style argument yields a lower bound on the achievable rate of a code of a given length, minimum Euclidean distance and maximum PAPR. The derivation of these bounds relies on a geometrical analysis of the PAPR of such a code. Further analysis shows that there exist asymptotically good codes whose PAPR is at most 8 log n. These bounds motivate the explicit construction of error-correcting codes with low PAPR. Bounds for exponential sums over Galois fields and rings are applied to obtain an upper bound of order (log n)2 on the PAPRs of a constructive class of codes, the trace codes. This class includes the binary simplex code, duals of binary, primitive Bose-Chaudhuri-Hocquenghem (BCH) codes and a variety of their nonbinary analogs. Some open problems are identified",
                "id": "02131"
            },
            {
                "title": "Multi-key hierarchical identity-based signatures",
                "abstract": "We motivate and investigate a new cryptographic primitive that we call multi-key hierarchical identity-based signatures (multikey HIBS). Using this primitive, a user is able to prove possession of a set of identity-based private keys associated with nodes at arbitrary levels of a hierarchy when signing a message. Our primitive is related to, but distinct from, the notions of identity-based multi-signatures and aggregate signatures. We develop a security model for multi-key HIBS. We then present and prove secure an efficient multi-key HIBS scheme that is based on the Gentry-Silverberg hierarchical identity-based signature scheme.",
                "id": "02132"
            },
            {
                "title": "Attacking the IPsec Standards in Encryption-only Configurations",
                "abstract": "We describe new attacks which break any RFC- compliant implementation of IPsec making use of encryption-only ESP in tunnel mode. The new attacks are both efficient and realistic: they are ciphertext-only and need only the capability to eavesdrop on ESP-encrypted traffic and to inject traffic into the network. We report on our experiences in applying the attacks to a variety of implementations of IPsec.",
                "id": "02133"
            },
            {
                "title": "Key Agreement Using Statically Keyed Authenticators",
                "abstract": "A family of authenticators based on static shared keys is identified and proven secure. The authenticators can be used in a variety of settings, including identity-based ones. Application of the authenticators to Diffie-Hellman variants in appropriate groups leads to authenticated key agreement protocols which have attractive properties in comparison with other proven-secure protocols. We explore two key agreement protocols that result.",
                "id": "02134"
            },
            {
                "title": "Permutation polynomials, de Bruijn sequences, and linear complexity",
                "abstract": "The paper establishes a connection between the theory of permutation polynomials and the question of whether a de Bruijn sequence over a general finite field of a given linear complexity exists. The connection is used both to construct span 1 de Bruijn sequences (permutations) of a range of linear complexities and to prove non-existence results for arbitrary spans. Upper and lower bounds for the linear complexity of a de Bruijn sequence of span n over a finite field are established. Constructions are given to show that the upper bound is always tight, and that the lower bound is also tight in many cases.",
                "id": "02135"
            },
            {
                "title": "Concurrent Signatures",
                "abstract": "We introduce the concept of concurrent signatures. These allow two entities to produce two signatures in such a way that, from the point of view of any third party, both signatures are ambiguous with respect to the identity of the signing party until an extra piece of information (the keystone) is released by one of the parties. Upon release of the keystone, both signatures become binding to their true signers concurrently. Concurrent signatures fall just short of providing a full solution to the problemoffairexchangeofsignatures,butwediscusssomeapplications in whichconcurrentsignaturessu-ce. Concurrentsignaturesarehighly e-cientandrequireneitheratrustedarbitratornorahighdegreeofin- teractionbetweenparties.Weprovideamodelofsecurityforconcurrent signatures,andaconcreteschemewhichweprovesecureintherandom oracle model underthe discrete logarithm assumption.",
                "id": "02136"
            },
            {
                "title": "Quantum cryptography: a practical information security perspective",
                "abstract": "Quantum Key Exchange (QKE, also known as Quantum Key Distribution or QKD)\nallows communicating parties to securely establish cryptographic keys. It is a\nwell-established fact that all QKE protocols require that the parties have\naccess to an authentic channel. Without this authenticated link, QKE is\nvulnerable to man-in-the-middle attacks. Overlooking this fact results in\nexaggerated claims and/or false expectations about the potential impact of QKE.\nIn this paper we present a systematic comparison of QKE with traditional key\nestablishment protocols in realistic secure communication systems.",
                "id": "02137"
            },
            {
                "title": "Challenges for Trusted Computing",
                "abstract": "Whether being heralded as a means of securing sensitive user data or condemned as a method of usurping owner control, trusted computing is proving to be one of the most controversial security technologies in recent years. Rather than become embroiled in the debate over possible (mis)appropriations of trusted computing technologies, the authors highlight some of the technical obstacles that can hinder trusted computing's widespread adoption.",
                "id": "02138"
            },
            {
                "title": "Binary sequence sets with favorable correlations from difference sets and MDS codes",
                "abstract": "We propose new families of pseudorandom binary sequences based on Hadamard difference sets and MDS codes. We obtain, for p=4k-1 prime and t an integer with 1&les;t&les;(p-1)/2, a set of pt binary sequences of period p2 whose peak correlation is bounded by 1+2t(p+1). The sequences are balanced, have high linear complexity, and are easily generated",
                "id": "02139"
            },
            {
                "title": "Computing the error linear complexity spectrum of a binary sequence of period 2n",
                "abstract": "Binary sequences with high linear complexity are of interest in cryptography. The linear complexity should remain high even when a small number of changes are made to the sequence. The error linear complexity spectrum of a sequence reveals how the linear complexity of the sequence varies as an increasing number of the bits of the sequence are changed. We present an algorithm which computes the error linear complexity for binary sequences of period \u2113=2n using O(\u2113(log\u2113)2) bit operations. The algorithm generalizes both the Games-Chan (1983) and Stamp-Martin (1993) algorithms, which compute the linear complexity and the k-error linear complexity of a binary sequence of period \u2113=2n, respectively. We also discuss an application of an extension of our algorithm to decoding a class of linear subcodes of Reed-Muller codes.",
                "id": "02140"
            },
            {
                "title": "Perfect Factors from Cyclic Codes and Interleaving",
                "abstract": "In this paper, we introduce new construction methods for Perfect Factors. These are based on the theory of cyclic codes, interleaving techniques and the Lempel homomorphism. The constructions enable us to settle the existence question for Perfect Factors for window sizes at most six.",
                "id": "02141"
            },
            {
                "title": "What can identity-based cryptography offer to web services?",
                "abstract": "Web services are seen as the enabler of service-oriented computing, a promising next generation distributed computing technology. Independently, identity-based cryptography is emerging as a serious contender to more conventional certificate-based public key cryptography. However, the application of identity-based cryptography in web services appears largely unexplored. This paper sets out to examine how identity-based cryptography might be used to secure web services. We show that identity-based cryptography has some attractive properties which naturally suit the message-level security needed by web services.",
                "id": "02142"
            },
            {
                "title": "Provably secure key assignment schemes from factoring",
                "abstract": "We provide constructions for key assignment schemes that are provably secure under the factoring assumption in the standard model. Our first construction is for simple \"chain\" hierarchies, and achieves security against key recovery attacks with a tight reduction from the problem of factoring integers of a special form. Our second construction applies for general hierarchies, achieves the stronger notion of key indistinguishability, and has security based on the hardness of factoring Blum integers. We compare our constructions to previous schemes, in terms of security and efficiency.",
                "id": "02143"
            },
            {
                "title": "Lucky Thirteen: Breaking the TLS and DTLS Record Protocols",
                "abstract": "The Transport Layer Security (TLS) protocol aims to provide confidentiality and integrity of data in transit across untrusted networks. TLS has become the de facto secure protocol of choice for Internet and mobile applications. DTLS is a variant of TLS that is growing in importance. In this paper, we present distinguishing and plaintext recovery attacks against TLS and DTLS. The attacks are based on a delicate timing analysis of decryption processing in the two protocols. We include experimental results demonstrating the feasibility of the attacks in realistic network environments for several different implementations of TLS and DTLS, including the leading OpenSSL implementations. We provide countermeasures for the attacks. Finally, we discuss the wider implications of our attacks for the cryptographic design used by TLS and DTLS.",
                "id": "02144"
            },
            {
                "title": "Cryptography in theory and practice: the case of encryption in IPsec",
                "abstract": "Despite well-known results in theoretical cryptography highlighting the vulnerabilities of unauthenticated encryption, the IPsec standards mandate its support. We present evidence that such \u201cencryption-only\u201d configurations are in fact still often selected by users of IPsec in practice, even with strong warnings advising against this in the IPsec standards. We then describe a variety of attacks against such configurations and report on their successful implementation in the case of the Linux kernel implementation of IPsec. Our attacks are realistic in their requirements, highly efficient, and recover the complete contents of IPsec-protected datagrams. Our attacks still apply when integrity protection is provided by a higher layer protocol, and in some cases even when it is supplied by IPsec itself.",
                "id": "02145"
            },
            {
                "title": "A comparison between traditional public key infrastructures and identity-based cryptography",
                "abstract": "With the recent acceleration in research into identity-based public key cryptography (ID-PKC), we consider this to be an opportune moment to compare and contrast ID-PKC with more traditional public key infrastructures (PKI). Because of the similarity in the nature of both approaches, we aim to identify the distinguishing features of each approach. In doing so, we highlight the important questions to be asked when weighing up the benefits and drawbacks of the two technologies.",
                "id": "02146"
            },
            {
                "title": "Padding oracle attacks on CBC-Mode encryption with secret and random IVs",
                "abstract": "In [8], Paterson and Yau presented padding oracle attacks against a committee draft version of a revision of the ISO CBC-mode encryption standard [3]. Some of the attacks in [8] require knowledge and manipulation of the initialisation vector (IV). The latest draft of the revision of the standard [4] recommends the use of IVs that are secret and random. This obviates most of the attacks of [8]. In this paper we consider the security of CBC-mode encryption against padding oracle attacks in this secret, random IV setting. We present new attacks showing that several ISO padding methods are still weak in this situation.",
                "id": "02147"
            },
            {
                "title": "A method for constructing decodable de Bruijn sequences",
                "abstract": "We present two related methods of construction for de Bruijn (1946) sequences, both based on interleaving \u201csmaller\u201d de Bruijn sequences. Sequences obtained using these construction methods have the advantage that they can be \u201cdecoded\u201d very efficiently, i.e., the position within the sequence of any particular \u201cwindow\u201d can be found very simply. Sequences with simple decoding algorithms are of considerable practical importance in position location applications",
                "id": "02148"
            },
            {
                "title": "Storage Efficient Decoding For A Class Of Binary Be Bruijn Sequences",
                "abstract": "A binary span k de Bruijn sequence is a binary sequence of period 2(k) such that each k-tuple of bits occurs exactly once as a subsequence in a period of the sequence. The de Bruijn sequences have applications in position sensing and range finding, as well as in other areas, by virtue of this subsequence property. To make use of a particular de Bruijn sequence, we need to be able to determine the position of an arbitrary k-tuple in the sequence. We refer to this as 'decoding' the sequence. In this paper we use the structure of a class of de Bruijn sequences, originally constructed by Lempel, to give an algorithm for decoding that is fast, yet more space efficient than a complete look-up table of subsequences and their positions in a sequence. We also consider the case where the displacement between consecutive subsequences whose positions are decoded is restricted and obtain a further marked improvement in storage.",
                "id": "02149"
            },
            {
                "title": "User-friendly and certificate-free grid security infrastructure",
                "abstract": "Certificate-based public key infrastructures are currently widely used in computational grids to support security services. From a user\u2019s perspective, however, certificate acquisition is time-consuming and public/private key management is non-trivial. In this paper, we propose a security infrastructure for grid applications, in which users are authenticated using passwords. Our infrastructure allows a user to perform single sign-on based only on a password, without requiring a public key infrastructure. Moreover, hosting servers in our infrastructure are not required to have public key certificates. Nevertheless, our infrastructure supports essential grid security services, such as mutual authentication and delegation, using public key cryptographic techniques without incurring significant additional overheads in comparison with existing approaches.",
                "id": "02150"
            },
            {
                "title": "Analyzing Multi-Key Security Degradation.",
                "abstract": "The multi-key, or multi-user, setting challenges cryptographic algorithms to maintain high levels of security when used with many different keys, by many different users. Its significance lies in the fact that in the real world, cryptography is rarely used with a single key in isolation. A folklore result, proved by Bellare, Boldyreva, and Micali for public-key encryption in EUROCRYPT 2000, states that the success probability in attacking any one of many independently keyed algorithms can be bounded by the success probability of attacking a single instance of the algorithm, multiplied by the number of keys present. Although sufficient for settings in which not many keys are used, once cryptographic algorithms are used on an internet-wide scale, as is the case with TLS, the effect of multiplying by the number of keys can drastically erode security claims. We establish a sufficient condition on cryptographic schemes and security games under which multi-key degradation is avoided. As illustrative examples, we discuss how AES and GCM behave in the multi-key setting, and prove that GCM, as a mode, does not have multi-key degradation. Our analysis allows limits on the amount of data that can be processed per key by GCM to be significantly increased. This leads directly to improved security for GCM as deployed in TLS on the Internet today.",
                "id": "02151"
            },
            {
                "title": "Comments on \u201cTheory and applications of cellular automata in cryptography\u201d",
                "abstract": "The cipher systems based on Cellular Automata proposed by Nandi et al. [3] are affine and are insecure.",
                "id": "02152"
            },
            {
                "title": "Security of Symmetric Encryption in the Presence of Ciphertext Fragmentation.",
                "abstract": "In recent years, a number of standardized symmetric encryption schemes have fallen foul of attacks exploiting the fact that in some real world scenarios ciphertexts can be delivered in a fragmented fashion. We initiate the first general and formal study of the security of symmetric encryption against such attacks. We extend the SSH-specific work of Paterson and Watson (Eurocrypt 2010) to develop security models for the fragmented setting. We also develop security models to formalize the additional desirable properties of ciphertext boundary hiding and robustness against Denial-of-Service (DoS) attacks for schemes in this setting. We illustrate the utility of each of our models via efficient constructions for schemes using only standard cryptographic components, including constructions that simultaneously achieve confidentiality, ciphertext boundary hiding and DoS robustness.",
                "id": "02153"
            },
            {
                "title": "Lucky Microseconds: A Timing Attack on Amazon's s2n Implementation of TLS.",
                "abstract": "s2n is an implementation of the TLS protocol that was released in late June 2015 by Amazon. It is implemented in around 6,000 lines of C99 code. By comparison, OpenSSL needs around 70,000 lines of code to implement the protocol. At the time of its release, Amazon announced that s2n had undergone three external security evaluations and penetration tests. We show that, despite this, s2n -- as initially released -- was vulnerable to a timing attack in the case of CBC-mode ciphersuites, which could be extended to complete plaintext recovery in some settings. Our attack has two components. The first part is a novel variant of the Lucky 13 attack that works even though protections against Lucky 13 were implemented in s2n. The second part deals with the randomised delays that were put in place in s2n as an additional countermeasure to Lucky 13. Our work highlights the challenges of protecting implementations against sophisticated timing attacks. It also illustrates that standard code audits are insufficient to uncover all cryptographic attack vectors.",
                "id": "02154"
            },
            {
                "title": "Identity crisis: on the problem of namespace design for ID-PKC and MANETs",
                "abstract": "In this paper, we explore the 'interface' between identity-based public key cryptography (ID-PKC) and mobile ad hoc networks (MANETs). In particular, we examine the problem of naming and namespace design in an identity-based key infrastructure (IKI). We examine the potential impact that different types of identifiers may have on the utility of ad hoc networks where an IKI provides the underlying key infrastructure. We also highlight a number of open problems inherent in extending namespaces to allow inter-operability amongst heterogeneous trust domains. Copyright (c) 2009 John Wiley & Sons, Ltd.",
                "id": "02155"
            },
            {
                "title": "Attacks Only Get Better: Password Recovery Attacks Against RC4 in TLS",
                "abstract": "Despite recent high-profile attacks on the RC4 algorithm in TLS, its usage is still running at about 30% of all TLS traffic. We provide new attacks against RC4 in TLS that are focussed on recovering user passwords, still the pre-eminent means of user authentication on the Internet today. Our new attacks use a generally applicable Bayesian inference approach to transform a priori information about passwords in combination with gathered ciphertexts into a posteriori likelihoods for passwords. We report on extensive simulations of the attacks. We also report on a \\\"proof of concept\\\" implementation of the attacks for a specific application layer protocol, namely BasicAuth. Our work validates the truism that attacks only get better with time: we obtain good success rates in recovering user passwords with 226 encryptions, whereas the previous generation of attacks required around 234 encryptions to recover an HTTP session cookie.",
                "id": "02156"
            },
            {
                "title": "Simple, efficient and strongly KI-Secure hierarchical key assignment schemes",
                "abstract": "Hierarchical Key Assignment Schemes can be used to enforce access control policies by cryptographic means. In this paper, we present a new, enhanced security model for such schemes. We also give simple, efficient, and strongly-secure constructions for Hierarchical Key Assignment Schemes for arbitrary hierarchies using pseudorandom functions and forward-secure pseudorandom generators. We compare instantiations of our constructions with state-of-the-art Hierarchical Key Assignment Schemes, demonstrating that our new schemes possess an attractive trade-off between storage requirements and efficiency of key derivation.",
                "id": "02157"
            },
            {
                "title": "Plaintext Recovery Attacks against SSH",
                "abstract": "This paper presents a variety of plaintext-recovering attacks against SSH. We implemented a proof of concept of our attacks against OpenSSH, where we can verifiably recover 14 bits of plaintext from an arbitrary block of ciphertext with probability $2^{-14}$ and 32 bits of plaintext from an arbitrary block of ciphertext with probability $2^{-18}$. These attacks assume the default configuration of a 128-bit block cipher operating in CBC mode. The paper explains why a combination of flaws in the basic design of SSH leads implementations such as OpenSSH to be open to our attacks, why current provable security results for SSH do not cover our attacks, and how the attacks can be prevented in practice.",
                "id": "02158"
            },
            {
                "title": "Security Against Related Randomness Attacks via Reconstructive Extractors.",
                "abstract": "This paper revisits related randomness attacks against public key encryption schemes as introduced by Paterson, Schuldt and Sibborn PKC 2014. We present a general transform achieving security for public key encryption in the related randomness setting using as input any secure public key encryption scheme in combination with an auxiliary-input reconstructive extractor. Specifically, we achieve security in the function-vector model introduced by Paterson et al., obtaining the first constructions providing CCA security in this setting. We consider instantiations of our transform using the Goldreich-Levin extractor; these outperform the previous constructions in terms of public-key size and reduction tightness, as well as enjoying CCA security. Finally, we also point out that our approach leads to an elegant construction for Correlation Input Secure hash functions, which have proven to be a versatile tool in diverse areas of cryptography.",
                "id": "02159"
            },
            {
                "title": "Security of Symmetric Encryption against Mass Surveillance.",
                "abstract": "Motivated by revelations concerning population-wide surveillance of encrypted communications, we formalize and investigate the resistance of symmetric encryption schemes to mass surveillance. The focus is on algorithm-substitution attacks (ASAs), where a subverted encryption algorithm replaces the real one. We assume that the goal of \"big brother\" is undetectable subversion, meaning that ciphertexts produced by the subverted encryption algorithm should reveal plaintexts to big brother yet be indistinguishable to users from those produced by the real encryption scheme. We formalize security notions to capture this goal and then offer both attacks and defenses. In the first category we show that successful (from the point of view of big brother) ASAs may be mounted on a large class of common symmetric encryption schemes. In the second category we show how to design symmetric encryption schemes that avoid such attacks and meet our notion of security. The lesson that emerges is the danger of choice: randomized, stateless schemes are subject to attack while deterministic, stateful ones are not.",
                "id": "02160"
            },
            {
                "title": "A Practical Attack Against the Use of RC4 in the HIVE Hidden Volume Encryption System",
                "abstract": "The HIVE hidden volume encryption system was proposed by Blass et al. at ACM-CCS 2014. Even though HIVE has a security proof, this paper demonstrates an attack on its implementation that breaks the main security property claimed for the system by its authors, namely plausible hiding against arbitrary-access adversaries. Our attack is possible because of the HIVE implementation's reliance on the RC4 stream cipher to fill unused blocks with pseudorandom data. While the attack can be easily eliminated by using a better pseudorandom generator, it serves as an example of why RC4 should be avoided in all new applications and a reminder that one has to be careful when instantiating primitives.",
                "id": "02161"
            },
            {
                "title": "Provable Security in the Real World",
                "abstract": "Provable security is sometimes portrayed as having revolutionized cryptography, transforming it from an art into a science. Three decades after its inception, is this transition complete? Are cryptanalysts out of business? If so, why do we still hear about attacks against real-world cryptographic systems?",
                "id": "02162"
            },
            {
                "title": "Big Bias Hunting in Amazonia: Large-Scale Computation and Exploitation of RC4 Biases (Invited Paper).",
                "abstract": "RC4 is (still) a very widely-used stream cipher. Previous work by AlFardan et al. (USENIX Security 2013) and Paterson et al. (FSE 2014) exploited the presence of biases in the RC4 keystreams to mount plaintext recovery attacks against TLS-RC4 and WPA/TKIP. We improve on the latter work by performing large-scale computations to obtain accurate estimates of the single-byte and double-byte distributions in the early portions of RC4 keystreams for the WPA/TKIP context and by then using these distributions in a novel variant of the previous plaintext recovery attacks. The distribution computations were conducted using the Amazon EC2 cloud computing infrastructure and involved the coordination of 2(13) hyper-threaded cores running in parallel over a period of several days. We report on our experiences of computing at this scale using commercial cloud services. We also study Microsoft's Point-to-Point Encryption protocol and its use of RC4, showing that it is also vulnerable to our attack techniques.",
                "id": "02163"
            },
            {
                "title": "Prime and Prejudice: Primality Testing Under Adversarial Conditions.",
                "abstract": "This work provides a systematic analysis of primality testing under adversarial conditions, where the numbers being tested for primality are not generated randomly, but instead provided by a possibly malicious party. Such a situation can arise in secure messaging protocols where a server supplies Diffie-Hellman parameters to the peers, or in a secure communications protocol like TLS where a developer can insert such a number to be able to later passively spy on client-server data. We study a broad range of cryptographic libraries and assess their performance in this adversarial setting. As examples of our findings, we are able to construct 2048-bit composites that are declared prime with probability (1/16) by OpenSSL's primality testing in its default configuration; the advertised performance is (2-80). We can also construct 1024-bit composites that always pass the primality testing routine in GNU GMP when configured with the recommended minimum number of rounds. And, for a number of libraries (Cryptlib, LibTomCrypt, JavaScript Big Number, WolfSSL), we can construct composites that always pass the supplied primality tests. We explore the implications of these security failures in applications, focusing on the construction of malicious Diffie-Hellman parameters. We show that, unless careful primality testing is performed, an adversary can supply parameters (p,q,g) which on the surface look secure, but where the discrete logarithm problem in the subgroup of order q generated by g is easy. We close by making recommendations for users and developers. In particular, we promote the Baillie-PSW primality test which is both efficient and conjectured to be robust even in the adversarial setting for numbers up to a few thousand bits.\n\n",
                "id": "02164"
            },
            {
                "title": "Immunising CBC Mode Against Padding Oracle Attacks: A Formal Security Treatment",
                "abstract": "Padding oracle attacks against CBC mode encryption were introduced by Vaudenay. They are a powerful class of side-channel, plaintext recovering attacks which have been shown to work in practice against CBC mode when it is implemented in specific ways in software. In particular, padding oracle attacks have been demonstrated for certain implementations of SSL/TLS and IPsec. In this paper, we extend the theory of provable security for symmetric encryption to incorporate padding oracle attacks. We develop new security models and proofs for CBC mode (with padding) in the chosen-plaintext setting. These models show how to select padding schemes which provably provide a strong security notion (indistinguishability of encryptions) in the face of padding oracle attacks. We also show that an existing padding method, OZ-PAD, that is recommended for use with CBC mode in ISO/IEC 10116:2006, provably resists Vaudenay's original attack, even though it does not attain our indistinguishability notion.",
                "id": "02165"
            },
            {
                "title": "Zero/Positive Capacities of Two-Dimensional Runlength-Constrained Arrays",
                "abstract": "A binary sequence satisfies a one-dimensional$(d_1, k_1, d_2, k_2)$runlength constraint if every run of zeros has length at least$d_1$and at most$k_1$and every run of ones has length at least$d_2$and at most$k_2$. A two-dimensional binary array is$(d_1, k_1, d_2, k_2; d_3, k_3, d_4, k_4)$-constrained if it satisfies the one-dimensional$(d_1, k_1, d_2, k_2)$runlength constraint horizontally and the one-dimensional$(d_3, k_3, d_4, k_4)$runlength constraint vertically. For given$d_1, k_1, d_2, k_2, d_3, k_3, d_4, k_4$, the two-dimensional capacity is defined as $$displaylines C(d_1, k_1, d_2, k_2; d_3, k_3, d_4, k_4) hfillcr hfill=, lim_m,n rightarrow infty log_2 N(m, n ,vert, d_1, k_1, d_2, k_2; d_3, k_3, d_4, k_4)over mn $$ where $$N(m, n ,vert, d_1, k_1, d_2, k_2; d_3, k_3, d_4, k_4)$$ denotes the number of$m times n$binary arrays that are$(d_1, k_1, d_2, k_2; d_3, k_3, d_4, k_4)$-constrained. Such constrained systems may have applications in digital storage applications. We consider the question for which values of$d_i$and$k_i$is the capacity$C(d_1, k_1, d_2, k_2; d_3, k_3, d_4, k_4)$positive and for which values is the capacity zero. The question is answered for many choices of the$d_i$and the$k_i$.",
                "id": "02166"
            },
            {
                "title": "Certificateless encryption schemes strongly secure in the standard model",
                "abstract": "This paper presents the first constructions for certificateless encryption (CLE) schemes that are provably secure against strong adversaries in the standard model. It includes both a generic construction for a strongly secure CLE scheme from any passively secure scheme as well as a concrete construction based on the Waters identity-based encryption scheme.",
                "id": "02167"
            },
            {
                "title": "On the Security of the TLS Protocol: A Systematic Analysis.",
                "abstract": "TLS is the most widely-used cryptographic protocol on the Internet. It comprises the TLS Handshake Protocol, responsible for authentication and key establishment, and the TLS Record Protocol, which takes care of subsequent use of those keys to protect bulk data. In this paper, we present the most complete analysis to date of the TLS Handshake protocol and its application to data encryption (in the Record Protocol). We show how to extract a key-encapsulation mechanism (KEM) from the TLS Handshake Protocol, and how the security of the entire TLS protocol follows from security properties of this KEM when composed with a secure authenticated encryption scheme in the Record Protocol. The security notion we achieve is a variant of the ACCE notion recently introduced by Jager et al. (Crypto '12). Our approach enables us to analyse multiple different key establishment methods in a modular fashion, including the first proof of the most common deployment mode that is based on RSA PKCS # 1v1.5 encryption, as well as Diffie-Hellman modes. Our results can be applied to settings where mutual authentication is provided and to the more common situation where only server authentication is applied.",
                "id": "02168"
            },
            {
                "title": "On the joint security of encryption and signature, revisited",
                "abstract": "We revisit the topic of joint security for combined public key schemes, wherein a single keypair is used for both encryption and signature primitives in a secure manner. While breaking the principle of key separation, such schemes have attractive properties and are sometimes used in practice. We give a general construction for a combined public key scheme having joint security that uses IBE as a component and that works in the standard model. We provide a more efficient direct construction, also in the standard model.",
                "id": "02169"
            },
            {
                "title": "ASICS: authenticated key exchange security incorporating certification systems",
                "abstract": "Most security models for authenticated key exchange (AKE) do not explicitly model the associated certification system, which includes the certification authority and its behaviour. However, there are several well-known and realistic attacks on AKE protocols which exploit various forms of malicious key registration and which therefore lie outside the scope of these models. We provide the first systematic analysis of AKE security incorporating certification systems. We define a family of security models that, in addition to allowing different sets of standard AKE adversary queries, also permit the adversary to register arbitrary bitstrings as keys. For this model family, we prove generic results that enable the design and verification of protocols that achieve security even if some keys have been produced maliciously. Our approach is applicable to a wide range of models and protocols; as a concrete illustration of its power, we apply it to the CMQV protocol in the natural strengthening of the eCK model to the ASICS setting.",
                "id": "02170"
            },
            {
                "title": "CBE from CL-PKE: a generic construction and efficient schemes",
                "abstract": "We present a new Certificateless Public Key Encryption (CL-PKE) scheme whose security is proven to rest on the hardness of the Bilinear Diffie-Hellman Problem (BDHP) and that is more efficient than the original scheme of Al-Riyami and Paterson. We then give an analysis of Gentry's Certificate Based Encryption (CBE) concept, repairing a number of problems with the original definition and security model for CBE. We provide a generic conversion showing that a secure CBE scheme can be constructed from any secure CL-PKE scheme. We apply this result to our new efficient CL-PKE scheme to obtain a CBE scheme that improves on the original scheme of Gentry.",
                "id": "02171"
            },
            {
                "title": "Cryptanalysis of a Message Authentication Code due to Cary and Venkatesan.",
                "abstract": "A cryptanalysis is given of a MAC proposal presented at CRYPTO 2003 by Cary and Venkatesan. A nice feature of the Cary-Venkatesan MAC is that a lower bound on its security can be proved when a certain block cipher is modelled as an ideal cipher. Our attacks find collisions for the MAC and yield MAC forgeries, both faster than a straightforward application of the birthday paradox would suggest. For the suggested parameter sizes (where the MAC is 128 bits long) we give a method to find collisions using about 2(48.5) MAC queries, and to forge MACs using about 2(55) MAC queries. We emphasise that our results do not contradict the lower bounds on security proved by Cary and Venkatesan. Rather, they establish an upper bound on the MAC's security that is substantially lower than one would expect for a 128-bit MAC.",
                "id": "02172"
            },
            {
                "title": "RKA security beyond the linear barrier: IBE, encryption and signatures",
                "abstract": "We provide a framework enabling the construction of IBE schemes that are secure under related-key attacks (RKAs). Specific instantiations of the framework yield RKA-secure IBE schemes for sets of related key derivation functions that are non-linear, thus overcoming a current barrier in RKA security. In particular, we obtain IBE schemes that are RKA secure for sets consisting of all affine functions and all polynomial functions of bounded degree. Based on this we obtain the first constructions of RKA-secure schemes for the same sets for the following primitives: CCA-secure public-key encryption, CCA-secure symmetric encryption and Signatures. All our results are in the standard model and hold under reasonable hardness assumptions.",
                "id": "02173"
            },
            {
                "title": "Non-interactive designated verifier proofs and undeniable signatures",
                "abstract": "Non-interactive designated verifier (NIDV) proofs were first introduced by Jakobsson et al. and have widely been used as confirmation and denial proofs for undeniable signature schemes. There appears to be no formal security modelling for NIDV undeniable signatures or for NIDV proofs in general. Indeed, recent work by Wang has shown the original NIDV undeniable signature scheme of Jakobsson et al. to be flawed. We argue that NIDV proofs may have applications outside of the context of undeniable signatures and are therefore of independent interest. We therefore present two security models, one for general NIDV proof systems, and one specifically for NIDV undeniable signatures. We go on to repair the NIDV proofs of Jakobsson et al., producing secure NIDV proofs suited to combination with Chaum's original undeniable signature scheme resulting in a secure and efficient concrete NIDV undeniable signature scheme.",
                "id": "02174"
            },
            {
                "title": "Single-track circuit codes",
                "abstract": "Single-track circuit codes (STTCs) are circuit codes with codewords of length n such that all the n tracks which correspond to the n distinct coordinates of the codewords are cyclic shifts of the first track. These codes simultaneously generalize single-track Gray codes and ordinary circuit codes. They are useful in angular quantization applications in which error detecting and/or correcting capabilities are needed. A parameter k, called the spread of the code, measures the strength of this error control capability. We consider the existence of STCCs for small lengths n&les;17 and spreads k&les;6, constructing some optimal and many good examples. We then give a general construction method for STCCs which makes use of ordinary circuit codes. We use this construction to construct examples of codes with 360 and 1000 codewords which are of practical importance. We also use the construction to prove a general result on the existence of STCCs for general spreads",
                "id": "02175"
            },
            {
                "title": "Secret Public Key Protocols Revisited",
                "abstract": "Password-based protocols are an important and popular means of providing human-to-machine authentication. The concept of secret public keys was proposed more than a decade ago as a means of securing password-based authentication protocols against o-line password guess- ing attacks, but was later found vulnerable to various attacks. In this paper, we revisit the concept and introduce the notion of identity-based secret public keys. Our new identity-based approach allows secret pub- lic keys to be constructed in a very natural way using arbitrary random strings, eliminating the structure found in, for example, RSA or ElGamal keys. We examine identity-based secret public key protocols and give in- formal security analyses, indicating that they are secure against o-line password guessing and other attacks.",
                "id": "02176"
            },
            {
                "title": "Sequences for OFDM and Multi-Code CDMA: Two Problems in Algebraic Coding Theory",
                "abstract": "We study the peak-to-average power ratio (PAPR) problem for two different kinds of communications systems, Orthogonal Frequency Division Multiplexing (OFDM) and Multi-Code Code-Division Multiple Access (MC-CDMA). We describe a common coding theoretic approach to reducing the PAPR of both kinds of transmissions. In both cases, the classical Reed-Muller codes turn out to play a critical role. There is an intimate connection between Reed-Muller codes and Golay complementary sequences which can be exploited to produce codes suitable for OFDM. For MC-CDMA, it turns out that bent functions lead to transmissions with ideal power characteristics. In this way, the problem of finding good codes for OFDM and MC-CDMA can be closely related to some old and new problems in algebraic coding theory and sequence design.",
                "id": "02177"
            },
            {
                "title": "Four Attacks and a Proof for Telegram",
                "abstract": "We study the use of symmetric cryptography in the MTProto 2.0 protocol, Telegram\u2019s equivalent of the TLS record protocol. We give positive and negative results. On the one hand, we formally and in detail model a slight variant of Telegram\u2019s \u201crecord protocol\u201d and prove that it achieves security in a suitable bidirectional secure channel model, albeit under unstudied assumptions; this model itself advances the state-of-the-art for secure channels. On the other hand, we first motivate our modelling deviation from MTProto as deployed by giving two attacks \u2013 one of practical, one of theoretical interest \u2013 against MTProto without our modifications. We then also give a third attack exploiting timing side channels, of varying strength, in three official Telegram clients. On its own this attack is thwarted by the secrecy of salt and id fields that are established by Telegram\u2019s key exchange protocol. To recover these, we chain the third attack with a fourth one against the implementation of the key exchange protocol on Telegram\u2019s servers. In totality, our results provide the first comprehensive study of MTProto\u2019s use of symmetric cryptography.",
                "id": "02178"
            },
            {
                "title": "Perfect maps",
                "abstract": "Given positive integers r, s, u, and &upsi;, an (r, s; u, &upsi;) perfect map (PM) is defined to be a periodic r\u00d7s binary array in which every u\u00d7&upsi; binary array appears exactly once as a periodic subarray. Perfect maps are the natural extension of the de Bruijn sequences to two dimensions. In the paper the existence question for perfect maps is settled by giving constructions for perfect maps for all parameter sets subject to certain simple necessary conditions. Extensive use is made of previously known constructions by finding new conditions which guarantee their repeated application. These conditions are expressed as bounds on the linear complexities of the periodic sequences formed from the rows and columns of perfect maps",
                "id": "02179"
            },
            {
                "title": "Lost in Translation: Theory and Practice in Cryptography",
                "abstract": "When two cultures clash, an air passenger may be wished a good \"fright,\" or one may end up regretting his karaoke performance after a sake too many. For theoreticians and practioners in information security, however, their lack of mutual understanding may lead to more serious consequences.",
                "id": "02180"
            },
            {
                "title": "Signal-flow-based analysis of wireless security protocols.",
                "abstract": "Security protocols operating over wireless channels can incur significant communication costs (e.g., energy, delay), especially under adversarial attacks unique to the wireless environment such as signal jamming, fake signal transmission, etc. Since wireless devices are resource constrained, it is important to optimize security protocols for wireless environments by taking into account their communication costs. Towards this goal, we first present a novel application of a signal-flow-based approach to analyze the communication costs of security protocols in the presence of adversaries. Our approach models a protocol run as a dynamic probabilistic system and then utilizes Linear System theory to evaluate the moment generating function of the end-to-end cost. Applying this technique to the problem of secret key exchange over a wireless channel, we quantify the efficiency of existing families of key exchange cryptographic protocols, showing, for example, that an ID-based approach can offer an almost 10-fold improvement in energy consumption when compared to a traditional PKI-based protocol. We then present a new key exchange protocol that combines traditional cryptographic methods with physical-layer techniques, including the use of \u201cephemeral\u201d spreading codes, cooperative jamming, and role-switching. Utilizing signal flow analysis, we demonstrate that this new protocol offers performance advantages over traditional designs.",
                "id": "02181"
            },
            {
                "title": "A Surfeit of SSH Cipher Suites.",
                "abstract": "This work presents a systematic analysis of symmetric encryption modes for SSH that are in use on the Internet, providing deployment statistics, new attacks, and security proofs for widely used modes. We report deployment statistics based on two Internet-wide scans of SSH servers conducted in late 2015 and early 2016. Dropbear and OpenSSH implementations dominate in our scans. From our first scan, we found 130,980 OpenSSH servers that are still vulnerable to the CBC-mode-specific attack of Albrecht et al. (IEEE S&P 2009), while we found a further 20,000 OpenSSH servers that are vulnerable to a new attack on CBC-mode that bypasses the counter-measures introduced in OpenSSH 5.2 to defeat the attack of Albrecht et al. At the same time, 886,449 Dropbear servers in our first scan are vulnerable to a variant of the original CBC-mode attack. On the positive side, we provide formal security analyses for other popular SSH encryption modes, namely ChaCha20-Poly1305, generic Encrypt-then-MAC, and AES-GCM. Our proofs hold for detailed pseudo-code descriptions of these algorithms as implemented in OpenSSH. Our proofs use a corrected and extended version of the \\\"fragmented decryption\\\" security model that was specifically developed for the SSH setting by Boldyreva et al. (Eurocrypt 2012). These proofs provide strong confidentiality and integrity guarantees for these alternatives to CBC-mode encryption in SSH. However, we also show that these alternatives do not meet additional, desirable notions of security (boundary-hiding under passive and active attacks, and denial-of-service resistance) that were formalised by Boldyreva et al.",
                "id": "02182"
            },
            {
                "title": "On Symmetric Encryption with Distinguishable Decryption Failures.",
                "abstract": "We propose to relax the assumption that decryption failures are indistinguishable in security models for symmetric encryption. Our main purpose is to build models that better reflect the reality of cryptographic implementations, and to surface the security issues that arise from doing so. We systematically explore the consequences of this relaxation, with some surprising consequences for our understanding of this basic cryptographic primitive. Our results should be useful to practitioners who wish to build accurate models of their implementations and then analyse them. They should also be of value to more theoretical cryptographers proposing new encryption schemes, who, in an ideal world, would be compelled by this work to consider the possibility that their schemes might leak more than simple decryption failures.",
                "id": "02183"
            },
            {
                "title": "Pump up the Volume: Practical Database Reconstruction from Volume Leakage on Range Queries.",
                "abstract": "We present attacks that use only the volume of responses to range queries to reconstruct databases. Our focus is on practical attacks that work for large-scale databases with many values and records, without requiring assumptions on the data or query distributions. Our work improves on the previous state-of-the-art due to Kellaris et al. (CCS 2016) in all of these dimensions. Our main attack targets reconstruction of database counts and involves a novel graph-theoretic approach. It generally succeeds when R , the number of records, exceeds $N^2/2$, where N is the number of possible values in the database. For a uniform query distribution, we show that it requires volume leakage from only O(N2 \u0142og N) queries (cf. O(N4\u0142og N) in prior work). We present two ancillary attacks. The first identifies the value of a new item added to a database using the volume leakage from fresh queries, in the setting where the adversary knows or has previously recovered the database counts. The second shows how to efficiently recover the ranges involved in queries in an online fashion, given an auxiliary distribution describing the database. Our attacks are all backed with mathematical analyses and extensive simulations using real data.\n\n",
                "id": "02184"
            },
            {
                "title": "On the (in)security of IPsec in MAC-then-encrypt configurations",
                "abstract": "IPsec allows a huge amount of flexibility in the ways in which its component cryptographic mechanisms can be combined to build a secure communications service. This may be good for supporting different security requirements but is potentially bad for security. We demonstrate the reality of this by describing efficient, plaintext-recovering attacks against all configurations of IPsec in which integrity protection is applied {\\em prior} to encryption -- so-called MAC-then-encrypt configurations. We report on the implementation of our attacks against a specific IPsec implementation, and reflect on the implications of our attacks for real-world IPsec deployments as well as for theoretical cryptography.",
                "id": "02185"
            },
            {
                "title": "Related-Key Security for Pseudorandom Functions Beyond the Linear Barrier.",
                "abstract": "Related-key attacks (RKAs) concern the security of cryptographic primitives in the situation where the key can be manipulated by the adversary. In the RKA setting, the adversary\u2019s power is expressed through the class of related-key deriving (\\(\\mathrm {RKD}\\)) functions which the adversary is restricted to using when modifying keys. Bellare and Kohno (EUROCRYPT 2003, volume 2656 of LNCS, Springer, Heidelberg, pp 491\u2013506,\u00a02003) first formalized RKAs and pinpointed the foundational problem of constructing RKA-secure pseudorandom functions (RKA-PRFs). To date there are few constructions for RKA-PRFs under standard assumptions, and it is a major open problem to construct RKA-PRFs for larger classes of \\(\\mathrm {RKD}\\) functions. We make significant progress on this problem. We first show how to repair the framework for constructing RKA-PRF by Bellare and Cash (CRYPTO 2010, volume 6223 of LNCS, Springer, Heidelberg, pp 666\u2013684,\u00a02010) and extend it to handle the more challenging case of classes of \\(\\mathrm {RKD}\\) functions that contain claws. We apply this extension to show that a variant of the Naor\u2013Reingold function already considered by Bellare and Cash is an RKA-PRF for a class of affine \\(\\mathrm {RKD}\\) functions under the Decisional Diffie\u2013Hellman (DDH) assumption, albeit with a blowup that is exponential in the PRF input size. We then develop a second extension of the Bellare\u2013Cash framework and use it to show that the same Naor\u2013Reingold variant is actually an RKA-PRF for a class of degree d polynomial \\(\\mathrm {RKD}\\) functions under the stronger decisional d-Diffie\u2013Hellman inversion assumption. As a significant technical contribution, our proof of this result avoids the exponential-time security reduction that was inherent in the work of Bellare and Cash and in our first result. In particular, by setting \\(d = 1\\) (affine functions), we obtain a construction of RKA-secure PRF for affine relation based on the polynomial hardness of DDH.",
                "id": "02186"
            },
            {
                "title": "Efficient identity-based signatures secure in the standard model",
                "abstract": "The only known construction of identity-based signatures that can be proven secure in the standard model is based on the approach of attaching certificates to non-identity-based signatures. This folklore construction method leads to schemes that are somewhat inefficient and leaves open the problem of finding more efficient direct constructions. We present the first such construction. Our scheme is obtained from a modification of Waters' recently proposed identity-based encryption scheme. It is computationally efficient and the signatures are short. The scheme's security is proven in the standard model and rests on the hardness of the computational Diffie-Hellman problem in groups equipped with a pairing.",
                "id": "02187"
            },
            {
                "title": "Key Rotation for Authenticated Encryption.",
                "abstract": "A common requirement in practice is to periodically rotate the keys used to encrypt stored data. Systems used by Amazon and Google do so using a hybrid encryption technique which is eminently practical but has questionable security in the face of key compromises and does not provide full key rotation. Meanwhile, symmetric updatable encryption schemes (introduced by Boneh et al. CRYPTO 2013) support full key rotation without performing decryption: ciphertexts created under one key can be rotated to ciphertexts created under a different key with the help of a re-encryption token. By design, the tokens do not leak information about keys or plaintexts and so can be given to storage providers without compromising security. But the prior work of Boneh et al. addresses relatively weak confidentiality goals and does not consider integrity at all. Moreover, as we show, a subtle issue with their concrete scheme obviates a security proof even for confidentiality against passive attacks. This paper presents a systematic study of updatable Authenticated Encryption (AE). We provide a set of security notions that strengthen those in prior work. These notions enable us to tease out real-world security requirements of different strengths and build schemes that satisfy them efficiently. We show that the hybrid approach currently used in industry achieves relatively weak forms of confidentiality and integrity, but can be modified at low cost to meet our stronger confidentiality and integrity goals. This leads to a practical scheme that has negligible overhead beyond conventional AE. We then introduce re-encryption indistinguishability, a security notion that formally captures the idea of fully refreshing keys upon rotation. We show how to repair the scheme of Boneh et al., attaining our stronger confidentiality notion. We also show how to extend the scheme to provide integrity, and we prove that it meets our re-encryption indistinguishability notion. Finally, we discuss how to instantiate our scheme efficiently using off-the-shelf cryptographic components (AE, hashing, elliptic curves). We report on the performance of a prototype implementation, showing that fully secure key rotations can be performed at a throughput of approximately 116 kB/s.",
                "id": "02188"
            },
            {
                "title": "Proxy signatures secure against proxy key exposure",
                "abstract": "We provide an enhanced security model for proxy signatures that captures a more realistic set of attacks than previous models of Boldyreva et al. and of Malkin et al.. Our model is motivated by concrete attacks on existing schemes in scenarios in which proxy signatures are likely to be used. We provide a generic construction for proxy signatures secure in our enhanced model using sequential aggregate signatures; our construction provides a benchmark by which future specific constructions may be judged. Finally, we consider the extension of our model and constructions to the identity-based setting.",
                "id": "02189"
            },
            {
                "title": "A coding-theoretic approach to recovering noisy RSA keys",
                "abstract": "Inspired by cold boot attacks, Heninger and Shacham (Crypto 2009) initiated the study of the problem of how to recover an RSA private key from a noisy version of that key. They gave an algorithm for the case where some bits of the private key are known with certainty. Their ideas were extended by Henecka, May and Meurer (Crypto 2010) to produce an algorithm that works when all the key bits are subject to error. In this paper, we bring a coding-theoretic viewpoint to bear on the problem of noisy RSA key recovery. This viewpoint allows us to cast the previous work as part of a more general framework. In turn, this enables us to explain why the previous algorithms do not solve the motivating cold boot problem, and to design a new algorithm that does (and more). In addition, we are able to use concepts and tools from coding theory --- channel capacity, list decoding algorithms, and random coding techniques --- to derive bounds on the performance of the previous and our new algorithm.",
                "id": "02190"
            },
            {
                "title": "Security and Anonymity of Identity-Based Encryption with Multiple Trusted Authorities",
                "abstract": "We consider the security of Identity-Based Encryption (IBE) in the setting of multiple Trusted Authorities (TAs). In this multi-TA setting, we envisage multiple TAs sharing some common parameters, but each TA generating its own master secrets and master public keys. We provide security notions and security models for the multi-TA setting which can be seen as natural extensions of existing notions and models for the single-TA setting. In addition, we study the concept of TA anonymity, which formally models the inability of an adversary to distinguish two ciphertexts corresponding to the same message and identity but generated using different TA master public keys. We argue that this anonymity property is a natural one of importance in enhancing privacy and limiting traffic analysis in multi-TA environments. We study a modified version of a Fujisaki-Okamoto conversion in the multi-TA setting, proving that our modification lifts security and anonymity properties from the CPA to the CCA setting. Finally, we apply these results to study the security of the Boneh-Franklin and Sakai-Kasahara IBE schemes in the multi-TA setting.",
                "id": "02191"
            },
            {
                "title": "Near optimal single-track Gray codes",
                "abstract": "Single-track Gray codes are a special class of Gray codes which have advantages over conventional Gray codes in certain quantization and coding applications. The problem of constructing high period single-track Gray codes is considered. Three iterative constructions are given, along with a heuristic method for obtaining good seed-codes. In combination, these yield many families of very high period single-track Gray codes. In particular, for m&ges;3, length n=2m, period 2 n-2n codes are obtained",
                "id": "02192"
            },
            {
                "title": "Certificateless Public Key Cryptography.",
                "abstract": "This paper introduces and makes concrete the concept of certificateless public key cryptography (CL-PKC), a model for the use of public key cryptography which avoids the inherent escrow of identity-based cryptography and yet which does not require certificates to guarantee the authenticity of public keys. The lack of certificates and the presence of an adversary who has access to a master key necessitates the careful development of a new security model. We focus on certificateless public key encryption (CL-PKE), showing that a concrete pairing-based CL-PKE scheme is secure provided that an underlying problem closely related to the Bilinear Diffie-Hellman Problem is hard.",
                "id": "02193"
            }
        ]
    },
    {
        "id": "022",
        "input": "For an author who has written the paper with the title \"GP-Fileprints: file types detection using genetic programming\", which reference is related? Just answer with [1] or [2] without explanation. [1]: \"Evolving a statistics class using object oriented evolutionary programming\" [2]: \"Exact and Approximation Algorithms for Sorting by Reversals, with Application to Genome Rearrangement\"",
        "profile": [
            {
                "title": "Automated Game Balancing in Ms PacMan and StarCraft Using Evolutionary Algorithms.",
                "abstract": "Games, particularly online games, have an ongoing requirement to exhibit the ability to react to player behaviour and change their mechanics and available tools to keep their audience both entertained and feeling that their strategic choices and in-game decisions have value. Game designers invest time both gathering data and analysing it to introduce minor changes that bring their game closer to a state of balance, a task with a lot of potential that has recently come to the attention of researchers. This paper first provides a method for automating the process of finding the best game parameters to reduce the difficulty of Ms PacMan through the use of evolutionary algorithms and then applies the same method to a much more complex and commercially successful PC game, StarCraft, to curb the prowess of a dominant strategy. Results show both significant promise and several avenues for future improvement that may lead to a useful balancing tool for the games industry.",
                "id": "0220"
            },
            {
                "title": "Memory with memory: soft assignment in genetic programming",
                "abstract": "Based in part on observations about the incremental nature of most state changes in biological systems, we introduce the idea of Memory with Memory in Genetic Programming (GP), where we use \"soft\" assignments to registers instead of the \"hard\" assignments used in most computer science (including traditional GP). Instead of having the new value completely overwrite the old value of the register, these soft assignments combine the old and new values. We then report on extensive empirical tests (a total of 12,800 runs) on symbolic regression problems where Memory with Memory GP almost always does as well as traditional GP, while significantly outperforming it in several cases. Memory with Memory GP also tends to be far more consistent, having much less variation in its best-of-run fitnesses than traditional GP. The data suggest that Memory with Memory GP works by successively refining an approximate solution to the target problem. This means it can continue to improve (if slowly) over time, but that it is less likely to get the sort of exact solution that one might find with traditional GP. The use of soft assignment also means that Memory with Memory GP is much less likely to have truly ineffective code, but the action of successive refinement of approximations means that the average program size is often larger than with traditional GP.",
                "id": "0221"
            },
            {
                "title": "Unsupervised problem decomposition using genetic programming",
                "abstract": "We propose a new framework based on Genetic Programming (GP) to automatically decompose problems into smaller and simpler tasks. The framework uses GP at two levels. At the top level GP evolves ways of splitting the fitness cases into subsets. At the lower level GP evolves programs that solve the fitness cases in each subset. The top level GP programs include two components. Each component receives a training case as the input. The components' outputs act as coordinates to project training examples onto a 2-D Euclidean space. When an individual is evaluated, K-means clustering is applied to group the fitness cases of the problem. The number of clusters is decided based on the density of the projected samples. Each cluster then invokes an independent GP run to solve its member fitness cases. The fitness of the lower level GP individuals is evaluated as usual. The fitness of the high-level GP individuals is a combination of the fitness of the best evolved programs in each of the lower level GP runs. The proposed framework has been tested on several symbolic regression problems and has been seen to significantly outperforming standard GP systems.",
                "id": "0222"
            },
            {
                "title": "Fitness Causes Bloat: Mutation",
                "abstract": "Abstract. The problem of evolving, using mutation, an artificial ant to follow the Santa Fe trail is used to study the well known genetic program-ming feature of growth in solution length. Known variously as \"bloat\", \"flu \" and increasing \"structural complexity\", this is often described in terms of increasing \"redundancy\" in the code caused by \"introns\". Comparison between runs with and without fitness selection pressure, backed by Price's Theorem, shows the tendency for solutions to grow in size is caused by fitness based selection. We argue that such growth is inherent in using a fixed evaluation function with a discrete but variable length representation. With simple static evaluation search converges to mainly finding trial solutions with the same fitness as existing trial solu-tions. In general variable length allows many more long representations of a given solution than short ones. Thus in search (without a length bias) we expect longer representations to occur more often and so repre-sentation length to tend to increase. I. e. fitness based selection leads to bloat. ",
                "id": "0223"
            },
            {
                "title": "Recursive Conditional Schema Theorem, Convergence and Population Sizing in Genetic Algorithms",
                "abstract": " In this paper we present two forms of schema theorem in which expectations are not present. These theorems allow one to predict with a known probability whether the number of instances of a schema at the next generation will be above a given threshold. Then we clarify that in the presence of stochasticity schema theorems should be interpreted as conditional statements and we use a conditional version of schema theorem backwards to predict the past from the future. Assuming that at least x... ",
                "id": "0224"
            },
            {
                "title": "Evolving Problems To Learn About Particle Swarm And Other Optimisers",
                "abstract": "We use evolutionary computation (EC) to automatically find problems which demonstrate the strength and weaknesses of modern search heuristics. In particular we analyse Particle Swarm Optimization (PSO) and Differential Evolution (DE). Both evolutionary algorithms are contrasted with a robust deterministic gradient based searcher (based on Newton-Raphson). The fitness landscapes made by genetic programming (GP) are used to illustrate difficulties in GAs and PSOs thereby explaining how they work and allowing us to devise better extended particle swarm systems (XPS).",
                "id": "0225"
            },
            {
                "title": "Finding Social Landscapes For Psos Via Kernels",
                "abstract": "Particle swarm optimiser and genetic algorithm populations are macro-organisms, which perceive their environment as if filtered via a kernel. The kernel assimilates each individual's sensory abilities so that the collective moves using a greedy hill-climbing strategy. This model is fitted to data collected in real PSO and GA runs by using genetic programming to evolve the kernel.In nature animals tend to live within groups. The social interactions effectively transform the fitness selection landscape seen by an isolated individual. In some cases a group behaves (or even can be said to think) like a single organism. Kernels provide a lens which coarse-grains or averages individual senses and so may help explain joint actions and social responses.The original multi-modal problem is smoothed by convolving it with a problem specific filter designed by GP. Because populations see the transformed social fitness landscape, they can pass over local optima. GP can give a good fit between the predicted behaviour of the macroscopic organism and the actual runs.",
                "id": "0226"
            },
            {
                "title": "Sub-machine-code GP: New Results and Extensions",
                "abstract": "Sub-machine-code GP (SMCGP) is a technique to speed up genetic programming (GP) and to extend its scope based on the idea of exploiting the internal parallelism of sequential CPUs. In previous work [20] we have shown examples of applications of this technique to the evolution of parallel programs and to the parallel evaluation of 32 or 64 fitness cases per program execution in Boolean classification problems. After recalling the basic features of SMCGP, in this paper we first apply this technique to the problem of evolving parallel binary multipliers.s.Then we describe how SMCGP can be extended to process multiple fitness cases per program execution in continuous symbolic regression problems where inputs and outputs are real-valued numbers, reporting experimental results on a quartic polynomial approximation task.",
                "id": "0227"
            },
            {
                "title": "Models of Performance of Evolutionary Program Induction Algorithms Based on Indicators of Problem Difficulty",
                "abstract": "Modeling the behavior of algorithms is the realm of evolutionary algorithm theory. From a practitioner's point of view, theory must provide some guidelines regarding which algorithm/parameters to use in order to solve a particular problem. Unfortunately, most theoretical models of evolutionary algorithms are difficult to apply to realistic situations. However, in recent work (Graff and Poli, 2008, 2010), where we developed a method to practically estimate the performance of evolutionary program-induction algorithms (EPAs), we started addressing this issue. The method was quite general; however, it suffered from some limitations: it required the identification of a set of reference problems, it required hand picking a distance measure in each particular domain, and the resulting models were opaque, typically being linear combinations of 100 features or more. In this paper, we propose a significant improvement of this technique that overcomes the three limitations of our previous method. We achieve this through the use of a novel set of features for assessing problem difficulty for EPAs which are very general, essentially based on the notion of finite difference. To show the capabilities or our technique and to compare it with our previous performance models, we create models for the same two important classes of problems\u2014symbolic regression on rational functions and Boolean function induction\u2014used in our previous work. We model a variety of EPAs. The comparison showed that for the majority of the algorithms and problem classes, the new method produced much simpler and more accurate models than before. To further illustrate the practicality of the technique and its generality (beyond EPAs), we have also used it to predict the performance of both autoregressive models and EPAs on the problem of wind speed forecasting, obtaining simpler and more accurate models that outperform - n all cases our previous performance models.",
                "id": "0228"
            },
            {
                "title": "Exact Schema Theory and Markov Chain Models for Genetic Programming and Variable-length Genetic Algorithms with Homologous Crossover",
                "abstract": "Genetic Programming (GP) homologous crossovers are a group of operators, including GP one-point crossover and GP uniform crossover, where the offspring are created preserving the position of the genetic material taken from the parents. In this paper we present an exact schema theory for GP and variable-length Genetic Algorithms (GAs) which is applicable to this class of operators. The theory is based on the concepts of GP crossover masks and GP recombination distributions that are generalisations of the corresponding notions used in GA theory and in population genetics, as well as the notions of hyperschema and node reference systems, which are specifically required when dealing with variable size representations.In this paper we also present a Markov chain model for GP and variable-length GAs with homologous crossover. We obtain this result by using the core of Vose's model for GAs in conjunction with the GP schema theory just described. The model is then specialised for the case of GP operating on 0/1 trees: a tree-like generalisation of the concept of binary string. For these, symmetries exist that can be exploited to obtain further simplifications.In the absence of mutation, the Markov chain model presented here generalises Vose's GA model to GP and variable-length GAs. Likewise, our schema theory generalises and refines a variety of previous results in GP and GA theory.",
                "id": "0229"
            },
            {
                "title": "On the moments of the sampling distribution of particle swarm optimisers",
                "abstract": "A method is presented that allows one to exactly determine all the characteristics of a PSO's sampling distribution and explain how it changes over time, in the presence stochasticity. The only assumption made is stagnation (particles are in search for a better personal best).",
                "id": "02210"
            },
            {
                "title": "Understanding the biases of generalised recombination: part II.",
                "abstract": "This is the second part of a two-part paper where we propose, model theoretically and study a general notion of recombination for fixed-length strings where homologous recombination, inversion, gene duplication, gene deletion, diploidy and more are just special cases. In Part I, we derived both microscopic and coarse-grained evolution equations for strings and schemata for a selecto-recombinative GA using generalised recombination, and we explained the hierarchical nature of the schema evolution equations. In this part, we provide a variety of fixed points for evolution in the case where recombination is used alone, thereby generalising Geiringer's theorem. In addition, we numerically integrate the infinite-population schema equations for some interesting problems, where selection and recombination are used together to illustrate how these operators interact. Finally, to assess by how much genetic drift can make a system deviate from the infinite-population-model predictions we discuss the results of real GA runs for the same model problems with generalised recombination, selection and finite populations of different sizes.",
                "id": "02211"
            },
            {
                "title": "Evolutionary lossless compression with GP-ZIP",
                "abstract": "In recent research we proposed GP-zip, a system which uses evolution to find optimal ways to combine standard compression algorithms for the purpose of maximally losslessly compressing files and archives. The system divides files into blocks of predefined length. It then uses a linear, fixed-length representation where each primitive indicates what compression algorithm to use for a specific data block. GP-zip worked well with heterogonous data sets, providing significant improvements in compression ratio compared to some of the best standard compression algorithms. In this paper we propose a substantial improvement, called GP-zip*, which uses a new representation and intelligent crossover and mutation operators such that blocks of different sizes can be evolved. Like GP-zip, GP-zip* finds what the best compression technique to use for each block is. The compression algorithms available in the primitive set of GP-zip* are: Arithmetic coding (AC), Lempel-Ziv-Welch (LZW), Unbounded Prediction by Partial Matching (PPMD), Run Length Encoding (RLE), and Boolean Minimization. In addition, two transformation techniques are available: the Burrows-Wheeler Transformation (BWT) and Move to Front (MTF). Results show that GP-zip* provides improvements in compression ratio ranging from a fraction to several tens of percent over its predecessor.",
                "id": "02212"
            },
            {
                "title": "Geometric particle swarm optimisation on binary and real spaces: from theory to practice",
                "abstract": "Geometric particle swarm optimization (GPSO) is a recently introduced formal generalization of traditional particle swarm optimization (PSO) that applies naturally to both continuous and combinatorial spaces. In previous work we have developed the theory behind it. The aim of this paper is to demonstrate the applicability of GPSO in practice. We demonstrate this for the cases of Euclidean, Manhattan and Hamming spaces and report extensive experimental results.",
                "id": "02213"
            },
            {
                "title": "Genetic programming theory",
                "abstract": "Genetic Programming (GP) is a complex adaptive system with an immens number of degrees of freedom. Understanding how, why and when it work is difficult. Its behaviour is typically investigated in two ways experimentally and theoretically. Experimental studies require the experimenter to choose which problems, parameter settings and descriptors to use. Plotting the wrong data increases the confusion about GP's behaviour, rather that clarify it. A more powerful alternative is to study GP theoretically. In this tutorial I will look at GP as a search process and explain its behaviour by considering the GP search space, in terms of its size in its limiting fitness distributions and also the halting probability. I will then use modern schema theory to characterise GP search. Finally, I will be in a position to explain the reasons for spurious phenomena (such as bloat) in GP and I will look into theoretically-sound ways of curing them. Some prior knowledge of GP will be assumed",
                "id": "02214"
            },
            {
                "title": "Dual Network Representation Applied to the Evolution of Neural Controllers",
                "abstract": "This paper presents a new approach to the evolution of neural networks. A linear chromosome combined with a grid-based representation of the network and a new crossover operator allow the evolution of the architecture and the weights simultaneously. There is no need for a separate weight optimization procedure and networks with more than one type of activation function can be evolved. This paper describes the representation, the crossover operator, and reports on results of the application of the method to evolve a neural controller for the pole-balancing problem.",
                "id": "02215"
            },
            {
                "title": "A simple but theoretically-motivated method to control bloat in genetic programming",
                "abstract": "This paper presents a simple method to control bloat which is based on the idea of strategically and dynamically creating fitness \"holes\" in the fitness landscape which repel the population. In particular we create holes by zeroing the fitness of a certain proportion of the offspring that have above average length. Unlike other methods where all individuals are penalised when length constraints are violated, here we randomly penalise only a fixed proportion of all the constraint-violating offspring. The paper describes the theoretical foundation for this method and reports the results of its empirical validation with two relatively hard test problems, which has confirmed the effectiveness of the approach.",
                "id": "02216"
            },
            {
                "title": "Evolution of a brain-computer interface mouse via genetic programming",
                "abstract": "We propose the use of genetic programming as a means to evolve brain-computer interfaces for mouse control. Our objective is to synthesise complete systems, which analyse electroencephalographic signals and directly transform them into pointer movements, almost from scratch, the only input provided by us in the process being the set of visual stimuli to be used to generate recognisable brain activity. Experimental results with our GP approach are very promising and compare favourably with those produced by support vector machines.",
                "id": "02217"
            },
            {
                "title": "Exact Results From A Coarse Grained Formulation Of The Dynamics Of Variable-length Genetic Algorithms",
                "abstract": "We consider the dynamics of variable-length Genetic Algorithms (GAs) with strings of length using a recently developed exact, coarse-grained for- mulation where the relevant coarse-grained degrees of freedom are \"building block\" schemata. We derive an exact formal solution of the equations showing how a hierarchical structure in time and degree of coarse- graining emerges, the effect of recombination being to successively form more fine-grained objects from their more coarse-grained building blocks, where in this case the building blocks can come from strings of different lengths. We examine the limit distributions of the dynamics in the case of a flat fitness landscape, one-point homologous crossover and no mutation. By taking advantage of the existence of a set of conserved quantities in the dynamics we provide exact solutions for the cases and use these to investigate the phenomenon of inter-length-class allele diffusion. We also study the general case showing what exact re- sults may be easily derived using our particular coarse- grained formulation.",
                "id": "02218"
            },
            {
                "title": "Geometric crossover for sets, multisets and partitions",
                "abstract": "This paper extends a geometric framework for interpreting crossover and mutation[5] to the case of sets and related representations. We show that a deep geometric duality exists between the set representation and the vector representation. This duality reveals the equivalence of geometric crossovers for these representations.",
                "id": "02219"
            },
            {
                "title": "Theoretical Analysis Of Generalised Recombination",
                "abstract": "In this paper we propose, model theoretically and study a general notion of recombination for fixed-length strings where homologous crossover, inversion, gene duplication, gene deletion, diploidy and more are just special cases. The analysis of the model reveals similarities and differences between genetic systems based on these operations. It also reveals that the notion of schema emerges naturally from the model's equations even for the strangest of recombination operations. The study provides a variety of fixed points for the case where recombination is used alone, which generalise Geiringer's manifold.",
                "id": "02220"
            },
            {
                "title": "A neural network expert system for diagnosing and treating hypertension",
                "abstract": "Hypernet (Hypertension Neural Expert Therapist), a neural network expert system for diagnosing and treating hypertension, is described. After a brief look at artificial neural networks, the authors describe the structure of the three modules that make up Hypernet, starting with the specific problem each network is intended to solve and explaining how the network is expected to operate. The tools developed for implementing the system are a compiler for a simple descriptive language that enables the authors to define, train, and test networks; a graphic editor that translates the network drawn by the user into the proper statements; and a set of programs for interactively generating the examples. The data used as examples, the learning phase, and the results of tests for evaluating the performance of each network are described. and conclusions about overall system results are presented.<>",
                "id": "02221"
            },
            {
                "title": "A Model for Analysing the Collective Dynamic Behaviour and Characterising the Exploitation of Population-Based Algorithms",
                "abstract": "Several previous studies have focused on modelling and analysing the collective dynamic behaviour of population-based algorithms. However, an empirical approach for identifying and characterising such a behaviour is surprisingly lacking. In this paper, we present a new model to capture this collective behaviour, and to extract and quantify features associated with it. The proposed model studies the topological distribution of an algorithm's activity from both a genotypic and a phenotypic perspective, and represents population dynamics using multiple levels of abstraction. The model can have different instantiations. Here it has been implemented using a modified version of self-organising maps. These are used to represent and track the population motion in the fitness landscape as the algorithm operates on solving a problem. Based on this model, we developed a set of features that characterise the population's collective dynamic behaviour. By analysing them and revealing their dependency on fitness distributions, we were then able to define an indicator of the exploitation behaviour of an algorithm. This is an entropy-based measure that assesses the dependency on fitness distributions of different features of population dynamics. To test the proposed measures, evolutionary algorithms with different crossover operators, selection pressure levels and population handling techniques have been examined, which lead populations to exhibit a wide range of exploitation-exploration behaviours.",
                "id": "02222"
            },
            {
                "title": "Allele Diffusion in Linear Genetic Programming and Variable-Length Genetic Algorithms with Subtree Crossover",
                "abstract": "In this paper we study, theoretically, the search biases produced by GP subtree crossover when applied to linear representations, such as those used in linear GP or in variable length GAs. The study naturally leads to generalisations of Geiringer's theorem and of the notion of linkage equilibrium, which, until now, were applicable only to fixed-length representations. This indicates the presence of a diffusion process by which, even in the absence of selective pressure and mutation, the alleles in a particular individual tend not just to be swapped with those of other individuals in the population, but also to diffuse within the representation of each individual. More precisely, crossover attempts to push the population towards distributions of primitives where each primitive is equally likely to be found in any position in any individual.",
                "id": "02223"
            },
            {
                "title": "Smooth Uniform Crossover with Smooth Point Mutation in Genetic Programming: A Preliminary Study",
                "abstract": "In this paper we examine the behaviour of the uniform crossover and point mutation GP operators [12] on the even-n-parity problem for n = 3,4,6 and present a novel representation of function nodes, designed to allow the search operators to make smaller movements around the solution space. Using this representation, performance on the even-6-parity problem is improved by three orders of magnitude relative to the estimate given for standard GP in [5].",
                "id": "02224"
            },
            {
                "title": "Boolean Functions Fitness Spaces",
                "abstract": "We investigate the distribution of performance of the Boolean functions of 3 Boolean inputs (particularly that of the parity functions), the always-on-6 and even-6 parity functions. We use enumeration, uniform Monte-Carlo random sampling and sampling random full trees. As expected XOR dramatically changes the fitness distributions. In all cases once some minimum size threshold has been exceeded, the distribution of performance is approximately independent of program size. However the distribution of the performance of full trees is different from that of asymmetric trees and varies with tree depth.",
                "id": "02225"
            },
            {
                "title": "A social behaviour evolution approach for evolutionary optimisation",
                "abstract": "Evolutionary algorithms were originally designed to locate basins of optimum solutions in a stationary environment. Therefore, additional techniques and modifications have been introduced to deal with further requirements such as handling dynamic fitness functions or finding multiple optima. In this paper, we present a new approach for building evolutionary algorithms that is based on concepts borrowed from social behaviour evolution. Algorithms built with the proposed paradigm operate on a population of individuals that move in the search space as they interact and form groups. The interaction follows a set of social behaviours evolved by each group to enhance its adaptation to the environment (and other groups) and to achieve different desirable goals such as finding multiple optima, maintaining diversity, or tracking a moving peak in a changing environment. Each group has two sets of behaviours: one for intra-group interactions and one for inter-group interactions. These behaviours are evolved using mathematical models from the field of evolutionary game theory. This paper describes the proposed paradigm and starts studying it characteristics by building a new evolutionary algorithm and studying its behavior. The algorithm has been tested using a benchmark problem generator with promising initial results, which are also reported.",
                "id": "02226"
            },
            {
                "title": "Free lunches for function and program induction",
                "abstract": "In this paper we prove that for a variety of practical problems and representations, there is a free lunch for search algorithms that specialise in the task of finding functions or programs that solve problems, such as genetic programming. In other words, not all such algorithms are equally good under all possible performance measures. We focus in particular on the case where the objective is to discover functions that fit sets of data-points - a task that we will call symbolic regression. We show under what conditions there is a free lunch for symbolic regression, highlighting that these are extremely restrictive.",
                "id": "02227"
            },
            {
                "title": "Investigating a new paradigm for designing evolutionary optimisation algorithms using social behaviour evolution",
                "abstract": "This paper describes a new approach for building evolutionary optimisation algorithms inspired by concepts borrowed from evolution of social behaviour. The proposed approach utilises a set of behaviours used as operators that work on a population of individuals. These behaviours are used and evolved by groups of individuals to enhance a group adaptation to the environment and to other groups. Each group has two sets of behaviours: one for intra-group interactions and one for inter-group interactions. These behaviours are evolved using mathematical models from the field of evolutionary game theory. This paper describes the proposed paradigm and starts studying its characteristics by building a new evolutionary algorithm and studying its behaviour. The algorithm has been tested using a benchmark problem generator with promising initial results, which are also reported. We conclude the paper by identifying promising directions for the continuation of this research.",
                "id": "02228"
            },
            {
                "title": "Introduction to genetic programming",
                "abstract": "Genetic programming (GP) is a collection of evolutionary computation techniques that allow computers to solve problems automatically. Using ideas from natural evolution, GP starts from an ooze of random computer programs, and progressively refines them through processes of mutation and sexual recombination, until solutions emerge. All this without the user having to know or specify the form or structure of solutions in advance. Since its inception twenty years ago, GP has been used to solve a wide range of practical problems, producing a number of human-competitive results and even patentable new inventions. Like many other areas of computer science, GP is evolving rapidly, with new ideas, techniques and applications being constantly proposed, making it is difficult for people to identify the key ideas in the field and keep up with the pace of new developments. The aim of this tutorial is to provide a roadmap to GP for both newcomers and old-timers. The tutorial starts with a gentle introduction which describes how a population of programs is stored in the computer so that they can evolve with time. We explain how programs are represented, how random programs are initially created, and how GP creates a new generation by mutating the better existing programs or combining pairs of good parent programs to produce offspring programs. Then, we describe a variety of alternative representations for programs and some advanced GP techniques. These include: the evolution of machine-code and parallel programs, the use of grammars and probability distributions for the generation of programs, variants of GP which allow the solution of problems with multiple objectives, many speed-up techniques and some useful theoretical tools. To illustrate genetic programming's scope, we then review some real-world applications of GP. The tutorial is concluded by a series of recommendations and suggestions to obtain the most from a GP system and open questions.",
                "id": "02229"
            },
            {
                "title": "A genetic programming approach to the matrix bandwidth-minimization problem",
                "abstract": "The bandwidth of a sparse matrix is the distance from the main diagonal beyond which all elements of the matrix are zero. The bandwidth minimisation problem for a matrix consists of finding the permutation of rows and columns of the matrix which ensures that the non-zero elements are located in as narrow a band as possible along the main diagonal. This problem, which is known to be NP-complete, can also be formulated as a vertex labelling problem for a graph whose edges represent the non-zero elements of the matrix. In this paper, a Genetic Programming approach is proposed and tested against two of the best-known and widely used bandwidth reduction algorithms. Results have been extremely encouraging.",
                "id": "02230"
            },
            {
                "title": "There Is a Free Lunch for Hyper-Heuristics, Genetic Programming and Computer Scientists",
                "abstract": "In this paper we prove that in some practical situations, there is a free lunch for hyper-heuristics, i.e., for search algorithms that search the space of solvers, searchers, meta-heuristics and heuristics for problems. This has consequences for the use of genetic programming as a method to discover new search algorithms and, more generally, problem solvers. Furthermore, it has also rather important philosophical consequences in relation to the efforts of computer scientists to discover useful novel search algorithms.",
                "id": "02231"
            },
            {
                "title": "Evolutionary Brain Computer Interfaces",
                "abstract": "We propose a BCI mouse and speller based on the manipulation of P300 waves in EEG signals. The 2---D motion of the pointer on the screen is controlled by directly combining the amplitudes of the output produced by a filter in the presence of different stimuli. This filter and the features to be combined within it are optimised by a GA.",
                "id": "02232"
            },
            {
                "title": "Schema theory for genetic programming with one-point crossover and point mutation.",
                "abstract": "In this paper we first review the main re- sults obtained in the theory of schemata in Genetic Programming (GP) emphasis- ing their strengths and weaknesses. Then we propose a new, simpler definition of the concept of schema for GP which is quite close to the original concept of schema in genetic algorithms (GAs). Along with a new form of crossover, one-point crossover, and point mutation this concept of schema has been used to derive an improved schema theorem for GP which describes the propagation of schemata from one generation to the next. In the paper we discuss this re- sult and show that our schema theorem is the natural counterpart for GP of the schema theorem for GAs, to which it asymptotically converges.",
                "id": "02233"
            },
            {
                "title": "Some steps towards understanding how neutrality affects evolutionary search",
                "abstract": "The effects of neutrality on evolutionary search have been considered in a number of interesting studies, the results of which, however, have been contradictory. We believe that this confusion is due to several reasons. In this paper, we shed some light on neutrality by addressing these problems. That is, we use the simplest possible definition of neutrality, we consider one of the simplest possible algorithms, we apply it to two problems (a unimodal landscape and a deceptive landscape), which we analyse using fitness distance correlation, performance statistics and, critically, tracking the full evolutionary path of individuals within their family tree.",
                "id": "02234"
            },
            {
                "title": "Backward-chaining evolutionary algorithms",
                "abstract": "Starting from some simple observations on a popular selection method in Evolutionary Algorithms (EAs)--tournament selection--we highlight a previously-unknown source of inefficiency. This leads us to rethink the order in which operations are performed within EAs, and to suggest an algorithm--the EA with efficient macro-selection--that avoids the inefficiencies associated with tournament selection. This algorithm has the same expected behaviour as the standard EA but yields considerable savings in terms of fitness evaluations. Since fitness evaluation typically dominates the resources needed to solve any non-trivial problem, these savings translate into a reduction in computer time. Noting the connection between the algorithm and rule-based systems, we then further modify the order of operations in the EA, effectively turning the evolutionary search into an inference process operating in backward-chaining mode. The resulting backward-chaining EA creates and evaluates individuals recursively, backward from the last generation to the first, using depth-first search and backtracking. It is even more powerful than the EA with efficient macro-selection in that it shares all its benefits, but it also provably finds fitter solutions sooner, i.e., it is a faster algorithm. These algorithms can be applied to any form of population based search, any representation, fitness function, crossover and mutation, provided they use tournament selection. We analyse their behaviour and benefits both theoretically, using Markov chain theory and space/time complexity analysis, and empirically, by performing a variety of experiments with standard and back-ward chaining versions of genetic algorithms and genetic programming.",
                "id": "02235"
            },
            {
                "title": "Sub-tree Swapping Crossover, Allele Diffusion and GP Convergence",
                "abstract": "We provide strong evidence that sub-tree swapping crossover when applied to tree-based representations will cause alleles (node labels) to diffuse within length classes. For a-ary trees we provide further confirmation that all programs are equally likely to be sampled within any length class when sub-tree swapping crossover is applied in the absence of selection and mutation. Therefore, we propose that this form of search is unbiased - within length classes - for a-ary trees. Unexpectedly, however, for mixed-arity trees this is not found and a more complicated form of search is taking place where certain tree shapes, hence programs, are more likely to be sampled than others within each class. We examine the reasons for such shape bias in mixed arity representations and provide the practitioner with a thorough examination of sub-tree swapping crossover bias. The results of this, when combined with crossover length bias research, explain Genetic Programming's lack of structural convergence during later stages of an experimental run. Several operators are discussed where a broader form of convergence may be detected in a similar way to that found in Genetic Algorithm experimentation.",
                "id": "02236"
            },
            {
                "title": "A collaborative Brain-Computer Interface to improve human performance in a visual search task",
                "abstract": "In this paper we use a collaborative brain-computer interface to integrate the decision confidence of multiple non-communicating observers as a mechanism to improve group decisions. In recent research we tested this idea with the decisions associated with a simple visual matching task and found that a collaborative BCI can outperform group decisions made by a majority vote. Here we extend these initial findings in two ways. Firstly, we look at a more traditional (and more difficult) visual search task involving deciding whether a red vertical bar is present in a random set of 40 red and green, horizontal and vertical bars shown for a very short time. Secondly, to extract features from the neural signals we use spatial CSP filters instead of the spatio-temporal PCA we used in previous research, resulting in a significant reduction in the number of features and free parameters used in the system. Results obtained with 10 participants indicate that for almost all group sizes our new CSP-based collaborative BCI yields group decisions that are statistically significantly better than both traditional (majority-based) group decisions and group decisions made by a PCA-based collaborative BCI.",
                "id": "02237"
            },
            {
                "title": "Tournament selection, iterated coupon-collection problem, and backward-chaining evolutionary algorithms",
                "abstract": "Tournament selection performs tournaments by first sampling individuals uniformly at random from the population and then selecting the best of the sample for some genetic operation. This sampling process needs to be repeated many times when creating a new generation. However, even upon iteration, it may happen not to sample some of the individuals in the population. These individuals can therefore play no role in future generations. Under conditions of low selection pressure, the fraction of individuals not involved in any way in the selection process may be substantial. In this paper we investigate how we can model this process and we explore the possibility, methods and consequences of not generating and evaluating those individuals with the aim of increasing the efficiency of evolutionary algorithms based on tournament selection. In some conditions, considerable savings in terms of fitness evaluations are easily achievable, without altering in any way the expected behaviour of such algorithms.",
                "id": "02238"
            },
            {
                "title": "Exact Schema Theorem and Effective Fitness for GP with One-Point Crossover",
                "abstract": "This paper extends recent results in the GP schema theory by formulating a proper exact schema theorem for GP with one-point crossover. This gives an exact expression for the expected number of instances of a schema at the next gen- eration in terms of macroscopic quantities. This result allows the exact formulation of the notion of effective fitness in GP.",
                "id": "02239"
            },
            {
                "title": "No Free Lunch, Kolmogorov Complexity And The Information Landscape",
                "abstract": "The permutation closure of a single function is the finest level of granularity at which a no-free-lunch result can hold [1]. Using the information landscape framework which was introduced in [2], we are able to identify the unique properties of each closure. In particular, we associate each closure with the amount of information its members contain. This allows us to compute bounds on the expected performance of an algorithm on members of that closure. Moreover, we suggest a new way to measure the Kolmogorov complexity of a landscape. This allows us to associate each permutation closure with a particular Kolmogorov complexity.",
                "id": "02240"
            },
            {
                "title": "Geometric Crossover For The Permutation Representation",
                "abstract": "Geometric crossovers are a class of representation-independent search operators for evolutionary algorithms that are well-defined once a notion of distance over a solution space is defined. In this paper we explore the specialisation of geometric crossovers to the permutation representation analysing the consequences of the availability of more than one notion of distance. Also, we study the relations among distances and build a rational picture in which pre-existing recombination operators for permutations fit naturally. Lastly, we illustrate the application of geometric crossover to the Travelling Salesman Problem (TSP).",
                "id": "02241"
            },
            {
                "title": "Robust mobile robot localisation from sparse and noisy proximity readings using Hough transform and probability grids",
                "abstract": "We present a robust position-tracking method for a mobile robot with seven sonar sensors. The method is based on Hough transform and probability grids. The focus of the paper is on the problem of how to handle sparse sensors and noisy data in order to develop a low-cost navigation system for real-world applications. The proposed method consists of three steps. It computes a two-dimensional feature space by applying a straight-line Hough transform to the sonar readings. The detected features are then matched with the world map as reference pattern. The correlation counts obtained in the previous step are used for updating the position probability grid. We demonstrate that this method, on the one hand, avoids the common problems of feature detection in sonar data such as erroneous lines through separate clusters, corner inference, and line artefacts through reflection. On the other hand, it achieves a robustness that dense sensor-matching techniques, such as Markov localisation, can only deliver if they use a complex sensor model which takes into account the angle to the object reflecting the sonar beam.",
                "id": "02242"
            },
            {
                "title": "Practical performance models of algorithms in evolutionary program induction and other domains",
                "abstract": "Evolutionary computation techniques have seen a considerable popularity as problem solving and optimisation tools in recent years. Theoreticians have developed a variety of both exact and approximate models for evolutionary program induction algorithms. However, these models are often criticised for being only applicable to simplistic problems or algorithms with unrealistic parameters. In this paper, we start rectifying this situation in relation to what matters the most to practitioners and users of program induction systems: performance. That is, we introduce a simple and practical model for the performance of program-induction algorithms. To test our approach, we consider two important classes of problems - symbolic regression and Boolean function induction - and we model different versions of genetic programming, gene expression programming and stochastic iterated hill climbing in program space. We illustrate the generality of our technique by also accurately modelling the performance of a training algorithm for artificial neural networks and two heuristics for the off-line bin packing problem. We show that our models, besides performing accurate predictions, can help in the analysis and comparison of different algorithms and/or algorithms with different parameters setting. We illustrate this via the automatic construction of a taxonomy for the stochastic program-induction algorithms considered in this study. The taxonomy reveals important features of these algorithms from the performance point of view, which are not detected by ordinary experimentation.",
                "id": "02243"
            },
            {
                "title": "Hyperschema Theory for GP with One-Point Crossover, Building Blocks, and Some New Results in GA Theory",
                "abstract": " . Two main weaknesses of GA and GP schema theorems arethat they provide only information on the expected value of the numberof instances of a given schema at the next generation E[m(H; t + 1)],and they can only give a lower bound for such a quantity. This paperpresents new theoretical results on GP and GA schemata which largelyovercome these weaknesses. Firstly, unlike previous results which concentratedon schema survival and disruption, our results extend to GPrecent work on GA... ",
                "id": "02244"
            },
            {
                "title": "Decomposition of fitness functions in random heuristic search",
                "abstract": "We show that a fitness function, when taken together with an algorithm, can be reformulated as a set of probability distributions. This set can, in some cases, be equivalently viewed as an information vector which gives ordering information about pairs of search points in the domain. Certain performance criteria definable over such an information vector can be learned by linear regression in such a way that extrapolations can sometimes be made: the regression can make performance predictions about functions it has not seen. In addition, the vector can be taken as a model of the fitness function and used to compute features of it like difficultly via vector calculations.\n\n",
                "id": "02245"
            },
            {
                "title": "The Effects of Constant and Bit-Wise Neutrality on Problem Hardness, Fitness Distance Correlation and Phenotypic Mutation Rates",
                "abstract": "Kimura's neutral theory of evolution has inspired researchers from the evolutionary computation community to incorporate neutrality into evolutionary algorithms (EAs) in the hope that it can aid evolution. The effects of neutrality on evolutionary search have been considered in a number of studies, the results of which, however, have been highly contradictory. In this paper, we analyze the reasons for this and make an effort to shed some light on neutrality by addressing them. We consider two very simple forms of neutrality: constant neutrality\u2014a neutral network of constant fitness, identically distributed in the whole search space\u2014and bit-wise neutrality, where each phenotypic bit is obtained by transforming a group of genotypic bits via an encoding function. We study these forms of neutrality both theoretically and empirically (both for standard benchmark functions and a class of random MAX-SAT problems) to see how and why they influence the behavior and performance of a mutation-based EA. In particular, we analyze how the fitness distance correlation of landscapes changes under the effect of different neutral encodings and how phenotypic mutation rates vary as a function of genotypic mutation rates. Both help explain why the behavior of a mutation-based EA may change so radically as problem, form of neutrality, and mutation rate are varied.",
                "id": "02246"
            },
            {
                "title": "The effects of constant neutrality on performance and problem hardness in GP",
                "abstract": "The neutral theory of molecular evolution and the associated notion of neutrality have interested many researchers in Evolutionary Computation. The hope is that the presence of neutrality can aid evolution. However, despite the vast number of publications on neutrality, there is still a big controversy on its effects. The aim of this paper is to clarify under what circumstances neutrality could aid Genetic Programming using the traditional representation (i.e. treelike structures). For this purpose, we use fitness distance correlation as a measure of hardness. In addition we have conducted extensive empirical experimentation to corroborate the fitness distance correlation predictions. This has been done using two test problems with very different landscape features that represent two extreme cases where the different effects of neutrality can be emphasised. Finally, we study the distances between individuals and global optimum to understand how neutrality affects evolution (at least with the one proposed in this paper).",
                "id": "02247"
            },
            {
                "title": "Extending particle swarm optimisation via genetic programming",
                "abstract": "Particle Swarm Optimisers (PSOs) search using a set of interacting particles flying over the fitness landscape. These are typically controlled by forces that encourage each particle to fly back both towards the best point sampled by it and towards the swarm's best. Here we explore the possibility of evolving optimal force generating equations to control the particles in a PSO using genetic programming.",
                "id": "02248"
            },
            {
                "title": "An empirical investigation of how and why neutrality affects evolutionary search",
                "abstract": "The effects of neutrality on evolutionary search have been considered in a number of studies, the results of which, however, have been contradictory. Some have found neutrality to be beneficial to aid evolution whereas others have argued that neutrality in the evolutionary process is useless. We believe that this confusion is due to several reasons: many studies have based their conclusions on performance statistics rather than a more in-depth analysis of population dynamics, studies often consider problems, representations and search algorithms that are relatively complex and so results represent the compositions of multiple effects, there is not a single definition of neutrality and different studies have added neutrality to problems in radically different ways. In this paper, we try to shed some light on neutrality by addressing these problems. That is, we use the simplest possible definition of neutrality (a neutral network of constant fitness, identically distributed in the whole search space), we consider one of the simplest possible algorithms (a mutation based, binary genetic algorithm) applied to two simple problems (a unimodal landscape and a deceptive landscape), and analyse both performance figures and, critically, population flows from and to the neutral network and the basins of attraction of the optima.",
                "id": "02249"
            },
            {
                "title": "Fitness Distributions and GA Hardness",
                "abstract": "Considerable research effort has been spent in trying to formulate a good definition of GA-Hardness. Given an instance of a problem, the objective is to estimate the performance of a GA. Despite partial successes current definitions are still unsatisfactory. In this paper we make some steps towards a new, more powerful way of assessing problem difficulty based on the properties of a problem's fitness distribution. We present experimental results that strongly support this idea.",
                "id": "02250"
            },
            {
                "title": "The no free lunch and realistic search algorithms",
                "abstract": "The No-Free-Lunch theorems (NFLTs) are criticized for being too general to be of any relevance to the real world scenario. This paper investigates, both formally and empirically, the implications of the NFLTs for realistic search algorithms. In the first part of the paper, by restricting ourselves to a specific performance measure, we derive a new NFL result for a class of problems which is not closed under permutations. In the second part, we discuss properties of this set which are likely to be true for realistic search algorithms. We provide empirical support for this in [1].",
                "id": "02251"
            },
            {
                "title": "Evolution of human-competitive lossless compression algorithms with GP-zip2",
                "abstract": "We propose GP-zip2, a new approach to lossless data compression based on Genetic Programming (GP). GP is used to optimally combine well-known lossless compression algorithms to maximise data compression. GP-zip2 evolves programs with multiple components. One component analyses statistical features extracted by sequentially scanning the data to be compressed and divides the data into blocks. These blocks are projected onto a two-dimensional Euclidean space via two further (evolved) program components. K-means clustering is then applied to group similar data blocks. Each cluster is labelled with the optimal compression algorithm for its member blocks. After evolution, evolved programs can be used to compress unseen data. The compression algorithms available to GP-zip2 are: Arithmetic coding, Lempel-Ziv-Welch, Unbounded Prediction by Partial Matching, Run Length Encoding, and Bzip2. Experimentation shows that the results produced by GP-zip2 are human-competitive, being typically superior to well-established human-designed compression algorithms in terms of the compression ratios achieved in heterogeneous archive files.",
                "id": "02252"
            },
            {
                "title": "On the effects of bit-wise neutrality on fitness distance correlation, phenotypic mutation rates and problem hardness",
                "abstract": "The effects of neutrality on evolutionary search are not fully understood. In this paper we make an effort to shed some light on how and why bit-wise neutrality - an important form of neutrality induced by a genotype-phenotype map where each phenotypic bit is obtained by transforming a group of genotypic bits via an encoding function - influences the behaviour of a mutation-based GA on functions of unitation. To do so we study how the fitness distance correlation (fdc) of landscapes changes under the effect of different (neutral) encodings. We also study how phenotypic mutation rates change as a function of the genotypic mutation rate for different encodings. This allows us to formulate simple explanations for why the behaviour of a GA changes so radically with different types of neutrality and mutation rates. Finally, we corroborate these conjectures with extensive empirical experimentation.\n\n",
                "id": "02253"
            },
            {
                "title": "Brain-Computer Interfaces for Detection and Localization of Targets in Aerial Images.",
                "abstract": "Objective. The N2pc event-related potential (ERP) appears on the opposite side of the scalp with respect to the visual hemisphere where an object of interest is located. We explored the feasibility of using it to extract information on the spatial location of targets in aerial images shown by means of a rapid serial visual presentation (RSVP) protocol using single-trial classification. Methods. Im...",
                "id": "02254"
            },
            {
                "title": "Genetic Programming for Feature Detection and Image Segmentation",
                "abstract": " Genetic Programming is a method of program discovery/optimisation consistingof a special kind of genetic algorithm capable of operating on non-linear chromosomes(parse trees) representing programs and an interpreter which can run theprograms being optimised. In this paper we describe a set of terminals and functionsfor the parse trees handled by genetic programming which enable it to developeffective image filters. These filters can either be used to highly enhance and detect... ",
                "id": "02255"
            },
            {
                "title": "Learning a Navigation Task in Changing Environments by Multi-task Reinforcement Learning",
                "abstract": "This work is concerned with practical issues surrounding the application of reinforcement learning to a mobile robot. The robot's task is to navigate in a controlled environment and to collect objects using its gripper. Our aim is to build a control system that enables the robot to learn incrementally and to adapt to changes in the environment. The former is known as multi-task learning, the latter is usually referred to as continual 'lifelong' learning. First, we emphasize the connection between adaptive state-space quantisation and continual learning. Second, we describe a novel method for multi-task learning in reinforcement environments. This method is based on constructive neural networks and uses instance-based learning and dynamic programming to compute a task-dependent agent-internal state space. Third, we describe how the learning system is integrated with the control architecture of the robot. Finally, we investigate the capabilities of the learning algorithm with respect to the transfer of information between related reinforcement learning tasks, like navigation tasks in different environments. It is hoped that this method will lead to a speed-up in reinforcement learning and enable an autonomous robot to adapt its behaviour as the environment changes.",
                "id": "02256"
            },
            {
                "title": "Improving Decision-Making Based On Visual Perception Via A Collaborative Brain-Computer Interface",
                "abstract": "In the presence of complex stimuli, in the absence of sufficient time to complete the visual parsing of a scene, or when attention is divided, an observer can only take in a subset of the features of a scene, potentially leading to poor decisions. In this paper we look at the possibility of integrating the percepts from multiple non-communicating observers as a means of achieving better joint perception and better decision making. Our approach involves the combination of brain-computer interface (Bel) technology with human behavioural responses.To test our ideas in controlled conditions, we asked observers to perform a simple visual matching task involving the rapid sequential presentation of pairs of visual patterns and the subsequent decision as whether the two patterns in a pair were the same or different. Visual stimuli were presented for insufficient time for the observers to be certain of the decision. The degree of difficulty of the task also depended on the number of matching features between the two patterns. The higher the number, the more difficult the task.We recorded the response times of observers as well as a neural feature which predicts incorrect decisions and, thus, indirectly indicates the confidence of the decisions made by the observers. We then built a composite neuro-behavioural feature which optimally combines these behavioural and neural measures.For group decisions, we tested the use of a majority rule and three further decision rules which weigh the decisions of each observer based on response times and our neural and neuro-behavioural features. Results indicate that the integration of behavioural responses and neural features can significantly improve accuracy when compared with individual performance. Also, within groups of each size, decision rules based on such features outperform the majority rule.",
                "id": "02257"
            },
            {
                "title": "A Schema Theory Analysis of the Evolution of Size in Genetic Programming with Linear Representations",
                "abstract": "In this paper we use the schema theory presented in [20] to better understand the changes in size distribution when using GP with standard crossover and linear structures. Applications of the theory to problems both with and without fitness suggest that standard crossover induces specific biases in the distributions of sizes, with a strong tendency to over sample small structures, and indicate the existence of strong redistribution effects that may be a major force in the early stages of a GP run. We also present two important theoretical results: An exact theory of bloat, and a general theory of how average size changes on flat landscapes with glitches. The latter implies the surprising result that a single program glitch in an otherwise flat fitness landscape is sufficient to drive the average program size of an infinite population, which may have important implications for the control of code growth.",
                "id": "02258"
            },
            {
                "title": "Evolving a designer-balanced neural network for Ms PacMan",
                "abstract": "Balancing games towards designer requirements is an on-going research area with proven potential for use in industry. However, other elements beyond game mechanics can be tweaked and optimised to offer a rewarding gaming experience to players. This work looks at using proven techniques and tools to change not the parameters of a game, but the parameters of an agent playing a game to create a version of that agent that behaves in a designer-specified manner. These new agents can then be utilised for a wide variety of tasks, from offering new challenges to players, to aiding designers in automating parts of their pipeline when balancing games towards given requirements. Through the use of genetic algorithms and intelligent collection of game metrics, we are able to successfully generate varying neural-network controlled agents, each with different styles of play and levels of skill.",
                "id": "02259"
            },
            {
                "title": "Evolution of Graph-Like Programs with Parallel Distributed Genetic Programming",
                "abstract": "Parallel Distributed Genetic Pro- gramming (PDGP) is a new form of Genetic Pro- gramming (GP) suitable for the development of programs with a high degree of parallelism. Pro- grams are represented in PDGP as graphs with nodes representing functions and terminals, and links representing the flow of control and results. The paper presents the representations, the op- erators and the interpreters used in PDGP, and describes experiments in which PDGP has been compared to standard GP.",
                "id": "02260"
            },
            {
                "title": "Backward-chaining genetic programming",
                "abstract": "This paper presents a backward-chaining version of GP.",
                "id": "02261"
            },
            {
                "title": "Modelling group-foraging behaviour with particle swarms",
                "abstract": "Despite the many features that the behaviour of the standard particle swarm algorithm shares with grouping behaviour in animals (e.g. social attraction and communication between individuals), this biologically inspired technique has been mainly used in classical optimisation problems (i.e. finding the optimal value in a fitness landscape). We present here a novel application for particle swarms: the simulation of group-foraging in animals. Animals looking for food sources are modelled as particles in a swarm moving over an abstract food landscape. The particles are guided to the food by a smell (or aura), which surrounds it and whose intensity is proportional to the amount of food available. The results show that this new extended version of the algorithm produces qualitatively realistic behaviour. For example, the simulation shows the emergence of group-foraging behaviour among particles.",
                "id": "02262"
            },
            {
                "title": "Mapping non-conventional extensions of genetic programming",
                "abstract": "Conventional genetic programming research excludes memory and iteration. We have begun an extensive analysis of the space through which GP or other unconventional AI approaches search and extend it to consider explicit program stop instructions (T8), including Markov analysis and any time models (T7). We report halting probability, run time and functionality (including entropy of binary functions) of both halting and anytime programs. Irreversible Turing complete program fitness landscapes, even with halt, scale poorly however loops lock-in variation allowing more interesting functions.",
                "id": "02263"
            },
            {
                "title": "Generating SAT local-search heuristics using a GP hyper-heuristic framework",
                "abstract": "We present GP-HH, a framework for evolving local-search 3-SAT heuristics based on GP. The aim is to obtain \"disposable\" heuristicswhich are evolved and used for a specific subset of instances of a problem.We test the heuristics evolved by GP-HH against well-known local-search heuristics on a variety of benchmark SAT problems. Results are veryencouraging.",
                "id": "02264"
            },
            {
                "title": "Lessons from Testing an Evolutionary Automated Game Balancer in Industry",
                "abstract": "Game balancing has been an important area of academic research in the past few years, with various methods of approaching the task being proposed. At this point in time, however, industry impact has been minimal, with these approaches appearing overwhelming or expensive to game designers and developers. The work presented in this paper takes one of the algorithms and approaches previously researched and, in cooperation with a commercial games studio, defines a specification language and tests its applicability in a real world scenario. The game being balanced in this paper is of a different genre to those previously targeted by the algorithm, with vastly different mechanics and expectations of what is considered a balanced state. Results indicate both great potential of the research area in general, but also highlight challenges to achieving mainstream use in the real world of balancing techniques.",
                "id": "02265"
            },
            {
                "title": "Markov chain models of bare-bones particle swarm optimizers",
                "abstract": "We apply a novel theoretical approach to better understand the behaviour of different types of bare-bones PSOs. It avoids many common but unrealistic assumptions often used in analyses of PSOs. Using finite element grid techniques, it builds a discrete Markov chain model of the BB-PSO which can approximate it on arbitrary continuous problems to any precision. Iterating the chain's transition matrix gives precise information about the behaviour of the BB-PSO at each generation, including the probability of it finding the global optimum or being deceived. The predictions of the model are remarkably accurate and explain the features of Cauchy, Gaussian and other sampling distributions.",
                "id": "02266"
            },
            {
                "title": "Evolutionary Solo Pong Players",
                "abstract": "An Internet Java Applet http://www.cs.essex.ac.uk/staff/poli/ SoloPong/ allows users anywhere to play the Solo Pong game. We compare people's performance to a hand coded \"Optimal\" player and programs automatically produced by computational intelligence. The computational intelligence techniques are: genetic programming, including a hybrid of GP and a human designed algorithm, and a particle swarm optimiser. The computational intelligence approaches are not fine tuned. GP and PSO find good players. Evolutionary computation (EC) is able to beat both human designed code and human players.",
                "id": "02267"
            },
            {
                "title": "Emergent Behaviour, Population-Based Search And Low-Pass Filtering",
                "abstract": "In recent work we have formulated a model of emergent coordinated behaviour for a population of interacting entities. The model is a modified spring mass model where masses can perceive the environment and generate external forces. As a result of the interactions the population behaves like a single organism moving under the effect the vector sum of the external forces generated by each entity. When such forces are proportional to the gradient of a resource distribution f (x), the resultant force controlling the single emergent organism Is proportional to the gradient of a modified food distribution. This is the result of applying a filtering kernel to f (x). The kernel is typically a low-pass filter.This model can be applied to genetic algorithms (GAs) and other population-based search algorithms. For example, in previous research, we have found kernels (via genetic programming) that allow the single organism model to track the motion of the centre of mass of GAs and particle swarm optimisers accurately for many generations.In this paper we corroborate this model in several ways. Firstly, we provide a mathematical proof that on any problem and for any crossover operator, the effect of crossover is that of reducing the amplitude of the derivatives (slopes) of the population distribution. This implies that a GA perceives an effective fitness landscape which is a smoothed, low-pass filtered version of the original. Then, taking inspiration from this result and our active mass-spring model, we propose a class of fitness functions, OneMix, where there is an area of the landscape with high frequency variations. This area contains the global optimum but a genetic algorithm with high crossover probability should not be able \"see\" it due to its low-pass behaviour. So, a GA with strong crossover should be deceived and attracted towards a local optimum, while with low crossover probability this should not happen. This is, indeed, what happens as we demonstrate with a variety of empirical runs and with infinite-population model simulations. Finally, following our earlier approach, we also evolved kernels for OneMix, obtaining again a good fit between the behaviour of the \"single-organism\" hill-climber and the GA.",
                "id": "02268"
            },
            {
                "title": "On Turing complete T7 and MISC F--4 program fitnes landscapes",
                "abstract": "We use the minimal instruction set F-4 computer to define a minimal Turing complete T7 computer suitable for genetic programming (GP) and amenable to theoretical analy- sis. Experimental runs and mathematical analysis of the T7, show the fraction of halting programs is drops to zero as bigger programs are run.",
                "id": "02269"
            },
            {
                "title": "Generalized cycle crossover for graph partitioning",
                "abstract": "We propose a new crossover that generalizes cycle crossover to permutations with repetitions and naturally suits partition problems. We tested it on graph partitioning problems obtaining excellent results.",
                "id": "02270"
            },
            {
                "title": "How and why a bit-wise neutrality with and without locality affects evolutionary search",
                "abstract": "Despite the vast work on neutrality, there are not general conclusions on its effects. In this paper we make an effort to understand how neutrality influences evolution. For this purpose we will use a type of neutrality that allows locality (which is believed to be a desirable feature of neutrality).",
                "id": "02271"
            },
            {
                "title": "Cost-benefit investigation of a genetic-programming hyperheuristic",
                "abstract": "In previous work, we have introduced an effective, grammar-based, linear Genetic-Programming hyperheuristic, i.e., a search heuristicon the space of heuristics. Here we further investigate this approach inthe context of search performance and resource utilisation. For the chosenrealistic travelling salesperson problems it shows that the hyperheuristicroutinely produces metaheuristics that find tours whose lengths arehighly competitive with the best results from literature, while populationsize, genotype size, and run time can be kept very moderate.",
                "id": "02272"
            },
            {
                "title": "Bistability in a Gene Pool GA with Mutation",
                "abstract": "It is possible for a GA to have two stable fixed points on a single-peak fitness landscape. These can correspond to meta-stable finite populations. This phenomenon is called bista- bility, and is only known to happen in the presence of recombination, selection, and mu- tation. This paper models the bistability phenomenon using an infinite population model of a GA based on gene pool recombination. Fixed points and their stability are explicitly calculated. This is possible since the infinite population model of the gene pool GA is much more tractable than the infinite population model for the standard simple GA. For the needle-in-the-haystack fitness function, the fixed point equations reduce to a single variable polynomial equation, and stability of fixed points can be determined from the derivative of the single variable equation. We also show empirically that bistability can occur on a single-peak landscape where there is selective pressure toward the optimum at every point of the search space.",
                "id": "02273"
            },
            {
                "title": "Automatic Creation of Taxonomies of Genetic Programming Systems",
                "abstract": "A few attempts to create taxonomies in evolutionary computation have been made. These either group algorithms or group problems on the basis of their similarities. Similarity is typically evaluated by manually analysing algorithms/problems to identify key characteristics that are then used as a basis to form the groups of a taxonomy. This task is not only very tedious but it is also rather subjective. As a consequence the resulting taxonomies lack universality and are sometimes even questionable. In this paper we present a new and powerful approach to the construction of taxonomies and we apply it to Genetic Programming (GP). Only one manually constructed taxonomy of problems has been proposed in GP before, while no GP algorithm taxonomy has ever been suggested. Our approach is entirely automated and objective. We apply it to the problem of grouping GP systems with their associated parameter settings. We do this on the basis of performance signatures which represent the behaviour of each system across a class of problems. These signatures are obtained thorough a process which involves the instantiation of models of GP's performance. We test the method on a large class of Boolean induction problems.",
                "id": "02274"
            },
            {
                "title": "Genetic-programming based prediction of data compression saving",
                "abstract": "We use Genetic Programming (GP) to generate programs that predict the data compression ratio for compression algorithms. GP evolves programs with multiple components. One component analyses statistical features extracted from the files' byte frequency distribution to come up with a compression ratio prediction. Another component does the same but by analysing statistical features extracted from the files' raw ASCII representation. A further (evolved) component acts as a decision tree to determine the overall output (compression ratio estimation) returned by an individual. The decision tree produces its result based on a series of comparisons among statistical features extracted from the files and the outputs of the two prediction components. The evolved decision tree has the choice to select either the outputs of the two compression prediction trees or alternatively, to integrate them into an evolved mathematical formula. Experiments with the proposed approach show that GP is able to accurately estimate the compression ratio of unseen files thereby avoiding the need to run multiple compressions on a file to decide which one provide best results.",
                "id": "02275"
            },
            {
                "title": "Genetic programming theory I & II",
                "abstract": "We start by describing and characterising the search space explored by genetic programming (GP). We show how to compute the size of the search space. Then, we introduce some work on the distribution of functionality of the programs in the search space and indicate its scientific and practical consequences. In particular, we explain why GP can work despite the immensity of the search space it explores. Then, we show recent theory on halting probability that extends these results to forms of Turing complete GP. This indicates that Turing complete GP has a hard time solving problems unless certain measures are put in place. Having characterised the search space, in the second part of the tutorial, we characterise GP as a search algorithm by using the schema theory. In the tutorial we introduce the basics of schema theory, explaining how one can derive an exact probabilistic description of GAs and GP based on schemata. We illustrate the lessons that can be learnt from the theory and indicate some recipes to do GP well for practitioners. These include important new results on bloat in GP and ways to cure it. Despite its technical contents, an big effort has been made to limit the use of mathematical formulae to a minimum.",
                "id": "02276"
            },
            {
                "title": "The impact of population size on code growth in GP: analysis and empirical validation",
                "abstract": "The crossover bias theory for bloat [18] is a recent result which predicts that bloat is caused by the sampling of short, unfit programs. This theory is clear and simple, but it has some weaknesses: (1) it implicitly assumes that the population is large enough to allow sampling of all relevant program sizes (although it does explain what to expect in the many practical cases where this is not true, e.g., because the population is small); (2) it does not explain what is meant by its assumption that short programs are unfit. In this paper we discuss these weaknesses and propose a refined version of the crossover bias theory that clarifies the relationship between bloat and finite populations, and explains what features of the fitness landscape cause bloat to occur. The theory, in particular, predicts that smaller populations will bloat more slowly than larger ones. Additionally, the theory predicts that bloat will only be observed in problems where short programs are less fit than longer ones when looking at samples created by fitness-based importance sampling, i.e. samplings of the search space in which fitter programs have a higher probability of being sampled (e.g., the Metropolis-Hastings method). Experiments with two classical GP benchmarks fully corroborate the theory.",
                "id": "02277"
            },
            {
                "title": "Structure and metaheuristics",
                "abstract": "Metaheuristics have often been shown to be eectiv e for dif- cult combinatorial optimization problems. The reason for that, however, remains unclear. A framework for a theory of metaheuristics crucially depends on a formal representa- tive model of such algorithms. This paper unies/reconciles in a single framework the model of a black box algorithm coming from the no-free-lunch research (e.g. Wolpert et al. (25), Wegener (23)) with the study of tness landscape. Both are important to the understanding of meta-heuristics, but they have so far been studied separately. The new model is a natural environment to study meta-heuristics. Categories and Subject Descriptors F.2 (Theory of Computation): ANALYSIS OF ALGO- RITHMS AND PROBLEM COMPLEXITY",
                "id": "02278"
            },
            {
                "title": "Evolving the Topology and the Weights of Neural Networks Using a Dual Representation",
                "abstract": "Evolutionary computation is a class of global search techniquesbased on the learning process of a population of potential solutions to agiven problem, that has been successfully applied to a variety of problems.In this paper a new approach to the construction of neural networks based onevolutionary computation is presented. A linear chromosome combined to agraph representation of the network are used by genetic operators, whichallow the evolution of the architecture and the weights simultaneouslywithout the need of local weight optimization. This paper describes theapproach, the operators and reports results of the application of thistechnique to several binary classification problems.",
                "id": "02279"
            },
            {
                "title": "Information landscapes and the analysis of search algorithms",
                "abstract": "In [15] we introduced the information landscape as a new concept of a landscape. We showed that for a landscape of a small size, information landscape theory can be used to predict the performance of a GA without running the algorithm. Based on this framework, here we develop a new theoretical model to study search algorithms in general. Particularly, we are able to infer important properties of a search algorithm without having knowledge about its specific operators. We give an example of this technique for a simple GA.",
                "id": "02280"
            },
            {
                "title": "Exact analysis of the sampling distribution for the canonical particle swarm optimiser and its convergence during stagnation",
                "abstract": "Several theoretical analyses of the dynamics of particle swarms have been offered in the literature over the last decade. Virtually all rely on substantial simplifications, including the assumption that the particles are deterministic. This has prevented the exact characterisation of the sampling distribution of the PSO. In this paper we introduce a novel method, which allows one to exactly determine all the characteristics of a PSO's sampling distribution and explain how they change over any number of generations, in the presence stochasticity. The only assumption we make is stagnation, i.e., we study the sampling distribution produced by particles in search for a better personal best. We apply the analysis to the PSO with inertia weight, but the analysis is also valid for the PSO with constriction.",
                "id": "02281"
            },
            {
                "title": "Neutrality in evolutionary algorithms\u2026 What do we know?",
                "abstract": "Over the last years, the effects of neutrality have attracted the attention of many researchers in the Evolutionary Algorithms (EAs) community. A mutation from one gene to another is considered as neutral if this modification does not affect the phenotype. This article provides a general overview on the work carried out on neutrality in EAs. Using as a framework the origin of neutrality and its study in different paradigms of EAs (e.g., Genetic Algorithms, Genetic Programming), we discuss the most significant works and findings on this topic. This work points towards open issues, which we belive the community needs to address.",
                "id": "02282"
            },
            {
                "title": "General Schema Theory for Genetic Programming with Subtree-Swapping Crossover",
                "abstract": "In this paper a new, general and exact schema theory for genetic programming is presented. The theory includes a microscopic schema theorem applicable to crossover operators which replace a subtree in one parent with a sub-tree from the other parent to produce the offspring. A more macroscopic schema theorem is also provided which is valid for crossover operators in which the probability of selecting any two crossover points in the parents depends only on their size and shape. The theory is based on the notions of Cartesian node reference systems and variable-arity hyperschemata both introduced here for the first time. In the paper we provide examples which show how the theory can be specialised to specific crossover operators and how it can be used to derive an exact definition of effective fitness and a size-evolution equation for GP.",
                "id": "02283"
            },
            {
                "title": "Using Schema Theory To Explore Interactions Of Multiple Operators",
                "abstract": "In the last two years the schema theory for Ge- netic Programming (GP) has been applied to the problem of understanding the length biases of a variety of crossover and mutation operators on variable length linear structures. In these initial papers, operators were studied in isolation. In practice, however, they are typically used in var- ious combinations, and in this paper we present the first schema theory analysis of the complex interactions of multiple operators. In particular, we apply the schema theory to the use of standard subtree crossover, full mutation, and grow muta- tion (in varying proportions) to variable length linear structures in the one-then-zeros problem. We then show how the results can be used to guide choices about the relative proportion of these operators in order to achieve certain struc- tural goals during a run.",
                "id": "02284"
            },
            {
                "title": "Fitness-proportional negative slope coefficient as a hardness measure for genetic algorithms",
                "abstract": "The Negative Slope Coefficient (nsc) is an empirical measure of problem hardness based on the analysis of offspring-fitness vs. parent-fitness scatterplots. The nsc has been tested empirically on a large variety problems showing considerable reliability in distinguishing easy from hard problems. However, neither a theoretical justification nor a theoretical analysis of the nsc have ever been given. This paper presents a modification of nsc, the fitness-proportional negative slope coefficient (fpncs), for which it is possible to give a theoretical explanation and analysis. To illustrate the approach we compute fpnsc theoretically for the class of invertible functions of unitation, and for two mutation operators. We apply the theory to compute fpnsc for three benchmark functions: Onemax, Trap and Onemix. We then compare the predictions of fpnsc with the success probability recorded in actual runs. The results suggest that fpnsc is able to broadly discriminate between easy and hard GA problems.",
                "id": "02285"
            },
            {
                "title": "Mean and variance of the sampling distribution of particle swarm optimizers during stagnation",
                "abstract": "Several theoretical analyses of the dynamics of particle swarms have been offered in the literature over the last decade. Virtually all rely on substantial simplifications, often including the assumption that the particles are deterministic. This has prevented the exact characterization of the sampling distribution of the particle swarm optimizer (PSO). In this paper we introduce a novel method that allows us to exactly determine all the characteristics of a PSO sampling distribution and explain how it changes over any number of generations, in the presence stochasticity. The only assumption we make is stagnation, i.e., we study the sampling distribution produced by particles in search for a better personal best. We apply the analysis to the PSO with inertia weight, but the analysis is also valid for the PSO with constriction and other forms of PSO.",
                "id": "02286"
            },
            {
                "title": "Information landscapes and problem hardness",
                "abstract": "In [20] we introduced a new concept of a landscape: the information landscape. We showed that for problems of very small size (e.g. a 3-bit problem), it can be used to generally and accurately predict the performance of a GA. Based on this framework, in this paper we develop a method to predict GA hardness on realistic landscapes. We give empirical results which support our approach.",
                "id": "02287"
            },
            {
                "title": "Elitism reduces bloat in genetic programming",
                "abstract": "Elitism is commonly used in generational GP to ensure that the best individuals discovered in a generation are not lost, and are made available for possible further improvements to new generations. Using two GP systems and four problems, we show how elitism reduces the growth of mean program size.",
                "id": "02288"
            },
            {
                "title": "Optimization via Parameter Mapping with Genetic Programming",
                "abstract": "This paper describes a new approach for parameter optimization that uses a novel representation for the parameters to be optimized. By using genetic programming, the new method evolves functions that transform initial random values for the parameters into optimal ones. This new representation allows the incorporation of knowledge about the problem being solved. Moreover, the new approach addresses the scalability problem by using a representation that, in principle, is independent of the size of the problem being addressed. Promising results are reported, comparing the new method with differential evolution and particle swarm optimization on a test suite of benchmark problems.",
                "id": "02289"
            },
            {
                "title": "Speeding up genetic algorithm-based game balancing using fitness predictors.",
                "abstract": "Genetic Algorithms (GAs) can find game parameters that fit a designer's requirements. An issue with this is the long time taken to evaluate fitness, as this requires running the game many times. Here we use fitness predictors, currently neural networks, to speed up the process by reducing the number of fitness evaluations. The predictors are trained using data generated by the GA at runtime. After training, the model is invoked to estimate the fitness of newly created individuals. If the estimate is below a threshold, it is accepted. Otherwise, the original fitness function is invoked. We have used this approach on Ms PacMan with promising results.",
                "id": "02290"
            },
            {
                "title": "Geometric crossover for biological sequences",
                "abstract": "This paper extends a geometric framework for interpreting crossover and mutation [4] to the case of sequences. This representation is important because it is the link between artificial evolution and biological evolution. We define and theoretically study geometric crossover for sequences under edit distance and show its intimate connection with the biological notion of sequence homology.",
                "id": "02291"
            },
            {
                "title": "An empirical tool for analysing the collective behaviour of population-based algorithms",
                "abstract": "Understanding the emergent collective behaviour (and the properties associated with it) of population-based algorithms is an important prerequisite for making technically sound choices of algorithms and also for designing new algorithms for specific applications. In this paper, we present an empirical approach to analyse and quantify the collective emergent behaviour of populations. In particular, our long term objective is to understand and characterise the notions of exploration and exploitation and to make it possible to characterise and compare algorithms based on such notions. The proposed approach uses self-organising maps as a tool to track the population dynamics and extract features that describe a population \"functionality\" and \"structure\".",
                "id": "02292"
            },
            {
                "title": "Crossover, sampling, bloat and the harmful effects of size limits",
                "abstract": "Recent research [9,2] has enabled the accurate prediction of the limiting distribution of tree sizes for Genetic Programming with standard sub-tree swapping crossover when GP is applied to a flat fitness landscape. In that work, however, tree sizes are measured in terms of number of internal nodes. While the relationship between internal nodes and length is one-to-one for the case of a-ary trees, it is much more complex in the case of mixed arities. So, practically the length bias of subtree crossover remains unknown. This paper starts to fill this theoretical gap, by providing accurate estimates of the limiting distribution of lengths approached by tree-based GP with standard crossover in the absence of selection pressure. The resulting models confirm that short programs can be expected to be heavily resampled. Empirical validation shows that this is indeed the case. We also study empirically how the situation is modified by the application of program length limits. Surprisingly, the introduction of such limits further exacerbates the effect. However, this has more profound consequences than one might imagine at first. We analyse these consequences and predict that, in the presence of fitness, size limits may initially speed up bloat, almost completely defeating their original purpose (combating bloat). Indeed, experiments confirm that this is the case for the first 10 or 15 generations. This leads us to suggest a better way of using size limits. Finally, this paper proposes a novel technique to counteract bloat, sampling parsimony, the application of a penalty to resampling.",
                "id": "02293"
            },
            {
                "title": "Parsimony pressure made easy",
                "abstract": "The parsimony pressure method is perhaps the simplest and most frequently used method to control bloat in genetic programming. In this paper we first reconsider the size evolution equation for genetic programming developed in [26] and rewrite it in a form that shows its direct relationship to Price's theorem. We then use this new formulation to derive theoretical results that show how to practically and optimally set the parsimony coefficient dynamically during a run so as to achieve complete control over the growth of the programs in a population. Experimental results confirm the effectiveness of the method, as we are able to tightly control the average program size under a variety of conditions. These include such unusual cases as dynamically varying target sizes such that the mean program size is allowed to grow during some phases of a run, while being forced to shrink in others.",
                "id": "02294"
            },
            {
                "title": "Parallel genetic algorithm taxonomy",
                "abstract": "Genetic Algorithms (GAs) are powerful search techniques that are used to solve difficult problems in many disciplines. Unfortunately, they can be very demanding in terms of computation load and memory. Parallel Genetic Algorithms (PGAs) are parallel implementations of GAs which can provide considerable gains in terms of performance and scalability. PGAs can easily be implemented on net- works of heterogeneous computers or on parallel mainframes. In this paper we review the state of the art on PGAs and propose a new taxono- my also including a new form of PGA (the dynamic deme model) we have recently developed. I. INTRODUCTION",
                "id": "02295"
            },
            {
                "title": "Practical model of genetic programming's performance on rational symbolic regression problems",
                "abstract": "Many theoretical studies on GP are criticized for not being applicable to the real world. Here we present a practical model for the performance of a standard GP system in real problems. The model gives accurate predictions and has a variety of applications, including the assessment of the similarities and differences of different GP systems.",
                "id": "02296"
            },
            {
                "title": "A Review of Theoretical and Experimental Results on Schemata in Genetic Programming",
                "abstract": " . Schemata and the schema theorem, although criticised, areoften used to explain why genetic algorithms (GAs) work. A considerableresearch effort has been produced recently to extend the GA schematheory to Genetic Programming (GP). In this paper we review the mainresults available to date in the theory of schemata for GP and somerecent experimental work on schemata.1 IntroductionGenetic Programming (GP) has been applied successfully to a large numberof difficult problems [8, 7,... ",
                "id": "02297"
            },
            {
                "title": "Genetic Programming Discovers Efficient Learning Rules for the Hidden and Output Layers of Freeforward Neural Networks",
                "abstract": "The learning method is critical for obtaining good generalisation in neural networks with limited training data. The Standard BackPropagation (SBP) training algorithm suffers from several problems such as sensitivity to the initial conditions and very slow convergence. The aim of this work is to use Genetic Programming (GP) to discover new supervised learning algorithms which can overcome some of these problems. In previous research a new learning algorithms for the output layer has been discovered using GP. By comparing this with SBP on different problems better performance was demonstrated. This paper shows that GP can also discover better learning algorithms for the hidden layers to be used in conjunction with the algorithm previously discovered. Comparing these with SBP on different problems we show they p rovide better performances. This study indicates that there exist many supervised learning algorithms better than SBP and that GP can be used to discover them.",
                "id": "02298"
            },
            {
                "title": "Solving High-Order Boolean Parity Problems with Smooth Uniform Crossover, Sub-Machine Code GP and Demes",
                "abstract": "We propose and study new search operators and a novel node representation that can make GP fitness landscapes smoother. Together with a tree evaluation method known as sub-machine-code GP and the use of demes, these make up a recipe for solving very large parity problems using GP. We tested this recipe on parity problems with up to 22 input variables, solving them with a very high success probability.",
                "id": "02299"
            },
            {
                "title": "Limitations of the fitness-proportional negative slope coefficient as a difficulty measure",
                "abstract": "Fitness-Proportional Negative Slope Coefficient is a fitness landscapes measure that has recently been introduced as a potential indicator of problem hardness for optimisation. It is inspired to an older measure, the Negative Slope Coefficient, and it has been theoretically modelled. Preliminary experiments have suggested that it may be a good predictor of problem hardness. However, this measure has not undergone any convincing and comprehensive empirical testing. Our objective is to fill this gap. So, we perform empirical tests using a large set of invertible functions of unitation. We find that while this measure may correctly predict the degree of evolvability of a landscape, this does not necessarily correlate with the difficulty of problems. Some landscapes may show, for example, limited evolvability and yet be easy to solve because either solutions are already present in the initial population or the computational resources provided exceed evolvability obstacles. Or it may be impossible to solve them irrespective of their evolvability simply because they are far too vast for the computational resources provided. These situations are hardly captured by the Fitness-Proportional Negative Slope Coefficient.",
                "id": "022100"
            },
            {
                "title": "Theoretical results in genetic programming: the next ten years?",
                "abstract": "We consider the theoretical results in GP so far and prospective areas for the future. We begin by reviewing the state of the art in genetic programming (GP) theory including: schema theories, Markov chain models, the distribution of functionality in program search spaces, the problem of bloat, the applicability of the no-free-lunch theory to GP, and how we can estimate the difficulty of problems before actually running the system. We then look at how each of these areas might develop in the next decade, considering also new possible avenues for theory, the challenges ahead and the open issues.",
                "id": "022101"
            },
            {
                "title": "Classes of problems in the black box scenario",
                "abstract": "The no-free-lunch theorems (NFLTs) do not consider explicitly the structure of problems. In [1] we gave a formal definition of structure. We showed that many metaheuristics have identical performance on problems which belong to the same structural class. In this paper, we define a notion of a distance between fitness functions. We argue that an algorithm cannot be efficient on a class of problems if the distance between the fitness function associated with instances of that class is too big. In [2] we corroborate our ideas using several problems.",
                "id": "022102"
            },
            {
                "title": "GP-Fileprints: file types detection using genetic programming",
                "abstract": "We propose a novel application of Genetic Programming (GP): the identification of file types via the analysis of raw binary streams (i.e., without the use of meta data). GP evolves programs with multiple components. One component analyses statistical features extracted from the raw byte-series to divide the data into blocks. These blocks are then analysed via another component to obtain a signature for each file in a training set. These signatures are then projected onto a two-dimensional Euclidean space via two further (evolved) program components. K-means clustering is applied to group similar signatures. Each cluster is then labelled according to the dominant label for its members. Once a program that achieves good classification is evolved it can be used on unseen data without requiring any further evolution. Experimental results show that GP compares very well with established file classification algorithms (i.e., Neural Networks, Bayes Networks and J48 Decision Trees).",
                "id": "022103"
            },
            {
                "title": "On the limiting distribution of program sizes in tree-based genetic programming",
                "abstract": "We provide strong theoretical and experimental evidence that standard sub-tree crossover with uniform selection of crossover points pushes a population of a-ary GP trees towards a distribution of tree sizes of the form: Pr{n} = (1 - apa) (an+1 n) (1-pa)(a-1)n+1 pan where n is the number of internal nodes in a tree and pa is a constant. This result generalises the result previously reported for the case a = 1.",
                "id": "022104"
            },
            {
                "title": "Free lunches for neural network search",
                "abstract": "In this paper we prove that for a variety of practical situations, the no-free-lunch (NFL) theorem does not apply to algorithms that search the space of artificial neural networks, such as evolutionary algorithms. We find, in particular, that, while conditions under which NFL applies exist, these require extremely restrictive symmetries on the set of possible problems which are unlikely encountered in practice. In other words, not all algorithms are equally good at finding neural networks that solve problems under all possible performance measures: a superior search algorithm for this domain does exist.",
                "id": "022105"
            },
            {
                "title": "SIM_AGENT: A Toolkit for Exploring Agent Designs",
                "abstract": " SIM AGENT is a toolkit that arose out of a project concerned withdesigning an architecture for an autonomous agent with human-like capabilities. ",
                "id": "022106"
            },
            {
                "title": "A genetic programming approach to the evolution of brain\u2013computer interfaces for 2-D mouse\u2013pointer control",
                "abstract": "We propose the use of genetic programming (GP) as a means to evolve brain\u2013computer interfaces for mouse control. Our objective is to synthesise complete systems, which analyse electrical brain signals and directly transform them into pointer movements, almost from scratch, the only input provided by us in the process being the set of visual stimuli to be used to generate recognisable brain activity. Experimental results with our GP approach are very promising and compare favourably with those produced by support vector machines.",
                "id": "022107"
            },
            {
                "title": "An artificial vision system for X-ray images of human coronary trees",
                "abstract": "The coronary tree expert (CORTEX) analyzer, which is a vision system for the description of the bidimensional shape and position of coronary vessels using standard nonsubtracted radiographic images, is described. A bottom-up approach was used to deal with the typical characteristics of medical images, such as structural and nonstructural noise and complexity and variability of biological shapes. On these grounds, grouping criteria were utilized to produce intermediate image representations with an increasing complexity in a hierarchical manner (from edge points to curves, segments, bars, and finally to vessels and their mutual relations). In this way, uncertain, inconsistent, and deficient information was efficiently processed. The evaluation of CORTEX segmentation is also performed according to a signal-detection-theory-like approach.",
                "id": "022108"
            },
            {
                "title": "An Empirical Investigation of How Degree Neutrality Affects GP Search",
                "abstract": "Over the last years, neutrality has inspired many researchers in the area of Evolutionary Computation (EC) systems in the hope that it can aid evolution. However, there are contradictory results on the effects of neutrality in evolutionary search. The aim of this paper is to understand how neutrality - named in this paper degree neutrality - affects GP search. For analysis purposes, we use a well-defined measure of hardness (i.e., fitness distance correlation) as an indicator of difficulty in the absence and in the presence of neutrality, we propose a novel approach to normalise distances between a pair of trees and finally, we use a problem with deceptive features where GP is well-known to have poor performance and see the effects of neutrality in GP search.",
                "id": "022109"
            },
            {
                "title": "Performance models for evolutionary program induction algorithms based on problem difficulty indicators",
                "abstract": "Most theoretical models of evolutionary algorithms are difficult to apply to realistic situations. In this paper, two models of evolutionary program-induction algorithms (EPAs) are proposed which overcome this limitation. We test our approach with two important classes of problems -- symbolic regression and Boolean function induction -- and a variety of EPAs including: different versions of genetic programming, gene expression programing, stochastic iterated hill climbing in program space and one version of cartesian genetic programming. We compare the proposed models against a practical model of EPAs we previously developed and find that in most cases the new models are simpler and produce better predictions. A great deal can also be learnt about an EPA via a simple inspection of our new models. E.g., it is possible to infer which characteristics make a problem difficult or easy for the EPA.",
                "id": "022110"
            },
            {
                "title": "Communication, leadership, publicity and group formation in particle swarms",
                "abstract": "We look at how the structure of social networks and the nature of social interactions affect the behaviour of Particle Swarms Optimisers. To this end, we propose a general model of communication and consensus which focuses on the effects of social interactions putting the details of the dynamics and the optimum seeking behaviour of PSOs into the background.",
                "id": "022111"
            },
            {
                "title": "A collaborative Brain-Computer Interface for improving group detection of visual targets in complex natural environments",
                "abstract": "Detecting a target in a complex environment can be a difficult task, both for a single individual and a group, especially if the scene is very rich of structure and there are strict time constraints. In recent research, we have demonstrated that collaborative Brain-Computer Interfaces (cBCIs) can use neural signals and response times to estimate the decision confidence of participants and use this to improve group decisions in visual-matching and visual-search tasks with artificial stimuli. This paper extends that work in two ways. Firstly, we use a much harder target detection task where observers are presented with complex natural scenes where targets are very difficult to identify. Secondly, we complement the neural and behavioural information used in our previous cBCIs with physiological features representing eye movements and eye blinks of participants in the period preceding their decisions. Results obtained with 10 participants indicate that the proposed cBCI improves decision errors by up to 3.4% (depending on group size) over group decisions made by a majority vote. Furthermore, results show that providing the system with information about eye movements and blinks further significantly improves performance over our best previously reported method.",
                "id": "022112"
            },
            {
                "title": "Crossover Operators For A Hardware Implementation Of GP Using FPGAs And Handel-C",
                "abstract": "This paper analyses the behavior of the crossover operator in a hardware implementation of Ge- netic Programming using Field Programmable Gate Arrays. Three different crossover operators that limit the lengths of programs are analysed: A truncating operator, a limiting operator that con- strains the lengths of both offspring and a limit- ing operator that only constrains the length of one offspring. The latter has some interesting prop- erties that suggest a new method of limiting code growth in the presence of fitness.",
                "id": "022113"
            },
            {
                "title": "EcoPS: a particle swarm algorithm to model group-foraging",
                "abstract": "Recent work has introduced a simulation model of ecological processes in terms of a very simple Particle Swarm algorithm. This abstract model produced qualitatively realistic behaviours, but do these results hold up in a model constrained by more plausible biological assumptions? The objective of this paper is to answer this question.",
                "id": "022114"
            },
            {
                "title": "General schema theory for genetic programming with subtree-swapping crossover: Part II.",
                "abstract": "This paper is the second part of a two-part paper which introduces a general schema theory for genetic programming (GP) with subtree-swapping crossover (Part I (Poli and McPhee, 2003)). Like other recent GP schema theory results, the theory gives an exact formulation (rather than a lower bound) for the expected number of instances of a schema at the next generation. The theory is based on a Cartesian node reference system, introduced in Part I, and on the notion of a variable-arity hyperschema, introduced here, which generalises previous definitions of a schema. The theory includes two main theorems describing the propagation of GP schemata: a microscopic and a macroscopic schema theorem. The microscopic version is applicable to crossover operators which replace a subtree in one parent with a subtree from the other parent to produce the offspring. Therefore, this theorem is applicable to Koza's GP crossover with and without uniform selection of the crossover points, as well as one-point crossover, size-fair crossover, strongly-typed GP crossover, context-preserving crossover and many others. The macroscopic version is applicable to crossover operators in which the probability of selecting any two crossover points in the parents depends only on the parents' size and shape. In the paper we provide examples, we show how the theory can be specialised to specific crossover operators and we illustrate how it can be used to derive other general results. These include an exact definition of effective fitness and a size-evolution equation for GP with subtree-swapping crossover.",
                "id": "022115"
            },
            {
                "title": "An Experimental Analysis of Schema Creation, Propagation and Disruption in Genetic Programming",
                "abstract": "In this paper we first review the main results in the theory of schemata in Genetic Programming (GP) and summarise a new GP schema theory which is based on a new definition of schema. Then we study the creation, propagation and dis- ruption of this new form of schemata in real runs, for standard crossover, one-point crossover and selection only. Finally, we discuss these results in the light our GP schema theorem.",
                "id": "022116"
            },
            {
                "title": "Topological crossover for the permutation representation",
                "abstract": "Topological crossovers are a class of representation-independent operators that are well-defined once a notion of distance over the solution space is defined. In this paper we explore how the topological framework applies to the permutation representation and in particular analyze the consequences of having more than one notion of distance available. Also, we study the interactions among distances and build a rational picture in which pre-existing recombination/crossover operators for permutation fit naturally. Lastly, we also analyze the application of topological crossover to TSP.",
                "id": "022117"
            },
            {
                "title": "Enhancement of Group Perception via a Collaborative Brain-Computer Interface.",
                "abstract": "Objective: We aimed at improving group performance in a challenging visual search task via a hybrid collaborative brain-computer interface (cBCI). Methods: Ten participants individually undertook a visual search task where a display was presented for 250 ms, and they had to decide whether a target was present or not. Local temporal correlation common spatial pattern (LTCCSP) was used to extract ne...",
                "id": "022118"
            },
            {
                "title": "Evolving heuristics with genetic programming",
                "abstract": "Hyper-Heuristics are methods to choose and combine heuristics to generate new ones. In this work, we use a grammar-based genetic programming system as a Hyper-Heuristic framework. The framework is used for evolving effective incremental solvers for SAT (Inc*). Tests against well-known local search heuristics on a variety of benchmark problems reveal that the evolved heuristics are superior.",
                "id": "022119"
            },
            {
                "title": "Semiotic Aspects of Generalized Bases of Data",
                "abstract": "We have systems rich in information but poor in knowledge. The paper shall sketch a framework in which information and knowledge could be properly integrated. Our initial hypothesis is that passing from information to knowledge involves the elaboration of a well structured theory of semiotic units. The latter is codified by a three-fold structure: an expression, a content and an ontological component. Expressions and contents together give rise to the symbolic component of the semiotic unit. The three components are all further subdividable into types and kinds. Their intertwined relationships become easier to elaborate if we find a uniform way to represent the various components. This requires formal tools as sophisticated as those provided by category theory. It follows that other operators beyond the usual logical operators should be used, namely the 'geometric operators' of category theory.",
                "id": "022120"
            },
            {
                "title": "Developmental plasticity in linear genetic programming",
                "abstract": "Biological organisms exhibit numerous types of plasticity, where they respond both developmentally and behaviorally to environmental factors. In some organisms, for example, environmental conditions can lead to the developmental expression of genes that would otherwise remain dormant, leading to significant phenotypic variation and allowing selection to act on these otherwise \"invisible\" genes. In contrast to biological plasticity, the vast majority of evolutionary computation systems, including genetic programming, are rigid and can only adapt to very limited external changes. In this paper we extend the N-gram GP system, a recently introduced estimation of distribution algorithm for program evolution, using Incremental Fitness-based Development (IFD), a novel technique which allows for developmental plasticity in the generation of linear-GP style programs. Tests with a large set of problems show that the new system outperforms the original N-gram GP system and is competitive with standard GP. Analysis of the evolved programs indicates that IFD allows for the generation of more complex programs than standard N-gram GP, with the generated programs often containing several separate sequences of instructions that are reused multiple times, often with variations.",
                "id": "022121"
            },
            {
                "title": "Evolutionary synthesis of a trajectory integrator for an analogue brain-computer interface mouse",
                "abstract": "Recently significant steps have been made towards effective EEG-based brain-computer interfaces for mouse control. A major obstacle in this line of research, however, is the integration of the noisy and contradictory information provided at each time step by the signal processing systems into a coherent and precise trajectory for the mouse pointer. In this paper we attack this difficult problem using genetic programming, obtaining extremely promising results.",
                "id": "022122"
            },
            {
                "title": "Evolving timetabling heuristics using a grammar-based genetic programming hyper-heuristic framework",
                "abstract": "This paper introduces a Grammar-based Genetic Programming Hyper-Heuristic framework (GPHH) for evolving constructive heuristics\n for timetabling. In this application GP is used as an online learning method which evolves heuristics while solving the problem.\n In other words, the system keeps on evolving heuristics for a problem instance until a good solution is found. The framework\n is tested on some of the most widely used benchmarks in the field of exam timetabling and compared with the best state-of-the-art\n approaches. Results show that the framework is very competitive with other constructive techniques, and did outperform other\n hyper-heuristic frameworks on many occasions.",
                "id": "022123"
            },
            {
                "title": "Exact Schema Theorems for GP with One-Point and Standard Crossover Operating on Linear Structures and Their Application to the Study of the Evolution of Size",
                "abstract": "In this paper, firstly we specialise the exact GP schema theorem for one-point crossover to the case of linear structures of variable length, for example binary strings or programs with arity-1 primitives only. Secondly, we extend this to an exact schema theorem for GP with standard crossover applicable to the case of linear structures. Then we study, both mathematically and numerically, the schema equations and their fixed points for infinite populations for both a constant and a length-related fitness function. This allows us to characterise the bias induced by standard crossover. This is very peculiar. In the case of a constant fitness function, at the fixed-point, structures of any length are present with non-zero probability. However, shorter structures are sampled exponentially much more frequently than longer ones.",
                "id": "022124"
            },
            {
                "title": "Genetic Programming Bloat with Dynamic Fitness",
                "abstract": "In artificial evolution individuals which perform as their par- ents are usually rewarded identically to their parents. We note that Na- ture is more dynamic and there may be a penalty to pay for doing the same thing as your parents. We report two sets of experiments where static fitness functions are firstly augmented by a penalty for unchanged offspring and secondly the static fitness case is replaced by randomly generated dynamic test cases. We conclude genetic programming, when evolving artificial ant control programs, is surprisingly little effected by large penalties and program growth is observed in all our experiments.",
                "id": "022125"
            },
            {
                "title": "Parallel Distributed Genetic Programming Applied to the Evolution of Natural Language Recognisers",
                "abstract": " . This paper describes an application of Parallel DistributedGenetic Programming (PDGP) to the problem of inducing recognisersfor natural language from positive and negative examples. PDGP is anew form of Genetic Programming (GP) which is suitable for the developmentof programs with a high degree of parallelism and an efficientand effective reuse of partial results. Programs are represented in PDGPas graphs with nodes representing functions and terminals, and links representingthe ... ",
                "id": "022126"
            },
            {
                "title": "Evolving an Improved Algorithm for Envelope Reduction Using a Hyper-Heuristic Approach",
                "abstract": "Large sparse symmetric matrices are typical characteristics of the linear systems found in various scientific and engineering disciplines, such as fluid mechanics, structural engineering, finite element analysis, and network analysis. In all such systems, the performance of solvers crucially depends on the sum of the distance of each matrix element from the matrix's main diagonal. This quantity is known as the envelope. In order to reduce the envelope of a matrix, its rows and columns should be reordered properly. The problem of minimizing the envelope size is exceptionally hard and known to be NP-complete. A considerable number of methods have been developed for reducing the envelope among which the Sloan algorithm offered a substantial improvement over previous algorithms. In this paper, a hyper-heuristic approach based on genetic programming, which we term genetic hyper-heuristic, is introduced for evolving an enhanced version of the Sloan algorithm. We also present a local search algorithm and integrate it with the new algorithm produced by our hyper-heuristic to further improve the quality of the solutions. The new algorithms are evaluated on a large set of standard benchmarks against six state-of-the-art algorithms from the literature. Our algorithms outperform all the methods under testing by a wide margin.",
                "id": "022127"
            },
            {
                "title": "Exploring extended particle swarms: a genetic programming approach",
                "abstract": "Particle Swarm Optimisation (PSO) uses a population of particles that fly over the fitness landscape in search of an optimal solution. The particles are controlled by forces that encourage each particle to fly back both towards the best point sampled by it and towards the swarm's best point, while its momentum tries to keep it moving in its current direction.Previous research started exploring the possibility of evolving the force generating equations which control the particles through the use of genetic programming (GP).We independently verify the findings of the previous research and then extend it by considering additional meaningful ingredients for the PSO force-generating equations, such as global measures of dispersion and position of the swarm. We show that, on a range of problems, GP can automatically generate new PSO algorithms that outperform standard human-generated as well as some previously evolved ones.",
                "id": "022128"
            },
            {
                "title": "Generalisation of the limiting distribution of program sizes in tree-based genetic programming and analysis of its effects on bloat",
                "abstract": "Recent research [1] has found that standard sub-tree crossover with uniform selection of crossover points, in the absence of fitness pressure, pushes a population of GP trees towards a Lagrange distribution of tree sizes. However, the result applied to the case of single arity function plus leaf node combinations, e.g., unary, binary, ternary, etc trees only. In this paper we extend those findings and show that the same distribution is also applicable to the more general case where the function set includes functions of mixed arities. We also provide empirical evidence that strongly corroborates this generalisation. Both predicted and observed results show a distinct bias towards the sampling of shorter programs irrespective of the mix of function arities used. Practical applications and implications of this knowledge are investigated with regard to search efficiency and program bloat. Work is also presented regarding the applicability of the theory to the traditional 90%-function 10%-terminal crossover node selection policy.",
                "id": "022129"
            },
            {
                "title": "Towards cooperative brain-computer interfaces for space navigation",
                "abstract": "We explored the possibility of controlling a spacecraft simulator using an analogue Brain-Computer Interface (BCI) for 2-D pointer control. This is a difficult task, for which no previous attempt has been reported in the literature. Our system relies on an active display which produces event-related potentials (ERPs) in the user's brain. These are analysed in real-time to produce control vectors for the user interface. In tests, users of the simulator were told to pass as close as possible to the Sun. Performance was very promising, on average users managing to satisfy the simulation success criterion in 67.5% of the runs. Furthermore, to study the potential of a collaborative approach to spacecraft navigation, we developed BCIs where the system is controlled via the integration of the ERPs of two users. Performance analysis indicates that collaborative BCIs produce trajectories that are statistically significantly superior to those obtained by single users.",
                "id": "022130"
            },
            {
                "title": "Morphological algorithm design for binary images using genetic programming",
                "abstract": "This paper presents a Genetic Programming (GP) approach to the design of Mathematical Morphology (MM) algorithms for binary images. The algorithms are constructed using logic operators and the basic MM operators, i.e. erosion and dilation, with a variety of structuring elements. GP is used to evolve MM algorithms that convert a binary image into another containing just a particular feature of interest. In the study we have tested three fitness functions, training sets with different numbers of elements, training images of different sizes, and 7 different features in two different kinds of applications. The results obtained show that it is possible to evolve good MM algorithms using GP.",
                "id": "022131"
            },
            {
                "title": "Inc*: an incremental approach for improving local search heuristics",
                "abstract": "This paper presents Inc*, a general algorithm that can be used in conjunction with any local search heuristic and that has the potential to substantially improve the overall performance of the heuristic. Genetic programming is used to discover new strategies for the Inc* algorithm. We experimentally compare performance of local heuristics for SAT with and without the Inc* algorithm. Results show that Inc* consistently improves performance.",
                "id": "022132"
            },
            {
                "title": "Continuous optimisation theory made easy? finite-element models of evolutionary strategies, genetic algorithms and particle swarm optimizers",
                "abstract": "We propose a method to build discrete Markov chain models of continuous stochastic optimisers that can approximate them on arbitrary continuous problems to any precision. We discretise the objective function using a finite element method grid which produces corresponding distinct states in the search algorithm. Iterating the transition matrix gives precise information about the behaviour of the optimiser at each generation, including the probability of it finding the global optima or being deceived. The approach is tested on a (1+1)-ES, a bare bones PSO and a real-valued GA. The predictions are remarkably accurate.\n\n",
                "id": "022133"
            },
            {
                "title": "Subheuristic search and scalability in a hyperheuristic",
                "abstract": "Our previous work has introduced a {hyperheuristic} (HH) approach based on Genetic Programming (GP). There, GP employs user-given languages where domain-specific local heuristics are used as primitives for producing specialised metaheuristics (MH). Here, we show that the GP-HH works well with simple generic languages over subheuristic primitives, dealing with increases of problem size and reduction of resources. The system produces effective and efficient MHs that deliver best results known in a chosen test domain. We also demonstrate that user-given, modest domain information allows the HH to produce an improvement over a previous best result from the literature.",
                "id": "022134"
            },
            {
                "title": "Shape from radiological density",
                "abstract": "In this paper we propose a strategy to solve the problem of recovering the 3-D shape of anatomical structures from single X-ray images, i.e., the problem of Shape from Radiological Density (SFRD). In order to overcome the noninvertibility of the process of image generation, we formulate a minimal set of physical assumptions that are used to constrain SFRD and to transform it into a well-posed problem. Our shape recovery strategy requires the solution of four problems: (a) linearization of the process of X-ray image generation, (b) image segmentation, (c) estimation of a map of the local thickness of each anatomical structure of interest, and (d) recovery of the 3-D shape of each structure from its boundaries and thickness map. In this paper we assume that problems (a) and (b) have already been faced, and propose a solution for problems (c) and (d). Experimental results on synthetic images, X-ray images of phantoms, and real radiograms are reported.",
                "id": "022135"
            },
            {
                "title": "Operator equalisation and bloat free GP",
                "abstract": "Research has shown that beyond a certain minimum program length the distributions of program functionality and fitness converge to a limit. Before that limit, however, there may be program-length classes with a higher or lower average fitness than that achieved beyond the limit. Ideally, therefore, GP search should be limited to program lengths that are within the limit and that can achieve optimum fitness. This has the dual benefits of providing the simplest/smallest solutions and preventing GP bloat thus shortening run times. Here we introduce a novel and simple technique, which we call Operator Equalisation, to control how GP will sample certain length classes. This allows us to finely and freely bias the search towards shorter or longer programs and also to search specific length classes during a GP run. This gives the user total control on the program length distribution, thereby completely freeing GP from bloat. Results show that we can automatically identify potentially optimal solution length classes quickly using small samples and that, for particular classes of problems, simple length biases can significantly improve the best fitness found during a GP run.",
                "id": "022136"
            },
            {
                "title": "Target Detection in Video Feeds with Selected Dyads and Groups Assisted by Collaborative Brain-Computer Interfaces",
                "abstract": "We present a collaborative Brain-Computer Interface (cBCI) to aid group decision-making based on realistic video feeds. The cBCI combines neural features extracted from EEG and response times to estimate the decision confidence of users. Confidence estimates are used to weigh individual responses and obtain group decisions. Results obtained with 10 participants indicate that cBCI groups are significantly more accurate than equally-sized groups using standard majority. Also, selecting dyads on the basis of the average performance of their members and then assisting them with our cBCI halves the error rates with respect to majority-based performance. Also, this allows most participants to be included in at least one selected dyad, hence being quite inclusive. Results indicate that this selection strategy makes cBCIs even more effective as methods for human augmentation in realistic scenarios.",
                "id": "022137"
            },
            {
                "title": "Efficient Evolution of Asymmetric Recurrent Neural Networks Using a PDGP-inspired Two-Dimensional Representation",
                "abstract": "Recurrent neural networks are particularly useful for processing time sequences and simulating dynamical systems. However, methods for building recurrent architectures have been hindered by the fact that available training algorithms are considerably more complex than those for feedforward networks. In this paper, we present a new method to build recurrent neural networks based on evolutionary computation, which combines a linear chromosome with a two- dimensional representation inspired by Parallel Distributed Genetic Programming (a form of genetic programmingfor the evolution of graph-like programs) to evolve the architecture and the weights simultaneously. Our method can evolve general asymmetric recurrent architectures as well as specialized recurrent architectures. This paper describes the method and reports on results of its application.",
                "id": "022138"
            },
            {
                "title": "Coarse graining in an evolutionary algorithm with recombination, duplication and inversion",
                "abstract": "A generalised form of recombination, wherein an offspring can be formed from any of the genetic material of the parents, is analysed in the context of a two-locus recombinative GA. A complete, exact solution, is derived, showing how the dynamical behaviour is radically different to that of homologous crossover. Inversion is shown to potentially introduce oscillations in the dynamics, while gene duplication leads to an asymmetry between homogeneous and heterogeneous strings. All non-homologous operators lead to allele \"diffusion\" along the chromosome. We discuss how inferences from the two-locus results extend to the case of a recombinative GA with selection and more than two loci.",
                "id": "022139"
            },
            {
                "title": "Parameter Mapping: A genetic programming approach to function optimization",
                "abstract": "This paper describes a new approach to optimization that uses a novel representation for the parameters to be optimized. By using genetic programming, the method evolves a population of functions. The purpose of such functions is to transform initial random values of the parameters into better ones. The representation is, in principle, independent of the size of the problem being addressed. Promising results are reported, comparing the new method with differential evolution, particle swarm optimization, and genetic algorithms, on a test suite of benchmark problems.",
                "id": "022140"
            },
            {
                "title": "Geometric Landscape Of Homologous Crossover For Syntactic Trees",
                "abstract": "The relationship between search space, distances and genetic operators for syntactic trees is little understood. Geometric crossover and geometric mutation are representation-independent operators that are well-defined once a notion of distance over the solution space is defined. In this paper we apply this geometric framework to the syntactic tree representation and show how the well-known structural distance is naturally associated with homologous crossover and sub-tree mutation.",
                "id": "022141"
            },
            {
                "title": "A Fixed Point Analysis Of A Gene Pool GA With Mutation",
                "abstract": "This paper analyzes a recombina- tion/mutation/selection genetic algorithm that uses gene pool recombination. For linear fitness functions, the infinite population model can be described by ' equations where ' is the string length. For linear fitness functions, we show that there is a single fixed point and that this fixed point is stable. For the ONEMAX fitness function, the model reduces to a linear recurrence in a single variable which can be explicitly solved. The time-to-convergence for ONEMAX is given.",
                "id": "022142"
            },
            {
                "title": "Inbreeding properties of geometric crossover and non-geometric recombinations",
                "abstract": "Geometric crossover is a representation-independent generalization of traditional crossover for binary strings. It is defined in a simple geometric way by using the distance associated with the search space. Many interesting recombination operators for the most frequently used representations are geometric crossovers under some suitable distance. Showing that a given recombination operator is a geometric crossover requires finding a distance for which offspring are in the metric segment between parents. However, proving that a recombination operator is not a geometric crossover requires excluding that one such distance exists. It is, therefore, very difficult to draw a clear-cut line between geometric crossovers and non-geometric crossovers. In this paper we develop some theoretical tools to solve this problem and we prove that some well-known operators are not geometric. Finally, we discuss the implications of these results.\n\n",
                "id": "022143"
            },
            {
                "title": "Memory with Memory in Tree-Based Genetic Programming",
                "abstract": "In recent work on linear register-based genetic programming (GP) we introduced the notion of Memory-with-Memory (MwM), where the results of operations are stored in registers using a form of soft assignment which blends a result into the current content of a register rather than entirely replace it. The MwM system yielded very promising results on a set of symbolic regression problems. In this paper, we propose a way of introducing MwM style behaviour in tree-based GP systems. The technique requires only very minor modifications to existing code, and, therefore, is easy to apply. Experiments on a variety of synthetic and real-world problems show that MwM is very beneficial in tree-based GP, too.",
                "id": "022144"
            },
            {
                "title": "Detecting Localised Muscle Fatigue During Isometric Contraction Using Genetic Programming",
                "abstract": "We propose the use of Genetic Programming (GP) to generate new features to predict localised muscles fatigue from pre-filtered surface EMG signals. In a training phase, GP evolves programs with multiple components. One component analyses statistical features extracted from EMG to divide the signals into blocks. The blocks' labels are decided based on the number of zero crossings. These blocks are then projected onto a two-dimensional Euclidean space via two further (evolved) program components. K-means clustering is applied to group similar data blocks. Each cluster is then labelled into one of three types (Fatigue, Transition-to-Fatigue and Non-Fatigue) according to the dominant label among its members. Once a program is evolved that achieves good classification, it can be used on unseen signals without requiring any further evolution. During normal operation the data are again divided into blocks by the first component of the program. The blocks are again projected onto a two-dimensional Euclidean space by the two other components of the program. Finally blocks are labelled according to the k-nearest neighbours. The system alerts the user of possible approaching fatigue once it detects a Transition-to-Fatigue. In experimentation with the proposed technique, the system provides very encouraging results.",
                "id": "022145"
            },
            {
                "title": "Information perspective of optimization",
                "abstract": "In this paper we relate information theory and Kolmogorov Complexity (KC) to optimization in the black box scenario. We define the set of all possible decisions an algorithm might make during a run, we associate a function with a probability distribution over this set and define accordingly its entropy. We show that the expected KC of the set (rather than the function) is a better measure of problem difficulty. We analyze the effect of the entropy on the expected KC. Finally, we show, for a restricted scenario, that any permutation closure of a single function, the finest level of granularity for which a No Free Lunch Theorem can hold [7], can be associated with a particular value of entropy. This implies bounds on the expected performance of an algorithm on members of that closure.",
                "id": "022146"
            },
            {
                "title": "Product Geometric Crossover",
                "abstract": "Geometric crossover is a representation-independent defini- tion of crossover based on the distance of the search space interpreted as a metric space. It generalizes the traditional crossover for binary strings and other important recombination operators for the most frequently used representations. Using a distance tailored to the problem at hand, the abstract definition of crossover can be used to design new problem specific crossovers that embed problem knowledge in the search. In this paper, we introduce the important notion of product geometric crossover that allows to build new geometric crossovers combining pre-existing ge- ometric crossovers in a simple way.",
                "id": "022147"
            },
            {
                "title": "A linear estimation-of-distribution GP system",
                "abstract": "We present N-gram GP, an estimation of distribution algorithm for the evolution of linear computer programs. The algorithm learns and samples a joint probability distribution of triplets of instructions (or 3-grams) at the same time as it is learning and sampling a program length distribution. We have tested N-gram GP on symbolic regressions problems where the target function is a polynomial of up to degree 12 and lawn-mower problems with lawn sizes of up to 12\u00d712. Results show that the algorithm is effective and scales better on these problems than either linear GP or simple stochastic hill-climbing.",
                "id": "022148"
            },
            {
                "title": "Extending the particle swarm algorithm to model animal foraging behaviour",
                "abstract": "The particle swarm algorithm [1] contains elements which map fairly strongly to the group-foraging problem in behavioural ecology: its continuous equations of motion include concepts of social attraction and communication between individuals, two of the general requirements for grouping behaviour [2]. Despite its socio-biological background, the particle swarm algorithm has rarely been applied to biological problems, largely remaining a technique used in classical optimisation problems. In this paper [3], we show how some simple adaptions to the standard algorithm can make it well suited for the foraging problem.",
                "id": "022149"
            },
            {
                "title": "Linear Genetic Programming Of Parsimonious Metaheuristics",
                "abstract": "We use a form of grammar-based linear Genetic Programming (GP) as a hyperheuristic, i.e., a search heuristic on the space of heuristics. This technique is guided by domain-specific languages that one designs taking inspiration from elementary components of specialised heuristics and metaheuristics for a domain. We demonstrate this approach for traveling-salesperson problems for which we test different languages, including one containing a looping construct. Experimentation with benchmark instances from the TSPLIB shows that the GP hyperheuristic routinely and rapidly produces parsimonious metaheuristics that find tours whose lengths are highly competitive with the best real-valued lengths from literature.",
                "id": "022150"
            },
            {
                "title": "On The Search Biases Of Homologuous Crossover In Linear Genetic Programming And Variable-length Genetic Algorithms",
                "abstract": "With a schema-theoretic approach and experi- ments we study the search biases produced by GP/GA homologous crossovers when applied to linear, variable-length representations. By specialising the schema theory for homologous crossovers we show that these operators are unbi- ased with respect to string length. Then, we pro- vide a fixed point for the schema evolution equa- tions where the population presents a statistically independent distribution of primitives. This is an important step towards generalising Geiringer's theorem and the notion of linkage equilibrium.",
                "id": "022151"
            },
            {
                "title": "A genetic programming approach for evolving highly-competitive general algorithms for envelope reduction in sparse matrices",
                "abstract": "Sparse matrices emerge in a number of problems in science and engineering. Typically the efficiency of solvers for such problems depends crucially on the distances between the first non-zero element in each row and the main diagonal of the problem's matrix -- a property assessed by a quantity called the size of the envelope of the matrix. This depends on the ordering of the variables (i.e., the order of the rows and columns in the matrix). So, some permutations of the variables may reduce the envelope size which in turn makes a problem easier to solve. However, finding the permutation that minimises the envelope size is an NP-complete problem. In this paper, we introduce a hyper-heuristic approach based on genetic programming for evolving envelope reduction algorithms. We evaluate the best of such evolved algorithms on a large set of standard benchmarks against two state-of-the-art algorithms from the literature and the best algorithm produced by a modified version of a previous hyper-heuristic introduced for a related problem. The new algorithm outperforms these methods by a wide margin, and it is also extremely efficient.",
                "id": "022152"
            },
            {
                "title": "Linear selection",
                "abstract": "We investigate a form of selection, linear selection, where parents are not selected independently. One form of dependent selection, semi-linear selection, where the parents are jointly selected with a probability proportional to the average of their selection probabilities, leads the GA to behave half-way between an algorithm driven by crossover and one driven by mutation.",
                "id": "022153"
            },
            {
                "title": "Exact Schema Theory for Genetic Programming and Variable-Length Genetic Algorithms with One-Point Crossover",
                "abstract": "A few schema theorems for genetic programming (GP) have been proposed in the literature in the last few years. Since they consider schema survival and disruption only, they can only provide a lower bound for the expected value of the number of instances of a given schema at the next generation rather than an exact value. This paper presents theoretical results for GP with one-point crossover which overcome this problem. First, we give an exact formulation for the expected number of instances of a schema at the next generation in terms of microscopic quantities. Due to this formulation we are then able to provide an improved version of an earlier GP schema theorem in which some (but not all) schema creation events are accounted for. Then, we extend this result to obtain an exact formulation in terms of macroscopic quantities which makes all the mechanisms of schema creation explicit. This theorem allows the exact formulation of the notion of effective fitness in GP and opens the way to future work on GP convergence, population sizing, operator biases, and bloat, to mention only some of the possibilities.",
                "id": "022154"
            },
            {
                "title": "Solution-Locked averages and solution-time binning in genetic programming",
                "abstract": "Averaging data collected in multiple independent runs across generations is the standard method to study the behaviour of GP. We show that while averaging may represent with good resolution GP's behaviour in the early stages of a run, it blurs later components of the dynamics. We propose two techniques to improve the situation: solution-locked averaging and solution-time binning. Results indicate that there are significant benefits in adopting these techniques.",
                "id": "022155"
            },
            {
                "title": "Grammar-based genetic programming for timetabling",
                "abstract": "We present a grammar-based genetic programming framework for the solving the timetabling problem via the evolution of constructive heuristics. The grammar used for producing new generations is based on graph colouring heuristics that have previously proved to be effective in constructing timetables as well as different slot allocation heuristics. The framework is tested on a widely used benchmarks in the field of exam time-tabling and compared with highly-tuned state-of-the-art approaches. Results shows that the framework is very competitive with other constructive techniques.",
                "id": "022156"
            },
            {
                "title": "Sub-tree swapping crossover and arity histogram distributions",
                "abstract": "Recent theoretical work has characterised the search bias of GP sub-tree swapping crossover in terms of program length distributions, providing an exact fixed point for trees with internal nodes of identical arity. However, only an approximate model (based on the notion of average arity) for the mixed-arity case has been proposed. This leaves a particularly important gap in our knowledge because multi-arity function sets are commonplace in GP and deep lessons could be learnt from the fixed point. In this paper, we present an accurate theoretical model of program length distributions when mixed-arity function sets are employed. The new model is based on the notion of an arity histogram, a count of the number of primitives of each arity in a program. Empirical support is provided and a discussion of the model is used to place earlier findings into a more general context.",
                "id": "022157"
            },
            {
                "title": "Addressing the envelope reduction of sparse matrices using a genetic programming system",
                "abstract": "Large sparse symmetric matrix problems arise in a number of scientific and engineering fields such as fluid mechanics, structural engineering, finite element analysis and network analysis. In all such problems, the performance of solvers depends critically on the sum of the row bandwidths of the matrix, a quantity known as envelope size. This can be reduced by appropriately reordering the rows and columns of the matrix, but for an $$N\\times N$$N N matrix, there are $$N!$$N! such permutations, and it is difficult to predict how each permutation affects the envelope size without actually performing the reordering of rows and columns. These two facts compounded with the large values of $$N$$N used in practical applications, make the problem of minimising the envelope size of a matrix an exceptionally hard one. Several methods have been developed to reduce the envelope size. These methods are mainly heuristic in nature and based on graph-theoretic concepts. While metaheuristic approaches are popular alternatives to classical optimisation techniques in a variety of domains, in the case of the envelope reduction problem, there has been a very limited exploration of such methods. In this paper, a Genetic Programming system capable of reducing the envelope size of sparse matrices is presented and evaluated against four of the best-known and broadly used envelope reduction algorithms. The results obtained on a wide-ranging set of standard benchmarks from the Harwell---Boeing sparse matrix collection show that the proposed method compares very favourably with these algorithms.",
                "id": "022158"
            },
            {
                "title": "On two approaches to image processing algorithm design for binary images using GP",
                "abstract": "In this paper we describe and compare two different approaches to design image processing algorithms for binary images using Genetic Programming (GP). The first approach is based on the use of mathematical morphology primitives. The second is based on Sub-Machine-Code GP: a technique to speed up and extend GP based on the idea of exploiting the internal parallelism of sequential CPUs. In both cases the objective is to find programs which can transform binary images of a certain kind into other binary images containing just a particular characteristic of interest. In particular, here we focus on the extraction of three different features in music sheets.",
                "id": "022159"
            }
        ]
    },
    {
        "id": "023",
        "input": "For an author who has written the paper with the title \"Science Fiction and the Reality of HCI: Inspirations, Achievements or a Mismatch\", which reference is related? Just answer with [1] or [2] without explanation. [1]: \"AUTHENTIC INTERACTIVE REENACTMENT OF CULTURAL HERITAGE WITH 3D VIRTUAL WORLDS AND ARTIFICIAL INTELLIGENCE\" [2]: \"Robots as dogs?: children's interactions with the robotic dog AIBO and a live australian shepherd\"",
        "profile": [
            {
                "title": "Variations on Multimedia Data Mining",
                "abstract": "Is multimedia data mining just a new combination of buzz-words or is it a new interdisciplinary field which not only incorporates methods and techniques from the relevant disciplines, but is also capable to produce new methodologies and influence related interdisciplinary fields. Rather than making an overview of existing methods and techniques, or presenting a particular technique, this paper aims to present some facets of multimedia data mining in the context of its potential to influence some relatively new interdisciplinary domains.",
                "id": "0230"
            },
            {
                "title": "Managing Collaboration in a Multiagent System",
                "abstract": "\n In a multiagent system agents negotiate with one another to distribute the work in an attempt to balance the incompatible\n goals of optimising the quality of the result, optimising system performance, maximising payoff, providing opportunities for\n poor performers to improve and balancing workload. This distribution of work is achieved by the delegation of responsibility\n for sub-processes by one agent to another. This leads to estimates of the probability that one agent is a better choice than\n another. The probability of delegating responsibility to an agent is then expressed as a function of these probability estimates.\n This apparently convoluted probabilistic method is easy to compute and gives good results in process management applications\n even when successive payoff measurements are unpredictably varied.\n \n ",
                "id": "0231"
            },
            {
                "title": "Narrowing the gap between humans and agents in e-commerce: 3D electronic institutions",
                "abstract": "Electronic Institution are regulated environments populated by autonomous software agents that perform tasks on behalf of users. Users, however, are reluctant in delegating full control of critical decisions to agents and prefer to make them on their own. In order to increase trust in agents we propose 3D Electronic Institutions as an environment inhabited by a heterogenous society of humans and agents. We present a novel approach that introduces humans to Electronic Institutions via 3D Virtual Worlds. Such a 3D Virtual World provides an immersive user interface that allows humans to observe the behavior of their agents as well as the intervention in the agents' decision process if necessary. We step beyond the agents view on Electronic Institutions, take a human-centered perspective and concentrate on the relation between humans and agents in the amalgamation of 3D Electronic Institutions.",
                "id": "0232"
            },
            {
                "title": "The Metaphor of the Face as an Interface for Communicating Non-Quantitative Information",
                "abstract": "In this paper we propose that a metaphor can be used to represent domains that are not easily quantifiable. Formal representation of the metaphor then can be used as an interface to communicate information about those domains between the human and the computer at a cognitive and visual level. We propose a model, which uses the metaphor of a human face as an interface data formatting system for the perception and evaluation of universal aesthetics.",
                "id": "0233"
            },
            {
                "title": "Managing Emergent Processes",
                "abstract": "Emergent processes are non-routine business processes whose execution is guided by the knowledge that emerges during a process\n instance. Managing emergent processes needs an intelligent agent that is guided not by a process goal, but by a continual\n in-flow of information, where the integrity of each chunk of information may be uncertain.\n ",
                "id": "0234"
            },
            {
                "title": "Visualization of relational structure among scientific articles",
                "abstract": "This paper describes an ongoing technique to collecting, mining, clustering and visualizing scientific articles and their relations in information science. We aim to provide a valuable tool for researchers in quick analyzing the relationship and retrieving the relevant documents. Our system, called CAVis, first automatically searches and retrieves articles from the Internet using given keywords. These articles are next converted into readable text documents. The system next analyzes these documents and it creates similarity matrix. A clustering algorithm is then applied to group the relevant papers into corresponding clusters. Finally, we provide a visual interface so that users can easily view the structure and the citing relations among articles. From the view, they can navigate through the collection as well as retrieve a particular article.",
                "id": "0235"
            },
            {
                "title": "A novel 3D interactive visualization for medical data analysis",
                "abstract": "This paper describes a new three-dimensional interactive visualization supporting large scale medical data analysis. We provide a simple and effective view so that the biomedical information can be easily perceived. Our visualization also embeds a novel mechanism to prevent disorientation by maintaining the orientation of objects and labels during the navigation. From the overview of patient population, users can select one, multiple patients or a group of patients to analyse further. We demonstrate our approach with the medical scientists working on a case study of childhood cancer patients, examplifying how they could use the tool to confirm existing hypotheses and to discover new scientific insights.",
                "id": "0236"
            },
            {
                "title": "Kernel: based visualisation of genes with the gene ontology",
                "abstract": "With the development of microarray--based high-- throughput technologies for examining genetic and biological information en masse, biologists are now faced with making sense of large lists of genes identified from their biological experiments. There is a vital need for \"system biology\" approaches which can allow biologists to see new or unanticipated potential relationships which will lead to new hypotheses and eventual new knowledge. Finding and understanding relationships in this data is a problem well suited to visualisation. We augment genes with their associated terms from the Gene Ontology and visualise them using kernel Principal Component Analysis with both specialised linear and Gaussian kernels. Our results show that this method can correctly visualise genes by their functional relationships and we describe the difference between using the linear and Gaussian kernels on the problem.",
                "id": "0237"
            },
            {
                "title": "Humans and agents in 3D electronic institutions",
                "abstract": "In this paper we propose the use of 3D Virtual Worlds for the visualization of Electronic Institutions. We show how 3D representation helps to open Electronic Institutions to human users and support co-learning between autonomous agents and humans.",
                "id": "0238"
            },
            {
                "title": "Negotiating Intelligently.",
                "abstract": "\n The predominant approaches to automating competitive interaction appeal to the central notion of a utility function that represents\n an agent\u2019s preferences. Agent\u2019s are then endowed with machinery that enables them to perform actions that are intended to\n optimise their expected utility. Despite the extent of this work, the deployment of automatic negotiating agents in real world\n scenarios is rare. We propose that utility functions, or preference orderings, are often not known with certainty; further,\n the uncertainty that underpins them is typically in a state of flux. We propose that the key to building intelligent negotiating\n agents is to take an agent\u2019s historic observations as primitive, to model that agent\u2019s changing uncertainty in that information,\n and to use that model as the foundation for the agent\u2019s reasoning. We describe an agent architecture, with an attendant theory,\n that is based on that model. In this approach, the utility of contracts, and the trust and reliability of a trading partner\n are intermediate concepts that an agent may estimate from its information model. This enables us to describe intelligent agents\n that are not necessarily utility optimisers, that value information as a commodity, and that build relationships with other\n agents through the trusted exchange of information as well as contracts.\n \n ",
                "id": "0239"
            },
            {
                "title": "Interactive visualization with user perspective: a new concept",
                "abstract": "With an astonishing amount of data generated for processing on a daily basic, it is essential to provide an effective methodology for understanding, reasoning and supporting decision making of large information spaces. This paper presents a new concept that provides an intelligent and interactive visualization in supporting large scale analysis. This aims to provide a much greater flexibility and control for the users to interactively customize the visualizations according to their preferences. A simple prototype is also presented to demonstrate the concept on hierarchical structures.",
                "id": "02310"
            },
            {
                "title": "Mediation = Information Revelation + Analogical Reasoning",
                "abstract": "This paper presents an initial study of the relevant issues on the development of an automated mediation agent. The work is conducted within the `curious negotiator' framework [1] . The paper demonstrates that mediation is a knowledge intensive process that integrates information revelation and analogical reasoning. The introduced formalism is used to demonstrate how via revealing the appropriate information and reshaping the set of issues of the disputing parties mediation can succeed. The paper presents MediaThor - a mediating agent that utilises past experiences and information from negotiating parties to mediate disputes and change the positions of negotiating parties.",
                "id": "02311"
            },
            {
                "title": "Enhancing the believability of embodied conversational agents through environment-, self- and interaction-awareness",
                "abstract": "Research on embodied conversational agents' reasoning and actions has mostly ignored the external environment. This papers argues that believability of such agents is tightly connected with their ability to relate to the environment during a conversation. This ability, defined as awareness believability, is formalised in terms of three components - environment-, self- and interaction-awareness. The paper presents a method enabling virtual agents to reason about their environment, understand the interaction capabilities of other participants, own goals and current state of the environment, as well as to include these elements into conversations. We present the implementation of the method and a case study, which demonstrates that such abilities improve the overall believability of virtual agents.",
                "id": "02312"
            },
            {
                "title": "From Ad-hoc to Engineered Collaboration in Virtual Workspaces.",
                "abstract": "Distributed collaboration over the Internet has become increasingly common in recent years, supported by various technologies such as virtual workspace systems. Often such collaboration is ad-hoc, and virtual workspaces are set up anew for each new instance of collaboration. We propose that much of the ad-hoc collaboration can be captured and transformed into patterns for reuse in future collaboration. This paper presents the results of the past five years of our work in this area. We introduce the notion of patterns of virtual collaboration; present a framework for extracting patterns of work in virtual workspace systems; and introduce an information model of virtual collaboration. We then present an overview of our data and process mining methods and reverse engineering techniques for discerning work processes carried out through virtual workspace systems. Finally we present our visual mining techniques that we use to discern aspects of work processes in virtual workspaces.",
                "id": "02313"
            },
            {
                "title": "Extracting and Explaining Biological Knowledge in Microarray Data",
                "abstract": "This paper describes a method of clustering lists of genes mined from a microaxray dataset using functional information from the Gene Ontology. The method uses relationships between terms in the ontology both to build clusters and to extract meaningful cluster descriptions. The approach is general and may be applied to assist explanation of other datasets associated with ontologies.",
                "id": "02314"
            },
            {
                "title": "Virtual Institutions: Normative Environments Facilitating Imitation Learning in Virtual Agents",
                "abstract": "The most popular two methods of extending the intelligence of virtual agents are explicit programming of the agents' decision making apparatus and learning agent behaviors from humans or other agents. The major obstacles of the existing approaches are making the agent understand the environment it is situated in and interpreting the actions and goals of other participants. Instead of trying to solve these problems we propose to formalize the environment in a way that these problems are minimized. The proposed solution, called Virtual Institutions, facilitates formalization of participants' interactions inside Virtual Worlds, helping the agent to interpret the actions of other participants, understand its options and determine the goals of the principal that is conducting the training of the agent. Such formalization creates facilities to express the principal's goals during training, as well as establishes a way to communicate desires of the human to the agent once the training is completed.",
                "id": "02315"
            },
            {
                "title": "A Strategic Analytics Methodology",
                "abstract": "Businesses are experiencing difficulties with integrating data-mining analytics with decision-making and action. At present, two data-mining methodologies play a central role in enabling data-mining as a process. However, the results of reflecting on the application of these methodologies in real-world business cases against specific criteria indicate that both methodologies provide limited integration with business decision-making and action. In this paper we demonstrate the impact of these limitations on a Telco customer retention management project for a global mobile phone company. We also introduce a data-mining and analytics project methodology with improved business integration - the Strategic Analytics Methodology (SAM). The advantage of the methodology is demonstrated through its application in the same project, and comparison of the results.",
                "id": "02316"
            },
            {
                "title": "Using Entropy as a Measure of Acceptance for Multi-label Classification",
                "abstract": "Multi-label classifiers allow us to predict the state of a set of responses using a single model. A multi-label model is able to make use of the correlation between the labels to potentially increase the accuracy of its prediction. Critical applications of multi-label classifiers (such as medical diagnoses) require that the system's confidence in prediction also be provided with the multi-label prediction. The specialist then uses the measure of confidence to assess whether to accept the system's prediction. Probabilistic multi-label classification provides a categorical distribution over the set of responses, allowing us to observe the distribution, select the most probable response, and obtain an indication of confidence by the shape of the distribution. In this article, we examine if normalised entropy, a parameter of the probabilistic multi-label response distribution, correlates with the accuracy of the prediction and therefore can be used to gauge confidence in the system's prediction. We found that for all three methods examined on each data set, the accuracy increases for the majority of the observations where the normalised entropy threshold decreases, showing that we can use normalised entropy to gauge a systems confidence, and hence use it as a measure of acceptance.",
                "id": "02317"
            },
            {
                "title": "Rectangle orientation in area judgment task for treemap design",
                "abstract": "Prior works on treemaps have mainly focused on developing the new layouts. The existing treemaps generated from various algorithms require careful examination on design parameter. However, current research does not provide usability studies of treemap guidelines on effectiveness of design parameters. Hence, selecting the most effective parameter for certain type of task is primarily based on intuition preference of visualization designer. For example, in the existing research, there is insufficient guidance on orientation for treemap design yet. Therefore, the impact of orientation remains unclear in visual analysis tasks performance. The contribution of this paper is to assess the effect of orientation in visual data analysis process so that we will further investigate treemap design guidance.",
                "id": "02318"
            },
            {
                "title": "The co-creation machine: managing co-creative processes for the crowd",
                "abstract": "Co-creative processes have spawned successes such as Wikipedia. They are also used to draw innovative ideas from consumers to producers, and from voters to government. This paper describes the initial stages of a collaboration between two Sydney-based universities to build a customisable co-creative process management system. The system has embedded intelligence that will make it easy and enjoyable to use. It will enable these powerful systems to be quickly deployed on the Internet to the benefit of the universities as well as industry and government. The innovation in the design of this project is that it is founded on normative multiagent systems that are an established technology for (business) process management but have yet to be deployed to support the co-creative process.",
                "id": "02319"
            },
            {
                "title": "Making informed automated trading a reality",
                "abstract": "Three core technologies are needed to fully automate the trading process: data mining, intelligent trading agents and virtual institutions in which informed trading agents can trade securely both with each other and with human agents in a natural way. This paper describes a demonstrable prototype e-trading system that integrates these three technologies and is available on the World Wide Web. This is part of a larger project that aims to make informed automated trading a reality.",
                "id": "02320"
            },
            {
                "title": "Visual analytics of clinical and genetic datasets of acute lymphoblastic leukaemia",
                "abstract": "This paper presents a novel visual analytics method that incorporates knowledge from the analysis domain so that it can extract knowledge from complex genetic and clinical data and then visualizing them in a meaningful and interpretable way. The domain experts that are both contributors to formulating the requirements for the design of the system and the actual user of the system include microbiologists, biostatisticians, clinicians and computational biologists. A comprehensive prototype has been developed to support the visual analytics process. The system consists of multiple components enabling the complete analysis process, including data mining, interactive visualization, analytical views, gene comparison. A visual highlighting method is also implemented to support the decision making process. The paper demonstrates its effectiveness on a case study of childhood cancer patients.",
                "id": "02321"
            },
            {
                "title": "Informed Recommender: Basing Recommendations on Consumer Product Reviews",
                "abstract": "Consumer reviews, opinions, and shared experiences in using a product are a powerful source of information that recommender systems can use. Despite the importance and value of such information, no comprehensive mechanism formalizes the opinions' selection, retrieval, and use owing to the difficulty of extracting information from text data. A new recommender system prioritizes consumer product reviews on the basis of the reviewer's level of expertise in using a product. The system uses text mining techniques to map each piece of each review comment into an ontology. Using consumer reviews also helps solve the cold-start problem that plagues traditional approaches. This article is part of a special issue on Recommender Systems.",
                "id": "02322"
            },
            {
                "title": "Intelligent agents for multi-issue auctions and bidding",
                "abstract": "An approach to auctions and bidding is founded on observations and expectations of the opponents' behavior and not on assumptions concerning the opponents' motivations or internal reasoning. The approach draws ideas from information theory. A bidding agent employs maximum entropy inference to determine its actions on the basis of this uncertain data. Maximum entropy inference may be applied both to multi-issue and to single-issue negotiation. Multi-issue variants of the four common auction mechanisms are discussed.",
                "id": "02323"
            },
            {
                "title": "Digging in the details: a case study in network data mining",
                "abstract": "Network Data Mining builds network linkages (network models) between myriads of individual data items and utilizes special algorithms that aid visualization of \u2018emergent' patterns and trends in the linkage. It complements conventional and statistically based data mining methods. Statistical approaches typically flag, alert or alarm instances or events that could represent anomalous behavior or irregularities because of a match with pre-defined patterns or rules. They serve as \u2018exception detection' methods where the rules or definitions of what might constitute an exception are able to be known and specified ahead of time. Many problems are suited to this approach. Many problems however, especially those of a more complex nature, are not well suited. The rules or definitions simply cannot be specified; there are no known suspicious transactions. This paper presents a human-centered network data mining methodology. A case study from the area of security illustrates the application of the methodology and corresponding data mining techniques. The paper argues that for many problems, a \u2018discovery' phase in the investigative process based on visualization and human cognition is a logical precedent to, and complement of, more automated \u2018exception detection' phases.",
                "id": "02324"
            },
            {
                "title": "'Believable' Agents Build Relationships on the Web.",
                "abstract": "In this paper we present the Believable Negotiator - the formalism behind a Web business negotiation technology that treats relationships as a commodity. It supports relationship building, maintaining, evolving, and passing to other agents, and utilises such relationships in agent interaction. The Believable Negotiator also takes in account the \"relationship gossip\" - the information, supplied by its information providing agents, about the position of respective agents in their networks of relationships beyond the trading space. It is embodied in a 3D web space, that is translated to different virtual worlds platforms, enabling the creation of an integrated 3D trading space, geared for Web 3.0.",
                "id": "02325"
            },
            {
                "title": "Electronic Trading Environments for Web 3.0",
                "abstract": "This paper proposes that electronic marketplaces for Web 3.0 can be described through three metaphors: \"marketplaces where people are\", \"marketplaces that are alive and engaging\", and \"market places where information is valuable and useful\". The paper presents the core technologies that enable the perceivable reality of electronic marketplaces. It describes a demonstrable prototype of a Web-based electronic marketplace that integrates these technologies. This is part of a larger project that aims to make informed automated trading an enjoyable reality of Web 3.0.",
                "id": "02326"
            },
            {
                "title": "Playing the e-business game in 3D virtual worlds",
                "abstract": "In this paper we present an integrated, game-like e-Business environment that follows the role model of Massively Multi-User Online Role-Playing Games (MMORPGs). The interface is realized as a 3D virtual world using affordable game engine technology. Our environment provides a platform for conducting business and it is supposed to be a community facilitator to create and establish a lively and sustainable online community involving both, providers and consumers. It is information-rich and multimedia-based offering transparent access to disparate information sources.",
                "id": "02327"
            },
            {
                "title": "Knowledge discovery from multimedia case libraries",
                "abstract": "Case-based reasoning and knowledge discovery are two independent fields in Al, which together can provide a design support environment for structural enigineers during the synthesis of new designs. Case-based reasoning relies on the representation of previous design cases for reminding designers of relevant past experience. Knowledge discovery is a way of finding patterns in data that can be considered new or generalised knowledge. By combining the two Al techniques, a case library can be the source of past episodic information as well as a source for discovering new patterns. We discuss the development of a multimedia library of structural design cases and the use of knowledge discovery techniques on multmimedia data to provide an environment for assisting in the development of new structural designs. We demonstrate the text analysis part of knowledge discovery from the SAM multimedia case library.",
                "id": "02328"
            },
            {
                "title": "Angular Treemaps - A New Technique for Visualizing and Emphasizing Hierarchical Structures",
                "abstract": "Space-filling visualization techniques have proved their capability in visualizing large hierarchical structured data. However, most existing techniques restrict their partitioning process in vertical and horizontal direction only, which cause problem with identifying hierarchical structures. This paper presents a new space-filling method named Angular Treemaps that relax the constraint of the rectangular subdivision. The approach of Angular Treemaps utilizes divide and conquer paradigm to visualize and emphasize large hierarchical structures within a compact and limited display area with better interpretability. Angular Treemaps generate various layouts to highlight hierarchical sub-structure based on user's preferences or system recommendations. It offers flexibility to be adopted into a wider range of applications, regarding different enclosing shapes. Preliminary usability results suggest user's performance by using this technique is improved in locating and identifying categorized analysis tasks.",
                "id": "02329"
            },
            {
                "title": "From graphs to Euclidean virtual worlds: visualization of 3D electronic institutions",
                "abstract": "In this paper we propose an algorithm for automatic transformation of a graph into a 3D Virtual World and its Euclidean map, using the rectangular dualization technique. The nodes of the initial graph are transformed into rooms, the connecting arcs between nodes determine which rooms have to be placed next to each other and define the positions of the doors connecting those rooms. The proposed algorithm is general enough to be used for automatic generation of 3D Virtual Worlds representation of any planar graph, however, our research is particulary focused on the automatic generation of 3D Electronic Institutions from the Performative Structure graph.",
                "id": "02330"
            },
            {
                "title": "Investigating the Evolution of Electronic Markets",
                "abstract": "Markets evolve through 'entrepreneurial' intervention which is based on intuition and on timely information. An electronic market has been constructed in the laboratory as a collaborative virtual environment to identify timely entrepreneurial information for e-markets. This information is distilled from individual signals in the markets themselves and from signals observed on the Internet. Distributed, concurrent, time-constrained data mining methods are managed using business process management technology to extract timely, reliable information from this inherently unreliable environment.",
                "id": "02331"
            },
            {
                "title": "Visualising the Dynamics of Unfolding Interactions on Mobile Devices",
                "abstract": "Human activities in various domains include interactions. In some areas, like healthcare, there is an opportunity to collect rich data about such interactions. The utilisation of such data in order to extract information about the dynamics of unfolding of interactions and visualising such information on mobile devices is the focus of this paper. The paper presents a methodology for designing visual representations about interactions for mobile devices. The shape and behaviour of the elements of such visualisations utilise elements of human movement. The paper presents an instance of such visualisation on mobile devices and its utilisation on case studies of interactions in healthcare.",
                "id": "02332"
            },
            {
                "title": "Design ontology in context \u2014 a situated cognition approach to conceptual modelling",
                "abstract": "If we take a situated view of cognition, human thought and action are inextricably connected and affected by the context. It is not just the external environment that will affect the context but that thinking itself modifies further action and context occurs at a conceptual level that exists within a social setting. Thus, a situated view of knowledge necessitates knowledge acquisition techniques which handle change. This is particularly true of design knowledge where the design will change as more experience is gained and the changing model will itself change the perception of a design while designing. The approach described in this paper is based on the view that knowledge is always evolving and the premise that it is not easy to capture or evaluate a conceptual model. The alternative offered is based on the combined use of cases, ripple-down rules (RDR), formal concept analysis (FCA) and the Activity/Space (A/S) ontology. Cases are design episodes and used to motivate the capture of rules in a simple user-driven manner. Cases ground the KBS in the real world and provide the context in which the knowledge applies. Rules are the indexes by which the cases are retrieved. Using FCA, we are able to build an abstraction hierarchy of the rules and cases. To facilitate comparison and validation we use A/S design ontology to acquire a consistently organised set of cases. This ontology provides a common structure and shared set of descriptive terms. The ease with which the knowledge is acquired and maintained using RDR, the use of a dynamic design ontology and the automatic generation of conceptual models using FCA allows for the continual evolution of the KBS in keeping with the notion that knowledge is continually evolving and \u2018made-up\u2019 to fit the situation.",
                "id": "02333"
            },
            {
                "title": "Recommender System Based on Consumer Product Reviews",
                "abstract": "Consumer reviews, opinions and shared experiences in the use of a product is a powerful source of information about consumer preferences that can be used in recommender systems. Despite the importance and value of such information, there is no comprehensive mechanism that formalizes the opinions selection and retrieval process and the utilization of retrieved opinions due to the difficulty of extracting information from text data. In this paper, a new recommender system that is built on consumer product reviews is proposed. A prioritizing mechanism is developed for the system. The proposed approach is illustrated using the case study of a recommender system for digital cameras.",
                "id": "02334"
            },
            {
                "title": "Second order probabilistic models for within-document novelty detection in academic articles",
                "abstract": "It is becoming increasingly difficult to stay aware of the state-of-the-art in any research field due to the exponential increase in the number of academic publications. This problem effects authors and reviewers of submissions to academic journals and conferences, who must be able to identify which portions of an article are novel and which are not. Therefore, having a process to automatically judge the flow of novelty though a document would assist academics in their quest for truth. In this article, we propose the concept of Within Document Novelty Location, a method of identifying locations of novelty and non-novelty within a given document. In this preliminary investigation, we examine if a second order statistical model has any benefit, in terms of accuracy and confidence, over a simpler first order model. Experiments on 928 text sequences taken from three academic articles showed that the second order model provided a significant increase in novelty location accuracy for two of the three documents. There was no significant difference in accuracy for the remaining document, which is likely to be due to the absence of context analysis.",
                "id": "02335"
            },
            {
                "title": "Power walk: revisiting the random surfer.",
                "abstract": "Measurement of graph centrality provides us with an indication of the importance or popularity of each vertex in a graph. When dealing with graphs that are not centrally controlled (such as the Web, social networks and academic citation graphs), centrality measure must 1) correlate with vertex importance/popularity, 2) scale well in terms of computation, and 3) be difficult to manipulate by individuals. The Random Surfer probability transition model, combined with Eigenvalue Centrality produced PageRank, which has shown to satisfy the required properties. Existing centrality measures (including PageRank) make the assumption that all directed edges are positive, implying an endorsement. Recent work on sentiment analysis has shown that this assumption is not valid. In this article, we introduce a new method of transitioning a graph, called Power Walk, that can successfully compute centrality scores for graphs with real weighted edges. We show that it satisfies the desired properties, and that its computation time and centrality ranking is similar to when using the Random Surfer model for non-negative matrices. Finally, stability and convergence analysis shows us that both stability and convergence when using the power method, are dependent on the Power Walk parameter \u03b2.",
                "id": "02336"
            },
            {
                "title": "Training Believable Agents In 3d Electronic Business Environments Using Recursive-Arc Graphs",
                "abstract": "Using 3D Virtual Worlds for commercial activities on the Web and the development of human-like sales assistants operating in such environments are ongoing trends of E-Commerce. The majority of the existing approaches oriented towards the development of such assistants are agent-based and are focused on explicit programming of the agents' decision making apparatus. While effective in some very specific situations, these approaches often restrict agents' capabilities to adapt to the changes in the environment and learn new behaviors. In this paper we propose an implicit training method that can address the aforementioned drawbacks. In this method we formalize the virtual environment using Electronic Institutions and make the agent use these formalizations for observing a human principle and learning believable behaviors from the human. The training of the agent can be conducted implicitly using the specific data structures called recursive-arc graphs.",
                "id": "02337"
            },
            {
                "title": "Power Conservation in Wired Wireless Networks.",
                "abstract": "A joint university / industry collaboration has designed a system for conserving power in LTE (Long Term Evolution) wireless networks for mobile devices such as phones. The solution may be applied to any wireless technology in which all stations are wired to a backbone (e.g. it may not be applied to an 802.11 mash). This paper describes the solution method that is based on a distributed multiagent system in which one agent is associated with each and every station. Extensive simulations show that the system delivers robust performance: the computational overhead is within acceptable limits, the solution is stable in the presence of unexpected fluctuations in demand patterns, and scalability is achieved by the agents making decisions locally.",
                "id": "02338"
            },
            {
                "title": "Informed Agents Integrating Data Mining and Agency",
                "abstract": "We propose that the key to building informed negotiating agents is to develop a form of agency that integrates naturally with data mining and information sources. These agent's take their historic observations as primitive, model their changing uncertainty in that information, and use that model as the foundation for the agent's reasoning. We describe an agent architecture, with an attendant theory, that is based on that model. In this approach, the utility of contracts, and the trust and reliability of a trading partner are intermediate concepts that an agent may estimate from its information model.",
                "id": "02339"
            },
            {
                "title": "Managing power conservation in wireless networks",
                "abstract": "Amajor project is investigating methods for conserving power in wireless networks. A component of this project addresses methods for predicting whether the user demand load in each zone of a network is increasing, decreasing or approximately constant.These predictions are then fed into the power regulation system. This paper describes a real-time predictive model of network traffic load which is derived from experiments on real data. This model combines a linear regression based model and a highly reactive model that are applied to real-time data that is aggregated at two levels of granularity. The model gives excellent performance predictions when applied to network traffic load data.",
                "id": "02340"
            },
            {
                "title": "Opening new dimensions for e-Tourism",
                "abstract": "In this paper we describe an e-Tourism environment that takes a community-driven approach to foster a lively society of travelers who exchange travel experiences, recommend tourism destinations or just listen to catch some interesting gossip. Moreover, business transactions such as booking a trip or getting assistance from travel advisors or community members are constituent parts of this environment. All these happen in an integrated, game-like e-Business application where each e-Tourist is impersonated as an avatar. More precisely, we apply 3D Electronic Institutions, a framework developed and employed in the area of multi-agent systems, to the tourism domain. The system interface is realized by means of a 3D game engine that provides sophisticated 3D visualization and enables humans to interact with the environment. We present \u201citchy feet\u201d, a prototype implementing this 3D e-Tourism environment to showcase first visual impressions. This new environment is a perfect research playground for examining heterogeneous societies comprising humans and software agents, and their relationship in e-Tourism.",
                "id": "02341"
            },
            {
                "title": "Mining 'Living' Data - Providing Context Information to a Negotiation Process",
                "abstract": "Negotiation is the process whereby two (or more) individual agents with conflicting interests reach a mutually beneficial agreement on a set of issues. In negotiation the exchange of information is as important as the exchange of offers. During a negotiation, an agent may actively acquire contextual information that it may, or may not, choose to place on the negotiation table. Contextual information includes information that can be extracted either from 'historical' data (e.g. the results of previous negotiations such as market information on deals struck or being struck) or from data that is coming during the negotiation process (e.g. the behaviour of the negotiating parties like a view on the opponent's current negotiation strategy). In addition, contextual information can be extracted from general sources, like companies white papers and news feeds (e.g. general information on the opponent - reputation, 'future stability', etc.; relations with other parties in the area of negotiation). This paper presents a framework for on-line data mining that supports the identification of the above-mentioned information and providing it on negotiator's demand. The paper discusses the issues related with timely information discovery and combining the output of the data mining agents into a meaningful and valuable recommendation to the negotiator. The integration of negotiation theory and data mining enables such system to discover and exploit negotiation opportunities.",
                "id": "02342"
            },
            {
                "title": "Exchange rate modelling using news articles and economic data",
                "abstract": "This paper provides a framework of using news articles and economic data to model the exchange rate changes between Euro and US dollars. Many studies have conducted on the approach of regressing exchange rate movement using numerical data such as macroeconomic indicators. However, this approach is effective in studying the long term trend of the movement but not so accurate in short to middle term behaviour. Recent research suggests that the market daily movement is the result of the market reaction to the daily news. In this paper, it is proposed to use text mining methods to incorporate the daily economic news as well as economic and political events into the prediction model. While this type of news is not included in most of existing models due to its non-quantitative nature, it has important influence in short to middle terms of market behaviour. It is expected that this approach will lead to an exchange rate model with improved accuracy.",
                "id": "02343"
            },
            {
                "title": "What makes virtual agents believable?",
                "abstract": "AbstractIn this paper we investigate the concept of believability and make an attempt to isolate individual characteristics features that contribute to making virtual characters believable. As the result of this investigation we have produced a formalisation of believability and based on this formalisation built a computational framework focused on simulation of believable virtual agents that possess the identified features. In order to test whether the identified features are, in fact, responsible for agents being perceived as more believable, we have conducted a user study. In this study we tested user reactions towards the virtual characters that were created for a simulation of aboriginal inhabitants of a particular area of Sydney, Australia in 1770 A.D. The participants of our user study were exposed to short simulated scenes, in which virtual agents performed some behaviour in two different ways while possessing a certain aspect of believability vs. not possessing it. The results of the study indicate that virtual agents that appear resource bounded, are aware of their environment, own interaction capabilities and their state in the world, agents that can adapt to changes in the environment and exist in correct social context are those that are being perceived as more believable. Further in the paper we discuss these and other believability features and provide a quantitative analysis of the level of contribution for each such feature to the overall perceived believability of a virtual agent.",
                "id": "02344"
            },
            {
                "title": "Identification Of Important News For Exchange Rate Modeling",
                "abstract": "Associating the pattern in text data with the pattern with time series data is a novel task. In this paper, an approach that utilizes the features of the time series data and domain knowledge is proposed and used to identify the patterns for exchange rate modeling. A set of rules to identify the patterns are firstly specified using domain knowledge. The text data are then associated with the exchange rate data and pre-classified according to the trend of the time series. The rules are further refined by the characteristics of the pre-classified data. Classification solely based on time series data requires precise and timely data, which are difficult to obtain from financial market reports. On the other hand, domain knowledge is often very expensive to be acquired and often has a modest inter-rater reliability. The proposed method combines both methods, leading to a \"grey box\" approach that can handle the data with some time delay and overcome these drawbacks.",
                "id": "02345"
            },
            {
                "title": "An integrative framework for knowledge extraction in collaborative virtual environments",
                "abstract": "Collaborative virtual environments are becoming an intrinsic part of professional practices. In addition to providing collaboration support, they have the potential to collect vast amounts of data about collaborative activities. The aim of this research is to utilize this data effectively, extract meaningful insights out of it and feeding discovered knowledge back into the environment. The paper presents a framework for integrating knowledge discovery techniques with collaborative virtual environments, starting from early conceptual development. Discovered patterns are deposited in an organizational memory which makes these available within the virtual environment. Two examples of the application of the framework are included.",
                "id": "02346"
            },
            {
                "title": "Leading conversations: Communication behaviours of emergent leaders in virtual teams",
                "abstract": "Virtual teams and their leaders are key players in global organisations. Using teams of workers dispersed temporally and geographically has changed the way people work in groups and has redefined the nature of teamwork. Emergent leadership issues in computer-mediated communication are vital today because of the increasing prevalence of the virtual organisation, the flattening of organisational structures and the corresponding interest in managing virtual groups and teams. This paper examines the communication behaviours of participants in two different case studies to determine if number, length and content of messages are sufficient criteria to identify emergent leaders in asynchronous and synchronous environments. The methodology used can be embedded in collaborative virtual environments as technology for detecting potential leaders.",
                "id": "02347"
            },
            {
                "title": "Combine Vector Quantization And Support Vector Machine For Imbalanced Datasets",
                "abstract": "In cases of extremely imbalanced dataset with high dimensions, standard machine learning techniques tend to be overwhelmed by the large classes. This paper rebalances skewed datasets by compressing the majority class. This approach combines Vector Quantization and Support Vector Machine and constructs a new approach, VQ-SVM, to rebalance datasets without significant information loss. Some issues, e.g. distortion and support vectors, have been discussed to address the trade-off between the information loss and undersampling. Experiments compare VQ-SVM and standard SVM on some imbalanced datasets with varied imbalance ratios, and results show that the performance of VQ-SVM is superior to SVM, especially in case of extremely imbalanced large datasets.",
                "id": "02348"
            },
            {
                "title": "Unlocking the Complexity of Port Data With Visualization",
                "abstract": "Interactive visual analysis can support displaying complex multidimensional data. We developed a novel system, which can incorporate domain knowledge and visual guidelines, to present complex logistical data. This paper presents the details of the development process of that system. Diagrammatic visualization approach was adopted for presenting multidimensional port data, in relation to events. The paper discusses case studies with land-side data and wharf-side data obtained from Port Botany in Sydney, Australia. We conducted a usability study with 20 students that compared performance with our visualization against a traditional bar chart. Task completion was significantly shorter with the diagrammatic visualization, and the visualization was rated higher in overall user preference.",
                "id": "02349"
            },
            {
                "title": "Comparison of visualization methods of genome-wide SNP profiles in childhood acute lymphoblastic leukaemia",
                "abstract": "Data mining and knowledge discovery have been applied to datasets in various industries including biomedical data. Modelling, data mining and visualization in biomedical data address the problem of extracting knowledge from large and complex biomedical data. The current challenge of dealing with such data is to develop statistical-based and data mining methods that search and browse the underlying patterns within the data. In this paper, we employ several data reduction methods for visualizing genome--wide Single Nucleotide Polymorphism (SNP) datasets based on state--of--art data reduction techniques. Visualization approach has been selected based on the trustworthiness of the resultant visualizations. To deal with large amounts of genetic variation data, we have chosen to apply different data reduction methods to deal with the problem induced by high dimensionality. Based on the trustworthiness metric we found that neighbour Retrieval Visualizer (NeRV) outperformed other methods. This method optimizes the retrieval quality of Stochastic neighbour Embedding. The quality measure of the visualization (i.e. NeRV) showed excellent results, even though the dataset was reduced from 13917 to 2 dimensions. The visualization results will assist clinicians and biomedical researchers in understanding the systems biology of patients and how to compare different groups of clusters in visualizations.",
                "id": "02350"
            },
            {
                "title": "A methodology for developing multiagent systems as 3D electronic institutions",
                "abstract": "In this paper we propose viewing Virtual Worlds as openMultiagent Systems and propose the 3D Electronic Institutions methodologyfor their development. 3D Electronic Institutions are VirtualWorlds with normative regulation of interactions. More precisely, themethodology we propose here helps in separating the development ofVirtual Worlds based on the concept of 3D Electronic Institutions intotwo independent phases: specification of the institutional rules and designof the 3D interaction environment. The new methodology is suppliedwith a set of graphical tools that support the development process onevery level, from specification to deployment. The resulting system facilitatesthe direct integration of humans into Multi-Agent Systems as theyparticipate by driving an avatar in the generated 3D environment andinteracting with other humans or software agents, while the institutionensures the validity of their interactions.",
                "id": "02351"
            },
            {
                "title": "Network data mining: methods and techniques for discovering deep linkage between attributes",
                "abstract": "Network Data Mining identifies emergent networks between myriads of individual data items and utilises special algorithms that aid visualisation of 'emergent' patterns and trends in the linkage. It complements conventional data mining methods, which assume the independence between the attributes and the independence between the values of these attributes. These techniques typically flag, alert or alarm instances or events that could represent anomalous behaviour or irregularities because of a match with pre-defined patterns or rules. They serve as 'exception detection' methods where the rules or definitions of what might constitute an exception are able to be known and specified ahead of time. Many problems are suited to this approach. Many problems however, especially those of a more complex nature, are not well suited. The rules or definitions simply cannot be specified. For example, in the analysis of transaction data there are no known suspicious transactions. This chapter presents a human-centred network data mining methodology that addresses the issues of depicting implicit relationships between data attributes and/or specific values of these attributes. A case study from the area of security illustrates the application of the methodology and corresponding data mining techniques. The chapter argues that for many problems, a 'discovery' phase in the investigative process based on visualisation and human cognition is a logical precedent to, and complement of, more automated 'exception detection' phases.",
                "id": "02352"
            },
            {
                "title": "Classify Unexpected News Impacts to Stock Price by Incorporating Time Series Analysis into Support Vector Machine",
                "abstract": "The paper discusses an approach of using traditional time series analysis, as domain knowledge, to help the data-preparation of support vector machine for classifying documents. Classifying unexpected news impacts to the stock prices is selected as a case study. As a result, we present a novel approach for providing approximate answers to classifying news events into simple three categories. The process of constructing training datasets is emphasized, and some time series analysis techniques are utilized to pre-process the dataset. A rule-base associated with the net-of-market return and piecewise linear fitting constructs the training data set. A classifier mainly built by support vector machine uses the training data set to extract the interrelationship between unexpected news events and the stock price movements.",
                "id": "02353"
            },
            {
                "title": "Incorporating prior domain knowledge into a kernel based feature selection algorithm",
                "abstract": "This paper proposes a new method of incorporating prior domain knowledge into a kernel based feature selection algorithm. The proposed feature selection algorithm combines the Fast Correlation-Based Filter (FCBF) and the kernel methods in order to uncover an optimal subset of features for the support vector regression. In the proposed algorithm, the Kernel Canonical Correlation Analysis (KCCA) is employed as a measurement of mutual information between feature candidates. Domain knowledge in forms of constraints is used to guide the tuning of the KCCA. In the second experiments, the audit quality research carried by Yang Li and Donald Stokes [1] provides the domain knowledge, and the result extends the original subset of features.",
                "id": "02354"
            },
            {
                "title": "The city of uruk: teaching ancient history in a virtual world",
                "abstract": "In this paper we show how 3D Virtual Worlds can be utilised for teaching ancient history. Our goal is to build an accurate replica of one of humanityu0027s first cities in a 3D Virtual World and provide history students with facilities to explore the virtual city and learn about its past in the simulated 3D environment. Unlike the majority of similar historical reconstructions, an important feature of our approach is having virtual agents that are capable of simulating everyday life of ancient inhabitants, which includes common tasks like eating, sleeping, working and communicating with one another. In order to offer educational value the agents act as autonomous tutors and are capable of sensing the students through their avatars and interact with them both in terms of performing joint actions and through verbal communication. We show how such virtual environments can be built, explain the technology behind its artificial intelligence controlled population and highlight the corresponding educational benefits. To validate the impact of using the 3D environment and virtual agents in history education we conducted a case study that confirmed the beneficial educational aspects of our approach.",
                "id": "02355"
            },
            {
                "title": "Enterprise architecture modelling using elastic metaphors",
                "abstract": "Despite the hype surrounding enterprise architectures, they have delivered little on their promise. In this paper, we argue that enterprise architectures built using component-based frameworks are fundamentally flawed, in that they model the enterprise as a set of independent structures with discrete boundaries. Disparate concrete metaphors are used to describe each of these structures, with the result that enterprise architectures can only achieve partial success, at best, in providing a unified view of the enterprise.This paper introduces the concept of 'elastic metaphors' as society-sourced metaphors for the conceptual modelling of information systems. By modelling the organisation using elastic metaphors sourced from naturally occurring enterprise structures, the enterprise architecture approach presented in this paper avoids the framework segmentation problem.",
                "id": "02356"
            },
            {
                "title": "Travel Agents vs. Online Booking: Tackling the Shortcomings of Nowadays Online Tourism Portals",
                "abstract": "Abstract: In this paper we present the findings of a study that aims at identifying the reasons that letmany people still rely on traditional travel agents instead of booking their trips online. Theprime motivation for investigating this issue is that it is impossible to have direct experiencewith the product prior to consumption in the domain of tourism. The Internet provides apowerful environment for the creation of virtual representations of tourism destinationsallowing indirect experience that...",
                "id": "02357"
            },
            {
                "title": "Believable Electronic Trading Environments on the Web",
                "abstract": "Contemporary Web-based electronic markets reflect the dominating content-based systems approach of Web 2.0. Though useful, these electronic markets are far from being believable trading places. Marketplace is where things and traders have presence, constituting a rich interaction space. The believability of the place depends on the believability of the presence and interactions in it, including the players' behaviour and the narrative scenarios of the marketplace. This paper discusses what constitutes the believability of electronic marketplaces and presents the technologies that support it. Believability of electronic marketplaces can be described through three metaphors: ``marketplaces where people are'', ``marketplaces that are alive and engaging'', and ``market places where information is valuable and useful''. The paper presents the core technologies that enable the perceivable believability of electronic marketplaces. It describes a demonstrable prototype of a Web-based electronic marketplace that integrates these technologies. This is part of a larger project that aims to make informed automated trading an enjoyable reality of Web 3.0.",
                "id": "02358"
            },
            {
                "title": "The essential ingredients of collaboration",
                "abstract": "We propose there are eight essential ingredients for collaboration, regardless of any underlying technology - including face-to-face environments. These ingredients are: two or more people; shared space; time; a common objective; focus on the objective; common language; knowledge in the area of the objective; and interaction. These ingredients are useful for identifying aspects for improvement in both collaboration technology and human collaboration processes. Based on these essential ingredients glimpses of future technology can be seen.",
                "id": "02359"
            },
            {
                "title": "Network data mining: discovering patterns of interaction between attributes",
                "abstract": "Network Data Mining identifies emergent networks between myriads of individual data items and utilises special statistical algorithms that aid visualisation of \u2018emergent' patterns and trends in the linkage. It complements predictive data mining methods and methods for outlier detection, which assume the independence between the attributes and the independence between the values of these attributes. Many problems, however, especially phenomena of a more complex nature, are not well suited for these methods. For example, in the analysis of transaction data there are no known suspicious transactions. This paper presents a human-centred methodology and supporting techniques that address the issues of depicting implicit relationships between data attributes and/or specific values of these attributes. The methodology and corresponding techniques are illustrated on a case study from the area of security.",
                "id": "02360"
            },
            {
                "title": "A Visual Analytics Tool for Analysing Microarray Data",
                "abstract": "This paper presents a new visual analytics tool for analysing microarray data with several thousands of attributes. The tool includes two components 1) automated data analysis and 2) interactive visualization. Automated data analysis is used to reduce of the amount of attributes, through the construction of data groups and principle component analysis (PCA). Interactive visualization provides an interface that allows the user to explore the variables of these data groups and to extract more information from all the attributes",
                "id": "02361"
            },
            {
                "title": "Informing the curious negotiator: automatic news extraction from the internet",
                "abstract": "Information acquisition and validation play an important role in the decision making process during negotiation. In this chapter we briefly present the framework of a smart data mining system for providing contextual information extracted from the Internet to a negotiation agent. We then present one of its components in more details \u2013 an effective automated technique for extracting relevant articles from news web sites, so that they can be used further by the mining agents. Most current techniques experience difficulties in coping with changes in web site structure and formats. The proposed extraction process is completely automatic and independent of web site formats. Proposed technique identifies regularities in both format and content of news web sites. The algorithms are applicable to both single- and multi-document web sites. Since invalid URLs can cause errors in data extraction, we also present a method for the negotiation agent to estimate the validity of the extracted data based on the frequency of the relevant words in the news title. Once the news articles are extracted the next task is to construct sets of given articles. This chapter presents a new procedure for constructing news data sets on given topics. The extracted news data set is further utilised by the parties involved in negotiation. The information retrieved from the data set can support both human and automated negotiators.",
                "id": "02362"
            },
            {
                "title": "Supporting Strategic Decision Making in an Enterprise University Through Detecting Patterns of Academic Collaboration",
                "abstract": "Collaborative networks are a topic, broadly researched from several perspectives, including the social network analysis (SNA). The organisations take advantage from the results of SNA to determine collaborative channels, information fusion through such channels and key participants or groups in the network. This work is focused on multi-facet analysis of academic collaboration, as it has been identified as a key factor in success and growth in the global educational market. The data sets include integrated data about different aspects of academic collaboration, including co-authorship, co-participation, co-supervision and other related data. We explore the concept of interestingness and its application to the field of network mining. Composing an appropriate interpretable set of interestingness measures will benefit decision makers in organisations in taking specific actions depending on the patterns in these measures. In this study we focus on interesting measures such as unexpectedness for academic networks and a collaborative score.",
                "id": "02363"
            },
            {
                "title": "Virtual Agents and 3D Virtual Worlds for Preserving and Simulating Cultures",
                "abstract": "Many researchers associate a culture with some form of knowledge; other scholars stress the importance of the environment inhabited by the knowledge carriers; while archaeologists learn about cultures through the objects produced in the environment as a result of utilizing this knowledge. In our work we propose a model of virtual culture that preserves the environment, objects and knowledge associated with a certain culture in a 3D Virtual World. We highlight the significance of virtual agents in our model as, on the one hand, being the knowledge carriers and, on the other hand, being an important element establishing the connection between the environment, objects and knowledge. For testing the resulting model we have developed a research prototype simulating the culture of the ancient City of Uruk 3000 B. C. (one of the first human-built cities on Earth) within a Virtual World of Second Life.",
                "id": "02364"
            },
            {
                "title": "Science Fiction and the Reality of HCI: Inspirations, Achievements or a Mismatch",
                "abstract": "The aim of this workshop is to explore and exchange ideas on topics emerging from both science fiction and Human-Computer Interaction (HCI). In particular, the main aims are to discuss the contradictions between science fiction and HCI, explore and elaborate on various methodologies that can be used to evaluate fictional content, and how fiction can be used to inspire design.",
                "id": "02365"
            },
            {
                "title": "Building intelligent negotiating agents",
                "abstract": "We propose that the key to building intelligent negotiating agents is to take an agent's historic observations as primitive, to model that agent's changing uncertainty in that information, and to use that model as the foundation for the agent's reasoning. We describe an agent architecture, with an attendant theory, that is based on that model. In this approach, the utility of contracts, and the trust and reliability of a trading partner are intermediate concepts that an agent may estimate from its information model. This enables us to describe intelligent agents that are not necessarily utility optimisers, that value information as a commodity, and that build relationships with other agents through the trusted exchange of information as well as contracts.",
                "id": "02366"
            },
            {
                "title": "Recognizing Customers' Mood in 3D Shopping Malls Based on the Trajectories of Their Avatars",
                "abstract": "This paper proposes a method to assess the cognitive state of a human embodied as an avatar inside a 3-dimensional virtual shop. In order to do so we analyze the trajectories of the avatar movements to classify them against the set of predefined prototypes. To perform the classification we use the trajectory comparison algorithm based on the combination of the Levenshtein Distance and the Euclidean Distance. The proposed method is applied in a distributed manner to solving the problem of making autonomous assistants in virtual stores recognize the intentions of the customers.",
                "id": "02367"
            },
            {
                "title": "Customer analytics projects: addressing existing problems with a process that leads to success",
                "abstract": "This article explicitly outlines an approach designed to allow optimal utilisation of Analytics in the industry setting. The paper focuses on the key stages of the Analytics process that have not been identified in previous Analytics methodologies and draws on industry, consulting and research experience to show that correct design of the project trajectory can allow the industry to fully realise the benefits that Analytics has to offer. As the case studies provided demonstrate, it is often the skipping of key stages, especially the preliminary analysis stage, that are currently responsible for preventing success of an Analytics project. It has been shown how, using the outlined approach, project can achieve maximum effectiveness and business buy-in.",
                "id": "02368"
            },
            {
                "title": "Deep Exploration of Multidimensional Data with Linkable Scatterplots.",
                "abstract": "Clarity, simplicity and visual adjustability to the preference of the analyst are key aspects of the visualization techniques required by visual analytics in broad sense. Scatterplots and scatterplot matrices are commonly used for visually analyzing multidimensional multivariate data. This paper presents a new approach for deep visual exploration of large multi-attribute data using linkable scatterplots. Proposed method overcomes the limitations of the single scatterplot by providing more plot panels for better comparison while it reduces the unnecessary number of panels of the scatterplot matrix method. The panels are fully interactive and linking together where variables can be mapped on axes independently or on common visual attributes such as color, size and shape. We illustrate the effectiveness of proposed linkable scatterplot method on various data sets.",
                "id": "02369"
            },
            {
                "title": "Generating diverse ethnic groups with genetic algorithms",
                "abstract": "Simulating large crowds of virtual agents has become an important problem in virtual reality applications, video games, cinematography and training simulators. In this paper, we show how to achieve a high degree of appearance variation among individual 3D avatars in generated crowds through the use of genetic algorithms, while also manifesting unique characteristic features of a given population group. We show how virtual cities can be populated with diverse crowds of virtual agents that preserve their ethnic features, illustrate how our approach can be used to simulate full body avatar appearance, present a case study and analyze our results.",
                "id": "02370"
            },
            {
                "title": "Visualizing large trees with divide & conquer partition",
                "abstract": "While prior works on enclosure approach, guarantees the space utilization of a single geometrical area, mostly rectangle, this paper proposes a flexible enclosure tree layout method for partitioning various polygonal shapes that break through the limitation of rectangular constraint. Similar to Treemap techniques, it uses enclosure to divide display space into smaller areas for its sub-hierarchies. The algorithm can partition a polygonal shape or even an arbitrary shape into smaller polygons, rotated rectangles or vertical-horizontal rectangles. The proposed method and implementation algorithms provide an effective interactive visualization tool for partitioning large hierarchical structures within a confined display area with different shapes for real-time applications. We demonstrated the effective of the new method with a case study, an automated evaluation and a usability study.",
                "id": "02371"
            },
            {
                "title": "Normative Virtual Environments - Integrating Physical And Virtual Under The One Umbrella",
                "abstract": "The paper Outlines a normative approach to the design of distributed applications that Must consistently integrate a number of environments (i.e. form-based interfaces, 3D Virtual Worlds, physical world). The application of the described ideas is illustrated on the example of a fish market, which is an application that can simultaneously be accessed by the people from the physical world, people using form-based interfaces and people embodied in a 3D Virtual World as avatars. The Normative Virtual Environments approach in this case allows for maintaining a consistent causal connection amongst all these environments.",
                "id": "02372"
            },
            {
                "title": "The Honourable Negotiator: When the Relationship Is What Matters",
                "abstract": "Relationships are fundamental to all but the most impersonal forms of interaction in business. Human agents who are unsure of themselves seek honourable trading relationships. The establishment and growth of interpersonal relationships is a result of reciprocal exchange of information. This paper addresses the problem of use of information for developing and utilising relationships between negotiating agents. The presence of measurable information on the Internet underpins the philosophy of transparency in electronic business, which has an impact on the behaviour of involved agents. It takes 'two to tango' for conducting business in such a 'net landscape' where communities and cliques emerge, shape and evolve. The paper presents a formalism for electronic negotiation technology that treats relationships as a commodity. It supports relationship building, maintaining, evolving, and passing to other agents, and utilises such relationships in agent interaction. The Honourable Negotiator also takes in account information about the relationships in networks of respective agents outside the trading space.",
                "id": "02373"
            },
            {
                "title": "Framing Interaction through Engagement in Interactive Open Ended Environments",
                "abstract": "--In this paper we present preliminary pilot study of how people's interactions can be characterized in open-ended environments through the concept of engagement. By open--ended environments we refer to physical spaces that construct content and associated semantics through non-didactic methods supportive of non - linear navigation. This work presents an overview of an approach to interactions as mapping human actions and their characteristics, through which behavior features can be identified. The interpretation is based on the extension of previous work, the Kinetic Inter-Acting System, that interprets, visualises and offers means for analysis of the interaction process between parties enabling visual reasoning about the quality of interactions. The approach is examined in the context of new developments in museums that perceives them as becoming creative and reflective agents in digital humanities. Examples are taken from a large public immersive exhibition that relies upon interaction delivered through various modalities for content assimilation and participant experience.",
                "id": "02374"
            },
            {
                "title": "A Model for Informed Negotiating Agents",
                "abstract": "We propose that the key to building intelligent negotiating agents is to take an agent's historic observations as primitive, to model that agent's changing uncertainty in that information, and to use that model as the foundation for the agent's reasoning. We describe an agent architecture, with an attendant theory, that is based on that model. In this approach, the utility of contracts, and the trust and reliability of a trading partner are intermediate concepts that an agent may estimate from its information model. This enables us to describe intelligent agents that are not necessarily utility optimisers, that value information as a commodity, and that build relationships with other agents through the trusted exchange of information as well as contracts.",
                "id": "02375"
            },
            {
                "title": "IntelliViz- A Tool for Visualizing Social Networks with Hashtags.",
                "abstract": "Visualizing a real-time social network, such as from Twitter, can potentially discover the patterns and insight of actors' interconnectedness and interactions according to the links between actors and activities. This paper presents a novel system for an intelligent and interactive visualization of social networks with hashtags. We provide a flexible, animated and simple, yet powerful, visualization to represent activities, relations and networks of involvers associated with hashtags. The system consists of multiple phases, including data collection and processing, and interactive visualization with intelligence. Early experimental results indicate its effectiveness for real-time analyzing the property of dynamic networks based on hashtags.",
                "id": "02376"
            },
            {
                "title": "Intelligent environments for next-generation e-markets",
                "abstract": "A complete, immersive, distributed virtual trading environment is described. Virtual worlds technology provides an immersive environment in which traders are represented as avatars that interact with each other, and have access to market data and general information that is delivered by data and text mining machinery. To enrich this essentially social market place, synthetic smart bots have also been constructed. They too are represented by avatars, and provide \u201cinformed idle chatter\u201d so enriching the social fabric. The middle-ware in this environment is based on powerful multiagent technology that manages all processes including the information delivery and data mining. The investigation of network evolution leads to the development of new \u201cnetwork mining\u201d techniques.",
                "id": "02377"
            },
            {
                "title": "Developing virtual heritage applications as normative multiagent systems",
                "abstract": "The majority of existing virtual heritage applications are focused on detailed 3D reconstruction of historically significant sites and ancient artifacts. Recreating the way of life of ancient people is only considered by some researchers, who employ crowd simulation for this task. Existing crowd simulation algorithms are not suitable for modeling complex individual behaviors and role dependent agent interactions with other participants in the Virtual World. To address this problem we suggest treating 3D Virtual Worlds as Normative Multiagent Systems and propose the Virtual Institutions Methodology to be used for design and deployment of Virtual Worlds that require complex interactions involving both humans and autonomous agents. To highlight the usefulness of this approach we illustrate how Virtual Institutions are employed in the development of the Uruk prototype, which integrates 3D Virtual Worlds and Artificial Intelligence in the domain of cultural heritage.",
                "id": "02378"
            },
            {
                "title": "A Hierarchical Vqsvm For Imbalanced Data Sets",
                "abstract": "First, a hierarchical modelling method, VQSVM, is introduced, and some remarks are discussed. Secondly the proposed VQSVM is applied to a nonstandard learning environment, imbalanced data sets. In cases of extremely imbalanced dataset with high dimensions, standard machine learning techniques tend to be overwhelmed by the large classes. The hierarchical VQSVM contains a set of local models i.e. codevectors produced by the Vector Quantization and a global model, i.e. Support Vector Machine, to rebalance datasets without significant information loss. Some issues, e.g. distortion and support vectors, have been discussed to address the trade-off between the information loss and undersampling rate. Experiments compare VQSVM with random resampling techniques on some imbalanced datasets with varied imbalance ratios, and results show that the performance of VQSVM is superior or equivalent to random resampling techniques, especially in case of extremely imbalanced large datasets.",
                "id": "02379"
            },
            {
                "title": "Agents for information-rich environments",
                "abstract": "Information-rich environments, such as electronic markets, or even more generally the World Wide Web, require agents that can assimilate and use real-time information flows wisely. A new breed of \u201cinformation-based\u201d agents aim to meet this requirement. They are founded on concepts from information theory, and are designed to operate with information flows of varying and questionable integrity. These agents are part of a larger project that aims to make informed automated trading, in applications such as eProcurement, a reality.",
                "id": "02380"
            },
            {
                "title": "Informed Recommender Agent: Utilizing Consumer Product Reviews through Text Mining",
                "abstract": "Consumer reviews, opinions and shared experiences in the use of a product form a powerful source of information about consumer preferences that can be used for making recommendations. A novel framework, which utilizes this valuable information sources first time to create recommendations in recommender agents was recently developed by the authors [1]. In this recommender agent, the most critical issue is how to convert the review comments into ontology instances that can be understood and utilized by computers. This problem was not addressed in our previous work. This paper presents an automatic mapping process using text mining techniques. The ontology contains a controlled vocabulary and their relationships. The attributes of the ontology are learnt from the semantic features in the review comments using supervised learning techniques. The proposed approach is demonstrated using a case study of digital camera reviews.",
                "id": "02381"
            },
            {
                "title": "An e-market framework for informed trading",
                "abstract": "Fully automated trading, such as e-procurement, using the Internet is virtually unheard of today. Three core technologies are needed to fully automate the trading process: data mining, intelligent trading agents and virtual institutions in which informed trading agents can trade securely both with each other and with human agents in a natural way. This paper describes a demonstrable prototype e-trading system that integrates these three technologies and is available on the World Wide Web. This is part of a larger project that aims to make informed automated trading a reality.",
                "id": "02382"
            },
            {
                "title": "Three Technologies For Automated Trading",
                "abstract": "Three core technologies are needed for automated trading: data mining, intelligent trading agents and virtual institutions in which informed trading agents can trade securely both with each other and with human agents in a natural way. This paper describes a demonstrable prototype that integrates these three technologies and is available on the World Wide Web. This is part of a larger project that aims to make informed automated trading a reality.",
                "id": "02383"
            },
            {
                "title": "Implicit Training of Virtual Agents",
                "abstract": "This paper provides a brief overview of an implicit training method used for teaching autonomous agents to represent humans in 3D Virtual Worlds without any explicit training efforts being required.",
                "id": "02384"
            },
            {
                "title": "Designing a curious negotiator",
                "abstract": "In negotiation the exchange of contextual information is as important as the exchange of specific offers. The curious negotiator is a multiagent system with three types of agent. Two negotiation agents, each representing an individual, develop consecutive offers, supported by information, whilst requesting information from its opponent. A mediator agent, with experience of prior negotiations, suggests how the negotiation may develop. A failed negotiation is a missed opportunity. An observer agent analyses failures looking for new opportunities. The integration of negotiation theory and data mining enables the curious negotiator to discover and exploit negotiation opportunities. Trials will be conducted in electronic business.",
                "id": "02385"
            },
            {
                "title": "CBR with commonsense reasoning and structure mapping: an application to mediation",
                "abstract": "Mediation is an important method in dispute resolution. We implement a case based reasoning approach to mediation integrating analogical and commonsense reasoning components that allow an artificial mediation agent to satisfy requirements expected from a human mediator, in particular: utilizing experience with cases in different domains; and structurally transforming the set of issues for a better solution. We utilize a case structure based on ontologies reflecting the perceptions of the parties in dispute. The analogical reasoning component, employing the Structure Mapping Theory from psychology, provides a flexibility to respond innovatively in unusual circumstances, in contrast with conventional approaches confined into specialized problem domains. We aim to build a mediation case base incorporating real world instances ranging from interpersonal or intergroup disputes to international conflicts.",
                "id": "02386"
            },
            {
                "title": "VQSVM: A case study for incorporating prior domain knowledge into inductive machine learning",
                "abstract": "When dealing with real-world problems, there is considerable amount of prior domain knowledge that can provide insights on various aspect of the problem. On the other hand, many machine learning methods rely solely on the data sets for their learning phase and do not take into account any explicitly expressed domain knowledge. This paper proposes a framework that investigates and enables the incorporation of prior domain knowledge with respect to three key characteristics of inductive machine learning algorithms: consistency, generalization and convergence. The framework is used to review, classify and analyse key existing approaches to incorporating domain knowledge into inductive machine learning, as well as to consider the risks of doing so. The paper also demonstrates the design of a novel hierarchical semi-parametric machine learning method, capable of incorporating prior domain knowledge. The method-VQSVM-extends the support vector machine (SVM) family of methods with vector quantization (VQ) techniques to address the problem of learning from imbalanced data sets. The paper presents the results of testing the method on a collection of imbalanced data sets with various imbalance ratios and various numbers of subclasses. The learning process of the VQSVM method utilizes some domain knowledge to solve problem of fitting imbalance data. The experiments in the paper demonstrate that enabling the incorporation of prior domain knowledge into the SVM framework is an effective way to overcome the sensitivity of SVM towards the imbalance ratio in a data set.",
                "id": "02387"
            }
        ]
    }
]